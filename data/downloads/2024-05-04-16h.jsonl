{"created":"2024-05-02 17:59:52","title":"Customizing Text-to-Image Models with a Single Image Pair","abstract":"Art reinterpretation is the practice of creating a variation of a reference work, making a paired artwork that exhibits a distinct artistic style. We ask if such an image pair can be used to customize a generative model to capture the demonstrated stylistic difference. We propose Pair Customization, a new customization method that learns stylistic difference from a single image pair and then applies the acquired style to the generation process. Unlike existing methods that learn to mimic a single concept from a collection of images, our method captures the stylistic difference between paired images. This allows us to apply a stylistic change without overfitting to the specific image content in the examples. To address this new task, we employ a joint optimization method that explicitly separates the style and content into distinct LoRA weight spaces. We optimize these style and content weights to reproduce the style and content images while encouraging their orthogonality. During inference, we modify the diffusion process via a new style guidance based on our learned weights. Both qualitative and quantitative experiments show that our method can effectively learn style while avoiding overfitting to image content, highlighting the potential of modeling such stylistic differences from a single image pair.","sentences":["Art reinterpretation is the practice of creating a variation of a reference work, making a paired artwork that exhibits a distinct artistic style.","We ask if such an image pair can be used to customize a generative model to capture the demonstrated stylistic difference.","We propose Pair Customization, a new customization method that learns stylistic difference from a single image pair and then applies the acquired style to the generation process.","Unlike existing methods that learn to mimic a single concept from a collection of images, our method captures the stylistic difference between paired images.","This allows us to apply a stylistic change without overfitting to the specific image content in the examples.","To address this new task, we employ a joint optimization method that explicitly separates the style and content into distinct LoRA weight spaces.","We optimize these style and content weights to reproduce the style and content images while encouraging their orthogonality.","During inference, we modify the diffusion process via a new style guidance based on our learned weights.","Both qualitative and quantitative experiments show that our method can effectively learn style while avoiding overfitting to image content, highlighting the potential of modeling such stylistic differences from a single image pair."],"url":"http://arxiv.org/abs/2405.01536v1","category":"cs.CV"}
{"created":"2024-05-02 17:59:35","title":"Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models","abstract":"Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.","sentences":["Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs.","However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations.","On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment.","Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness.","To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements.","Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria.","On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs.","Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval."],"url":"http://arxiv.org/abs/2405.01535v1","category":"cs.CL"}
{"created":"2024-05-02 17:59:31","title":"Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks","abstract":"Large Language Models (LLMs) have been shown to be capable of performing high-level planning for long-horizon robotics tasks, yet existing methods require access to a pre-defined skill library (e.g. picking, placing, pulling, pushing, navigating). However, LLM planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings. Furthermore, for many tasks of interest, the robot needs to be able to adjust its behavior in a fine-grained manner, requiring the agent to be capable of modifying low-level control actions. Can we instead use the internet-scale knowledge from LLMs for high-level policies, guiding reinforcement learning (RL) policies to efficiently solve robotic control tasks online without requiring a pre-determined set of skills? In this paper, we propose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to bridge the gap between abstract language and learned low-level control for solving long-horizon robotics tasks from scratch. We demonstrate that PSL achieves state-of-the-art results on over 25 challenging robotics tasks with up to 10 stages. PSL solves long-horizon tasks from raw visual input spanning four benchmarks at success rates of over 85%, out-performing language-based, classical, and end-to-end approaches. Video results and code at https://mihdalal.github.io/planseqlearn/","sentences":["Large Language Models (LLMs) have been shown to be capable of performing high-level planning for long-horizon robotics tasks, yet existing methods require access to a pre-defined skill library (e.g. picking, placing, pulling, pushing, navigating).","However, LLM planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings.","Furthermore, for many tasks of interest, the robot needs to be able to adjust its behavior in a fine-grained manner, requiring the agent to be capable of modifying low-level control actions.","Can we instead use the internet-scale knowledge from LLMs for high-level policies, guiding reinforcement learning (RL) policies to efficiently solve robotic control tasks online without requiring a pre-determined set of skills?","In this paper, we propose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to bridge the gap between abstract language and learned low-level control for solving long-horizon robotics tasks from scratch.","We demonstrate that PSL achieves state-of-the-art results on over 25 challenging robotics tasks with up to 10 stages.","PSL solves long-horizon tasks from raw visual input spanning four benchmarks at success rates of over 85%, out-performing language-based, classical, and end-to-end approaches.","Video results and code at https://mihdalal.github.io/planseqlearn/"],"url":"http://arxiv.org/abs/2405.01534v1","category":"cs.LG"}
{"created":"2024-05-02 17:59:19","title":"Robustness of Fixed Points of Quantum Channels and Application to Approximate Quantum Markov Chains","abstract":"Given a quantum channel and a state which satisfy a fixed point equation approximately (say, up to an error $\\varepsilon$), can one find a new channel and a state, which are respectively close to the original ones, such that they satisfy an exact fixed point equation? It is interesting to ask this question for different choices of constraints on the structures of the original channel and state, and requiring that these are also satisfied by the new channel and state. We affirmatively answer the above question, under fairly general assumptions on these structures, through a compactness argument. Additionally, for channels and states satisfying certain specific structures, we find explicit upper bounds on the distances between the pairs of channels (and states) in question. When these distances decay quickly (in a particular, desirable manner) as $\\varepsilon\\to 0$, we say that the original approximate fixed point equation is rapidly fixable. We establish rapid fixability, not only for general quantum channels, but also when the original and new channels are both required to be unitary, mixed unitary or unital. In contrast, for the case of bipartite quantum systems with channels acting trivially on one subsystem, we prove that approximate fixed point equations are not rapidly fixable. In this case, the distance to the closest channel (and state) which satisfy an exact fixed point equation can depend on the dimension of the quantum system in an undesirable way. We apply our results on approximate fixed point equations to the question of robustness of quantum Markov chains (QMC) and establish the following: For any tripartite quantum state, there exists a dimension-dependent upper bound on its distance to the set of QMCs, which decays to zero as the conditional mutual information of the state vanishes.","sentences":["Given a quantum channel and a state which satisfy a fixed point equation approximately (say, up to an error $\\varepsilon$), can one find a new channel and a state, which are respectively close to the original ones, such that they satisfy an exact fixed point equation?","It is interesting to ask this question for different choices of constraints on the structures of the original channel and state, and requiring that these are also satisfied by the new channel and state.","We affirmatively answer the above question, under fairly general assumptions on these structures, through a compactness argument.","Additionally, for channels and states satisfying certain specific structures, we find explicit upper bounds on the distances between the pairs of channels (and states) in question.","When these distances decay quickly (in a particular, desirable manner) as $\\varepsilon\\to 0$, we say that the original approximate fixed point equation is rapidly fixable.","We establish rapid fixability, not only for general quantum channels, but also when the original and new channels are both required to be unitary, mixed unitary or unital.","In contrast, for the case of bipartite quantum systems with channels acting trivially on one subsystem, we prove that approximate fixed point equations are not rapidly fixable.","In this case, the distance to the closest channel (and state) which satisfy an exact fixed point equation can depend on the dimension of the quantum system in an undesirable way.","We apply our results on approximate fixed point equations to the question of robustness of quantum Markov chains (QMC) and establish the following: For any tripartite quantum state, there exists a dimension-dependent upper bound on its distance to the set of QMCs, which decays to zero as the conditional mutual information of the state vanishes."],"url":"http://arxiv.org/abs/2405.01532v1","category":"quant-ph"}
{"created":"2024-05-02 17:59:01","title":"Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models","abstract":"Concept Bottleneck Models (CBMs) ground image classification on human-understandable concepts to allow for interpretable model decisions. Crucially, the CBM design inherently allows for human interventions, in which expert users are given the ability to modify potentially misaligned concept choices to influence the decision behavior of the model in an interpretable fashion. However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human feedback is expensive. In this paper, we find that this is noticeably driven by an independent treatment of concepts during intervention, wherein a change of one concept does not influence the use of other ones in the model's final decision. To address this issue, we introduce a trainable concept intervention realignment module, which leverages concept relations to realign concept assignments post-intervention. Across standard, real-world benchmarks, we find that concept realignment can significantly improve intervention efficacy; significantly reducing the number of interventions needed to reach a target classification performance or concept prediction accuracy. In addition, it easily integrates into existing concept-based architectures without requiring changes to the models themselves. This reduced cost of human-model collaboration is crucial to enhancing the feasibility of CBMs in resource-constrained environments.","sentences":["Concept Bottleneck Models (CBMs) ground image classification on human-understandable concepts to allow for interpretable model decisions.","Crucially, the CBM design inherently allows for human interventions, in which expert users are given the ability to modify potentially misaligned concept choices to influence the decision behavior of the model in an interpretable fashion.","However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human feedback is expensive.","In this paper, we find that this is noticeably driven by an independent treatment of concepts during intervention, wherein a change of one concept does not influence the use of other ones in the model's final decision.","To address this issue, we introduce a trainable concept intervention realignment module, which leverages concept relations to realign concept assignments post-intervention.","Across standard, real-world benchmarks, we find that concept realignment can significantly improve intervention efficacy; significantly reducing the number of interventions needed to reach a target classification performance or concept prediction accuracy.","In addition, it easily integrates into existing concept-based architectures without requiring changes to the models themselves.","This reduced cost of human-model collaboration is crucial to enhancing the feasibility of CBMs in resource-constrained environments."],"url":"http://arxiv.org/abs/2405.01531v1","category":"cs.LG"}
{"created":"2024-05-02 17:57:11","title":"Polarization dependent non-Hermitian atomic grating controlled by dipole blockade effect","abstract":"We propose a theoretical scheme for a non-Hermitian atomic grating within an ultra-cold rubidium-87 ($^{87}Rb$) atomic ensemble. The grating's diffraction properties depend on the polarization states of incident photons and are controlled non-locally through Rydberg interactions. Multiple types of polarization-dependent diffraction modes are generated, benefiting from no crosstalk atomic transition channels based on transition selection rules. Those polarization-dependent diffraction modes can be switched using dynamic optical pulse trains, exploiting the Rydberg blockade effect, and are tunable by non-Hermitian optical modulation. Our work will advance the application of asymmetric optical scattering by utilizing the polarization degree of freedom within continuous media and benefit the application of versatile non-Hermitian/asymmetric optical devices.","sentences":["We propose a theoretical scheme for a non-Hermitian atomic grating within an ultra-cold rubidium-87 ($^{87}Rb$) atomic ensemble.","The grating's diffraction properties depend on the polarization states of incident photons and are controlled non-locally through Rydberg interactions.","Multiple types of polarization-dependent diffraction modes are generated, benefiting from no crosstalk atomic transition channels based on transition selection rules.","Those polarization-dependent diffraction modes can be switched using dynamic optical pulse trains, exploiting the Rydberg blockade effect, and are tunable by non-Hermitian optical modulation.","Our work will advance the application of asymmetric optical scattering by utilizing the polarization degree of freedom within continuous media and benefit the application of versatile non-Hermitian/asymmetric optical devices."],"url":"http://arxiv.org/abs/2405.01528v1","category":"physics.atom-ph"}
{"created":"2024-05-02 17:56:55","title":"Track2Act: Predicting Point Tracks from Internet Videos enables Diverse Zero-shot Robot Manipulation","abstract":"We seek to learn a generalizable goal-conditioned policy that enables zero-shot robot manipulation: interacting with unseen objects in novel scenes without test-time adaptation. While typical approaches rely on a large amount of demonstration data for such generalization, we propose an approach that leverages web videos to predict plausible interaction plans and learns a task-agnostic transformation to obtain robot actions in the real world. Our framework,Track2Act predicts tracks of how points in an image should move in future time-steps based on a goal, and can be trained with diverse videos on the web including those of humans and robots manipulating everyday objects. We use these 2D track predictions to infer a sequence of rigid transforms of the object to be manipulated, and obtain robot end-effector poses that can be executed in an open-loop manner. We then refine this open-loop plan by predicting residual actions through a closed loop policy trained with a few embodiment-specific demonstrations. We show that this approach of combining scalably learned track prediction with a residual policy requiring minimal in-domain robot-specific data enables zero-shot robot manipulation, and present a wide array of real-world robot manipulation results across unseen tasks, objects, and scenes. https://homangab.github.io/track2act/","sentences":["We seek to learn a generalizable goal-conditioned policy that enables zero-shot robot manipulation: interacting with unseen objects in novel scenes without test-time adaptation.","While typical approaches rely on a large amount of demonstration data for such generalization, we propose an approach that leverages web videos to predict plausible interaction plans and learns a task-agnostic transformation to obtain robot actions in the real world.","Our framework,Track2Act predicts tracks of how points in an image should move in future time-steps based on a goal, and can be trained with diverse videos on the web including those of humans and robots manipulating everyday objects.","We use these 2D track predictions to infer a sequence of rigid transforms of the object to be manipulated, and obtain robot end-effector poses that can be executed in an open-loop manner.","We then refine this open-loop plan by predicting residual actions through a closed loop policy trained with a few embodiment-specific demonstrations.","We show that this approach of combining scalably learned track prediction with a residual policy requiring minimal in-domain robot-specific data enables zero-shot robot manipulation, and present a wide array of real-world robot manipulation results across unseen tasks, objects, and scenes.","https://homangab.github.io/track2act/"],"url":"http://arxiv.org/abs/2405.01527v1","category":"cs.RO"}
{"created":"2024-05-02 17:56:22","title":"Centerless-BMS charge algebra","abstract":"We show that when the Wald-Zoupas prescription is implemented, the resulting charges realize the BMS symmetry algebra without any 2-cocycle nor central extension, at any cut of future null infinity. We refine the covariance prescription for application to the charge aspects, and introduce a new aspect for Geroch's super-momentum with better covariance properties. For the extended BMS symmetry with singular conformal Killing vectors we find that a Wald-Zoupas symplectic potential exists, if one is willing to modify the symplectic structure by a corner term. The resulting algebra of Noether currents between two arbitrary cuts is center-less. The charge algebra at a given cut has a residual field-dependent 2-cocycle, but time-independent and non-radiative. More precisely, super-rotation fluxes act covariantly, but super-rotation charges act covariantly only on global translations. The take home message is that in any situation where 2-cocycles appears in the literature, covariance has likely been lost in the charge prescription, and that the criterium of covariance is a powerful one to reduce ambiguities in the charges, and can be used also for ambiguities in the charge aspects.","sentences":["We show that when the Wald-Zoupas prescription is implemented, the resulting charges realize the BMS symmetry algebra without any 2-cocycle nor central extension, at any cut of future null infinity.","We refine the covariance prescription for application to the charge aspects, and introduce a new aspect for Geroch's super-momentum with better covariance properties.","For the extended BMS symmetry with singular conformal Killing vectors we find that a Wald-Zoupas symplectic potential exists, if one is willing to modify the symplectic structure by a corner term.","The resulting algebra of Noether currents between two arbitrary cuts is center-less.","The charge algebra at a given cut has a residual field-dependent 2-cocycle, but time-independent and non-radiative.","More precisely, super-rotation fluxes act covariantly, but super-rotation charges act covariantly only on global translations.","The take home message is that in any situation where 2-cocycles appears in the literature, covariance has likely been lost in the charge prescription, and that the criterium of covariance is a powerful one to reduce ambiguities in the charges, and can be used also for ambiguities in the charge aspects."],"url":"http://arxiv.org/abs/2405.01526v1","category":"hep-th"}
{"created":"2024-05-02 17:54:54","title":"FLAME: Factuality-Aware Alignment for Large Language Models","abstract":"Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.","sentences":["Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants.","We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination).","In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL).","In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination.","This makes SFT less factual as it trains on human labeled data that may be novel to the LLM.","Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses.","Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization.","Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability."],"url":"http://arxiv.org/abs/2405.01525v1","category":"cs.CL"}
{"created":"2024-05-02 17:54:35","title":"A separability-based approach to quantifying generalization: which layer is best?","abstract":"Generalization to unseen data remains poorly understood for deep learning classification and foundation models. How can one assess the ability of networks to adapt to new or extended versions of their input space in the spirit of few-shot learning, out-of-distribution generalization, and domain adaptation? Which layers of a network are likely to generalize best? We provide a new method for evaluating the capacity of networks to represent a sampled domain, regardless of whether the network has been trained on all classes in the domain. Our approach is the following: after fine-tuning state-of-the-art pre-trained models for visual classification on a particular domain, we assess their performance on data from related but distinct variations in that domain. Generalization power is quantified as a function of the latent embeddings of unseen data from intermediate layers for both unsupervised and supervised settings. Working throughout all stages of the network, we find that (i) high classification accuracy does not imply high generalizability; and (ii) deeper layers in a model do not always generalize the best, which has implications for pruning. Since the trends observed across datasets are largely consistent, we conclude that our approach reveals (a function of) the intrinsic capacity of the different layers of a model to generalize.","sentences":["Generalization to unseen data remains poorly understood for deep learning classification and foundation models.","How can one assess the ability of networks to adapt to new or extended versions of their input space in the spirit of few-shot learning, out-of-distribution generalization, and domain adaptation?","Which layers of a network are likely to generalize best?","We provide a new method for evaluating the capacity of networks to represent a sampled domain, regardless of whether the network has been trained on all classes in the domain.","Our approach is the following: after fine-tuning state-of-the-art pre-trained models for visual classification on a particular domain, we assess their performance on data from related but distinct variations in that domain.","Generalization power is quantified as a function of the latent embeddings of unseen data from intermediate layers for both unsupervised and supervised settings.","Working throughout all stages of the network, we find that (i) high classification accuracy does not imply high generalizability; and (ii) deeper layers in a model do not always generalize the best, which has implications for pruning.","Since the trends observed across datasets are largely consistent, we conclude that our approach reveals (a function of) the intrinsic capacity of the different layers of a model to generalize."],"url":"http://arxiv.org/abs/2405.01524v1","category":"cs.LG"}
{"created":"2024-05-02 17:50:05","title":"AI for Manufacturing and Healthcare: a chemistry and engineering perspective","abstract":"Artificial Intelligence (AI) approaches are increasingly being applied to more and more domains of Science, Engineering, Chemistry, and Industries to not only improve efficiencies and enhance productivity, but also enable new capabilities. The new opportunities range from automated molecule design and screening, properties prediction, gaining insights of chemical reactions, to computer-aided design, predictive maintenance of systems, robotics, and autonomous vehicles. This review focuses on the new applications of AI in manufacturing and healthcare. For the Manufacturing Industries, we focus on AI and algorithms for (1) Battery, (2) Flow Chemistry, (3) Additive Manufacturing, (4) Sensors, and (5) Machine Vision. For Healthcare applications, we focus on: (1) Medical Vision (2) Diagnosis, (3) Protein Design, and (4) Drug Discovery. In the end, related topics are discussed, including physics integrated machine learning, model explainability, security, and governance during model deployment.","sentences":["Artificial Intelligence (AI) approaches are increasingly being applied to more and more domains of Science, Engineering, Chemistry, and Industries to not only improve efficiencies and enhance productivity, but also enable new capabilities.","The new opportunities range from automated molecule design and screening, properties prediction, gaining insights of chemical reactions, to computer-aided design, predictive maintenance of systems, robotics, and autonomous vehicles.","This review focuses on the new applications of AI in manufacturing and healthcare.","For the Manufacturing Industries, we focus on AI and algorithms for (1) Battery, (2) Flow Chemistry, (3) Additive Manufacturing, (4) Sensors, and (5) Machine Vision.","For Healthcare applications, we focus on: (1) Medical Vision (2) Diagnosis, (3) Protein Design, and (4) Drug Discovery.","In the end, related topics are discussed, including physics integrated machine learning, model explainability, security, and governance during model deployment."],"url":"http://arxiv.org/abs/2405.01520v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 17:48:46","title":"Driven Multiphoton Qubit-Resonator Interactions","abstract":"We develop a general theory for multiphoton qubit-resonator interactions enhanced by a qubit drive. The interactions generate qubit-conditional operations in the resonator when the driving is near $n$-photon cross-resonance, namely, the qubit drive is $n$-times the resonator frequency. We pay special attention to the strong driving regime, where the interactions are conditioned on the qubit dressed states. We consider the specific case where $n=2$, which results in qubit-conditional squeezing (QCS). We propose to use the QCS protocol for amplifying resonator displacements and their superpositions. We find the QCS protocol to generate a superposition of orthogonally squeezed states following a properly chosen qubit measurement. We outline quantum information processing applications for these states, including encoding a qubit in a resonator and performing a quantum non-demolition measurement of the qubit inferred from the resonator's second statistical moment. Next, we employ a two-tone drive to engineer an effective $n$-photon Rabi Hamiltonian in any desired coupling regime. In other words, the effective coupling strengths can be tuned over a wide range, thus allowing for the realization of new regimes that have so far been inaccessible. Finally, we propose a multiphoton circuit QED implementation based on a transmon qubit coupled to a resonator via an asymmetric SQUID. We provide realistic parameter estimates for the two-photon operation regime that can host the aforementioned two-photon protocols. We use numerical simulations to show that even in the presence of spurious terms and decoherence, our analytical predictions are robust.","sentences":["We develop a general theory for multiphoton qubit-resonator interactions enhanced by a qubit drive.","The interactions generate qubit-conditional operations in the resonator when the driving is near $n$-photon cross-resonance, namely, the qubit drive is $n$-times the resonator frequency.","We pay special attention to the strong driving regime, where the interactions are conditioned on the qubit dressed states.","We consider the specific case where $n=2$, which results in qubit-conditional squeezing (QCS).","We propose to use the QCS protocol for amplifying resonator displacements and their superpositions.","We find the QCS protocol to generate a superposition of orthogonally squeezed states following a properly chosen qubit measurement.","We outline quantum information processing applications for these states, including encoding a qubit in a resonator and performing a quantum non-demolition measurement of the qubit inferred from the resonator's second statistical moment.","Next, we employ a two-tone drive to engineer an effective $n$-photon Rabi Hamiltonian in any desired coupling regime.","In other words, the effective coupling strengths can be tuned over a wide range, thus allowing for the realization of new regimes that have so far been inaccessible.","Finally, we propose a multiphoton circuit QED implementation based on a transmon qubit coupled to a resonator via an asymmetric SQUID.","We provide realistic parameter estimates for the two-photon operation regime that can host the aforementioned two-photon protocols.","We use numerical simulations to show that even in the presence of spurious terms and decoherence, our analytical predictions are robust."],"url":"http://arxiv.org/abs/2405.01518v1","category":"quant-ph"}
{"created":"2024-05-02 17:48:10","title":"New Tools for Smoothed Analysis: Least Singular Value Bounds for Random Matrices with Dependent Entries","abstract":"We develop new techniques for proving lower bounds on the least singular value of random matrices with limited randomness. The matrices we consider have entries that are given by polynomials of a few underlying base random variables. This setting captures a core technical challenge for obtaining smoothed analysis guarantees in many algorithmic settings. Least singular value bounds often involve showing strong anti-concentration inequalities that are intricate and much less understood compared to concentration (or large deviation) bounds.   First, we introduce a general technique involving a hierarchical $\\epsilon$-nets to prove least singular value bounds. Our second tool is a new statement about least singular values to reason about higher-order lifts of smoothed matrices, and the action of linear operators on them.   Apart from getting simpler proofs of existing smoothed analysis results, we use these tools to now handle more general families of random matrices. This allows us to produce smoothed analysis guarantees in several previously open settings. These include new smoothed analysis guarantees for power sum decompositions, subspace clustering and certifying robust entanglement of subspaces, where prior work could only establish least singular value bounds for fully random instances or only show non-robust genericity guarantees.","sentences":["We develop new techniques for proving lower bounds on the least singular value of random matrices with limited randomness.","The matrices we consider have entries that are given by polynomials of a few underlying base random variables.","This setting captures a core technical challenge for obtaining smoothed analysis guarantees in many algorithmic settings.","Least singular value bounds often involve showing strong anti-concentration inequalities that are intricate and much less understood compared to concentration (or large deviation) bounds.   ","First, we introduce a general technique involving a hierarchical $\\epsilon$-nets to prove least singular value bounds.","Our second tool is a new statement about least singular values to reason about higher-order lifts of smoothed matrices, and the action of linear operators on them.   ","Apart from getting simpler proofs of existing smoothed analysis results, we use these tools to now handle more general families of random matrices.","This allows us to produce smoothed analysis guarantees in several previously open settings.","These include new smoothed analysis guarantees for power sum decompositions, subspace clustering and certifying robust entanglement of subspaces, where prior work could only establish least singular value bounds for fully random instances or only show non-robust genericity guarantees."],"url":"http://arxiv.org/abs/2405.01517v1","category":"cs.DS"}
{"created":"2024-05-02 17:44:44","title":"Euler Products at the Centre and Applications to Chebyshev's Bias","abstract":"Let $\\pi$ be an irreducible cuspidal automorphic representation of $\\text{GL}_n(\\mathbb A_\\mathbb Q)$ with associated $L$-function $L(s, \\pi)$. We study the behaviour of the partial Euler product of $L(s, \\pi)$ at the center of the critical strip. Under the assumption of the Generalized Riemann Hypothesis for $L(s, \\pi)$ and assuming the Ramanujan--Petersson conjecture when necessary, we establish an asymptotic, off a set of finite logarithmic measure, for the partial Euler product at the central point that confirms a conjecture of Kurokawa. As an application, we obtain results towards Chebyshev's bias in the recently proposed framework of Aoki-Koyama.","sentences":["Let $\\pi$ be an irreducible cuspidal automorphic representation of $\\text{GL}_n(\\mathbb A_\\mathbb Q)$","with associated $L$-function $L(s, \\pi)$. We study the behaviour of the partial Euler product of $L(s, \\pi)$ at the center of the critical strip.","Under the assumption of the Generalized Riemann Hypothesis for $L(s, \\pi)$ and assuming the Ramanujan--Petersson conjecture when necessary, we establish an asymptotic, off a set of finite logarithmic measure, for the partial Euler product at the central point that confirms a conjecture of Kurokawa.","As an application, we obtain results towards Chebyshev's bias in the recently proposed framework of Aoki-Koyama."],"url":"http://arxiv.org/abs/2405.01512v1","category":"math.NT"}
{"created":"2024-05-02 17:38:52","title":"Symbiotic stars in X-rays IV: XMM-Newton, Swift and TESS observations","abstract":"White dwarf symbiotic binaries are detected in X-rays with luminosities in the range of 10$^{30}$ to 10$^{34}$ lumcgs. Their X-ray emission arises either from the accretion disk boundary layer, from a region where the winds from both components collide or from nuclear burning on the white dwarf surface. In our continuous effort to identify X-ray emitting symbiotic stars, we studied four systems using observations from the Neil Gehrels Swift Observatory and XMM-Newton satellites in X-rays and from TESS in the optical. The X-ray spectra were fit with absorbed optically thin thermal plasma models, either single- or multitemperature with kT $<$ 8 keV for all targets. Based on the characteristics of their X-ray spectra, we classified BD Cam as possible $\\beta$-type, V1261 Ori and CD -27 8661 as $\\delta$-type, and confirmed NQ Gem as $\\beta$/$\\delta$-type. The $\\delta$-type X-ray emission most likely arise in the boundary layer of the accretion disk, while in the case of BD Cam, its mostly-soft emission originates from shocks, possibly between the red giant and WD/disk winds. In general, we have found that the observed X-ray emission is powered by accretion at a low accretion rate of about 10$^{-11}$ M$_{\\odot}$ yr$^{-1}$. The low ratio of X-ray to optical luminosities, however indicates that the accretion-disk boundary layer is mostly optically thick and tends to emit in the far or extreme UV. The detection of flickering in optical data provides evidence of the existence of an accretion disk.","sentences":["White dwarf symbiotic binaries are detected in X-rays with luminosities in the range of 10$^{30}$ to 10$^{34}$ lumcgs.","Their X-ray emission arises either from the accretion disk boundary layer, from a region where the winds from both components collide or from nuclear burning on the white dwarf surface.","In our continuous effort to identify X-ray emitting symbiotic stars, we studied four systems using observations from the Neil Gehrels Swift Observatory and XMM-Newton satellites in X-rays and from TESS in the optical.","The X-ray spectra were fit with absorbed optically thin thermal plasma models, either single- or multitemperature with kT $<$ 8 keV for all targets.","Based on the characteristics of their X-ray spectra, we classified BD Cam as possible $\\beta$-type, V1261 Ori and CD -27 8661 as $\\delta$-type, and confirmed NQ Gem as $\\beta$/$\\delta$-type.","The $\\delta$-type X-ray emission most likely arise in the boundary layer of the accretion disk, while in the case of BD Cam, its mostly-soft emission originates from shocks, possibly between the red giant and WD/disk winds.","In general, we have found that the observed X-ray emission is powered by accretion at a low accretion rate of about 10$^{-11}$ M$_{\\odot}$ yr$^{-1}$. The low ratio of X-ray to optical luminosities, however indicates that the accretion-disk boundary layer is mostly optically thick and tends to emit in the far or extreme UV.","The detection of flickering in optical data provides evidence of the existence of an accretion disk."],"url":"http://arxiv.org/abs/2405.01508v1","category":"astro-ph.SR"}
{"created":"2024-05-02 17:35:12","title":"Effective Lifshitz black holes, hydrodynamics, and transport coefficients in fluid/gravity correspondence","abstract":"Effective Lifshitz black holes with arbitrary dynamical exponent are addressed in the fluid/gravity membrane paradigm. The transport and the response coefficients in the dual Lifshitz field theory are calculated and analyzed, including the charge diffusion constant and the shear mode damping constant, along with the shear-viscosity-to-entropy density ratio. The Kubo formula is employed to obtain the electrical DC conductivity for the gauge sector corresponding to impurity through the holographic linear response of gauge vector fluctuations in the Lifshitz black brane geometry.","sentences":["Effective Lifshitz black holes with arbitrary dynamical exponent are addressed in the fluid/gravity membrane paradigm.","The transport and the response coefficients in the dual Lifshitz field theory are calculated and analyzed, including the charge diffusion constant and the shear mode damping constant, along with the shear-viscosity-to-entropy density ratio.","The Kubo formula is employed to obtain the electrical DC conductivity for the gauge sector corresponding to impurity through the holographic linear response of gauge vector fluctuations in the Lifshitz black brane geometry."],"url":"http://arxiv.org/abs/2405.01505v1","category":"hep-th"}
{"created":"2024-05-02 17:32:59","title":"Analyzing the Role of Semantic Representations in the Era of Large Language Models","abstract":"Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations. However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems. In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs? Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting method, which we call AMRCoT, and find that it generally hurts performance more than it helps. To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments. We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction. We recommend focusing on these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm.","sentences":["Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations.","However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems.","In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs?","Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks.","We propose an AMR-driven chain-of-thought prompting method, which we call AMRCoT, and find that it generally hurts performance more than it helps.","To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments.","We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction.","We recommend focusing on these areas for future work in semantic representations for LLMs.","Our code: https://github.com/causalNLP/amr_llm."],"url":"http://arxiv.org/abs/2405.01502v1","category":"cs.CL"}
{"created":"2024-05-02 17:27:56","title":"Auslander-Reiten conjecture for modules whose (self) dual has finite complete intersection dimension","abstract":"Over a commutative Noetherian ring, we show that Auslander-Reiten conjecture holds true for the class of (finitely generated) modules whose dual has finite complete intersection dimension. Our another result validates the conjecture for the class of modules whose self dual has finite complete intersection dimension and either the module or its dual has finite Gorenstein dimension.","sentences":["Over a commutative Noetherian ring, we show that Auslander-Reiten conjecture holds true for the class of (finitely generated) modules whose dual has finite complete intersection dimension.","Our another result validates the conjecture for the class of modules whose self dual has finite complete intersection dimension and either the module or its dual has finite Gorenstein dimension."],"url":"http://arxiv.org/abs/2405.01497v1","category":"math.AC"}
{"created":"2024-05-02 17:27:04","title":"LocInv: Localization-aware Inversion for Text-Guided Image Editing","abstract":"Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at https://github.com/wangkai930418/DPL","sentences":["Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts.","Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts.","However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps.","To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process.","Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt.","Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions.","Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.","The code will be released at https://github.com/wangkai930418/DPL"],"url":"http://arxiv.org/abs/2405.01496v1","category":"cs.CV"}
{"created":"2024-05-02 17:26:52","title":"Navigating Heterogeneity and Privacy in One-Shot Federated Learning with Diffusion Models","abstract":"Federated learning (FL) enables multiple clients to train models collectively while preserving data privacy. However, FL faces challenges in terms of communication cost and data heterogeneity. One-shot federated learning has emerged as a solution by reducing communication rounds, improving efficiency, and providing better security against eavesdropping attacks. Nevertheless, data heterogeneity remains a significant challenge, impacting performance. This work explores the effectiveness of diffusion models in one-shot FL, demonstrating their applicability in addressing data heterogeneity and improving FL performance. Additionally, we investigate the utility of our diffusion model approach, FedDiff, compared to other one-shot FL methods under differential privacy (DP). Furthermore, to improve generated sample quality under DP settings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method, enhancing the effectiveness of generated data for global model training.","sentences":["Federated learning (FL) enables multiple clients to train models collectively while preserving data privacy.","However, FL faces challenges in terms of communication cost and data heterogeneity.","One-shot federated learning has emerged as a solution by reducing communication rounds, improving efficiency, and providing better security against eavesdropping attacks.","Nevertheless, data heterogeneity remains a significant challenge, impacting performance.","This work explores the effectiveness of diffusion models in one-shot FL, demonstrating their applicability in addressing data heterogeneity and improving FL performance.","Additionally, we investigate the utility of our diffusion model approach, FedDiff, compared to other one-shot FL methods under differential privacy (DP).","Furthermore, to improve generated sample quality under DP settings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method, enhancing the effectiveness of generated data for global model training."],"url":"http://arxiv.org/abs/2405.01494v1","category":"cs.CV"}
{"created":"2024-05-02 17:25:33","title":"Exploring Privacy Issues in Mission Critical Communication: Navigating 5G and Beyond Networks","abstract":"Mission critical communication (MCC) involves the exchange of information and data among emergency services, including the police, fire brigade, and other first responders, particularly during emergencies, disasters, or critical incidents. The widely-adopted TETRA (Terrestrial Trunked Radio)-based communication for mission critical services faces challenges including limited data capacity, coverage limitations, spectrum congestion, and security concerns. Therefore, as an alternative, mission critical communication over cellular networks (4G and 5G) has emerged. While cellular-based MCC enables features like real-time video streaming and high-speed data transmission, the involvement of network operators and application service providers in the MCC architecture raises privacy concerns for mission critical users and services. For instance, the disclosure of a policeman's location details to the network operator raises privacy concerns. To the best of our knowledge, no existing work considers the privacy issues in mission critical system with respect to 5G and upcoming technologies. Therefore, in this paper, we analyse the 3GPP standardised MCC architecture within the context of 5G core network concepts and assess the privacy implications for MC users, network entities, and MC servers. The privacy analysis adheres to the deployment strategies in the standard for MCC. Additionally, we explore emerging 6G technologies, such as off-network communications, joint communication and sensing, and non-3GPP communications, to identify privacy challenges in MCC architecture. Finally, we propose privacy controls to establish a next-generation privacy-preserving MCC architecture.","sentences":["Mission critical communication (MCC) involves the exchange of information and data among emergency services, including the police, fire brigade, and other first responders, particularly during emergencies, disasters, or critical incidents.","The widely-adopted TETRA (Terrestrial Trunked Radio)-based communication for mission critical services faces challenges including limited data capacity, coverage limitations, spectrum congestion, and security concerns.","Therefore, as an alternative, mission critical communication over cellular networks (4G and 5G) has emerged.","While cellular-based MCC enables features like real-time video streaming and high-speed data transmission, the involvement of network operators and application service providers in the MCC architecture raises privacy concerns for mission critical users and services.","For instance, the disclosure of a policeman's location details to the network operator raises privacy concerns.","To the best of our knowledge, no existing work considers the privacy issues in mission critical system with respect to 5G and upcoming technologies.","Therefore, in this paper, we analyse the 3GPP standardised MCC architecture within the context of 5G core network concepts and assess the privacy implications for MC users, network entities, and MC servers.","The privacy analysis adheres to the deployment strategies in the standard for MCC.","Additionally, we explore emerging 6G technologies, such as off-network communications, joint communication and sensing, and non-3GPP communications, to identify privacy challenges in MCC architecture.","Finally, we propose privacy controls to establish a next-generation privacy-preserving MCC architecture."],"url":"http://arxiv.org/abs/2405.01492v1","category":"cs.NI"}
{"created":"2024-05-02 17:24:30","title":"Controllable Text Generation in the Instruction-Tuning Era","abstract":"While most research on controllable text generation has focused on steering base Language Models, the emerging instruction-tuning and prompting paradigm offers an alternate approach to controllability. We compile and release ConGenBench, a testbed of 17 different controllable generation tasks, using a subset of it to benchmark the performance of 9 different baselines and methods on Instruction-tuned Language Models. To our surprise, we find that prompting-based approaches outperform controllable text generation methods on most datasets and tasks, highlighting a need for research on controllable text generation with Instruction-tuned Language Models in specific. Prompt-based approaches match human performance on most stylistic tasks while lagging on structural tasks, foregrounding a need to study more varied constraints and more challenging stylistic tasks. To facilitate such research, we provide an algorithm that uses only a task dataset and a Large Language Model with in-context capabilities to automatically generate a constraint dataset. This method eliminates the fields dependence on pre-curated constraint datasets, hence vastly expanding the range of constraints that can be studied in the future.","sentences":["While most research on controllable text generation has focused on steering base Language Models, the emerging instruction-tuning and prompting paradigm offers an alternate approach to controllability.","We compile and release ConGenBench, a testbed of 17 different controllable generation tasks, using a subset of it to benchmark the performance of 9 different baselines and methods on Instruction-tuned Language Models.","To our surprise, we find that prompting-based approaches outperform controllable text generation methods on most datasets and tasks, highlighting a need for research on controllable text generation with Instruction-tuned Language Models in specific.","Prompt-based approaches match human performance on most stylistic tasks while lagging on structural tasks, foregrounding a need to study more varied constraints and more challenging stylistic tasks.","To facilitate such research, we provide an algorithm that uses only a task dataset and a Large Language Model with in-context capabilities to automatically generate a constraint dataset.","This method eliminates the fields dependence on pre-curated constraint datasets, hence vastly expanding the range of constraints that can be studied in the future."],"url":"http://arxiv.org/abs/2405.01490v1","category":"cs.CL"}
{"created":"2024-05-02 17:23:58","title":"Single-layer tensor network approach for three-dimensional quantum systems","abstract":"Calculation of observables with three-dimensional projected entangled pair states is generally hard, as it requires a contraction of complex multi-layer tensor networks. We utilize the multi-layer structure of these tensor networks to largely simplify the contraction. The proposed approach involves the usage of the layer structure both to simplify the search for the boundary projected entangled pair states and the single-layer mapping of the final corner transfer matrix renormalization group contraction. We benchmark our results on the cubic lattice Heisenberg model, reaching the bond dimension D = 7, and find a good agreement with the previous results.","sentences":["Calculation of observables with three-dimensional projected entangled pair states is generally hard, as it requires a contraction of complex multi-layer tensor networks.","We utilize the multi-layer structure of these tensor networks to largely simplify the contraction.","The proposed approach involves the usage of the layer structure both to simplify the search for the boundary projected entangled pair states and the single-layer mapping of the final corner transfer matrix renormalization group contraction.","We benchmark our results on the cubic lattice Heisenberg model, reaching the bond dimension D = 7, and find a good agreement with the previous results."],"url":"http://arxiv.org/abs/2405.01489v1","category":"cond-mat.str-el"}
{"created":"2024-05-02 17:23:04","title":"Digital Twin Generators for Disease Modeling","abstract":"A patient's digital twin is a computational model that describes the evolution of their health over time. Digital twins have the potential to revolutionize medicine by enabling individual-level computer simulations of human health, which can be used to conduct more efficient clinical trials or to recommend personalized treatment options. Due to the overwhelming complexity of human biology, machine learning approaches that leverage large datasets of historical patients' longitudinal health records to generate patients' digital twins are more tractable than potential mechanistic models. In this manuscript, we describe a neural network architecture that can learn conditional generative models of clinical trajectories, which we call Digital Twin Generators (DTGs), that can create digital twins of individual patients. We show that the same neural network architecture can be trained to generate accurate digital twins for patients across 13 different indications simply by changing the training set and tuning hyperparameters. By introducing a general purpose architecture, we aim to unlock the ability to scale machine learning approaches to larger datasets and across more indications so that a digital twin could be created for any patient in the world.","sentences":["A patient's digital twin is a computational model that describes the evolution of their health over time.","Digital twins have the potential to revolutionize medicine by enabling individual-level computer simulations of human health, which can be used to conduct more efficient clinical trials or to recommend personalized treatment options.","Due to the overwhelming complexity of human biology, machine learning approaches that leverage large datasets of historical patients' longitudinal health records to generate patients' digital twins are more tractable than potential mechanistic models.","In this manuscript, we describe a neural network architecture that can learn conditional generative models of clinical trajectories, which we call Digital Twin Generators (DTGs), that can create digital twins of individual patients.","We show that the same neural network architecture can be trained to generate accurate digital twins for patients across 13 different indications simply by changing the training set and tuning hyperparameters.","By introducing a general purpose architecture, we aim to unlock the ability to scale machine learning approaches to larger datasets and across more indications so that a digital twin could be created for any patient in the world."],"url":"http://arxiv.org/abs/2405.01488v1","category":"cs.LG"}
{"created":"2024-05-02 17:22:12","title":"A Formulation of Quantum Fluid Mechanics and Trajectories","abstract":"A formalism of classical mechanics is given for time-dependent many-body states of quantum mechanics, describing both fluid flow and point mass trajectories. The familiar equations of energy, motion, and those of Lagrangian mechanics are obtained. An energy and continuity equation is demonstrated to be equivalent to the real and imaginary parts of the time dependent Schroedinger equation, respectively, where the Schroedinger equation is in density matrix form. For certain stationary states, using Lagrangian mechanics and a Hamiltonian function for quantum mechanics, equations for point-mass trajectories are obtained. For 1-body states and fluid flows, the energy equation and equations of motion are the Bernoulli and Euler equations of fluid mechanics, respectively. Generalizations of the energy and Euler equations are derived to obtain equations that are in the same form as they are in classical mechanics. The fluid flow type is compressible, inviscid, irrotational, with the nonclassical element of local variable mass. Over all space mass is conserved. The variable mass is a necessary condition for the fluid flow to agree with the zero orbital angular momentum for s states of hydrogen. Cross flows are examined, where velocity directions are changed without changing the kinetic energy. For one-electron atoms, the velocity modification gives closed orbits for trajectories, and mass conservation, vortexes, and density stratification for fluid flows. For many body states, Under certain conditions, and by hypotheses, Euler equations of orbital-flows are obtained. One-body Schroedinger equations that are a generalization of the Hartree-Fock equations are also obtained. These equations contain a quantum Coulomb's law, involving the 2-body pair function of reduced density matrix theory that replace the charge densities.","sentences":["A formalism of classical mechanics is given for time-dependent many-body states of quantum mechanics, describing both fluid flow and point mass trajectories.","The familiar equations of energy, motion, and those of Lagrangian mechanics are obtained.","An energy and continuity equation is demonstrated to be equivalent to the real and imaginary parts of the time dependent Schroedinger equation, respectively, where the Schroedinger equation is in density matrix form.","For certain stationary states, using Lagrangian mechanics and a Hamiltonian function for quantum mechanics, equations for point-mass trajectories are obtained.","For 1-body states and fluid flows, the energy equation and equations of motion are the Bernoulli and Euler equations of fluid mechanics, respectively.","Generalizations of the energy and Euler equations are derived to obtain equations that are in the same form as they are in classical mechanics.","The fluid flow type is compressible, inviscid, irrotational, with the nonclassical element of local variable mass.","Over all space mass is conserved.","The variable mass is a necessary condition for the fluid flow to agree with the zero orbital angular momentum for s states of hydrogen.","Cross flows are examined, where velocity directions are changed without changing the kinetic energy.","For one-electron atoms, the velocity modification gives closed orbits for trajectories, and mass conservation, vortexes, and density stratification for fluid flows.","For many body states, Under certain conditions, and by hypotheses, Euler equations of orbital-flows are obtained.","One-body Schroedinger equations that are a generalization of the Hartree-Fock equations are also obtained.","These equations contain a quantum Coulomb's law, involving the 2-body pair function of reduced density matrix theory that replace the charge densities."],"url":"http://arxiv.org/abs/2405.01486v1","category":"quant-ph"}
{"created":"2024-05-02 17:22:12","title":"The Effect of a Dark Matter Core on the Structure of a Rotating Neutron Star","abstract":"Neutron stars represent unique laboratories, offering insights into the physics of supranuclear-density matter and serving as potential hosts for dark matter. This study explores the impact of dark matter cores on rapidly rotating neutron stars through the two-fluid approximation, assuming minimal interaction between baryonic matter and dark matter. The investigation employs phenomenological models for fermionic and bosonic dark matter, revealing that universal relations governing mass and radius changes due to rotation remain largely unaffected in the presence of a dark matter core. Specifically, for a 5 % dark matter mass fraction, the percent deviations in total mass ($M_{tot}$), the baryonic equatorial radius ($R_{Be}$), and polar-to-equatorial baryonic radius ratio ($R_{ratioB}$) are within 3.9 %, 1.8 %, and 1.4 %, respectively. These findings suggest that the universal relations governing neutron star shape can be utilized to infer constraints on the properties of dark matter cores even in cases where the dark matter significantly softens the neutron star's equation of state.","sentences":["Neutron stars represent unique laboratories, offering insights into the physics of supranuclear-density matter and serving as potential hosts for dark matter.","This study explores the impact of dark matter cores on rapidly rotating neutron stars through the two-fluid approximation, assuming minimal interaction between baryonic matter and dark matter.","The investigation employs phenomenological models for fermionic and bosonic dark matter, revealing that universal relations governing mass and radius changes due to rotation remain largely unaffected in the presence of a dark matter core.","Specifically, for a 5 % dark matter mass fraction, the percent deviations in total mass ($M_{tot}$), the baryonic equatorial radius ($R_{Be}$), and polar-to-equatorial baryonic radius ratio ($R_{ratioB}$) are within 3.9 %, 1.8 %, and 1.4 %, respectively.","These findings suggest that the universal relations governing neutron star shape can be utilized to infer constraints on the properties of dark matter cores even in cases where the dark matter significantly softens the neutron star's equation of state."],"url":"http://arxiv.org/abs/2405.01487v1","category":"astro-ph.HE"}
{"created":"2024-05-02 17:18:05","title":"Surviving tidal tails around the Milky Way bulge globular cluster NGC 6355","abstract":"We present results of the analysis of a set of images obtained in the field of the Milky Way bulge globular cluster NGC 6355 using the Dark Energy Camera, which is attached to the 4m Blanco telescope of the Cerro-Tololo Interamerican Observatory. We dealt with a heavy differential absorption across the observed field, a crowded field star population, and the superposition of field stars on to the cluster color-magnitude diagram main features to produce an intrinsic cluster stars density map. The resulting stellar density map reveals the presence of an extended envelope, a tidal tail, and scattered debris; the tidal tails pointing toward the Milky Way center. Such extra-tidal overdensities, detected above the mean star field density, resulted to be between four and six times larger that the local star field density fluctuation. They have also been recently generated by two independent studies which performed numerical simulations of synthetic tidal tails of Milky Way globular clusters. These results contrast with previous theoretical speculations about the possibility to detect tidal tails of globular clusters with chaotic orbits because they would be washed out after they were generated.","sentences":["We present results of the analysis of a set of images obtained in the field of the Milky Way bulge globular cluster NGC 6355 using the Dark Energy Camera, which is attached to the 4m Blanco telescope of the Cerro-Tololo Interamerican Observatory.","We dealt with a heavy differential absorption across the observed field, a crowded field star population, and the superposition of field stars on to the cluster color-magnitude diagram main features to produce an intrinsic cluster stars density map.","The resulting stellar density map reveals the presence of an extended envelope, a tidal tail, and scattered debris; the tidal tails pointing toward the Milky Way center.","Such extra-tidal overdensities, detected above the mean star field density, resulted to be between four and six times larger that the local star field density fluctuation.","They have also been recently generated by two independent studies which performed numerical simulations of synthetic tidal tails of Milky Way globular clusters.","These results contrast with previous theoretical speculations about the possibility to detect tidal tails of globular clusters with chaotic orbits because they would be washed out after they were generated."],"url":"http://arxiv.org/abs/2405.01485v1","category":"astro-ph.GA"}
{"created":"2024-05-02 17:14:57","title":"MANTIS: Interleaved Multi-Image Instruction Tuning","abstract":"The recent years have witnessed a great array of large multimodal models (LMMs) to effectively solve single-image vision language tasks. However, their abilities to solve multi-image visual language tasks is yet to be improved. The existing multi-image LMMs (e.g. OpenFlamingo, Emu, Idefics, etc) mostly gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from web, which is neither efficient nor effective. In this paper, we aim at building strong multi-image LMMs via instruction tuning with academic-level resources. Therefore, we meticulously construct Mantis-Instruct containing 721K instances from 14 multi-image datasets. We design Mantis-Instruct to cover different multi-image skills like co-reference, reasoning, comparing, temporal understanding. We combine Mantis-Instruct with several single-image visual-language datasets to train our model Mantis to handle any interleaved image-text inputs. We evaluate the trained Mantis on five multi-image benchmarks and eight single-image benchmarks. Though only requiring academic-level resources (i.e. 36 hours on 16xA100-40G), Mantis-8B can achieve state-of-the-art performance on all the multi-image benchmarks and beats the existing best multi-image LMM Idefics2-8B by an average of 9 absolute points. We observe that Mantis performs equivalently well on the held-in and held-out evaluation benchmarks. We further evaluate Mantis on single-image benchmarks and demonstrate that Mantis can maintain a strong single-image performance on par with CogVLM and Emu2. Our results are particularly encouraging as it shows that low-cost instruction tuning is indeed much more effective than intensive pre-training in terms of building multi-image LMMs.","sentences":["The recent years have witnessed a great array of large multimodal models (LMMs) to effectively solve single-image vision language tasks.","However, their abilities to solve multi-image visual language tasks is yet to be improved.","The existing multi-image LMMs (e.g. OpenFlamingo, Emu, Idefics, etc) mostly gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from web, which is neither efficient nor effective.","In this paper, we aim at building strong multi-image LMMs via instruction tuning with academic-level resources.","Therefore, we meticulously construct Mantis-Instruct containing 721K instances from 14 multi-image datasets.","We design Mantis-Instruct to cover different multi-image skills like co-reference, reasoning, comparing, temporal understanding.","We combine Mantis-Instruct with several single-image visual-language datasets to train our model Mantis to handle any interleaved image-text inputs.","We evaluate the trained Mantis on five multi-image benchmarks and eight single-image benchmarks.","Though only requiring academic-level resources (i.e. 36 hours on 16xA100-40G), Mantis-8B can achieve state-of-the-art performance on all the multi-image benchmarks and beats the existing best multi-image LMM Idefics2-8B by an average of 9 absolute points.","We observe that Mantis performs equivalently well on the held-in and held-out evaluation benchmarks.","We further evaluate Mantis on single-image benchmarks and demonstrate that Mantis can maintain a strong single-image performance on par with CogVLM and Emu2.","Our results are particularly encouraging as it shows that low-cost instruction tuning is indeed much more effective than intensive pre-training in terms of building multi-image LMMs."],"url":"http://arxiv.org/abs/2405.01483v1","category":"cs.CV"}
{"created":"2024-05-02 17:13:40","title":"NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment","abstract":"Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters. We create NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, our toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort. It is open-sourced with Apache 2.0 License and we invite community contributions at https://github.com/NVIDIA/NeMo-Aligner","sentences":["Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe.","However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters.","We create NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training.","NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN).","Additionally, our toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting.","NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort.","It is open-sourced with Apache 2.0 License and we invite community contributions at https://github.com/NVIDIA/NeMo-Aligner"],"url":"http://arxiv.org/abs/2405.01481v1","category":"cs.CL"}
{"created":"2024-05-02 17:07:25","title":"V-FLUTE: Visual Figurative Language Understanding with Textual Explanations","abstract":"Large Vision-Language models (VLMs) have demonstrated strong reasoning capabilities in tasks requiring a fine-grained understanding of literal images and text, such as visual question-answering or visual entailment. However, there has been little exploration of these models' capabilities when presented with images and captions containing figurative phenomena such as metaphors or humor, the meaning of which is often implicit. To close this gap, we propose a new task and a high-quality dataset: Visual Figurative Language Understanding with Textual Explanations (V-FLUTE). We frame the visual figurative language understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a claim (hypothesis) and justify the predicted label with a textual explanation. Using a human-AI collaboration framework, we build a high-quality dataset, V-FLUTE, that contains 6,027 <image, claim, label, explanation> instances spanning five diverse multimodal figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. The figurative phenomena can be present either in the image, the caption, or both. We further conduct both automatic and human evaluations to assess current VLMs' capabilities in understanding figurative phenomena.","sentences":["Large Vision-Language models (VLMs) have demonstrated strong reasoning capabilities in tasks requiring a fine-grained understanding of literal images and text, such as visual question-answering or visual entailment.","However, there has been little exploration of these models' capabilities when presented with images and captions containing figurative phenomena such as metaphors or humor, the meaning of which is often implicit.","To close this gap, we propose a new task and a high-quality dataset: Visual Figurative Language Understanding with Textual Explanations (V-FLUTE).","We frame the visual figurative language understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a claim (hypothesis) and justify the predicted label with a textual explanation.","Using a human-AI collaboration framework, we build a high-quality dataset, V-FLUTE, that contains 6,027 <image, claim, label, explanation> instances spanning five diverse multimodal figurative phenomena: metaphors, similes, idioms, sarcasm, and humor.","The figurative phenomena can be present either in the image, the caption, or both.","We further conduct both automatic and human evaluations to assess current VLMs' capabilities in understanding figurative phenomena."],"url":"http://arxiv.org/abs/2405.01474v1","category":"cs.CL"}
{"created":"2024-05-02 17:06:19","title":"IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning","abstract":"Imitation learning is a promising paradigm for training robot control policies, but these policies can suffer from distribution shift, where the conditions at evaluation time differ from those in the training data. A popular approach for increasing policy robustness to distribution shift is interactive imitation learning (i.e., DAgger and variants), where a human operator provides corrective interventions during policy rollouts. However, collecting a sufficient amount of interventions to cover the distribution of policy mistakes can be burdensome for human operators. We propose IntervenGen (I-Gen), a novel data generation system that can autonomously produce a large set of corrective interventions with rich coverage of the state space from a small number of human interventions. We apply I-Gen to 4 simulated environments and 1 physical environment with object pose estimation error and show that it can increase policy robustness by up to 39x with only 10 human interventions. Videos and more results are available at https://sites.google.com/view/intervengen2024.","sentences":["Imitation learning is a promising paradigm for training robot control policies, but these policies can suffer from distribution shift, where the conditions at evaluation time differ from those in the training data.","A popular approach for increasing policy robustness to distribution shift is interactive imitation learning (i.e., DAgger and variants), where a human operator provides corrective interventions during policy rollouts.","However, collecting a sufficient amount of interventions to cover the distribution of policy mistakes can be burdensome for human operators.","We propose IntervenGen (I-Gen), a novel data generation system that can autonomously produce a large set of corrective interventions with rich coverage of the state space from a small number of human interventions.","We apply I-Gen to 4 simulated environments and 1 physical environment with object pose estimation error and show that it can increase policy robustness by up to 39x with only 10 human interventions.","Videos and more results are available at https://sites.google.com/view/intervengen2024."],"url":"http://arxiv.org/abs/2405.01472v1","category":"cs.RO"}
{"created":"2024-05-02 17:04:13","title":"Saturation of the Multiparameter Quantum Cram\u00e9r-Rao Bound at the Single-Copy Level with Projective Measurements","abstract":"Quantum parameter estimation theory is an important component of quantum information theory and provides the statistical foundation that underpins important topics such as quantum system identification and quantum waveform estimation. When there is more than one parameter the ultimate precision in the mean square error given by the quantum Cram\\'er-Rao bound is not necessarily achievable. For non-full rank quantum states, it was not known when this bound can be saturated (achieved) when only a single copy of the quantum state encoding the unknown parameters is available. This single-copy scenario is important because of its experimental/practical tractability. Recently, necessary and sufficient conditions for saturability of the quantum Cram\\'er-Rao bound in the multiparameter single-copy scenario have been established in terms of i) the commutativity of a set of projected symmetric logarithmic derivatives and ii) the existence of a unitary solution to a system of coupled nonlinear partial differential equations. New sufficient conditions were also obtained that only depend on properties of the symmetric logarithmic derivatives. In this paper, key structural properties of optimal measurements that saturate the quantum Cram\\'er-Rao bound are illuminated. These properties are exploited to i) show that the sufficient conditions are in fact necessary and sufficient for an optimal measurement to be projective, ii) give an alternative proof of previously established necessary conditions, and iii) describe general POVMs, not necessarily projective, that saturate the multiparameter QCRB. Examples are given where a unitary solution to the system of nonlinear partial differential equations can be explicitly calculated when the required conditions are fulfilled.","sentences":["Quantum parameter estimation theory is an important component of quantum information theory and provides the statistical foundation that underpins important topics such as quantum system identification and quantum waveform estimation.","When there is more than one parameter the ultimate precision in the mean square error given by the quantum Cram\\'er-Rao bound is not necessarily achievable.","For non-full rank quantum states, it was not known when this bound can be saturated (achieved) when only a single copy of the quantum state encoding the unknown parameters is available.","This single-copy scenario is important because of its experimental/practical tractability.","Recently, necessary and sufficient conditions for saturability of the quantum Cram\\'er-Rao bound in the multiparameter single-copy scenario have been established in terms of i) the commutativity of a set of projected symmetric logarithmic derivatives and ii) the existence of a unitary solution to a system of coupled nonlinear partial differential equations.","New sufficient conditions were also obtained that only depend on properties of the symmetric logarithmic derivatives.","In this paper, key structural properties of optimal measurements that saturate the quantum Cram\\'er-Rao bound are illuminated.","These properties are exploited to i) show that the sufficient conditions are in fact necessary and sufficient for an optimal measurement to be projective, ii) give an alternative proof of previously established necessary conditions, and iii) describe general POVMs, not necessarily projective, that saturate the multiparameter QCRB.","Examples are given where a unitary solution to the system of nonlinear partial differential equations can be explicitly calculated when the required conditions are fulfilled."],"url":"http://arxiv.org/abs/2405.01471v1","category":"quant-ph"}
{"created":"2024-05-02 16:59:10","title":"Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning","abstract":"AI Foundation models are gaining traction in various applications, including medical fields like radiology. However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored. We present RayDINO, a large visual encoder trained by self-supervision on 873k chest X-rays. We compare RayDINO to previous state-of-the-art models across nine radiology tasks, from classification and dense segmentation to text generation, and provide an in depth analysis of population, age and sex biases of our model. Our findings suggest that self-supervision allows patient-centric AI proving useful in clinical workflows and interpreting X-rays holistically. With RayDINO and small task-specific adapters, we reach state-of-the-art results and improve generalization to unseen populations while mitigating bias, illustrating the true promise of foundation models: versatility and robustness.","sentences":["AI Foundation models are gaining traction in various applications, including medical fields like radiology.","However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored.","We present RayDINO, a large visual encoder trained by self-supervision on 873k chest X-rays.","We compare RayDINO to previous state-of-the-art models across nine radiology tasks, from classification and dense segmentation to text generation, and provide an in depth analysis of population, age and sex biases of our model.","Our findings suggest that self-supervision allows patient-centric AI proving useful in clinical workflows and interpreting X-rays holistically.","With RayDINO and small task-specific adapters, we reach state-of-the-art results and improve generalization to unseen populations while mitigating bias, illustrating the true promise of foundation models: versatility and robustness."],"url":"http://arxiv.org/abs/2405.01469v1","category":"cs.CV"}
{"created":"2024-05-02 16:59:05","title":"Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models","abstract":"Pre-trained contrastive vision-language models have demonstrated remarkable performance across a wide range of tasks. However, they often struggle on fine-trained datasets with categories not adequately represented during pre-training, which makes adaptation necessary. Recent works have shown promising results by utilizing samples from web-scale databases for retrieval-augmented adaptation, especially in low-data regimes. Despite the empirical success, understanding how retrieval impacts the adaptation of vision-language models remains an open research question. In this work, we adopt a reflective perspective by presenting a systematic study to understand the roles of key components in retrieval-augmented adaptation. We unveil new insights on uni-modal and cross-modal retrieval and highlight the critical role of logit ensemble for effective adaptation. We further present theoretical underpinnings that directly support our empirical observations.","sentences":["Pre-trained contrastive vision-language models have demonstrated remarkable performance across a wide range of tasks.","However, they often struggle on fine-trained datasets with categories not adequately represented during pre-training, which makes adaptation necessary.","Recent works have shown promising results by utilizing samples from web-scale databases for retrieval-augmented adaptation, especially in low-data regimes.","Despite the empirical success, understanding how retrieval impacts the adaptation of vision-language models remains an open research question.","In this work, we adopt a reflective perspective by presenting a systematic study to understand the roles of key components in retrieval-augmented adaptation.","We unveil new insights on uni-modal and cross-modal retrieval and highlight the critical role of logit ensemble for effective adaptation.","We further present theoretical underpinnings that directly support our empirical observations."],"url":"http://arxiv.org/abs/2405.01468v1","category":"cs.LG"}
{"created":"2024-05-02 16:58:17","title":"Student Reflections on Self-Initiated GenAI Use in HCI Education","abstract":"This study explores students' self-initiated use of Generative Artificial Intelligence (GenAI) tools in an interactive systems design class. Through 12 group interviews, students revealed the dual nature of GenAI in (1) stimulating creativity and (2) speeding up design iterations, alongside concerns over its potential to cause shallow learning and reliance. GenAI's benefits were pronounced in the execution phase of design, aiding rapid prototyping and ideation, while its use in initial insight generation posed risks to depth and reflective practice. This reflection highlights the complex role of GenAI in Human-Computer Interaction education, emphasizing the need for balanced integration to leverage its advantages without compromising fundamental learning outcomes.","sentences":["This study explores students' self-initiated use of Generative Artificial Intelligence (GenAI) tools in an interactive systems design class.","Through 12 group interviews, students revealed the dual nature of GenAI in (1) stimulating creativity and (2) speeding up design iterations, alongside concerns over its potential to cause shallow learning and reliance.","GenAI's benefits were pronounced in the execution phase of design, aiding rapid prototyping and ideation, while its use in initial insight generation posed risks to depth and reflective practice.","This reflection highlights the complex role of GenAI in Human-Computer Interaction education, emphasizing the need for balanced integration to leverage its advantages without compromising fundamental learning outcomes."],"url":"http://arxiv.org/abs/2405.01467v1","category":"cs.HC"}
{"created":"2024-05-02 16:53:54","title":"On-demand shaped photon emission based on a parametrically modulated qubit","abstract":"In the circuit quantum electrodynamics architectures, to realize a long-range quantum network mediated by flying photon, it is necessary to shape the temporal profile of emitted photons to achieve high transfer efficiency between two quantum nodes. In this work, we demonstrate a new single-rail and dual-rail time-bin shaped photon generator without additional flux-tunable elements, which can act as a quantum interface of a point-to-point quantum network. In our approach, we adopt a qubit-resonator-transmission line configuration, and the effective coupling strength between the qubit and the resonator can be varied by parametrically modulating the qubit frequency. In this way, the coupling is directly proportional to the parametric modulation amplitude and covers a broad tunable range beyond 20 MHz for the sample we used. Additionally, when emitting shaped photons, we find that the spurious frequency shift (-0.4 MHz) due to parametric modulation is small and can be readily calibrated through chirping. We develop an efficient photon field measurement setup based on the data stream processing of GPU. Utilizing this system, we perform photon temporal profile measurement, quantum state tomography of photon field, and quantum process tomography of single-rail quantum state transfer based on a heterodyne measurement scheme. The single-rail encoding state transfer fidelity of shaped photon emission is 90.32%, and that for unshaped photon is 97.20%, respectively. We believe that the fidelity of shaped photon emission is mainly limited by the qubit coherence time. The results demonstrate that our method is hardware efficient, simple to implement, and scalable. It could become a viable tool in a high-quality quantum network utilizing both single-rail and dual-rail time-bin encoding.","sentences":["In the circuit quantum electrodynamics architectures, to realize a long-range quantum network mediated by flying photon, it is necessary to shape the temporal profile of emitted photons to achieve high transfer efficiency between two quantum nodes.","In this work, we demonstrate a new single-rail and dual-rail time-bin shaped photon generator without additional flux-tunable elements, which can act as a quantum interface of a point-to-point quantum network.","In our approach, we adopt a qubit-resonator-transmission line configuration, and the effective coupling strength between the qubit and the resonator can be varied by parametrically modulating the qubit frequency.","In this way, the coupling is directly proportional to the parametric modulation amplitude and covers a broad tunable range beyond 20 MHz for the sample we used.","Additionally, when emitting shaped photons, we find that the spurious frequency shift (-0.4 MHz) due to parametric modulation is small and can be readily calibrated through chirping.","We develop an efficient photon field measurement setup based on the data stream processing of GPU.","Utilizing this system, we perform photon temporal profile measurement, quantum state tomography of photon field, and quantum process tomography of single-rail quantum state transfer based on a heterodyne measurement scheme.","The single-rail encoding state transfer fidelity of shaped photon emission is 90.32%, and that for unshaped photon is 97.20%, respectively.","We believe that the fidelity of shaped photon emission is mainly limited by the qubit coherence time.","The results demonstrate that our method is hardware efficient, simple to implement, and scalable.","It could become a viable tool in a high-quality quantum network utilizing both single-rail and dual-rail time-bin encoding."],"url":"http://arxiv.org/abs/2405.01464v1","category":"quant-ph"}
{"created":"2024-05-02 16:52:09","title":"Dynamic Local Average Treatment Effects","abstract":"We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors. We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods.","sentences":["We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials.","These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes.","Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors.","We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations.","Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step.","Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods."],"url":"http://arxiv.org/abs/2405.01463v1","category":"econ.EM"}
{"created":"2024-05-02 16:50:47","title":"Uncertainty for Active Learning on Graphs","abstract":"Uncertainty Sampling is an Active Learning strategy that aims to improve the data efficiency of machine learning models by iteratively acquiring labels of data points with the highest uncertainty. While it has proven effective for independent data its applicability to graphs remains under-explored. We propose the first extensive study of Uncertainty Sampling for node classification: (1) We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a significant performance gap to other Active Learning strategies. (2) We develop ground-truth Bayesian uncertainty estimates in terms of the data generating process and prove their effectiveness in guiding Uncertainty Sampling toward optimal queries. We confirm our results on synthetic data and design an approximate approach that consistently outperforms other uncertainty estimators on real datasets. (3) Based on this analysis, we relate pitfalls in modeling uncertainty to existing methods. Our analysis enables and informs the development of principled uncertainty estimation on graphs.","sentences":["Uncertainty Sampling is an Active Learning strategy that aims to improve the data efficiency of machine learning models by iteratively acquiring labels of data points with the highest uncertainty.","While it has proven effective for independent data its applicability to graphs remains under-explored.","We propose the first extensive study of Uncertainty Sampling for node classification: (1) We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a significant performance gap to other Active Learning strategies.","(2) We develop ground-truth Bayesian uncertainty estimates in terms of the data generating process and prove their effectiveness in guiding Uncertainty Sampling toward optimal queries.","We confirm our results on synthetic data and design an approximate approach that consistently outperforms other uncertainty estimators on real datasets.","(3) Based on this analysis, we relate pitfalls in modeling uncertainty to existing methods.","Our analysis enables and informs the development of principled uncertainty estimation on graphs."],"url":"http://arxiv.org/abs/2405.01462v1","category":"cs.LG"}
{"created":"2024-05-02 16:49:25","title":"Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders","abstract":"Unlearnable examples (UEs) seek to maximize testing error by making subtle modifications to training examples that are correctly labeled. Defenses against these poisoning attacks can be categorized based on whether specific interventions are adopted during training. The first approach is training-time defense, such as adversarial training, which can mitigate poisoning effects but is computationally intensive. The other approach is pre-training purification, e.g., image short squeezing, which consists of several simple compressions but often encounters challenges in dealing with various UEs. Our work provides a novel disentanglement mechanism to build an efficient pre-training purification method. Firstly, we uncover rate-constrained variational autoencoders (VAEs), demonstrating a clear tendency to suppress the perturbations in UEs. We subsequently conduct a theoretical analysis for this phenomenon. Building upon these insights, we introduce a disentangle variational autoencoder (D-VAE), capable of disentangling the perturbations with learnable class-wise embeddings. Based on this network, a two-stage purification approach is naturally developed. The first stage focuses on roughly eliminating perturbations, while the second stage produces refined, poison-free results, ensuring effectiveness and robustness across various scenarios. Extensive experiments demonstrate the remarkable performance of our method across CIFAR-10, CIFAR-100, and a 100-class ImageNet-subset. Code is available at https://github.com/yuyi-sd/D-VAE.","sentences":["Unlearnable examples (UEs) seek to maximize testing error by making subtle modifications to training examples that are correctly labeled.","Defenses against these poisoning attacks can be categorized based on whether specific interventions are adopted during training.","The first approach is training-time defense, such as adversarial training, which can mitigate poisoning effects but is computationally intensive.","The other approach is pre-training purification, e.g., image short squeezing, which consists of several simple compressions but often encounters challenges in dealing with various UEs.","Our work provides a novel disentanglement mechanism to build an efficient pre-training purification method.","Firstly, we uncover rate-constrained variational autoencoders (VAEs), demonstrating a clear tendency to suppress the perturbations in UEs.","We subsequently conduct a theoretical analysis for this phenomenon.","Building upon these insights, we introduce a disentangle variational autoencoder (D-VAE), capable of disentangling the perturbations with learnable class-wise embeddings.","Based on this network, a two-stage purification approach is naturally developed.","The first stage focuses on roughly eliminating perturbations, while the second stage produces refined, poison-free results, ensuring effectiveness and robustness across various scenarios.","Extensive experiments demonstrate the remarkable performance of our method across CIFAR-10, CIFAR-100, and a 100-class ImageNet-subset.","Code is available at https://github.com/yuyi-sd/D-VAE."],"url":"http://arxiv.org/abs/2405.01460v1","category":"cs.CR"}
{"created":"2024-05-02 16:44:31","title":"UQA: Corpus for Urdu Question Answering","abstract":"This paper introduces UQA, a novel dataset for question answering and text comprehension in Urdu, a low-resource language with over 70 million native speakers. UQA is generated by translating the Stanford Question Answering Dataset (SQuAD2.0), a large-scale English QA dataset, using a technique called EATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in the translated context paragraphs. The paper describes the process of selecting and evaluating the best translation model among two candidates: Google Translator and Seamless M4T. The paper also benchmarks several state-of-the-art multilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and reports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and 74.56 EM. UQA is a valuable resource for developing and testing multilingual NLP systems for Urdu and for enhancing the cross-lingual transferability of existing models. Further, the paper demonstrates the effectiveness of EATS for creating high-quality datasets for other languages and domains. The UQA dataset and the code are publicly available at www.github.com/sameearif/UQA.","sentences":["This paper introduces UQA, a novel dataset for question answering and text comprehension in Urdu, a low-resource language with over 70 million native speakers.","UQA is generated by translating the Stanford Question Answering Dataset (SQuAD2.0), a large-scale English QA dataset, using a technique called EATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in the translated context paragraphs.","The paper describes the process of selecting and evaluating the best translation model among two candidates: Google Translator and Seamless M4T.","The paper also benchmarks several state-of-the-art multilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and reports promising results.","For XLM-RoBERTa-XL, we have an F1 score of 85.99 and 74.56 EM.","UQA is a valuable resource for developing and testing multilingual NLP systems for Urdu and for enhancing the cross-lingual transferability of existing models.","Further, the paper demonstrates the effectiveness of EATS for creating high-quality datasets for other languages and domains.","The UQA dataset and the code are publicly available at www.github.com/sameearif/UQA."],"url":"http://arxiv.org/abs/2405.01458v1","category":"cs.CL"}
{"created":"2024-05-02 16:43:40","title":"Optimal anisotropies for p-Laplace type operators in the plane","abstract":"Sharp lower and upper uniform estimates are obtained for fundamental frequencies of $p$-Laplace type operators generated by quadratic forms. Optimal constants are exhibited, rigidity of the upper estimate is proved, anisotropic attainability of the lower estimate is derived as well as characterization of anisotropic extremizers for circular and rectangular membranes. Sharp quantitative anisotropic inequalities associated with lower constants are also established, providing as a by-product information on anisotropic stability. When the uniform ellipticity condition is relaxed, we show that the optimal lower constant remains positive, while anisotropic extremizers no longer exist. Our sharp lower estimate can be viewed as an isoanisotropic counterpart of the Faber-Krahn isoperimetric inequality in the plane.","sentences":["Sharp lower and upper uniform estimates are obtained for fundamental frequencies of $p$-Laplace type operators generated by quadratic forms.","Optimal constants are exhibited, rigidity of the upper estimate is proved, anisotropic attainability of the lower estimate is derived as well as characterization of anisotropic extremizers for circular and rectangular membranes.","Sharp quantitative anisotropic inequalities associated with lower constants are also established, providing as a by-product information on anisotropic stability.","When the uniform ellipticity condition is relaxed, we show that the optimal lower constant remains positive, while anisotropic extremizers no longer exist.","Our sharp lower estimate can be viewed as an isoanisotropic counterpart of the Faber-Krahn isoperimetric inequality in the plane."],"url":"http://arxiv.org/abs/2405.01457v1","category":"math.AP"}
{"created":"2024-05-02 16:36:26","title":"Creative Problem Solving in Large Language and Vision Models -- What Would it Take?","abstract":"In this paper, we discuss approaches for integrating Computational Creativity (CC) with research in large language and vision models (LLVMs) to address a key limitation of these models, i.e., creative problem solving. We present preliminary experiments showing how CC principles can be applied to address this limitation through augmented prompting. With this work, we hope to foster discussions of Computational Creativity in the context of ML algorithms for creative problem solving in LLVMs. Our code is at: https://github.com/lnairGT/creative-problem-solving-LLMs","sentences":["In this paper, we discuss approaches for integrating Computational Creativity (CC) with research in large language and vision models (LLVMs) to address a key limitation of these models, i.e., creative problem solving.","We present preliminary experiments showing how CC principles can be applied to address this limitation through augmented prompting.","With this work, we hope to foster discussions of Computational Creativity in the context of ML algorithms for creative problem solving in LLVMs.","Our code is at: https://github.com/lnairGT/creative-problem-solving-LLMs"],"url":"http://arxiv.org/abs/2405.01453v1","category":"cs.AI"}
{"created":"2024-05-02 16:32:41","title":"Convection and the Core $g$-mode in Proto-Compact Stars -- A detailed analysis","abstract":"We present a detailed analysis of the dynamics of proto-compact star (PCS) convection and the core ${}^2\\!g_1$-mode in core-collapse supernovae based on general relativistic 2D and 3D neutrino hydrodynamics simulations. Based on 2D simulations, we derive a mode relation for the core $g$-mode frequency in terms of PCS and equation of state parameters, and discuss its limits of accuracy. This relation may prove useful for parameter inference from future supernova gravitational wave (GW) signals if the core $g$-mode or an emission gap at the avoided crossing with the fundamental mode can be detected. The current 3D simulation does not show GW emission from the core $g$-mode due to less power in high-frequency convective motions to excite the mode, however. Analysing the dynamics of PCS convection in 3D, we find that simple scaling laws for convective velocity from mixing-length theory (MLT) do not apply. Energy and lepton number transport is instead governed by a more complex balance between neutrino fluxes and turbulent fluxes that results in roughly uniform rates of change of entropy and lepton number in the PCS convection zone. Electron fraction and enthalpy contrasts in PCS convection are not well captured by the MLT gradient approximation. We find distinctly different spectra for the turbulent kinetic energy and turbulent fluctuations in the electron fraction, which scale approximately as $l^{-1}$ without a downturn at low $l$. We suggest that the different turbulence spectrum of the electron fraction is naturally expected for a passive scalar quantity.","sentences":["We present a detailed analysis of the dynamics of proto-compact star (PCS) convection and the core ${}^2\\!g_1$-mode in core-collapse supernovae based on general relativistic 2D and 3D neutrino hydrodynamics simulations.","Based on 2D simulations, we derive a mode relation for the core $g$-mode frequency in terms of PCS and equation of state parameters, and discuss its limits of accuracy.","This relation may prove useful for parameter inference from future supernova gravitational wave (GW) signals if the core $g$-mode or an emission gap at the avoided crossing with the fundamental mode can be detected.","The current 3D simulation does not show GW emission from the core $g$-mode due to less power in high-frequency convective motions to excite the mode, however.","Analysing the dynamics of PCS convection in 3D, we find that simple scaling laws for convective velocity from mixing-length theory (MLT) do not apply.","Energy and lepton number transport is instead governed by a more complex balance between neutrino fluxes and turbulent fluxes that results in roughly uniform rates of change of entropy and lepton number in the PCS convection zone.","Electron fraction and enthalpy contrasts in PCS convection are not well captured by the MLT gradient approximation.","We find distinctly different spectra for the turbulent kinetic energy and turbulent fluctuations in the electron fraction, which scale approximately as $l^{-1}$ without a downturn at low $l$. We suggest that the different turbulence spectrum of the electron fraction is naturally expected for a passive scalar quantity."],"url":"http://arxiv.org/abs/2405.01449v1","category":"astro-ph.HE"}
{"created":"2024-05-02 16:31:16","title":"An Exploratory Case Study on Data Breach Journalism","abstract":"This paper explores the novel topic of data breach journalism and data breach news through the case of databreaches.net, a news outlet dedicated to data breaches and related cyber crime. Motivated by the issues in traditional crime news and crime journalism, the case is explored by the means of text mining. According to the results, the outlet has kept a steady publishing pace, mainly focusing on plain and short reporting but with generally high-quality source material for the news articles. Despite these characteristics, the news articles exhibit fairly strong sentiments, which is partially expected due to the presence of emotionally laden crime and the long history of sensationalism in crime news. The news site has also covered the full scope of data breaches, although many of these are fairly traditional, exposing personal identifiers and financial details of the victims. Also hospitals and the healthcare sector stand out. With these results, the paper advances the study of data breaches by considering these from the perspective of media and journalism.","sentences":["This paper explores the novel topic of data breach journalism and data breach news through the case of databreaches.net, a news outlet dedicated to data breaches and related cyber crime.","Motivated by the issues in traditional crime news and crime journalism, the case is explored by the means of text mining.","According to the results, the outlet has kept a steady publishing pace, mainly focusing on plain and short reporting but with generally high-quality source material for the news articles.","Despite these characteristics, the news articles exhibit fairly strong sentiments, which is partially expected due to the presence of emotionally laden crime and the long history of sensationalism in crime news.","The news site has also covered the full scope of data breaches, although many of these are fairly traditional, exposing personal identifiers and financial details of the victims.","Also hospitals and the healthcare sector stand out.","With these results, the paper advances the study of data breaches by considering these from the perspective of media and journalism."],"url":"http://arxiv.org/abs/2405.01446v1","category":"cs.CR"}
{"created":"2024-05-02 16:27:56","title":"On the existence of approximate problems that preserve the type of a bifurcation point of a nonlinear problem. Application to the stationary Navier-Stokes equations. Part 1. The overdetermined extended system","abstract":"We consider a nonlinear problem $F(\\lambda,u)=0$ on infinite-dimensional Banach spaces that correspond to the steady-state bifurcation case. In the literature, it is found again a bifurcation point of the approximate problem $F_{h}(\\lambda_{h},u_{h})=0$ only in some cases. We prove that, in every situation, given $F_{h}$ that approximates $F$, there exists an approximate problem $F_{h}(\\lambda_{h},u_{h})-\\varrho_{h} = 0$ that has a bifurcation point with the same properties as the bifurcation point of $F(\\lambda,u)=0$. First, we formulate, for a function $\\widehat{F}$ defined on general Banach spaces, some sufficient conditions for the existence of an equation that has a bifurcation point of certain type. For the proof of this result, we use some methods from variational analysis, Graves' theorem, one of its consequences and the contraction mapping principle for set-valued mappings. These techniques allow us to prove the existence of a solution with some desired components that equal zero of an overdetermined extended system. We then obtain the existence of a constant (or a function) $\\widehat{\\varrho}$ so that the equation $\\widehat{F}(\\lambda,u)-\\widehat{\\varrho} = 0$ has a bifurcation point of certain type. This equation has $\\widehat{F}(\\lambda,u) = 0$ as a perturbation. It is also made evident a class of maps $C^{p}$ - equivalent (right equivalent) at the bifurcation point to $\\widehat{F}(\\lambda,u)-\\widehat{\\varrho}$ at the bifurcation point. Then, for the study of the approximation of $F(\\lambda,u)=0$, we give conditions that relate the exact and the approximate functions. As an application of the theorem on general Banach spaces, we formulate conditions in order to obtain the existence of the approximate equation $F_{h}(\\lambda_{h},u_{h})-\\varrho_{h} = 0$.","sentences":["We consider a nonlinear problem $F(\\lambda,u)=0$ on infinite-dimensional Banach spaces that correspond to the steady-state bifurcation case.","In the literature, it is found again a bifurcation point of the approximate problem $F_{h}(\\lambda_{h},u_{h})=0$ only in some cases.","We prove that, in every situation, given $F_{h}$ that approximates $F$, there exists an approximate problem $F_{h}(\\lambda_{h},u_{h})-\\varrho_{h} = 0$ that has a bifurcation point with the same properties as the bifurcation point of $F(\\lambda,u)=0$. First, we formulate, for a function $\\widehat{F}$ defined on general Banach spaces, some sufficient conditions for the existence of an equation that has a bifurcation point of certain type.","For the proof of this result, we use some methods from variational analysis, Graves' theorem, one of its consequences and the contraction mapping principle for set-valued mappings.","These techniques allow us to prove the existence of a solution with some desired components that equal zero of an overdetermined extended system.","We then obtain the existence of a constant (or a function) $\\widehat{\\varrho}$ so that the equation $\\widehat{F}(\\lambda,u)-\\widehat{\\varrho} = 0$ has a bifurcation point of certain type.","This equation has $\\widehat{F}(\\lambda,u) = 0$ as a perturbation.","It is also made evident a class of maps $C^{p}$ - equivalent (right equivalent) at the bifurcation point to $\\widehat{F}(\\lambda,u)-\\widehat{\\varrho}$ at the bifurcation point.","Then, for the study of the approximation of $F(\\lambda,u)=0$, we give conditions that relate the exact and the approximate functions.","As an application of the theorem on general Banach spaces, we formulate conditions in order to obtain the existence of the approximate equation $F_{h}(\\lambda_{h},u_{h})-\\varrho_{h} = 0$."],"url":"http://arxiv.org/abs/2405.01443v1","category":"math.NA"}
{"created":"2024-05-02 16:27:52","title":"Market Power and Withholding Behavior of Energy Storage Units","abstract":"Electricity markets are experiencing a rapid increase in energy storage unit participation. Unlike conventional generation resources, quantifying the competitive operation and identifying if a storage unit is exercising market power is challenging, particularly in the context of multi-interval bidding strategies. We present a framework to differentiate strategic capacity withholding behaviors attributed to market power from inherent competitive bidding in storage unit strategies. Our framework evaluates the profitability of strategic storage unit participation, analyzing bidding behaviors as both price takers and price makers using a self-scheduling model, and investigates how they leverage market inefficiencies. Specifically, we propose a price sensitivity model derived from the linear supply function equilibrium model to examine the price-anticipating bidding strategy, effectively capturing the influence of market power. We introduce a sufficient ex-post analysis for market operators to identify potential exploitative behaviors by monitoring instances of withholding within the bidding profiles, ensuring market resilience and competitiveness. We discuss and verify applicability of the proposed framework to realistic settings. Our analysis substantiates commonly observed economic bidding behaviors of storage units. Furthermore, it demonstrates that significant price volatility offers considerable profit opportunities not only for participants possessing market power but also for typical strategic profit seekers.","sentences":["Electricity markets are experiencing a rapid increase in energy storage unit participation.","Unlike conventional generation resources, quantifying the competitive operation and identifying if a storage unit is exercising market power is challenging, particularly in the context of multi-interval bidding strategies.","We present a framework to differentiate strategic capacity withholding behaviors attributed to market power from inherent competitive bidding in storage unit strategies.","Our framework evaluates the profitability of strategic storage unit participation, analyzing bidding behaviors as both price takers and price makers using a self-scheduling model, and investigates how they leverage market inefficiencies.","Specifically, we propose a price sensitivity model derived from the linear supply function equilibrium model to examine the price-anticipating bidding strategy, effectively capturing the influence of market power.","We introduce a sufficient ex-post analysis for market operators to identify potential exploitative behaviors by monitoring instances of withholding within the bidding profiles, ensuring market resilience and competitiveness.","We discuss and verify applicability of the proposed framework to realistic settings.","Our analysis substantiates commonly observed economic bidding behaviors of storage units.","Furthermore, it demonstrates that significant price volatility offers considerable profit opportunities not only for participants possessing market power but also for typical strategic profit seekers."],"url":"http://arxiv.org/abs/2405.01442v1","category":"eess.SY"}
{"created":"2024-05-02 16:26:37","title":"Improving Domain Generalization on Gaze Estimation via Branch-out Auxiliary Regularization","abstract":"Despite remarkable advancements, mainstream gaze estimation techniques, particularly appearance-based methods, often suffer from performance degradation in uncontrolled environments due to variations in illumination and individual facial attributes. Existing domain adaptation strategies, limited by their need for target domain samples, may fall short in real-world applications. This letter introduces Branch-out Auxiliary Regularization (BAR), an innovative method designed to boost gaze estimation's generalization capabilities without requiring direct access to target domain data. Specifically, BAR integrates two auxiliary consistency regularization branches: one that uses augmented samples to counteract environmental variations, and another that aligns gaze directions with positive source domain samples to encourage the learning of consistent gaze features. These auxiliary pathways strengthen the core network and are integrated in a smooth, plug-and-play manner, facilitating easy adaptation to various other models. Comprehensive experimental evaluations on four cross-dataset tasks demonstrate the superiority of our approach.","sentences":["Despite remarkable advancements, mainstream gaze estimation techniques, particularly appearance-based methods, often suffer from performance degradation in uncontrolled environments due to variations in illumination and individual facial attributes.","Existing domain adaptation strategies, limited by their need for target domain samples, may fall short in real-world applications.","This letter introduces Branch-out Auxiliary Regularization (BAR), an innovative method designed to boost gaze estimation's generalization capabilities without requiring direct access to target domain data.","Specifically, BAR integrates two auxiliary consistency regularization branches: one that uses augmented samples to counteract environmental variations, and another that aligns gaze directions with positive source domain samples to encourage the learning of consistent gaze features.","These auxiliary pathways strengthen the core network and are integrated in a smooth, plug-and-play manner, facilitating easy adaptation to various other models.","Comprehensive experimental evaluations on four cross-dataset tasks demonstrate the superiority of our approach."],"url":"http://arxiv.org/abs/2405.01439v1","category":"cs.CV"}
{"created":"2024-05-02 16:25:16","title":"StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation","abstract":"For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.","sentences":["For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge.","In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner.","To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor.","It is trained to estimate the motion conditions between two provided images in the semantic spaces.","This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation.","By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents.","The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications.","Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion."],"url":"http://arxiv.org/abs/2405.01434v1","category":"cs.CV"}
{"created":"2024-05-02 16:22:59","title":"Inflationary complexity of thermal state","abstract":"In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models. First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect. For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter. By utilizing the method of open system, we first construct the wave function. As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects. For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe. Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function. Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases.","sentences":["In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models.","First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect.","For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter.","By utilizing the method of open system, we first construct the wave function.","As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects.","For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe.","Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function.","Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases."],"url":"http://arxiv.org/abs/2405.01433v1","category":"hep-th"}
{"created":"2024-05-02 16:18:51","title":"Quantum Field Theory of Black Hole Perturbations with Backreaction IV. Spherically symmetric 2nd order Einstein-Maxwell sector in generalised gauges","abstract":"In previous papers of this series we analysed the reduced phase space approach to perturbations of Einstein-Maxwell theory to second order around spherically symmetric backgrounds in the Gullstrand Painlev\\'e Gauge and confirmed consistency with previous approaches. In this paper we generalize this result and show that the analysis can be performed in gauges for the background variables compatible with the Gullstrand Painlev\\'e gauge. We obtain the same structure for the reduced Hamiltonian that contains the well known Regge-Wheeler and Zerilli potentials. Possible applications of this generalization are discussed.","sentences":["In previous papers of this series we analysed the reduced phase space approach to perturbations of Einstein-Maxwell theory to second order around spherically symmetric backgrounds in the Gullstrand Painlev\\'e Gauge and confirmed consistency with previous approaches.","In this paper we generalize this result and show that the analysis can be performed in gauges for the background variables compatible with the Gullstrand Painlev\\'e gauge.","We obtain the same structure for the reduced Hamiltonian that contains the well known Regge-Wheeler and Zerilli potentials.","Possible applications of this generalization are discussed."],"url":"http://arxiv.org/abs/2405.01430v1","category":"gr-qc"}
{"created":"2024-05-02 16:11:52","title":"Systematic Construction of Golay Complementary Sets of Arbitrary Lengths and Alphabet Sizes","abstract":"One of the important applications of Golay complementary sets (GCSs) is the reduction of peak-to-mean envelope power ratio (PMEPR) in orthogonal frequency division multiplexing (OFDM) systems. OFDM has played a major role in modern wireless systems such as long-term-evolution (LTE), 5th generation (5G) wireless standards, etc. This paper searches for systematic constructions of GCSs of arbitrary lengths and alphabet sizes. The proposed constructions are based on extended Boolean functions (EBFs). For the first time, we can generate codes of independent parameter choices.","sentences":["One of the important applications of Golay complementary sets (GCSs) is the reduction of peak-to-mean envelope power ratio (PMEPR) in orthogonal frequency division multiplexing (OFDM) systems.","OFDM has played a major role in modern wireless systems such as long-term-evolution (LTE), 5th generation (5G) wireless standards, etc.","This paper searches for systematic constructions of GCSs of arbitrary lengths and alphabet sizes.","The proposed constructions are based on extended Boolean functions (EBFs).","For the first time, we can generate codes of independent parameter choices."],"url":"http://arxiv.org/abs/2405.01421v1","category":"cs.IT"}
{"created":"2024-05-02 16:08:08","title":"Natural Language to Verilog: Design of a Recurrent Spiking Neural Network using Large Language Models and ChatGPT","abstract":"This paper investigates the use of Large Language Models (LLMs) for automating the generation of hardware description code, aiming to explore their potential in supporting and enhancing the development of efficient neuromorphic computing architectures. Building on our prior work, we employ OpenAI's ChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a programmable recurrent spiking neural network, while also generating test benches to assess the system's correctness. The resultant design was validated in three case studies, the exclusive OR,the IRIS flower classification and the MNIST hand-written digit classification, achieving accuracies of up to 96.6%. To verify its synthesizability and implementability, the design was prototyped on a field-programmable gate array and implemented on SkyWater 130 nm technology by using an open-source electronic design automation flow. Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication program to further evaluate the system on-chip performance in the future.","sentences":["This paper investigates the use of Large Language Models (LLMs) for automating the generation of hardware description code, aiming to explore their potential in supporting and enhancing the development of efficient neuromorphic computing architectures.","Building on our prior work, we employ OpenAI's ChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a programmable recurrent spiking neural network, while also generating test benches to assess the system's correctness.","The resultant design was validated in three case studies, the exclusive OR,the IRIS flower classification and the MNIST hand-written digit classification, achieving accuracies of up to 96.6%.","To verify its synthesizability and implementability, the design was prototyped on a field-programmable gate array and implemented on SkyWater 130 nm technology by using an open-source electronic design automation flow.","Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication program to further evaluate the system on-chip performance in the future."],"url":"http://arxiv.org/abs/2405.01419v1","category":"cs.AR"}
{"created":"2024-05-02 16:04:30","title":"MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors","abstract":"Large 2D vision-language models (2D-LLMs) have gained significant attention by bridging Large Language Models (LLMs) with images using a simple projector. Inspired by their success, large 3D point cloud-language models (3D-LLMs) also integrate point clouds into LLMs. However, directly aligning point clouds with LLM requires expensive training costs, typically in hundreds of GPU-hours on A100, which hinders the development of 3D-LLMs. In this paper, we introduce MiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA results while training for only 27 hours on one RTX 3090. Specifically, we propose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which can leverage the similarity between 2D and 3D visual information. We introduce a novel four-stage training strategy for modality alignment in a cascaded way, and a mixture of query experts module to adaptively aggregate features with high efficiency. Moreover, we utilize parameter-efficient fine-tuning methods LoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which is up to 260x fewer than existing methods. Extensive experiments show that MiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with significantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12 increase on GPT-4 evaluation score for the challenging object captioning task compared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800. We are the first to explore the efficient 3D-LLM, offering new insights to the community. Code and weights are available at https://github.com/TangYuan96/MiniGPT-3D.","sentences":["Large 2D vision-language models (2D-LLMs) have gained significant attention by bridging Large Language Models (LLMs) with images using a simple projector.","Inspired by their success, large 3D point cloud-language models (3D-LLMs) also integrate point clouds into LLMs.","However, directly aligning point clouds with LLM requires expensive training costs, typically in hundreds of GPU-hours on A100, which hinders the development of 3D-LLMs.","In this paper, we introduce MiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA results while training for only 27 hours on one RTX 3090.","Specifically, we propose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which can leverage the similarity between 2D and 3D visual information.","We introduce a novel four-stage training strategy for modality alignment in a cascaded way, and a mixture of query experts module to adaptively aggregate features with high efficiency.","Moreover, we utilize parameter-efficient fine-tuning methods LoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which is up to 260x fewer than existing methods.","Extensive experiments show that MiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with significantly cheaper training costs.","Notably, MiniGPT-3D gains an 8.12 increase on GPT-4 evaluation score for the challenging object captioning task compared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.","We are the first to explore the efficient 3D-LLM, offering new insights to the community.","Code and weights are available at https://github.com/TangYuan96/MiniGPT-3D."],"url":"http://arxiv.org/abs/2405.01413v1","category":"cs.CV"}
{"created":"2024-05-02 16:01:58","title":"Goal-conditioned reinforcement learning for ultrasound navigation guidance","abstract":"Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for diagnostic and interventional procedures. However, using it effectively requires extensive training due to the intricate nature of image acquisition and interpretation. To enhance the efficiency of novice sonographers and reduce variability in scan acquisitions, we propose a novel ultrasound (US) navigation assistance method based on contrastive learning as goal-conditioned reinforcement learning (GCRL). We augment the previous framework using a novel contrastive patient batching method (CPB) and a data-augmented contrastive loss, both of which we demonstrate are essential to ensure generalization to anatomical variations across patients. The proposed framework enables navigation to both standard diagnostic as well as intricate interventional views with a single model. Our method was developed with a large dataset of 789 patients and obtained an average error of 6.56 mm in position and 9.36 degrees in angle on a testing dataset of 140 patients, which is competitive or superior to models trained on individual views. Furthermore, we quantitatively validate our method's ability to navigate to interventional views such as the Left Atrial Appendage (LAA) view used in LAA closure. Our approach holds promise in providing valuable guidance during transesophageal ultrasound examinations, contributing to the advancement of skill acquisition for cardiac ultrasound practitioners.","sentences":["Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for diagnostic and interventional procedures.","However, using it effectively requires extensive training due to the intricate nature of image acquisition and interpretation.","To enhance the efficiency of novice sonographers and reduce variability in scan acquisitions, we propose a novel ultrasound (US) navigation assistance method based on contrastive learning as goal-conditioned reinforcement learning (GCRL).","We augment the previous framework using a novel contrastive patient batching method (CPB) and a data-augmented contrastive loss, both of which we demonstrate are essential to ensure generalization to anatomical variations across patients.","The proposed framework enables navigation to both standard diagnostic as well as intricate interventional views with a single model.","Our method was developed with a large dataset of 789 patients and obtained an average error of 6.56 mm in position and 9.36 degrees in angle on a testing dataset of 140 patients, which is competitive or superior to models trained on individual views.","Furthermore, we quantitatively validate our method's ability to navigate to interventional views such as the Left Atrial Appendage (LAA) view used in LAA closure.","Our approach holds promise in providing valuable guidance during transesophageal ultrasound examinations, contributing to the advancement of skill acquisition for cardiac ultrasound practitioners."],"url":"http://arxiv.org/abs/2405.01409v1","category":"cs.CV"}
{"created":"2024-05-02 15:58:40","title":"Graviton-photon oscillations as a probe of quantum gravity","abstract":"The Gertsenshtein effect could in principle be used to detect a single graviton by firing it through a region filled with a constant magnetic field that enables its conversion to a photon, which can be efficiently detected via standard techniques. The quantization of the gravitational field could then be inferred indirectly. We show that for currently available single-photon detector technology, the Gertsenshtein detector is generically inefficient, meaning that the probability of detection is $\\ll 1$. The Gertsenshtein detector can become efficient on astrophysical scales for futuristic single-photon detectors sensitive to frequencies in the Hz to kHz range. It is not clear whether such devices are in principle possible.","sentences":["The Gertsenshtein effect could in principle be used to detect a single graviton by firing it through a region filled with a constant magnetic field that enables its conversion to a photon, which can be efficiently detected via standard techniques.","The quantization of the gravitational field could then be inferred indirectly.","We show that for currently available single-photon detector technology, the Gertsenshtein detector is generically inefficient, meaning that the probability of detection is $\\ll 1$.","The Gertsenshtein detector can become efficient on astrophysical scales for futuristic single-photon detectors sensitive to frequencies in the Hz to kHz range.","It is not clear whether such devices are in principle possible."],"url":"http://arxiv.org/abs/2405.01407v1","category":"gr-qc"}
{"created":"2024-05-02 15:54:36","title":"Unsupervised Flow Discovery from Task-oriented Dialogues","abstract":"The design of dialogue flows is a critical but time-consuming task when developing task-oriented dialogue (TOD) systems. We propose an approach for the unsupervised discovery of flows from dialogue history, thus making the process applicable to any domain for which such an history is available. Briefly, utterances are represented in a vector space and clustered according to their semantic similarity. Clusters, which can be seen as dialogue states, are then used as the vertices of a transition graph for representing the flows visually. We present concrete examples of flows, discovered from MultiWOZ, a public TOD dataset. We further elaborate on their significance and relevance for the underlying conversations and introduce an automatic validation metric for their assessment. Experimental results demonstrate the potential of the proposed approach for extracting meaningful flows from task-oriented conversations.","sentences":["The design of dialogue flows is a critical but time-consuming task when developing task-oriented dialogue (TOD) systems.","We propose an approach for the unsupervised discovery of flows from dialogue history, thus making the process applicable to any domain for which such an history is available.","Briefly, utterances are represented in a vector space and clustered according to their semantic similarity.","Clusters, which can be seen as dialogue states, are then used as the vertices of a transition graph for representing the flows visually.","We present concrete examples of flows, discovered from MultiWOZ, a public TOD dataset.","We further elaborate on their significance and relevance for the underlying conversations and introduce an automatic validation metric for their assessment.","Experimental results demonstrate the potential of the proposed approach for extracting meaningful flows from task-oriented conversations."],"url":"http://arxiv.org/abs/2405.01403v1","category":"cs.CL"}
{"created":"2024-05-02 15:53:43","title":"Learning Force Control for Legged Manipulation","abstract":"Controlling contact forces during interactions is critical for locomotion and manipulation tasks. While sim-to-real reinforcement learning (RL) has succeeded in many contact-rich problems, current RL methods achieve forceful interactions implicitly without explicitly regulating forces. We propose a method for training RL policies for direct force control without requiring access to force sensing. We showcase our method on a whole-body control platform of a quadruped robot with an arm. Such force control enables us to perform gravity compensation and impedance control, unlocking compliant whole-body manipulation. The learned whole-body controller with variable compliance makes it intuitive for humans to teleoperate the robot by only commanding the manipulator, and the robot's body adjusts automatically to achieve the desired position and force. Consequently, a human teleoperator can easily demonstrate a wide variety of loco-manipulation tasks. To the best of our knowledge, we provide the first deployment of learned whole-body force control in legged manipulators, paving the way for more versatile and adaptable legged robots.","sentences":["Controlling contact forces during interactions is critical for locomotion and manipulation tasks.","While sim-to-real reinforcement learning (RL) has succeeded in many contact-rich problems, current RL methods achieve forceful interactions implicitly without explicitly regulating forces.","We propose a method for training RL policies for direct force control without requiring access to force sensing.","We showcase our method on a whole-body control platform of a quadruped robot with an arm.","Such force control enables us to perform gravity compensation and impedance control, unlocking compliant whole-body manipulation.","The learned whole-body controller with variable compliance makes it intuitive for humans to teleoperate the robot by only commanding the manipulator, and the robot's body adjusts automatically to achieve the desired position and force.","Consequently, a human teleoperator can easily demonstrate a wide variety of loco-manipulation tasks.","To the best of our knowledge, we provide the first deployment of learned whole-body force control in legged manipulators, paving the way for more versatile and adaptable legged robots."],"url":"http://arxiv.org/abs/2405.01402v1","category":"cs.RO"}
{"created":"2024-05-02 15:38:58","title":"Simple rules for two-photon state preparation with linear optics","abstract":"Entangling photons is a critical challenge for photonic quantum information processing: entanglement is a crucial resource for quantum communication and computation but can only be performed in a probabilistic manner when using linear optics. In this work, we leverage a two-photon state matrix representation to derive necessary and sufficient conditions on two-photon entangling operations with linear optics. We give a characterization of the input photonic states that can be used to prepare arbitrary two-qudit states in d-rail encoding with post-selection. We determine how many auxiliary photons are required to prepare any two-photon state with heralding. In addition, we present a construction for generalized post-selected n-qubit control-rotation gates.","sentences":["Entangling photons is a critical challenge for photonic quantum information processing: entanglement is a crucial resource for quantum communication and computation but can only be performed in a probabilistic manner when using linear optics.","In this work, we leverage a two-photon state matrix representation to derive necessary and sufficient conditions on two-photon entangling operations with linear optics.","We give a characterization of the input photonic states that can be used to prepare arbitrary two-qudit states in d-rail encoding with post-selection.","We determine how many auxiliary photons are required to prepare any two-photon state with heralding.","In addition, we present a construction for generalized post-selected n-qubit control-rotation gates."],"url":"http://arxiv.org/abs/2405.01395v1","category":"quant-ph"}
{"created":"2024-05-02 15:35:13","title":"$\\bar{b}c$ susceptibilities from fully relativistic lattice QCD","abstract":"We compute the $\\bar{h}c$ (pseudo)scalar, (axial-)vector and (axial-)tensor susceptibilities as a function of $u=m_c/m_h$ between $u=m_c/m_b$ and $u=0.8$ using fully relativistic lattice QCD, employing nonperturbative current renormalisation and using the second generation 2+1+1 MILC HISQ gluon field configurations. We include ensembles with $a\\approx 0.09\\mathrm{fm}$, $0.06\\mathrm{fm}$, $0.045\\mathrm{fm}$ and $0.033\\mathrm{fm}$ and we are able to reach the physical $b$-quark on the two finest ensembles. At the physical $m_h=m_b$ point we find $\\overline{m}_b^2 \\chi_{1^+}={0.720(34)\\times 10^{-2}}$, $\\overline{m}_b^2 \\chi_{1^-}={1.161(54)\\times 10^{-2}}$, $\\chi_{0^-}={2.374(33)\\times 10^{-2}}$, $\\chi_{0^+}={0.609(14)\\times 10^{-2}}$. Our results for the (pseudo)scalar, vector and axial-vector are compatible with the expected small size of nonperturbative effects at $u=m_c/m_b$. We also give the first nonperturbative determination of the tensor susceptibilities, finding $\\overline{m}_b^2 \\chi_{T}={0.891(44)\\times 10^{-2}}$ and $\\overline{m}_b^2 \\chi_{AT}={0.441(33)\\times 10^{-2}}$. Our value of $\\overline{m}_b^2\\chi_{AT}$ is in good agreement with the $\\mathcal{O}(\\alpha_s)$ perturbation theory, while our result for $\\overline{m}_b^2\\chi_{T}$ is in tension with the $\\mathcal{O}(\\alpha_s)$ perturbation theory at the level of $2\\sigma$. These results will allow for dispersively bounded parameterisations to be employed using lattice inputs for the full set of $h\\to c$ semileptonic form factors in future calculations, for heavy-quark masses in the range $1.25\\times m_c \\leq m_h \\leq m_b$.","sentences":["We compute the $\\bar{h}c$ (pseudo)scalar, (axial-)vector and (axial-)tensor susceptibilities as a function of $u=m_c/m_h$ between $u=m_c/m_b$ and $u=0.8$ using fully relativistic lattice QCD, employing nonperturbative current renormalisation and using the second generation 2+1+1 MILC HISQ gluon field configurations.","We include ensembles with $a\\approx 0.09\\mathrm{fm}$, $0.06\\mathrm{fm}$, $0.045\\mathrm{fm}$ and $0.033\\mathrm{fm}$ and we are able to reach the physical $b$-quark on the two finest ensembles.","At the physical $m_h=m_b$ point we find $\\overline{m}_b^2 \\chi_{1^+}={0.720(34)\\times 10^{-2}}$, $\\overline{m}_b^2 \\chi_{1^-}={1.161(54)\\times","10^{-2}}$, $\\chi_{0^-}={2.374(33)\\times 10^{-2}}$, $\\chi_{0^+}={0.609(14)\\times 10^{-2}}$.","Our results for the (pseudo)scalar, vector and axial-vector are compatible with the expected small size of nonperturbative effects at $u=m_c/m_b$. We also give the first nonperturbative determination of the tensor susceptibilities, finding $\\overline{m}_b^2 \\chi_{T}={0.891(44)\\times 10^{-2}}$ and $\\overline{m}_b^2 \\chi_{AT}={0.441(33)\\times","10^{-2}}$.","Our value of $\\overline{m}_b^2\\chi_{AT}$ is in good agreement with the $\\mathcal{O}(\\alpha_s)$ perturbation theory, while our result for $\\overline{m}_b^2\\chi_{T}$ is in tension with the $\\mathcal{O}(\\alpha_s)$ perturbation theory at the level of $2\\sigma$. These results will allow for dispersively bounded parameterisations to be employed using lattice inputs for the full set of $h\\to c$ semileptonic form factors in future calculations, for heavy-quark masses in the range $1.25\\times m_c \\leq m_h","\\leq m_b$."],"url":"http://arxiv.org/abs/2405.01390v1","category":"hep-lat"}
{"created":"2024-05-02 15:34:14","title":"Invariant Risk Minimization Is A Total Variation Model","abstract":"Invariant risk minimization (IRM) is an arising approach to generalize invariant features to different environments in machine learning. While most related works focus on new IRM settings or new application scenarios, the mathematical essence of IRM remains to be properly explained. We verify that IRM is essentially a total variation based on $L^2$ norm (TV-$\\ell_2$) of the learning risk with respect to the classifier variable. Moreover, we propose a novel IRM framework based on the TV-$\\ell_1$ model. It not only expands the classes of functions that can be used as the learning risk, but also has robust performance in denoising and invariant feature preservation based on the coarea formula. We also illustrate some requirements for IRM-TV-$\\ell_1$ to achieve out-of-distribution generalization. Experimental results show that the proposed framework achieves competitive performance in several benchmark machine learning scenarios.","sentences":["Invariant risk minimization (IRM) is an arising approach to generalize invariant features to different environments in machine learning.","While most related works focus on new IRM settings or new application scenarios, the mathematical essence of IRM remains to be properly explained.","We verify that IRM is essentially a total variation based on $L^2$ norm (TV-$\\ell_2$) of the learning risk with respect to the classifier variable.","Moreover, we propose a novel IRM framework based on the TV-$\\ell_1$ model.","It not only expands the classes of functions that can be used as the learning risk, but also has robust performance in denoising and invariant feature preservation based on the coarea formula.","We also illustrate some requirements for IRM-TV-$\\ell_1$ to achieve out-of-distribution generalization.","Experimental results show that the proposed framework achieves competitive performance in several benchmark machine learning scenarios."],"url":"http://arxiv.org/abs/2405.01389v1","category":"cs.LG"}
{"created":"2024-05-02 15:32:49","title":"Meson spectroscopy from spectral densities in lattice gauge theories","abstract":"Spectral densities encode non-perturbative information that enters the calculation of a plethora of physical observables in strongly coupled field theories. Phenomenological applications encompass aspects of standard-model hadronic physics, observable at current colliders, as well as correlation functions characterizing new physics proposals, testable in future experiments. By making use of numerical data produced with lattice gauge theories, we perform a systematic study to demonstrate the effectiveness of recent technological progress in the reconstruction of spectral densities. To this purpose, we write and test new software packages that use energy-smeared spectral densities to analyze the mass spectrum of mesons. We assess the effectiveness of different smearing kernels and optimize the smearing parameters to the characteristics of available lattice ensembles. For concreteness, we analyze the Sp(4) lattice gauge theory with matter transforming in an admixture of fundamental and 2-index antisymmetric representations of the gauge group. We generate new ensembles for this theory, with lattices that have a longer extent in the time direction with respect to the spatial ones. We run our tests on these ensembles, obtaining new results about the spectrum of light mesons and their excitations. We make available our algorithm and software for the extraction of spectral densities, that can be applied to theories with other gauge groups, including the theory of strong interactions (QCD) governing hadronic physics in the standard model.","sentences":["Spectral densities encode non-perturbative information that enters the calculation of a plethora of physical observables in strongly coupled field theories.","Phenomenological applications encompass aspects of standard-model hadronic physics, observable at current colliders, as well as correlation functions characterizing new physics proposals, testable in future experiments.","By making use of numerical data produced with lattice gauge theories, we perform a systematic study to demonstrate the effectiveness of recent technological progress in the reconstruction of spectral densities.","To this purpose, we write and test new software packages that use energy-smeared spectral densities to analyze the mass spectrum of mesons.","We assess the effectiveness of different smearing kernels and optimize the smearing parameters to the characteristics of available lattice ensembles.","For concreteness, we analyze the Sp(4) lattice gauge theory with matter transforming in an admixture of fundamental and 2-index antisymmetric representations of the gauge group.","We generate new ensembles for this theory, with lattices that have a longer extent in the time direction with respect to the spatial ones.","We run our tests on these ensembles, obtaining new results about the spectrum of light mesons and their excitations.","We make available our algorithm and software for the extraction of spectral densities, that can be applied to theories with other gauge groups, including the theory of strong interactions (QCD) governing hadronic physics in the standard model."],"url":"http://arxiv.org/abs/2405.01388v1","category":"hep-lat"}
{"created":"2024-05-02 15:27:10","title":"Anti-seizure medication tapering is associated with delta band power reduction in a dose, region and time-dependent manner","abstract":"Anti-seizure medications (ASMs) are the primary treatment for epilepsy, yet medication tapering effects have not been investigated in a dose, region, and time-dependent manner, despite their potential impact on research and clinical practice.   We examined over 3000 hours of intracranial EEG recordings in 32 subjects during long-term monitoring, of which 22 underwent concurrent ASM tapering. We estimated ASM plasma levels based on known pharmaco-kinetics of all the major ASM types.   We found an overall decrease in the power of delta band activity around the period of maximum medication withdrawal in most (80%) subjects, independent of their epilepsy type or medication combination. The degree of withdrawal correlated positively with the magnitude of delta power decrease. This dose-dependent effect was strongly seen across all recorded cortical regions during daytime; but not in sub-cortical regions, or during night time. We found no evidence of differential effect in seizure onset, spiking, or pathological brain regions.   The finding of decreased delta band power during ASM tapering agrees with previous literature. Our observed dose-dependent effect indicates that monitoring ASM levels in cortical regions may be feasible for applications such as medication reminder systems, or closed-loop ASM delivery systems. ASMs are also used in other neurological and psychiatric conditions, making our findings relevant to a general neuroscience and neurology audience.","sentences":["Anti-seizure medications (ASMs) are the primary treatment for epilepsy, yet medication tapering effects have not been investigated in a dose, region, and time-dependent manner, despite their potential impact on research and clinical practice.   ","We examined over 3000 hours of intracranial EEG recordings in 32 subjects during long-term monitoring, of which 22 underwent concurrent ASM tapering.","We estimated ASM plasma levels based on known pharmaco-kinetics of all the major ASM types.   ","We found an overall decrease in the power of delta band activity around the period of maximum medication withdrawal in most (80%) subjects, independent of their epilepsy type or medication combination.","The degree of withdrawal correlated positively with the magnitude of delta power decrease.","This dose-dependent effect was strongly seen across all recorded cortical regions during daytime; but not in sub-cortical regions, or during night time.","We found no evidence of differential effect in seizure onset, spiking, or pathological brain regions.   ","The finding of decreased delta band power during ASM tapering agrees with previous literature.","Our observed dose-dependent effect indicates that monitoring ASM levels in cortical regions may be feasible for applications such as medication reminder systems, or closed-loop ASM delivery systems.","ASMs are also used in other neurological and psychiatric conditions, making our findings relevant to a general neuroscience and neurology audience."],"url":"http://arxiv.org/abs/2405.01385v1","category":"q-bio.NC"}
{"created":"2024-05-02 15:24:13","title":"MUSE observations of small-scale heating events","abstract":"Constraining the processes that drive coronal heating from observations is a difficult task due to the complexity of the solar atmosphere. As upcoming missions such as MUSE will provide coronal observations with unprecedented spatial and temporal resolution, numerical simulations are becoming increasingly realistic. Despite the availability of synthetic observations from numerical models, line-of-sight effects and the complexity of the magnetic topology in a realistic setup still complicate the prediction of signatures for specific heating processes. 3D MHD simulations have shown that a significant part of the Poynting flux injected into the solar atmosphere is carried by small-scale motions, such as vortices driven by rotational flows inside intergranular lanes. MHD waves excited by these vortices have been suggested to play an important role in the energy transfer between different atmospheric layers. Using synthetic spectroscopic data generated from a coronal loop model incorporating realistic driving by magnetoconvection, we study whether signatures of energy transport by vortices and eventual dissipation can be identified with future missions such as MUSE.","sentences":["Constraining the processes that drive coronal heating from observations is a difficult task due to the complexity of the solar atmosphere.","As upcoming missions such as MUSE will provide coronal observations with unprecedented spatial and temporal resolution, numerical simulations are becoming increasingly realistic.","Despite the availability of synthetic observations from numerical models, line-of-sight effects and the complexity of the magnetic topology in a realistic setup still complicate the prediction of signatures for specific heating processes.","3D MHD simulations have shown that a significant part of the Poynting flux injected into the solar atmosphere is carried by small-scale motions, such as vortices driven by rotational flows inside intergranular lanes.","MHD waves excited by these vortices have been suggested to play an important role in the energy transfer between different atmospheric layers.","Using synthetic spectroscopic data generated from a coronal loop model incorporating realistic driving by magnetoconvection, we study whether signatures of energy transport by vortices and eventual dissipation can be identified with future missions such as MUSE."],"url":"http://arxiv.org/abs/2405.01384v1","category":"astro-ph.SR"}
{"created":"2024-05-02 15:23:38","title":"Benchmarking DFT-based excited-state methods for intermolecular charge-transfer excitations","abstract":"Intermolecular charge-transfer is a highly important process in biology and energy-conversion applications where generated charges need to be transported over several moieties. However, its theoretical description is challenging since the high accuracy required to describe these excited states must be accessible for calculations on large molecular systems. In this benchmark study, we identify reliable low-scaling computational methods for this task. Our reference results were obtained from highly accurate wavefunction calculations that restrict the size of the benchmark systems. However, the density-functional theory based methods that we identify as accurate can be applied to much larger systems. Since targeting charge-transfer states requires the unambiguous classification of an excited state, we first analyze several charge-transfer descriptors for their reliability concerning intermolecular charge-transfer and single out DCT as an optimal choice for our purposes. In general, best results are obtained for orbital-optimized methods - and among those, IMOM proved to be the most numerically stable variant - but optimally-tuned range-separated hybrid functionals combined with rather small basis sets proved to yield surprisingly good results. This makes these fast calculations attractive for high-throughput screening applications.","sentences":["Intermolecular charge-transfer is a highly important process in biology and energy-conversion applications where generated charges need to be transported over several moieties.","However, its theoretical description is challenging since the high accuracy required to describe these excited states must be accessible for calculations on large molecular systems.","In this benchmark study, we identify reliable low-scaling computational methods for this task.","Our reference results were obtained from highly accurate wavefunction calculations that restrict the size of the benchmark systems.","However, the density-functional theory based methods that we identify as accurate can be applied to much larger systems.","Since targeting charge-transfer states requires the unambiguous classification of an excited state, we first analyze several charge-transfer descriptors for their reliability concerning intermolecular charge-transfer and single out DCT as an optimal choice for our purposes.","In general, best results are obtained for orbital-optimized methods - and among those, IMOM proved to be the most numerically stable variant - but optimally-tuned range-separated hybrid functionals combined with rather small basis sets proved to yield surprisingly good results.","This makes these fast calculations attractive for high-throughput screening applications."],"url":"http://arxiv.org/abs/2405.01382v1","category":"physics.chem-ph"}
{"created":"2024-05-02 15:21:32","title":"Arrows of time in bouncing cosmologies","abstract":"Different approaches to quantum gravity, such as loop quantum cosmology and group field theory, predict the resolution of the initial cosmological singularity via a 'bounce': a regular spacetime region that connects the expanding branch of the universe to a contracting branch. The cosmological arrow of time, which by definition points in the direction of cosmic expansion, is reversed at the bounce. Nonetheless, it is still possible to discriminate between the two branches by considering different arrows, as defined for instance by the growth of perturbations. After reviewing general aspects of the time arrow problem in cosmology, we examine the properties of different arrows of time in bouncing cosmologies, focusing on the loop quantum cosmology bounce as a case study. We also present a new exact solution to the effective Friedmann equations of loop quantum cosmology with pressureless dust and a cosmological constant, which is a simplified version of the $\\Lambda$CDM bounce scenario, where these issues can be examined in detail.","sentences":["Different approaches to quantum gravity, such as loop quantum cosmology and group field theory, predict the resolution of the initial cosmological singularity via a 'bounce': a regular spacetime region that connects the expanding branch of the universe to a contracting branch.","The cosmological arrow of time, which by definition points in the direction of cosmic expansion, is reversed at the bounce.","Nonetheless, it is still possible to discriminate between the two branches by considering different arrows, as defined for instance by the growth of perturbations.","After reviewing general aspects of the time arrow problem in cosmology, we examine the properties of different arrows of time in bouncing cosmologies, focusing on the loop quantum cosmology bounce as a case study.","We also present a new exact solution to the effective Friedmann equations of loop quantum cosmology with pressureless dust and a cosmological constant, which is a simplified version of the $\\Lambda$CDM bounce scenario, where these issues can be examined in detail."],"url":"http://arxiv.org/abs/2405.01380v1","category":"gr-qc"}
{"created":"2024-05-02 15:20:01","title":"Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving","abstract":"Natural language explanations have become a proxy for evaluating explainable and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that augments a TP with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of human-annotated explanations of variable complexity in different domains.","sentences":["Natural language explanations have become a proxy for evaluating explainable and multi-step Natural Language Inference (NLI) models.","However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors.","To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs).","Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that augments a TP with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI.","In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements.","We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of human-annotated explanations of variable complexity in different domains."],"url":"http://arxiv.org/abs/2405.01379v1","category":"cs.CL"}
{"created":"2024-05-02 15:19:39","title":"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances","abstract":"Benchmarking Quantum Process Units (QPU) at an application level usually requires considering the whole programming stack of the quantum computer. One critical task is the minor-embedding (resp. transpilation) step, which involves space-time overheads for annealing-based (resp. gate-based) quantum computers. This paper establishes a new protocol to generate graph instances with their associated near-optimal minor-embedding mappings to D-Wave Quantum Annealers (QA). This set of favorable mappings is used to generate a wide diversity of optimization problem instances. We use this method to benchmark QA on large instances of unconstrained and constrained optimization problems and compare the performance of the QPU with efficient classical solvers. The benchmark aims to evaluate and quantify the key characteristics of instances that could benefit from the use of a quantum computer. In this context, existing QA seem best suited for unconstrained problems on instances with densities less than $10\\%$. For constrained problems, the penalty terms used to encode the hard constraints restrict the performance of QA and suggest that these QPU will be less efficient on these problems of comparable size.","sentences":["Benchmarking Quantum Process Units (QPU) at an application level usually requires considering the whole programming stack of the quantum computer.","One critical task is the minor-embedding (resp. transpilation) step, which involves space-time overheads for annealing-based (resp.","gate-based) quantum computers.","This paper establishes a new protocol to generate graph instances with their associated near-optimal minor-embedding mappings to D-Wave Quantum Annealers (QA).","This set of favorable mappings is used to generate a wide diversity of optimization problem instances.","We use this method to benchmark QA on large instances of unconstrained and constrained optimization problems and compare the performance of the QPU with efficient classical solvers.","The benchmark aims to evaluate and quantify the key characteristics of instances that could benefit from the use of a quantum computer.","In this context, existing QA seem best suited for unconstrained problems on instances with densities less than $10\\%$. For constrained problems, the penalty terms used to encode the hard constraints restrict the performance of QA and suggest that these QPU will be less efficient on these problems of comparable size."],"url":"http://arxiv.org/abs/2405.01378v1","category":"quant-ph"}
{"created":"2024-05-02 15:15:01","title":"ATOM: Attention Mixer for Efficient Dataset Distillation","abstract":"Recent works in dataset distillation seek to minimize training expenses by generating a condensed synthetic dataset that encapsulates the information present in a larger real dataset. These approaches ultimately aim to attain test accuracy levels akin to those achieved by models trained on the entirety of the original dataset. Previous studies in feature and distribution matching have achieved significant results without incurring the costs of bi-level optimization in the distillation process. Despite their convincing efficiency, many of these methods suffer from marginal downstream performance improvements, limited distillation of contextual information, and subpar cross-architecture generalization. To address these challenges in dataset distillation, we propose the ATtentiOn Mixer (ATOM) module to efficiently distill large datasets using a mixture of channel and spatial-wise attention in the feature matching process. Spatial-wise attention helps guide the learning process based on consistent localization of classes in their respective images, allowing for distillation from a broader receptive field. Meanwhile, channel-wise attention captures the contextual information associated with the class itself, thus making the synthetic image more informative for training. By integrating both types of attention, our ATOM module demonstrates superior performance across various computer vision datasets, including CIFAR10/100 and TinyImagenet. Notably, our method significantly improves performance in scenarios with a low number of images per class, thereby enhancing its potential. Furthermore, we maintain the improvement in cross-architectures and applications such as neural architecture search.","sentences":["Recent works in dataset distillation seek to minimize training expenses by generating a condensed synthetic dataset that encapsulates the information present in a larger real dataset.","These approaches ultimately aim to attain test accuracy levels akin to those achieved by models trained on the entirety of the original dataset.","Previous studies in feature and distribution matching have achieved significant results without incurring the costs of bi-level optimization in the distillation process.","Despite their convincing efficiency, many of these methods suffer from marginal downstream performance improvements, limited distillation of contextual information, and subpar cross-architecture generalization.","To address these challenges in dataset distillation, we propose the ATtentiOn Mixer (ATOM) module to efficiently distill large datasets using a mixture of channel and spatial-wise attention in the feature matching process.","Spatial-wise attention helps guide the learning process based on consistent localization of classes in their respective images, allowing for distillation from a broader receptive field.","Meanwhile, channel-wise attention captures the contextual information associated with the class itself, thus making the synthetic image more informative for training.","By integrating both types of attention, our ATOM module demonstrates superior performance across various computer vision datasets, including CIFAR10/100 and TinyImagenet.","Notably, our method significantly improves performance in scenarios with a low number of images per class, thereby enhancing its potential.","Furthermore, we maintain the improvement in cross-architectures and applications such as neural architecture search."],"url":"http://arxiv.org/abs/2405.01373v1","category":"cs.CV"}
{"created":"2024-05-02 15:13:56","title":"Pseudodifferential operators on time-frequency invariant Banach spaces and applications to Gabor Frames","abstract":"Starting from the study of pseudodifferential operators with completely periodic symbols, we obtain results of continuity and invertibility of a class of Gabor operators on time-frequency invariant Banach spaces. As an applications we find sufficient conditions for the existence of Gabor frames on the space of square integrable functions, associated to a general lattice.","sentences":["Starting from the study of pseudodifferential operators with completely periodic symbols, we obtain results of continuity and invertibility of a class of Gabor operators on time-frequency invariant Banach spaces.","As an applications we find sufficient conditions for the existence of Gabor frames on the space of square integrable functions, associated to a general lattice."],"url":"http://arxiv.org/abs/2405.01370v1","category":"math.FA"}
{"created":"2024-05-02 15:13:39","title":"Sub-uniformity of harmonic mean p-values","abstract":"We obtain several inequalities on the generalized means of dependent p-values. In particular, the weighted harmonic mean of p-values is strictly sub-uniform under several dependence assumptions of p-values, including independence, weak negative association, the class of extremal mixture copulas, and some Clayton copulas. Sub-uniformity of the harmonic mean of p-values has an important implication in multiple hypothesis testing: It is statistically invalid to merge p-values using the harmonic mean unless a proper threshold or multiplier adjustment is used, and this invalidity applies across all significance levels. The required multiplier adjustment on the harmonic mean explodes as the number of p-values increases, and hence there does not exist a constant multiplier that works for any number of p-values, even under independence.","sentences":["We obtain several inequalities on the generalized means of dependent p-values.","In particular, the weighted harmonic mean of p-values is strictly sub-uniform under several dependence assumptions of p-values, including independence, weak negative association, the class of extremal mixture copulas, and some Clayton copulas.","Sub-uniformity of the harmonic mean of p-values has an important implication in multiple hypothesis testing: It is statistically invalid to merge p-values using the harmonic mean unless a proper threshold or multiplier adjustment is used, and this invalidity applies across all significance levels.","The required multiplier adjustment on the harmonic mean explodes as the number of p-values increases, and hence there does not exist a constant multiplier that works for any number of p-values, even under independence."],"url":"http://arxiv.org/abs/2405.01368v1","category":"math.ST"}
{"created":"2024-05-02 15:13:06","title":"Supersymmetric Expansion Algorithm and complete analytical solution for the Hulth\u00e9n and anharmonic potentials","abstract":"An algorithm for providing analytical solutions to Schr\\\"{o}dinger's equation with non-exactly solvable potentials is elaborated. It represents a symbiosis between the logarithmic expansion method and the techniques of the superymmetric quantum mechanics as extended toward non shape invariant potentials. The complete solution to a given Hamiltonian $H_{0}$ is obtained from the nodeless states of the Hamiltonian $H_{0}$ and of a set of supersymmetric partners $H_{1}, H_{2},..., H_{r}$. The nodeless states (dubbed \"edge\" states) are unique and in general can be ground or excited states. They are solved using the logarithmic expansion which yields an infinite systems of coupled first order hierarchical differential equations, converted later into algebraic equations with recurrence relations which can be solved order by order. We formulate the aforementioned scheme, termed to as \"Supersymmetric Expansion Algorithm'' step by step and apply it to obtain for the first time the complete analytical solutions of the three dimensional Hulth\\'en--, and the one-dimensional anharmonic oscillator potentials.","sentences":["An algorithm for providing analytical solutions to Schr\\\"{o}dinger's equation with non-exactly solvable potentials is elaborated.","It represents a symbiosis between the logarithmic expansion method and the techniques of the superymmetric quantum mechanics as extended toward non shape invariant potentials.","The complete solution to a given Hamiltonian $H_{0}$ is obtained from the nodeless states of the Hamiltonian $H_{0}$ and of a set of supersymmetric partners $H_{1}, H_{2},..., H_{r}$.","The nodeless states (dubbed \"edge\" states) are unique and in general can be ground or excited states.","They are solved using the logarithmic expansion which yields an infinite systems of coupled first order hierarchical differential equations, converted later into algebraic equations with recurrence relations which can be solved order by order.","We formulate the aforementioned scheme, termed to as \"Supersymmetric Expansion Algorithm'' step by step and apply it to obtain for the first time the complete analytical solutions of the three dimensional Hulth\\'en--, and the one-dimensional anharmonic oscillator potentials."],"url":"http://arxiv.org/abs/2405.01367v1","category":"quant-ph"}
{"created":"2024-05-02 15:09:59","title":"Dynamic Online Ensembles of Basis Expansions","abstract":"Practical Bayesian learning often requires (1) online inference, (2) dynamic models, and (3) ensembling over multiple different models. Recent advances have shown how to use random feature approximations to achieve scalable, online ensembling of Gaussian processes with desirable theoretical properties and fruitful applications. One key to these methods' success is the inclusion of a random walk on the model parameters, which makes models dynamic. We show that these methods can be generalized easily to any basis expansion model and that using alternative basis expansions, such as Hilbert space Gaussian processes, often results in better performance. To simplify the process of choosing a specific basis expansion, our method's generality also allows the ensembling of several entirely different models, for example, a Gaussian process and polynomial regression. Finally, we propose a novel method to ensemble static and dynamic models together.","sentences":["Practical Bayesian learning often requires (1) online inference, (2) dynamic models, and (3) ensembling over multiple different models.","Recent advances have shown how to use random feature approximations to achieve scalable, online ensembling of Gaussian processes with desirable theoretical properties and fruitful applications.","One key to these methods' success is the inclusion of a random walk on the model parameters, which makes models dynamic.","We show that these methods can be generalized easily to any basis expansion model and that using alternative basis expansions, such as Hilbert space Gaussian processes, often results in better performance.","To simplify the process of choosing a specific basis expansion, our method's generality also allows the ensembling of several entirely different models, for example, a Gaussian process and polynomial regression.","Finally, we propose a novel method to ensemble static and dynamic models together."],"url":"http://arxiv.org/abs/2405.01365v1","category":"cs.LG"}
{"created":"2024-05-02 15:09:20","title":"Information propagation in Gaussian processes on multilayer networks","abstract":"Complex systems with multiple processes evolving on different temporal scales are naturally described by multilayer networks, where each layer represents a different timescale. In this work, we show how the multilayer structure shapes the generation and propagation of information between layers. We derive a general decomposition of the multilayer probability for continuous stochastic processes described by Fokker-Planck operators. In particular, we focus on Gaussian processes, for which this solution can be obtained analytically. By explicitly computing the mutual information between the layers, we derive the fundamental principles that govern how information is propagated by the topology of the multilayer network. In particular, we unravel how edges between nodes in different layers affect their functional couplings. We find that interactions from fast to slow layers alone do not generate information, leaving the layers statistically independent even if they affect their dynamical evolution. On the other hand, interactions from slow to fast nodes lead to non-zero mutual information, which can then be propagated along specific paths of interactions between layers. We employ our results to study the interplay between information and instability, identifying the critical layers that drive information when pushed to the edge of stability. Our work generalizes previous results obtained in the context of discrete stochastic processes, allowing us to understand how the multilayer nature of complex systems affects their functional structure.","sentences":["Complex systems with multiple processes evolving on different temporal scales are naturally described by multilayer networks, where each layer represents a different timescale.","In this work, we show how the multilayer structure shapes the generation and propagation of information between layers.","We derive a general decomposition of the multilayer probability for continuous stochastic processes described by Fokker-Planck operators.","In particular, we focus on Gaussian processes, for which this solution can be obtained analytically.","By explicitly computing the mutual information between the layers, we derive the fundamental principles that govern how information is propagated by the topology of the multilayer network.","In particular, we unravel how edges between nodes in different layers affect their functional couplings.","We find that interactions from fast to slow layers alone do not generate information, leaving the layers statistically independent even if they affect their dynamical evolution.","On the other hand, interactions from slow to fast nodes lead to non-zero mutual information, which can then be propagated along specific paths of interactions between layers.","We employ our results to study the interplay between information and instability, identifying the critical layers that drive information when pushed to the edge of stability.","Our work generalizes previous results obtained in the context of discrete stochastic processes, allowing us to understand how the multilayer nature of complex systems affects their functional structure."],"url":"http://arxiv.org/abs/2405.01363v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-02 15:08:01","title":"Haptic-Based Bilateral Teleoperation of Aerial Manipulator for Extracting Wedged Object with Compensation of Human Reaction Time","abstract":"Bilateral teleoperation of an aerial manipulator facilitates the execution of industrial missions thanks to the combination of the aerial platform's maneuverability and the ability to conduct complex tasks with human supervision. Heretofore, research on such operations has focused on flying without any physical interaction or exerting a pushing force on a contact surface that does not involve abrupt changes in the interaction force. In this paper, we propose a human reaction time compensating haptic-based bilateral teleoperation strategy for an aerial manipulator extracting a wedged object from a static structure (i.e., plug-pulling), which incurs an abrupt decrease in the interaction force and causes additional difficulty for an aerial platform. A haptic device composed of a 4-degree-of-freedom robotic arm and a gripper is made for the teleoperation of aerial wedged object-extracting tasks, and a haptic-based teleoperation method to execute the aerial manipulator by the haptic device is introduced. We detect the extraction of the object by the estimation of the external force exerted on the aerial manipulator and generate reference trajectories for both the aerial manipulator and the haptic device after the extraction. As an example of the extraction of a wedged object, we conduct comparative plug-pulling experiments with a quadrotor-based aerial manipulator. The results validate that the proposed bilateral teleoperation method reduces the overshoot in the aerial manipulator's position and ensures fast recovery to its initial position after extracting the wedged object.","sentences":["Bilateral teleoperation of an aerial manipulator facilitates the execution of industrial missions thanks to the combination of the aerial platform's maneuverability and the ability to conduct complex tasks with human supervision.","Heretofore, research on such operations has focused on flying without any physical interaction or exerting a pushing force on a contact surface that does not involve abrupt changes in the interaction force.","In this paper, we propose a human reaction time compensating haptic-based bilateral teleoperation strategy for an aerial manipulator extracting a wedged object from a static structure (i.e., plug-pulling), which incurs an abrupt decrease in the interaction force and causes additional difficulty for an aerial platform.","A haptic device composed of a 4-degree-of-freedom robotic arm and a gripper is made for the teleoperation of aerial wedged object-extracting tasks, and a haptic-based teleoperation method to execute the aerial manipulator by the haptic device is introduced.","We detect the extraction of the object by the estimation of the external force exerted on the aerial manipulator and generate reference trajectories for both the aerial manipulator and the haptic device after the extraction.","As an example of the extraction of a wedged object, we conduct comparative plug-pulling experiments with a quadrotor-based aerial manipulator.","The results validate that the proposed bilateral teleoperation method reduces the overshoot in the aerial manipulator's position and ensures fast recovery to its initial position after extracting the wedged object."],"url":"http://arxiv.org/abs/2405.01361v1","category":"cs.RO"}
{"created":"2024-05-02 15:07:58","title":"Investigations on Lorentzian Spin-foams and Semiclassical Space-times","abstract":"This thesis is developed in the context of the spin-foam approach to quantum gravity; all results are concerned with the Lorentzian theory and with semiclassical methods. A correspondence is given between Majorana 2-spinors and time-like hypersurfaces in Minkowski 3-space based on complexified quaternions. It is shown that the former suggest a symplectic structure on the spinor phase space which, together with an area-matching constraint, yields a symplectomorphism to $T^*\\mathrm{SU}(1,1)$. A complete 3-dimensional Lorentzian spin-foam amplitude for both space- and time-like triangles is proposed. It is shown to asymptote to Regge theory in the semiclassical regime. The asymptotic limit of the 4-dimensional Conrady-Hnybida model for general polytopes is scrutinized. Minkowski's theorem on convex polyhedra is generalized to Lorentzian signature, and new boundary states for time-like polygons are introduced. It is found that the semiclassical amplitude for such polygons is insufficiently constrained. A method for the asymptotic evaluation of integrals subject to external parameters is discussed. The method is developed in detail for the special problem of spin-foam gluing constraints away from their dominant critical points. A relation to the gluing constraints of effective spin-foams is suggested.","sentences":["This thesis is developed in the context of the spin-foam approach to quantum gravity; all results are concerned with the Lorentzian theory and with semiclassical methods.","A correspondence is given between Majorana 2-spinors and time-like hypersurfaces in Minkowski 3-space based on complexified quaternions.","It is shown that the former suggest a symplectic structure on the spinor phase space which, together with an area-matching constraint, yields a symplectomorphism to $T^*\\mathrm{SU}(1,1)$. A complete 3-dimensional Lorentzian spin-foam amplitude for both space- and time-like triangles is proposed.","It is shown to asymptote to Regge theory in the semiclassical regime.","The asymptotic limit of the 4-dimensional Conrady-Hnybida model for general polytopes is scrutinized.","Minkowski's theorem on convex polyhedra is generalized to Lorentzian signature, and new boundary states for time-like polygons are introduced.","It is found that the semiclassical amplitude for such polygons is insufficiently constrained.","A method for the asymptotic evaluation of integrals subject to external parameters is discussed.","The method is developed in detail for the special problem of spin-foam gluing constraints away from their dominant critical points.","A relation to the gluing constraints of effective spin-foams is suggested."],"url":"http://arxiv.org/abs/2405.01360v1","category":"gr-qc"}
{"created":"2024-05-02 15:06:18","title":"GAIA: A General AI Assistant for Intelligent Accelerator Operations","abstract":"Large-scale machines like particle accelerators are usually run by a team of experienced operators. In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine. Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to. In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation. By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts. This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators.","sentences":["Large-scale machines like particle accelerators are usually run by a team of experienced operators.","In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine.","Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to.","In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation.","By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts.","This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators."],"url":"http://arxiv.org/abs/2405.01359v1","category":"cs.CL"}
{"created":"2024-05-02 15:03:52","title":"Schwarz-Pick type inequalities from an operator theoretical point of view","abstract":"We use (versions of) the von Neumann inequality for Hilbert space contractions to prove several Schwarz-Pick inequalities. Specifically, we derive an alternate proof for a multi-point Schwarz-Pick inequality by Beardon and Minda, along with a generalized version for operators. Connections with model spaces and Peschl's invariant derivatives are established. Finally, Schwarz-Pick inequalities for analytic functions on polydisks and for higher order derivatives are discussed. An enhanced version of the Schwarz-Pick lemma, using the notion of distinguished variety, is obtained for the bidisk.","sentences":["We use (versions of) the von Neumann inequality for Hilbert space contractions to prove several Schwarz-Pick inequalities.","Specifically, we derive an alternate proof for a multi-point Schwarz-Pick inequality by Beardon and Minda, along with a generalized version for operators.","Connections with model spaces and Peschl's invariant derivatives are established.","Finally, Schwarz-Pick inequalities for analytic functions on polydisks and for higher order derivatives are discussed.","An enhanced version of the Schwarz-Pick lemma, using the notion of distinguished variety, is obtained for the bidisk."],"url":"http://arxiv.org/abs/2405.01357v1","category":"math.FA"}
{"created":"2024-05-02 15:03:07","title":"Neural-Parareal: Dynamically Training Neural Operators as Coarse Solvers for Time-Parallelisation of Fusion MHD Simulations","abstract":"The fusion research facility ITER is currently being assembled to demonstrate that fusion can be used for industrial energy production, while several other programmes across the world are also moving forward, such as EU-DEMO, CFETR, SPARC and STEP. The high engineering complexity of a tokamak makes it an extremely challenging device to optimise, and test-based optimisation would be too slow and too costly. Instead, digital design and optimisation must be favored, which requires strongly-coupled suites of High-Performance Computing calculations. In this context, having surrogate models to provide quick estimates with uncertainty quantification is essential to explore and optimise new design options. Furthermore, these surrogates can in turn be used to accelerate simulations in the first place. This is the case of Parareal, a time-parallelisation method that can speed-up large HPC simulations, where the coarse-solver can be replaced by a surrogate. A novel framework, Neural-Parareal, is developed to integrate the training of neural operators dynamically as more data becomes available. For a given input-parameter domain, as more simulations are being run with Parareal, the large amount of data generated by the algorithm is used to train new surrogate models to be used as coarse-solvers for future Parareal simulations, leading to progressively more accurate coarse-solvers, and thus higher speed-up. It is found that such neural network surrogates can be much more effective than traditional coarse-solver in providing a speed-up with Parareal. This study is a demonstration of the convergence of HPC and AI which simply has to become common practice in the world of digital engineering design.","sentences":["The fusion research facility ITER is currently being assembled to demonstrate that fusion can be used for industrial energy production, while several other programmes across the world are also moving forward, such as EU-DEMO, CFETR, SPARC and STEP.","The high engineering complexity of a tokamak makes it an extremely challenging device to optimise, and test-based optimisation would be too slow and too costly.","Instead, digital design and optimisation must be favored, which requires strongly-coupled suites of High-Performance Computing calculations.","In this context, having surrogate models to provide quick estimates with uncertainty quantification is essential to explore and optimise new design options.","Furthermore, these surrogates can in turn be used to accelerate simulations in the first place.","This is the case of Parareal, a time-parallelisation method that can speed-up large HPC simulations, where the coarse-solver can be replaced by a surrogate.","A novel framework, Neural-Parareal, is developed to integrate the training of neural operators dynamically as more data becomes available.","For a given input-parameter domain, as more simulations are being run with Parareal, the large amount of data generated by the algorithm is used to train new surrogate models to be used as coarse-solvers for future Parareal simulations, leading to progressively more accurate coarse-solvers, and thus higher speed-up.","It is found that such neural network surrogates can be much more effective than traditional coarse-solver in providing a speed-up with Parareal.","This study is a demonstration of the convergence of HPC and AI which simply has to become common practice in the world of digital engineering design."],"url":"http://arxiv.org/abs/2405.01355v1","category":"physics.plasm-ph"}
{"created":"2024-05-02 15:00:00","title":"High harmonic generation from electrons moving on topological spin textures","abstract":"High harmonic generation (HHG) is a striking phenomenon, which reflects the ultrafast dynamics of electrons. Recently, it has been demonstrated that HHG can be used to reconstruct not only the energy band structure but also the geometric structure characterized by the Berry curvature. Here, we numerically investigate HHG arising from electrons coupled with a topological spin texture in a spin scalar chiral state where time reversal symmetry is broken. In this system, a sign change in scalar chirality alters the sign of the Berry curvature while keeping the energy band structure unchanged, allowing us to discuss purely geometrical effects on HHG. Notably, we found that, when the optical frequency is significantly lower than the energy gap, the sign of scalar chirality largely affects the longitudinal response parallel to the optical field rather than the transverse response. Our analysis suggests that this can be attributed to interband currents induced by the recombination of electron-hole pairs whose real-space trajectories are modulated by the anomalous velocity term.","sentences":["High harmonic generation (HHG) is a striking phenomenon, which reflects the ultrafast dynamics of electrons.","Recently, it has been demonstrated that HHG can be used to reconstruct not only the energy band structure but also the geometric structure characterized by the Berry curvature.","Here, we numerically investigate HHG arising from electrons coupled with a topological spin texture in a spin scalar chiral state where time reversal symmetry is broken.","In this system, a sign change in scalar chirality alters the sign of the Berry curvature while keeping the energy band structure unchanged, allowing us to discuss purely geometrical effects on HHG.","Notably, we found that, when the optical frequency is significantly lower than the energy gap, the sign of scalar chirality largely affects the longitudinal response parallel to the optical field rather than the transverse response.","Our analysis suggests that this can be attributed to interband currents induced by the recombination of electron-hole pairs whose real-space trajectories are modulated by the anomalous velocity term."],"url":"http://arxiv.org/abs/2405.01351v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-02 14:59:58","title":"Community-Invariant Graph Contrastive Learning","abstract":"Graph augmentation has received great attention in recent years for graph contrastive learning (GCL) to learn well-generalized node/graph representations. However, mainstream GCL methods often favor randomly disrupting graphs for augmentation, which shows limited generalization and inevitably leads to the corruption of high-level graph information, i.e., the graph community. Moreover, current knowledge-based graph augmentation methods can only focus on either topology or node features, causing the model to lack robustness against various types of noise. To address these limitations, this research investigated the role of the graph community in graph augmentation and figured out its crucial advantage for learnable graph augmentation. Based on our observations, we propose a community-invariant GCL framework to maintain graph community structure during learnable graph augmentation. By maximizing the spectral changes, this framework unifies the constraints of both topology and feature augmentation, enhancing the model's robustness. Empirical evidence on 21 benchmark datasets demonstrates the exclusive merits of our framework. Code is released on Github (https://github.com/ShiyinTan/CI-GCL.git).","sentences":["Graph augmentation has received great attention in recent years for graph contrastive learning (GCL) to learn well-generalized node/graph representations.","However, mainstream GCL methods often favor randomly disrupting graphs for augmentation, which shows limited generalization and inevitably leads to the corruption of high-level graph information, i.e., the graph community.","Moreover, current knowledge-based graph augmentation methods can only focus on either topology or node features, causing the model to lack robustness against various types of noise.","To address these limitations, this research investigated the role of the graph community in graph augmentation and figured out its crucial advantage for learnable graph augmentation.","Based on our observations, we propose a community-invariant GCL framework to maintain graph community structure during learnable graph augmentation.","By maximizing the spectral changes, this framework unifies the constraints of both topology and feature augmentation, enhancing the model's robustness.","Empirical evidence on 21 benchmark datasets demonstrates the exclusive merits of our framework.","Code is released on Github (https://github.com/ShiyinTan/CI-GCL.git)."],"url":"http://arxiv.org/abs/2405.01350v1","category":"cs.LG"}
{"created":"2024-05-02 14:57:14","title":"GRBoondi: A code for evolving Generalized Proca theories on arbitrary backgrounds","abstract":"While numerical simulations offer unparalleled precision and robustness in studying complex physical systems, their execution is often hindered by complexity, costliness, and time consumption due to the intricate equations involved. This challenge is already encountered in General Relativity, where non-flat spacetimes exacerbate the computational burden. This complexity is further intensified when dealing with additional degrees of freedom. To address this challenge head-on, we introduce GRBoondi, a groundbreaking fixed-background numerical relativity code designed to provide a unified interface for numerically solving Generalized Proca theories. GRBoondi grants users the ability to make arbitrary modifications to the Proca equations of motion on any background, providing a robust and versatile tool for exploring diverse classes of Generalized Proca theories. This letter serves as part of the submission of GRBoondi to the Journal of Open Source Software. For access to the code, please visit https://github.com/ShaunFell/GRBoondi.git.","sentences":["While numerical simulations offer unparalleled precision and robustness in studying complex physical systems, their execution is often hindered by complexity, costliness, and time consumption due to the intricate equations involved.","This challenge is already encountered in General Relativity, where non-flat spacetimes exacerbate the computational burden.","This complexity is further intensified when dealing with additional degrees of freedom.","To address this challenge head-on, we introduce GRBoondi, a groundbreaking fixed-background numerical relativity code designed to provide a unified interface for numerically solving Generalized Proca theories.","GRBoondi grants users the ability to make arbitrary modifications to the Proca equations of motion on any background, providing a robust and versatile tool for exploring diverse classes of Generalized Proca theories.","This letter serves as part of the submission of GRBoondi to the Journal of Open Source Software.","For access to the code, please visit https://github.com/ShaunFell/GRBoondi.git."],"url":"http://arxiv.org/abs/2405.01348v1","category":"gr-qc"}
{"created":"2024-05-02 14:46:35","title":"Conditioned stochastic stability of equilibrium states on uniformly expanding repellers","abstract":"We propose a notion of conditioned stochastic stability of invariant measures on repellers: we consider whether quasi-ergodic measures of absorbing Markov processes, generated by random perturbations of the deterministic dynamics and conditioned upon survival in a neighbourhood of a repeller, converge to an invariant measure in the zero-noise limit. Under suitable choices of the random perturbation, we find that equilibrium states on uniformly expanding repellers are conditioned stochastically stable. In the process, we establish a rigorous foundation for the existence of ``natural measures'', which were proposed by Kantz and Grassberger in 1984 to aid the understanding of chaotic transients.","sentences":["We propose a notion of conditioned stochastic stability of invariant measures on repellers: we consider whether quasi-ergodic measures of absorbing Markov processes, generated by random perturbations of the deterministic dynamics and conditioned upon survival in a neighbourhood of a repeller, converge to an invariant measure in the zero-noise limit.","Under suitable choices of the random perturbation, we find that equilibrium states on uniformly expanding repellers are conditioned stochastically stable.","In the process, we establish a rigorous foundation for the existence of ``natural measures'', which were proposed by Kantz and Grassberger in 1984 to aid the understanding of chaotic transients."],"url":"http://arxiv.org/abs/2405.01343v1","category":"math.DS"}
{"created":"2024-05-02 14:43:55","title":"Sensitivity Sampling for $k$-Means: Worst Case and Stability Optimal Coreset Bounds","abstract":"Coresets are arguably the most popular compression paradigm for center-based clustering objectives such as $k$-means. Given a point set $P$, a coreset $\\Omega$ is a small, weighted summary that preserves the cost of all candidate solutions $S$ up to a $(1\\pm \\varepsilon)$ factor. For $k$-means in $d$-dimensional Euclidean space the cost for solution $S$ is $\\sum_{p\\in P}\\min_{s\\in S}\\|p-s\\|^2$.   A very popular method for coreset construction, both in theory and practice, is Sensitivity Sampling, where points are sampled in proportion to their importance. We show that Sensitivity Sampling yields optimal coresets of size $\\tilde{O}(k/\\varepsilon^2\\min(\\sqrt{k},\\varepsilon^{-2}))$ for worst-case instances. Uniquely among all known coreset algorithms, for well-clusterable data sets with $\\Omega(1)$ cost stability, Sensitivity Sampling gives coresets of size $\\tilde{O}(k/\\varepsilon^2)$, improving over the worst-case lower bound. Notably, Sensitivity Sampling does not have to know the cost stability in order to exploit it: It is appropriately sensitive to the clusterability of the data set while being oblivious to it.   We also show that any coreset for stable instances consisting of only input points must have size $\\Omega(k/\\varepsilon^2)$. Our results for Sensitivity Sampling also extend to the $k$-median problem, and more general metric spaces.","sentences":["Coresets are arguably the most popular compression paradigm for center-based clustering objectives such as $k$-means.","Given a point set $P$, a coreset $\\Omega$ is a small, weighted summary that preserves the cost of all candidate solutions $S$ up to a $(1\\pm \\varepsilon)$ factor.","For $k$-means in $d$-dimensional Euclidean space the cost for solution $S$ is $\\sum_{p\\in P}\\min_{s\\in S}\\|p-s\\|^2$.   A very popular method for coreset construction, both in theory and practice, is Sensitivity Sampling, where points are sampled in proportion to their importance.","We show that Sensitivity Sampling yields optimal coresets of size $\\tilde{O}(k/\\varepsilon^2\\min(\\sqrt{k},\\varepsilon^{-2}))$ for worst-case instances.","Uniquely among all known coreset algorithms, for well-clusterable data sets with $\\Omega(1)$ cost stability, Sensitivity Sampling gives coresets of size $\\tilde{O}(k/\\varepsilon^2)$, improving over the worst-case lower bound.","Notably, Sensitivity Sampling does not have to know the cost stability in order to exploit it: It is appropriately sensitive to the clusterability of the data set while being oblivious to it.   ","We also show that any coreset for stable instances consisting of only input points must have size $\\Omega(k/\\varepsilon^2)$. Our results for Sensitivity Sampling also extend to the $k$-median problem, and more general metric spaces."],"url":"http://arxiv.org/abs/2405.01339v1","category":"cs.DS"}
{"created":"2024-05-02 14:35:55","title":"How much entanglement is needed for quantum error correction?","abstract":"It is commonly believed that logical states of quantum error-correcting codes have to be highly entangled such that codes capable of correcting more errors require more entanglement to encode a qubit. Here we show that this belief may or may not be true depending on a particular code. To this end, we characterize a tradeoff between the code distance $d$ quantifying the number of correctable errors, and geometric entanglement of logical states quantifying their maximal overlap with product states or more general \"topologically trivial\" states. The maximum overlap is shown to be exponentially small in $d$ for three families of codes: (1) low-density parity check (LDPC) codes with commuting check operators, (2) stabilizer codes, and (3) codes with a constant encoding rate. Equivalently, the geometric entanglement of any logical state of these codes grows at least linearly with $d$. On the opposite side, we also show that this distance-entanglement tradeoff does not hold in general. For any constant $d$ and $k$ (number of logical qubits), we show there exists a family of codes such that the geometric entanglement of some logical states approaches zero in the limit of large code length.","sentences":["It is commonly believed that logical states of quantum error-correcting codes have to be highly entangled such that codes capable of correcting more errors require more entanglement to encode a qubit.","Here we show that this belief may or may not be true depending on a particular code.","To this end, we characterize a tradeoff between the code distance $d$ quantifying the number of correctable errors, and geometric entanglement of logical states quantifying their maximal overlap with product states or more general \"topologically trivial\" states.","The maximum overlap is shown to be exponentially small in $d$ for three families of codes: (1) low-density parity check (LDPC) codes with commuting check operators, (2) stabilizer codes, and (3) codes with a constant encoding rate.","Equivalently, the geometric entanglement of any logical state of these codes grows at least linearly with $d$. On the opposite side, we also show that this distance-entanglement tradeoff does not hold in general.","For any constant $d$ and $k$ (number of logical qubits), we show there exists a family of codes such that the geometric entanglement of some logical states approaches zero in the limit of large code length."],"url":"http://arxiv.org/abs/2405.01332v1","category":"quant-ph"}
{"created":"2024-05-02 14:32:07","title":"An Advanced Framework for Ultra-Realistic Simulation and Digital Twinning for Autonomous Vehicles","abstract":"Simulation is a fundamental tool in developing autonomous vehicles, enabling rigorous testing without the logistical and safety challenges associated with real-world trials. As autonomous vehicle technologies evolve and public safety demands increase, advanced, realistic simulation frameworks are critical. Current testing paradigms employ a mix of general-purpose and specialized simulators, such as CARLA and IVRESS, to achieve high-fidelity results. However, these tools often struggle with compatibility due to differing platform, hardware, and software requirements, severely hampering their combined effectiveness. This paper introduces BlueICE, an advanced framework for ultra-realistic simulation and digital twinning, to address these challenges. BlueICE's innovative architecture allows for the decoupling of computing platforms, hardware, and software dependencies while offering researchers customizable testing environments to meet diverse fidelity needs. Key features include containerization to ensure compatibility across different systems, a unified communication bridge for seamless integration of various simulation tools, and synchronized orchestration of input and output across simulators. This framework facilitates the development of sophisticated digital twins for autonomous vehicle testing and sets a new standard in simulation accuracy and flexibility. The paper further explores the application of BlueICE in two distinct case studies: the ICAT indoor testbed and the STAR campus outdoor testbed at the University of Delaware. These case studies demonstrate BlueICE's capability to create sophisticated digital twins for autonomous vehicle testing and underline its potential as a standardized testbed for future autonomous driving technologies.","sentences":["Simulation is a fundamental tool in developing autonomous vehicles, enabling rigorous testing without the logistical and safety challenges associated with real-world trials.","As autonomous vehicle technologies evolve and public safety demands increase, advanced, realistic simulation frameworks are critical.","Current testing paradigms employ a mix of general-purpose and specialized simulators, such as CARLA and IVRESS, to achieve high-fidelity results.","However, these tools often struggle with compatibility due to differing platform, hardware, and software requirements, severely hampering their combined effectiveness.","This paper introduces BlueICE, an advanced framework for ultra-realistic simulation and digital twinning, to address these challenges.","BlueICE's innovative architecture allows for the decoupling of computing platforms, hardware, and software dependencies while offering researchers customizable testing environments to meet diverse fidelity needs.","Key features include containerization to ensure compatibility across different systems, a unified communication bridge for seamless integration of various simulation tools, and synchronized orchestration of input and output across simulators.","This framework facilitates the development of sophisticated digital twins for autonomous vehicle testing and sets a new standard in simulation accuracy and flexibility.","The paper further explores the application of BlueICE in two distinct case studies: the ICAT indoor testbed and the STAR campus outdoor testbed at the University of Delaware.","These case studies demonstrate BlueICE's capability to create sophisticated digital twins for autonomous vehicle testing and underline its potential as a standardized testbed for future autonomous driving technologies."],"url":"http://arxiv.org/abs/2405.01328v1","category":"cs.RO"}
{"created":"2024-05-02 14:31:29","title":"The effects of a minimal length on the Kerr metric and the Hawking temperature","abstract":"A brief review of the pseudo complex General Relativity (pcGR) will be presented, with its consequences, as the accumulation of a dark energy around a mass and a generalized Machs principle. The main objective in this contribution is to determine the Hawking temperature and the Entropy for various limits: i) The pc-Schwarzschild case with no minimal length present, ii) the pc-Kerr metric without a minimal length and iii) the general case, the pc-Kerr metric with a minimal length present. The physical consequences of a minimal length will be discussed, a possible interpretation of a gravitational Schwinger effect and the appearance of negative temperature. For large masses a minimal length does not show any sensible effect, but only for very small masses, several orders of the Planck mass, where non-trivial effects emerge, important for the production of mini-black holes in the early universe. Our results are more general than being restricted to pcGR. Any other model which assumes a distribution of dark energy around a stellar body produces the same effects. In contrast to these models, pcGR demands the presence of a dark energy term.","sentences":["A brief review of the pseudo complex General Relativity (pcGR) will be presented, with its consequences, as the accumulation of a dark energy around a mass and a generalized Machs principle.","The main objective in this contribution is to determine the Hawking temperature and the Entropy for various limits: i)","The pc-Schwarzschild case with no minimal length present, ii) the pc-Kerr metric without a minimal length and iii) the general case, the pc-Kerr metric with a minimal length present.","The physical consequences of a minimal length will be discussed, a possible interpretation of a gravitational Schwinger effect and the appearance of negative temperature.","For large masses a minimal length does not show any sensible effect, but only for very small masses, several orders of the Planck mass, where non-trivial effects emerge, important for the production of mini-black holes in the early universe.","Our results are more general than being restricted to pcGR.","Any other model which assumes a distribution of dark energy around a stellar body produces the same effects.","In contrast to these models, pcGR demands the presence of a dark energy term."],"url":"http://arxiv.org/abs/2405.01325v1","category":"gr-qc"}
{"created":"2024-05-02 14:29:11","title":"Direct evidence for efficient carrier multiplication in the topological insulator Bi$_2$Se$_3$","abstract":"Carrier multiplication (CM), where the absorption of a single photon results in the generation of several electron-hole pairs via impact ionization, plays a pivotal role in the quest for enhancing the performance of solar cells beyond the Shockley-Queisser limit. The combination of its narrow bandgap relative to the photon energy of visible light, along with its low phonon frequencies that hinder efficient energy dissipation into phonons, makes the topological insulator Bi$_2$Se$_3$ an optimal candidate material for efficient CM. Here we use time- and angle-resolved photoemission spectroscopy (trARPES) to trace the number of electron-hole pairs after photoexcitation of Bi$_2$Se$_3$ with visible pump pulses at $\\hbar\\omega=2$ eV. We find that both the number of electrons inside the conduction band as well as the number of holes inside the valence band keep increasing long after the pump pulse is gone, providing direct evidence for CM. We also analyze the transient band structure as well as the hot carrier dynamics inside the conduction band, providing a complete picture of the non-equilibrium carrier dynamics in photoexcited Bi$_2$Se$_3$ which can now serve as a basis for novel optoelectronic applications.","sentences":["Carrier multiplication (CM), where the absorption of a single photon results in the generation of several electron-hole pairs via impact ionization, plays a pivotal role in the quest for enhancing the performance of solar cells beyond the Shockley-Queisser limit.","The combination of its narrow bandgap relative to the photon energy of visible light, along with its low phonon frequencies that hinder efficient energy dissipation into phonons, makes the topological insulator Bi$_2$Se$_3$ an optimal candidate material for efficient CM.","Here we use time- and angle-resolved photoemission spectroscopy (trARPES) to trace the number of electron-hole pairs after photoexcitation of Bi$_2$Se$_3$ with visible pump pulses at $\\hbar\\omega=2$ eV. We find that both the number of electrons inside the conduction band as well as the number of holes inside the valence band keep increasing long after the pump pulse is gone, providing direct evidence for CM.","We also analyze the transient band structure as well as the hot carrier dynamics inside the conduction band, providing a complete picture of the non-equilibrium carrier dynamics in photoexcited Bi$_2$Se$_3$ which can now serve as a basis for novel optoelectronic applications."],"url":"http://arxiv.org/abs/2405.01323v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 14:24:56","title":"Data Scoping: Effectively Learning the Evolution of Generic Transport PDEs","abstract":"Transport phenomena (e.g., fluid flows) are governed by time-dependent partial differential equations (PDEs) describing mass, momentum, and energy conservation, and are ubiquitous in many engineering applications. However, deep learning architectures are fundamentally incompatible with the simulation of these PDEs. This paper clearly articulates and then solves this incompatibility. The local-dependency of generic transport PDEs implies that it only involves local information to predict the physical properties at a location in the next time step. However, the deep learning architecture will inevitably increase the scope of information to make such predictions as the number of layers increases, which can cause sluggish convergence and compromise generalizability. This paper aims to solve this problem by proposing a distributed data scoping method with linear time complexity to strictly limit the scope of information to predict the local properties. The numerical experiments over multiple physics show that our data scoping method significantly accelerates training convergence and improves the generalizability of benchmark models on large-scale engineering simulations. Specifically, over the geometries not included in the training data for heat transferring simulation, it can increase the accuracy of Convolutional Neural Networks (CNNs) by 21.7 \\% and that of Fourier Neural Operators (FNOs) by 38.5 \\% on average.","sentences":["Transport phenomena (e.g., fluid flows) are governed by time-dependent partial differential equations (PDEs) describing mass, momentum, and energy conservation, and are ubiquitous in many engineering applications.","However, deep learning architectures are fundamentally incompatible with the simulation of these PDEs.","This paper clearly articulates and then solves this incompatibility.","The local-dependency of generic transport PDEs implies that it only involves local information to predict the physical properties at a location in the next time step.","However, the deep learning architecture will inevitably increase the scope of information to make such predictions as the number of layers increases, which can cause sluggish convergence and compromise generalizability.","This paper aims to solve this problem by proposing a distributed data scoping method with linear time complexity to strictly limit the scope of information to predict the local properties.","The numerical experiments over multiple physics show that our data scoping method significantly accelerates training convergence and improves the generalizability of benchmark models on large-scale engineering simulations.","Specifically, over the geometries not included in the training data for heat transferring simulation, it can increase the accuracy of Convolutional Neural Networks (CNNs) by 21.7 \\% and that of Fourier Neural Operators (FNOs) by 38.5 \\% on average."],"url":"http://arxiv.org/abs/2405.01319v1","category":"cs.LG"}
{"created":"2024-05-02 14:22:34","title":"On small-amplitude asymmetric water waves","abstract":"We generalize the method used by M{\\ae}hlen & Seth [17] used to prove the existence of small-amplitude asymmetric solutions to the capillary-gravity Whiham equation, so that it can be applied directly to a class of similar equations. The purpose is to prove or disprove the existence of asymmetric waves for the water wave problem or other model equations for water waves. Our main result in this paper is a theorem that gives both necessary and sufficient conditions for the existence of small-amplitude periodic asymmetry solutions for this class of equations. The result is then applied to an infinite depth capillary-gravity Whitham equation and an infinite depth capillary-gravity Babenko equation to show the nonexistence of small-amplitude waves for these equations. This example also highlights the similarities between these equations suggesting the potential existence of small-amplitude asymmetric waves for the finite depth capillary-gravity Babenko equation.","sentences":["We generalize the method used by M{\\ae}hlen & Seth","[17] used to prove the existence of small-amplitude asymmetric solutions to the capillary-gravity Whiham equation, so that it can be applied directly to a class of similar equations.","The purpose is to prove or disprove the existence of asymmetric waves for the water wave problem or other model equations for water waves.","Our main result in this paper is a theorem that gives both necessary and sufficient conditions for the existence of small-amplitude periodic asymmetry solutions for this class of equations.","The result is then applied to an infinite depth capillary-gravity Whitham equation and an infinite depth capillary-gravity Babenko equation to show the nonexistence of small-amplitude waves for these equations.","This example also highlights the similarities between these equations suggesting the potential existence of small-amplitude asymmetric waves for the finite depth capillary-gravity Babenko equation."],"url":"http://arxiv.org/abs/2405.01315v1","category":"math.AP"}
{"created":"2024-05-02 14:21:29","title":"Non-iterative Optimization of Trajectory and Radio Resource for Aerial Network","abstract":"We address a joint trajectory planning, user association, resource allocation, and power control problem to maximize proportional fairness in the aerial IoT network, considering practical end-to-end quality-of-service (QoS) and communication schedules. Though the problem is rather ancient, apart from the fact that the previous approaches have never considered user- and time-specific QoS, we point out a prevalent mistake in coordinate optimization approaches adopted by the majority of the literature. Coordinate optimization approaches, which repetitively optimize radio resources for a fixed trajectory and vice versa, generally converge to local optima when all variables are differentiable. However, these methods often stagnate at a non-stationary point, significantly degrading the network utility in mixed-integer problems such as joint trajectory and radio resource optimization. We detour this problem by converting the formulated problem into the Markov decision process (MDP). Exploiting the beneficial characteristics of the MDP, we design a non-iterative framework that cooperatively optimizes trajectory and radio resources without initial trajectory choice. The proposed framework can incorporate various trajectory planning algorithms such as the genetic algorithm, tree search, and reinforcement learning. Extensive comparisons with diverse baselines verify that the proposed framework significantly outperforms the state-of-the-art method, nearly achieving the global optimum. Our implementation code is available at https://github.com/hslyu/dbspf.","sentences":["We address a joint trajectory planning, user association, resource allocation, and power control problem to maximize proportional fairness in the aerial IoT network, considering practical end-to-end quality-of-service (QoS) and communication schedules.","Though the problem is rather ancient, apart from the fact that the previous approaches have never considered user- and time-specific QoS, we point out a prevalent mistake in coordinate optimization approaches adopted by the majority of the literature.","Coordinate optimization approaches, which repetitively optimize radio resources for a fixed trajectory and vice versa, generally converge to local optima when all variables are differentiable.","However, these methods often stagnate at a non-stationary point, significantly degrading the network utility in mixed-integer problems such as joint trajectory and radio resource optimization.","We detour this problem by converting the formulated problem into the Markov decision process (MDP).","Exploiting the beneficial characteristics of the MDP, we design a non-iterative framework that cooperatively optimizes trajectory and radio resources without initial trajectory choice.","The proposed framework can incorporate various trajectory planning algorithms such as the genetic algorithm, tree search, and reinforcement learning.","Extensive comparisons with diverse baselines verify that the proposed framework significantly outperforms the state-of-the-art method, nearly achieving the global optimum.","Our implementation code is available at https://github.com/hslyu/dbspf."],"url":"http://arxiv.org/abs/2405.01314v1","category":"eess.SY"}
{"created":"2024-05-02 14:20:24","title":"Privacy-Enhanced Database Synthesis for Benchmark Publishing","abstract":"Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often fail to reflect the varied nature of user workloads. As a result, there is increasing momentum toward creating databases that incorporate real-world user data to more accurately mirror business environments. However, privacy concerns deter users from directly sharing their data, underscoring the importance of creating synthesized databases for benchmarking that also prioritize privacy protection. Differential privacy has become a key method for safeguarding privacy when sharing data, but the focus has largely been on minimizing errors in aggregate queries or classification tasks, with less attention given to benchmarking factors like runtime performance. This paper delves into the creation of privacy-preserving databases specifically for benchmarking, aiming to produce a differentially private database whose query performance closely resembles that of the original data. Introducing PrivBench, an innovative synthesis framework, we support the generation of high-quality data that maintains privacy. PrivBench uses sum-product networks (SPNs) to partition and sample data, enhancing data representation while securing privacy. The framework allows users to adjust the detail of SPN partitions and privacy settings, crucial for customizing privacy levels. We validate our approach, which uses the Laplace and exponential mechanisms, in maintaining privacy. Our tests show that PrivBench effectively generates data that maintains privacy and excels in query performance, consistently reducing errors in query execution time, query cardinality, and KL divergence.","sentences":["Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often fail to reflect the varied nature of user workloads.","As a result, there is increasing momentum toward creating databases that incorporate real-world user data to more accurately mirror business environments.","However, privacy concerns deter users from directly sharing their data, underscoring the importance of creating synthesized databases for benchmarking that also prioritize privacy protection.","Differential privacy has become a key method for safeguarding privacy when sharing data, but the focus has largely been on minimizing errors in aggregate queries or classification tasks, with less attention given to benchmarking factors like runtime performance.","This paper delves into the creation of privacy-preserving databases specifically for benchmarking, aiming to produce a differentially private database whose query performance closely resembles that of the original data.","Introducing PrivBench, an innovative synthesis framework, we support the generation of high-quality data that maintains privacy.","PrivBench uses sum-product networks (SPNs) to partition and sample data, enhancing data representation while securing privacy.","The framework allows users to adjust the detail of SPN partitions and privacy settings, crucial for customizing privacy levels.","We validate our approach, which uses the Laplace and exponential mechanisms, in maintaining privacy.","Our tests show that PrivBench effectively generates data that maintains privacy and excels in query performance, consistently reducing errors in query execution time, query cardinality, and KL divergence."],"url":"http://arxiv.org/abs/2405.01312v1","category":"cs.DB"}
{"created":"2024-05-02 14:20:20","title":"Imagine the Unseen: Occluded Pedestrian Detection via Adversarial Feature Completion","abstract":"Pedestrian detection has significantly progressed in recent years, thanks to the development of DNNs. However, detection performance at occluded scenes is still far from satisfactory, as occlusion increases the intra-class variance of pedestrians, hindering the model from finding an accurate classification boundary between pedestrians and background clutters. From the perspective of reducing intra-class variance, we propose to complete features for occluded regions so as to align the features of pedestrians across different occlusion patterns. An important premise for feature completion is to locate occluded regions. From our analysis, channel features of different pedestrian proposals only show high correlation values at visible parts and thus feature correlations can be used to model occlusion patterns. In order to narrow down the gap between completed features and real fully visible ones, we propose an adversarial learning method, which completes occluded features with a generator such that they can hardly be distinguished by the discriminator from real fully visible features. We report experimental results on the CityPersons, Caltech and CrowdHuman datasets. On CityPersons, we show significant improvements over five different baseline detectors, especially on the heavy occlusion subset. Furthermore, we show that our proposed method FeatComp++ achieves state-of-the-art results on all the above three datasets without relying on extra cues.","sentences":["Pedestrian detection has significantly progressed in recent years, thanks to the development of DNNs.","However, detection performance at occluded scenes is still far from satisfactory, as occlusion increases the intra-class variance of pedestrians, hindering the model from finding an accurate classification boundary between pedestrians and background clutters.","From the perspective of reducing intra-class variance, we propose to complete features for occluded regions so as to align the features of pedestrians across different occlusion patterns.","An important premise for feature completion is to locate occluded regions.","From our analysis, channel features of different pedestrian proposals only show high correlation values at visible parts and thus feature correlations can be used to model occlusion patterns.","In order to narrow down the gap between completed features and real fully visible ones, we propose an adversarial learning method, which completes occluded features with a generator such that they can hardly be distinguished by the discriminator from real fully visible features.","We report experimental results on the CityPersons, Caltech and CrowdHuman datasets.","On CityPersons, we show significant improvements over five different baseline detectors, especially on the heavy occlusion subset.","Furthermore, we show that our proposed method FeatComp++ achieves state-of-the-art results on all the above three datasets without relying on extra cues."],"url":"http://arxiv.org/abs/2405.01311v1","category":"cs.CV"}
{"created":"2024-05-02 14:19:25","title":"Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf Disease Remediation","abstract":"This research introduces an innovative AI-driven precision agriculture system, leveraging YOLOv8 for disease identification and Retrieval Augmented Generation (RAG) for context-aware diagnosis. Focused on addressing the challenges of diseases affecting the coffee production sector in Karnataka, The system integrates sophisticated object detection techniques with language models to address the inherent constraints associated with Large Language Models (LLMs). Our methodology not only tackles the issue of hallucinations in LLMs, but also introduces dynamic disease identification and remediation strategies. Real-time monitoring, collaborative dataset expansion, and organizational involvement ensure the system's adaptability in diverse agricultural settings. The effect of the suggested system extends beyond automation, aiming to secure food supplies, protect livelihoods, and promote eco-friendly farming practices. By facilitating precise disease identification, the system contributes to sustainable and environmentally conscious agriculture, reducing reliance on pesticides. Looking to the future, the project envisions continuous development in RAG-integrated object detection systems, emphasizing scalability, reliability, and usability. This research strives to be a beacon for positive change in agriculture, aligning with global efforts toward sustainable and technologically enhanced food production.","sentences":["This research introduces an innovative AI-driven precision agriculture system, leveraging YOLOv8 for disease identification and Retrieval Augmented Generation (RAG) for context-aware diagnosis.","Focused on addressing the challenges of diseases affecting the coffee production sector in Karnataka, The system integrates sophisticated object detection techniques with language models to address the inherent constraints associated with Large Language Models (LLMs).","Our methodology not only tackles the issue of hallucinations in LLMs, but also introduces dynamic disease identification and remediation strategies.","Real-time monitoring, collaborative dataset expansion, and organizational involvement ensure the system's adaptability in diverse agricultural settings.","The effect of the suggested system extends beyond automation, aiming to secure food supplies, protect livelihoods, and promote eco-friendly farming practices.","By facilitating precise disease identification, the system contributes to sustainable and environmentally conscious agriculture, reducing reliance on pesticides.","Looking to the future, the project envisions continuous development in RAG-integrated object detection systems, emphasizing scalability, reliability, and usability.","This research strives to be a beacon for positive change in agriculture, aligning with global efforts toward sustainable and technologically enhanced food production."],"url":"http://arxiv.org/abs/2405.01310v1","category":"cs.IR"}
{"created":"2024-05-02 14:13:58","title":"Prediction of Room-Temperature Electric Field Reversal of Magnetization in the Family of $A_4B_3$O$_9$ Layered Oxides","abstract":"The promise of a strong magnetoelectric (ME) coupling in a multiferroic (MF) material is not only of fundamental interest, but also forms the basis of next generation memory devices where the direction of magnetization can be reversed by an external electric field. Using group-theory led first-principles calculations, we determine the ME properties of a relatively understudied family of layered oxides with the general formula $A_4B_3$O$_9$. We show how the tetrahedral rotations in these oxides can lead to a variety of hitherto unknown structural phases with different symmetries. In particular, a polar phase in the $Cmc2_1$ space group has been identified where a weak ferromagnetic mode arises spontaneously via a canting of the antiferromagnetically ordered $B$-site spins. In this polar phase, the polar mode couples to the magnetic modes through a rare $\\Gamma$-point ME-MF coupling scheme such that the net magnetization can be directly reversed by an electric field switching of the polar mode. Moreover, in agreement with previous experimental observations, we predict room-temperature magnetism in $A_4B_3$O$_9$ layered oxides which is supported by our calculations of the magnetic exchange interaction parameters, further indicating the potential of these compounds in practical technological applications.","sentences":["The promise of a strong magnetoelectric (ME) coupling in a multiferroic (MF) material is not only of fundamental interest, but also forms the basis of next generation memory devices where the direction of magnetization can be reversed by an external electric field.","Using group-theory led first-principles calculations, we determine the ME properties of a relatively understudied family of layered oxides with the general formula $A_4B_3$O$_9$. We show how the tetrahedral rotations in these oxides can lead to a variety of hitherto unknown structural phases with different symmetries.","In particular, a polar phase in the $Cmc2_1$ space group has been identified where a weak ferromagnetic mode arises spontaneously via a canting of the antiferromagnetically ordered $B$-site spins.","In this polar phase, the polar mode couples to the magnetic modes through a rare $\\Gamma$-point ME-MF coupling scheme such that the net magnetization can be directly reversed by an electric field switching of the polar mode.","Moreover, in agreement with previous experimental observations, we predict room-temperature magnetism in $A_4B_3$O$_9$ layered oxides which is supported by our calculations of the magnetic exchange interaction parameters, further indicating the potential of these compounds in practical technological applications."],"url":"http://arxiv.org/abs/2405.01307v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 14:12:58","title":"Graph is all you need? Lightweight data-agnostic neural architecture search without training","abstract":"Neural architecture search (NAS) enables the automatic design of neural network models. However, training the candidates generated by the search algorithm for performance evaluation incurs considerable computational overhead. Our method, dubbed nasgraph, remarkably reduces the computational costs by converting neural architectures to graphs and using the average degree, a graph measure, as the proxy in lieu of the evaluation metric. Our training-free NAS method is data-agnostic and light-weight. It can find the best architecture among 200 randomly sampled architectures from NAS-Bench201 in 217 CPU seconds. Besides, our method is able to achieve competitive performance on various datasets including NASBench-101, NASBench-201, and NDS search spaces. We also demonstrate that nasgraph generalizes to more challenging tasks on Micro TransNAS-Bench-101.","sentences":["Neural architecture search (NAS) enables the automatic design of neural network models.","However, training the candidates generated by the search algorithm for performance evaluation incurs considerable computational overhead.","Our method, dubbed nasgraph, remarkably reduces the computational costs by converting neural architectures to graphs and using the average degree, a graph measure, as the proxy in lieu of the evaluation metric.","Our training-free NAS method is data-agnostic and light-weight.","It can find the best architecture among 200 randomly sampled architectures from NAS-Bench201 in 217 CPU seconds.","Besides, our method is able to achieve competitive performance on various datasets including NASBench-101, NASBench-201, and NDS search spaces.","We also demonstrate that nasgraph generalizes to more challenging tasks on Micro TransNAS-Bench-101."],"url":"http://arxiv.org/abs/2405.01306v1","category":"cs.LG"}
{"created":"2024-05-02 14:11:50","title":"Distributed Representations Enable Robust Multi-Timescale Computation in Neuromorphic Hardware","abstract":"Programming recurrent spiking neural networks (RSNNs) to robustly perform multi-timescale computation remains a difficult challenge. To address this, we show how the distributed approach offered by vector symbolic architectures (VSAs), which uses high-dimensional random vectors as the smallest units of representation, can be leveraged to embed robust multi-timescale dynamics into attractor-based RSNNs. We embed finite state machines into the RSNN dynamics by superimposing a symmetric autoassociative weight matrix and asymmetric transition terms. The transition terms are formed by the VSA binding of an input and heteroassociative outer-products between states. Our approach is validated through simulations with highly non-ideal weights; an experimental closed-loop memristive hardware setup; and on Loihi 2, where it scales seamlessly to large state machines. This work demonstrates the effectiveness of VSA representations for embedding robust computation with recurrent dynamics into neuromorphic hardware, without requiring parameter fine-tuning or significant platform-specific optimisation. This advances VSAs as a high-level representation-invariant abstract language for cognitive algorithms in neuromorphic hardware.","sentences":["Programming recurrent spiking neural networks (RSNNs) to robustly perform multi-timescale computation remains a difficult challenge.","To address this, we show how the distributed approach offered by vector symbolic architectures (VSAs), which uses high-dimensional random vectors as the smallest units of representation, can be leveraged to embed robust multi-timescale dynamics into attractor-based RSNNs.","We embed finite state machines into the RSNN dynamics by superimposing a symmetric autoassociative weight matrix and asymmetric transition terms.","The transition terms are formed by the VSA binding of an input and heteroassociative outer-products between states.","Our approach is validated through simulations with highly non-ideal weights; an experimental closed-loop memristive hardware setup; and on Loihi 2, where it scales seamlessly to large state machines.","This work demonstrates the effectiveness of VSA representations for embedding robust computation with recurrent dynamics into neuromorphic hardware, without requiring parameter fine-tuning or significant platform-specific optimisation.","This advances VSAs as a high-level representation-invariant abstract language for cognitive algorithms in neuromorphic hardware."],"url":"http://arxiv.org/abs/2405.01305v1","category":"cs.NE"}
{"created":"2024-05-02 14:00:22","title":"The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation","abstract":"Large Language Models (LLMs) have emerged as powerful support tools across various natural language tasks and a range of application domains. Recent studies focus on exploring their capabilities for data annotation. This paper provides a comparative overview of twelve studies investigating the potential of LLMs in labelling data. While the models demonstrate promising cost and time-saving benefits, there exist considerable limitations, such as representativeness, bias, sensitivity to prompt variations and English language preference. Leveraging insights from these studies, our empirical analysis further examines the alignment between human and GPT-generated opinion distributions across four subjective datasets. In contrast to the studies examining representation, our methodology directly obtains the opinion distribution from GPT. Our analysis thereby supports the minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.","sentences":["Large Language Models (LLMs) have emerged as powerful support tools across various natural language tasks and a range of application domains.","Recent studies focus on exploring their capabilities for data annotation.","This paper provides a comparative overview of twelve studies investigating the potential of LLMs in labelling data.","While the models demonstrate promising cost and time-saving benefits, there exist considerable limitations, such as representativeness, bias, sensitivity to prompt variations and English language preference.","Leveraging insights from these studies, our empirical analysis further examines the alignment between human and GPT-generated opinion distributions across four subjective datasets.","In contrast to the studies examining representation, our methodology directly obtains the opinion distribution from GPT.","Our analysis thereby supports the minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction."],"url":"http://arxiv.org/abs/2405.01299v1","category":"cs.CL"}
{"created":"2024-05-02 14:00:20","title":"Reorthogonalized Pythagorean variants of block classical Gram-Schmidt","abstract":"Block classical Gram-Schmidt (BCGS) is commonly used for orthogonalizing a set of vectors $X$ in distributed computing environments due to its favorable communication properties relative to other orthogonalization approaches, such as modified Gram-Schmidt or Householder. However, it is known that BCGS (as well as recently developed low-synchronization variants of BCGS) can suffer from a significant loss of orthogonality in finite-precision arithmetic, which can contribute to instability and inaccurate solutions in downstream applications such as $s$-step Krylov subspace methods. A common solution to improve the orthogonality among the vectors is reorthogonalization. Focusing on the \"Pythagorean\" variant of BCGS, introduced in [E. Carson, K. Lund, & M. Rozlo\\v{z}n\\'{i}k. SIAM J. Matrix Anal. Appl. 42(3), pp. 1365--1380, 2021], which guarantees an $O(\\varepsilon)\\kappa^2(X)$ bound on the loss of orthogonality as long as $O(\\varepsilon)\\kappa^2(X)<1$, where $\\varepsilon$ denotes the unit roundoff, we introduce and analyze two reorthogonalized Pythagorean BCGS variants. These variants feature favorable communication properties, with asymptotically two synchronization points per block column, as well as an improved $O(\\varepsilon)$ bound on the loss of orthogonality. Our bounds are derived in a general fashion to additionally allow for the analysis of mixed-precision variants. We verify our theoretical results with a panel of test matrices and experiments from a new version of the \\texttt{BlockStab} toolbox.","sentences":["Block classical Gram-Schmidt (BCGS) is commonly used for orthogonalizing a set of vectors $X$ in distributed computing environments due to its favorable communication properties relative to other orthogonalization approaches, such as modified Gram-Schmidt or Householder.","However, it is known that BCGS (as well as recently developed low-synchronization variants of BCGS) can suffer from a significant loss of orthogonality in finite-precision arithmetic, which can contribute to instability and inaccurate solutions in downstream applications such as $s$-step Krylov subspace methods.","A common solution to improve the orthogonality among the vectors is reorthogonalization.","Focusing on the \"Pythagorean\" variant of BCGS, introduced in [E. Carson, K. Lund, & M. Rozlo\\v{z}n\\'{i}k.","SIAM J. Matrix Anal.","Appl.","42(3)",", pp. 1365--1380, 2021], which guarantees an $O(\\varepsilon)\\kappa^2(X)$ bound on the loss of orthogonality as long as $O(\\varepsilon)\\kappa^2(X)<1$, where $\\varepsilon$ denotes the unit roundoff, we introduce and analyze two reorthogonalized Pythagorean BCGS variants.","These variants feature favorable communication properties, with asymptotically two synchronization points per block column, as well as an improved $O(\\varepsilon)$ bound on the loss of orthogonality.","Our bounds are derived in a general fashion to additionally allow for the analysis of mixed-precision variants.","We verify our theoretical results with a panel of test matrices and experiments from a new version of the \\texttt{BlockStab} toolbox."],"url":"http://arxiv.org/abs/2405.01298v1","category":"math.NA"}
{"created":"2024-05-02 14:00:00","title":"Boosting gravitational waves: a review of kinematic effects on amplitude, polarization, frequency and energy density","abstract":"We review the kinematic effects on a gravitational wave due to either a peculiar motion of the astrophysical source emitting it or a local motion of the observer. We show, at fully non-linear order in velocity, that the amplitude of the wave is amplified by the Doppler factor in the case in which the source moves with respect to a reference frame, while it is invariant if the observer moves (with respect to a reference observer). However, the observed specific intensity transforms in the same way under a boost of the source or of the observer. We also show at fully non-linear order that under a boost (of either source or observer), the polarization tensor is rotated in the same way the wave direction is rotated by aberration, such that the only net effect of a boost on polarization is to change the phase of the helicity components. We apply these results to a wave emitted by a binary system of compact objects in the cosmological context.","sentences":["We review the kinematic effects on a gravitational wave due to either a peculiar motion of the astrophysical source emitting it or a local motion of the observer.","We show, at fully non-linear order in velocity, that the amplitude of the wave is amplified by the Doppler factor in the case in which the source moves with respect to a reference frame, while it is invariant if the observer moves (with respect to a reference observer).","However, the observed specific intensity transforms in the same way under a boost of the source or of the observer.","We also show at fully non-linear order that under a boost (of either source or observer), the polarization tensor is rotated in the same way the wave direction is rotated by aberration, such that the only net effect of a boost on polarization is to change the phase of the helicity components.","We apply these results to a wave emitted by a binary system of compact objects in the cosmological context."],"url":"http://arxiv.org/abs/2405.01297v1","category":"gr-qc"}
{"created":"2024-05-02 13:58:14","title":"Cutoff Scale of Quadratic Gravity from Quantum Focusing Conjecture","abstract":"We derive the cutoff length scale of the quadratic gravity in $d \\geq 5$ dimensional spacetime by demanding that the quantum focusing conjecture for the smeared quantum expansion holds at the classical level. The cutoff scale has different dependence on the spacetime dimension depending on the sign of the coupling constant of the quadratic gravity. We also investigate a concrete example of the 5-dimensional Schwarzschild spacetime and directly confirm that the quantum focusing conjecture holds when the quantum expansion is smeared over the scale larger than our cutoff scale.","sentences":["We derive the cutoff length scale of the quadratic gravity in $d \\geq 5$ dimensional spacetime by demanding that the quantum focusing conjecture for the smeared quantum expansion holds at the classical level.","The cutoff scale has different dependence on the spacetime dimension depending on the sign of the coupling constant of the quadratic gravity.","We also investigate a concrete example of the 5-dimensional Schwarzschild spacetime and directly confirm that the quantum focusing conjecture holds when the quantum expansion is smeared over the scale larger than our cutoff scale."],"url":"http://arxiv.org/abs/2405.01296v1","category":"hep-th"}
{"created":"2024-05-02 13:57:56","title":"Thermodynamic theory of inverse current in coupled quantum transport","abstract":"The inverse current in coupled (ICC) quantum transport, where one induced current opposes all thermodynamic forces of a system, is a highly counter-intuitive transport phenomenon. Using an exactly solvable model of strongly-coupled quantum dots, we present thermodynamic description of ICC in energy and spin-induced particle currents, with potential applications towards unconventional and autonomous nanoscale thermoelectric generators. Our analysis reveals the connection between microscopic and macroscopic formulations of entropy production rates, elucidating the often-overlooked role of proper thermodynamic forces and conjugate fluxes in characterizing genuine ICC. In our model, the seemingly paradoxical results of ICC in the energy current arise from chemical work done by current-carrying quantum particles, while in spin-induced particle current, it stems from the relative competition between electron reservoirs controlling one particular transition.","sentences":["The inverse current in coupled (ICC) quantum transport, where one induced current opposes all thermodynamic forces of a system, is a highly counter-intuitive transport phenomenon.","Using an exactly solvable model of strongly-coupled quantum dots, we present thermodynamic description of ICC in energy and spin-induced particle currents, with potential applications towards unconventional and autonomous nanoscale thermoelectric generators.","Our analysis reveals the connection between microscopic and macroscopic formulations of entropy production rates, elucidating the often-overlooked role of proper thermodynamic forces and conjugate fluxes in characterizing genuine ICC.","In our model, the seemingly paradoxical results of ICC in the energy current arise from chemical work done by current-carrying quantum particles, while in spin-induced particle current, it stems from the relative competition between electron reservoirs controlling one particular transition."],"url":"http://arxiv.org/abs/2405.01295v1","category":"quant-ph"}
{"created":"2024-05-02 13:54:39","title":"Low-resource speech recognition and dialect identification of Irish in a multi-task framework","abstract":"This paper explores the use of Hybrid CTC/Attention encoder-decoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID). Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN). An optimal InterCTC setting is initially established using a Conformer encoder. This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared. A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion. The experiments yielded an improvement in DID accuracy of 10.8% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID.","sentences":["This paper explores the use of Hybrid CTC/Attention encoder-decoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID).","Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN).","An optimal InterCTC setting is initially established using a Conformer encoder.","This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared.","A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion.","The experiments yielded an improvement in DID accuracy of 10.8% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model.","This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID."],"url":"http://arxiv.org/abs/2405.01293v1","category":"cs.CL"}
{"created":"2024-05-02 13:50:58","title":"A hypergraph model shows the carbon reduction potential of effective space use in housing","abstract":"Humans spend over 90% of their time in buildings which account for 40% of anthropogenic greenhouse gas (GHG) emissions, making buildings the leading cause of climate change. To incentivize more sustainable construction, building codes are used to enforce indoor comfort standards and maximum energy use. However, they currently only reward energy efficiency measures such as equipment or envelope upgrades and disregard the actual spatial configuration and usage. Using a new hypergraph model that encodes building floorplan organization and facilitates automatic geometry creation, we demonstrate that space efficiency outperforms envelope upgrades in terms of operational carbon emissions in 72%, 61% and 33% of surveyed buildings in Zurich, New York, and Singapore. Automatically generated floorplans for a case study in Zurich further increase access to daylight by up to 24%, revealing that auto-generated floorplans have the potential to improve the quality of residential spaces in terms of environmental performance and access to daylight.","sentences":["Humans spend over 90% of their time in buildings which account for 40% of anthropogenic greenhouse gas (GHG) emissions, making buildings the leading cause of climate change.","To incentivize more sustainable construction, building codes are used to enforce indoor comfort standards and maximum energy use.","However, they currently only reward energy efficiency measures such as equipment or envelope upgrades and disregard the actual spatial configuration and usage.","Using a new hypergraph model that encodes building floorplan organization and facilitates automatic geometry creation, we demonstrate that space efficiency outperforms envelope upgrades in terms of operational carbon emissions in 72%, 61% and 33% of surveyed buildings in Zurich, New York, and Singapore.","Automatically generated floorplans for a case study in Zurich further increase access to daylight by up to 24%, revealing that auto-generated floorplans have the potential to improve the quality of residential spaces in terms of environmental performance and access to daylight."],"url":"http://arxiv.org/abs/2405.01290v1","category":"cs.CG"}
{"created":"2024-05-02 13:49:41","title":"Strength and Sensitivity of Land-Atmosphere Interaction","abstract":"The land-atmosphere coupling strength has been defined as the percentage of precipitation variability explained by the variation of soil moisture in the Global Land-Atmosphere Coupling Experiment (GLACE). While it is useful to identify global hotspots of land-atmosphere interaction, this coupling strength is different from coupling sensitivity, which directly quantifies how precipitation generation responds to the perturbation of soil moisture and is essential for our understanding of the global water cycle. To disentangle these two quantities, here we theoretically explore the relationships among coupling strength, sensitivity, and soil moisture variances. We use climate model outputs to show that the largest soil moisture variances are located in the transitional climate zones and the variations of soil moisture largely account for the geographical patterns of coupling hotspots. The coupling sensitivity is not necessarily low in non-hotspot regions, which could impose great impacts on the development of extreme climate events. We therefore call for more research attention on coupling sensitivity to improve our understanding of the climate system.","sentences":["The land-atmosphere coupling strength has been defined as the percentage of precipitation variability explained by the variation of soil moisture in the Global Land-Atmosphere Coupling Experiment (GLACE).","While it is useful to identify global hotspots of land-atmosphere interaction, this coupling strength is different from coupling sensitivity, which directly quantifies how precipitation generation responds to the perturbation of soil moisture and is essential for our understanding of the global water cycle.","To disentangle these two quantities, here we theoretically explore the relationships among coupling strength, sensitivity, and soil moisture variances.","We use climate model outputs to show that the largest soil moisture variances are located in the transitional climate zones and the variations of soil moisture largely account for the geographical patterns of coupling hotspots.","The coupling sensitivity is not necessarily low in non-hotspot regions, which could impose great impacts on the development of extreme climate events.","We therefore call for more research attention on coupling sensitivity to improve our understanding of the climate system."],"url":"http://arxiv.org/abs/2405.01287v1","category":"physics.ao-ph"}
{"created":"2024-05-02 13:46:29","title":"Data Feminism for AI","abstract":"This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.","sentences":["This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research.","In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science.","Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent.","Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive."],"url":"http://arxiv.org/abs/2405.01286v1","category":"cs.CY"}
{"created":"2024-05-02 13:43:22","title":"Behavior Imitation for Manipulator Control and Grasping with Deep Reinforcement Learning","abstract":"The existing Motion Imitation models typically require expert data obtained through MoCap devices, but the vast amount of training data needed is difficult to acquire, necessitating substantial investments of financial resources, manpower, and time. This project combines 3D human pose estimation with reinforcement learning, proposing a novel model that simplifies Motion Imitation into a prediction problem of joint angle values in reinforcement learning. This significantly reduces the reliance on vast amounts of training data, enabling the agent to learn an imitation policy from just a few seconds of video and exhibit strong generalization capabilities. It can quickly apply the learned policy to imitate human arm motions in unfamiliar videos. The model first extracts skeletal motions of human arms from a given video using 3D human pose estimation. These extracted arm motions are then morphologically retargeted onto a robotic manipulator. Subsequently, the retargeted motions are used to generate reference motions. Finally, these reference motions are used to formulate a reinforcement learning problem, enabling the agent to learn a policy for imitating human arm motions. This project excels at imitation tasks and demonstrates robust transferability, accurately imitating human arm motions from other unfamiliar videos. This project provides a lightweight, convenient, efficient, and accurate Motion Imitation model. While simplifying the complex process of Motion Imitation, it achieves notably outstanding performance.","sentences":["The existing Motion Imitation models typically require expert data obtained through MoCap devices, but the vast amount of training data needed is difficult to acquire, necessitating substantial investments of financial resources, manpower, and time.","This project combines 3D human pose estimation with reinforcement learning, proposing a novel model that simplifies Motion Imitation into a prediction problem of joint angle values in reinforcement learning.","This significantly reduces the reliance on vast amounts of training data, enabling the agent to learn an imitation policy from just a few seconds of video and exhibit strong generalization capabilities.","It can quickly apply the learned policy to imitate human arm motions in unfamiliar videos.","The model first extracts skeletal motions of human arms from a given video using 3D human pose estimation.","These extracted arm motions are then morphologically retargeted onto a robotic manipulator.","Subsequently, the retargeted motions are used to generate reference motions.","Finally, these reference motions are used to formulate a reinforcement learning problem, enabling the agent to learn a policy for imitating human arm motions.","This project excels at imitation tasks and demonstrates robust transferability, accurately imitating human arm motions from other unfamiliar videos.","This project provides a lightweight, convenient, efficient, and accurate Motion Imitation model.","While simplifying the complex process of Motion Imitation, it achieves notably outstanding performance."],"url":"http://arxiv.org/abs/2405.01284v1","category":"cs.RO"}
{"created":"2024-05-02 13:40:12","title":"Oort's conjecture and automorphisms of supersingular curves of genus four","abstract":"We show that every component of the locus of smooth supersingular curves of genus $4$ in characteristic $p>2$ has a trivial generic automorphism group. As a result, we prove Oort's conjecture about automorphism groups of supersingular abelian fourfolds for $p>2$. Our main idea consists of estimating dimensions of the loci of smooth supersingular curves that admit an automorphism of prime order by considering possible choices of the corresponding quotient curves. This reasoning also results in a new proof of Oort's conjecture for $g = 3$ and $p>2$, previously proved by Karemaker, Yuboko, and Yu.","sentences":["We show that every component of the locus of smooth supersingular curves of genus $4$ in characteristic $p>2$ has a trivial generic automorphism group.","As a result, we prove Oort's conjecture about automorphism groups of supersingular abelian fourfolds for $p>2$. Our main idea consists of estimating dimensions of the loci of smooth supersingular curves that admit an automorphism of prime order by considering possible choices of the corresponding quotient curves.","This reasoning also results in a new proof of Oort's conjecture for $g = 3$ and $p>2$, previously proved by Karemaker, Yuboko, and Yu."],"url":"http://arxiv.org/abs/2405.01282v1","category":"math.AG"}
{"created":"2024-05-02 13:39:51","title":"Demistifying Inference after Adaptive Experiments","abstract":"Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far. This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data. Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees. The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity. In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity.","sentences":["Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far.","This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data.","Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees.","The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity.","In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity."],"url":"http://arxiv.org/abs/2405.01281v1","category":"stat.ME"}
{"created":"2024-05-02 13:39:28","title":"Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation","abstract":"Non-autoregressive (NAR) language models are known for their low latency in neural machine translation (NMT). However, a performance gap exists between NAR and autoregressive models due to the large decoding space and difficulty in capturing dependency between target words accurately. Compounding this, preparing appropriate training data for NAR models is a non-trivial task, often exacerbating exposure bias. To address these challenges, we apply reinforcement learning (RL) to Levenshtein Transformer, a representative edit-based NAR model, demonstrating that RL with self-generated data can enhance the performance of edit-based NAR models. We explore two RL approaches: stepwise reward maximization and episodic reward maximization. We discuss the respective pros and cons of these two approaches and empirically verify them. Moreover, we experimentally investigate the impact of temperature setting on performance, confirming the importance of proper temperature setting for NAR models' training.","sentences":["Non-autoregressive (NAR) language models are known for their low latency in neural machine translation (NMT).","However, a performance gap exists between NAR and autoregressive models due to the large decoding space and difficulty in capturing dependency between target words accurately.","Compounding this, preparing appropriate training data for NAR models is a non-trivial task, often exacerbating exposure bias.","To address these challenges, we apply reinforcement learning (RL) to Levenshtein Transformer, a representative edit-based NAR model, demonstrating that RL with self-generated data can enhance the performance of edit-based NAR models.","We explore two RL approaches: stepwise reward maximization and episodic reward maximization.","We discuss the respective pros and cons of these two approaches and empirically verify them.","Moreover, we experimentally investigate the impact of temperature setting on performance, confirming the importance of proper temperature setting for NAR models' training."],"url":"http://arxiv.org/abs/2405.01280v1","category":"cs.CL"}
{"created":"2024-05-02 13:38:40","title":"Core QUIC: Enabling Dynamic, Implementation-Agnostic Protocol Extensions","abstract":"While applications quickly evolve, Internet protocols do not follow the same pace. There are two root causes for this. First, extending protocol with cleartext control plane is usually hindered by various network devices such as middleboxes. Second, such extensions usually require support from all participating entities, but often these run different implementations, leading to the chicken-and-egg deployment issue. The recently standardized QUIC protocol paved the way for dealing with the first concern by embedding encryption by design. However, it attracted so much interest that there is now a large heterogeneity in QUIC implementations, hence amplifying the second problem.   To get rid of these deployment issues and to enable inter-operable, implementation-independent innovation at transport layer, we propose a paradigm shift called Core QUIC. While Core QUIC keeps compliant with the standardized QUIC protocol, it enforces implementation architecture such that any Core QUIC-supporting participant can be extended with the same, generic bytecode. To achieve this, Core QUIC defines a standardized representation format of common QUIC structures on which plugins running in a controlled environment can operate to extend the underlying host implementation. We demonstrate the feasibility of our approach by making two implementations Core QUIC-compliant. Then, we show that we can extend both with the same plugin code over several use cases.","sentences":["While applications quickly evolve, Internet protocols do not follow the same pace.","There are two root causes for this.","First, extending protocol with cleartext control plane is usually hindered by various network devices such as middleboxes.","Second, such extensions usually require support from all participating entities, but often these run different implementations, leading to the chicken-and-egg deployment issue.","The recently standardized QUIC protocol paved the way for dealing with the first concern by embedding encryption by design.","However, it attracted so much interest that there is now a large heterogeneity in QUIC implementations, hence amplifying the second problem.   ","To get rid of these deployment issues and to enable inter-operable, implementation-independent innovation at transport layer, we propose a paradigm shift called Core QUIC.","While Core QUIC keeps compliant with the standardized QUIC protocol, it enforces implementation architecture such that any Core QUIC-supporting participant can be extended with the same, generic bytecode.","To achieve this, Core QUIC defines a standardized representation format of common QUIC structures on which plugins running in a controlled environment can operate to extend the underlying host implementation.","We demonstrate the feasibility of our approach by making two implementations Core QUIC-compliant.","Then, we show that we can extend both with the same plugin code over several use cases."],"url":"http://arxiv.org/abs/2405.01279v1","category":"cs.NI"}
{"created":"2024-05-02 13:38:27","title":"Generalized cyclotomic polynomials associated with regular systems of divisors and arbitrary sets of positive integers","abstract":"We introduce and study the generalized cyclotomic polynomials $\\Phi_{A,S,n}(x)$ associated with a regular system $A$ of divisors and an arbitrary set $S$ of positive integers. We show that all of these polynomials have integer coefficients, they can be expressed as the product of certain classical cyclotomic polynomials $\\Phi_d(x)$ with $d\\mid n$, and enjoy many other properties which are similar to the classical and unitary cases. We also point out some related Menon-type identities. One of them seems to be new even for the cyclotomic polynomials $\\Phi_n(x)$.","sentences":["We introduce and study the generalized cyclotomic polynomials $\\Phi_{A,S,n}(x)$ associated with a regular system $A$ of divisors and an arbitrary set $S$ of positive integers.","We show that all of these polynomials have integer coefficients, they can be expressed as the product of certain classical cyclotomic polynomials $\\Phi_d(x)$ with $d\\mid n$, and enjoy many other properties which are similar to the classical and unitary cases.","We also point out some related Menon-type identities.","One of them seems to be new even for the cyclotomic polynomials $\\Phi_n(x)$."],"url":"http://arxiv.org/abs/2405.01278v1","category":"math.NT"}
{"created":"2024-05-02 13:35:15","title":"Quantifying Spatial Domain Explanations in BCI using Earth Mover's Distance","abstract":"Brain-computer interface (BCI) systems facilitate unique communication between humans and computers, benefiting severely disabled individuals. Despite decades of research, BCIs are not fully integrated into clinical and commercial settings. It's crucial to assess and explain BCI performance, offering clear explanations for potential users to avoid frustration when it doesn't work as expected. This work investigates the efficacy of different deep learning and Riemannian geometry-based classification models in the context of motor imagery (MI) based BCI using electroencephalography (EEG). We then propose an optimal transport theory-based approach using earth mover's distance (EMD) to quantify the comparison of the feature relevance map with the domain knowledge of neuroscience. For this, we utilized explainable AI (XAI) techniques for generating feature relevance in the spatial domain to identify important channels for model outcomes. Three state-of-the-art models are implemented - 1) Riemannian geometry-based classifier, 2) EEGNet, and 3) EEG Conformer, and the observed trend in the model's accuracy across different architectures on the dataset correlates with the proposed feature relevance metrics. The models with diverse architectures perform significantly better when trained on channels relevant to motor imagery than data-driven channel selection. This work focuses attention on the necessity for interpretability and incorporating metrics beyond accuracy, underscores the value of combining domain knowledge and quantifying model interpretations with data-driven approaches in creating reliable and robust Brain-Computer Interfaces (BCIs).","sentences":["Brain-computer interface (BCI) systems facilitate unique communication between humans and computers, benefiting severely disabled individuals.","Despite decades of research, BCIs are not fully integrated into clinical and commercial settings.","It's crucial to assess and explain BCI performance, offering clear explanations for potential users to avoid frustration when it doesn't work as expected.","This work investigates the efficacy of different deep learning and Riemannian geometry-based classification models in the context of motor imagery (MI) based BCI using electroencephalography (EEG).","We then propose an optimal transport theory-based approach using earth mover's distance (EMD) to quantify the comparison of the feature relevance map with the domain knowledge of neuroscience.","For this, we utilized explainable AI (XAI) techniques for generating feature relevance in the spatial domain to identify important channels for model outcomes.","Three state-of-the-art models are implemented - 1) Riemannian geometry-based classifier, 2) EEGNet, and 3) EEG Conformer, and the observed trend in the model's accuracy across different architectures on the dataset correlates with the proposed feature relevance metrics.","The models with diverse architectures perform significantly better when trained on channels relevant to motor imagery than data-driven channel selection.","This work focuses attention on the necessity for interpretability and incorporating metrics beyond accuracy, underscores the value of combining domain knowledge and quantifying model interpretations with data-driven approaches in creating reliable and robust Brain-Computer Interfaces (BCIs)."],"url":"http://arxiv.org/abs/2405.01277v1","category":"cs.HC"}
{"created":"2024-05-02 13:31:09","title":"Towards Inclusive Face Recognition Through Synthetic Ethnicity Alteration","abstract":"Numerous studies have shown that existing Face Recognition Systems (FRS), including commercial ones, often exhibit biases toward certain ethnicities due to under-represented data. In this work, we explore ethnicity alteration and skin tone modification using synthetic face image generation methods to increase the diversity of datasets. We conduct a detailed analysis by first constructing a balanced face image dataset representing three ethnicities: Asian, Black, and Indian. We then make use of existing Generative Adversarial Network-based (GAN) image-to-image translation and manifold learning models to alter the ethnicity from one to another. A systematic analysis is further conducted to assess the suitability of such datasets for FRS by studying the realistic skin-tone representation using Individual Typology Angle (ITA). Further, we also analyze the quality characteristics using existing Face image quality assessment (FIQA) approaches. We then provide a holistic FRS performance analysis using four different systems. Our findings pave the way for future research works in (i) developing both specific ethnicity and general (any to any) ethnicity alteration models, (ii) expanding such approaches to create databases with diverse skin tones, (iii) creating datasets representing various ethnicities which further can help in mitigating bias while addressing privacy concerns.","sentences":["Numerous studies have shown that existing Face Recognition Systems (FRS), including commercial ones, often exhibit biases toward certain ethnicities due to under-represented data.","In this work, we explore ethnicity alteration and skin tone modification using synthetic face image generation methods to increase the diversity of datasets.","We conduct a detailed analysis by first constructing a balanced face image dataset representing three ethnicities: Asian, Black, and Indian.","We then make use of existing Generative Adversarial Network-based (GAN) image-to-image translation and manifold learning models to alter the ethnicity from one to another.","A systematic analysis is further conducted to assess the suitability of such datasets for FRS by studying the realistic skin-tone representation using Individual Typology Angle (ITA).","Further, we also analyze the quality characteristics using existing Face image quality assessment (FIQA) approaches.","We then provide a holistic FRS performance analysis using four different systems.","Our findings pave the way for future research works in (i) developing both specific ethnicity and general (any to any) ethnicity alteration models, (ii) expanding such approaches to create databases with diverse skin tones, (iii) creating datasets representing various ethnicities which further can help in mitigating bias while addressing privacy concerns."],"url":"http://arxiv.org/abs/2405.01273v1","category":"cs.CV"}
{"created":"2024-05-02 13:30:59","title":"Learned frequency-domain scattered wavefield solutions using neural operators","abstract":"Solving the wave equation is essential to seismic imaging and inversion. The numerical solution of the Helmholtz equation, fundamental to this process, often encounters significant computational and memory challenges. We propose an innovative frequency-domain scattered wavefield modeling method employing neural operators adaptable to diverse seismic velocities. The source location and frequency information are embedded within the input background wavefield, enhancing the neural operator's ability to process source configurations effectively. In addition, we utilize a single reference frequency, which enables scaling from larger-domain forward modeling to higher-frequency scenarios, thereby improving our method's accuracy and generalization capabilities for larger-domain applications. Several tests on the OpenFWI datasets and realistic velocity models validate the accuracy and efficacy of our method as a surrogate model, demonstrating its potential to address the computational and memory limitations of numerical methods.","sentences":["Solving the wave equation is essential to seismic imaging and inversion.","The numerical solution of the Helmholtz equation, fundamental to this process, often encounters significant computational and memory challenges.","We propose an innovative frequency-domain scattered wavefield modeling method employing neural operators adaptable to diverse seismic velocities.","The source location and frequency information are embedded within the input background wavefield, enhancing the neural operator's ability to process source configurations effectively.","In addition, we utilize a single reference frequency, which enables scaling from larger-domain forward modeling to higher-frequency scenarios, thereby improving our method's accuracy and generalization capabilities for larger-domain applications.","Several tests on the OpenFWI datasets and realistic velocity models validate the accuracy and efficacy of our method as a surrogate model, demonstrating its potential to address the computational and memory limitations of numerical methods."],"url":"http://arxiv.org/abs/2405.01272v1","category":"physics.geo-ph"}
{"created":"2024-05-02 13:13:52","title":"MFTraj: Map-Free, Behavior-Driven Trajectory Prediction for Autonomous Driving","abstract":"This paper introduces a trajectory prediction model tailored for autonomous driving, focusing on capturing complex interactions in dynamic traffic scenarios without reliance on high-definition maps. The model, termed MFTraj, harnesses historical trajectory data combined with a novel dynamic geometric graph-based behavior-aware module. At its core, an adaptive structure-aware interactive graph convolutional network captures both positional and behavioral features of road users, preserving spatial-temporal intricacies. Enhanced by a linear attention mechanism, the model achieves computational efficiency and reduced parameter overhead. Evaluations on the Argoverse, NGSIM, HighD, and MoCAD datasets underscore MFTraj's robustness and adaptability, outperforming numerous benchmarks even in data-challenged scenarios without the need for additional information such as HD maps or vectorized maps. Importantly, it maintains competitive performance even in scenarios with substantial missing data, on par with most existing state-of-the-art models. The results and methodology suggest a significant advancement in autonomous driving trajectory prediction, paving the way for safer and more efficient autonomous systems.","sentences":["This paper introduces a trajectory prediction model tailored for autonomous driving, focusing on capturing complex interactions in dynamic traffic scenarios without reliance on high-definition maps.","The model, termed MFTraj, harnesses historical trajectory data combined with a novel dynamic geometric graph-based behavior-aware module.","At its core, an adaptive structure-aware interactive graph convolutional network captures both positional and behavioral features of road users, preserving spatial-temporal intricacies.","Enhanced by a linear attention mechanism, the model achieves computational efficiency and reduced parameter overhead.","Evaluations on the Argoverse, NGSIM, HighD, and MoCAD datasets underscore MFTraj's robustness and adaptability, outperforming numerous benchmarks even in data-challenged scenarios without the need for additional information such as HD maps or vectorized maps.","Importantly, it maintains competitive performance even in scenarios with substantial missing data, on par with most existing state-of-the-art models.","The results and methodology suggest a significant advancement in autonomous driving trajectory prediction, paving the way for safer and more efficient autonomous systems."],"url":"http://arxiv.org/abs/2405.01266v1","category":"cs.RO"}
{"created":"2024-05-02 13:09:01","title":"Unified inverse correspondence for LE-logics","abstract":"We generalize Kracht's theory of internal describability from classical modal logic to the family of all logics canonically associated with varieties of normal lattice expansions (LE algebras). We work in the purely algebraic setting of perfect LEs; the formulas playing the role of Kracht's formulas in this generalized setting pertain to a first order language whose atoms are special inequalities between terms of perfect algebras. Via duality, formulas in this language can be equivalently translated into first order conditions in the frame correspondence languages of several types of relational semantics for LE-logics.","sentences":["We generalize Kracht's theory of internal describability from classical modal logic to the family of all logics canonically associated with varieties of normal lattice expansions (LE algebras).","We work in the purely algebraic setting of perfect LEs; the formulas playing the role of Kracht's formulas in this generalized setting pertain to a first order language whose atoms are special inequalities between terms of perfect algebras.","Via duality, formulas in this language can be equivalently translated into first order conditions in the frame correspondence languages of several types of relational semantics for LE-logics."],"url":"http://arxiv.org/abs/2405.01262v1","category":"math.LO"}
{"created":"2024-05-02 13:06:24","title":"Identification of Entailment and Contradiction Relations between Natural Language Sentences: A Neurosymbolic Approach","abstract":"Natural language inference (NLI), also known as Recognizing Textual Entailment (RTE), is an important aspect of natural language understanding. Most research now uses machine learning and deep learning to perform this task on specific datasets, meaning their solution is not explainable nor explicit. To address the need for an explainable approach to RTE, we propose a novel pipeline that is based on translating text into an Abstract Meaning Representation (AMR) graph. For this we use a pre-trained AMR parser. We then translate the AMR graph into propositional logic and use a SAT solver for automated reasoning. In text, often commonsense suggests that an entailment (or contradiction) relationship holds between a premise and a claim, but because different wordings are used, this is not identified from their logical representations. To address this, we introduce relaxation methods to allow replacement or forgetting of some propositions. Our experimental results show this pipeline performs well on four RTE datasets.","sentences":["Natural language inference (NLI), also known as Recognizing Textual Entailment (RTE), is an important aspect of natural language understanding.","Most research now uses machine learning and deep learning to perform this task on specific datasets, meaning their solution is not explainable nor explicit.","To address the need for an explainable approach to RTE, we propose a novel pipeline that is based on translating text into an Abstract Meaning Representation (AMR) graph.","For this we use a pre-trained AMR parser.","We then translate the AMR graph into propositional logic and use a SAT solver for automated reasoning.","In text, often commonsense suggests that an entailment (or contradiction) relationship holds between a premise and a claim, but because different wordings are used, this is not identified from their logical representations.","To address this, we introduce relaxation methods to allow replacement or forgetting of some propositions.","Our experimental results show this pipeline performs well on four RTE datasets."],"url":"http://arxiv.org/abs/2405.01259v1","category":"cs.AI"}
{"created":"2024-05-02 13:00:29","title":"Automatic generation of helicity amplitudes in Feynman-Diagram gauge","abstract":"We develop a method to calculate helicity amplitudes of an arbitrary tree-level process in Feynman-Diagram (FD) gauge for an arbitrary gauge model with MadGraph5_aMC@NLO. We start from the 't Hooft-Feynman gauge Lagrangian in FeynRules and generate scattering amplitudes by identifying the Goldstone boson as the $5^{\\rm th}$ component of each weak boson. All the vertices of the 5-component weak bosons are then created automatically by assembling the relevant weak boson and Goldstone boson vertices in the Feynman gauge. The 5-component weak boson vertices are then connected by the $5\\times5$ matrix propagator in the FD gauge. As a demonstration of the method we calculate the cross section for the process $\\mu^-\\mu^+\\to\\nu_\\mu\\bar{\\nu}_\\mu t\\bar{t}H$ with complex top Yukawa coupling, which can be obtained by adding a gauge invariant dimension-6 operator to the Standard Model (SM) Lagrangian. The FD gauge and the unitary (U) gauge amplitudes give exactly the same cross section, and subtle gauge theory cancellation among diagrams in the U gauge at high energies is absent in the FD gauge, as has been observed for various SM processes. In addition, we find that the total cross sections at high energies are dominated by a single, or a set of non-vanishing Feynman amplitudes with the higher dimensional vertices in the FD gauge.","sentences":["We develop a method to calculate helicity amplitudes of an arbitrary tree-level process in Feynman-Diagram (FD) gauge for an arbitrary gauge model with MadGraph5_aMC@NLO.","We start from the 't Hooft-Feynman gauge Lagrangian in FeynRules and generate scattering amplitudes by identifying the Goldstone boson as the $5^{\\rm th}$ component of each weak boson.","All the vertices of the 5-component weak bosons are then created automatically by assembling the relevant weak boson and Goldstone boson vertices in the Feynman gauge.","The 5-component weak boson vertices are then connected by the $5\\times5$ matrix propagator in the FD gauge.","As a demonstration of the method we calculate the cross section for the process $\\mu^-\\mu^+\\to\\nu_\\mu\\bar{\\nu}_\\mu t\\bar{t}H$ with complex top Yukawa coupling, which can be obtained by adding a gauge invariant dimension-6 operator to the Standard Model (SM) Lagrangian.","The FD gauge and the unitary (U) gauge amplitudes give exactly the same cross section, and subtle gauge theory cancellation among diagrams in the U gauge at high energies is absent in the FD gauge, as has been observed for various SM processes.","In addition, we find that the total cross sections at high energies are dominated by a single, or a set of non-vanishing Feynman amplitudes with the higher dimensional vertices in the FD gauge."],"url":"http://arxiv.org/abs/2405.01256v1","category":"hep-ph"}
{"created":"2024-05-02 13:00:28","title":"Note on holographic torus stress tensor correlators in $AdS_3$ gravity","abstract":"In the AdS$_3$/CFT$_2$ framework, the Euclidean BTZ black hole corresponds to the dominant high-temperature phase of its dual field theory. We initially employ perturbative methods to solve the Einstein equations as boundary value problems, providing correlators for the energy-momentum tensor operator at low points. Utilizing operator equations established in our previous work, we further compute arbitrary high-point correlators for the energy-momentum tensor operator in the high-temperature phase and recursive relations for these high-point functions. Concurrently, we employ the Chern-Simons formalism to derive consistent results. Further, using the cut-off AdS/$T\\bar{T}$-deformed CFT duality, we calculate the energy-momentum tensor correlators, contributing to the comprehensive understanding of the system's dynamics. Finally, stress tensor correlators enable us to ascertain the corresponding KdV operator correlators at low-temperature.","sentences":["In the AdS$_3$/CFT$_2$ framework, the Euclidean BTZ black hole corresponds to the dominant high-temperature phase of its dual field theory.","We initially employ perturbative methods to solve the Einstein equations as boundary value problems, providing correlators for the energy-momentum tensor operator at low points.","Utilizing operator equations established in our previous work, we further compute arbitrary high-point correlators for the energy-momentum tensor operator in the high-temperature phase and recursive relations for these high-point functions.","Concurrently, we employ the Chern-Simons formalism to derive consistent results.","Further, using the cut-off AdS/$T\\bar{T}$-deformed CFT duality, we calculate the energy-momentum tensor correlators, contributing to the comprehensive understanding of the system's dynamics.","Finally, stress tensor correlators enable us to ascertain the corresponding KdV operator correlators at low-temperature."],"url":"http://arxiv.org/abs/2405.01255v1","category":"hep-th"}
{"created":"2024-05-02 13:00:07","title":"Optimal Lagrange Interpolation Projectors and Legendre Polynomials","abstract":"Let $K$ be a convex body in ${\\mathbb R}^n$, and let $\\Pi_1({\\mathbb R}^n)$ be the space of polynomials in $n$ variables of degree at most $1$. Given an $(n+1)$-element set $Y\\subset K$ in general position, we let $P_Y$ denote the Lagrange interpolation projector $P_Y: C(K)\\to \\Pi_1({\\mathbb R}^n)$ with nodes in $Y$. In this paper, we study upper and lower bounds for the norm of the optimal Lagrange interpolation projector, i.e., the projector with minimal operator norm where the minimum is taken over all $(n+1)$-element sets of interpolation nodes in $K$. We denote this minimal norm by $\\theta_n(K)$. Our main result, Theorem 5.2, provides an explicit lower bound for the constant $\\theta_n(K)$ for an arbitrary convex body $K\\subset{\\mathbb R}^n$ and an arbitrary $n\\ge 1$. We prove that $\\theta_n(K)\\ge \\chi_n^{-1}\\left({{\\rm vol}(K)}/{{\\rm simp}(K)}\\right)$ where $\\chi_n$ is the Legendre polynomial of degree $n$ and ${\\rm simp}(K)$ is the maximum volume of a simplex contained in $K$. The proof of this result relies on a geometric characterization of the Legendre polynomials in terms of the volumes of certain convex polyhedra. More specifically, we show that for every $\\gamma\\ge 1$ the volume of the set $\\left\\{x=(x_1,...,x_n)\\in{\\mathbb R}^n : \\sum |x_j| +\\left|1- \\sum x_j\\right|\\le\\gamma\\right\\}$ is equal to ${\\chi_n(\\gamma)}/{n!}$. If $K$ is an $n$-dimensional ball, this approach leads us to the equivalence $\\theta_n(K) \\asymp\\sqrt{n}$ which is complemented by the exact formula for $\\theta_n(K)$. If $K$ is an $n$-dimensional cube, we obtain explicit efficient formulae for upper and lower bounds of the constant $\\theta_n(K)$; moreover, for small $n$, these estimates enable us to compute the exact values of this constant.","sentences":["Let $K$ be a convex body in ${\\mathbb R}^n$, and let $\\Pi_1({\\mathbb R}^n)$ be the space of polynomials in $n$ variables of degree at most $1$. Given an $(n+1)$-element set $Y\\subset K$ in general position, we let $P_Y$ denote the Lagrange interpolation projector $P_Y: C(K)\\to \\Pi_1({\\mathbb R}^n)$ with nodes in $Y$. In this paper, we study upper and lower bounds for the norm of the optimal Lagrange interpolation projector, i.e., the projector with minimal operator norm where the minimum is taken over all $(n+1)$-element sets of interpolation nodes in $K$. We denote this minimal norm by $\\theta_n(K)$. Our main result, Theorem 5.2, provides an explicit lower bound for the constant $\\theta_n(K)$ for an arbitrary convex body $K\\subset{\\mathbb R}^n$ and an arbitrary $n\\ge 1$.","We prove that $\\theta_n(K)\\ge \\chi_n^{-1}\\left({{\\rm vol}(K)}/{{\\rm simp}(K)}\\right)$ where $\\chi_n$ is the Legendre polynomial of degree $n$ and ${\\rm simp}(K)$ is the maximum volume of a simplex contained in $K$. The proof of this result relies on a geometric characterization of the Legendre polynomials in terms of the volumes of certain convex polyhedra.","More specifically, we show that for every $\\gamma\\ge 1$ the volume of the set $\\left\\{x=(x_1,...,x_n)\\in{\\mathbb R}^n : \\sum |x_j|","+\\left|1- \\sum x_j\\right|\\le\\gamma\\right\\}$ is equal to ${\\chi_n(\\gamma)}/{n!}$. If $K$ is an $n$-dimensional ball, this approach leads us to the equivalence $\\theta_n(K) \\asymp\\sqrt{n}$ which is complemented by the exact formula for $\\theta_n(K)$. If $K$ is an $n$-dimensional cube, we obtain explicit efficient formulae for upper and lower bounds of the constant $\\theta_n(K)$; moreover, for small $n$, these estimates enable us to compute the exact values of this constant."],"url":"http://arxiv.org/abs/2405.01254v1","category":"math.MG"}
{"created":"2024-05-02 12:58:59","title":"2d Ising Critical Couplings from Quantum Gravity","abstract":"Using an exact holographic duality formula between the inhomogeneous 2d Ising model and 3d quantum gravity, we provide a formula for \"real\" zeroes of the 2d Ising partition function on finite graphs in terms of the geometry of a 2d triangulation embedded in the three-dimensional Euclidean space. The complex phase of those zeroes is given by the dihedral angles of the triangulation, which reflect its extrinsic curvature within the ambient 3d space, while the modulus is given by the angles within the 2d triangles, thus encoding the intrinsic geometry of the triangulation. Our formula can not cover the whole set of Ising zeroes, but we conjecture that a suitable complexification of these \"real\" zeroes would provide a more thorough formula. Nevertheless, in the thermodynamic limit, in the case of flat planar 2d triangulations, our Ising zeroes formula gives the critical couplings for isoradial graphs, confirming its generality. This approach shows an intricate, but precise, new relation between statistical mechanics and quantum geometry.","sentences":["Using an exact holographic duality formula between the inhomogeneous 2d Ising model and 3d quantum gravity, we provide a formula for \"real\" zeroes of the 2d Ising partition function on finite graphs in terms of the geometry of a 2d triangulation embedded in the three-dimensional Euclidean space.","The complex phase of those zeroes is given by the dihedral angles of the triangulation, which reflect its extrinsic curvature within the ambient 3d space, while the modulus is given by the angles within the 2d triangles, thus encoding the intrinsic geometry of the triangulation.","Our formula can not cover the whole set of Ising zeroes, but we conjecture that a suitable complexification of these \"real\" zeroes would provide a more thorough formula.","Nevertheless, in the thermodynamic limit, in the case of flat planar 2d triangulations, our Ising zeroes formula gives the critical couplings for isoradial graphs, confirming its generality.","This approach shows an intricate, but precise, new relation between statistical mechanics and quantum geometry."],"url":"http://arxiv.org/abs/2405.01253v1","category":"hep-th"}
{"created":"2024-05-02 12:51:55","title":"DiffusionPipe: Training Large Diffusion Models with Efficient Pipelines","abstract":"Diffusion models have emerged as dominant performers for image generation. To support training large diffusion models, this paper studies pipeline parallel training of diffusion models and proposes DiffusionPipe, a synchronous pipeline training system that advocates innovative pipeline bubble filling technique, catering to structural characteristics of diffusion models. State-of-the-art diffusion models typically include trainable (the backbone) and non-trainable (e.g., frozen input encoders) parts. We first unify optimal stage partitioning and pipeline scheduling of single and multiple backbones in representative diffusion models with a dynamic programming approach. We then propose to fill the computation of non-trainable model parts into idle periods of the pipeline training of the backbones by an efficient greedy algorithm, thus achieving high training throughput. Extensive experiments show that DiffusionPipe can achieve up to 1.41x speedup over pipeline parallel methods and 1.28x speedup over data parallel training on popular diffusion models.","sentences":["Diffusion models have emerged as dominant performers for image generation.","To support training large diffusion models, this paper studies pipeline parallel training of diffusion models and proposes DiffusionPipe, a synchronous pipeline training system that advocates innovative pipeline bubble filling technique, catering to structural characteristics of diffusion models.","State-of-the-art diffusion models typically include trainable (the backbone) and non-trainable (e.g., frozen input encoders) parts.","We first unify optimal stage partitioning and pipeline scheduling of single and multiple backbones in representative diffusion models with a dynamic programming approach.","We then propose to fill the computation of non-trainable model parts into idle periods of the pipeline training of the backbones by an efficient greedy algorithm, thus achieving high training throughput.","Extensive experiments show that DiffusionPipe can achieve up to 1.41x speedup over pipeline parallel methods and 1.28x speedup over data parallel training on popular diffusion models."],"url":"http://arxiv.org/abs/2405.01248v1","category":"cs.DC"}
{"created":"2024-05-02 12:49:47","title":"On well/ill-posedness for the generalized surface quasi-geostrophic equations in H\u00f6lder spaces","abstract":"We establish the well/ill-posedness theories for the inviscid $\\alpha$-surface quasi-geostrophic ($\\alpha$-SQG) equations in H\\\"older spaces, where $\\alpha = 0$ and $\\alpha = 1$ correspond to the two-dimensional Euler equation in the vorticity formulation and SQG equation of geophysical significance, respectively. We first prove the local-in-time well-posedness of $\\alpha$-SQG equations in $C([0,T);C^{0,\\beta}(\\mathbb{R}^2))$ with $\\beta \\in (\\alpha,1)$ for some $T>0$. We then analyze the strong ill-posedness in $C^{0,\\alpha}(\\mathbb{R}^2)$ constructing smooth solutions to the $\\alpha$-SQG equations that exhibit $C^{0,\\alpha}$--norm growth in a short time. In particular, we develop the nonexistence theory for $\\alpha$-SQG equations in $C^{0,\\alpha}(\\mathbb{R}^2)$.","sentences":["We establish the well/ill-posedness theories for the inviscid $\\alpha$-surface quasi-geostrophic ($\\alpha$-SQG) equations in H\\\"older spaces, where $\\alpha = 0$ and $\\alpha = 1$ correspond to the two-dimensional Euler equation in the vorticity formulation and SQG equation of geophysical significance, respectively.","We first prove the local-in-time well-posedness of $\\alpha$-SQG equations in $C([0,T);C^{0,\\beta}(\\mathbb{R}^2))$ with $\\beta \\in (\\alpha,1)$ for some $T>0$. We then analyze the strong ill-posedness in $C^{0,\\alpha}(\\mathbb{R}^2)$ constructing smooth solutions to the $\\alpha$-SQG equations that exhibit $C^{0,\\alpha}$--norm growth in a short time.","In particular, we develop the nonexistence theory for $\\alpha$-SQG equations in $C^{0,\\alpha}(\\mathbb{R}^2)$."],"url":"http://arxiv.org/abs/2405.01245v1","category":"math.AP"}
{"created":"2024-05-02 12:45:48","title":"TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms","abstract":"We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.","sentences":["We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms.","Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems.","To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available.","Then, users fine-tune with a small amount of bone conduction data.","TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times.","We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB."],"url":"http://arxiv.org/abs/2405.01242v1","category":"cs.SD"}
{"created":"2024-05-02 12:41:56","title":"Port-Hamiltonian systems with energy and power ports","abstract":"We extend the port-Hamiltonian framework defined with respect to a Lagrangian submanifold and a Dirac structure by augmenting the Lagrangian submanifold with the space of external variables. The new pair of conjugated variables is called energy port. We show that in the most general case, the extension describes constrained Hamiltonian systems whose Hamiltonian function depends on inputs.","sentences":["We extend the port-Hamiltonian framework defined with respect to a Lagrangian submanifold and a Dirac structure by augmenting the Lagrangian submanifold with the space of external variables.","The new pair of conjugated variables is called energy port.","We show that in the most general case, the extension describes constrained Hamiltonian systems whose Hamiltonian function depends on inputs."],"url":"http://arxiv.org/abs/2405.01241v1","category":"math.OC"}
{"created":"2024-05-02 12:39:11","title":"Machine-learned tuning of artificial Kitaev chains from tunneling-spectroscopy measurements","abstract":"We demonstrate reliable machine-learned tuning of quantum-dot-based artificial Kitaev chains to Majorana sweet spots, using the covariance matrix adaptation algorithm. We show that a loss function based on local tunnelling-spectroscopy features of a chain with two additional sensor dots added at its ends provides a reliable metric to navigate parameter space and find points where crossed Andreev reflection and elastic cotunneling between neighbouring sites balance in such a way to yield near-zero-energy modes with very high Majorana quality. We simulate tuning of two- and three-site Kitaev chains, where the loss function is found from calculating the low-energy spectrum of a model Hamiltonian that includes Coulomb interactions and finite Zeeman splitting. In both cases, the algorithm consistently converges towards high-quality sweet spots. Since tunnelling spectroscopy provides one global metric for tuning all on-site potentials simultaneously, this presents a promising way towards tuning longer Kitaev chains, which are required for achieving topological protection of the Majorana modes.","sentences":["We demonstrate reliable machine-learned tuning of quantum-dot-based artificial Kitaev chains to Majorana sweet spots, using the covariance matrix adaptation algorithm.","We show that a loss function based on local tunnelling-spectroscopy features of a chain with two additional sensor dots added at its ends provides a reliable metric to navigate parameter space and find points where crossed Andreev reflection and elastic cotunneling between neighbouring sites balance in such a way to yield near-zero-energy modes with very high Majorana quality.","We simulate tuning of two- and three-site Kitaev chains, where the loss function is found from calculating the low-energy spectrum of a model Hamiltonian that includes Coulomb interactions and finite Zeeman splitting.","In both cases, the algorithm consistently converges towards high-quality sweet spots.","Since tunnelling spectroscopy provides one global metric for tuning all on-site potentials simultaneously, this presents a promising way towards tuning longer Kitaev chains, which are required for achieving topological protection of the Majorana modes."],"url":"http://arxiv.org/abs/2405.01240v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-02 12:27:50","title":"Numerical experiments on stationary, oscillating, and damped spherical galaxy models","abstract":"We numerically analyse solutions of the spherically symmetric gravitational Vlasov-Poisson system close to compactly supported stable steady states. We observe either partially undamped oscillations or macroscopically damped solutions. We investigate for many steady states to which of these behaviours they correspond. A linear relation between the exponents of polytropic steady states and the qualitative behaviour close to them is identified. Undamped oscillations are also observed around not too concentrated King models and around all shells with a sufficiently large vacuum region. We analyse all solutions both at the non-linear and linearised level and find that the qualitative behaviours are identical at both. To relate the observed phenomena to theoretical results, we further include a comprehensive numerical study of the radial particle periods in the equilibria.","sentences":["We numerically analyse solutions of the spherically symmetric gravitational Vlasov-Poisson system close to compactly supported stable steady states.","We observe either partially undamped oscillations or macroscopically damped solutions.","We investigate for many steady states to which of these behaviours they correspond.","A linear relation between the exponents of polytropic steady states and the qualitative behaviour close to them is identified.","Undamped oscillations are also observed around not too concentrated King models and around all shells with a sufficiently large vacuum region.","We analyse all solutions both at the non-linear and linearised level and find that the qualitative behaviours are identical at both.","To relate the observed phenomena to theoretical results, we further include a comprehensive numerical study of the radial particle periods in the equilibria."],"url":"http://arxiv.org/abs/2405.01235v1","category":"gr-qc"}
{"created":"2024-05-02 12:26:04","title":"Matrix invertible extensions over commutative rings. Part III: Hermite rings","abstract":"We reobtain and often refine prior criteria due to Kaplansky, McGovern, Roitman, Shchedryk, Wiegand, and Zabavsky--Bilavska and obtain new criteria for a Hermite ring to be an \\textsl{EDR}. We mention three criteria: (1) a Hermite ring $R$ is an \\textsl{EDR} iff for all pairs $(a,c)\\in R^2$, the product homomorphism $U(R/Rac)\\times U\\bigl(R/Rc(1-a)\\bigr)\\to U(R/Rc)$ between groups of units is surjective; (2) a reduced Hermite ring is an \\textsl{EDR} iff it is a pre-Schreier ring and for each $a\\in R$, every zero determinant unimodular $2\\times 2$ matrix with entries in $R/Ra$ lifts to a zero determinant matrix with entries in $R$; (3) a B\\'{e}zout domain $R$ is an \\textsl{EDD} iff for all triples $(a,b,c)\\in R^3$ there exists a unimodular pair $(e,f)\\in R^2$ such that $(a,e)$ and $(be+af,1-a-bc)$ are unimodular pairs. We use these criteria to show that each B\\'{e}zout ring $R$ that is an $(SU)_2$ ring (as introduced by Lorenzini) such that for each nonzero $a\\in R$ there exists no nontrivial self-dual projective $R/Ra$-module of rank $1$ generated by $2$ elements (e.g., all its elements are squares), is an \\textsl{EDR}.","sentences":["We reobtain and often refine prior criteria due to Kaplansky, McGovern, Roitman, Shchedryk, Wiegand, and Zabavsky--Bilavska and obtain new criteria for a Hermite ring to be an \\textsl{EDR}.","We mention three criteria: (1) a Hermite ring $R$ is an \\textsl{EDR} iff for all pairs $(a,c)\\in R^2$, the product homomorphism $U(R/Rac)\\times U\\bigl(R/Rc(1-a)\\bigr)\\to U(R/Rc)$ between groups of units is surjective; (2) a reduced Hermite ring is an \\textsl{EDR} iff it is a pre-Schreier ring and for each $a\\in R$, every zero determinant unimodular $2\\times 2$ matrix with entries in $R/Ra$ lifts to a zero determinant matrix with entries in $R$; (3) a B\\'{e}zout domain $R$ is an \\textsl{EDD} iff for all triples $(a,b,c)\\in R^3$ there exists a unimodular pair $(e,f)\\in R^2$ such that $(a,e)$ and $(be+af,1-a-bc)$ are unimodular pairs.","We use these criteria to show that each B\\'{e}zout ring $R$ that is an $(SU)_2$ ring (as introduced by Lorenzini) such that for each nonzero $a\\in R$ there exists no nontrivial self-dual projective $R/Ra$-module of rank $1$ generated by $2$ elements (e.g., all its elements are squares), is an \\textsl{EDR}."],"url":"http://arxiv.org/abs/2405.01234v1","category":"math.AC"}
{"created":"2024-05-02 12:23:44","title":"Kinetic Theories for Metropolis Monte Carlo Methods","abstract":"We consider generalizations of the classical inverse problem to Bayesien type estimators, where the result is not one optimal parameter but an optimal probability distribution in parameter space. The practical computational tool to compute these distributions is the Metropolis Monte Carlo algorithm. We derive kinetic theories for the Metropolis Monte Carlo method in different scaling regimes. The derived equations yield a different point of view on the classical algorithm. It further inspired modifications to exploit the difference scalings shown on an simulation example of the Lorenz system.","sentences":["We consider generalizations of the classical inverse problem to Bayesien type estimators, where the result is not one optimal parameter but an optimal probability distribution in parameter space.","The practical computational tool to compute these distributions is the Metropolis Monte Carlo algorithm.","We derive kinetic theories for the Metropolis Monte Carlo method in different scaling regimes.","The derived equations yield a different point of view on the classical algorithm.","It further inspired modifications to exploit the difference scalings shown on an simulation example of the Lorenz system."],"url":"http://arxiv.org/abs/2405.01232v1","category":"math.OC"}
{"created":"2024-05-02 12:18:14","title":"Boosting Jailbreak Attack with Momentum","abstract":"Large Language Models (LLMs) have achieved remarkable success across diverse tasks, yet they remain vulnerable to adversarial attacks, notably the well-documented \\textit{jailbreak} attack. Recently, the Greedy Coordinate Gradient (GCG) attack has demonstrated efficacy in exploiting this vulnerability by optimizing adversarial prompts through a combination of gradient heuristics and greedy search. However, the efficiency of this attack has become a bottleneck in the attacking process. To mitigate this limitation, in this paper we rethink the generation of adversarial prompts through an optimization lens, aiming to stabilize the optimization process and harness more heuristic insights from previous iterations. Specifically, we introduce the \\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack, which incorporates a momentum term into the gradient heuristic. Experimental results showcase the notable enhancement achieved by MAP in gradient-based attacks on aligned language models. Our code is available at https://github.com/weizeming/momentum-attack-llm.","sentences":["Large Language Models (LLMs) have achieved remarkable success across diverse tasks, yet they remain vulnerable to adversarial attacks, notably the well-documented \\textit{jailbreak} attack.","Recently, the Greedy Coordinate Gradient (GCG) attack has demonstrated efficacy in exploiting this vulnerability by optimizing adversarial prompts through a combination of gradient heuristics and greedy search.","However, the efficiency of this attack has become a bottleneck in the attacking process.","To mitigate this limitation, in this paper we rethink the generation of adversarial prompts through an optimization lens, aiming to stabilize the optimization process and harness more heuristic insights from previous iterations.","Specifically, we introduce the \\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack, which incorporates a momentum term into the gradient heuristic.","Experimental results showcase the notable enhancement achieved by MAP in gradient-based attacks on aligned language models.","Our code is available at https://github.com/weizeming/momentum-attack-llm."],"url":"http://arxiv.org/abs/2405.01229v1","category":"cs.LG"}
{"created":"2024-05-02 12:13:00","title":"RaffeSDG: Random Frequency Filtering enabled Single-source Domain Generalization for Medical Image Segmentation","abstract":"Deep learning models often encounter challenges in making accurate inferences when there are domain shifts between the source and target data. This issue is particularly pronounced in clinical settings due to the scarcity of annotated data resulting from the professional and private nature of medical data. Despite the existence of decent solutions, many of them are hindered in clinical settings due to limitations in data collection and computational complexity. To tackle domain shifts in data-scarce medical scenarios, we propose a Random frequency filtering enabled Single-source Domain Generalization algorithm (RaffeSDG), which promises robust out-of-domain inference with segmentation models trained on a single-source domain. A filter-based data augmentation strategy is first proposed to promote domain variability within a single-source domain by introducing variations in frequency space and blending homologous samples. Then Gaussian filter-based structural saliency is also leveraged to learn robust representations across augmented samples, further facilitating the training of generalizable segmentation models. To validate the effectiveness of RaffeSDG, we conducted extensive experiments involving out-of-domain inference on segmentation tasks for three human tissues imaged by four diverse modalities. Through thorough investigations and comparisons, compelling evidence was observed in these experiments, demonstrating the potential and generalizability of RaffeSDG. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.","sentences":["Deep learning models often encounter challenges in making accurate inferences when there are domain shifts between the source and target data.","This issue is particularly pronounced in clinical settings due to the scarcity of annotated data resulting from the professional and private nature of medical data.","Despite the existence of decent solutions, many of them are hindered in clinical settings due to limitations in data collection and computational complexity.","To tackle domain shifts in data-scarce medical scenarios, we propose a Random frequency filtering enabled Single-source Domain Generalization algorithm (RaffeSDG), which promises robust out-of-domain inference with segmentation models trained on a single-source domain.","A filter-based data augmentation strategy is first proposed to promote domain variability within a single-source domain by introducing variations in frequency space and blending homologous samples.","Then Gaussian filter-based structural saliency is also leveraged to learn robust representations across augmented samples, further facilitating the training of generalizable segmentation models.","To validate the effectiveness of RaffeSDG, we conducted extensive experiments involving out-of-domain inference on segmentation tasks for three human tissues imaged by four diverse modalities.","Through thorough investigations and comparisons, compelling evidence was observed in these experiments, demonstrating the potential and generalizability of RaffeSDG.","The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation."],"url":"http://arxiv.org/abs/2405.01228v1","category":"cs.CV"}
{"created":"2024-05-02 12:12:32","title":"Revisiting the fermion-field nontopological solitons","abstract":"Nontopological fermionic solitons exist across a diverse range of particle physics models and have rich cosmological implications. This study establishes a general framework for calculating fermionic soliton profiles under arbitrary scalar potentials, utilizing relativistic mean field theory to accurately depict the interaction between the fermion condensate and the background scalar field. Within this framework, the conventional fermion bound states are revealed as a subset of fermionic solitons. In addition, we demonstrate how the analytical formulae in previous studies are derived as special cases of our algorithm, discussing the validity of such approximations. Furthermore, we explore the phenomenology of fermionic solitons, highlighting new formation mechanisms and evolution paths, and reconsidering the possibility of collapse into primordial black holes.","sentences":["Nontopological fermionic solitons exist across a diverse range of particle physics models and have rich cosmological implications.","This study establishes a general framework for calculating fermionic soliton profiles under arbitrary scalar potentials, utilizing relativistic mean field theory to accurately depict the interaction between the fermion condensate and the background scalar field.","Within this framework, the conventional fermion bound states are revealed as a subset of fermionic solitons.","In addition, we demonstrate how the analytical formulae in previous studies are derived as special cases of our algorithm, discussing the validity of such approximations.","Furthermore, we explore the phenomenology of fermionic solitons, highlighting new formation mechanisms and evolution paths, and reconsidering the possibility of collapse into primordial black holes."],"url":"http://arxiv.org/abs/2405.01227v1","category":"hep-ph"}
{"created":"2024-05-02 12:08:46","title":"Bumpy Superluminous Supernovae Powered by Magnetar-star Binary Engine","abstract":"Wolf-Rayet stars in close binary systems can be tidally spun up by their companions, potentially leaving behind fast-spinning highly-magnetized neutron stars, known as ``magnetars\", after core collapse. These newborn magnetars can transfer rotational energy into heating and accelerating the ejecta, producing hydrogen-poor superluminous supernovae (SLSNe). In this {\\em{Letter}}, we propose that the magnetar wind of the newborn magnetar could significantly evaporate its companion star, typically a main-sequence or helium star, if the binary system is not disrupted by the SN kick. The subsequent heating and acceleration of the evaporated star material along with the SN ejecta by the magnetar wind can produce a post-peak bump in the SLSN lightcurve. Our model can reproduce the primary peaks and post-peak bumps of four example observed multiband SLSN lightcurves, revealing that the mass of the evaporated material could be $\\sim0.4-0.6\\,M_\\odot$ if the material is hydrogen-rich. We suggest that the magnetar could induce strongly enhanced evaporation from its companion star near the pericenter if the orbit of the post-SN binary is highly eccentric, ultimately generating multiple post-peak bumps in the SLSN lightcurves. This ``magnetar-star binary engine\" model offers a possible explanation for the evolution of polarization, along with the origin and velocity broadening of late-time hydrogen or helium broad spectral features observed in some bumpy SLSNe. The diversity in the lightcurves and spectra of SLSNe may be attributed to the wide variety of companion stars and post-SN binary systems.","sentences":["Wolf-Rayet stars in close binary systems can be tidally spun up by their companions, potentially leaving behind fast-spinning highly-magnetized neutron stars, known as ``magnetars\", after core collapse.","These newborn magnetars can transfer rotational energy into heating and accelerating the ejecta, producing hydrogen-poor superluminous supernovae (SLSNe).","In this {\\em{Letter}}, we propose that the magnetar wind of the newborn magnetar could significantly evaporate its companion star, typically a main-sequence or helium star, if the binary system is not disrupted by the SN kick.","The subsequent heating and acceleration of the evaporated star material along with the SN ejecta by the magnetar wind can produce a post-peak bump in the SLSN lightcurve.","Our model can reproduce the primary peaks and post-peak bumps of four example observed multiband SLSN lightcurves, revealing that the mass of the evaporated material could be $\\sim0.4-0.6\\,M_\\odot$ if the material is hydrogen-rich.","We suggest that the magnetar could induce strongly enhanced evaporation from its companion star near the pericenter if the orbit of the post-SN binary is highly eccentric, ultimately generating multiple post-peak bumps in the SLSN lightcurves.","This ``magnetar-star binary engine\" model offers a possible explanation for the evolution of polarization, along with the origin and velocity broadening of late-time hydrogen or helium broad spectral features observed in some bumpy SLSNe.","The diversity in the lightcurves and spectra of SLSNe may be attributed to the wide variety of companion stars and post-SN binary systems."],"url":"http://arxiv.org/abs/2405.01224v1","category":"astro-ph.HE"}
{"created":"2024-05-02 12:04:35","title":"A Survey on Semantic Communication Networks: Architecture, Security, and Privacy","abstract":"Semantic communication, emerging as a breakthrough beyond the classical Shannon paradigm, aims to convey the essential meaning of source data rather than merely focusing on precise yet content-agnostic bit transmission. By interconnecting diverse intelligent agents (e.g., autonomous vehicles and VR devices) via semantic communications, the semantic communication networks (SemComNet) supports semantic-oriented transmission, efficient spectrum utilization, and flexible networking among collaborative agents. Consequently, SemComNet stands out for enabling ever-increasing intelligent applications, such as autonomous driving and Metaverse. However, being built on a variety of cutting-edge technologies including AI and knowledge graphs, SemComNet introduces diverse brand-new and unexpected threats, which pose obstacles to its widespread development. Besides, due to the intrinsic characteristics of SemComNet in terms of heterogeneous components, autonomous intelligence, and large-scale structure, a series of critical challenges emerge in securing SemComNet. In this paper, we provide a comprehensive and up-to-date survey of SemComNet from its fundamentals, security, and privacy aspects. Specifically, we first introduce a novel three-layer architecture of SemComNet for multi-agent interaction, which comprises the control layer, semantic transmission layer, and cognitive sensing layer. Then, we discuss its working modes and enabling technologies. Afterward, based on the layered architecture of SemComNet, we outline a taxonomy of security and privacy threats, while discussing state-of-the-art defense approaches. Finally, we present future research directions, clarifying the path toward building intelligent, robust, and green SemComNet. To our knowledge, this survey is the first to comprehensively cover the fundamentals of SemComNet, alongside a detailed analysis of its security and privacy issues.","sentences":["Semantic communication, emerging as a breakthrough beyond the classical Shannon paradigm, aims to convey the essential meaning of source data rather than merely focusing on precise yet content-agnostic bit transmission.","By interconnecting diverse intelligent agents (e.g., autonomous vehicles and VR devices) via semantic communications, the semantic communication networks (SemComNet) supports semantic-oriented transmission, efficient spectrum utilization, and flexible networking among collaborative agents.","Consequently, SemComNet stands out for enabling ever-increasing intelligent applications, such as autonomous driving and Metaverse.","However, being built on a variety of cutting-edge technologies including AI and knowledge graphs, SemComNet introduces diverse brand-new and unexpected threats, which pose obstacles to its widespread development.","Besides, due to the intrinsic characteristics of SemComNet in terms of heterogeneous components, autonomous intelligence, and large-scale structure, a series of critical challenges emerge in securing SemComNet.","In this paper, we provide a comprehensive and up-to-date survey of SemComNet from its fundamentals, security, and privacy aspects.","Specifically, we first introduce a novel three-layer architecture of SemComNet for multi-agent interaction, which comprises the control layer, semantic transmission layer, and cognitive sensing layer.","Then, we discuss its working modes and enabling technologies.","Afterward, based on the layered architecture of SemComNet, we outline a taxonomy of security and privacy threats, while discussing state-of-the-art defense approaches.","Finally, we present future research directions, clarifying the path toward building intelligent, robust, and green SemComNet.","To our knowledge, this survey is the first to comprehensively cover the fundamentals of SemComNet, alongside a detailed analysis of its security and privacy issues."],"url":"http://arxiv.org/abs/2405.01221v1","category":"cs.NI"}
{"created":"2024-05-02 11:56:16","title":"DMON: A Simple yet Effective Approach for Argument Structure Learning","abstract":"Argument structure learning~(ASL) entails predicting relations between arguments. Because it can structure a document to facilitate its understanding, it has been widely applied in many fields~(medical, commercial, and scientific domains). Despite its broad utilization, ASL remains a challenging task because it involves examining the complex relationships between the sentences in a potentially unstructured discourse. To resolve this problem, we have developed a simple yet effective approach called Dual-tower Multi-scale cOnvolution neural Network~(DMON) for the ASL task. Specifically, we organize arguments into a relationship matrix that together with the argument embeddings forms a relationship tensor and design a mechanism to capture relations with contextual arguments. Experimental results on three different-domain argument mining datasets demonstrate that our framework outperforms state-of-the-art models. The code is available at https://github.com/VRCMF/DMON.git .","sentences":["Argument structure learning~(ASL) entails predicting relations between arguments.","Because it can structure a document to facilitate its understanding, it has been widely applied in many fields~(medical, commercial, and scientific domains).","Despite its broad utilization, ASL remains a challenging task because it involves examining the complex relationships between the sentences in a potentially unstructured discourse.","To resolve this problem, we have developed a simple yet effective approach called Dual-tower Multi-scale cOnvolution neural Network~(DMON) for the ASL task.","Specifically, we organize arguments into a relationship matrix that together with the argument embeddings forms a relationship tensor and design a mechanism to capture relations with contextual arguments.","Experimental results on three different-domain argument mining datasets demonstrate that our framework outperforms state-of-the-art models.","The code is available at https://github.com/VRCMF/DMON.git ."],"url":"http://arxiv.org/abs/2405.01216v1","category":"cs.CL"}
{"created":"2024-05-02 11:50:14","title":"Non-perturbative Quantum Gravity in Fock representations","abstract":"Perturbative quantum gravity starts from prescribing a background metric. That background metric is then used in order to carry out two separate steps: 1. One splits the non-perturbative metric into background and deviation from it (graviton) and expands the action in terms of the graviton which results in an ifinite series of unknown radius of convergence. 2. One constructs a Fock representation for the graviton and performs perturbative graviton quantum field theory on the fixed background as dictated by the perturbative action. The result is a non-renormalisable theory without predictive power.   It is therefore widely believed that a non-perturbative approach is mandatory in order to construct a fundamental, not only effective, predictive quantum field theory of the gravitational interaction. Since perturbation theory is by definition background dependent, the notions of background dependence (BD) and perturbation theory (PT) are often considered as symbiotic, as if they imply each other.   In the present work we point out that there is no such symbiosis, these two notions are in fact logically independent. In particular, one can use BD structures while while not using PT at all. Specifically, we construct BD Fock representations (step 2 above) for the full, non-perturbative metric rather than the graviton (not step 1 above) and therefore never perform a perturbative expansion. Despite the fact that the gravitational Lagrangean is a non-polynomial, not even analytic, function of the metric we show that e.g. the Hamiltonian constraint with any density weight can be defined as a quadratic form with dense form domain in such a representation.","sentences":["Perturbative quantum gravity starts from prescribing a background metric.","That background metric is then used in order to carry out two separate steps: 1.","One splits the non-perturbative metric into background and deviation from it (graviton) and expands the action in terms of the graviton which results in an ifinite series of unknown radius of convergence.","2.","One constructs a Fock representation for the graviton and performs perturbative graviton quantum field theory on the fixed background as dictated by the perturbative action.","The result is a non-renormalisable theory without predictive power.   ","It is therefore widely believed that a non-perturbative approach is mandatory in order to construct a fundamental, not only effective, predictive quantum field theory of the gravitational interaction.","Since perturbation theory is by definition background dependent, the notions of background dependence (BD) and perturbation theory (PT) are often considered as symbiotic, as if they imply each other.   ","In the present work we point out that there is no such symbiosis, these two notions are in fact logically independent.","In particular, one can use BD structures while while not using PT at all.","Specifically, we construct BD Fock representations (step 2 above) for the full, non-perturbative metric rather than the graviton (not step 1 above) and therefore never perform a perturbative expansion.","Despite the fact that the gravitational Lagrangean is a non-polynomial, not even analytic, function of the metric we show that e.g. the Hamiltonian constraint with any density weight can be defined as a quadratic form with dense form domain in such a representation."],"url":"http://arxiv.org/abs/2405.01212v1","category":"gr-qc"}
{"created":"2024-05-02 11:48:43","title":"On the Cauchy problem for the fractional Keller-Segel system in variable Lebesgue spaces","abstract":"In this paper, we are mainly concerned with the well-posed problem of the fractional Keller--Segel system in the framework of variable Lebesgue spaces. Based on carefully examining the algebraical structure of the system, we reduced the fractional Keller--Segel system into the generalized nonlinear heat equation to overcome the difficulties caused by the boundedness of the Riesz potential in a variable Lebesgue spaces, then by mixing some structural properties of the variable Lebesgue spaces with the optimal decay estimates of the fractional heat kernel, we were able to establish two well-posedness results of the fractional Keller--Segel system in this functional setting.","sentences":["In this paper, we are mainly concerned with the well-posed problem of the fractional Keller--Segel system in the framework of variable Lebesgue spaces.","Based on carefully examining the algebraical structure of the system, we reduced the fractional Keller--Segel system into the generalized nonlinear heat equation to overcome the difficulties caused by the boundedness of the Riesz potential in a variable Lebesgue spaces, then by mixing some structural properties of the variable Lebesgue spaces with the optimal decay estimates of the fractional heat kernel, we were able to establish two well-posedness results of the fractional Keller--Segel system in this functional setting."],"url":"http://arxiv.org/abs/2405.01209v1","category":"math.AP"}
{"created":"2024-05-02 11:48:33","title":"On generators of $k$-PSD closures of the positive semidefinite cone","abstract":"Positive semidefinite (PSD) cone is the cone of positive semidefinite matrices, and is the object of interest in semidefinite programming (SDP). A computational efficient approximation of the PSD cone is the $k$-PSD closure, $1 \\leq k < n$, cone of $n\\times n$ real symmetric matrices such that all of their $k\\times k$ principal submatrices are positive semidefinite. For $k=1$, one obtains a polyhedral approximation, while $k=2$ yields a second order conic (SOC) approximation of the PSD cone. These approximations of the PSD cone have been used extensively in real-world applications such as AC Optimal Power Flow (ACOPF) to address computational inefficiencies where SDP relaxations are utilized for convexification the non-convexities. However a theoretical discussion about the geometry of these conic approximations of the PSD cone is rather sparse. In this short communication, we attempt to provide a characterization of some family of generators of the aforementioned conic approximations.","sentences":["Positive semidefinite (PSD) cone is the cone of positive semidefinite matrices, and is the object of interest in semidefinite programming (SDP).","A computational efficient approximation of the PSD cone is the $k$-PSD closure, $1 \\leq k < n$, cone of $n\\times n$ real symmetric matrices such that all of their $k\\times k$ principal submatrices are positive semidefinite.","For $k=1$, one obtains a polyhedral approximation, while $k=2$ yields a second order conic (SOC) approximation of the PSD cone.","These approximations of the PSD cone have been used extensively in real-world applications such as AC Optimal Power Flow (ACOPF) to address computational inefficiencies where SDP relaxations are utilized for convexification the non-convexities.","However a theoretical discussion about the geometry of these conic approximations of the PSD cone is rather sparse.","In this short communication, we attempt to provide a characterization of some family of generators of the aforementioned conic approximations."],"url":"http://arxiv.org/abs/2405.01208v1","category":"math.OC"}
{"created":"2024-05-02 11:46:12","title":"Towards Cross-Scale Attention and Surface Supervision for Fractured Bone Segmentation in CT","abstract":"Bone segmentation is an essential step for the preoperative planning of fracture trauma surgery. The automated segmentation of fractured bone from computed tomography (CT) scans remains challenging, due to the large differences of fractures in position and morphology, and also the inherent anatomical characteristics of different bone structures. To alleviate these issues, we propose a cross-scale attention mechanism as well as a surface supervision strategy for fractured bone segmentation in CT. Specifically, a cross-scale attention mechanism is introduced to effectively aggregate the features among different scales to provide more powerful fracture representation. Moreover, a surface supervision strategy is employed, which explicitly constrains the network to pay more attention to the bone boundary. The efficacy of the proposed method is evaluated on a public dataset containing CT scans with hip fractures. The evaluation metrics are Dice similarity coefficient (DSC), average symmetric surface distance (ASSD), and Hausdorff distance (95HD). The proposed method achieves an average DSC of 93.36%, ASSD of 0.85mm, 95HD of 7.51mm. Our method offers an effective fracture segmentation approach for the pelvic CT examinations, and has the potential to be used for improving the segmentation performance of other types of fractures.","sentences":["Bone segmentation is an essential step for the preoperative planning of fracture trauma surgery.","The automated segmentation of fractured bone from computed tomography (CT) scans remains challenging, due to the large differences of fractures in position and morphology, and also the inherent anatomical characteristics of different bone structures.","To alleviate these issues, we propose a cross-scale attention mechanism as well as a surface supervision strategy for fractured bone segmentation in CT.","Specifically, a cross-scale attention mechanism is introduced to effectively aggregate the features among different scales to provide more powerful fracture representation.","Moreover, a surface supervision strategy is employed, which explicitly constrains the network to pay more attention to the bone boundary.","The efficacy of the proposed method is evaluated on a public dataset containing CT scans with hip fractures.","The evaluation metrics are Dice similarity coefficient (DSC), average symmetric surface distance (ASSD), and Hausdorff distance (95HD).","The proposed method achieves an average DSC of 93.36%, ASSD of 0.85mm, 95HD of 7.51mm.","Our method offers an effective fracture segmentation approach for the pelvic CT examinations, and has the potential to be used for improving the segmentation performance of other types of fractures."],"url":"http://arxiv.org/abs/2405.01204v1","category":"cs.CV"}
{"created":"2024-05-02 11:44:52","title":"DLAP: A Deep Learning Augmented Large Language Model Prompting Framework for Software Vulnerability Detection","abstract":"Software vulnerability detection is generally supported by automated static analysis tools, which have recently been reinforced by deep learning (DL) models. However, despite the superior performance of DL-based approaches over rule-based ones in research, applying DL approaches to software vulnerability detection in practice remains a challenge due to the complex structure of source code, the black-box nature of DL, and the domain knowledge required to understand and validate the black-box results for addressing tasks after detection. Conventional DL models are trained by specific projects and, hence, excel in identifying vulnerabilities in these projects but not in others. These models with poor performance in vulnerability detection would impact the downstream tasks such as location and repair. More importantly, these models do not provide explanations for developers to comprehend detection results. In contrast, Large Language Models (LLMs) have made lots of progress in addressing these issues by leveraging prompting techniques. Unfortunately, their performance in identifying vulnerabilities is unsatisfactory. This paper contributes \\textbf{\\DLAP}, a \\underline{\\textbf{D}}eep \\underline{\\textbf{L}}earning \\underline{\\textbf{A}}ugmented LLMs \\underline{\\textbf{P}}rompting framework that combines the best of both DL models and LLMs to achieve exceptional vulnerability detection performance. Experimental evaluation results confirm that \\DLAP outperforms state-of-the-art prompting frameworks, including role-based prompts, auxiliary information prompts, chain-of-thought prompts, and in-context learning prompts, as well as fine-turning on multiple metrics.","sentences":["Software vulnerability detection is generally supported by automated static analysis tools, which have recently been reinforced by deep learning (DL) models.","However, despite the superior performance of DL-based approaches over rule-based ones in research, applying DL approaches to software vulnerability detection in practice remains a challenge due to the complex structure of source code, the black-box nature of DL, and the domain knowledge required to understand and validate the black-box results for addressing tasks after detection.","Conventional DL models are trained by specific projects and, hence, excel in identifying vulnerabilities in these projects but not in others.","These models with poor performance in vulnerability detection would impact the downstream tasks such as location and repair.","More importantly, these models do not provide explanations for developers to comprehend detection results.","In contrast, Large Language Models (LLMs) have made lots of progress in addressing these issues by leveraging prompting techniques.","Unfortunately, their performance in identifying vulnerabilities is unsatisfactory.","This paper contributes \\textbf{\\DLAP}, a \\underline{\\textbf{D}}eep \\underline{\\textbf{L}}earning \\underline{\\textbf{A}}ugmented LLMs \\underline{\\textbf{P}}rompting framework that combines the best of both DL models and LLMs to achieve exceptional vulnerability detection performance.","Experimental evaluation results confirm that \\DLAP outperforms state-of-the-art prompting frameworks, including role-based prompts, auxiliary information prompts, chain-of-thought prompts, and in-context learning prompts, as well as fine-turning on multiple metrics."],"url":"http://arxiv.org/abs/2405.01202v1","category":"cs.SE"}
{"created":"2024-05-02 11:43:14","title":"Observations on representations of the spatial diffeomorphism group and algebra in all dimensions","abstract":"The canonical quantisation of General Relativity including matter on a spacetime manifold in the globally hyperbolic setting involves in particular the representation theory of the spatial diffeomorphism group (SDG), and/or its Lie algebra (SDA), of the underlying spatial submanifold. There are well known Fock representations of the SDA in one spatial dimension and non-Fock representations of the SDG in all dimensions. The latter are not strongly continuous and do not descend to representations of the SDA.   In this work we report some partial results on non anomalous representations of the SDA for both geometry and matter: 1. Background independent Fock representations of the SDA by operators exist in all dimensions. 2. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by operators exist in one dimension but these do not extend to higher dimensions. 3. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA of volume preserving diffeomorphisms by operators exist in all dimensions. 4. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by quadratic forms exist in all dimensions.   Except for 1. these representations do not descend from an invariant state of the Weyl algebra and 4. points to a new strategy for solving the quantum constraints.","sentences":["The canonical quantisation of General Relativity including matter on a spacetime manifold in the globally hyperbolic setting involves in particular the representation theory of the spatial diffeomorphism group (SDG), and/or its Lie algebra (SDA), of the underlying spatial submanifold.","There are well known Fock representations of the SDA in one spatial dimension and non-Fock representations of the SDG in all dimensions.","The latter are not strongly continuous and do not descend to representations of the SDA.   ","In this work we report some partial results on non anomalous representations of the SDA for both geometry and matter: 1. Background independent Fock representations of the SDA by operators exist in all dimensions.","2.","Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by operators exist in one dimension but these do not extend to higher dimensions.","3.","Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA of volume preserving diffeomorphisms by operators exist in all dimensions.","4.","Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by quadratic forms exist in all dimensions.   ","Except for 1.","these representations do not descend from an invariant state of the Weyl algebra and 4. points to a new strategy for solving the quantum constraints."],"url":"http://arxiv.org/abs/2405.01201v1","category":"gr-qc"}
{"created":"2024-05-02 11:40:15","title":"Towards Interpretable Reinforcement Learning with Constrained Normalizing Flow Policies","abstract":"Reinforcement learning policies are typically represented by black-box neural networks, which are non-interpretable and not well-suited for safety-critical domains. To address both of these issues, we propose constrained normalizing flow policies as interpretable and safe-by-construction policy models. We achieve safety for reinforcement learning problems with instantaneous safety constraints, for which we can exploit domain knowledge by analytically constructing a normalizing flow that ensures constraint satisfaction. The normalizing flow corresponds to an interpretable sequence of transformations on action samples, each ensuring alignment with respect to a particular constraint. Our experiments reveal benefits beyond interpretability in an easier learning objective and maintained constraint satisfaction throughout the entire learning process. Our approach leverages constraints over reward engineering while offering enhanced interpretability, safety, and direct means of providing domain knowledge to the agent without relying on complex reward functions.","sentences":["Reinforcement learning policies are typically represented by black-box neural networks, which are non-interpretable and not well-suited for safety-critical domains.","To address both of these issues, we propose constrained normalizing flow policies as interpretable and safe-by-construction policy models.","We achieve safety for reinforcement learning problems with instantaneous safety constraints, for which we can exploit domain knowledge by analytically constructing a normalizing flow that ensures constraint satisfaction.","The normalizing flow corresponds to an interpretable sequence of transformations on action samples, each ensuring alignment with respect to a particular constraint.","Our experiments reveal benefits beyond interpretability in an easier learning objective and maintained constraint satisfaction throughout the entire learning process.","Our approach leverages constraints over reward engineering while offering enhanced interpretability, safety, and direct means of providing domain knowledge to the agent without relying on complex reward functions."],"url":"http://arxiv.org/abs/2405.01198v1","category":"cs.LG"}
{"created":"2024-05-02 11:29:48","title":"Gradient-Congruity Guided Federated Sparse Training","abstract":"Edge computing allows artificial intelligence and machine learning models to be deployed on edge devices, where they can learn from local data and collaborate to form a global model. Federated learning (FL) is a distributed machine learning technique that facilitates this process while preserving data privacy. However, FL also faces challenges such as high computational and communication costs regarding resource-constrained devices, and poor generalization performance due to the heterogeneity of data across edge clients and the presence of out-of-distribution data. In this paper, we propose the Gradient-Congruity Guided Federated Sparse Training (FedSGC), a novel method that integrates dynamic sparse training and gradient congruity inspection into federated learning framework to address these issues. Our method leverages the idea that the neurons, in which the associated gradients with conflicting directions with respect to the global model contain irrelevant or less generalized information for other clients, and could be pruned during the sparse training process. Conversely, the neurons where the associated gradients with consistent directions could be grown in a higher priority. In this way, FedSGC can greatly reduce the local computation and communication overheads while, at the same time, enhancing the generalization abilities of FL. We evaluate our method on challenging non-i.i.d settings and show that it achieves competitive accuracy with state-of-the-art FL methods across various scenarios while minimizing computation and communication costs.","sentences":["Edge computing allows artificial intelligence and machine learning models to be deployed on edge devices, where they can learn from local data and collaborate to form a global model.","Federated learning (FL) is a distributed machine learning technique that facilitates this process while preserving data privacy.","However, FL also faces challenges such as high computational and communication costs regarding resource-constrained devices, and poor generalization performance due to the heterogeneity of data across edge clients and the presence of out-of-distribution data.","In this paper, we propose the Gradient-Congruity Guided Federated Sparse Training (FedSGC), a novel method that integrates dynamic sparse training and gradient congruity inspection into federated learning framework to address these issues.","Our method leverages the idea that the neurons, in which the associated gradients with conflicting directions with respect to the global model contain irrelevant or less generalized information for other clients, and could be pruned during the sparse training process.","Conversely, the neurons where the associated gradients with consistent directions could be grown in a higher priority.","In this way, FedSGC can greatly reduce the local computation and communication overheads while, at the same time, enhancing the generalization abilities of FL.","We evaluate our method on challenging non-i.i.d settings and show that it achieves competitive accuracy with state-of-the-art FL methods across various scenarios while minimizing computation and communication costs."],"url":"http://arxiv.org/abs/2405.01189v1","category":"cs.LG"}
{"created":"2024-05-02 11:28:24","title":"The Atacama Cosmology Telescope: Reionization kSZ trispectrum methodology and limits","abstract":"Patchy reionization generates kinematic Sunyaev-Zeldovich (kSZ) anisotropies in the cosmic microwave background (CMB). Large-scale velocity perturbations along the line of sight modulate the small-scale kSZ power spectrum, leading to a trispectrum (or four-point function) in the CMB that depends on the physics of reionization. We investigate the challenges in detecting this trispectrum and use tools developed for CMB lensing, such as realization-dependent bias subtraction and cross-correlation based estimators, to counter uncertainties in the instrumental noise and assumed CMB power spectrum. We also find that both lensing and extragalactic foregrounds can impart larger trispectrum contributions than the reionization kSZ signal. We present a range of mitigation methods for both of these sources of contamination, validated on microwave-sky simulations. We use ACT DR6 and Planck data to calculate an upper limit on the reionization kSZ trispectrum from a measurement dominated by foregrounds. The upper limit is about 50 times the signal predicted from recent simulations.","sentences":["Patchy reionization generates kinematic Sunyaev-Zeldovich (kSZ) anisotropies in the cosmic microwave background (CMB).","Large-scale velocity perturbations along the line of sight modulate the small-scale kSZ power spectrum, leading to a trispectrum (or four-point function) in the CMB that depends on the physics of reionization.","We investigate the challenges in detecting this trispectrum and use tools developed for CMB lensing, such as realization-dependent bias subtraction and cross-correlation based estimators, to counter uncertainties in the instrumental noise and assumed CMB power spectrum.","We also find that both lensing and extragalactic foregrounds can impart larger trispectrum contributions than the reionization kSZ signal.","We present a range of mitigation methods for both of these sources of contamination, validated on microwave-sky simulations.","We use ACT DR6 and Planck data to calculate an upper limit on the reionization kSZ trispectrum from a measurement dominated by foregrounds.","The upper limit is about 50 times the signal predicted from recent simulations."],"url":"http://arxiv.org/abs/2405.01188v1","category":"astro-ph.CO"}
{"created":"2024-05-02 11:26:52","title":"Li\u00e9nard Type Nonlinear Oscillators and Quantum Solvability","abstract":"Li\\'{e}nard-type nonlinear oscillators with linear and nonlinear damping terms exhibit diverse dynamical behavior in both the classical and quantum regimes. In this paper, we consider examples of various one-dimensional Li\\'{e}nard type-I and type-II oscillators. The associated Euler-Lagrange equations are divided into groups based on the characteristics of the damping and forcing terms. The Li\\'{e}nard type-I oscillators often display localized solutions, isochronous and non-isochronous oscillations and are also precisely solvable in quantum mechanics in general, where the ordering parameters play an important role. These include Mathews-Lakshmanan and Higgs oscillators. However, the classical solutions of some of the nonlinear oscillators are expressed in terms of elliptic functions and have been found to be quasi-exactly solvable in the quantum region. The three-dimensional generalizations of these classical systems add more degrees of freedom, which show complex dynamics. Their quantum equivalents are also explored in this article. The isotonic generalizations of the non-isochronous nonlinear oscillators have also been solved both classically and quantum mechanically to advance the studies. The modified Emden equation categorized as Li\\'{e}nard type-II exhibits isochronous oscillations at the classical level. This property makes it a valuable tool for studying the underlying nonlinear dynamics. The study on the quantum counterpart of the system provides a deeper understanding of the behavior in the quantum realm as a typical PT-symmetric system.","sentences":["Li\\'{e}nard-type nonlinear oscillators with linear and nonlinear damping terms exhibit diverse dynamical behavior in both the classical and quantum regimes.","In this paper, we consider examples of various one-dimensional Li\\'{e}nard type-I and type-II oscillators.","The associated Euler-Lagrange equations are divided into groups based on the characteristics of the damping and forcing terms.","The Li\\'{e}nard type-I oscillators often display localized solutions, isochronous and non-isochronous oscillations and are also precisely solvable in quantum mechanics in general, where the ordering parameters play an important role.","These include Mathews-Lakshmanan and Higgs oscillators.","However, the classical solutions of some of the nonlinear oscillators are expressed in terms of elliptic functions and have been found to be quasi-exactly solvable in the quantum region.","The three-dimensional generalizations of these classical systems add more degrees of freedom, which show complex dynamics.","Their quantum equivalents are also explored in this article.","The isotonic generalizations of the non-isochronous nonlinear oscillators have also been solved both classically and quantum mechanically to advance the studies.","The modified Emden equation categorized as Li\\'{e}nard type-II exhibits isochronous oscillations at the classical level.","This property makes it a valuable tool for studying the underlying nonlinear dynamics.","The study on the quantum counterpart of the system provides a deeper understanding of the behavior in the quantum realm as a typical PT-symmetric system."],"url":"http://arxiv.org/abs/2405.01187v1","category":"quant-ph"}
{"created":"2024-05-02 11:19:57","title":"Potential Energy based Mixture Model for Noisy Label Learning","abstract":"Training deep neural networks (DNNs) from noisy labels is an important and challenging task. However, most existing approaches focus on the corrupted labels and ignore the importance of inherent data structure. To bridge the gap between noisy labels and data, inspired by the concept of potential energy in physics, we propose a novel Potential Energy based Mixture Model (PEMM) for noise-labels learning. We innovate a distance-based classifier with the potential energy regularization on its class centers. Embedding our proposed classifier with existing deep learning backbones, we can have robust networks with better feature representations. They can preserve intrinsic structures from the data, resulting in a superior noisy tolerance. We conducted extensive experiments to analyze the efficiency of our proposed model on several real-world datasets. Quantitative results show that it can achieve state-of-the-art performance.","sentences":["Training deep neural networks (DNNs) from noisy labels is an important and challenging task.","However, most existing approaches focus on the corrupted labels and ignore the importance of inherent data structure.","To bridge the gap between noisy labels and data, inspired by the concept of potential energy in physics, we propose a novel Potential Energy based Mixture Model (PEMM) for noise-labels learning.","We innovate a distance-based classifier with the potential energy regularization on its class centers.","Embedding our proposed classifier with existing deep learning backbones, we can have robust networks with better feature representations.","They can preserve intrinsic structures from the data, resulting in a superior noisy tolerance.","We conducted extensive experiments to analyze the efficiency of our proposed model on several real-world datasets.","Quantitative results show that it can achieve state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.01186v1","category":"cs.LG"}
{"created":"2024-05-02 11:17:24","title":"Third Medium Finite Element Contact Formulation for Pneumatically Actuated Systems","abstract":"Mechanical metamaterials are artificially engineered microstructures that exhibit novel mechanical behavior on the macroscopic scale. Active metamaterials can be externally controlled. Pneumatically actuated metamaterials can change their mechanical, acoustic, or other types of effective behavior in response to applied pressure with possible applications ranging from soft robotic actuators to phononic crystals. To facilitate the design of such pneumatically actuated metamaterials and structures by topology optimization, a robust way of their computational modeling, capturing both pneumatic actuation of internal voids and internal contact, is needed. Since voids in topology optimization are often modeled using a soft material model, the third medium contact formulation lends itself as a suitable stepping stone. We propose a single hyperelastic material model capable of maintaining a prescribed hydrostatic Cauchy stress within a void in the pre-contact phase while simultaneously acting as a third medium to enforce frictionless contact, contrasting existing third medium approaches focused solely on contact. We split the overall third-medium energy density into contact, regularization, and pneumatic pressure contributions, all of which can be individually controlled and tuned. To prevent distortions of the compliant third medium, we include curvature penalization in our model. This improves on existing formulations in terms of compliant third medium behavior, leading ultimately to better numerical stability of the solution. Since our formulation is energetically consistent, we are able to employ more advanced finite element solvers, such as the modified Cholesky algorithm to detect instabilities. We demonstrate the behavior of the proposed formulation on several examples of traditional contact benchmarks, including a standard patch test, and validate it with experimental measurement.","sentences":["Mechanical metamaterials are artificially engineered microstructures that exhibit novel mechanical behavior on the macroscopic scale.","Active metamaterials can be externally controlled.","Pneumatically actuated metamaterials can change their mechanical, acoustic, or other types of effective behavior in response to applied pressure with possible applications ranging from soft robotic actuators to phononic crystals.","To facilitate the design of such pneumatically actuated metamaterials and structures by topology optimization, a robust way of their computational modeling, capturing both pneumatic actuation of internal voids and internal contact, is needed.","Since voids in topology optimization are often modeled using a soft material model, the third medium contact formulation lends itself as a suitable stepping stone.","We propose a single hyperelastic material model capable of maintaining a prescribed hydrostatic Cauchy stress within a void in the pre-contact phase while simultaneously acting as a third medium to enforce frictionless contact, contrasting existing third medium approaches focused solely on contact.","We split the overall third-medium energy density into contact, regularization, and pneumatic pressure contributions, all of which can be individually controlled and tuned.","To prevent distortions of the compliant third medium, we include curvature penalization in our model.","This improves on existing formulations in terms of compliant third medium behavior, leading ultimately to better numerical stability of the solution.","Since our formulation is energetically consistent, we are able to employ more advanced finite element solvers, such as the modified Cholesky algorithm to detect instabilities.","We demonstrate the behavior of the proposed formulation on several examples of traditional contact benchmarks, including a standard patch test, and validate it with experimental measurement."],"url":"http://arxiv.org/abs/2405.01185v1","category":"cs.CE"}
{"created":"2024-05-02 11:12:32","title":"An efficient quantifier elimination procedure for Presburger arithmetic","abstract":"All known quantifier elimination procedures for Presburger arithmetic require doubly exponential time for eliminating a single block of existentially quantified variables. It has even been claimed in the literature that this upper bound is tight. We observe that this claim is incorrect and develop, as the main result of this paper, a quantifier elimination procedure eliminating a block of existentially quantified variables in singly exponential time. As corollaries, we can establish the precise complexity of numerous problems. Examples include deciding (i) monadic decomposability for existential formulas, (ii) whether an existential formula defines a well-quasi ordering or, more generally, (iii) certain formulas of Presburger arithmetic with Ramsey quantifiers. Moreover, despite the exponential blowup, our procedure shows that under mild assumptions, even NP upper bounds for decision problems about quantifier-free formulas can be transferred to existential formulas. The technical basis of our results is a kind of small model property for parametric integer programming that generalizes the seminal results by von zur Gathen and Sieveking on small integer points in convex polytopes.","sentences":["All known quantifier elimination procedures for Presburger arithmetic require doubly exponential time for eliminating a single block of existentially quantified variables.","It has even been claimed in the literature that this upper bound is tight.","We observe that this claim is incorrect and develop, as the main result of this paper, a quantifier elimination procedure eliminating a block of existentially quantified variables in singly exponential time.","As corollaries, we can establish the precise complexity of numerous problems.","Examples include deciding (i) monadic decomposability for existential formulas, (ii) whether an existential formula defines a well-quasi ordering or, more generally, (iii) certain formulas of Presburger arithmetic with Ramsey quantifiers.","Moreover, despite the exponential blowup, our procedure shows that under mild assumptions, even NP upper bounds for decision problems about quantifier-free formulas can be transferred to existential formulas.","The technical basis of our results is a kind of small model property for parametric integer programming that generalizes the seminal results by von zur Gathen and Sieveking on small integer points in convex polytopes."],"url":"http://arxiv.org/abs/2405.01183v1","category":"cs.LO"}
{"created":"2024-05-02 11:05:15","title":"A Direct Translation from LTL with Past to Deterministic Rabin Automata","abstract":"We present a translation from linear temporal logic with past to deterministic Rabin automata. The translation is direct in the sense that it does not rely on intermediate non-deterministic automata, and asymptotically optimal, resulting in Rabin automata of doubly exponential size. It is based on two main notions. One is that it is possible to encode the history contained in the prefix of a word, as relevant for the formula under consideration, by performing simple rewrites of the formula itself. As a consequence, a formula involving past operators can (through such rewrites, which involve alternating between weak and strong versions of past operators in the formula's syntax tree) be correctly evaluated at an arbitrary point in the future without requiring backtracking through the word. The other is that this allows us to generalize to linear temporal logic with past the result that the language of a pure-future formula can be decomposed into a Boolean combination of simpler languages, for which deterministic automata with simple acceptance conditions are easily constructed.","sentences":["We present a translation from linear temporal logic with past to deterministic Rabin automata.","The translation is direct in the sense that it does not rely on intermediate non-deterministic automata, and asymptotically optimal, resulting in Rabin automata of doubly exponential size.","It is based on two main notions.","One is that it is possible to encode the history contained in the prefix of a word, as relevant for the formula under consideration, by performing simple rewrites of the formula itself.","As a consequence, a formula involving past operators can (through such rewrites, which involve alternating between weak and strong versions of past operators in the formula's syntax tree) be correctly evaluated at an arbitrary point in the future without requiring backtracking through the word.","The other is that this allows us to generalize to linear temporal logic with past the result that the language of a pure-future formula can be decomposed into a Boolean combination of simpler languages, for which deterministic automata with simple acceptance conditions are easily constructed."],"url":"http://arxiv.org/abs/2405.01178v1","category":"cs.FL"}
{"created":"2024-05-02 11:01:31","title":"Uncertainty-aware self-training with expectation maximization basis transformation","abstract":"Self-training is a powerful approach to deep learning. The key process is to find a pseudo-label for modeling. However, previous self-training algorithms suffer from the over-confidence issue brought by the hard labels, even some confidence-related regularizers cannot comprehensively catch the uncertainty. Therefore, we propose a new self-training framework to combine uncertainty information of both model and dataset. Specifically, we propose to use Expectation-Maximization (EM) to smooth the labels and comprehensively estimate the uncertainty information. We further design a basis extraction network to estimate the initial basis from the dataset. The obtained basis with uncertainty can be filtered based on uncertainty information. It can then be transformed into the real hard label to iteratively update the model and basis in the retraining process. Experiments on image classification and semantic segmentation show the advantages of our methods among confidence-aware self-training algorithms with 1-3 percentage improvement on different datasets.","sentences":["Self-training is a powerful approach to deep learning.","The key process is to find a pseudo-label for modeling.","However, previous self-training algorithms suffer from the over-confidence issue brought by the hard labels, even some confidence-related regularizers cannot comprehensively catch the uncertainty.","Therefore, we propose a new self-training framework to combine uncertainty information of both model and dataset.","Specifically, we propose to use Expectation-Maximization (EM) to smooth the labels and comprehensively estimate the uncertainty information.","We further design a basis extraction network to estimate the initial basis from the dataset.","The obtained basis with uncertainty can be filtered based on uncertainty information.","It can then be transformed into the real hard label to iteratively update the model and basis in the retraining process.","Experiments on image classification and semantic segmentation show the advantages of our methods among confidence-aware self-training algorithms with 1-3 percentage improvement on different datasets."],"url":"http://arxiv.org/abs/2405.01175v1","category":"cs.CV"}
{"created":"2024-05-02 10:52:20","title":"Frame Codes for the Block-Erasure Channel","abstract":"Analog codes add redundancy by expanding the dimension using real/complex-valued operations. Frame theory provides a mathematical basis for constructing such codes, with diverse applications in non-orthogonal code-division multiple access (NOMA-CDMA), distributed computation, multiple description source coding, space-time coding (STC), and more. The channel model corresponding to these applications is a combination of noise and erasures. Recent analyses showed a useful connection between spectral random-matrix theory and large equiangular tight frames (ETFs) under random uniform erasures. In this work we generalize this model to a channel where the erasures come in blocks. This particularly fits NOMA-CDMA with multiple transmit antennas for each user and STC with known spatial grouping. We present a method to adjust ETF codes to suit block erasures, and find minimum intra-block-correlation frames which outperform ETFs in this setting.","sentences":["Analog codes add redundancy by expanding the dimension using real/complex-valued operations.","Frame theory provides a mathematical basis for constructing such codes, with diverse applications in non-orthogonal code-division multiple access (NOMA-CDMA), distributed computation, multiple description source coding, space-time coding (STC), and more.","The channel model corresponding to these applications is a combination of noise and erasures.","Recent analyses showed a useful connection between spectral random-matrix theory and large equiangular tight frames (ETFs) under random uniform erasures.","In this work we generalize this model to a channel where the erasures come in blocks.","This particularly fits NOMA-CDMA with multiple transmit antennas for each user and STC with known spatial grouping.","We present a method to adjust ETF codes to suit block erasures, and find minimum intra-block-correlation frames which outperform ETFs in this setting."],"url":"http://arxiv.org/abs/2405.01172v1","category":"cs.IT"}
{"created":"2024-05-02 10:49:14","title":"Modeling pedestrian fundamental diagram based on Directional Statistics","abstract":"Understanding pedestrian dynamics is crucial for appropriately designing pedestrian spaces. The pedestrian fundamental diagram (FD), which describes the relationship between pedestrian flow and density within a given space, characterizes these dynamics. Pedestrian FDs are significantly influenced by the flow type, such as uni-directional, bi-directional, and crossing flows. However, to the authors' knowledge, generalized pedestrian FDs that are applicable to various flow types have not been proposed. This may be due to the difficulty of using statistical methods to characterize the flow types. The flow types significantly depend on the angles of pedestrian movement; however, these angles cannot be processed by standard statistics due to their periodicity. In this study, we propose a comprehensive model for pedestrian FDs that can describe the pedestrian dynamics for various flow types by applying Directional Statistics. First, we develop a novel statistic describing the pedestrian flow type solely from pedestrian trajectory data using Directional Statistics. Then, we formulate a comprehensive pedestrian FD model that can be applied to various flow types by incorporating the proposed statistics into a traditional pedestrian FD model. The proposed model was validated using actual pedestrian trajectory data. The results confirmed that the model effectively represents the essential nature of pedestrian dynamics, such as the capacity reduction due to conflict of crossing flows and the capacity improvement due to the lane formation in bi-directional flows.","sentences":["Understanding pedestrian dynamics is crucial for appropriately designing pedestrian spaces.","The pedestrian fundamental diagram (FD), which describes the relationship between pedestrian flow and density within a given space, characterizes these dynamics.","Pedestrian FDs are significantly influenced by the flow type, such as uni-directional, bi-directional, and crossing flows.","However, to the authors' knowledge, generalized pedestrian FDs that are applicable to various flow types have not been proposed.","This may be due to the difficulty of using statistical methods to characterize the flow types.","The flow types significantly depend on the angles of pedestrian movement; however, these angles cannot be processed by standard statistics due to their periodicity.","In this study, we propose a comprehensive model for pedestrian FDs that can describe the pedestrian dynamics for various flow types by applying Directional Statistics.","First, we develop a novel statistic describing the pedestrian flow type solely from pedestrian trajectory data using Directional Statistics.","Then, we formulate a comprehensive pedestrian FD model that can be applied to various flow types by incorporating the proposed statistics into a traditional pedestrian FD model.","The proposed model was validated using actual pedestrian trajectory data.","The results confirmed that the model effectively represents the essential nature of pedestrian dynamics, such as the capacity reduction due to conflict of crossing flows and the capacity improvement due to the lane formation in bi-directional flows."],"url":"http://arxiv.org/abs/2405.01171v1","category":"nlin.AO"}
{"created":"2024-05-02 10:42:23","title":"Ergodic Spectral Efficiency Analysis of Intelligent Omni-Surface Aided Systems Suffering From Imperfect CSI and Hardware Impairments","abstract":"In contrast to the conventional reconfigurable intelligent surfaces (RIS), intelligent omni-surfaces (IOS) are capable of full-space coverage of smart radio environments by simultaneously transmitting and reflecting the incident signals. In this paper, we investigate the ergodic spectral efficiency of IOS-aided systems for transmission over random channel links, while considering both realistic imperfect channel state information (CSI) and transceiver hardware impairments (HWIs). Firstly, we formulate the linear minimum mean square error estimator of the equivalent channel spanning from the user equipments (UEs) to the access point (AP), where the transceiver HWIs are also considered. Then, we apply a two-timescale protocol for designing the beamformer of the IOS-aided system. Specifically, for the active AP beamformer, the minimum mean square error combining method is employed, which relies on the estimated equivalent channels, on the statistical information of the channel estimation error, on the inter-user interference as well as on the HWIs at the AP and UEs. By contrast, the passive IOS beamformer is designed based on the statistical CSI for maximizing the upper bound of the ergodic spectral efficiency. The theoretical analysis and simulation results show that the transceiver HWIs have a significant effect on the ergodic spectral efficiency, especially in the high transmit power region. Furthermore, we show that the HWIs at the AP can be effectively compensated by deploying more AP antennas.","sentences":["In contrast to the conventional reconfigurable intelligent surfaces (RIS), intelligent omni-surfaces (IOS) are capable of full-space coverage of smart radio environments by simultaneously transmitting and reflecting the incident signals.","In this paper, we investigate the ergodic spectral efficiency of IOS-aided systems for transmission over random channel links, while considering both realistic imperfect channel state information (CSI) and transceiver hardware impairments (HWIs).","Firstly, we formulate the linear minimum mean square error estimator of the equivalent channel spanning from the user equipments (UEs) to the access point (AP), where the transceiver HWIs are also considered.","Then, we apply a two-timescale protocol for designing the beamformer of the IOS-aided system.","Specifically, for the active AP beamformer, the minimum mean square error combining method is employed, which relies on the estimated equivalent channels, on the statistical information of the channel estimation error, on the inter-user interference as well as on the HWIs at the AP and UEs.","By contrast, the passive IOS beamformer is designed based on the statistical CSI for maximizing the upper bound of the ergodic spectral efficiency.","The theoretical analysis and simulation results show that the transceiver HWIs have a significant effect on the ergodic spectral efficiency, especially in the high transmit power region.","Furthermore, we show that the HWIs at the AP can be effectively compensated by deploying more AP antennas."],"url":"http://arxiv.org/abs/2405.01167v1","category":"cs.IT"}
{"created":"2024-05-02 10:36:55","title":"How A/B testing changes the dynamics of information spreading on a social network","abstract":"A/B testing methodology is generally performed by private companies to increase user engagement and satisfaction about online features. Their usage is far from being transparent and may undermine user autonomy (e.g. polarizing individual opinions, mis- and dis- information spreading). For our analysis we leverage a crucial case study dataset (i.e. Upworthy) where news headlines were allocated to users and reshuffled for optimizing clicks. Our centre of focus is to determine how and under which conditions A/B testing affects the distribution of content on the collective level, specifically on different social network structures. In order to achieve that, we set up an agent-based model reproducing social interaction and an individual decision-making model. Our preliminary results indicate that A/B testing has a substantial influence on the qualitative dynamics of information dissemination on a social network. Moreover, our modeling framework promisingly embeds conjecturing policy (e.g. nudging, boosting) interventions.","sentences":["A/B testing methodology is generally performed by private companies to increase user engagement and satisfaction about online features.","Their usage is far from being transparent and may undermine user autonomy (e.g. polarizing individual opinions, mis- and dis- information spreading).","For our analysis we leverage a crucial case study dataset (i.e. Upworthy) where news headlines were allocated to users and reshuffled for optimizing clicks.","Our centre of focus is to determine how and under which conditions A/B testing affects the distribution of content on the collective level, specifically on different social network structures.","In order to achieve that, we set up an agent-based model reproducing social interaction and an individual decision-making model.","Our preliminary results indicate that A/B testing has a substantial influence on the qualitative dynamics of information dissemination on a social network.","Moreover, our modeling framework promisingly embeds conjecturing policy (e.g. nudging, boosting) interventions."],"url":"http://arxiv.org/abs/2405.01165v1","category":"cs.SI"}
{"created":"2024-05-02 10:30:25","title":"Exponentially Consistent Outlier Hypothesis Testing for Continuous Sequences","abstract":"In outlier hypothesis testing, one aims to detect outlying sequences among a given set of sequences, where most sequences are generated i.i.d. from a nominal distribution while outlying sequences (outliers) are generated i.i.d. from a different anomalous distribution. Most existing studies focus on discrete-valued sequences, where each data sample takes values in a finite set. To account for practical scenarios where data sequences usually take real values, we study outlier hypothesis testing for continuous sequences when both the nominal and anomalous distributions are \\emph{unknown}. Specifically, we propose distribution free tests and prove that the probabilities of misclassification error, false reject and false alarm decay exponentially fast for three different test designs: fixed-length test, sequential test, and two-phase test. In a fixed-length test, one fixes the sample size of each observed sequence; in a sequential test, one takes a sample sequentially from each sequence per unit time until a reliable decision can be made; in a two-phase test, one adapts the sample size from two different fixed values. Remarkably, the two-phase test achieves a good balance between test design complexity and theoretical performance. We first consider the case of at most one outlier, and then generalize our results to the case with multiple outliers where the number of outliers is unknown.","sentences":["In outlier hypothesis testing, one aims to detect outlying sequences among a given set of sequences, where most sequences are generated","i.i.d.","from a nominal distribution while outlying sequences (outliers) are generated i.i.d.","from a different anomalous distribution.","Most existing studies focus on discrete-valued sequences, where each data sample takes values in a finite set.","To account for practical scenarios where data sequences usually take real values, we study outlier hypothesis testing for continuous sequences when both the nominal and anomalous distributions are \\emph{unknown}.","Specifically, we propose distribution free tests and prove that the probabilities of misclassification error, false reject and false alarm decay exponentially fast for three different test designs: fixed-length test, sequential test, and two-phase test.","In a fixed-length test, one fixes the sample size of each observed sequence; in a sequential test, one takes a sample sequentially from each sequence per unit time until a reliable decision can be made; in a two-phase test, one adapts the sample size from two different fixed values.","Remarkably, the two-phase test achieves a good balance between test design complexity and theoretical performance.","We first consider the case of at most one outlier, and then generalize our results to the case with multiple outliers where the number of outliers is unknown."],"url":"http://arxiv.org/abs/2405.01161v1","category":"eess.SP"}
{"created":"2024-05-02 10:28:52","title":"TartuNLP at EvaLatin 2024: Emotion Polarity Detection","abstract":"This paper presents the TartuNLP team submission to EvaLatin 2024 shared task of the emotion polarity detection for historical Latin texts. Our system relies on two distinct approaches to annotating training data for supervised learning: 1) creating heuristics-based labels by adopting the polarity lexicon provided by the organizers and 2) generating labels with GPT4. We employed parameter efficient fine-tuning using the adapters framework and experimented with both monolingual and cross-lingual knowledge transfer for training language and task adapters. Our submission with the LLM-generated labels achieved the overall first place in the emotion polarity detection task. Our results show that LLM-based annotations show promising results on texts in Latin.","sentences":["This paper presents the TartuNLP team submission to EvaLatin 2024 shared task of the emotion polarity detection for historical Latin texts.","Our system relies on two distinct approaches to annotating training data for supervised learning: 1) creating heuristics-based labels by adopting the polarity lexicon provided by the organizers and 2) generating labels with GPT4.","We employed parameter efficient fine-tuning using the adapters framework and experimented with both monolingual and cross-lingual knowledge transfer for training language and task adapters.","Our submission with the LLM-generated labels achieved the overall first place in the emotion polarity detection task.","Our results show that LLM-based annotations show promising results on texts in Latin."],"url":"http://arxiv.org/abs/2405.01159v1","category":"cs.CL"}
{"created":"2024-05-02 10:23:17","title":"Interpretable Data-driven Anomaly Detection in Industrial Processes with ExIFFI","abstract":"Anomaly detection (AD) is a crucial process often required in industrial settings. Anomalies can signal underlying issues within a system, prompting further investigation. Industrial processes aim to streamline operations as much as possible, encompassing the production of the final product, making AD an essential mean to reach this goal.Conventional anomaly detection methodologies typically classify observations as either normal or anomalous without providing insight into the reasons behind these classifications.Consequently, in light of the emergence of Industry 5.0, a more desirable approach involves providing interpretable outcomes, enabling users to understand the rationale behind the results.This paper presents the first industrial application of ExIFFI, a recently developed approach focused on the production of fast and efficient explanations for the Extended Isolation Forest (EIF) Anomaly detection method. ExIFFI is tested on two publicly available industrial datasets demonstrating superior effectiveness in explanations and computational efficiency with the respect to other state-of-the-art explainable AD models.","sentences":["Anomaly detection (AD) is a crucial process often required in industrial settings.","Anomalies can signal underlying issues within a system, prompting further investigation.","Industrial processes aim to streamline operations as much as possible, encompassing the production of the final product, making AD an essential mean to reach this goal.","Conventional anomaly detection methodologies typically classify observations as either normal or anomalous without providing insight into the reasons behind these classifications.","Consequently, in light of the emergence of Industry 5.0, a more desirable approach involves providing interpretable outcomes, enabling users to understand the rationale behind the results.","This paper presents the first industrial application of ExIFFI, a recently developed approach focused on the production of fast and efficient explanations for the Extended Isolation Forest (EIF) Anomaly detection method.","ExIFFI is tested on two publicly available industrial datasets demonstrating superior effectiveness in explanations and computational efficiency with the respect to other state-of-the-art explainable AD models."],"url":"http://arxiv.org/abs/2405.01158v1","category":"cs.LG"}
{"created":"2024-05-02 10:18:22","title":"Self-Supervised Learning for Interventional Image Analytics: Towards Robust Device Trackers","abstract":"An accurate detection and tracking of devices such as guiding catheters in live X-ray image acquisitions is an essential prerequisite for endovascular cardiac interventions. This information is leveraged for procedural guidance, e.g., directing stent placements. To ensure procedural safety and efficacy, there is a need for high robustness no failures during tracking. To achieve that, one needs to efficiently tackle challenges, such as: device obscuration by contrast agent or other external devices or wires, changes in field-of-view or acquisition angle, as well as the continuous movement due to cardiac and respiratory motion. To overcome the aforementioned challenges, we propose a novel approach to learn spatio-temporal features from a very large data cohort of over 16 million interventional X-ray frames using self-supervision for image sequence data. Our approach is based on a masked image modeling technique that leverages frame interpolation based reconstruction to learn fine inter-frame temporal correspondences. The features encoded in the resulting model are fine-tuned downstream. Our approach achieves state-of-the-art performance and in particular robustness compared to ultra optimized reference solutions (that use multi-stage feature fusion, multi-task and flow regularization). The experiments show that our method achieves 66.31% reduction in maximum tracking error against reference solutions (23.20% when flow regularization is used); achieving a success score of 97.95% at a 3x faster inference speed of 42 frames-per-second (on GPU). The results encourage the use of our approach in various other tasks within interventional image analytics that require effective understanding of spatio-temporal semantics.","sentences":["An accurate detection and tracking of devices such as guiding catheters in live X-ray image acquisitions is an essential prerequisite for endovascular cardiac interventions.","This information is leveraged for procedural guidance, e.g., directing stent placements.","To ensure procedural safety and efficacy, there is a need for high robustness no failures during tracking.","To achieve that, one needs to efficiently tackle challenges, such as: device obscuration by contrast agent or other external devices or wires, changes in field-of-view or acquisition angle, as well as the continuous movement due to cardiac and respiratory motion.","To overcome the aforementioned challenges, we propose a novel approach to learn spatio-temporal features from a very large data cohort of over 16 million interventional X-ray frames using self-supervision for image sequence data.","Our approach is based on a masked image modeling technique that leverages frame interpolation based reconstruction to learn fine inter-frame temporal correspondences.","The features encoded in the resulting model are fine-tuned downstream.","Our approach achieves state-of-the-art performance and in particular robustness compared to ultra optimized reference solutions (that use multi-stage feature fusion, multi-task and flow regularization).","The experiments show that our method achieves 66.31% reduction in maximum tracking error against reference solutions (23.20% when flow regularization is used); achieving a success score of 97.95% at a 3x faster inference speed of 42 frames-per-second (on GPU).","The results encourage the use of our approach in various other tasks within interventional image analytics that require effective understanding of spatio-temporal semantics."],"url":"http://arxiv.org/abs/2405.01156v1","category":"cs.CV"}
{"created":"2024-05-02 10:15:59","title":"SynFlowNet: Towards Molecule Design with Guaranteed Synthesis Pathways","abstract":"Recent breakthroughs in generative modelling have led to a number of works proposing molecular generation models for drug discovery. While these models perform well at capturing drug-like motifs, they are known to often produce synthetically inaccessible molecules. This is because they are trained to compose atoms or fragments in a way that approximates the training distribution, but they are not explicitly aware of the synthesis constraints that come with making molecules in the lab. To address this issue, we introduce SynFlowNet, a GFlowNet model whose action space uses chemically validated reactions and reactants to sequentially build new molecules. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool. SynFlowNet consistently samples synthetically feasible molecules, while still being able to find diverse and high-utility candidates. Furthermore, we compare molecules designed with SynFlowNet to experimentally validated actives, and find that they show comparable properties of interest, such as molecular weight, SA score and predicted protein binding affinity.","sentences":["Recent breakthroughs in generative modelling have led to a number of works proposing molecular generation models for drug discovery.","While these models perform well at capturing drug-like motifs, they are known to often produce synthetically inaccessible molecules.","This is because they are trained to compose atoms or fragments in a way that approximates the training distribution, but they are not explicitly aware of the synthesis constraints that come with making molecules in the lab.","To address this issue, we introduce SynFlowNet, a GFlowNet model whose action space uses chemically validated reactions and reactants to sequentially build new molecules.","We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool.","SynFlowNet consistently samples synthetically feasible molecules, while still being able to find diverse and high-utility candidates.","Furthermore, we compare molecules designed with SynFlowNet to experimentally validated actives, and find that they show comparable properties of interest, such as molecular weight, SA score and predicted protein binding affinity."],"url":"http://arxiv.org/abs/2405.01155v1","category":"cs.LG"}
{"created":"2024-05-02 10:15:44","title":"Ulrich subvarieties and the non-existence of low rank Ulrich bundles on complete intersections","abstract":"We characterize the existence of an Ulrich vector bundle on a variety $X \\subset P^N$ in terms of the existence of a subvariety satisfying some precise conditions. Then we use this fact to prove that a complete intersection of dimension $n \\ge 4$, which if $n=4$ is very general and not of type $(2,2)$, does not carry any Ulrich bundles of rank $r \\le 3$ unless $n=4, r=2$ and $X$ is a quadric.","sentences":["We characterize the existence of an Ulrich vector bundle on a variety $X \\subset P^N$ in terms of the existence of a subvariety satisfying some precise conditions.","Then we use this fact to prove that a complete intersection of dimension $n \\ge 4$, which if $n=4$ is very general and not of type $(2,2)$, does not carry any Ulrich bundles of rank $r \\le 3$ unless $n=4, r=2$ and $X$ is a quadric."],"url":"http://arxiv.org/abs/2405.01154v1","category":"math.AG"}
{"created":"2024-05-02 10:05:36","title":"Qualia and the Formal Structure of Meaning","abstract":"This work explores the hypothesis that subjectively attributed meaning constitutes the phenomenal content of conscious experience. That is, phenomenal content is semantic. This form of subjective meaning manifests as an intrinsic and non-representational character of qualia. Empirically, subjective meaning is ubiquitous in conscious experiences. We point to phenomenological studies that lend evidence to support this. Furthermore, this notion of meaning closely relates to what Frege refers to as \"sense\", in metaphysics and philosophy of language. It also aligns with Peirce's \"interpretant\", in semiotics. We discuss how Frege's sense can also be extended to the raw feels of consciousness. Sense and reference both play a role in phenomenal experience. Moreover, within the context of the mind-matter relation, we provide a formalization of subjective meaning associated to one's mental representations. Identifying the precise maps between the physical and mental domains, we argue that syntactic and semantic structures transcend language, and are realized within each of these domains. Formally, meaning is a relational attribute, realized via a map that interprets syntactic structures of a formal system within an appropriate semantic space. The image of this map within the mental domain is what is relevant for experience, and thus comprises the phenomenal content of qualia. We conclude with possible implications this may have for experience-based theories of consciousness.","sentences":["This work explores the hypothesis that subjectively attributed meaning constitutes the phenomenal content of conscious experience.","That is, phenomenal content is semantic.","This form of subjective meaning manifests as an intrinsic and non-representational character of qualia.","Empirically, subjective meaning is ubiquitous in conscious experiences.","We point to phenomenological studies that lend evidence to support this.","Furthermore, this notion of meaning closely relates to what Frege refers to as \"sense\", in metaphysics and philosophy of language.","It also aligns with Peirce's \"interpretant\", in semiotics.","We discuss how Frege's sense can also be extended to the raw feels of consciousness.","Sense and reference both play a role in phenomenal experience.","Moreover, within the context of the mind-matter relation, we provide a formalization of subjective meaning associated to one's mental representations.","Identifying the precise maps between the physical and mental domains, we argue that syntactic and semantic structures transcend language, and are realized within each of these domains.","Formally, meaning is a relational attribute, realized via a map that interprets syntactic structures of a formal system within an appropriate semantic space.","The image of this map within the mental domain is what is relevant for experience, and thus comprises the phenomenal content of qualia.","We conclude with possible implications this may have for experience-based theories of consciousness."],"url":"http://arxiv.org/abs/2405.01148v1","category":"q-bio.NC"}
{"created":"2024-05-02 10:00:37","title":"Localized RETE for Incremental Graph Queries","abstract":"Context: The growing size of graph-based modeling artifacts in model-driven engineering calls for techniques that enable efficient execution of graph queries. Incremental approaches based on the RETE algorithm provide an adequate solution in many scenarios, but are generally designed to search for query results over the entire graph. However, in certain situations, a user may only be interested in query results for a subgraph, for instance when a developer is working on a large model of which only a part is loaded into their workspace. In this case, the global execution semantics can result in significant computational overhead.   Contribution: To mitigate the outlined shortcoming, in this paper we propose an extension of the RETE approach that enables local, yet fully incremental execution of graph queries, while still guaranteeing completeness of results with respect to the relevant subgraph.   Results: We empirically evaluate the presented approach via experiments inspired by a scenario from software development and an independent social network benchmark. The experimental results indicate that the proposed technique can significantly improve performance regarding memory consumption and execution time in favorable cases, but may incur a noticeable linear overhead in unfavorable cases.","sentences":["Context: The growing size of graph-based modeling artifacts in model-driven engineering calls for techniques that enable efficient execution of graph queries.","Incremental approaches based on the RETE algorithm provide an adequate solution in many scenarios, but are generally designed to search for query results over the entire graph.","However, in certain situations, a user may only be interested in query results for a subgraph, for instance when a developer is working on a large model of which only a part is loaded into their workspace.","In this case, the global execution semantics can result in significant computational overhead.   ","Contribution: To mitigate the outlined shortcoming, in this paper we propose an extension of the RETE approach that enables local, yet fully incremental execution of graph queries, while still guaranteeing completeness of results with respect to the relevant subgraph.   ","Results: We empirically evaluate the presented approach via experiments inspired by a scenario from software development and an independent social network benchmark.","The experimental results indicate that the proposed technique can significantly improve performance regarding memory consumption and execution time in favorable cases, but may incur a noticeable linear overhead in unfavorable cases."],"url":"http://arxiv.org/abs/2405.01145v1","category":"cs.SE"}
{"created":"2024-05-02 10:00:16","title":"Boosting Communication Efficiency of Federated Learning's Secure Aggregation","abstract":"Federated Learning (FL) is a decentralized machine learning approach where client devices train models locally and send them to a server that performs aggregation to generate a global model. FL is vulnerable to model inversion attacks, where the server can infer sensitive client data from trained models. Google's Secure Aggregation (SecAgg) protocol addresses this data privacy issue by masking each client's trained model using shared secrets and individual elements generated locally on the client's device. Although SecAgg effectively preserves privacy, it imposes considerable communication and computation overhead, especially as network size increases. Building upon SecAgg, this poster introduces a Communication-Efficient Secure Aggregation (CESA) protocol that substantially reduces this overhead by using only two shared secrets per client to mask the model. We propose our method for stable networks with low delay variation and limited client dropouts. CESA is independent of the data distribution and network size (for higher than 6 nodes), preventing the honest-but-curious server from accessing unmasked models. Our initial evaluation reveals that CESA significantly reduces the communication cost compared to SecAgg.","sentences":["Federated Learning (FL) is a decentralized machine learning approach where client devices train models locally and send them to a server that performs aggregation to generate a global model.","FL is vulnerable to model inversion attacks, where the server can infer sensitive client data from trained models.","Google's Secure Aggregation (SecAgg) protocol addresses this data privacy issue by masking each client's trained model using shared secrets and individual elements generated locally on the client's device.","Although SecAgg effectively preserves privacy, it imposes considerable communication and computation overhead, especially as network size increases.","Building upon SecAgg, this poster introduces a Communication-Efficient Secure Aggregation (CESA) protocol that substantially reduces this overhead by using only two shared secrets per client to mask the model.","We propose our method for stable networks with low delay variation and limited client dropouts.","CESA is independent of the data distribution and network size (for higher than 6 nodes), preventing the honest-but-curious server from accessing unmasked models.","Our initial evaluation reveals that CESA significantly reduces the communication cost compared to SecAgg."],"url":"http://arxiv.org/abs/2405.01144v1","category":"cs.CR"}
{"created":"2024-05-02 09:58:49","title":"Sharp Bounds for Sequential Federated Learning on Heterogeneous Data","abstract":"There are two paradigms in Federated Learning (FL): parallel FL (PFL), where models are trained in a parallel manner across clients; and sequential FL (SFL), where models are trained in a sequential manner across clients. In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking. To resolve the theoretical dilemma of SFL, we establish sharp convergence guarantees for SFL on heterogeneous data with both upper and lower bounds. Specifically, we derive the upper bounds for strongly convex, general convex and non-convex objective functions, and construct the matching lower bounds for the strongly convex and general convex objective functions. Then, we compare the upper bounds of SFL with those of PFL, showing that SFL outperforms PFL (at least, when the level of heterogeneity is relatively high). Experimental results on quadratic functions and real data sets validate the counterintuitive comparison result.","sentences":["There are two paradigms in Federated Learning (FL): parallel FL (PFL), where models are trained in a parallel manner across clients; and sequential FL (SFL), where models are trained in a sequential manner across clients.","In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking.","To resolve the theoretical dilemma of SFL, we establish sharp convergence guarantees for SFL on heterogeneous data with both upper and lower bounds.","Specifically, we derive the upper bounds for strongly convex, general convex and non-convex objective functions, and construct the matching lower bounds for the strongly convex and general convex objective functions.","Then, we compare the upper bounds of SFL with those of PFL, showing that SFL outperforms PFL (at least, when the level of heterogeneity is relatively high).","Experimental results on quadratic functions and real data sets validate the counterintuitive comparison result."],"url":"http://arxiv.org/abs/2405.01142v1","category":"cs.LG"}
{"created":"2024-05-02 09:55:19","title":"It Couldn't Help But Overhear: On the Limits of Modelling Meta-Communicative Grounding Acts with Supervised Learning","abstract":"Active participation in a conversation is key to building common ground, since understanding is jointly tailored by producers and recipients. Overhearers are deprived of the privilege of performing grounding acts and can only conjecture about intended meanings. Still, data generation and annotation, modelling, training and evaluation of NLP dialogue models place reliance on the overhearing paradigm. How much of the underlying grounding processes are thereby forfeited? As we show, there is evidence pointing to the impossibility of properly modelling human meta-communicative acts with data-driven learning models. In this paper, we discuss this issue and provide a preliminary analysis on the variability of human decisions for requesting clarification. Most importantly, we wish to bring this topic back to the community's table, encouraging discussion on the consequences of having models designed to only \"listen in\".","sentences":["Active participation in a conversation is key to building common ground, since understanding is jointly tailored by producers and recipients.","Overhearers are deprived of the privilege of performing grounding acts and can only conjecture about intended meanings.","Still, data generation and annotation, modelling, training and evaluation of NLP dialogue models place reliance on the overhearing paradigm.","How much of the underlying grounding processes are thereby forfeited?","As we show, there is evidence pointing to the impossibility of properly modelling human meta-communicative acts with data-driven learning models.","In this paper, we discuss this issue and provide a preliminary analysis on the variability of human decisions for requesting clarification.","Most importantly, we wish to bring this topic back to the community's table, encouraging discussion on the consequences of having models designed to only \"listen in\"."],"url":"http://arxiv.org/abs/2405.01139v1","category":"cs.CL"}
{"created":"2024-05-02 09:55:17","title":"Scaling of phase count in multicomponent liquids","abstract":"Mixtures with many components can segregate into coexisting phases, e.g., in biological cells and synthetic materials such as metallic glass. The interactions between components dictate what phases form in equilibrium, but quantifying this relationship has proven difficult. We derive scaling relations for the number of coexisting phases in multicomponent liquids with random interactions and compositions, which we verify numerically. Our results indicate that interactions only need to increase logarithmically with the number of components for the liquid to segregate into many phases. In contrast, a stability analysis of the homogeneous state predicts a power-law scaling. This discrepancy implies an enormous parameter regime where the number of coexisting phases exceeds the number of unstable modes, generalizing the nucleation and growth regime of binary mixtures to many components.","sentences":["Mixtures with many components can segregate into coexisting phases, e.g., in biological cells and synthetic materials such as metallic glass.","The interactions between components dictate what phases form in equilibrium, but quantifying this relationship has proven difficult.","We derive scaling relations for the number of coexisting phases in multicomponent liquids with random interactions and compositions, which we verify numerically.","Our results indicate that interactions only need to increase logarithmically with the number of components for the liquid to segregate into many phases.","In contrast, a stability analysis of the homogeneous state predicts a power-law scaling.","This discrepancy implies an enormous parameter regime where the number of coexisting phases exceeds the number of unstable modes, generalizing the nucleation and growth regime of binary mixtures to many components."],"url":"http://arxiv.org/abs/2405.01138v1","category":"cond-mat.soft"}
{"created":"2024-05-02 09:54:10","title":"Modelling user behavior towards smartphones and wearable technologies: A bibliometric study and brief literature review","abstract":"The study uses bibliometric as well as content analysis to determine the current situation regarding the application of technology adoption models (i.e., the Technology Acceptance Model, Unified Theory of Acceptance and Use of Technology, and Innovation Diffusion Theory) to the smartphone market that also includes smart wearables. Hereby the author would like to determine the connection between smartphone usage and adoption models and enrich literature by defining state-of-the-art tendencies and approaches. To achieve the goal, the author applied a two-stage approach: in the first stage, 213 articles were analyzed using Citation and Bibliographic coupling tools in VOSviewer (1.6.20). The papers were selected from the Scopus database and the search of the papers was conducted in the fields of Economics, Business, and Computer technologies. In the second stage, the author conducted a brief literature review of the most influential papers. The results illustrate the situation regarding the implementation of different models in the case of smartphone adoption. Content analyses of the most influential papers were applied to explain and enrich the results of bibliometric analyses as well as determine research gaps and future research development.","sentences":["The study uses bibliometric as well as content analysis to determine the current situation regarding the application of technology adoption models (i.e., the Technology Acceptance Model, Unified Theory of Acceptance and Use of Technology, and Innovation Diffusion Theory) to the smartphone market that also includes smart wearables.","Hereby the author would like to determine the connection between smartphone usage and adoption models and enrich literature by defining state-of-the-art tendencies and approaches.","To achieve the goal, the author applied a two-stage approach: in the first stage, 213 articles were analyzed using Citation and Bibliographic coupling tools in VOSviewer (1.6.20).","The papers were selected from the Scopus database and the search of the papers was conducted in the fields of Economics, Business, and Computer technologies.","In the second stage, the author conducted a brief literature review of the most influential papers.","The results illustrate the situation regarding the implementation of different models in the case of smartphone adoption.","Content analyses of the most influential papers were applied to explain and enrich the results of bibliometric analyses as well as determine research gaps and future research development."],"url":"http://arxiv.org/abs/2405.01137v1","category":"econ.GN"}
{"created":"2024-05-02 09:53:02","title":"Achievable Rate Analysis of Intelligent Omni-Surface Assisted NOMA Holographic MIMO Systems","abstract":"An intelligent omni-surface (IOS) assisted holographic multiple-input and multiple-output architecture is conceived for $360^\\circ$ full-space coverage at a low energy consumption. The theoretical ergodic rate lower bound of our non-orthogonal multiple access (NOMA) scheme is derived based on the moment matching approximation method, while considering the signal distortion at transceivers imposed by hardware impairments (HWIs). Furthermore, the asymptotically ergodic rate lower bound is derived both for an infinite number of IOS elements and for continuous aperture surfaces. Both the theoretical analysis and the simulation results show that the achievable rate of the NOMA scheme is higher than that of its orthogonal multiple access counterpart. Furthermore, owing to the HWIs at the transceivers, the achievable rate saturates at high signal-to-noise ratio region, instead of reaching its theoretical maximum.","sentences":["An intelligent omni-surface (IOS) assisted holographic multiple-input and multiple-output architecture is conceived for $360^\\circ$ full-space coverage at a low energy consumption.","The theoretical ergodic rate lower bound of our non-orthogonal multiple access (NOMA) scheme is derived based on the moment matching approximation method, while considering the signal distortion at transceivers imposed by hardware impairments (HWIs).","Furthermore, the asymptotically ergodic rate lower bound is derived both for an infinite number of IOS elements and for continuous aperture surfaces.","Both the theoretical analysis and the simulation results show that the achievable rate of the NOMA scheme is higher than that of its orthogonal multiple access counterpart.","Furthermore, owing to the HWIs at the transceivers, the achievable rate saturates at high signal-to-noise ratio region, instead of reaching its theoretical maximum."],"url":"http://arxiv.org/abs/2405.01136v1","category":"cs.IT"}
{"created":"2024-05-02 09:50:45","title":"Characteristic determinants for a second order difference equation on the half-line arising in hydrodynamics","abstract":"We study the point spectrum of a second order difference operator with complex potential on the half-line via Fredholm determinants of the corresponding Birman-Schwinger operator pencils, the Evans and the Jost functions. An application is given to instability of a generalization of the Kolmogorov flow for the Euler equation of ideal fluid on the two dimensional torus.","sentences":["We study the point spectrum of a second order difference operator with complex potential on the half-line via Fredholm determinants of the corresponding Birman-Schwinger operator pencils, the Evans and the Jost functions.","An application is given to instability of a generalization of the Kolmogorov flow for the Euler equation of ideal fluid on the two dimensional torus."],"url":"http://arxiv.org/abs/2405.01135v1","category":"math.SP"}
{"created":"2024-05-02 09:50:01","title":"Leveraging Procedural Generation for Learning Autonomous Peg-in-Hole Assembly in Space","abstract":"The ability to autonomously assemble structures is crucial for the development of future space infrastructure. However, the unpredictable conditions of space pose significant challenges for robotic systems, necessitating the development of advanced learning techniques to enable autonomous assembly. In this study, we present a novel approach for learning autonomous peg-in-hole assembly in the context of space robotics. Our focus is on enhancing the generalization and adaptability of autonomous systems through deep reinforcement learning. By integrating procedural generation and domain randomization, we train agents in a highly parallelized simulation environment across a spectrum of diverse scenarios with the aim of acquiring a robust policy. The proposed approach is evaluated using three distinct reinforcement learning algorithms to investigate the trade-offs among various paradigms. We demonstrate the adaptability of our agents to novel scenarios and assembly sequences while emphasizing the potential of leveraging advanced simulation techniques for robot learning in space. Our findings set the stage for future advancements in intelligent robotic systems capable of supporting ambitious space missions and infrastructure development beyond Earth.","sentences":["The ability to autonomously assemble structures is crucial for the development of future space infrastructure.","However, the unpredictable conditions of space pose significant challenges for robotic systems, necessitating the development of advanced learning techniques to enable autonomous assembly.","In this study, we present a novel approach for learning autonomous peg-in-hole assembly in the context of space robotics.","Our focus is on enhancing the generalization and adaptability of autonomous systems through deep reinforcement learning.","By integrating procedural generation and domain randomization, we train agents in a highly parallelized simulation environment across a spectrum of diverse scenarios with the aim of acquiring a robust policy.","The proposed approach is evaluated using three distinct reinforcement learning algorithms to investigate the trade-offs among various paradigms.","We demonstrate the adaptability of our agents to novel scenarios and assembly sequences while emphasizing the potential of leveraging advanced simulation techniques for robot learning in space.","Our findings set the stage for future advancements in intelligent robotic systems capable of supporting ambitious space missions and infrastructure development beyond Earth."],"url":"http://arxiv.org/abs/2405.01134v1","category":"cs.RO"}
{"created":"2024-05-02 09:44:13","title":"Automated Virtual Product Placement and Assessment in Images using Diffusion Models","abstract":"In Virtual Product Placement (VPP) applications, the discrete integration of specific brand products into images or videos has emerged as a challenging yet important task. This paper introduces a novel three-stage fully automated VPP system. In the first stage, a language-guided image segmentation model identifies optimal regions within images for product inpainting. In the second stage, Stable Diffusion (SD), fine-tuned with a few example product images, is used to inpaint the product into the previously identified candidate regions. The final stage introduces an \"Alignment Module\", which is designed to effectively sieve out low-quality images. Comprehensive experiments demonstrate that the Alignment Module ensures the presence of the intended product in every generated image and enhances the average quality of images by 35%. The results presented in this paper demonstrate the effectiveness of the proposed VPP system, which holds significant potential for transforming the landscape of virtual advertising and marketing strategies.","sentences":["In Virtual Product Placement (VPP) applications, the discrete integration of specific brand products into images or videos has emerged as a challenging yet important task.","This paper introduces a novel three-stage fully automated VPP system.","In the first stage, a language-guided image segmentation model identifies optimal regions within images for product inpainting.","In the second stage, Stable Diffusion (SD), fine-tuned with a few example product images, is used to inpaint the product into the previously identified candidate regions.","The final stage introduces an \"Alignment Module\", which is designed to effectively sieve out low-quality images.","Comprehensive experiments demonstrate that the Alignment Module ensures the presence of the intended product in every generated image and enhances the average quality of images by 35%.","The results presented in this paper demonstrate the effectiveness of the proposed VPP system, which holds significant potential for transforming the landscape of virtual advertising and marketing strategies."],"url":"http://arxiv.org/abs/2405.01130v1","category":"cs.CV"}
{"created":"2024-05-02 09:44:08","title":"Generic Torelli with denominators for elliptic surfaces","abstract":"We show that a very general Jacobian elliptic surface is determined by its polarized rational Hodge structure, subject to various constraints on the irregularity and the geometric genus.","sentences":["We show that a very general Jacobian elliptic surface is determined by its polarized rational Hodge structure, subject to various constraints on the irregularity and the geometric genus."],"url":"http://arxiv.org/abs/2405.01129v1","category":"math.AG"}
{"created":"2024-05-02 09:42:57","title":"A matter of performance & criticality: a review of rare-earth-based magnetocaloric intermetallic compounds for hydrogen liquefaction","abstract":"The low efficiency of conventional liquefaction technologies based on the Joule-Thomson expansion makes liquid hydrogen currently not attractive enough for large-scale energy-related technologies that are important for the transition to a carbon-neutral society. Magnetocaloric hydrogen liquefaction has great potential to achieve higher efficiency and is therefore a crucial enabler for affordable liquid hydrogen. Cost-effective magnetocaloric materials with large magnetic entropy and adiabatic temperature changes in the temperature range of 77 $\\sim$ 20 K under commercially practicable magnetic fields are the foundation for the success of magnetocaloric hydrogen liquefaction. Heavy rare-earth-based magnetocaloric intermetallic compounds generally show excellent magnetocaloric performances, but the heavy rare-earth elements (Gd, Tb, Dy, Ho, Er, and Tm) are highly critical in resources. Yttrium and light rare-earth elements (La, Ce, Pr, and Nd) are relatively abundant, but their alloys generally show less excellent magnetocaloric properties. A dilemma appears: higher performance or lower criticality? In this review, we study how cryogenic temperature influences magnetocaloric performance by first reviewing heavy rare-earth-based intermetallic compounds. Next, we look at light rare-earth-based, \"mixed\" rare-earth-based, and Gd-based intermetallic compounds with the nature of the phase transition order taken into consideration, and summarize ways to resolve the dilemma.","sentences":["The low efficiency of conventional liquefaction technologies based on the Joule-Thomson expansion makes liquid hydrogen currently not attractive enough for large-scale energy-related technologies that are important for the transition to a carbon-neutral society.","Magnetocaloric hydrogen liquefaction has great potential to achieve higher efficiency and is therefore a crucial enabler for affordable liquid hydrogen.","Cost-effective magnetocaloric materials with large magnetic entropy and adiabatic temperature changes in the temperature range of 77 $\\sim$ 20 K under commercially practicable magnetic fields are the foundation for the success of magnetocaloric hydrogen liquefaction.","Heavy rare-earth-based magnetocaloric intermetallic compounds generally show excellent magnetocaloric performances, but the heavy rare-earth elements (Gd, Tb, Dy, Ho, Er, and Tm) are highly critical in resources.","Yttrium and light rare-earth elements (La, Ce, Pr, and Nd) are relatively abundant, but their alloys generally show less excellent magnetocaloric properties.","A dilemma appears: higher performance or lower criticality?","In this review, we study how cryogenic temperature influences magnetocaloric performance by first reviewing heavy rare-earth-based intermetallic compounds.","Next, we look at light rare-earth-based, \"mixed\" rare-earth-based, and Gd-based intermetallic compounds with the nature of the phase transition order taken into consideration, and summarize ways to resolve the dilemma."],"url":"http://arxiv.org/abs/2405.01128v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 09:38:16","title":"Lipschitz constant estimation for general neural network architectures using control tools","abstract":"This paper is devoted to the estimation of the Lipschitz constant of neural networks using semidefinite programming. For this purpose, we interpret neural networks as time-varying dynamical systems, where the $k$-th layer corresponds to the dynamics at time $k$. A key novelty with respect to prior work is that we use this interpretation to exploit the series interconnection structure of neural networks with a dynamic programming recursion. Nonlinearities, such as activation functions and nonlinear pooling layers, are handled with integral quadratic constraints. If the neural network contains signal processing layers (convolutional or state space model layers), we realize them as 1-D/2-D/N-D systems and exploit this structure as well. We distinguish ourselves from related work on Lipschitz constant estimation by more extensive structure exploitation (scalability) and a generalization to a large class of common neural network architectures. To show the versatility and computational advantages of our method, we apply it to different neural network architectures trained on MNIST and CIFAR-10.","sentences":["This paper is devoted to the estimation of the Lipschitz constant of neural networks using semidefinite programming.","For this purpose, we interpret neural networks as time-varying dynamical systems, where the $k$-th layer corresponds to the dynamics at time $k$. A key novelty with respect to prior work is that we use this interpretation to exploit the series interconnection structure of neural networks with a dynamic programming recursion.","Nonlinearities, such as activation functions and nonlinear pooling layers, are handled with integral quadratic constraints.","If the neural network contains signal processing layers (convolutional or state space model layers), we realize them as 1-D/2-D/N-D systems and exploit this structure as well.","We distinguish ourselves from related work on Lipschitz constant estimation by more extensive structure exploitation (scalability) and a generalization to a large class of common neural network architectures.","To show the versatility and computational advantages of our method, we apply it to different neural network architectures trained on MNIST and CIFAR-10."],"url":"http://arxiv.org/abs/2405.01125v1","category":"cs.LG"}
{"created":"2024-05-02 09:36:00","title":"Generative Relevance Feedback and Convergence of Adaptive Re-Ranking: University of Glasgow Terrier Team at TREC DL 2023","abstract":"This paper describes our participation in the TREC 2023 Deep Learning Track. We submitted runs that apply generative relevance feedback from a large language model in both a zero-shot and pseudo-relevance feedback setting over two sparse retrieval approaches, namely BM25 and SPLADE. We couple this first stage with adaptive re-ranking over a BM25 corpus graph scored using a monoELECTRA cross-encoder. We investigate the efficacy of these generative approaches for different query types in first-stage retrieval. In re-ranking, we investigate operating points of adaptive re-ranking with different first stages to find the point in graph traversal where the first stage no longer has an effect on the performance of the overall retrieval pipeline. We find some performance gains from the application of generative query reformulation. However, our strongest run in terms of P@10 and nDCG@10 applied both adaptive re-ranking and generative pseudo-relevance feedback, namely uogtr_b_grf_e_gb.","sentences":["This paper describes our participation in the TREC 2023 Deep Learning Track.","We submitted runs that apply generative relevance feedback from a large language model in both a zero-shot and pseudo-relevance feedback setting over two sparse retrieval approaches, namely BM25 and SPLADE.","We couple this first stage with adaptive re-ranking over a BM25 corpus graph scored using a monoELECTRA cross-encoder.","We investigate the efficacy of these generative approaches for different query types in first-stage retrieval.","In re-ranking, we investigate operating points of adaptive re-ranking with different first stages to find the point in graph traversal where the first stage no longer has an effect on the performance of the overall retrieval pipeline.","We find some performance gains from the application of generative query reformulation.","However, our strongest run in terms of P@10 and nDCG@10 applied both adaptive re-ranking and generative pseudo-relevance feedback, namely uogtr_b_grf_e_gb."],"url":"http://arxiv.org/abs/2405.01122v1","category":"cs.IR"}
{"created":"2024-05-02 09:35:06","title":"Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts","abstract":"Existing methods for creating source-grounded information-seeking dialog datasets are often costly and hard to implement due to their sole reliance on human annotators. We propose combining large language models (LLMs) prompting with human expertise for more efficient and reliable data generation. Instead of the labor-intensive Wizard-of-Oz (WOZ) method, where two annotators generate a dialog from scratch, role-playing agent and user, we use LLM generation to simulate the two roles. Annotators then verify the output and augment it with attribution data. We demonstrate our method by constructing MISeD -- Meeting Information Seeking Dialogs dataset -- the first information-seeking dialog dataset focused on meeting transcripts. Models finetuned with MISeD demonstrate superior performance on our test set, as well as on a novel fully-manual WOZ test set and an existing query-based summarization benchmark, suggesting the utility of our approach.","sentences":["Existing methods for creating source-grounded information-seeking dialog datasets are often costly and hard to implement due to their sole reliance on human annotators.","We propose combining large language models (LLMs) prompting with human expertise for more efficient and reliable data generation.","Instead of the labor-intensive Wizard-of-Oz (WOZ) method, where two annotators generate a dialog from scratch, role-playing agent and user, we use LLM generation to simulate the two roles.","Annotators then verify the output and augment it with attribution data.","We demonstrate our method by constructing MISeD -- Meeting Information Seeking Dialogs dataset -- the first information-seeking dialog dataset focused on meeting transcripts.","Models finetuned with MISeD demonstrate superior performance on our test set, as well as on a novel fully-manual WOZ test set and an existing query-based summarization benchmark, suggesting the utility of our approach."],"url":"http://arxiv.org/abs/2405.01121v1","category":"cs.CL"}
{"created":"2024-05-02 09:30:08","title":"Towards Understanding Worldwide Cross-cultural Differences in Implicit Driving Cues: Review, Comparative Analysis, and Research Roadmap","abstract":"Recognizing and understanding implicit driving cues across diverse cultures is imperative for fostering safe and efficient global transportation systems, particularly when training new immigrants holding driving licenses from culturally disparate countries. Additionally, it is essential to consider cross-cultural differences in the development of Automated Driving features tailored to different countries. Previous piloting studies have compared and analyzed cross-cultural differences in selected implicit driving cues, but they typically examine only limited countries. However, a comprehensive worldwide comparison and analysis are lacking. This study conducts a thorough review of existing literature, online blogs, and expert insights from diverse countries to investigate cross-cultural disparities in driving behaviors, specifically focusing on implicit cues such as non-verbal communication (e.g., hand gestures, signal lighting, honking), norms, and social expectations. Through comparative analysis, variations in driving cues are illuminated across different cultural contexts. Based on the findings and identified gaps, a research roadmap is proposed for future research to further explore and address these differences, aiming to enhance intercultural communication, improve road safety, and increase transportation efficiency on a global scale. This paper presents the pioneering work towards a comprehensive understanding of the implicit driving cues across cultures. Moreover, this understanding will inform the development of automated driving systems tailored to different countries considering cross-cultural differences.","sentences":["Recognizing and understanding implicit driving cues across diverse cultures is imperative for fostering safe and efficient global transportation systems, particularly when training new immigrants holding driving licenses from culturally disparate countries.","Additionally, it is essential to consider cross-cultural differences in the development of Automated Driving features tailored to different countries.","Previous piloting studies have compared and analyzed cross-cultural differences in selected implicit driving cues, but they typically examine only limited countries.","However, a comprehensive worldwide comparison and analysis are lacking.","This study conducts a thorough review of existing literature, online blogs, and expert insights from diverse countries to investigate cross-cultural disparities in driving behaviors, specifically focusing on implicit cues such as non-verbal communication (e.g., hand gestures, signal lighting, honking), norms, and social expectations.","Through comparative analysis, variations in driving cues are illuminated across different cultural contexts.","Based on the findings and identified gaps, a research roadmap is proposed for future research to further explore and address these differences, aiming to enhance intercultural communication, improve road safety, and increase transportation efficiency on a global scale.","This paper presents the pioneering work towards a comprehensive understanding of the implicit driving cues across cultures.","Moreover, this understanding will inform the development of automated driving systems tailored to different countries considering cross-cultural differences."],"url":"http://arxiv.org/abs/2405.01119v1","category":"cs.SI"}
{"created":"2024-05-02 09:26:30","title":"Faster Learned Sparse Retrieval with Block-Max Pruning","abstract":"Learned sparse retrieval systems aim to combine the effectiveness of contextualized language models with the scalability of conventional data structures such as inverted indexes. Nevertheless, the indexes generated by these systems exhibit significant deviations from the ones that use traditional retrieval models, leading to a discrepancy in the performance of existing query optimizations that were specifically developed for traditional structures. These disparities arise from structural variations in query and document statistics, including sub-word tokenization, leading to longer queries, smaller vocabularies, and different score distributions within posting lists. This paper introduces Block-Max Pruning (BMP), an innovative dynamic pruning strategy tailored for indexes arising in learned sparse retrieval environments. BMP employs a block filtering mechanism to divide the document space into small, consecutive document ranges, which are then aggregated and sorted on the fly, and fully processed only as necessary, guided by a defined safe early termination criterion or based on approximate retrieval requirements. Through rigorous experimentation, we show that BMP substantially outperforms existing dynamic pruning strategies, offering unparalleled efficiency in safe retrieval contexts and improved tradeoffs between precision and efficiency in approximate retrieval tasks.","sentences":["Learned sparse retrieval systems aim to combine the effectiveness of contextualized language models with the scalability of conventional data structures such as inverted indexes.","Nevertheless, the indexes generated by these systems exhibit significant deviations from the ones that use traditional retrieval models, leading to a discrepancy in the performance of existing query optimizations that were specifically developed for traditional structures.","These disparities arise from structural variations in query and document statistics, including sub-word tokenization, leading to longer queries, smaller vocabularies, and different score distributions within posting lists.","This paper introduces Block-Max Pruning (BMP), an innovative dynamic pruning strategy tailored for indexes arising in learned sparse retrieval environments.","BMP employs a block filtering mechanism to divide the document space into small, consecutive document ranges, which are then aggregated and sorted on the fly, and fully processed only as necessary, guided by a defined safe early termination criterion or based on approximate retrieval requirements.","Through rigorous experimentation, we show that BMP substantially outperforms existing dynamic pruning strategies, offering unparalleled efficiency in safe retrieval contexts and improved tradeoffs between precision and efficiency in approximate retrieval tasks."],"url":"http://arxiv.org/abs/2405.01117v1","category":"cs.IR"}
{"created":"2024-05-02 09:25:24","title":"\"In-Context Learning\" or: How I learned to stop worrying and love \"Applied Information Retrieval\"","abstract":"With the increasing ability of large language models (LLMs), in-context learning (ICL) has evolved as a new paradigm for natural language processing (NLP), where instead of fine-tuning the parameters of an LLM specific to a downstream task with labeled examples, a small number of such examples is appended to a prompt instruction for controlling the decoder's generation process. ICL, thus, is conceptually similar to a non-parametric approach, such as $k$-NN, where the prediction for each instance essentially depends on the local topology, i.e., on a localised set of similar instances and their labels (called few-shot examples). This suggests that a test instance in ICL is analogous to a query in IR, and similar examples in ICL retrieved from a training set relate to a set of documents retrieved from a collection in IR. While standard unsupervised ranking models can be used to retrieve these few-shot examples from a training set, the effectiveness of the examples can potentially be improved by re-defining the notion of relevance specific to its utility for the downstream task, i.e., considering an example to be relevant if including it in the prompt instruction leads to a correct prediction. With this task-specific notion of relevance, it is possible to train a supervised ranking model (e.g., a bi-encoder or cross-encoder), which potentially learns to optimally select the few-shot examples. We believe that the recent advances in neural rankers can potentially find a use case for this task of optimally choosing examples for more effective downstream ICL predictions.","sentences":["With the increasing ability of large language models (LLMs), in-context learning (ICL) has evolved as a new paradigm for natural language processing (NLP), where instead of fine-tuning the parameters of an LLM specific to a downstream task with labeled examples, a small number of such examples is appended to a prompt instruction for controlling the decoder's generation process.","ICL, thus, is conceptually similar to a non-parametric approach, such as $k$-NN, where the prediction for each instance essentially depends on the local topology, i.e., on a localised set of similar instances and their labels (called few-shot examples).","This suggests that a test instance in ICL is analogous to a query in IR, and similar examples in ICL retrieved from a training set relate to a set of documents retrieved from a collection in IR.","While standard unsupervised ranking models can be used to retrieve these few-shot examples from a training set, the effectiveness of the examples can potentially be improved by re-defining the notion of relevance specific to its utility for the downstream task, i.e., considering an example to be relevant if including it in the prompt instruction leads to a correct prediction.","With this task-specific notion of relevance, it is possible to train a supervised ranking model (e.g., a bi-encoder or cross-encoder), which potentially learns to optimally select the few-shot examples.","We believe that the recent advances in neural rankers can potentially find a use case for this task of optimally choosing examples for more effective downstream ICL predictions."],"url":"http://arxiv.org/abs/2405.01116v1","category":"cs.IR"}
{"created":"2024-05-02 09:21:10","title":"Domain-Transferred Synthetic Data Generation for Improving Monocular Depth Estimation","abstract":"A major obstacle to the development of effective monocular depth estimation algorithms is the difficulty in obtaining high-quality depth data that corresponds to collected RGB images. Collecting this data is time-consuming and costly, and even data collected by modern sensors has limited range or resolution, and is subject to inconsistencies and noise. To combat this, we propose a method of data generation in simulation using 3D synthetic environments and CycleGAN domain transfer. We compare this method of data generation to the popular NYUDepth V2 dataset by training a depth estimation model based on the DenseDepth structure using different training sets of real and simulated data. We evaluate the performance of the models on newly collected images and LiDAR depth data from a Husky robot to verify the generalizability of the approach and show that GAN-transformed data can serve as an effective alternative to real-world data, particularly in depth estimation.","sentences":["A major obstacle to the development of effective monocular depth estimation algorithms is the difficulty in obtaining high-quality depth data that corresponds to collected RGB images.","Collecting this data is time-consuming and costly, and even data collected by modern sensors has limited range or resolution, and is subject to inconsistencies and noise.","To combat this, we propose a method of data generation in simulation using 3D synthetic environments and CycleGAN domain transfer.","We compare this method of data generation to the popular NYUDepth V2 dataset by training a depth estimation model based on the DenseDepth structure using different training sets of real and simulated data.","We evaluate the performance of the models on newly collected images and LiDAR depth data from a Husky robot to verify the generalizability of the approach and show that GAN-transformed data can serve as an effective alternative to real-world data, particularly in depth estimation."],"url":"http://arxiv.org/abs/2405.01113v1","category":"cs.CV"}
{"created":"2024-05-02 09:17:32","title":"Hypergraph $p$-Laplacian regularization on point clouds for data interpolation","abstract":"As a generalization of graphs, hypergraphs are widely used to model higher-order relations in data. This paper explores the benefit of the hypergraph structure for the interpolation of point cloud data that contain no explicit structural information. We define the $\\varepsilon_n$-ball hypergraph and the $k_n$-nearest neighbor hypergraph on a point cloud and study the $p$-Laplacian regularization on the hypergraphs. We prove the variational consistency between the hypergraph $p$-Laplacian regularization and the continuum $p$-Laplacian regularization in a semisupervised setting when the number of points $n$ goes to infinity while the number of labeled points remains fixed. A key improvement compared to the graph case is that the results rely on weaker assumptions on the upper bound of $\\varepsilon_n$ and $k_n$. To solve the convex but non-differentiable large-scale optimization problem, we utilize the stochastic primal-dual hybrid gradient algorithm. Numerical experiments on data interpolation verify that the hypergraph $p$-Laplacian regularization outperforms the graph $p$-Laplacian regularization in preventing the development of spikes at the labeled points.","sentences":["As a generalization of graphs, hypergraphs are widely used to model higher-order relations in data.","This paper explores the benefit of the hypergraph structure for the interpolation of point cloud data that contain no explicit structural information.","We define the $\\varepsilon_n$-ball hypergraph and the $k_n$-nearest neighbor hypergraph on a point cloud and study the $p$-Laplacian regularization on the hypergraphs.","We prove the variational consistency between the hypergraph $p$-Laplacian regularization and the continuum $p$-Laplacian regularization in a semisupervised setting when the number of points $n$ goes to infinity while the number of labeled points remains fixed.","A key improvement compared to the graph case is that the results rely on weaker assumptions on the upper bound of $\\varepsilon_n$ and $k_n$. To solve the convex but non-differentiable large-scale optimization problem, we utilize the stochastic primal-dual hybrid gradient algorithm.","Numerical experiments on data interpolation verify that the hypergraph $p$-Laplacian regularization outperforms the graph $p$-Laplacian regularization in preventing the development of spikes at the labeled points."],"url":"http://arxiv.org/abs/2405.01109v1","category":"math.NA"}
{"created":"2024-05-02 09:14:59","title":"Federated Learning with Heterogeneous Data Handling for Robust Vehicular Object Detection","abstract":"In the pursuit of refining precise perception models for fully autonomous driving, continual online model training becomes essential. Federated Learning (FL) within vehicular networks offers an efficient mechanism for model training while preserving raw sensory data integrity. Yet, FL struggles with non-identically distributed data (e.g., quantity skew), leading to suboptimal convergence rates during model training. In previous work, we introduced FedLA, an innovative Label-Aware aggregation method addressing data heterogeneity in FL for generic scenarios.   In this paper, we introduce FedProx+LA, a novel FL method building upon the state-of-the-art FedProx and FedLA to tackle data heterogeneity, which is specifically tailored for vehicular networks. We evaluate the efficacy of FedProx+LA in continuous online object detection model training. Through a comparative analysis against conventional and state-of-the-art methods, our findings reveal the superior convergence rate of FedProx+LA. Notably, if the label distribution is very heterogeneous, our FedProx+LA approach shows substantial improvements in detection performance compared to baseline methods, also outperforming our previous FedLA approach. Moreover, both FedLA and FedProx+LA increase convergence speed by 30% compared to baseline methods.","sentences":["In the pursuit of refining precise perception models for fully autonomous driving, continual online model training becomes essential.","Federated Learning (FL) within vehicular networks offers an efficient mechanism for model training while preserving raw sensory data integrity.","Yet, FL struggles with non-identically distributed data (e.g., quantity skew), leading to suboptimal convergence rates during model training.","In previous work, we introduced FedLA, an innovative Label-Aware aggregation method addressing data heterogeneity in FL for generic scenarios.   ","In this paper, we introduce FedProx+LA, a novel FL method building upon the state-of-the-art FedProx and FedLA to tackle data heterogeneity, which is specifically tailored for vehicular networks.","We evaluate the efficacy of FedProx+LA in continuous online object detection model training.","Through a comparative analysis against conventional and state-of-the-art methods, our findings reveal the superior convergence rate of FedProx+LA.","Notably, if the label distribution is very heterogeneous, our FedProx+LA approach shows substantial improvements in detection performance compared to baseline methods, also outperforming our previous FedLA approach.","Moreover, both FedLA and FedProx+LA increase convergence speed by 30% compared to baseline methods."],"url":"http://arxiv.org/abs/2405.01108v1","category":"cs.CV"}
{"created":"2024-05-02 09:14:40","title":"High-dimensional families of holomorphic curves and three-dimensional energy surfaces","abstract":"Let $H: \\mathbb{R}^4 \\to \\mathbb{R}$ be any smooth function. This article introduces some arguments for extracting dynamical information about the Hamiltonian flow of $H$ from high-dimensional families of closed holomorphic curves. We work in a very general setting, without imposing convexity or contact-type assumptions. For any compact regular level set $Y$, we prove that the Hamiltonian flow admits an infinite family of pairwise distinct, proper, compact invariant subsets whose union is dense in $Y$. This is a generalization of the Fish-Hofer theorem, which showed that $Y$ has at least one proper compact invariant subset. We then establish a global Le Calvez-Yoccoz property for almost every compact regular level set $Y$: any compact invariant subset containing all closed orbits is either equal to $Y$ or is not locally maximal. Next, we prove quantitative versions, in four dimensions, of the celebrated almost-existence theorem for Hamiltonian systems; such questions have been open for general Hamiltonians since the late $1980$s. We prove that almost every compact regular level set of $H$ contains at least two closed orbits, a sharp lower bound. Under explicit and $C^\\infty$-generic conditions on $H$, we prove almost-existence of infinitely many closed orbits.","sentences":["Let $H: \\mathbb{R}^4 \\to \\mathbb{R}$ be any smooth function.","This article introduces some arguments for extracting dynamical information about the Hamiltonian flow of $H$ from high-dimensional families of closed holomorphic curves.","We work in a very general setting, without imposing convexity or contact-type assumptions.","For any compact regular level set $Y$, we prove that the Hamiltonian flow admits an infinite family of pairwise distinct, proper, compact invariant subsets whose union is dense in $Y$. This is a generalization of the Fish-Hofer theorem, which showed that $Y$ has at least one proper compact invariant subset.","We then establish a global Le Calvez-Yoccoz property for almost every compact regular level set $Y$: any compact invariant subset containing all closed orbits is either equal to $Y$ or is not locally maximal.","Next, we prove quantitative versions, in four dimensions, of the celebrated almost-existence theorem for Hamiltonian systems; such questions have been open for general Hamiltonians since the late $1980$s.","We prove that almost every compact regular level set of $H$ contains at least two closed orbits, a sharp lower bound.","Under explicit and $C^\\infty$-generic conditions on $H$, we prove almost-existence of infinitely many closed orbits."],"url":"http://arxiv.org/abs/2405.01106v1","category":"math.SG"}
{"created":"2024-05-02 09:13:32","title":"Multi-user ISAC through Stacked Intelligent Metasurfaces: New Algorithms and Experiments","abstract":"This paper investigates a Stacked Intelligent Metasurfaces (SIM)-assisted Integrated Sensing and Communications (ISAC) system. An extended target model is considered, where the BS aims to estimate the complete target response matrix relative to the SIM. Under the constraints of minimum Signal-to-Interference-plus-Noise Ratio (SINR) for the communication users (CUs) and maximum transmit power, we jointly optimize the transmit beamforming at the base station (BS) and the end-to-end transmission matrix of the SIM, to minimize the Cram\\'er-Rao Bound (CRB) for target estimation. Effective algorithms such as the alternating optimization (AO) and semidefinite relaxation (SDR) are employed to solve the non-convex SINR-constrained CRB minimization problem. Finally, we design and build an experimental platform for SIM, and evaluate the performance of the proposed algorithms for communication and sensing tasks.","sentences":["This paper investigates a Stacked Intelligent Metasurfaces (SIM)-assisted Integrated Sensing and Communications (ISAC) system.","An extended target model is considered, where the BS aims to estimate the complete target response matrix relative to the SIM.","Under the constraints of minimum Signal-to-Interference-plus-Noise Ratio (SINR) for the communication users (CUs) and maximum transmit power, we jointly optimize the transmit beamforming at the base station (BS) and the end-to-end transmission matrix of the SIM, to minimize the Cram\\'er-Rao Bound (CRB) for target estimation.","Effective algorithms such as the alternating optimization (AO) and semidefinite relaxation (SDR) are employed to solve the non-convex SINR-constrained CRB minimization problem.","Finally, we design and build an experimental platform for SIM, and evaluate the performance of the proposed algorithms for communication and sensing tasks."],"url":"http://arxiv.org/abs/2405.01104v1","category":"cs.IT"}
{"created":"2024-05-02 09:13:01","title":"LLM Security Guard for Code","abstract":"Many developers rely on Large Language Models (LLMs) to facilitate software development. Nevertheless, these models have exhibited limited capabilities in the security domain. We introduce LLMSecGuard, an open-source framework that offers enhanced code security through the synergy between static code analyzers and LLMs. LLMSecGuard aims to equip practitioners with code solutions that are more secure than the code initially generated by LLMs. It also benchmarks LLMs, providing valuable insights into the evolving security properties of these models.","sentences":["Many developers rely on Large Language Models (LLMs) to facilitate software development.","Nevertheless, these models have exhibited limited capabilities in the security domain.","We introduce LLMSecGuard, an open-source framework that offers enhanced code security through the synergy between static code analyzers and LLMs.","LLMSecGuard aims to equip practitioners with code solutions that are more secure than the code initially generated by LLMs.","It also benchmarks LLMs, providing valuable insights into the evolving security properties of these models."],"url":"http://arxiv.org/abs/2405.01103v1","category":"cs.SE"}
{"created":"2024-05-02 09:12:22","title":"Less is More: on the Over-Globalizing Problem in Graph Transformers","abstract":"Graph Transformer, due to its global attention mechanism, has emerged as a new tool in dealing with graph-structured data. It is well recognized that the global attention mechanism considers a wider receptive field in a fully connected graph, leading many to believe that useful information can be extracted from all the nodes. In this paper, we challenge this belief: does the globalizing property always benefit Graph Transformers? We reveal the over-globalizing problem in Graph Transformer by presenting both empirical evidence and theoretical analysis, i.e., the current attention mechanism overly focuses on those distant nodes, while the near nodes, which actually contain most of the useful information, are relatively weakened. Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes. Moreover, the collaborative training is proposed to improve the model's generalization ability with a theoretical guarantee. Extensive experiments on various graphs well validate the effectiveness of our proposed CoBFormer.","sentences":["Graph Transformer, due to its global attention mechanism, has emerged as a new tool in dealing with graph-structured data.","It is well recognized that the global attention mechanism considers a wider receptive field in a fully connected graph, leading many to believe that useful information can be extracted from all the nodes.","In this paper, we challenge this belief: does the globalizing property always benefit Graph Transformers?","We reveal the over-globalizing problem in Graph Transformer by presenting both empirical evidence and theoretical analysis, i.e., the current attention mechanism overly focuses on those distant nodes, while the near nodes, which actually contain most of the useful information, are relatively weakened.","Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes.","Moreover, the collaborative training is proposed to improve the model's generalization ability with a theoretical guarantee.","Extensive experiments on various graphs well validate the effectiveness of our proposed CoBFormer."],"url":"http://arxiv.org/abs/2405.01102v1","category":"cs.LG"}
{"created":"2024-05-02 09:09:48","title":"Enhancing Person Re-Identification via Uncertainty Feature Fusion and Wise Distance Aggregation","abstract":"The quest for robust Person re-identification (Re-ID) systems capable of accurately identifying subjects across diverse scenarios remains a formidable challenge in surveillance and security applications. This study presents a novel methodology that significantly enhances Person Re-Identification (Re-ID) by integrating Uncertainty Feature Fusion (UFFM) with Wise Distance Aggregation (WDA). Tested on benchmark datasets - Market-1501, DukeMTMC-ReID, and MSMT17 - our approach demonstrates substantial improvements in Rank-1 accuracy and mean Average Precision (mAP). Specifically, UFFM capitalizes on the power of feature synthesis from multiple images to overcome the limitations imposed by the variability of subject appearances across different views. WDA further refines the process by intelligently aggregating similarity metrics, thereby enhancing the system's ability to discern subtle but critical differences between subjects. The empirical results affirm the superiority of our method over existing approaches, achieving new performance benchmarks across all evaluated datasets. Code is available on Github.","sentences":["The quest for robust Person re-identification (Re-ID) systems capable of accurately identifying subjects across diverse scenarios remains a formidable challenge in surveillance and security applications.","This study presents a novel methodology that significantly enhances Person Re-Identification (Re-ID) by integrating Uncertainty Feature Fusion (UFFM) with Wise Distance Aggregation (WDA).","Tested on benchmark datasets - Market-1501, DukeMTMC-ReID, and MSMT17 - our approach demonstrates substantial improvements in Rank-1 accuracy and mean Average Precision (mAP).","Specifically, UFFM capitalizes on the power of feature synthesis from multiple images to overcome the limitations imposed by the variability of subject appearances across different views.","WDA further refines the process by intelligently aggregating similarity metrics, thereby enhancing the system's ability to discern subtle but critical differences between subjects.","The empirical results affirm the superiority of our method over existing approaches, achieving new performance benchmarks across all evaluated datasets.","Code is available on Github."],"url":"http://arxiv.org/abs/2405.01101v1","category":"cs.CV"}
{"created":"2024-05-02 08:57:34","title":"Development of a Bi-solvent Liquid Scintillator with Slow Light Emission","abstract":"One of the most promising approaches for the next generation of neutrino experiments is the realization of large hybrid Cherenkov/scintillation detectors made possible by recent innovations in photodetection technology and liquid scintillator chemistry. The development of a potentially suitable future detector liquid with particularly slow light emission is discussed in the present publication. This cocktail is compared with respect to its fundamental characteristics (scintillation efficiency, transparency, and time profile of light emission) with liquid scintillators currently used in large-scale neutrino detectors. In addition, the optimization of the admixture of wavelength shifters for a scintillator with particularly high light emission is presented. Furthermore, the pulse-shape discrimination capabilities of the novel medium was studied using a pulsed particle accelerator driven neutron source. Beyond that, purification methods based on column chromatography and fractional vacuum distillation for the co-solvent DIN (Diisopropylnaphthalene) are discussed.","sentences":["One of the most promising approaches for the next generation of neutrino experiments is the realization of large hybrid Cherenkov/scintillation detectors made possible by recent innovations in photodetection technology and liquid scintillator chemistry.","The development of a potentially suitable future detector liquid with particularly slow light emission is discussed in the present publication.","This cocktail is compared with respect to its fundamental characteristics (scintillation efficiency, transparency, and time profile of light emission) with liquid scintillators currently used in large-scale neutrino detectors.","In addition, the optimization of the admixture of wavelength shifters for a scintillator with particularly high light emission is presented.","Furthermore, the pulse-shape discrimination capabilities of the novel medium was studied using a pulsed particle accelerator driven neutron source.","Beyond that, purification methods based on column chromatography and fractional vacuum distillation for the co-solvent DIN (Diisopropylnaphthalene) are discussed."],"url":"http://arxiv.org/abs/2405.01100v1","category":"physics.ins-det"}
{"created":"2024-05-02 08:57:03","title":"General synthetic iterative scheme for rarefied gas mixture flows","abstract":"The numerical simulation of rarefied gas mixtures with disparate mass and concentration is a huge research challenge. Based on our recent kinetic modelling for monatomic gas mixture flows, this problem is tackled by the general synthetic iterative scheme (GSIS), where the mesoscopic kinetic and macroscopic synthetic equations are alternately solved by the finite-volume discrete velocity method. Three important features of GSIS are highlighted. First, the synthetic equations are precisely derived from the kinetic equation, naturally reducing to the Navier-Stokes equations in the continuum flow regime; in other flow regimes, the kinetic equation provides high-order closure of the constitutive relations to capture the rarefaction effects. Second, these synthetic equations, which can be solved quickly, help to adjust the kinetic system to relax rapidly toward the steady state. Furthermore, in such a two-way coupling, the constraint on the spatial cell size is relieved. Third, the linear Fourier stability analysis demonstrates that the error decay rate in GSIS is smaller than 0.5 for various combinations of mass, concentration and viscosity ratios, such that the error can be reduced by three orders of magnitude after 10 iterations. The efficiency and accuracy of GSIS are demonstrated through several challenging cases covering a wide range of mass ratio, species concentration, and flow speed.","sentences":["The numerical simulation of rarefied gas mixtures with disparate mass and concentration is a huge research challenge.","Based on our recent kinetic modelling for monatomic gas mixture flows, this problem is tackled by the general synthetic iterative scheme (GSIS), where the mesoscopic kinetic and macroscopic synthetic equations are alternately solved by the finite-volume discrete velocity method.","Three important features of GSIS are highlighted.","First, the synthetic equations are precisely derived from the kinetic equation, naturally reducing to the Navier-Stokes equations in the continuum flow regime; in other flow regimes, the kinetic equation provides high-order closure of the constitutive relations to capture the rarefaction effects.","Second, these synthetic equations, which can be solved quickly, help to adjust the kinetic system to relax rapidly toward the steady state.","Furthermore, in such a two-way coupling, the constraint on the spatial cell size is relieved.","Third, the linear Fourier stability analysis demonstrates that the error decay rate in GSIS is smaller than 0.5 for various combinations of mass, concentration and viscosity ratios, such that the error can be reduced by three orders of magnitude after 10 iterations.","The efficiency and accuracy of GSIS are demonstrated through several challenging cases covering a wide range of mass ratio, species concentration, and flow speed."],"url":"http://arxiv.org/abs/2405.01099v1","category":"physics.comp-ph"}
{"created":"2024-05-02 08:54:28","title":"Multivariate trace estimation using quantum state space linear algebra","abstract":"In this paper, we present a quantum algorithm for approximating multivariate traces, i.e. the traces of matrix products. Our research is motivated by the extensive utility of multivariate traces in elucidating spectral characteristics of matrices, as well as by recent advancements in leveraging quantum computing for faster numerical linear algebra. Central to our approach is a direct translation of a multivariate trace formula into a quantum circuit, achieved through a sequence of low-level circuit construction operations. To facilitate this translation, we introduce \\emph{quantum Matrix States Linear Algebra} (qMSLA), a framework tailored for the efficient generation of state preparation circuits via primitive matrix algebra operations. Our algorithm relies on sets of state preparation circuits for input matrices as its primary inputs and yields two state preparation circuits encoding the multivariate trace as output. These circuits are constructed utilizing qMSLA operations, which enact the aforementioned multivariate trace formula. We emphasize that our algorithm's inputs consist solely of state preparation circuits, eschewing harder to synthesize constructs such as Block Encodings. Furthermore, our approach operates independently of the availability of specialized hardware like QRAM, underscoring its versatility and practicality.","sentences":["In this paper, we present a quantum algorithm for approximating multivariate traces, i.e. the traces of matrix products.","Our research is motivated by the extensive utility of multivariate traces in elucidating spectral characteristics of matrices, as well as by recent advancements in leveraging quantum computing for faster numerical linear algebra.","Central to our approach is a direct translation of a multivariate trace formula into a quantum circuit, achieved through a sequence of low-level circuit construction operations.","To facilitate this translation, we introduce \\emph{quantum Matrix States Linear Algebra} (qMSLA), a framework tailored for the efficient generation of state preparation circuits via primitive matrix algebra operations.","Our algorithm relies on sets of state preparation circuits for input matrices as its primary inputs and yields two state preparation circuits encoding the multivariate trace as output.","These circuits are constructed utilizing qMSLA operations, which enact the aforementioned multivariate trace formula.","We emphasize that our algorithm's inputs consist solely of state preparation circuits, eschewing harder to synthesize constructs such as Block Encodings.","Furthermore, our approach operates independently of the availability of specialized hardware like QRAM, underscoring its versatility and practicality."],"url":"http://arxiv.org/abs/2405.01098v1","category":"quant-ph"}
{"created":"2024-05-02 08:47:56","title":"Superquantization rule for multistability in driven-dissipative quantum systems","abstract":"We present a superquantization rule which indicates the possible robust stationary states of a generic driven-dissipative quantum system. Multistability in a driven cavity mode interacting with a qudit is revealed in this way within a simple intuitive picture. The accuracy of the superquantization approach is confirmed by numerical simulations of the underlying quantum model. In the case when the qudit is composed of several two-level emitters coupled homogeneously to the cavity, we demonstrate the robustness of the superquantized steady states to single-emitter decay.","sentences":["We present a superquantization rule which indicates the possible robust stationary states of a generic driven-dissipative quantum system.","Multistability in a driven cavity mode interacting with a qudit is revealed in this way within a simple intuitive picture.","The accuracy of the superquantization approach is confirmed by numerical simulations of the underlying quantum model.","In the case when the qudit is composed of several two-level emitters coupled homogeneously to the cavity, we demonstrate the robustness of the superquantized steady states to single-emitter decay."],"url":"http://arxiv.org/abs/2405.01093v1","category":"quant-ph"}
{"created":"2024-05-02 08:43:16","title":"Learning Object States from Actions via Large Language Models","abstract":"Temporally localizing the presence of object states in videos is crucial in understanding human activities beyond actions and objects. This task has suffered from a lack of training data due to object states' inherent ambiguity and variety. To avoid exhaustive annotation, learning from transcribed narrations in instructional videos would be intriguing. However, object states are less described in narrations compared to actions, making them less effective. In this work, we propose to extract the object state information from action information included in narrations, using large language models (LLMs). Our observation is that LLMs include world knowledge on the relationship between actions and their resulting object states, and can infer the presence of object states from past action sequences. The proposed LLM-based framework offers flexibility to generate plausible pseudo-object state labels against arbitrary categories. We evaluate our method with our newly collected Multiple Object States Transition (MOST) dataset including dense temporal annotation of 60 object state categories. Our model trained by the generated pseudo-labels demonstrates significant improvement of over 29% in mAP against strong zero-shot vision-language models, showing the effectiveness of explicitly extracting object state information from actions through LLMs.","sentences":["Temporally localizing the presence of object states in videos is crucial in understanding human activities beyond actions and objects.","This task has suffered from a lack of training data due to object states' inherent ambiguity and variety.","To avoid exhaustive annotation, learning from transcribed narrations in instructional videos would be intriguing.","However, object states are less described in narrations compared to actions, making them less effective.","In this work, we propose to extract the object state information from action information included in narrations, using large language models (LLMs).","Our observation is that LLMs include world knowledge on the relationship between actions and their resulting object states, and can infer the presence of object states from past action sequences.","The proposed LLM-based framework offers flexibility to generate plausible pseudo-object state labels against arbitrary categories.","We evaluate our method with our newly collected Multiple Object States Transition (MOST) dataset including dense temporal annotation of 60 object state categories.","Our model trained by the generated pseudo-labels demonstrates significant improvement of over 29% in mAP against strong zero-shot vision-language models, showing the effectiveness of explicitly extracting object state information from actions through LLMs."],"url":"http://arxiv.org/abs/2405.01090v1","category":"cs.CV"}
{"created":"2024-05-02 08:33:43","title":"Type2Branch: Keystroke Biometrics based on a Dual-branch Architecture with Attention Mechanisms and Set2set Loss","abstract":"In 2021, the pioneering work on TypeNet showed that keystroke dynamics verification could scale to hundreds of thousands of users with minimal performance degradation. Recently, the KVC-onGoing competition has provided an open and robust experimental protocol for evaluating keystroke dynamics verification systems of such scale, including considerations of algorithmic fairness. This article describes Type2Branch, the model and techniques that achieved the lowest error rates at the KVC-onGoing, in both desktop and mobile scenarios. The novelty aspects of the proposed Type2Branch include: i) synthesized timing features emphasizing user behavior deviation from the general population, ii) a dual-branch architecture combining recurrent and convolutional paths with various attention mechanisms, iii) a new loss function named Set2set that captures the global structure of the embedding space, and iv) a training curriculum of increasing difficulty. Considering five enrollment samples per subject of approximately 50 characters typed, the proposed Type2Branch achieves state-of-the-art performance with mean per-subject EERs of 0.77% and 1.03% on evaluation sets of respectively 15,000 and 5,000 subjects for desktop and mobile scenarios. With a uniform global threshold for all subjects, the EERs are 3.25% for desktop and 3.61% for mobile, outperforming previous approaches by a significant margin.","sentences":["In 2021, the pioneering work on TypeNet showed that keystroke dynamics verification could scale to hundreds of thousands of users with minimal performance degradation.","Recently, the KVC-onGoing competition has provided an open and robust experimental protocol for evaluating keystroke dynamics verification systems of such scale, including considerations of algorithmic fairness.","This article describes Type2Branch, the model and techniques that achieved the lowest error rates at the KVC-onGoing, in both desktop and mobile scenarios.","The novelty aspects of the proposed Type2Branch include: i) synthesized timing features emphasizing user behavior deviation from the general population, ii) a dual-branch architecture combining recurrent and convolutional paths with various attention mechanisms, iii) a new loss function named Set2set that captures the global structure of the embedding space, and iv) a training curriculum of increasing difficulty.","Considering five enrollment samples per subject of approximately 50 characters typed, the proposed Type2Branch achieves state-of-the-art performance with mean per-subject EERs of 0.77% and 1.03% on evaluation sets of respectively 15,000 and 5,000 subjects for desktop and mobile scenarios.","With a uniform global threshold for all subjects, the EERs are 3.25% for desktop and 3.61% for mobile, outperforming previous approaches by a significant margin."],"url":"http://arxiv.org/abs/2405.01088v1","category":"cs.CV"}
{"created":"2024-05-02 08:15:16","title":"Does Financial Literacy Impact Investment Participation and Retirement Planning in Japan?","abstract":"By employing causal discovery method, the Fast Causal Inference (FCI) model to analyze data from the 2022 \"Financial Literacy Survey,\" we explore the causal relationships between financial literacy and financial activities, specifically investment participation and retirement planning. Our findings indicate that increasing financial literacy may not directly boost engagement in financial investments or retirement planning in Japan, which underscores the necessity for alternative strategies to motivate financial activities among Japanese households. This research offers valuable insights for policymakers focused on improving financial well-being by advancing the use of causal discovery algorithms in understanding financial behaviors.","sentences":["By employing causal discovery method, the Fast Causal Inference (FCI) model to analyze data from the 2022 \"Financial Literacy Survey,\" we explore the causal relationships between financial literacy and financial activities, specifically investment participation and retirement planning.","Our findings indicate that increasing financial literacy may not directly boost engagement in financial investments or retirement planning in Japan, which underscores the necessity for alternative strategies to motivate financial activities among Japanese households.","This research offers valuable insights for policymakers focused on improving financial well-being by advancing the use of causal discovery algorithms in understanding financial behaviors."],"url":"http://arxiv.org/abs/2405.01078v1","category":"econ.GN"}
{"created":"2024-05-02 08:14:43","title":"Generic signalling-free white-noise limit for models of spontaneous unitarity violation","abstract":"Objective collapse theories propose modifications to Schr\\\"odinger's equation that solve the quantum measurement problem by interpolating between microscopic quantum dynamics and projective evolution of macroscopic objects. Objective collapse theories extending the equilibrium description of spontaneous symmetry breaking to spontaneous violations of unitarity in quantum dynamics were recently shown to possess a physical white noise limit when applied to initial two-state superpositions. Here, we show the existence of a generic physical white noise limit for models of spontaneous unitarity violation applicable to any initial state. We show that in this limit, the emergence of Born rule statistics is enforced by a fluctuation-dissipation relation, and that the ensemble averaged probability densities follow the GKSL master equation corresponding to a linear quantum semi-group, guaranteeing the absence of superluminal signalling.","sentences":["Objective collapse theories propose modifications to Schr\\\"odinger's equation that solve the quantum measurement problem by interpolating between microscopic quantum dynamics and projective evolution of macroscopic objects.","Objective collapse theories extending the equilibrium description of spontaneous symmetry breaking to spontaneous violations of unitarity in quantum dynamics were recently shown to possess a physical white noise limit when applied to initial two-state superpositions.","Here, we show the existence of a generic physical white noise limit for models of spontaneous unitarity violation applicable to any initial state.","We show that in this limit, the emergence of Born rule statistics is enforced by a fluctuation-dissipation relation, and that the ensemble averaged probability densities follow the GKSL master equation corresponding to a linear quantum semi-group, guaranteeing the absence of superluminal signalling."],"url":"http://arxiv.org/abs/2405.01077v1","category":"quant-ph"}
{"created":"2024-05-02 08:06:10","title":"Poisoning Attacks on Federated Learning for Autonomous Driving","abstract":"Federated Learning (FL) is a decentralized learning paradigm, enabling parties to collaboratively train models while keeping their data confidential. Within autonomous driving, it brings the potential of reducing data storage costs, reducing bandwidth requirements, and to accelerate the learning. FL is, however, susceptible to poisoning attacks. In this paper, we introduce two novel poisoning attacks on FL tailored to regression tasks within autonomous driving: FLStealth and Off-Track Attack (OTA). FLStealth, an untargeted attack, aims at providing model updates that deteriorate the global model performance while appearing benign. OTA, on the other hand, is a targeted attack with the objective to change the global model's behavior when exposed to a certain trigger. We demonstrate the effectiveness of our attacks by conducting comprehensive experiments pertaining to the task of vehicle trajectory prediction. In particular, we show that, among five different untargeted attacks, FLStealth is the most successful at bypassing the considered defenses employed by the server. For OTA, we demonstrate the inability of common defense strategies to mitigate the attack, highlighting the critical need for new defensive mechanisms against targeted attacks within FL for autonomous driving.","sentences":["Federated Learning (FL) is a decentralized learning paradigm, enabling parties to collaboratively train models while keeping their data confidential.","Within autonomous driving, it brings the potential of reducing data storage costs, reducing bandwidth requirements, and to accelerate the learning.","FL is, however, susceptible to poisoning attacks.","In this paper, we introduce two novel poisoning attacks on FL tailored to regression tasks within autonomous driving: FLStealth and Off-Track Attack (OTA).","FLStealth, an untargeted attack, aims at providing model updates that deteriorate the global model performance while appearing benign.","OTA, on the other hand, is a targeted attack with the objective to change the global model's behavior when exposed to a certain trigger.","We demonstrate the effectiveness of our attacks by conducting comprehensive experiments pertaining to the task of vehicle trajectory prediction.","In particular, we show that, among five different untargeted attacks, FLStealth is the most successful at bypassing the considered defenses employed by the server.","For OTA, we demonstrate the inability of common defense strategies to mitigate the attack, highlighting the critical need for new defensive mechanisms against targeted attacks within FL for autonomous driving."],"url":"http://arxiv.org/abs/2405.01073v1","category":"cs.LG"}
{"created":"2024-05-02 08:04:36","title":"Statistical Inference on the Cumulative Distribution Function using Judgment Post Stratification","abstract":"In this work, we discuss a general class of the estimators for the cumulative distribution function (CDF) based on judgment post stratification (JPS) sampling scheme which includes both empirical and kernel distribution functions. Specifically, we obtain the expectation of the estimators in this class and show that they are asymptotically more efficient than their competitors in simple random sampling (SRS), as long as the rankings are better than random guessing. We find a mild condition that is necessary and sufficient for them to be asymptotically unbiased. We also prove that given the same condition, the estimators in this class are strongly uniformly consistent estimators of the true CDF, and converge in distribution to a normal distribution when the sample size goes to infinity.   We then focus on the kernel distribution function (KDF) in the JPS design and obtain the optimal bandwidth. We next carry out a comprehensive Monte Carlo simulation to compare the performance of the KDF in the JPS design for different choices of sample size, set size, ranking quality, parent distribution, kernel function as well as both perfect and imperfect rankings set-ups with its counterpart in SRS design. It is found that the JPS estimator dramatically improves the efficiency of the KDF as compared to its SRS competitor for a wide range of the settings. Finally, we apply the described procedure on a real dataset from medical context to show their usefulness and applicability in practice.","sentences":["In this work, we discuss a general class of the estimators for the cumulative distribution function (CDF) based on judgment post stratification (JPS) sampling scheme which includes both empirical and kernel distribution functions.","Specifically, we obtain the expectation of the estimators in this class and show that they are asymptotically more efficient than their competitors in simple random sampling (SRS), as long as the rankings are better than random guessing.","We find a mild condition that is necessary and sufficient for them to be asymptotically unbiased.","We also prove that given the same condition, the estimators in this class are strongly uniformly consistent estimators of the true CDF, and converge in distribution to a normal distribution when the sample size goes to infinity.   ","We then focus on the kernel distribution function (KDF) in the JPS design and obtain the optimal bandwidth.","We next carry out a comprehensive Monte Carlo simulation to compare the performance of the KDF in the JPS design for different choices of sample size, set size, ranking quality, parent distribution, kernel function as well as both perfect and imperfect rankings set-ups with its counterpart in SRS design.","It is found that the JPS estimator dramatically improves the efficiency of the KDF as compared to its SRS competitor for a wide range of the settings.","Finally, we apply the described procedure on a real dataset from medical context to show their usefulness and applicability in practice."],"url":"http://arxiv.org/abs/2405.01072v1","category":"stat.ME"}
{"created":"2024-05-02 08:03:18","title":"Callico: a Versatile Open-Source Document Image Annotation Platform","abstract":"This paper presents Callico, a web-based open source platform designed to simplify the annotation process in document recognition projects. The move towards data-centric AI in machine learning and deep learning underscores the importance of high-quality data, and the need for specialised tools that increase the efficiency and effectiveness of generating such data. For document image annotation, Callico offers dual-display annotation for digitised documents, enabling simultaneous visualisation and annotation of scanned images and text. This capability is critical for OCR and HTR model training, document layout analysis, named entity recognition, form-based key value annotation or hierarchical structure annotation with element grouping. The platform supports collaborative annotation with versatile features backed by a commitment to open source development, high-quality code standards and easy deployment via Docker. Illustrative use cases - including the transcription of the Belfort municipal registers, the indexing of French World War II prisoners for the ICRC, and the extraction of personal information from the Socface project's census lists - demonstrate Callico's applicability and utility.","sentences":["This paper presents Callico, a web-based open source platform designed to simplify the annotation process in document recognition projects.","The move towards data-centric AI in machine learning and deep learning underscores the importance of high-quality data, and the need for specialised tools that increase the efficiency and effectiveness of generating such data.","For document image annotation, Callico offers dual-display annotation for digitised documents, enabling simultaneous visualisation and annotation of scanned images and text.","This capability is critical for OCR and HTR model training, document layout analysis, named entity recognition, form-based key value annotation or hierarchical structure annotation with element grouping.","The platform supports collaborative annotation with versatile features backed by a commitment to open source development, high-quality code standards and easy deployment via Docker.","Illustrative use cases - including the transcription of the Belfort municipal registers, the indexing of French World War II prisoners for the ICRC, and the extraction of personal information from the Socface project's census lists - demonstrate Callico's applicability and utility."],"url":"http://arxiv.org/abs/2405.01071v1","category":"cs.CV"}
{"created":"2024-05-02 07:54:21","title":"A Low Temperature Kinetic Study of the C(3P) + CH3OCH3 Reaction. Rate constants, H-atom Product Yields and Astrochemical Implications","abstract":"Atomic carbon in its ground electronic state, C(3P), is expected to be present at high abundances during the evolution of dense molecular clouds. Consequently, its reactions with other interstellar species could have a strong influence on the chemical composition of these regions. Here, we report the results of an investigation of the reaction between C(3P) and dimethylether, CH3OCH3, which was recently detected in dark cloud TMC-1. Experiments were performed to study the kinetics of this reaction using a continuous supersonic flow reactor employing pulsed laser photolysis and pulsed laser induced fluorescence for atomic radical generation and detection respectively. Rate constants for this process were measured between 50 K and 296 K, while additional measurements of the product atomic hydrogen yields were also performed over the 75-296 K range. To better understand the experimental results, statistical rate theory was used to calculate rate constants over the same temperature range and to provide insight on the major product channels. These simulations, based on quantum chemical calculations of the ground triplet state of the C3H6O molecule, allowed us to obtain the most important features of the underlying potential energy surface. The measured rate constant increases as the temperature falls, reaching a value of k_(C+CH_3 OCH_3 )= 7.5 x 10-11 cm3 s-1 at 50 K, while the low measured H-atom yields support the theoretical prediction that the major reaction products are CH3 + CH3 + CO. The effects of this reaction on the abundances of interstellar CH3OCH3 and related species were tested using a gas-grain dense cloud model, employing an expression for the rate constant, k(T) = alpha(T/300)^beta, with alpha = 1.27 x 10-11 and beta = -1.01. These simulations predict that the C(3P) + CH3OCH3 reaction decreases gas-phase CH3OCH3 abundances by more than an order of magnitude at early times.","sentences":["Atomic carbon in its ground electronic state, C(3P), is expected to be present at high abundances during the evolution of dense molecular clouds.","Consequently, its reactions with other interstellar species could have a strong influence on the chemical composition of these regions.","Here, we report the results of an investigation of the reaction between C(3P) and dimethylether, CH3OCH3, which was recently detected in dark cloud TMC-1.","Experiments were performed to study the kinetics of this reaction using a continuous supersonic flow reactor employing pulsed laser photolysis and pulsed laser induced fluorescence for atomic radical generation and detection respectively.","Rate constants for this process were measured between 50 K and 296 K, while additional measurements of the product atomic hydrogen yields were also performed over the 75-296 K range.","To better understand the experimental results, statistical rate theory was used to calculate rate constants over the same temperature range and to provide insight on the major product channels.","These simulations, based on quantum chemical calculations of the ground triplet state of the C3H6O molecule, allowed us to obtain the most important features of the underlying potential energy surface.","The measured rate constant increases as the temperature falls, reaching a value of k_(C+CH_3 OCH_3 )= 7.5 x 10-11 cm3 s-1 at 50 K, while the low measured H-atom yields support the theoretical prediction that the major reaction products are CH3 + CH3 + CO.","The effects of this reaction on the abundances of interstellar CH3OCH3 and related species were tested using a gas-grain dense cloud model, employing an expression for the rate constant, k(T) = alpha(T/300)^beta, with alpha = 1.27 x 10-11 and beta = -1.01.","These simulations predict that the C(3P)","+","CH3OCH3 reaction decreases gas-phase CH3OCH3 abundances by more than an order of magnitude at early times."],"url":"http://arxiv.org/abs/2405.01068v1","category":"astro-ph.GA"}
{"created":"2024-05-02 07:49:28","title":"AB-Training: A Communication-Efficient Approach for Distributed Low-Rank Learning","abstract":"Communication bottlenecks hinder the scalability of distributed neural network training, particularly on distributed-memory computing clusters. To significantly reduce this communication overhead, we introduce AB-training, a novel data-parallel training method that decomposes weight matrices into low-rank representations and utilizes independent group-based training. This approach consistently reduces network traffic by 50% across multiple scaling scenarios, increasing the training potential on communication-constrained systems. Our method exhibits regularization effects at smaller scales, leading to improved generalization for models like VGG16, while achieving a remarkable 44.14 : 1 compression ratio during training on CIFAR-10 and maintaining competitive accuracy. Albeit promising, our experiments reveal that large batch effects remain a challenge even in low-rank training regimes.","sentences":["Communication bottlenecks hinder the scalability of distributed neural network training, particularly on distributed-memory computing clusters.","To significantly reduce this communication overhead, we introduce AB-training, a novel data-parallel training method that decomposes weight matrices into low-rank representations and utilizes independent group-based training.","This approach consistently reduces network traffic by 50% across multiple scaling scenarios, increasing the training potential on communication-constrained systems.","Our method exhibits regularization effects at smaller scales, leading to improved generalization for models like VGG16, while achieving a remarkable 44.14 : 1 compression ratio during training on CIFAR-10 and maintaining competitive accuracy.","Albeit promising, our experiments reveal that large batch effects remain a challenge even in low-rank training regimes."],"url":"http://arxiv.org/abs/2405.01067v1","category":"cs.LG"}
{"created":"2024-05-02 17:44:41","title":"D2PO: Discriminator-Guided DPO with Response Evaluation Models","abstract":"Varied approaches for aligning language models have been proposed, including supervised fine-tuning, RLHF, and direct optimization methods such as DPO. Although DPO has rapidly gained popularity due to its straightforward training process and competitive results, there is an open question of whether there remain practical advantages of using a discriminator, like a reward model, to evaluate responses. We propose D2PO, discriminator-guided DPO, an approach for the online setting where preferences are being collected throughout learning. As we collect gold preferences, we use these not only to train our policy, but to train a discriminative response evaluation model to silver-label even more synthetic data for policy training. We explore this approach across a set of diverse tasks, including a realistic chat setting, we find that our approach leads to higher-quality outputs compared to DPO with the same data budget, and greater efficiency in terms of preference data requirements. Furthermore, we show conditions under which silver labeling is most helpful: it is most effective when training the policy with DPO, outperforming traditional PPO, and benefits from maintaining a separate discriminator from the policy model.","sentences":["Varied approaches for aligning language models have been proposed, including supervised fine-tuning, RLHF, and direct optimization methods such as DPO.","Although DPO has rapidly gained popularity due to its straightforward training process and competitive results, there is an open question of whether there remain practical advantages of using a discriminator, like a reward model, to evaluate responses.","We propose D2PO, discriminator-guided DPO, an approach for the online setting where preferences are being collected throughout learning.","As we collect gold preferences, we use these not only to train our policy, but to train a discriminative response evaluation model to silver-label even more synthetic data for policy training.","We explore this approach across a set of diverse tasks, including a realistic chat setting, we find that our approach leads to higher-quality outputs compared to DPO with the same data budget, and greater efficiency in terms of preference data requirements.","Furthermore, we show conditions under which silver labeling is most helpful: it is most effective when training the policy with DPO, outperforming traditional PPO, and benefits from maintaining a separate discriminator from the policy model."],"url":"http://arxiv.org/abs/2405.01511v1","category":"cs.CL"}
{"created":"2024-05-02 17:32:38","title":"Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models","abstract":"Knowledge workers often need to extract and analyze information from a collection of documents to solve complex information tasks in the workplace, e.g., hiring managers reviewing resumes or analysts assessing risk in contracts. However, foraging for relevant information can become tedious and repetitive over many documents and criteria of interest. We introduce Marco, a mixed-initiative workspace supporting sensemaking over diverse business document collections. Through collection-centric assistance, Marco reduces the cognitive costs of extracting and structuring information, allowing users to prioritize comparative synthesis and decision making processes. Users interactively communicate their information needs to an AI assistant using natural language and compose schemas that provide an overview of a document collection. Findings from a usability study (n=16) demonstrate that when using Marco, users complete sensemaking tasks 16% more quickly, with less effort, and without diminishing accuracy. A design probe with seven domain experts identifies how Marco can benefit various real-world workflows.","sentences":["Knowledge workers often need to extract and analyze information from a collection of documents to solve complex information tasks in the workplace, e.g., hiring managers reviewing resumes or analysts assessing risk in contracts.","However, foraging for relevant information can become tedious and repetitive over many documents and criteria of interest.","We introduce Marco, a mixed-initiative workspace supporting sensemaking over diverse business document collections.","Through collection-centric assistance, Marco reduces the cognitive costs of extracting and structuring information, allowing users to prioritize comparative synthesis and decision making processes.","Users interactively communicate their information needs to an AI assistant using natural language and compose schemas that provide an overview of a document collection.","Findings from a usability study (n=16) demonstrate that when using Marco, users complete sensemaking tasks 16% more quickly, with less effort, and without diminishing accuracy.","A design probe with seven domain experts identifies how Marco can benefit various real-world workflows."],"url":"http://arxiv.org/abs/2405.01501v1","category":"cs.HC"}
{"created":"2024-05-02 17:00:02","title":"WildChat: 1M ChatGPT Interaction Logs in the Wild","abstract":"Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual opt-in to anonymously collect their chat transcripts and request headers. From this, we compiled WildChat, a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, we demonstrate the dataset's potential utility in fine-tuning instruction-following models. WildChat is released at https://wildchat.allen.ai under AI2 ImpACT Licenses.","sentences":["Chatbots such as GPT-4 and ChatGPT are now serving millions of users.","Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice.","To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual opt-in to anonymously collect their chat transcripts and request headers.","From this, we compiled WildChat, a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns.","We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study.","In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers.","This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions.","Finally, because it captures a broad range of use cases, we demonstrate the dataset's potential utility in fine-tuning instruction-following models.","WildChat is released at https://wildchat.allen.ai under AI2 ImpACT Licenses."],"url":"http://arxiv.org/abs/2405.01470v1","category":"cs.CL"}
{"created":"2024-05-02 16:40:00","title":"The Digitization of Photographic Spectra in the Dominion Astrophysical Observatory Plate Collection with Commercial Scanners: A Pilot Study","abstract":"Commercial flatbed scanners have the potential to deliver a quick and efficient means of capturing the scientific content of spectra recorded on photographic plates. We discuss the digitization of selected spectra in the Dominion Astrophysical Observatory (DAO) photographic plate collection with commercial scanners. In this pilot study, emphasis is placed on assessing if the information on the plates can be recovered using Epson V800 and 12000XL scanners; the more complicated issues associated with the shortcomings of photographic materials, such as correcting for nonlinearity, are deferred to a future study. Spectra of Vega that were recorded over ~4 decades with the DAO 1.8 meter telescope are examined. These spectra sample a range of photographic emulsions, plate preparation techniques, calibration information, observing techniques, and spectrograph configuration. A scanning density of 2400 elements per inch recovers information in the spectra. Differences in the modulation transfer function (MTF) of the two scanners are found, with the Epson 12000XL having a superior MTF. Comparisons with a CCD spectrum of Vega confirm that moderately weak features are faithfully recovered in photographic spectra that have been digitized with the 12000XL scanner. The importance of scanning the full plate to cover the light profile of the target and calibration information is emphasized. Lessons learned from these experiments are also presented.","sentences":["Commercial flatbed scanners have the potential to deliver a quick and efficient means of capturing the scientific content of spectra recorded on photographic plates.","We discuss the digitization of selected spectra in the Dominion Astrophysical Observatory (DAO) photographic plate collection with commercial scanners.","In this pilot study, emphasis is placed on assessing if the information on the plates can be recovered using Epson V800 and 12000XL scanners; the more complicated issues associated with the shortcomings of photographic materials, such as correcting for nonlinearity, are deferred to a future study.","Spectra of Vega that were recorded over ~4 decades with the DAO 1.8 meter telescope are examined.","These spectra sample a range of photographic emulsions, plate preparation techniques, calibration information, observing techniques, and spectrograph configuration.","A scanning density of 2400 elements per inch recovers information in the spectra.","Differences in the modulation transfer function (MTF) of the two scanners are found, with the Epson 12000XL having a superior MTF.","Comparisons with a CCD spectrum of Vega confirm that moderately weak features are faithfully recovered in photographic spectra that have been digitized with the 12000XL scanner.","The importance of scanning the full plate to cover the light profile of the target and calibration information is emphasized.","Lessons learned from these experiments are also presented."],"url":"http://arxiv.org/abs/2405.01456v1","category":"astro-ph.IM"}
{"created":"2024-05-02 16:32:42","title":"A mixed effects cosinor modelling framework for circadian gene expression","abstract":"The cosinor model is frequently used to represent gene expression given the 24 hour day-night cycle time at which a corresponding tissue sample is collected. However, the timing of many biological processes are based on individual-specific internal timing systems that are offset relative to day-night cycle time. When these offsets are unknown, they pose a challenge in performing statistical analyses with a cosinor model. To clarify, when sample collection times are mis-recorded, cosinor regression can yield attenuated parameter estimates, which would also attenuate test statistics. This attenuation bias would inflate type II error rates in identifying genes with oscillatory behavior. This paper proposes a heuristic method to account for unknown offsets when tissue samples are collected in a longitudinal design. Specifically, this method involves first estimating individual-specific cosinor models for each gene. The times of sample collection for that individual are then translated based on the estimated phase-shifts across every gene. Simulation studies confirm that this method mitigates bias in estimation and inference. Illustrations with real data from three circadian biology studies highlight that this method produces parameter estimates and inferences akin to those obtained when each individual's offset is known.","sentences":["The cosinor model is frequently used to represent gene expression given the 24 hour day-night cycle time at which a corresponding tissue sample is collected.","However, the timing of many biological processes are based on individual-specific internal timing systems that are offset relative to day-night cycle time.","When these offsets are unknown, they pose a challenge in performing statistical analyses with a cosinor model.","To clarify, when sample collection times are mis-recorded, cosinor regression can yield attenuated parameter estimates, which would also attenuate test statistics.","This attenuation bias would inflate type II error rates in identifying genes with oscillatory behavior.","This paper proposes a heuristic method to account for unknown offsets when tissue samples are collected in a longitudinal design.","Specifically, this method involves first estimating individual-specific cosinor models for each gene.","The times of sample collection for that individual are then translated based on the estimated phase-shifts across every gene.","Simulation studies confirm that this method mitigates bias in estimation and inference.","Illustrations with real data from three circadian biology studies highlight that this method produces parameter estimates and inferences akin to those obtained when each individual's offset is known."],"url":"http://arxiv.org/abs/2405.01450v1","category":"stat.AP"}
{"created":"2024-05-02 16:08:03","title":"GTX: A Write-Optimized Latch-free Graph Data System with Transactional Support","abstract":"This paper introduces GTX a standalone main-memory write-optimized graph system that specializes in structural and graph property updates while maintaining concurrent reads and graph analytics with snapshot isolation-level transactional concurrency. Recent graph libraries target efficient concurrent read and write support while guaranteeing transactional consistency. However, their performance suffers for updates with strong temporal locality over the same vertexes and edges due to vertex-centric lock contentions. GTX introduces a new delta-chain-centric concurrency-control protocol that eliminates traditional mutually exclusive latches. GTX resolves the conflicts caused by vertex-level locking, and adapts to real-life workloads while maintaining sequential access to the graph's adjacency lists storage. This combination of features has been demonstrated to provide good performance in graph analytical queries. GTX's transactions support fast group commit, novel write-write conflict prevention, and lazy garbage collection. Based on extensive experimental and comparative studies, in addition to maintaining competitive concurrent read and analytical performance, GTX demonstrates high throughput over state-of-the-art techniques when handling concurrent transaction+analytics workloads. For write-heavy transactional workloads, GTX performs up to 11x better than the best-performing state-of-the-art systems in transaction throughput. At the same time, GTX does not sacrifice the performance of read-heavy analytical workloads, and has competitive performance similar to state-of-the-art systems.","sentences":["This paper introduces GTX a standalone main-memory write-optimized graph system that specializes in structural and graph property updates while maintaining concurrent reads and graph analytics with snapshot isolation-level transactional concurrency.","Recent graph libraries target efficient concurrent read and write support while guaranteeing transactional consistency.","However, their performance suffers for updates with strong temporal locality over the same vertexes and edges due to vertex-centric lock contentions.","GTX introduces a new delta-chain-centric concurrency-control protocol that eliminates traditional mutually exclusive latches.","GTX resolves the conflicts caused by vertex-level locking, and adapts to real-life workloads while maintaining sequential access to the graph's adjacency lists storage.","This combination of features has been demonstrated to provide good performance in graph analytical queries.","GTX's transactions support fast group commit, novel write-write conflict prevention, and lazy garbage collection.","Based on extensive experimental and comparative studies, in addition to maintaining competitive concurrent read and analytical performance, GTX demonstrates high throughput over state-of-the-art techniques when handling concurrent transaction+analytics workloads.","For write-heavy transactional workloads, GTX performs up to 11x better than the best-performing state-of-the-art systems in transaction throughput.","At the same time, GTX does not sacrifice the performance of read-heavy analytical workloads, and has competitive performance similar to state-of-the-art systems."],"url":"http://arxiv.org/abs/2405.01418v1","category":"cs.DB"}
{"created":"2024-05-02 16:02:13","title":"IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps","abstract":"Third-party applications have become an essential part of today's online ecosystem, enhancing the functionality of popular platforms. However, the intensive data exchange underlying their proliferation has increased concerns about interdependent privacy (IDP). This paper provides a comprehensive investigation into the previously underinvestigated IDP issues of third-party apps. Specifically, first, we analyze the permission structure of multiple app platforms, identifying permissions that have the potential to cause interdependent privacy issues by enabling a user to share someone else's personal data with an app. Second, we collect datasets and characterize the extent to which existing apps request these permissions, revealing the relationship between characteristics such as the respective app platform, the app's type, and the number of interdependent privacy-related permissions it requests. Third, we analyze the various reasons IDP is neglected by both data protection regulations and app platforms and then devise principles that should be followed when designing a mitigation solution. Finally, based on these principles and satisfying clearly defined objectives, we propose IDPFilter, a platform-agnostic API that enables application providers to minimize collateral information collection by filtering out data collected from their users but implicating others as data subjects. We implement a proof-of-concept prototype, IDPTextFilter, that implements the filtering logic on textual data, and provide its initial performance evaluation with regard to privacy, accuracy, and efficiency.","sentences":["Third-party applications have become an essential part of today's online ecosystem, enhancing the functionality of popular platforms.","However, the intensive data exchange underlying their proliferation has increased concerns about interdependent privacy (IDP).","This paper provides a comprehensive investigation into the previously underinvestigated IDP issues of third-party apps.","Specifically, first, we analyze the permission structure of multiple app platforms, identifying permissions that have the potential to cause interdependent privacy issues by enabling a user to share someone else's personal data with an app.","Second, we collect datasets and characterize the extent to which existing apps request these permissions, revealing the relationship between characteristics such as the respective app platform, the app's type, and the number of interdependent privacy-related permissions it requests.","Third, we analyze the various reasons IDP is neglected by both data protection regulations and app platforms and then devise principles that should be followed when designing a mitigation solution.","Finally, based on these principles and satisfying clearly defined objectives, we propose IDPFilter, a platform-agnostic API that enables application providers to minimize collateral information collection by filtering out data collected from their users but implicating others as data subjects.","We implement a proof-of-concept prototype, IDPTextFilter, that implements the filtering logic on textual data, and provide its initial performance evaluation with regard to privacy, accuracy, and efficiency."],"url":"http://arxiv.org/abs/2405.01411v1","category":"cs.CR"}
{"created":"2024-05-02 15:24:08","title":"On degenerate perturbation theory for the Floquet-Hilbert space","abstract":"We consider construction of effective Hamiltonians for periodically driven interacting systems in the case of resonant driving. The standard high-frequency expansion is not expected to converge due to the resonant creation of collective excitations, and one option is to resort to the application of degenerate perturbation theory (DPT) in the extended Floquet-Hilbert space. We propose an extension of DPT whereby the degenerate subspace includes not only the degenerate levels of interest but rather all the states of the system. The resulting approach, which we call extended DPT (EDPT), is shown to resemble a high-frequency expansion, provided the quasienergy matrix is constructed such that each $m$th diagonal block contains energies reduced to the $m$th Floquet zone. The proposed theory is applied to a driven Bose-Hubbard model and is shown to yield more accurate quasienergy spectra than the conventional DPT. The computational complexity of EDPT is intermediate between DPT and the numerically exact approach, thus providing a practical compromise between accuracy and efficiency.","sentences":["We consider construction of effective Hamiltonians for periodically driven interacting systems in the case of resonant driving.","The standard high-frequency expansion is not expected to converge due to the resonant creation of collective excitations, and one option is to resort to the application of degenerate perturbation theory (DPT) in the extended Floquet-Hilbert space.","We propose an extension of DPT whereby the degenerate subspace includes not only the degenerate levels of interest but rather all the states of the system.","The resulting approach, which we call extended DPT (EDPT), is shown to resemble a high-frequency expansion, provided the quasienergy matrix is constructed such that each $m$th diagonal block contains energies reduced to the $m$th Floquet zone.","The proposed theory is applied to a driven Bose-Hubbard model and is shown to yield more accurate quasienergy spectra than the conventional DPT.","The computational complexity of EDPT is intermediate between DPT and the numerically exact approach, thus providing a practical compromise between accuracy and efficiency."],"url":"http://arxiv.org/abs/2405.01383v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-02 15:01:25","title":"Sparse multi-view hand-object reconstruction for unseen environments","abstract":"Recent works in hand-object reconstruction mainly focus on the single-view and dense multi-view settings. On the one hand, single-view methods can leverage learned shape priors to generalise to unseen objects but are prone to inaccuracies due to occlusions. On the other hand, dense multi-view methods are very accurate but cannot easily adapt to unseen objects without further data collection. In contrast, sparse multi-view methods can take advantage of the additional views to tackle occlusion, while keeping the computational cost low compared to dense multi-view methods. In this paper, we consider the problem of hand-object reconstruction with unseen objects in the sparse multi-view setting. Given multiple RGB images of the hand and object captured at the same time, our model SVHO combines the predictions from each view into a unified reconstruction without optimisation across views. We train our model on a synthetic hand-object dataset and evaluate directly on a real world recorded hand-object dataset with unseen objects. We show that while reconstruction of unseen hands and objects from RGB is challenging, additional views can help improve the reconstruction quality.","sentences":["Recent works in hand-object reconstruction mainly focus on the single-view and dense multi-view settings.","On the one hand, single-view methods can leverage learned shape priors to generalise to unseen objects but are prone to inaccuracies due to occlusions.","On the other hand, dense multi-view methods are very accurate but cannot easily adapt to unseen objects without further data collection.","In contrast, sparse multi-view methods can take advantage of the additional views to tackle occlusion, while keeping the computational cost low compared to dense multi-view methods.","In this paper, we consider the problem of hand-object reconstruction with unseen objects in the sparse multi-view setting.","Given multiple RGB images of the hand and object captured at the same time, our model SVHO combines the predictions from each view into a unified reconstruction without optimisation across views.","We train our model on a synthetic hand-object dataset and evaluate directly on a real world recorded hand-object dataset with unseen objects.","We show that while reconstruction of unseen hands and objects from RGB is challenging, additional views can help improve the reconstruction quality."],"url":"http://arxiv.org/abs/2405.01353v1","category":"cs.CV"}
{"created":"2024-05-02 14:41:16","title":"Quantification of vaccine waning as a challenge effect","abstract":"Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose.","sentences":["Knowing whether vaccine protection wanes over time is important for health policy and drug development.","However, quantifying waning effects is difficult.","A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time.","Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect.","We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination.","We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data.","We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time.","Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial.","Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose."],"url":"http://arxiv.org/abs/2405.01336v1","category":"stat.ME"}
{"created":"2024-05-02 13:20:02","title":"The spectral evolution of white dwarfs: where do we stand?","abstract":"White dwarfs are the dense, burnt-out remnants of the vast majority of stars, condemned to cool over billions of years as they steadily radiate away their residual thermal energy. To first order, their atmosphere is expected to be made purely of hydrogen due to the efficient gravitational settling of heavier elements. However, observations reveal a much more complex situation, as the surface of a white dwarf (1) can be dominated by helium rather than hydrogen, (2) can be polluted by trace chemical species, and (3) can undergo significant composition changes with time. This indicates that various mechanisms of element transport effectively compete against gravitational settling in the stellar envelope. This phenomenon is known as the spectral evolution of white dwarfs and has important implications for Galactic, stellar, and planetary astrophysics. This invited review provides a comprehensive picture of our current understanding of white dwarf spectral evolution. We first describe the latest observational constraints on the variations in atmospheric composition along the cooling sequence, covering both the dominant and trace constituents. We then summarise the predictions of state-of-the-art models of element transport in white dwarfs and assess their ability to explain the observed spectral evolution. Finally, we highlight remaining open questions and suggest avenues for future work.","sentences":["White dwarfs are the dense, burnt-out remnants of the vast majority of stars, condemned to cool over billions of years as they steadily radiate away their residual thermal energy.","To first order, their atmosphere is expected to be made purely of hydrogen due to the efficient gravitational settling of heavier elements.","However, observations reveal a much more complex situation, as the surface of a white dwarf (1) can be dominated by helium rather than hydrogen, (2) can be polluted by trace chemical species, and (3) can undergo significant composition changes with time.","This indicates that various mechanisms of element transport effectively compete against gravitational settling in the stellar envelope.","This phenomenon is known as the spectral evolution of white dwarfs and has important implications for Galactic, stellar, and planetary astrophysics.","This invited review provides a comprehensive picture of our current understanding of white dwarf spectral evolution.","We first describe the latest observational constraints on the variations in atmospheric composition along the cooling sequence, covering both the dominant and trace constituents.","We then summarise the predictions of state-of-the-art models of element transport in white dwarfs and assess their ability to explain the observed spectral evolution.","Finally, we highlight remaining open questions and suggest avenues for future work."],"url":"http://arxiv.org/abs/2405.01268v1","category":"astro-ph.SR"}
{"created":"2024-05-02 12:08:28","title":"Dissipative phase transition: from qubits to qudits","abstract":"We investigate the fate of dissipative phase transitions in quantum many-body systems when the individual constituents are qudits ($d$-level systems) instead of qubits. As an example system, we employ a permutation-invariant $XY$ model of $N$ infinite-range interacting $d$-level spins undergoing individual and collective dissipation. In the mean-field limit, we identify a dissipative phase transition, whose critical point is independent of $d$ after a suitable rescaling of parameters. When the decay rates between all adjacent levels are identical and $d\\geq 4$, the critical point expands, in terms of the ratio between dissipation and interaction strengths, to a critical region in which two phases coexist and which increases as $d$ grows. In addition, a larger $d$ leads to a more pronounced change in spin expectation values at the critical point. Numerical investigations for finite $N$ reveal symmetry breaking signatures in the Liouvillian spectrum at the phase transition. The phase transition is furthermore marked by maximum entanglement negativity and a significant purity change of the steady state, which become more pronounced as $d$ increases. Considering qudits instead of qubits thus opens new perspectives on accessing rich phase diagrams in open many-body systems.","sentences":["We investigate the fate of dissipative phase transitions in quantum many-body systems when the individual constituents are qudits ($d$-level systems) instead of qubits.","As an example system, we employ a permutation-invariant $XY$ model of $N$ infinite-range interacting $d$-level spins undergoing individual and collective dissipation.","In the mean-field limit, we identify a dissipative phase transition, whose critical point is independent of $d$ after a suitable rescaling of parameters.","When the decay rates between all adjacent levels are identical and $d\\geq 4$, the critical point expands, in terms of the ratio between dissipation and interaction strengths, to a critical region in which two phases coexist and which increases as $d$ grows.","In addition, a larger $d$ leads to a more pronounced change in spin expectation values at the critical point.","Numerical investigations for finite $N$ reveal symmetry breaking signatures in the Liouvillian spectrum at the phase transition.","The phase transition is furthermore marked by maximum entanglement negativity and a significant purity change of the steady state, which become more pronounced as $d$ increases.","Considering qudits instead of qubits thus opens new perspectives on accessing rich phase diagrams in open many-body systems."],"url":"http://arxiv.org/abs/2405.01223v1","category":"quant-ph"}
{"created":"2024-05-02 11:33:54","title":"Imagine2touch: Predictive Tactile Sensing for Robotic Manipulation using Efficient Low-Dimensional Signals","abstract":"Humans seemingly incorporate potential touch signals in their perception. Our goal is to equip robots with a similar capability, which we term Imagine2touch. Imagine2touch aims to predict the expected touch signal based on a visual patch representing the area to be touched. We use ReSkin, an inexpensive and compact touch sensor to collect the required dataset through random touching of five basic geometric shapes, and one tool. We train Imagine2touch on two out of those shapes and validate it on the ood. tool. We demonstrate the efficacy of Imagine2touch through its application to the downstream task of object recognition. In this task, we evaluate Imagine2touch performance in two experiments, together comprising 5 out of training distribution objects. Imagine2touch achieves an object recognition accuracy of 58% after ten touches per object, surpassing a proprioception baseline.","sentences":["Humans seemingly incorporate potential touch signals in their perception.","Our goal is to equip robots with a similar capability, which we term Imagine2touch.","Imagine2touch aims to predict the expected touch signal based on a visual patch representing the area to be touched.","We use ReSkin, an inexpensive and compact touch sensor to collect the required dataset through random touching of five basic geometric shapes, and one tool.","We train Imagine2touch on two out of those shapes and validate it on the ood.","tool.","We demonstrate the efficacy of Imagine2touch through its application to the downstream task of object recognition.","In this task, we evaluate Imagine2touch performance in two experiments, together comprising 5 out of training distribution objects.","Imagine2touch achieves an object recognition accuracy of 58% after ten touches per object, surpassing a proprioception baseline."],"url":"http://arxiv.org/abs/2405.01192v1","category":"cs.RO"}
{"created":"2024-05-02 10:19:32","title":"Tabular and Deep Reinforcement Learning for Gittins Index","abstract":"In the realm of multi-arm bandit problems, the Gittins index policy is known to be optimal in maximizing the expected total discounted reward obtained from pulling the Markovian arms. In most realistic scenarios however, the Markovian state transition probabilities are unknown and therefore the Gittins indices cannot be computed. One can then resort to reinforcement learning (RL) algorithms that explore the state space to learn these indices while exploiting to maximize the reward collected. In this work, we propose tabular (QGI) and Deep RL (DGN) algorithms for learning the Gittins index that are based on the retirement formulation for the multi-arm bandit problem. When compared with existing RL algorithms that learn the Gittins index, our algorithms have a lower run time, require less storage space (small Q-table size in QGI and smaller replay buffer in DGN), and illustrate better empirical convergence to the Gittins index. This makes our algorithm well suited for problems with large state spaces and is a viable alternative to existing methods. As a key application, we demonstrate the use of our algorithms in minimizing the mean flowtime in a job scheduling problem when jobs are available in batches and have an unknown service time distribution. \\","sentences":["In the realm of multi-arm bandit problems, the Gittins index policy is known to be optimal in maximizing the expected total discounted reward obtained from pulling the Markovian arms.","In most realistic scenarios however, the Markovian state transition probabilities are unknown and therefore the Gittins indices cannot be computed.","One can then resort to reinforcement learning (RL) algorithms that explore the state space to learn these indices while exploiting to maximize the reward collected.","In this work, we propose tabular (QGI) and Deep RL (DGN) algorithms for learning the Gittins index that are based on the retirement formulation for the multi-arm bandit problem.","When compared with existing RL algorithms that learn the Gittins index, our algorithms have a lower run time, require less storage space (small Q-table size in QGI and smaller replay buffer in DGN), and illustrate better empirical convergence to the Gittins index.","This makes our algorithm well suited for problems with large state spaces and is a viable alternative to existing methods.","As a key application, we demonstrate the use of our algorithms in minimizing the mean flowtime in a job scheduling problem when jobs are available in batches and have an unknown service time distribution.","\\"],"url":"http://arxiv.org/abs/2405.01157v1","category":"cs.LG"}
{"created":"2024-05-02 09:19:43","title":"Sports Analysis and VR Viewing System Based on Player Tracking and Pose Estimation with Multimodal and Multiview Sensors","abstract":"Sports analysis and viewing play a pivotal role in the current sports domain, offering significant value not only to coaches and athletes but also to fans and the media. In recent years, the rapid development of virtual reality (VR) and augmented reality (AR) technologies have introduced a new platform for watching games. Visualization of sports competitions in VR/AR represents a revolutionary technology, providing audiences with a novel immersive viewing experience. However, there is still a lack of related research in this area. In this work, we present for the first time a comprehensive system for sports competition analysis and real-time visualization on VR/AR platforms. First, we utilize multiview LiDARs and cameras to collect multimodal game data. Subsequently, we propose a framework for multi-player tracking and pose estimation based on a limited amount of supervised data, which extracts precise player positions and movements from point clouds and images. Moreover, we perform avatar modeling of players to obtain their 3D models. Ultimately, using these 3D player data, we conduct competition analysis and real-time visualization on VR/AR. Extensive quantitative experiments demonstrate the accuracy and robustness of our multi-player tracking and pose estimation framework. The visualization results showcase the immense potential of our sports visualization system on the domain of watching games on VR/AR devices. The multimodal competition dataset we collected and all related code will be released soon.","sentences":["Sports analysis and viewing play a pivotal role in the current sports domain, offering significant value not only to coaches and athletes but also to fans and the media.","In recent years, the rapid development of virtual reality (VR) and augmented reality (AR) technologies have introduced a new platform for watching games.","Visualization of sports competitions in VR/AR represents a revolutionary technology, providing audiences with a novel immersive viewing experience.","However, there is still a lack of related research in this area.","In this work, we present for the first time a comprehensive system for sports competition analysis and real-time visualization on VR/AR platforms.","First, we utilize multiview LiDARs and cameras to collect multimodal game data.","Subsequently, we propose a framework for multi-player tracking and pose estimation based on a limited amount of supervised data, which extracts precise player positions and movements from point clouds and images.","Moreover, we perform avatar modeling of players to obtain their 3D models.","Ultimately, using these 3D player data, we conduct competition analysis and real-time visualization on VR/AR.","Extensive quantitative experiments demonstrate the accuracy and robustness of our multi-player tracking and pose estimation framework.","The visualization results showcase the immense potential of our sports visualization system on the domain of watching games on VR/AR devices.","The multimodal competition dataset we collected and all related code will be released soon."],"url":"http://arxiv.org/abs/2405.01112v1","category":"cs.CV"}
{"created":"2024-05-02 09:14:38","title":"Image segmentation of treated and untreated tumor spheroids by Fully Convolutional Networks","abstract":"Multicellular tumor spheroids (MCTS) are advanced cell culture systems for assessing the impact of combinatorial radio(chemo)therapy. They exhibit therapeutically relevant in-vivo-like characteristics from 3D cell-cell and cell-matrix interactions to radial pathophysiological gradients related to proliferative activity and nutrient/oxygen supply, altering cellular radioresponse. State-of-the-art assays quantify long-term curative endpoints based on collected brightfield image time series from large treated spheroid populations per irradiation dose and treatment arm. Here, spheroid control probabilities are documented analogous to in-vivo tumor control probabilities based on Kaplan-Meier curves. This analyses require laborious spheroid segmentation of up to 100.000 images per treatment arm to extract relevant structural information from the images, e.g., diameter, area, volume and circularity. While several image analysis algorithms are available for spheroid segmentation, they all focus on compact MCTS with clearly distinguishable outer rim throughout growth. However, treated MCTS may partly be detached and destroyed and are usually obscured by dead cell debris. We successfully train two Fully Convolutional Networks, UNet and HRNet, and optimize their hyperparameters to develop an automatic segmentation for both untreated and treated MCTS. We systematically validate the automatic segmentation on larger, independent data sets of spheroids derived from two human head-and-neck cancer cell lines. We find an excellent overlap between manual and automatic segmentation for most images, quantified by Jaccard indices at around 90%. For images with smaller overlap of the segmentations, we demonstrate that this error is comparable to the variations across segmentations from different biological experts, suggesting that these images represent biologically unclear or ambiguous cases.","sentences":["Multicellular tumor spheroids (MCTS) are advanced cell culture systems for assessing the impact of combinatorial radio(chemo)therapy.","They exhibit therapeutically relevant in-vivo-like characteristics from 3D cell-cell and cell-matrix interactions to radial pathophysiological gradients related to proliferative activity and nutrient/oxygen supply, altering cellular radioresponse.","State-of-the-art assays quantify long-term curative endpoints based on collected brightfield image time series from large treated spheroid populations per irradiation dose and treatment arm.","Here, spheroid control probabilities are documented analogous to in-vivo tumor control probabilities based on Kaplan-Meier curves.","This analyses require laborious spheroid segmentation of up to 100.000 images per treatment arm to extract relevant structural information from the images, e.g., diameter, area, volume and circularity.","While several image analysis algorithms are available for spheroid segmentation, they all focus on compact MCTS with clearly distinguishable outer rim throughout growth.","However, treated MCTS may partly be detached and destroyed and are usually obscured by dead cell debris.","We successfully train two Fully Convolutional Networks, UNet and HRNet, and optimize their hyperparameters to develop an automatic segmentation for both untreated and treated MCTS.","We systematically validate the automatic segmentation on larger, independent data sets of spheroids derived from two human head-and-neck cancer cell lines.","We find an excellent overlap between manual and automatic segmentation for most images, quantified by Jaccard indices at around 90%.","For images with smaller overlap of the segmentations, we demonstrate that this error is comparable to the variations across segmentations from different biological experts, suggesting that these images represent biologically unclear or ambiguous cases."],"url":"http://arxiv.org/abs/2405.01105v1","category":"cs.CV"}
{"created":"2024-05-02 08:18:37","title":"KDPrint: Passive Authentication using Keystroke Dynamics-to-Image Encoding via Standardization","abstract":"In contemporary mobile user authentication systems, verifying user legitimacy has become paramount due to the widespread use of smartphones. Although fingerprint and facial recognition are widely used for mobile authentication, PIN-based authentication is still employed as a fallback option if biometric authentication fails after multiple attempts. Consequently, the system remains susceptible to attacks targeting the PIN when biometric methods are unsuccessful. In response to these concerns, two-factor authentication has been proposed, albeit with the caveat of increased user effort. To address these challenges, this paper proposes a passive authentication system that utilizes keystroke data, a byproduct of primary authentication methods, for background user authentication. Additionally, we introduce a novel image encoding technique to capture the temporal dynamics of keystroke data, overcoming the performance limitations of deep learning models. Furthermore, we present a methodology for selecting suitable behavioral biometric features for image representation. The resulting images, depicting the user's PIN input patterns, enhance the model's ability to uniquely identify users through the secondary channel with high accuracy. Experimental results demonstrate that the proposed imaging approach surpasses existing methods in terms of information capacity. In self-collected dataset experiments, incorporating features from prior research, our method achieved an Equal Error Rate (EER) of 6.7\\%, outperforming the existing method's 47.7\\%. Moreover, our imaging technique attained a True Acceptance Rate (TAR) of 94.4\\% and a False Acceptance Rate (FAR) of 8\\% for 17 users.","sentences":["In contemporary mobile user authentication systems, verifying user legitimacy has become paramount due to the widespread use of smartphones.","Although fingerprint and facial recognition are widely used for mobile authentication, PIN-based authentication is still employed as a fallback option if biometric authentication fails after multiple attempts.","Consequently, the system remains susceptible to attacks targeting the PIN when biometric methods are unsuccessful.","In response to these concerns, two-factor authentication has been proposed, albeit with the caveat of increased user effort.","To address these challenges, this paper proposes a passive authentication system that utilizes keystroke data, a byproduct of primary authentication methods, for background user authentication.","Additionally, we introduce a novel image encoding technique to capture the temporal dynamics of keystroke data, overcoming the performance limitations of deep learning models.","Furthermore, we present a methodology for selecting suitable behavioral biometric features for image representation.","The resulting images, depicting the user's PIN input patterns, enhance the model's ability to uniquely identify users through the secondary channel with high accuracy.","Experimental results demonstrate that the proposed imaging approach surpasses existing methods in terms of information capacity.","In self-collected dataset experiments, incorporating features from prior research, our method achieved an Equal Error Rate (EER) of 6.7\\%, outperforming the existing method's 47.7\\%.","Moreover, our imaging technique attained a True Acceptance Rate (TAR) of 94.4\\% and a False Acceptance Rate (FAR) of 8\\% for 17 users."],"url":"http://arxiv.org/abs/2405.01080v1","category":"cs.CR"}
{"created":"2024-05-02 07:47:49","title":"HandSSCA: 3D Hand Mesh Reconstruction with State Space Channel Attention from RGB images","abstract":"Reconstructing a hand mesh from a single RGB image is a challenging task because hands are often occluded by objects. Most previous works attempted to introduce more additional information and adopt attention mechanisms to improve 3D reconstruction results, but it would increased computational complexity. This observation prompts us to propose a new and concise architecture while improving computational efficiency. In this work, we propose a simple and effective 3D hand mesh reconstruction network HandSSCA, which is the first to incorporate state space modeling into the field of hand pose estimation. In the network, we have designed a novel state space channel attention module that extends the effective sensory field, extracts hand features in the spatial dimension, and enhances hand regional features in the channel dimension. This design helps to reconstruct a complete and detailed hand mesh. Extensive experiments conducted on well-known datasets featuring challenging hand-object occlusions (such as FREIHAND, DEXYCB, and HO3D) demonstrate that our proposed HandSSCA achieves state-of-the-art performance while maintaining a minimal parameter count.","sentences":["Reconstructing a hand mesh from a single RGB image is a challenging task because hands are often occluded by objects.","Most previous works attempted to introduce more additional information and adopt attention mechanisms to improve 3D reconstruction results, but it would increased computational complexity.","This observation prompts us to propose a new and concise architecture while improving computational efficiency.","In this work, we propose a simple and effective 3D hand mesh reconstruction network HandSSCA, which is the first to incorporate state space modeling into the field of hand pose estimation.","In the network, we have designed a novel state space channel attention module that extends the effective sensory field, extracts hand features in the spatial dimension, and enhances hand regional features in the channel dimension.","This design helps to reconstruct a complete and detailed hand mesh.","Extensive experiments conducted on well-known datasets featuring challenging hand-object occlusions (such as FREIHAND, DEXYCB, and HO3D) demonstrate that our proposed HandSSCA achieves state-of-the-art performance while maintaining a minimal parameter count."],"url":"http://arxiv.org/abs/2405.01066v1","category":"cs.CV"}
{"created":"2024-05-02 07:34:12","title":"A text-based, generative deep learning model for soil reflectance spectrum simulation in the VIS-NIR (400-2499 nm) bands","abstract":"Simulating soil reflectance spectra is invaluable for soil-plant radiative modeling and training machine learning models, yet it is difficult as the intricate relationships between soil structure and its constituents. To address this, a fully data-driven soil optics generative model (SOGM) for simulation of soil reflectance spectra based on soil property inputs was developed. The model is trained on an extensive dataset comprising nearly 180,000 soil spectra-property pairs from 17 datasets. It generates soil reflectance spectra from text-based inputs describing soil properties and their values rather than only numerical values and labels in binary vector format. The generative model can simulate output spectra based on an incomplete set of input properties. SOGM is based on the denoising diffusion probabilistic model (DDPM). Two additional sub-models were also built to complement the SOGM: a spectral padding model that can fill in the gaps for spectra shorter than the full visible-near-infrared range (VIS-NIR; 400 to 2499 nm), and a wet soil spectra model that can estimate the effects of water content on soil reflectance spectra given the dry spectrum predicted by the SOGM. The SOGM was up-scaled by coupling with the Helios 3D plant modeling software, which allowed for generation of synthetic aerial images of simulated soil and plant scenes. It can also be easily integrated with soil-plant radiation model used for remote sensin research like PROSAIL. The testing results of the SOGM on new datasets that not included in model training proved that the model can generate reasonable soil reflectance spectra based on available property inputs. The presented models are openly accessible on: https://github.com/GEMINI-Breeding/SOGM_soil_spectra_simulation.","sentences":["Simulating soil reflectance spectra is invaluable for soil-plant radiative modeling and training machine learning models, yet it is difficult as the intricate relationships between soil structure and its constituents.","To address this, a fully data-driven soil optics generative model (SOGM) for simulation of soil reflectance spectra based on soil property inputs was developed.","The model is trained on an extensive dataset comprising nearly 180,000 soil spectra-property pairs from 17 datasets.","It generates soil reflectance spectra from text-based inputs describing soil properties and their values rather than only numerical values and labels in binary vector format.","The generative model can simulate output spectra based on an incomplete set of input properties.","SOGM is based on the denoising diffusion probabilistic model (DDPM).","Two additional sub-models were also built to complement the SOGM: a spectral padding model that can fill in the gaps for spectra shorter than the full visible-near-infrared range (VIS-NIR; 400 to 2499 nm), and a wet soil spectra model that can estimate the effects of water content on soil reflectance spectra given the dry spectrum predicted by the SOGM.","The SOGM was up-scaled by coupling with the Helios 3D plant modeling software, which allowed for generation of synthetic aerial images of simulated soil and plant scenes.","It can also be easily integrated with soil-plant radiation model used for remote sensin research like PROSAIL.","The testing results of the SOGM on new datasets that not included in model training proved that the model can generate reasonable soil reflectance spectra based on available property inputs.","The presented models are openly accessible on: https://github.com/GEMINI-Breeding/SOGM_soil_spectra_simulation."],"url":"http://arxiv.org/abs/2405.01060v1","category":"cs.LG"}
{"created":"2024-05-02 07:34:03","title":"Dense dipole-dipole-coupled two-level systems in a thermal bath","abstract":"The quantum dynamics of a dense and dipole-dipole coupled ensemble of two-level emitters interacting via their environmental thermostat is investigated. The static dipole-dipole interaction strengths are being considered strong enough but smaller than the transition frequency. Therefore, the established thermal equilibrium of ensemble's quantum dynamics is described with respect to the dipole-dipole coupling strengths. We have demonstrated the quantum nature of the spontaneously scattered light field in this process for weaker thermal baths as well as non-negligible dipole-dipole couplings compared to the emitter's transition frequency. Furthermore, the collectively emitted photon intensity suppresses or enhances depending on the environmental thermal baths intensities.","sentences":["The quantum dynamics of a dense and dipole-dipole coupled ensemble of two-level emitters interacting via their environmental thermostat is investigated.","The static dipole-dipole interaction strengths are being considered strong enough but smaller than the transition frequency.","Therefore, the established thermal equilibrium of ensemble's quantum dynamics is described with respect to the dipole-dipole coupling strengths.","We have demonstrated the quantum nature of the spontaneously scattered light field in this process for weaker thermal baths as well as non-negligible dipole-dipole couplings compared to the emitter's transition frequency.","Furthermore, the collectively emitted photon intensity suppresses or enhances depending on the environmental thermal baths intensities."],"url":"http://arxiv.org/abs/2405.01059v1","category":"quant-ph"}
{"created":"2024-05-02 07:28:27","title":"Leverage Multi-source Traffic Demand Data Fusion with Transformer Model for Urban Parking Prediction","abstract":"The escalation in urban private car ownership has worsened the urban parking predicament, necessitating effective parking availability prediction for urban planning and management. However, the existing prediction methods suffer from low prediction accuracy with the lack of spatial-temporal correlation features related to parking volume, and neglect of flow patterns and correlations between similar parking lots within certain areas. To address these challenges, this study proposes a parking availability prediction framework integrating spatial-temporal deep learning with multi-source data fusion, encompassing traffic demand data from multiple sources (e.g., metro, bus, taxi services), and parking lot data. The framework is based on the Transformer as the spatial-temporal deep learning model and leverages K-means clustering to establish parking cluster zones, extracting and integrating traffic demand characteristics from various transportation modes (i.e., metro, bus, online ride-hailing, and taxi) connected to parking lots. Real-world empirical data was used to verify the effectiveness of the proposed method compared with different machine learning, deep learning, and traditional statistical models for predicting parking availability. Experimental results reveal that, with the proposed pipeline, the developed Transformer model outperforms other models in terms of various metrics, e.g., Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). By fusing multi-source demanding data with spatial-temporal deep learning techniques, this approach offers the potential to develop parking availability prediction systems that furnish more accurate and timely information to both drivers and urban planners, thereby fostering more efficient and sustainable urban mobility.","sentences":["The escalation in urban private car ownership has worsened the urban parking predicament, necessitating effective parking availability prediction for urban planning and management.","However, the existing prediction methods suffer from low prediction accuracy with the lack of spatial-temporal correlation features related to parking volume, and neglect of flow patterns and correlations between similar parking lots within certain areas.","To address these challenges, this study proposes a parking availability prediction framework integrating spatial-temporal deep learning with multi-source data fusion, encompassing traffic demand data from multiple sources (e.g., metro, bus, taxi services), and parking lot data.","The framework is based on the Transformer as the spatial-temporal deep learning model and leverages K-means clustering to establish parking cluster zones, extracting and integrating traffic demand characteristics from various transportation modes (i.e., metro, bus, online ride-hailing, and taxi) connected to parking lots.","Real-world empirical data was used to verify the effectiveness of the proposed method compared with different machine learning, deep learning, and traditional statistical models for predicting parking availability.","Experimental results reveal that, with the proposed pipeline, the developed Transformer model outperforms other models in terms of various metrics, e.g., Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE).","By fusing multi-source demanding data with spatial-temporal deep learning techniques, this approach offers the potential to develop parking availability prediction systems that furnish more accurate and timely information to both drivers and urban planners, thereby fostering more efficient and sustainable urban mobility."],"url":"http://arxiv.org/abs/2405.01055v1","category":"cs.LG"}
{"created":"2024-05-02 07:15:23","title":"Explicitly Modeling Generality into Self-Supervised Learning","abstract":"The goal of generality in machine learning is to achieve excellent performance on various unseen tasks and domains. Recently, self-supervised learning (SSL) has been regarded as an effective method to achieve this goal. It can learn high-quality representations from unlabeled data and achieve promising empirical performance on multiple downstream tasks. Existing SSL methods mainly constrain generality from two aspects: (i) large-scale training data, and (ii) learning task-level shared knowledge. However, these methods lack explicit modeling of the SSL generality in the learning objective, and the theoretical understanding of SSL's generality remains limited. This may cause SSL models to overfit in data-scarce situations and generalize poorly in the real world, making it difficult to achieve true generality. To address these issues, we provide a theoretical definition of generality in SSL and define a $\\sigma$-measurement to help quantify it. Based on this insight, we explicitly model generality into self-supervised learning and further propose a novel SSL framework, called GeSSL. It introduces a self-motivated target based on $\\sigma$-measurement, which enables the model to find the optimal update direction towards generality. Extensive theoretical and empirical evaluations demonstrate the superior performance of the proposed GeSSL.","sentences":["The goal of generality in machine learning is to achieve excellent performance on various unseen tasks and domains.","Recently, self-supervised learning (SSL) has been regarded as an effective method to achieve this goal.","It can learn high-quality representations from unlabeled data and achieve promising empirical performance on multiple downstream tasks.","Existing SSL methods mainly constrain generality from two aspects: (i) large-scale training data, and (ii) learning task-level shared knowledge.","However, these methods lack explicit modeling of the SSL generality in the learning objective, and the theoretical understanding of SSL's generality remains limited.","This may cause SSL models to overfit in data-scarce situations and generalize poorly in the real world, making it difficult to achieve true generality.","To address these issues, we provide a theoretical definition of generality in SSL and define a $\\sigma$-measurement to help quantify it.","Based on this insight, we explicitly model generality into self-supervised learning and further propose a novel SSL framework, called GeSSL.","It introduces a self-motivated target based on $\\sigma$-measurement, which enables the model to find the optimal update direction towards generality.","Extensive theoretical and empirical evaluations demonstrate the superior performance of the proposed GeSSL."],"url":"http://arxiv.org/abs/2405.01053v1","category":"cs.LG"}
{"created":"2024-05-02 06:33:01","title":"LOQA: Learning with Opponent Q-Learning Awareness","abstract":"In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent Q-Learning Awareness (LOQA), a novel, decentralized reinforcement learning algorithm tailored to optimizing an agent's individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes the opponent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-the-art performance in benchmark scenarios such as the Iterated Prisoner's Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint, making it a promising approach for practical multi-agent applications.","sentences":["In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility.","Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare.","In this paper we introduce Learning with Opponent Q-Learning Awareness (LOQA), a novel, decentralized reinforcement learning algorithm tailored to optimizing an agent's individual utility while fostering cooperation among adversaries in partially competitive environments.","LOQA assumes the opponent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-the-art performance in benchmark scenarios such as the Iterated Prisoner's Dilemma and the Coin Game.","LOQA achieves these outcomes with a significantly reduced computational footprint, making it a promising approach for practical multi-agent applications."],"url":"http://arxiv.org/abs/2405.01035v1","category":"cs.GT"}
{"created":"2024-05-02 06:32:55","title":"Collective nature of high-Q resonances in finite-size photonic metastructures","abstract":"We study high quality-factor (high Q) resonances supported by periodic arrays of Mie resonators from the perspectives of both Bloch wave theory and multiple scattering theory. We reveal that, unlike a common belief, the bound states in the continuum (BICs) derived by the Bloch-wave theory do not directly determine the resonance with the highest Q value in large but finite arrays. Higher Q factors appear to be associated with collective resonances formed by nominally guided modes below the light line associated with strong effect of both electric and magnetic multipoles. Our findings offer valuable insights into accessing the modes with higher Q resonances via bonding modes within finite metastructures. Our results underpin the pivotal significance of magnetic and electric multipoles in the design of resonant metadevices and nonlocal flat-band optics. Moreover, our demonstrations reveal that coupled arrays of high-Q microcavities do not inherently result in a stronger light-matter interaction when compared to coupled low-Q nanoresonators. This result emphasizes the critical importance of the study of multiple light-scattering effects in cavity-based systems.","sentences":["We study high quality-factor (high Q) resonances supported by periodic arrays of Mie resonators from the perspectives of both Bloch wave theory and multiple scattering theory.","We reveal that, unlike a common belief, the bound states in the continuum (BICs) derived by the Bloch-wave theory do not directly determine the resonance with the highest Q value in large but finite arrays.","Higher Q factors appear to be associated with collective resonances formed by nominally guided modes below the light line associated with strong effect of both electric and magnetic multipoles.","Our findings offer valuable insights into accessing the modes with higher Q resonances via bonding modes within finite metastructures.","Our results underpin the pivotal significance of magnetic and electric multipoles in the design of resonant metadevices and nonlocal flat-band optics.","Moreover, our demonstrations reveal that coupled arrays of high-Q microcavities do not inherently result in a stronger light-matter interaction when compared to coupled low-Q nanoresonators.","This result emphasizes the critical importance of the study of multiple light-scattering effects in cavity-based systems."],"url":"http://arxiv.org/abs/2405.01034v1","category":"physics.optics"}
{"created":"2024-05-02 06:02:07","title":"MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts","abstract":"Learning to solve vehicle routing problems (VRPs) has garnered much attention. However, most neural solvers are only structured and trained independently on a specific problem, making them less generic and practical. In this paper, we aim to develop a unified neural solver that can cope with a range of VRP variants simultaneously. Specifically, we propose a multi-task vehicle routing solver with mixture-of-experts (MVMoE), which greatly enhances the model capacity without a proportional increase in computation. We further develop a hierarchical gating mechanism for the MVMoE, delivering a good trade-off between empirical performance and computational complexity. Experimentally, our method significantly promotes the zero-shot generalization performance on 10 unseen VRP variants, and showcases decent results on the few-shot setting and real-world benchmark instances. We further provide extensive studies on the effect of MoE configurations in solving VRPs. Surprisingly, the hierarchical gating can achieve much better out-of-distribution generalization performance. The source code is available at: https://github.com/RoyalSkye/Routing-MVMoE.","sentences":["Learning to solve vehicle routing problems (VRPs) has garnered much attention.","However, most neural solvers are only structured and trained independently on a specific problem, making them less generic and practical.","In this paper, we aim to develop a unified neural solver that can cope with a range of VRP variants simultaneously.","Specifically, we propose a multi-task vehicle routing solver with mixture-of-experts (MVMoE), which greatly enhances the model capacity without a proportional increase in computation.","We further develop a hierarchical gating mechanism for the MVMoE, delivering a good trade-off between empirical performance and computational complexity.","Experimentally, our method significantly promotes the zero-shot generalization performance on 10 unseen VRP variants, and showcases decent results on the few-shot setting and real-world benchmark instances.","We further provide extensive studies on the effect of MoE configurations in solving VRPs.","Surprisingly, the hierarchical gating can achieve much better out-of-distribution generalization performance.","The source code is available at: https://github.com/RoyalSkye/Routing-MVMoE."],"url":"http://arxiv.org/abs/2405.01029v1","category":"cs.AI"}
{"created":"2024-05-02 05:46:13","title":"UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation","abstract":"Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.","sentences":["Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference.","Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference.","However, their applicability to various domains is limited because they tend to generate domain-specific datasets.","In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain.","This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm.","Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs."],"url":"http://arxiv.org/abs/2405.01022v1","category":"cs.CL"}
{"created":"2024-05-02 05:35:10","title":"Addressing Diverging Training Costs using Local Restoration for Precise Bird's Eye View Map Construction","abstract":"Recent advancements in Bird's Eye View (BEV) fusion for map construction have demonstrated remarkable mapping of urban environments. However, their deep and bulky architecture incurs substantial amounts of backpropagation memory and computing latency. Consequently, the problem poses an unavoidable bottleneck in constructing high-resolution (HR) BEV maps, as their large-sized features cause significant increases in costs including GPU memory consumption and computing latency, named diverging training costs issue. Affected by the problem, most existing methods adopt low-resolution (LR) BEV and struggle to estimate the precise locations of urban scene components like road lanes, and sidewalks. As the imprecision leads to risky self-driving, the diverging training costs issue has to be resolved. In this paper, we address the issue with our novel Trumpet Neural Network (TNN) mechanism. The framework utilizes LR BEV space and outputs an up-sampled semantic BEV map to create a memory-efficient pipeline. To this end, we introduce Local Restoration of BEV representation. Specifically, the up-sampled BEV representation has severely aliased, blocky signals, and thick semantic labels. Our proposed Local Restoration restores the signals and thins (or narrows down) the width of the labels. Our extensive experiments show that the TNN mechanism provides a plug-and-play memory-efficient pipeline, thereby enabling the effective estimation of real-sized (or precise) semantic labels for BEV map construction.","sentences":["Recent advancements in Bird's Eye View (BEV) fusion for map construction have demonstrated remarkable mapping of urban environments.","However, their deep and bulky architecture incurs substantial amounts of backpropagation memory and computing latency.","Consequently, the problem poses an unavoidable bottleneck in constructing high-resolution (HR) BEV maps, as their large-sized features cause significant increases in costs including GPU memory consumption and computing latency, named diverging training costs issue.","Affected by the problem, most existing methods adopt low-resolution (LR) BEV and struggle to estimate the precise locations of urban scene components like road lanes, and sidewalks.","As the imprecision leads to risky self-driving, the diverging training costs issue has to be resolved.","In this paper, we address the issue with our novel Trumpet Neural Network (TNN) mechanism.","The framework utilizes LR BEV space and outputs an up-sampled semantic BEV map to create a memory-efficient pipeline.","To this end, we introduce Local Restoration of BEV representation.","Specifically, the up-sampled BEV representation has severely aliased, blocky signals, and thick semantic labels.","Our proposed Local Restoration restores the signals and thins (or narrows down) the width of the labels.","Our extensive experiments show that the TNN mechanism provides a plug-and-play memory-efficient pipeline, thereby enabling the effective estimation of real-sized (or precise) semantic labels for BEV map construction."],"url":"http://arxiv.org/abs/2405.01016v1","category":"cs.CV"}
{"created":"2024-05-02 05:29:22","title":"Non-clairvoyant Scheduling with Partial Predictions","abstract":"The non-clairvoyant scheduling problem has gained new interest within learning-augmented algorithms, where the decision-maker is equipped with predictions without any quality guarantees. In practical settings, access to predictions may be reduced to specific instances, due to cost or data limitations. Our investigation focuses on scenarios where predictions for only $B$ job sizes out of $n$ are available to the algorithm. We first establish near-optimal lower bounds and algorithms in the case of perfect predictions. Subsequently, we present a learning-augmented algorithm satisfying the robustness, consistency, and smoothness criteria, and revealing a novel tradeoff between consistency and smoothness inherent in the scenario with a restricted number of predictions.","sentences":["The non-clairvoyant scheduling problem has gained new interest within learning-augmented algorithms, where the decision-maker is equipped with predictions without any quality guarantees.","In practical settings, access to predictions may be reduced to specific instances, due to cost or data limitations.","Our investigation focuses on scenarios where predictions for only $B$ job sizes out of $n$ are available to the algorithm.","We first establish near-optimal lower bounds and algorithms in the case of perfect predictions.","Subsequently, we present a learning-augmented algorithm satisfying the robustness, consistency, and smoothness criteria, and revealing a novel tradeoff between consistency and smoothness inherent in the scenario with a restricted number of predictions."],"url":"http://arxiv.org/abs/2405.01013v1","category":"cs.LG"}
{"created":"2024-05-02 05:09:07","title":"Deep Learning Models in Speech Recognition: Measuring GPU Energy Consumption, Impact of Noise and Model Quantization for Edge Deployment","abstract":"Recent transformer-based ASR models have achieved word-error rates (WER) below 4%, surpassing human annotator accuracy, yet they demand extensive server resources, contributing to significant carbon footprints. The traditional server-based architecture of ASR also presents privacy concerns, alongside reliability and latency issues due to network dependencies. In contrast, on-device (edge) ASR enhances privacy, boosts performance, and promotes sustainability by effectively balancing energy use and accuracy for specific applications. This study examines the effects of quantization, memory demands, and energy consumption on the performance of various ASR model inference on the NVIDIA Jetson Orin Nano. By analyzing WER and transcription speed across models using FP32, FP16, and INT8 quantization on clean and noisy datasets, we highlight the crucial trade-offs between accuracy, speeds, quantization, energy efficiency, and memory needs. We found that changing precision from fp32 to fp16 halves the energy consumption for audio transcription across different models, with minimal performance degradation. A larger model size and number of parameters neither guarantees better resilience to noise, nor predicts the energy consumption for a given transcription load. These, along with several other findings offer novel insights for optimizing ASR systems within energy- and memory-limited environments, crucial for the development of efficient on-device ASR solutions. The code and input data needed to reproduce the results in this article are open sourced are available on [https://github.com/zzadiues3338/ASR-energy-jetson].","sentences":["Recent transformer-based ASR models have achieved word-error rates (WER) below 4%, surpassing human annotator accuracy, yet they demand extensive server resources, contributing to significant carbon footprints.","The traditional server-based architecture of ASR also presents privacy concerns, alongside reliability and latency issues due to network dependencies.","In contrast, on-device (edge)","ASR enhances privacy, boosts performance, and promotes sustainability by effectively balancing energy use and accuracy for specific applications.","This study examines the effects of quantization, memory demands, and energy consumption on the performance of various ASR model inference on the NVIDIA Jetson Orin Nano.","By analyzing WER and transcription speed across models using FP32, FP16, and INT8 quantization on clean and noisy datasets, we highlight the crucial trade-offs between accuracy, speeds, quantization, energy efficiency, and memory needs.","We found that changing precision from fp32 to fp16 halves the energy consumption for audio transcription across different models, with minimal performance degradation.","A larger model size and number of parameters neither guarantees better resilience to noise, nor predicts the energy consumption for a given transcription load.","These, along with several other findings offer novel insights for optimizing ASR systems within energy- and memory-limited environments, crucial for the development of efficient on-device ASR solutions.","The code and input data needed to reproduce the results in this article are open sourced are available on [https://github.com/zzadiues3338/ASR-energy-jetson]."],"url":"http://arxiv.org/abs/2405.01004v1","category":"cs.SD"}
{"created":"2024-05-02 04:58:29","title":"Spider: A Unified Framework for Context-dependent Concept Understanding","abstract":"Different from the context-independent (CI) concepts such as human, car, and airplane, context-dependent (CD) concepts require higher visual understanding ability, such as camouflaged object and medical lesion. Despite the rapid advance of many CD understanding tasks in respective branches, the isolated evolution leads to their limited cross-domain generalisation and repetitive technique innovation. Since there is a strong coupling relationship between foreground and background context in CD tasks, existing methods require to train separate models in their focused domains. This restricts their real-world CD concept understanding towards artificial general intelligence (AGI). We propose a unified model with a single set of parameters, Spider, which only needs to be trained once. With the help of the proposed concept filter driven by the image-mask group prompt, Spider is able to understand and distinguish diverse strong context-dependent concepts to accurately capture the Prompter's intention. Without bells and whistles, Spider significantly outperforms the state-of-the-art specialized models in 8 different context-dependent segmentation tasks, including 4 natural scenes (salient, camouflaged, and transparent objects and shadow) and 4 medical lesions (COVID-19, polyp, breast, and skin lesion with color colonoscopy, CT, ultrasound, and dermoscopy modalities). Besides, Spider shows obvious advantages in continuous learning. It can easily complete the training of new tasks by fine-tuning parameters less than 1\\% and bring a tolerable performance degradation of less than 5\\% for all old tasks. The source code will be publicly available at \\href{https://github.com/Xiaoqi-Zhao-DLUT/Spider-UniCDSeg}{Spider-UniCDSeg}.","sentences":["Different from the context-independent (CI) concepts such as human, car, and airplane, context-dependent (CD) concepts require higher visual understanding ability, such as camouflaged object and medical lesion.","Despite the rapid advance of many CD understanding tasks in respective branches, the isolated evolution leads to their limited cross-domain generalisation and repetitive technique innovation.","Since there is a strong coupling relationship between foreground and background context in CD tasks, existing methods require to train separate models in their focused domains.","This restricts their real-world CD concept understanding towards artificial general intelligence (AGI).","We propose a unified model with a single set of parameters, Spider, which only needs to be trained once.","With the help of the proposed concept filter driven by the image-mask group prompt, Spider is able to understand and distinguish diverse strong context-dependent concepts to accurately capture the Prompter's intention.","Without bells and whistles, Spider significantly outperforms the state-of-the-art specialized models in 8 different context-dependent segmentation tasks, including 4 natural scenes (salient, camouflaged, and transparent objects and shadow) and 4 medical lesions (COVID-19, polyp, breast, and skin lesion with color colonoscopy, CT, ultrasound, and dermoscopy modalities).","Besides, Spider shows obvious advantages in continuous learning.","It can easily complete the training of new tasks by fine-tuning parameters less than 1\\% and bring a tolerable performance degradation of less than 5\\% for all old tasks.","The source code will be publicly available at \\href{https://github.com/Xiaoqi-Zhao-DLUT/Spider-UniCDSeg}{Spider-UniCDSeg}."],"url":"http://arxiv.org/abs/2405.01002v1","category":"cs.CV"}
{"created":"2024-05-02 04:25:35","title":"Not a Swiss Army Knife: Academics' Perceptions of Trade-Offs Around Generative Artificial Intelligence Use","abstract":"In the rapidly evolving landscape of computing disciplines, substantial efforts are being dedicated to unraveling the sociotechnical implications of generative AI (Gen AI). While existing research has manifested in various forms, there remains a notable gap concerning the direct engagement of knowledge workers in academia with Gen AI. We interviewed 18 knowledge workers, including faculty and students, to investigate the social and technical dimensions of Gen AI from their perspective. Our participants raised concerns about the opacity of the data used to train Gen AI. This lack of transparency makes it difficult to identify and address inaccurate, biased, and potentially harmful, information generated by these models. Knowledge workers also expressed worries about Gen AI undermining trust in the relationship between instructor and student and discussed potential solutions, such as pedagogy readiness, to mitigate them. Additionally, participants recognized Gen AI's potential to democratize knowledge by accelerating the learning process and act as an accessible research assistant. However, there were also concerns about potential social and power imbalances stemming from unequal access to such technologies. Our study offers insights into the concerns and hopes of knowledge workers about the ethical use of Gen AI in educational settings and beyond, with implications for navigating this new landscape.","sentences":["In the rapidly evolving landscape of computing disciplines, substantial efforts are being dedicated to unraveling the sociotechnical implications of generative AI (Gen AI).","While existing research has manifested in various forms, there remains a notable gap concerning the direct engagement of knowledge workers in academia with Gen AI.","We interviewed 18 knowledge workers, including faculty and students, to investigate the social and technical dimensions of Gen AI from their perspective.","Our participants raised concerns about the opacity of the data used to train Gen AI.","This lack of transparency makes it difficult to identify and address inaccurate, biased, and potentially harmful, information generated by these models.","Knowledge workers also expressed worries about Gen AI undermining trust in the relationship between instructor and student and discussed potential solutions, such as pedagogy readiness, to mitigate them.","Additionally, participants recognized Gen AI's potential to democratize knowledge by accelerating the learning process and act as an accessible research assistant.","However, there were also concerns about potential social and power imbalances stemming from unequal access to such technologies.","Our study offers insights into the concerns and hopes of knowledge workers about the ethical use of Gen AI in educational settings and beyond, with implications for navigating this new landscape."],"url":"http://arxiv.org/abs/2405.00995v1","category":"cs.CY"}
{"created":"2024-05-02 03:53:59","title":"Estimate the building height at a 10-meter resolution based on Sentinel data","abstract":"Building height is an important indicator for scientific research and practical application. However, building height products with a high spatial resolution (10m) are still very scarce. To meet the needs of high-resolution building height estimation models, this study established a set of spatial-spectral-temporal feature databases, combining SAR data provided by Sentinel-1, optical data provided by Sentinel-2, and shape data provided by building footprints. The statistical indicators on the time scale are extracted to form a rich database of 160 features. This study combined with permutation feature importance, Shapley Additive Explanations, and Random Forest variable importance, and the final stable features are obtained through an expert scoring system. This study took 12 large, medium, and small cities in the United States as the training data. It used moving windows to aggregate the pixels to solve the impact of SAR image displacement and building shadows. This study built a building height model based on a random forest model and compared three model ensemble methods of bagging, boosting, and stacking. To evaluate the accuracy of the prediction results, this study collected Lidar data in the test area, and the evaluation results showed that its R-Square reached 0.78, which can prove that the building height can be obtained effectively. The fast production of high-resolution building height data can support large-scale scientific research and application in many fields.","sentences":["Building height is an important indicator for scientific research and practical application.","However, building height products with a high spatial resolution (10m) are still very scarce.","To meet the needs of high-resolution building height estimation models, this study established a set of spatial-spectral-temporal feature databases, combining SAR data provided by Sentinel-1, optical data provided by Sentinel-2, and shape data provided by building footprints.","The statistical indicators on the time scale are extracted to form a rich database of 160 features.","This study combined with permutation feature importance, Shapley Additive Explanations, and Random Forest variable importance, and the final stable features are obtained through an expert scoring system.","This study took 12 large, medium, and small cities in the United States as the training data.","It used moving windows to aggregate the pixels to solve the impact of SAR image displacement and building shadows.","This study built a building height model based on a random forest model and compared three model ensemble methods of bagging, boosting, and stacking.","To evaluate the accuracy of the prediction results, this study collected Lidar data in the test area, and the evaluation results showed that its R-Square reached 0.78, which can prove that the building height can be obtained effectively.","The fast production of high-resolution building height data can support large-scale scientific research and application in many fields."],"url":"http://arxiv.org/abs/2405.00989v1","category":"cs.CV"}
{"created":"2024-05-02 03:50:31","title":"Context-Aware Clustering using Large Language Models","abstract":"Despite the remarkable success of Large Language Models (LLMs) in text understanding and generation, their potential for text clustering tasks remains underexplored. We observed that powerful closed-source LLMs provide good quality clusterings of entity sets but are not scalable due to the massive compute power required and the associated costs. Thus, we propose CACTUS (Context-Aware ClusTering with aUgmented triplet losS), a systematic approach that leverages open-source LLMs for efficient and effective supervised clustering of entity subsets, particularly focusing on text-based entities. Existing text clustering methods fail to effectively capture the context provided by the entity subset. Moreover, though there are several language modeling based approaches for clustering, very few are designed for the task of supervised clustering. This paper introduces a novel approach towards clustering entity subsets using LLMs by capturing context via a scalable inter-entity attention mechanism. We propose a novel augmented triplet loss function tailored for supervised clustering, which addresses the inherent challenges of directly applying the triplet loss to this problem. Furthermore, we introduce a self-supervised clustering task based on text augmentation techniques to improve the generalization of our model. For evaluation, we collect ground truth clusterings from a closed-source LLM and transfer this knowledge to an open-source LLM under the supervised clustering framework, allowing a faster and cheaper open-source model to perform the same task. Experiments on various e-commerce query and product clustering datasets demonstrate that our proposed approach significantly outperforms existing unsupervised and supervised baselines under various external clustering evaluation metrics.","sentences":["Despite the remarkable success of Large Language Models (LLMs) in text understanding and generation, their potential for text clustering tasks remains underexplored.","We observed that powerful closed-source LLMs provide good quality clusterings of entity sets but are not scalable due to the massive compute power required and the associated costs.","Thus, we propose CACTUS (Context-Aware ClusTering with aUgmented triplet losS), a systematic approach that leverages open-source LLMs for efficient and effective supervised clustering of entity subsets, particularly focusing on text-based entities.","Existing text clustering methods fail to effectively capture the context provided by the entity subset.","Moreover, though there are several language modeling based approaches for clustering, very few are designed for the task of supervised clustering.","This paper introduces a novel approach towards clustering entity subsets using LLMs by capturing context via a scalable inter-entity attention mechanism.","We propose a novel augmented triplet loss function tailored for supervised clustering, which addresses the inherent challenges of directly applying the triplet loss to this problem.","Furthermore, we introduce a self-supervised clustering task based on text augmentation techniques to improve the generalization of our model.","For evaluation, we collect ground truth clusterings from a closed-source LLM and transfer this knowledge to an open-source LLM under the supervised clustering framework, allowing a faster and cheaper open-source model to perform the same task.","Experiments on various e-commerce query and product clustering datasets demonstrate that our proposed approach significantly outperforms existing unsupervised and supervised baselines under various external clustering evaluation metrics."],"url":"http://arxiv.org/abs/2405.00988v1","category":"cs.CL"}
{"created":"2024-05-02 03:48:08","title":"Progressive Feedforward Collapse of ResNet Training","abstract":"Neural collapse (NC) is a simple and symmetric phenomenon for deep neural networks (DNNs) at the terminal phase of training, where the last-layer features collapse to their class means and form a simplex equiangular tight frame aligning with the classifier vectors. However, the relationship of the last-layer features to the data and intermediate layers during training remains unexplored. To this end, we characterize the geometry of intermediate layers of ResNet and propose a novel conjecture, progressive feedforward collapse (PFC), claiming the degree of collapse increases during the forward propagation of DNNs. We derive a transparent model for the well-trained ResNet according to that ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase. The metrics of PFC indeed monotonically decrease across depth on various datasets. We propose a new surrogate model, multilayer unconstrained feature model (MUFM), connecting intermediate layers by an optimal transport regularizer. The optimal solution of MUFM is inconsistent with NC but is more concentrated relative to the input data. Overall, this study extends NC to PFC to model the collapse phenomenon of intermediate layers and its dependence on the input data, shedding light on the theoretical understanding of ResNet in classification problems.","sentences":["Neural collapse (NC) is a simple and symmetric phenomenon for deep neural networks (DNNs) at the terminal phase of training, where the last-layer features collapse to their class means and form a simplex equiangular tight frame aligning with the classifier vectors.","However, the relationship of the last-layer features to the data and intermediate layers during training remains unexplored.","To this end, we characterize the geometry of intermediate layers of ResNet and propose a novel conjecture, progressive feedforward collapse (PFC), claiming the degree of collapse increases during the forward propagation of DNNs.","We derive a transparent model for the well-trained ResNet according to that ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase.","The metrics of PFC indeed monotonically decrease across depth on various datasets.","We propose a new surrogate model, multilayer unconstrained feature model (MUFM), connecting intermediate layers by an optimal transport regularizer.","The optimal solution of MUFM is inconsistent with NC but is more concentrated relative to the input data.","Overall, this study extends NC to PFC to model the collapse phenomenon of intermediate layers and its dependence on the input data, shedding light on the theoretical understanding of ResNet in classification problems."],"url":"http://arxiv.org/abs/2405.00985v1","category":"cs.LG"}
{"created":"2024-05-02 03:43:19","title":"FREE: Faster and Better Data-Free Meta-Learning","abstract":"Data-Free Meta-Learning (DFML) aims to extract knowledge from a collection of pre-trained models without requiring the original data, presenting practical benefits in contexts constrained by data privacy concerns. Current DFML methods primarily focus on the data recovery from these pre-trained models. However, they suffer from slow recovery speed and overlook gaps inherent in heterogeneous pre-trained models. In response to these challenges, we introduce the Faster and Better Data-Free Meta-Learning (FREE) framework, which contains: (i) a meta-generator for rapidly recovering training tasks from pre-trained models; and (ii) a meta-learner for generalizing to new unseen tasks. Specifically, within the module Faster Inversion via Meta-Generator, each pre-trained model is perceived as a distinct task. The meta-generator can rapidly adapt to a specific task in just five steps, significantly accelerating the data recovery. Furthermore, we propose Better Generalization via Meta-Learner and introduce an implicit gradient alignment algorithm to optimize the meta-learner. This is achieved as aligned gradient directions alleviate potential conflicts among tasks from heterogeneous pre-trained models. Empirical experiments on multiple benchmarks affirm the superiority of our approach, marking a notable speed-up (20$\\times$) and performance enhancement (1.42\\% $\\sim$ 4.78\\%) in comparison to the state-of-the-art.","sentences":["Data-Free Meta-Learning (DFML) aims to extract knowledge from a collection of pre-trained models without requiring the original data, presenting practical benefits in contexts constrained by data privacy concerns.","Current DFML methods primarily focus on the data recovery from these pre-trained models.","However, they suffer from slow recovery speed and overlook gaps inherent in heterogeneous pre-trained models.","In response to these challenges, we introduce the Faster and Better Data-Free Meta-Learning (FREE) framework, which contains: (i) a meta-generator for rapidly recovering training tasks from pre-trained models; and (ii) a meta-learner for generalizing to new unseen tasks.","Specifically, within the module Faster Inversion via Meta-Generator, each pre-trained model is perceived as a distinct task.","The meta-generator can rapidly adapt to a specific task in just five steps, significantly accelerating the data recovery.","Furthermore, we propose Better Generalization via Meta-Learner and introduce an implicit gradient alignment algorithm to optimize the meta-learner.","This is achieved as aligned gradient directions alleviate potential conflicts among tasks from heterogeneous pre-trained models.","Empirical experiments on multiple benchmarks affirm the superiority of our approach, marking a notable speed-up (20$\\times$) and performance enhancement (1.42\\% $\\sim$ 4.78\\%) in comparison to the state-of-the-art."],"url":"http://arxiv.org/abs/2405.00984v1","category":"cs.LG"}
{"created":"2024-05-02 03:35:21","title":"Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation","abstract":"Designing preference elicitation (PE) methodologies that can quickly ascertain a user's top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) constitute a novel technology that enables fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multi-turn, decision-theoretic reasoning required to effectively balance the NL exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but fail to use NL item descriptions or generate NL queries, unrealistically assuming users can express preferences with direct item ratings and comparisons. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to generate NL queries which actively elicit natural language feedback to reduce uncertainty over item utilities to identify the best recommendation. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain preference beliefs and BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation. We numerically evaluate our methods in controlled experiments, finding that PEBOL achieves up to 131% improvement in MAP@10 after 10 turns of cold start NL-PE dialogue compared to monolithic GPT-3.5, despite relying on a much smaller 400M parameter NLI model for preference inference.","sentences":["Designing preference elicitation (PE) methodologies that can quickly ascertain a user's top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems.","While large language models (LLMs) constitute a novel technology that enables fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multi-turn, decision-theoretic reasoning required to effectively balance the NL exploration and exploitation of user preferences towards an arbitrary item set.","In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but fail to use NL item descriptions or generate NL queries, unrealistically assuming users can express preferences with direct item ratings and comparisons.","To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to generate NL queries which actively elicit natural language feedback to reduce uncertainty over item utilities to identify the best recommendation.","We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain preference beliefs and BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation.","We numerically evaluate our methods in controlled experiments, finding that PEBOL achieves up to 131% improvement in MAP@10 after 10 turns of cold start NL-PE dialogue compared to monolithic GPT-3.5, despite relying on a much smaller 400M parameter NLI model for preference inference."],"url":"http://arxiv.org/abs/2405.00981v1","category":"cs.AI"}
{"created":"2024-05-02 03:33:17","title":"A Hong Kong Sign Language Corpus Collected from Sign-interpreted TV News","abstract":"This paper introduces TVB-HKSL-News, a new Hong Kong sign language (HKSL) dataset collected from a TV news program over a period of 7 months. The dataset is collected to enrich resources for HKSL and support research in large-vocabulary continuous sign language recognition (SLR) and translation (SLT). It consists of 16.07 hours of sign videos of two signers with a vocabulary of 6,515 glosses (for SLR) and 2,850 Chinese characters or 18K Chinese words (for SLT). One signer has 11.66 hours of sign videos and the other has 4.41 hours. One objective in building the dataset is to support the investigation of how well large-vocabulary continuous sign language recognition/translation can be done for a single signer given a (relatively) large amount of his/her training data, which could potentially lead to the development of new modeling methods. Besides, most parts of the data collection pipeline are automated with little human intervention; we believe that our collection method can be scaled up to collect more sign language data easily for SLT in the future for any sign languages if such sign-interpreted videos are available. We also run a SOTA SLR/SLT model on the dataset and get a baseline SLR word error rate of 34.08% and a baseline SLT BLEU-4 score of 23.58 for benchmarking future research on the dataset.","sentences":["This paper introduces TVB-HKSL-News, a new Hong Kong sign language (HKSL) dataset collected from a TV news program over a period of 7 months.","The dataset is collected to enrich resources for HKSL and support research in large-vocabulary continuous sign language recognition (SLR) and translation (SLT).","It consists of 16.07 hours of sign videos of two signers with a vocabulary of 6,515 glosses (for SLR) and 2,850 Chinese characters or 18K Chinese words (for SLT).","One signer has 11.66 hours of sign videos and the other has 4.41 hours.","One objective in building the dataset is to support the investigation of how well large-vocabulary continuous sign language recognition/translation can be done for a single signer given a (relatively) large amount of his/her training data, which could potentially lead to the development of new modeling methods.","Besides, most parts of the data collection pipeline are automated with little human intervention; we believe that our collection method can be scaled up to collect more sign language data easily for SLT in the future for any sign languages if such sign-interpreted videos are available.","We also run a SOTA SLR/SLT model on the dataset and get a baseline SLR word error rate of 34.08% and a baseline SLT BLEU-4 score of 23.58 for benchmarking future research on the dataset."],"url":"http://arxiv.org/abs/2405.00980v1","category":"cs.CL"}
{"created":"2024-05-02 03:30:03","title":"Distillation for Multilingual Information Retrieval","abstract":"Recent work in cross-language information retrieval (CLIR), where queries and documents are in different languages, has shown the benefit of the Translate-Distill framework that trains a cross-language neural dual-encoder model using translation and distillation. However, Translate-Distill only supports a single document language. Multilingual information retrieval (MLIR), which ranks a multilingual document collection, is harder to train than CLIR because the model must assign comparable relevance scores to documents in different languages. This work extends Translate-Distill and propose Multilingual Translate-Distill (MTD) for MLIR. We show that ColBERT-X models trained with MTD outperform their counterparts trained ith Multilingual Translate-Train, which is the previous state-of-the-art training approach, by 5% to 25% in nDCG@20 and 15% to 45% in MAP. We also show that the model is robust to the way languages are mixed in training batches. Our implementation is available on GitHub.","sentences":["Recent work in cross-language information retrieval (CLIR), where queries and documents are in different languages, has shown the benefit of the Translate-Distill framework that trains a cross-language neural dual-encoder model using translation and distillation.","However, Translate-Distill only supports a single document language.","Multilingual information retrieval (MLIR), which ranks a multilingual document collection, is harder to train than CLIR because the model must assign comparable relevance scores to documents in different languages.","This work extends Translate-Distill and propose Multilingual Translate-Distill (MTD) for MLIR.","We show that ColBERT-X models trained with MTD outperform their counterparts trained ith Multilingual Translate-Train, which is the previous state-of-the-art training approach, by 5% to 25% in nDCG@20 and 15% to 45% in MAP.","We also show that the model is robust to the way languages are mixed in training batches.","Our implementation is available on GitHub."],"url":"http://arxiv.org/abs/2405.00977v1","category":"cs.IR"}
{"created":"2024-05-02 03:30:02","title":"Synchronization Dynamics in the Spatial Evolution of Optical Power in Optical Oligomer","abstract":"In this work, we investigated the spatial evolution of optical power in a closed-form optical waveguide configuration consisting of six passive waveguides and each of the waveguides exhibits equal strength of Kerr nonlinearity. We considered only nearest neighbor interaction between the waveguides. We found that in the case of low Kerr nonlinearity, evolution of optical power shows synchronization behavior. But when we increased the strength of Kerr nonlinearity, we discovered that spatial evolution of optical power in all waveguides shows independent characteristics. On the other hand, we have studied the impact of the coupling constant on the synchronization dynamics of our system. Our findings showed us that strong coupling can strengthen the collective dynamics in the presence of strong Kerr nonlinearity. From our results, we can conclude that Kerr nonlinearity in our system plays the role of disorder parameter that destroys as well as alters the synchronization behavior of evolution of optical power in the waveguides and coupling constant plays the role of an antagonist and restores synchronization in the model.","sentences":["In this work, we investigated the spatial evolution of optical power in a closed-form optical waveguide configuration consisting of six passive waveguides and each of the waveguides exhibits equal strength of Kerr nonlinearity.","We considered only nearest neighbor interaction between the waveguides.","We found that in the case of low Kerr nonlinearity, evolution of optical power shows synchronization behavior.","But when we increased the strength of Kerr nonlinearity, we discovered that spatial evolution of optical power in all waveguides shows independent characteristics.","On the other hand, we have studied the impact of the coupling constant on the synchronization dynamics of our system.","Our findings showed us that strong coupling can strengthen the collective dynamics in the presence of strong Kerr nonlinearity.","From our results, we can conclude that Kerr nonlinearity in our system plays the role of disorder parameter that destroys as well as alters the synchronization behavior of evolution of optical power in the waveguides and coupling constant plays the role of an antagonist and restores synchronization in the model."],"url":"http://arxiv.org/abs/2405.00976v1","category":"physics.optics"}
{"created":"2024-05-02 03:28:52","title":"PLAID SHIRTTT for Large-Scale Streaming Dense Retrieval","abstract":"PLAID, an efficient implementation of the ColBERT late interaction bi-encoder using pretrained language models for ranking, consistently achieves state-of-the-art performance in monolingual, cross-language, and multilingual retrieval. PLAID differs from ColBERT by assigning terms to clusters and representing those terms as cluster centroids plus compressed residual vectors. While PLAID is effective in batch experiments, its performance degrades in streaming settings where documents arrive over time because representations of new tokens may be poorly modeled by the earlier tokens used to select cluster centroids. PLAID Streaming Hierarchical Indexing that Runs on Terabytes of Temporal Text (PLAID SHIRTTT) addresses this concern using multi-phase incremental indexing based on hierarchical sharding. Experiments on ClueWeb09 and the multilingual NeuCLIR collection demonstrate the effectiveness of this approach both for the largest collection indexed to date by the ColBERT architecture and in the multilingual setting, respectively.","sentences":["PLAID, an efficient implementation of the ColBERT late interaction bi-encoder using pretrained language models for ranking, consistently achieves state-of-the-art performance in monolingual, cross-language, and multilingual retrieval.","PLAID differs from ColBERT by assigning terms to clusters and representing those terms as cluster centroids plus compressed residual vectors.","While PLAID is effective in batch experiments, its performance degrades in streaming settings where documents arrive over time because representations of new tokens may be poorly modeled by the earlier tokens used to select cluster centroids.","PLAID Streaming Hierarchical Indexing that Runs on Terabytes of Temporal Text (PLAID SHIRTTT) addresses this concern using multi-phase incremental indexing based on hierarchical sharding.","Experiments on ClueWeb09 and the multilingual NeuCLIR collection demonstrate the effectiveness of this approach both for the largest collection indexed to date by the ColBERT architecture and in the multilingual setting, respectively."],"url":"http://arxiv.org/abs/2405.00975v1","category":"cs.IR"}
{"created":"2024-05-02 03:20:08","title":"CACTUS: Chemistry Agent Connecting Tool-Usage to Science","abstract":"Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS's ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.","sentences":["Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools.","In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery.","We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions.","Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used.","Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy.","By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment.","Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery.","By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials.","Moreover, CACTUS's ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery."],"url":"http://arxiv.org/abs/2405.00972v1","category":"cs.CL"}
{"created":"2024-05-02 03:18:03","title":"How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee Responses","abstract":"One-on-one tutoring is widely acknowledged as an effective instructional method, conditioned on qualified tutors. However, the high demand for qualified tutors remains a challenge, often necessitating the training of novice tutors (i.e., trainees) to ensure effective tutoring. Research suggests that providing timely explanatory feedback can facilitate the training process for trainees. However, it presents challenges due to the time-consuming nature of assessing trainee performance by human experts. Inspired by the recent advancements of large language models (LLMs), our study employed the GPT-4 model to build an explanatory feedback system. This system identifies trainees' responses in binary form (i.e., correct/incorrect) and automatically provides template-based feedback with responses appropriately rephrased by the GPT-4 model. We conducted our study on 410 responses from trainees across three training lessons: Giving Effective Praise, Reacting to Errors, and Determining What Students Know. Our findings indicate that: 1) using a few-shot approach, the GPT-4 model effectively identifies correct/incorrect trainees' responses from three training lessons with an average F1 score of 0.84 and an AUC score of 0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases incorrect trainees' responses into desired responses, achieving performance comparable to that of human experts.","sentences":["One-on-one tutoring is widely acknowledged as an effective instructional method, conditioned on qualified tutors.","However, the high demand for qualified tutors remains a challenge, often necessitating the training of novice tutors (i.e., trainees) to ensure effective tutoring.","Research suggests that providing timely explanatory feedback can facilitate the training process for trainees.","However, it presents challenges due to the time-consuming nature of assessing trainee performance by human experts.","Inspired by the recent advancements of large language models (LLMs), our study employed the GPT-4 model to build an explanatory feedback system.","This system identifies trainees' responses in binary form (i.e., correct/incorrect) and automatically provides template-based feedback with responses appropriately rephrased by the GPT-4 model.","We conducted our study on 410 responses from trainees across three training lessons: Giving Effective Praise, Reacting to Errors, and Determining What Students Know.","Our findings indicate that: 1) using a few-shot approach, the GPT-4 model effectively identifies correct/incorrect trainees' responses from three training lessons with an average F1 score of 0.84 and an AUC score of 0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases incorrect trainees' responses into desired responses, achieving performance comparable to that of human experts."],"url":"http://arxiv.org/abs/2405.00970v1","category":"cs.CL"}
{"created":"2024-05-02 03:11:59","title":"Efficient Compression of Multitask Multilingual Speech Models","abstract":"Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we examine its limitations, demonstrating the presence of speaker-related (gender, age) and model-related (resourcefulness and model size) bias. Despite that, we show that only model-related bias are amplified by quantization, impacting more low-resource languages and smaller models. Searching for a better compression approach, we propose DistilWhisper, an approach that is able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.","sentences":["Whisper is a multitask and multilingual speech model covering 99 languages.","It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions.","In this work, we examine its limitations, demonstrating the presence of speaker-related (gender, age) and model-related (resourcefulness and model size) bias.","Despite that, we show that only model-related bias are amplified by quantization, impacting more low-resource languages and smaller models.","Searching for a better compression approach, we propose DistilWhisper, an approach that is able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities.","Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2.","This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training.","Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference."],"url":"http://arxiv.org/abs/2405.00966v1","category":"cs.CL"}
{"created":"2024-05-02 02:52:11","title":"Foundations for Digital Twins","abstract":"The growing reliance on digital twins across various industries and domains brings with it semantic interoperability challenges. Ontologies are a well-known strategy for addressing such challenges, though given the complexity of the phenomenon, there are risks of reintroducing the interoperability challenges at the level of ontology representations. In the interest of avoiding such pitfalls, we introduce and defend characterizations of digital twins within the context of the Common Core Ontologies, an extension of the widely-used Basic Formal Ontology. We provide a set of definitions and design patterns relevant to the domain of digital twins, highlighted by illustrative use cases of digital twins and their physical counterparts. In doing so, we provide a foundation on which to build more sophisticated ontological content related and connected to digital twins.","sentences":["The growing reliance on digital twins across various industries and domains brings with it semantic interoperability challenges.","Ontologies are a well-known strategy for addressing such challenges, though given the complexity of the phenomenon, there are risks of reintroducing the interoperability challenges at the level of ontology representations.","In the interest of avoiding such pitfalls, we introduce and defend characterizations of digital twins within the context of the Common Core Ontologies, an extension of the widely-used Basic Formal Ontology.","We provide a set of definitions and design patterns relevant to the domain of digital twins, highlighted by illustrative use cases of digital twins and their physical counterparts.","In doing so, we provide a foundation on which to build more sophisticated ontological content related and connected to digital twins."],"url":"http://arxiv.org/abs/2405.00960v1","category":"cs.AI"}
{"created":"2024-05-02 02:50:58","title":"Generative manufacturing systems using diffusion models and ChatGPT","abstract":"In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences. Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making. Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback. Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds. The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions.","sentences":["In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences.","Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making.","Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback.","Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds.","The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions."],"url":"http://arxiv.org/abs/2405.00958v1","category":"cs.LG"}
{"created":"2024-05-02 02:38:32","title":"IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors","abstract":"Graph Neural Networks (GNNs) demonstrate excellent performance on graphs, with their core idea about aggregating neighborhood information and learning from labels. However, the prevailing challenges in most graph datasets are twofold of Insufficient High-Quality Labels and Lack of Neighborhoods, resulting in weak GNNs. Existing data augmentation methods designed to address these two issues often tackle only one. They may either require extensive training of generators, rely on overly simplistic strategies, or demand substantial prior knowledge, leading to suboptimal generalization abilities. To simultaneously address both of these two challenges, we propose an elegant method called IntraMix. IntraMix innovatively employs Mixup among low-quality labeled data of the same class, generating high-quality labeled data at minimal cost. Additionally, it establishes neighborhoods for the generated data by connecting them with data from the same class with high confidence, thereby enriching the neighborhoods of graphs. IntraMix efficiently tackles both challenges faced by graphs and challenges the prior notion of the limited effectiveness of Mixup in node classification. IntraMix serves as a universal framework that can be readily applied to all GNNs. Extensive experiments demonstrate the effectiveness of IntraMix across various GNNs and datasets.","sentences":["Graph Neural Networks (GNNs) demonstrate excellent performance on graphs, with their core idea about aggregating neighborhood information and learning from labels.","However, the prevailing challenges in most graph datasets are twofold of Insufficient High-Quality Labels and Lack of Neighborhoods, resulting in weak GNNs.","Existing data augmentation methods designed to address these two issues often tackle only one.","They may either require extensive training of generators, rely on overly simplistic strategies, or demand substantial prior knowledge, leading to suboptimal generalization abilities.","To simultaneously address both of these two challenges, we propose an elegant method called IntraMix.","IntraMix innovatively employs Mixup among low-quality labeled data of the same class, generating high-quality labeled data at minimal cost.","Additionally, it establishes neighborhoods for the generated data by connecting them with data from the same class with high confidence, thereby enriching the neighborhoods of graphs.","IntraMix efficiently tackles both challenges faced by graphs and challenges the prior notion of the limited effectiveness of Mixup in node classification.","IntraMix serves as a universal framework that can be readily applied to all GNNs.","Extensive experiments demonstrate the effectiveness of IntraMix across various GNNs and datasets."],"url":"http://arxiv.org/abs/2405.00957v1","category":"cs.LG"}
{"created":"2024-05-02 02:34:19","title":"Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians","abstract":"Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy regularization into the Gaussian learning process. Furthermore, we apply the Material Point Method, which is integrated with physical properties, to the 3D Gaussians to achieve realistic scene deformations. Our method was evaluated on our collected in-house and public surgical videos datasets. Results show that it can reconstruct and simulate surgical scenes from endoscopic videos efficiently-taking only a few minutes to reconstruct the surgical scene-and produce both visually and physically plausible deformations at a speed approaching real-time. The results demonstrate great potential of our proposed method to enhance the efficiency and variety of simulations available for surgical education and robot learning.","sentences":["Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning.","Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations.","This manual approach is not only time-consuming but also limited in the scalability and realism.","In contrast, data-driven simulation offers a compelling alternative.","It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics.","This area, however, is relatively uncharted.","In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video.","To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy regularization into the Gaussian learning process.","Furthermore, we apply the Material Point Method, which is integrated with physical properties, to the 3D Gaussians to achieve realistic scene deformations.","Our method was evaluated on our collected in-house and public surgical videos datasets.","Results show that it can reconstruct and simulate surgical scenes from endoscopic videos efficiently-taking only a few minutes to reconstruct the surgical scene-and produce both visually and physically plausible deformations at a speed approaching real-time.","The results demonstrate great potential of our proposed method to enhance the efficiency and variety of simulations available for surgical education and robot learning."],"url":"http://arxiv.org/abs/2405.00956v1","category":"cs.RO"}
{"created":"2024-05-02 02:20:19","title":"Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback","abstract":"Restless multi-armed bandits (RMAB) play a central role in modeling sequential decision making problems under an instantaneous activation constraint that at most B arms can be activated at any decision epoch. Each restless arm is endowed with a state that evolves independently according to a Markov decision process regardless of being activated or not. In this paper, we consider the task of learning in episodic RMAB with unknown transition functions and adversarial rewards, which can change arbitrarily across episodes. Further, we consider a challenging but natural bandit feedback setting that only adversarial rewards of activated arms are revealed to the decision maker (DM). The goal of the DM is to maximize its total adversarial rewards during the learning process while the instantaneous activation constraint must be satisfied in each decision epoch. We develop a novel reinforcement learning algorithm with two key contributors: a novel biased adversarial reward estimator to deal with bandit feedback and unknown transitions, and a low-complexity index policy to satisfy the instantaneous activation constraint. We show $\\tilde{\\mathcal{O}}(H\\sqrt{T})$ regret bound for our algorithm, where $T$ is the number of episodes and $H$ is the episode length. To our best knowledge, this is the first algorithm to ensure $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret for adversarial RMAB in our considered challenging settings.","sentences":["Restless multi-armed bandits (RMAB) play a central role in modeling sequential decision making problems under an instantaneous activation constraint that at most B arms can be activated at any decision epoch.","Each restless arm is endowed with a state that evolves independently according to a Markov decision process regardless of being activated or not.","In this paper, we consider the task of learning in episodic RMAB with unknown transition functions and adversarial rewards, which can change arbitrarily across episodes.","Further, we consider a challenging but natural bandit feedback setting that only adversarial rewards of activated arms are revealed to the decision maker (DM).","The goal of the DM is to maximize its total adversarial rewards during the learning process while the instantaneous activation constraint must be satisfied in each decision epoch.","We develop a novel reinforcement learning algorithm with two key contributors: a novel biased adversarial reward estimator to deal with bandit feedback and unknown transitions, and a low-complexity index policy to satisfy the instantaneous activation constraint.","We show $\\tilde{\\mathcal{O}}(H\\sqrt{T})$ regret bound for our algorithm, where $T$ is the number of episodes and $H$ is the episode length.","To our best knowledge, this is the first algorithm to ensure $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret for adversarial RMAB in our considered challenging settings."],"url":"http://arxiv.org/abs/2405.00950v1","category":"cs.LG"}
{"created":"2024-05-02 02:16:32","title":"Co-Optimization of EV Charging Control and Incentivization for Enhanced Power System Stability","abstract":"We study how high charging rate demands from electric vehicles (EVs) in a power distribution grid may collectively cause its dynamic instability, and, accordingly, how a price incentivization strategy can be used to steer customers to settle for lesser charging rate demands so that these instabilities can be avoided. We pose the problem as a joint optimization and optimal control formulation. The optimization determines the optimal charging setpoints for EVs to minimize the $\\mathcal{H}_2$-norm of the transfer function of the grid model, while the optimal control simultaneously develops a linear quadratic regulator (LQR) based state-feedback control signal for the battery-currents of those EVs to jointly minimize the risk of grid instability. A subsequent algorithm is developed to determine how much customers may be willing to sacrifice their intended charging rate demands in return for financial incentives. Results are derived for both unidirectional and bidirectional charging, and validated using numerical simulations of multiple EV charging stations in the IEEE 33-bus power distribution model.","sentences":["We study how high charging rate demands from electric vehicles (EVs) in a power distribution grid may collectively cause its dynamic instability, and, accordingly, how a price incentivization strategy can be used to steer customers to settle for lesser charging rate demands so that these instabilities can be avoided.","We pose the problem as a joint optimization and optimal control formulation.","The optimization determines the optimal charging setpoints for EVs to minimize the $\\mathcal{H}_2$-norm of the transfer function of the grid model, while the optimal control simultaneously develops a linear quadratic regulator (LQR) based state-feedback control signal for the battery-currents of those EVs to jointly minimize the risk of grid instability.","A subsequent algorithm is developed to determine how much customers may be willing to sacrifice their intended charging rate demands in return for financial incentives.","Results are derived for both unidirectional and bidirectional charging, and validated using numerical simulations of multiple EV charging stations in the IEEE 33-bus power distribution model."],"url":"http://arxiv.org/abs/2405.00947v1","category":"math.OC"}
{"created":"2024-05-02 02:04:01","title":"LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs","abstract":"Communication is defined as ``Who says what to whom with what effect.'' A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals about the message, the behavior data is often ignored while training large language models. We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities. Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM's performance on a wide variety of downstream content understanding tasks. We show this performance increase over 40 video and image understanding tasks over 23 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines. Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch. We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data.","sentences":["Communication is defined as ``Who says what to whom with what effect.''","A message from a communicator generates downstream receiver effects, also known as behavior.","Receiver behavior, being a downstream effect of the message, carries rich signals about it.","Even after carrying signals about the message, the behavior data is often ignored while training large language models.","We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities.","Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM's performance on a wide variety of downstream content understanding tasks.","We show this performance increase over 40 video and image understanding tasks over 23 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines.","Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch.","We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data."],"url":"http://arxiv.org/abs/2405.00942v1","category":"cs.CV"}
{"created":"2024-05-02 00:04:02","title":"EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion","abstract":"We present EchoScene, an interactive and controllable generative model that generates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch diffusion model that dynamically adapts to scene graphs. Existing methods struggle to handle scene graphs due to varying numbers of nodes, multiple edge combinations, and manipulator-induced node-edge operations. EchoScene overcomes this by associating each node with a denoising process and enables collaborative information exchange, enhancing controllable and consistent generation aware of global constraints. This is achieved through an information echo scheme in both shape and layout branches. At every denoising step, all processes share their denoising data with an information exchange unit that combines these updates using graph convolution. The scheme ensures that the denoising processes are influenced by a holistic understanding of the scene graph, facilitating the generation of globally coherent scenes. The resulting scenes can be manipulated during inference by editing the input scene graph and sampling the noise in the diffusion model. Extensive experiments validate our approach, which maintains scene controllability and surpasses previous methods in generation fidelity. Moreover, the generated scenes are of high quality and thus directly compatible with off-the-shelf texture generation. Code and trained models are open-sourced.","sentences":["We present EchoScene, an interactive and controllable generative model that generates 3D indoor scenes on scene graphs.","EchoScene leverages a dual-branch diffusion model that dynamically adapts to scene graphs.","Existing methods struggle to handle scene graphs due to varying numbers of nodes, multiple edge combinations, and manipulator-induced node-edge operations.","EchoScene overcomes this by associating each node with a denoising process and enables collaborative information exchange, enhancing controllable and consistent generation aware of global constraints.","This is achieved through an information echo scheme in both shape and layout branches.","At every denoising step, all processes share their denoising data with an information exchange unit that combines these updates using graph convolution.","The scheme ensures that the denoising processes are influenced by a holistic understanding of the scene graph, facilitating the generation of globally coherent scenes.","The resulting scenes can be manipulated during inference by editing the input scene graph and sampling the noise in the diffusion model.","Extensive experiments validate our approach, which maintains scene controllability and surpasses previous methods in generation fidelity.","Moreover, the generated scenes are of high quality and thus directly compatible with off-the-shelf texture generation.","Code and trained models are open-sourced."],"url":"http://arxiv.org/abs/2405.00915v1","category":"cs.CV"}
{"created":"2024-05-01 23:40:12","title":"Transformer-Based Self-Supervised Learning for Histopathological Classification of Ischemic Stroke Clot Origin","abstract":"Background and Purpose: Identifying the thromboembolism source in ischemic stroke is crucial for treatment and secondary prevention yet is often undetermined. This study describes a self-supervised deep learning approach in digital pathology of emboli for classifying ischemic stroke clot origin from histopathological images. Methods: The dataset included whole slide images (WSI) from the STRIP AI Kaggle challenge, consisting of retrieved clots from ischemic stroke patients following mechanical thrombectomy. Transformer-based deep learning models were developed using transfer learning and self-supervised pretraining for classifying WSI. Customizations included an attention pooling layer, weighted loss function, and threshold optimization. Various model architectures were tested and compared, and model performances were primarily evaluated using weighted logarithmic loss. Results: The model achieved a logloss score of 0.662 in cross-validation and 0.659 on the test set. Different model backbones were compared, with the swin_large_patch4_window12_384 showed higher performance. Thresholding techniques for clot origin classification were employed to balance false positives and negatives. Conclusion: The study demonstrates the extent of efficacy of transformer-based deep learning models in identifying ischemic stroke clot origins from histopathological images and emphasizes the need for refined modeling techniques specifically adapted to thrombi WSI. Further research is needed to improve model performance, interpretability, validate its effectiveness. Future enhancement could include integrating larger patient cohorts, advanced preprocessing strategies, and exploring ensemble multimodal methods for enhanced diagnostic accuracy.","sentences":["Background and Purpose: Identifying the thromboembolism source in ischemic stroke is crucial for treatment and secondary prevention yet is often undetermined.","This study describes a self-supervised deep learning approach in digital pathology of emboli for classifying ischemic stroke clot origin from histopathological images.","Methods: The dataset included whole slide images (WSI) from the STRIP AI Kaggle challenge, consisting of retrieved clots from ischemic stroke patients following mechanical thrombectomy.","Transformer-based deep learning models were developed using transfer learning and self-supervised pretraining for classifying WSI.","Customizations included an attention pooling layer, weighted loss function, and threshold optimization.","Various model architectures were tested and compared, and model performances were primarily evaluated using weighted logarithmic loss.","Results:","The model achieved a logloss score of 0.662 in cross-validation and 0.659 on the test set.","Different model backbones were compared, with the swin_large_patch4_window12_384 showed higher performance.","Thresholding techniques for clot origin classification were employed to balance false positives and negatives.","Conclusion: The study demonstrates the extent of efficacy of transformer-based deep learning models in identifying ischemic stroke clot origins from histopathological images and emphasizes the need for refined modeling techniques specifically adapted to thrombi WSI.","Further research is needed to improve model performance, interpretability, validate its effectiveness.","Future enhancement could include integrating larger patient cohorts, advanced preprocessing strategies, and exploring ensemble multimodal methods for enhanced diagnostic accuracy."],"url":"http://arxiv.org/abs/2405.00908v1","category":"cs.CV"}
{"created":"2024-05-01 23:30:12","title":"LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets","abstract":"Vision transformers have revolutionized computer vision, but their computational demands present challenges for training and deployment. This paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel method that leverages data lottery ticket selection and sparsity pruning to accelerate vision transformer training while maintaining accuracy. Our approach focuses on identifying and utilizing the most informative data subsets and eliminating redundant model parameters to optimize the training process. Through extensive experiments, we demonstrate the effectiveness of LOTUS in achieving rapid convergence and high accuracy with significantly reduced computational requirements. This work highlights the potential of combining data selection and sparsity techniques for efficient vision transformer training, opening doors for further research and development in this area.","sentences":["Vision transformers have revolutionized computer vision, but their computational demands present challenges for training and deployment.","This paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel method that leverages data lottery ticket selection and sparsity pruning to accelerate vision transformer training while maintaining accuracy.","Our approach focuses on identifying and utilizing the most informative data subsets and eliminating redundant model parameters to optimize the training process.","Through extensive experiments, we demonstrate the effectiveness of LOTUS in achieving rapid convergence and high accuracy with significantly reduced computational requirements.","This work highlights the potential of combining data selection and sparsity techniques for efficient vision transformer training, opening doors for further research and development in this area."],"url":"http://arxiv.org/abs/2405.00906v1","category":"cs.CV"}
{"created":"2024-05-01 23:29:53","title":"Properties of Charge Recombination in Liquid Argon","abstract":"Liquid argon is an excellent medium for detecting particles, given its yields and transport properties of light and charge. The technology of liquid argon time projection chambers has reached its full maturity after four decades of continuous developments and is, or will be, used in world class experiments for neutrino and dark matter searches. The collection of ionization charge in these detectors allows to perform a complete tridimensional reconstruction of the tracks of charged particles, calorimetric measurements, particle identification. This work proposes a novel approach to the problem of charge recombination in liquid argon which moves from a microscopic model and is applied to the cases of low energy electrons, alpha particles and nuclear recoils. The model is able to describe precisely several sets of experimental data available in the literature, over wide ranges of electric field strengths and kinetic energies and can be easily extended to other particles.","sentences":["Liquid argon is an excellent medium for detecting particles, given its yields and transport properties of light and charge.","The technology of liquid argon time projection chambers has reached its full maturity after four decades of continuous developments and is, or will be, used in world class experiments for neutrino and dark matter searches.","The collection of ionization charge in these detectors allows to perform a complete tridimensional reconstruction of the tracks of charged particles, calorimetric measurements, particle identification.","This work proposes a novel approach to the problem of charge recombination in liquid argon which moves from a microscopic model and is applied to the cases of low energy electrons, alpha particles and nuclear recoils.","The model is able to describe precisely several sets of experimental data available in the literature, over wide ranges of electric field strengths and kinetic energies and can be easily extended to other particles."],"url":"http://arxiv.org/abs/2405.00905v1","category":"hep-ex"}
{"created":"2024-05-01 23:19:48","title":"MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure","abstract":"Multi-agent reinforcement learning (MARL) algorithms often struggle to find strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack of efficient exploration. The problem is exacerbated in sparse-reward settings, caused by the larger variance exhibited in policy learning. This paper introduces MESA, a novel meta-exploration method for cooperative multi-agent learning. It learns to explore by first identifying the agents' high-rewarding joint state-action subspace from training tasks and then learning a set of diverse exploration policies to \"cover\" the subspace. These trained exploration policies can be integrated with any off-policy MARL algorithm for test-time tasks. We first showcase MESA's advantage in a multi-step matrix game. Furthermore, experiments show that with learned exploration policies, MESA achieves significantly better performance in sparse-reward tasks in several multi-agent particle environments and multi-agent MuJoCo environments, and exhibits the ability to generalize to more challenging tasks at test time.","sentences":["Multi-agent reinforcement learning (MARL) algorithms often struggle to find strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack of efficient exploration.","The problem is exacerbated in sparse-reward settings, caused by the larger variance exhibited in policy learning.","This paper introduces MESA, a novel meta-exploration method for cooperative multi-agent learning.","It learns to explore by first identifying the agents' high-rewarding joint state-action subspace from training tasks and then learning a set of diverse exploration policies to \"cover\" the subspace.","These trained exploration policies can be integrated with any off-policy MARL algorithm for test-time tasks.","We first showcase MESA's advantage in a multi-step matrix game.","Furthermore, experiments show that with learned exploration policies, MESA achieves significantly better performance in sparse-reward tasks in several multi-agent particle environments and multi-agent MuJoCo environments, and exhibits the ability to generalize to more challenging tasks at test time."],"url":"http://arxiv.org/abs/2405.00902v1","category":"cs.LG"}
{"created":"2024-05-01 23:06:46","title":"Characterising the Creative Process in Humans and Large Language Models","abstract":"Large language models appear quite creative, often performing on par with the average human on creative tasks. However, research on LLM creativity has focused solely on \\textit{products}, with little attention on the creative \\textit{process}. Process analyses of human creativity often require hand-coded categories or exploit response times, which do not apply to LLMs. We provide an automated method to characterise how humans and LLMs explore semantic spaces on the Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task. We use sentence embeddings to identify response categories and compute semantic similarities, which we use to generate jump profiles. Our results corroborate earlier work in humans reporting both persistent (deep search in few semantic spaces) and flexible (broad search across multiple semantic spaces) pathways to creativity, where both pathways lead to similar creativity scores. LLMs were found to be biased towards either persistent or flexible paths, that varied across tasks. Though LLMs as a population match human profiles, their relationship with creativity is different, where the more flexible models score higher on creativity. Our dataset and scripts are available on \\href{https://github.com/surabhisnath/Creative_Process}{GitHub}.","sentences":["Large language models appear quite creative, often performing on par with the average human on creative tasks.","However, research on LLM creativity has focused solely on \\textit{products}, with little attention on the creative \\textit{process}.","Process analyses of human creativity often require hand-coded categories or exploit response times, which do not apply to LLMs.","We provide an automated method to characterise how humans and LLMs explore semantic spaces on the Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task.","We use sentence embeddings to identify response categories and compute semantic similarities, which we use to generate jump profiles.","Our results corroborate earlier work in humans reporting both persistent (deep search in few semantic spaces) and flexible (broad search across multiple semantic spaces) pathways to creativity, where both pathways lead to similar creativity scores.","LLMs were found to be biased towards either persistent or flexible paths, that varied across tasks.","Though LLMs as a population match human profiles, their relationship with creativity is different, where the more flexible models score higher on creativity.","Our dataset and scripts are available on \\href{https://github.com/surabhisnath/Creative_Process}{GitHub}."],"url":"http://arxiv.org/abs/2405.00899v1","category":"cs.HC"}
{"created":"2024-05-01 23:03:47","title":"Data-driven modeling of the aerodynamic deformation and drag for a freely moving drop in the sub-critical Weber number regime","abstract":"Accurate prediction of the dynamics and deformation of freely moving drops is crucial for numerous droplet applications. When the Weber number is finite but below a critical value, the drop deviates from its spherical shape and deforms as it is accelerated by the gas stream. Since aerodynamic drag on the drop depends on its shape oscillation, accurately modeling the drop shape evolution is essential for predicting the drop's velocity and position. In this study, 2D axisymmetric interface-resolved simulations were performed to provide a comprehensive dataset for developing a data-driven model. Parametric simulations were conducted by systematically varying the drop diameter and free-stream velocity, achieving wide ranges of Weber and Reynolds numbers. The instantaneous drop shapes obtained in simulations are characterized by spherical harmonics. Temporal data of the drag and modal coefficients are collected from the simulation data to train a Nonlinear Auto-Regressive models with eXogenous inputs (NARX) neural network model. The overall model consists of two multi-layer perceptron networks, which predict the modal coefficients and the drop drag, respectively. The drop shape can be reconstructed with the predicted modal coefficients. The model predictions are validated against the simulation data in the testing set, showing excellent agreement for the evolutions of both the drop shape and drag.","sentences":["Accurate prediction of the dynamics and deformation of freely moving drops is crucial for numerous droplet applications.","When the Weber number is finite but below a critical value, the drop deviates from its spherical shape and deforms as it is accelerated by the gas stream.","Since aerodynamic drag on the drop depends on its shape oscillation, accurately modeling the drop shape evolution is essential for predicting the drop's velocity and position.","In this study, 2D axisymmetric interface-resolved simulations were performed to provide a comprehensive dataset for developing a data-driven model.","Parametric simulations were conducted by systematically varying the drop diameter and free-stream velocity, achieving wide ranges of Weber and Reynolds numbers.","The instantaneous drop shapes obtained in simulations are characterized by spherical harmonics.","Temporal data of the drag and modal coefficients are collected from the simulation data to train a Nonlinear Auto-Regressive models with eXogenous inputs (NARX) neural network model.","The overall model consists of two multi-layer perceptron networks, which predict the modal coefficients and the drop drag, respectively.","The drop shape can be reconstructed with the predicted modal coefficients.","The model predictions are validated against the simulation data in the testing set, showing excellent agreement for the evolutions of both the drop shape and drag."],"url":"http://arxiv.org/abs/2405.00897v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 22:33:45","title":"Wake Vision: A Large-scale, Diverse Dataset and Benchmark Suite for TinyML Person Detection","abstract":"Machine learning applications on extremely low-power devices, commonly referred to as tiny machine learning (TinyML), promises a smarter and more connected world. However, the advancement of current TinyML research is hindered by the limited size and quality of pertinent datasets. To address this challenge, we introduce Wake Vision, a large-scale, diverse dataset tailored for person detection -- the canonical task for TinyML visual sensing. Wake Vision comprises over 6 million images, which is a hundredfold increase compared to the previous standard, and has undergone thorough quality filtering. Using Wake Vision for training results in a 2.41\\% increase in accuracy compared to the established benchmark. Alongside the dataset, we provide a collection of five detailed benchmark sets that assess model performance on specific segments of the test data, such as varying lighting conditions, distances from the camera, and demographic characteristics of subjects. These novel fine-grained benchmarks facilitate the evaluation of model quality in challenging real-world scenarios that are often ignored when focusing solely on overall accuracy. Through an evaluation of a MobileNetV2 TinyML model on the benchmarks, we show that the input resolution plays a more crucial role than the model width in detecting distant subjects and that the impact of quantization on model robustness is minimal, thanks to the dataset quality. These findings underscore the importance of a detailed evaluation to identify essential factors for model development. The dataset, benchmark suite, code, and models are publicly available under the CC-BY 4.0 license, enabling their use for commercial use cases.","sentences":["Machine learning applications on extremely low-power devices, commonly referred to as tiny machine learning (TinyML), promises a smarter and more connected world.","However, the advancement of current TinyML research is hindered by the limited size and quality of pertinent datasets.","To address this challenge, we introduce Wake Vision, a large-scale, diverse dataset tailored for person detection -- the canonical task for TinyML visual sensing.","Wake Vision comprises over 6 million images, which is a hundredfold increase compared to the previous standard, and has undergone thorough quality filtering.","Using Wake Vision for training results in a 2.41\\% increase in accuracy compared to the established benchmark.","Alongside the dataset, we provide a collection of five detailed benchmark sets that assess model performance on specific segments of the test data, such as varying lighting conditions, distances from the camera, and demographic characteristics of subjects.","These novel fine-grained benchmarks facilitate the evaluation of model quality in challenging real-world scenarios that are often ignored when focusing solely on overall accuracy.","Through an evaluation of a MobileNetV2 TinyML model on the benchmarks, we show that the input resolution plays a more crucial role than the model width in detecting distant subjects and that the impact of quantization on model robustness is minimal, thanks to the dataset quality.","These findings underscore the importance of a detailed evaluation to identify essential factors for model development.","The dataset, benchmark suite, code, and models are publicly available under the CC-BY 4.0 license, enabling their use for commercial use cases."],"url":"http://arxiv.org/abs/2405.00892v1","category":"cs.CV"}
{"created":"2024-05-01 21:44:47","title":"Machine Learning Techniques for Data Reduction of Climate Applications","abstract":"Scientists conduct large-scale simulations to compute derived quantities-of-interest (QoI) from primary data. Often, QoI are linked to specific features, regions, or time intervals, such that data can be adaptively reduced without compromising the integrity of QoI. For many spatiotemporal applications, these QoI are binary in nature and represent presence or absence of a physical phenomenon. We present a pipelined compression approach that first uses neural-network-based techniques to derive regions where QoI are highly likely to be present. Then, we employ a Guaranteed Autoencoder (GAE) to compress data with differential error bounds. GAE uses QoI information to apply low-error compression to only these regions. This results in overall high compression ratios while still achieving downstream goals of simulation or data collections. Experimental results are presented for climate data generated from the E3SM Simulation model for downstream quantities such as tropical cyclone and atmospheric river detection and tracking. These results show that our approach is superior to comparable methods in the literature.","sentences":["Scientists conduct large-scale simulations to compute derived quantities-of-interest (QoI) from primary data.","Often, QoI are linked to specific features, regions, or time intervals, such that data can be adaptively reduced without compromising the integrity of QoI.","For many spatiotemporal applications, these QoI are binary in nature and represent presence or absence of a physical phenomenon.","We present a pipelined compression approach that first uses neural-network-based techniques to derive regions where QoI are highly likely to be present.","Then, we employ a Guaranteed Autoencoder (GAE) to compress data with differential error bounds.","GAE uses QoI information to apply low-error compression to only these regions.","This results in overall high compression ratios while still achieving downstream goals of simulation or data collections.","Experimental results are presented for climate data generated from the E3SM Simulation model for downstream quantities such as tropical cyclone and atmospheric river detection and tracking.","These results show that our approach is superior to comparable methods in the literature."],"url":"http://arxiv.org/abs/2405.00879v1","category":"cs.LG"}
{"created":"2024-05-01 21:42:38","title":"Markov flow policy -- deep MC","abstract":"Discounted algorithms often encounter evaluation errors due to their reliance on short-term estimations, which can impede their efficacy in addressing simple, short-term tasks and impose undesired temporal discounts (\\(\\gamma\\)). Interestingly, these algorithms are often tested without applying a discount, a phenomenon we refer as the \\textit{train-test bias}. In response to these challenges, we propose the Markov Flow Policy, which utilizes a non-negative neural network flow to enable comprehensive forward-view predictions. Through integration into the TD7 codebase and evaluation using the MuJoCo benchmark, we observe significant performance improvements, positioning MFP as a straightforward, practical, and easily implementable solution within the domain of average rewards algorithms.","sentences":["Discounted algorithms often encounter evaluation errors due to their reliance on short-term estimations, which can impede their efficacy in addressing simple, short-term tasks and impose undesired temporal discounts (\\(\\gamma\\)).","Interestingly, these algorithms are often tested without applying a discount, a phenomenon we refer as the \\textit{train-test bias}.","In response to these challenges, we propose the Markov Flow Policy, which utilizes a non-negative neural network flow to enable comprehensive forward-view predictions.","Through integration into the TD7 codebase and evaluation using the MuJoCo benchmark, we observe significant performance improvements, positioning MFP as a straightforward, practical, and easily implementable solution within the domain of average rewards algorithms."],"url":"http://arxiv.org/abs/2405.00877v1","category":"cs.LG"}
{"created":"2024-05-01 21:35:04","title":"Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis","abstract":"Vision language models (VLMs) have recently emerged and gained the spotlight for their ability to comprehend the dual modality of image and textual data. VLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive performance on tasks such as natural image captioning, visual question answering (VQA), and spatial reasoning. Additionally, a universal segmentation model by Meta AI, Segment Anything Model (SAM) shows unprecedented performance at isolating objects from unforeseen images. Since medical experts, biologists, and materials scientists routinely examine microscopy or medical images in conjunction with textual information in the form of captions, literature, or reports, and draw conclusions of great importance and merit, it is indubitably essential to test the performance of VLMs and foundation models such as SAM, on these images. In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with classification, segmentation, counting, and VQA tasks on a variety of microscopy images. We observe that ChatGPT and Gemini are impressively able to comprehend the visual features in microscopy images, while SAM is quite capable at isolating artefacts in a general sense. However, the performance is not close to that of a domain expert - the models are readily encumbered by the introduction of impurities, defects, artefact overlaps and diversity present in the images.","sentences":["Vision language models (VLMs) have recently emerged and gained the spotlight for their ability to comprehend the dual modality of image and textual data.","VLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive performance on tasks such as natural image captioning, visual question answering (VQA), and spatial reasoning.","Additionally, a universal segmentation model by Meta AI, Segment Anything Model (SAM) shows unprecedented performance at isolating objects from unforeseen images.","Since medical experts, biologists, and materials scientists routinely examine microscopy or medical images in conjunction with textual information in the form of captions, literature, or reports, and draw conclusions of great importance and merit, it is indubitably essential to test the performance of VLMs and foundation models such as SAM, on these images.","In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with classification, segmentation, counting, and VQA tasks on a variety of microscopy images.","We observe that ChatGPT and Gemini are impressively able to comprehend the visual features in microscopy images, while SAM is quite capable at isolating artefacts in a general sense.","However, the performance is not close to that of a domain expert - the models are readily encumbered by the introduction of impurities, defects, artefact overlaps and diversity present in the images."],"url":"http://arxiv.org/abs/2405.00876v1","category":"cs.CV"}
{"created":"2024-05-01 21:22:33","title":"Artificial intelligence for context-aware visual change detection in software test automation","abstract":"Automated software testing is integral to the software development process, streamlining workflows and ensuring product reliability. Visual testing within this context, especially concerning user interface (UI) and user experience (UX) validation, stands as one of crucial determinants of overall software quality. Nevertheless, conventional methods like pixel-wise comparison and region-based visual change detection fall short in capturing contextual similarities, nuanced alterations, and understanding the spatial relationships between UI elements. In this paper, we introduce a novel graph-based method for visual change detection in software test automation. Leveraging a machine learning model, our method accurately identifies UI controls from software screenshots and constructs a graph representing contextual and spatial relationships between the controls. This information is then used to find correspondence between UI controls within screenshots of different versions of a software. The resulting graph encapsulates the intricate layout of the UI and underlying contextual relations, providing a holistic and context-aware model. This model is finally used to detect and highlight visual regressions in the UI. Comprehensive experiments on different datasets showed that our change detector can accurately detect visual software changes in various simple and complex test scenarios. Moreover, it outperformed pixel-wise comparison and region-based baselines by a large margin in more complex testing scenarios. This work not only contributes to the advancement of visual change detection but also holds practical implications, offering a robust solution for real-world software test automation challenges, enhancing reliability, and ensuring the seamless evolution of software interfaces.","sentences":["Automated software testing is integral to the software development process, streamlining workflows and ensuring product reliability.","Visual testing within this context, especially concerning user interface (UI) and user experience (UX) validation, stands as one of crucial determinants of overall software quality.","Nevertheless, conventional methods like pixel-wise comparison and region-based visual change detection fall short in capturing contextual similarities, nuanced alterations, and understanding the spatial relationships between UI elements.","In this paper, we introduce a novel graph-based method for visual change detection in software test automation.","Leveraging a machine learning model, our method accurately identifies UI controls from software screenshots and constructs a graph representing contextual and spatial relationships between the controls.","This information is then used to find correspondence between UI controls within screenshots of different versions of a software.","The resulting graph encapsulates the intricate layout of the UI and underlying contextual relations, providing a holistic and context-aware model.","This model is finally used to detect and highlight visual regressions in the UI.","Comprehensive experiments on different datasets showed that our change detector can accurately detect visual software changes in various simple and complex test scenarios.","Moreover, it outperformed pixel-wise comparison and region-based baselines by a large margin in more complex testing scenarios.","This work not only contributes to the advancement of visual change detection but also holds practical implications, offering a robust solution for real-world software test automation challenges, enhancing reliability, and ensuring the seamless evolution of software interfaces."],"url":"http://arxiv.org/abs/2405.00874v1","category":"cs.SE"}
{"created":"2024-05-01 20:10:44","title":"Can a Hallucinating Model help in Reducing Human \"Hallucination\"?","abstract":"The prevalence of unwarranted beliefs, spanning pseudoscience, logical fallacies, and conspiracy theories, presents substantial societal hurdles and the risk of disseminating misinformation. Utilizing established psychometric assessments, this study explores the capabilities of large language models (LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We undertake a philosophical inquiry, juxtaposing the rationality of humans against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs to counter misconceptions, drawing upon psychological models of persuasion such as cognitive dissonance theory and elaboration likelihood theory. Through this endeavor, we highlight the potential of LLMs as personalized misinformation debunking agents.","sentences":["The prevalence of unwarranted beliefs, spanning pseudoscience, logical fallacies, and conspiracy theories, presents substantial societal hurdles and the risk of disseminating misinformation.","Utilizing established psychometric assessments, this study explores the capabilities of large language models (LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls.","We undertake a philosophical inquiry, juxtaposing the rationality of humans against that of LLMs.","Furthermore, we propose methodologies for harnessing LLMs to counter misconceptions, drawing upon psychological models of persuasion such as cognitive dissonance theory and elaboration likelihood theory.","Through this endeavor, we highlight the potential of LLMs as personalized misinformation debunking agents."],"url":"http://arxiv.org/abs/2405.00843v1","category":"cs.AI"}
{"created":"2024-05-01 20:08:51","title":"Sim-Grasp: Learning 6-DOF Grasp Policies for Cluttered Environments Using a Synthetic Benchmark","abstract":"In this paper, we present Sim-Grasp, a robust 6-DOF two-finger grasping system that integrates advanced language models for enhanced object manipulation in cluttered environments. We introduce the Sim-Grasp-Dataset, which includes 1,550 objects across 500 scenarios with 7.9 million annotated labels, and develop Sim-GraspNet to generate grasp poses from point clouds. The Sim-Grasp-Polices achieve grasping success rates of 97.14% for single objects and 87.43% and 83.33% for mixed clutter scenarios of Levels 1-2 and Levels 3-4 objects, respectively. By incorporating language models for target identification through text and box prompts, Sim-Grasp enables both object-agnostic and target picking, pushing the boundaries of intelligent robotic systems.","sentences":["In this paper, we present Sim-Grasp, a robust 6-DOF two-finger grasping system that integrates advanced language models for enhanced object manipulation in cluttered environments.","We introduce the Sim-Grasp-Dataset, which includes 1,550 objects across 500 scenarios with 7.9 million annotated labels, and develop Sim-GraspNet to generate grasp poses from point clouds.","The Sim-Grasp-Polices achieve grasping success rates of 97.14% for single objects and 87.43% and 83.33% for mixed clutter scenarios of Levels 1-2 and Levels 3-4 objects, respectively.","By incorporating language models for target identification through text and box prompts, Sim-Grasp enables both object-agnostic and target picking, pushing the boundaries of intelligent robotic systems."],"url":"http://arxiv.org/abs/2405.00841v1","category":"cs.RO"}
{"created":"2024-05-01 20:03:37","title":"Communication-Efficient Training Workload Balancing for Decentralized Multi-Agent Learning","abstract":"Decentralized Multi-agent Learning (DML) enables collaborative model training while preserving data privacy. However, inherent heterogeneity in agents' resources (computation, communication, and task size) may lead to substantial variations in training time. This heterogeneity creates a bottleneck, lengthening the overall training time due to straggler effects and potentially wasting spare resources of faster agents. To minimize training time in heterogeneous environments, we present a Communication-Efficient Training Workload Balancing for Decentralized Multi-Agent Learning (ComDML), which balances the workload among agents through a decentralized approach. Leveraging local-loss split training, ComDML enables parallel updates, where slower agents offload part of their workload to faster agents. To minimize the overall training time, ComDML optimizes the workload balancing by jointly considering the communication and computation capacities of agents, which hinges upon integer programming. A dynamic decentralized pairing scheduler is developed to efficiently pair agents and determine optimal offloading amounts. We prove that in ComDML, both slower and faster agents' models converge, for convex and non-convex functions. Furthermore, extensive experimental results on popular datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants, with large models such as ResNet-56 and ResNet-110, demonstrate that ComDML can significantly reduce the overall training time while maintaining model accuracy, compared to state-of-the-art methods. ComDML demonstrates robustness in heterogeneous environments, and privacy measures can be seamlessly integrated for enhanced data protection.","sentences":["Decentralized Multi-agent Learning (DML) enables collaborative model training while preserving data privacy.","However, inherent heterogeneity in agents' resources (computation, communication, and task size) may lead to substantial variations in training time.","This heterogeneity creates a bottleneck, lengthening the overall training time due to straggler effects and potentially wasting spare resources of faster agents.","To minimize training time in heterogeneous environments, we present a Communication-Efficient Training Workload Balancing for Decentralized Multi-Agent Learning (ComDML), which balances the workload among agents through a decentralized approach.","Leveraging local-loss split training, ComDML enables parallel updates, where slower agents offload part of their workload to faster agents.","To minimize the overall training time, ComDML optimizes the workload balancing by jointly considering the communication and computation capacities of agents, which hinges upon integer programming.","A dynamic decentralized pairing scheduler is developed to efficiently pair agents and determine optimal offloading amounts.","We prove that in ComDML, both slower and faster agents' models converge, for convex and non-convex functions.","Furthermore, extensive experimental results on popular datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants, with large models such as ResNet-56 and ResNet-110, demonstrate that ComDML can significantly reduce the overall training time while maintaining model accuracy, compared to state-of-the-art methods.","ComDML demonstrates robustness in heterogeneous environments, and privacy measures can be seamlessly integrated for enhanced data protection."],"url":"http://arxiv.org/abs/2405.00839v1","category":"cs.LG"}
{"created":"2024-05-01 19:42:13","title":"Search for new resonances decaying to pairs of merged diphotons in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is presented for an extended Higgs sector with two new particles, X and $\\phi$, in the process X $\\to$ $\\phi\\phi$ $\\to$ $(\\gamma\\gamma)(\\gamma\\gamma)$. Novel neural networks classify events with diphotons that are merged and determine the diphoton masses. The search uses LHC proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected with the CMS detector, corresponding to an integrated luminosity of 138 fb$^{-1}$. No evidence of such resonances is seen. Upper limits are set on the production cross section versus the resonance masses, representing the most sensitive search in this channel.","sentences":["A search is presented for an extended Higgs sector with two new particles, X and $\\phi$, in the process X $\\to$ $\\phi\\phi$ $\\to$ $(\\gamma\\gamma)(\\gamma\\gamma)$. Novel neural networks classify events with diphotons that are merged and determine the diphoton masses.","The search uses LHC proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected with the CMS detector, corresponding to an integrated luminosity of 138 fb$^{-1}$. No evidence of such resonances is seen.","Upper limits are set on the production cross section versus the resonance masses, representing the most sensitive search in this channel."],"url":"http://arxiv.org/abs/2405.00834v1","category":"hep-ex"}
{"created":"2024-05-01 19:38:23","title":"Teaching Algorithm Design: A Literature Review","abstract":"Algorithm design is a vital skill developed in most undergraduate Computer Science (CS) programs, but few research studies focus on pedagogy related to algorithms coursework. To understand the work that has been done in the area, we present a systematic survey and literature review of CS Education studies. We search for research that is both related to algorithm design and evaluated on undergraduate-level students. Across all papers in the ACM Digital Library prior to August 2023, we only find 94 such papers.   We first classify these papers by topic, evaluation metric, evaluation methods, and intervention target. Through our classification, we find a broad sparsity of papers which indicates that many open questions remain about teaching algorithm design, with each algorithm topic only being discussed in between 0 and 10 papers. We also note the need for papers using rigorous research methods, as only 38 out of 88 papers presenting quantitative data use statistical tests, and only 15 out of 45 papers presenting qualitative data use a coding scheme. Only 17 papers report controlled trials.   We then synthesize the results of the existing literature to give insights into what the corpus reveals about how we should teach algorithms. Much of the literature explores implementing well-established practices, such as active learning or automated assessment, in the algorithms classroom. However, there are algorithms-specific results as well: a number of papers find that students may under-utilize certain algorithmic design techniques, and studies describe a variety of ways to select algorithms problems that increase student engagement and learning.   The results we present, along with the publicly available set of papers collected, provide a detailed representation of the current corpus of CS Education work related to algorithm design and can orient further research in the area.","sentences":["Algorithm design is a vital skill developed in most undergraduate Computer Science (CS) programs, but few research studies focus on pedagogy related to algorithms coursework.","To understand the work that has been done in the area, we present a systematic survey and literature review of CS Education studies.","We search for research that is both related to algorithm design and evaluated on undergraduate-level students.","Across all papers in the ACM Digital Library prior to August 2023, we only find 94 such papers.   ","We first classify these papers by topic, evaluation metric, evaluation methods, and intervention target.","Through our classification, we find a broad sparsity of papers which indicates that many open questions remain about teaching algorithm design, with each algorithm topic only being discussed in between 0 and 10 papers.","We also note the need for papers using rigorous research methods, as only 38 out of 88 papers presenting quantitative data use statistical tests, and only 15 out of 45 papers presenting qualitative data use a coding scheme.","Only 17 papers report controlled trials.   ","We then synthesize the results of the existing literature to give insights into what the corpus reveals about how we should teach algorithms.","Much of the literature explores implementing well-established practices, such as active learning or automated assessment, in the algorithms classroom.","However, there are algorithms-specific results as well: a number of papers find that students may under-utilize certain algorithmic design techniques, and studies describe a variety of ways to select algorithms problems that increase student engagement and learning.   ","The results we present, along with the publicly available set of papers collected, provide a detailed representation of the current corpus of CS Education work related to algorithm design and can orient further research in the area."],"url":"http://arxiv.org/abs/2405.00832v1","category":"cs.DS"}
{"created":"2024-05-01 19:33:02","title":"Analysis of Quantization Noise Suppression Gains in Digital Phased Arrays","abstract":"Digital phased arrays have often been disregarded for millimeter-wave communications since the analog-to-digital converters (ADCs) are power-hungry. In this paper, we provide a different perspective on this matter by demonstrating analytically and numerically how the ADC resolution can be reduced when using digital phased arrays. We perform a theoretical analysis of the quantization noise characteristics for an OFDM signal received and processed by a digital phased array, using Gaussian approximation of the OFDM signal. In particular, we quantify the quantization noise suppression factor analytically and numerically. This factor describes how much the coherent combining reduces the quantization noise as a function of the number of antennas, which allows for reducing the ADC bit resolution. For instance in a 8-16 antenna digital phased array the ADC resolution can be reduced with 1-2 bits compared to the ADC required for an analog phased array.","sentences":["Digital phased arrays have often been disregarded for millimeter-wave communications since the analog-to-digital converters (ADCs) are power-hungry.","In this paper, we provide a different perspective on this matter by demonstrating analytically and numerically how the ADC resolution can be reduced when using digital phased arrays.","We perform a theoretical analysis of the quantization noise characteristics for an OFDM signal received and processed by a digital phased array, using Gaussian approximation of the OFDM signal.","In particular, we quantify the quantization noise suppression factor analytically and numerically.","This factor describes how much the coherent combining reduces the quantization noise as a function of the number of antennas, which allows for reducing the ADC bit resolution.","For instance in a 8-16 antenna digital phased array the ADC resolution can be reduced with 1-2 bits compared to the ADC required for an analog phased array."],"url":"http://arxiv.org/abs/2405.00830v1","category":"cs.IT"}
{"created":"2024-05-01 19:07:03","title":"WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting","abstract":"We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being sent to the wrong person. WorkBench reveals weaknesses in agents' ability to undertake common business activities, raising questions about their use in high-stakes workplace settings. WorkBench is publicly available as a free resource at https://github.com/olly-styles/WorkBench.","sentences":["We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting.","WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks.","These tasks represent common business activities, such as sending emails and scheduling meetings.","The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions.","If a task has been successfully executed, one (or more) of the database values may change.","The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation.","We call this key contribution outcome-centric evaluation.","We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4).","We further find that agents' errors can result in the wrong action being taken, such as an email being sent to the wrong person.","WorkBench reveals weaknesses in agents' ability to undertake common business activities, raising questions about their use in high-stakes workplace settings.","WorkBench is publicly available as a free resource at https://github.com/olly-styles/WorkBench."],"url":"http://arxiv.org/abs/2405.00823v1","category":"cs.CL"}
{"created":"2024-05-01 18:42:43","title":"Classifying two-body Hamiltonians for Quantum Darwinism","abstract":"Quantum Darwinism is a paradigm to understand how classically objective reality emerges from within a fundamentally quantum universe. Despite the growing attention that this field of research as been enjoying, it is currently not known what specific properties a given Hamiltonian describing a generic quantum system must have to allow the emergence of classicality. Therefore, in the present work, we consider a broadly applicable generic model of an arbitrary finite-dimensional system interacting with an environment formed from an arbitrary collection of finite-dimensional degrees of freedom via an unspecified, potentially time-dependent Hamiltonian containing at most two-body interaction terms. We show that such models support quantum Darwinism if the set of operators acting on the system which enter the Hamiltonian satisfy a set of commutation relations with a pointer observable and with one other. We demonstrate our results by analyzing a wide range of example systems: a qutrit interacting with a qubit environment, a qubit-qubit model with interactions alternating in time, and a series of collision models including a minimal model of a quantum Maxwell demon.","sentences":["Quantum Darwinism is a paradigm to understand how classically objective reality emerges from within a fundamentally quantum universe.","Despite the growing attention that this field of research as been enjoying, it is currently not known what specific properties a given Hamiltonian describing a generic quantum system must have to allow the emergence of classicality.","Therefore, in the present work, we consider a broadly applicable generic model of an arbitrary finite-dimensional system interacting with an environment formed from an arbitrary collection of finite-dimensional degrees of freedom via an unspecified, potentially time-dependent Hamiltonian containing at most two-body interaction terms.","We show that such models support quantum Darwinism if the set of operators acting on the system which enter the Hamiltonian satisfy a set of commutation relations with a pointer observable and with one other.","We demonstrate our results by analyzing a wide range of example systems: a qutrit interacting with a qubit environment, a qubit-qubit model with interactions alternating in time, and a series of collision models including a minimal model of a quantum Maxwell demon."],"url":"http://arxiv.org/abs/2405.00805v1","category":"quant-ph"}
{"created":"2024-05-01 18:16:38","title":"Does Using Bazel Help Speed Up Continuous Integration Builds?","abstract":"A long continuous integration (CI) build forces developers to wait for CI feedback before starting subsequent development activities, leading to time wasted. In addition to a variety of build scheduling and test selection heuristics studied in the past, new artifact-based build technologies like Bazel have built-in support for advanced performance optimizations such as parallel build and incremental build (caching of build results). However, little is known about the extent to which new build technologies like Bazel deliver on their promised benefits, especially for long-build duration projects.   In this study, we collected 383 Bazel projects from GitHub, then studied their parallel and incremental build usage of Bazel in 4 popular CI services, and compared the results with Maven projects. We conducted 3,500 experiments on 383 Bazel projects and analyzed the build logs of a subset of 70 buildable projects to evaluate the performance impact of Bazel's parallel builds. Additionally, we performed 102,232 experiments on the 70 buildable projects' last 100 commits to evaluate Bazel's incremental build performance. Our results show that 31.23% of Bazel projects adopt a CI service but do not use Bazel in the CI service, while for those who do use Bazel in CI, 27.76% of them use other tools to facilitate Bazel's execution. Compared to sequential builds, the median speedups for long-build duration projects are 2.00x, 3.84x, 7.36x, and 12.80x, at parallelism degrees 2, 4, 8, and 16, respectively, even though, compared to a clean build, applying incremental build achieves a median speedup of 4.22x (with a build system tool-independent CI cache) and 4.71x (with a build system tool-specific cache) for long-build duration projects. Our results provide guidance for developers to improve the usage of Bazel in their projects.","sentences":["A long continuous integration (CI) build forces developers to wait for CI feedback before starting subsequent development activities, leading to time wasted.","In addition to a variety of build scheduling and test selection heuristics studied in the past, new artifact-based build technologies like Bazel have built-in support for advanced performance optimizations such as parallel build and incremental build (caching of build results).","However, little is known about the extent to which new build technologies like Bazel deliver on their promised benefits, especially for long-build duration projects.   ","In this study, we collected 383 Bazel projects from GitHub, then studied their parallel and incremental build usage of Bazel in 4 popular CI services, and compared the results with Maven projects.","We conducted 3,500 experiments on 383 Bazel projects and analyzed the build logs of a subset of 70 buildable projects to evaluate the performance impact of Bazel's parallel builds.","Additionally, we performed 102,232 experiments on the 70 buildable projects' last 100 commits to evaluate Bazel's incremental build performance.","Our results show that 31.23% of Bazel projects adopt a CI service but do not use Bazel in the CI service, while for those who do use Bazel in CI, 27.76% of them use other tools to facilitate Bazel's execution.","Compared to sequential builds, the median speedups for long-build duration projects are 2.00x, 3.84x, 7.36x, and 12.80x, at parallelism degrees 2, 4, 8, and 16, respectively, even though, compared to a clean build, applying incremental build achieves a median speedup of 4.22x (with a build system tool-independent CI cache) and 4.71x (with a build system tool-specific cache) for long-build duration projects.","Our results provide guidance for developers to improve the usage of Bazel in their projects."],"url":"http://arxiv.org/abs/2405.00796v1","category":"cs.SE"}
{"created":"2024-05-01 18:08:03","title":"The Impact of IMSI Catcher Deployments on Cellular Network Security: Challenges and Countermeasures in 4G and 5G Networks","abstract":"IMSI (International Mobile Subscriber Identity) catchers, also known as \"Stingrays\" or \"cell site simulators,\" are rogue devices that pose a significant threat to cellular network security [1]. IMSI catchers can intercept and manipulate cellular communications, compromising the privacy and security of mobile devices and their users. With the advent of 4G and 5G networks, IMSI catchers have become more sophisticated and pose new challenges to cellular network security [2]. This paper provides an overview of the impact of IMSI catcher deployments on cellular network security in the context of 4G and 5G networks. It discusses the challenges posed by IMSI catchers, including the unauthorized collection of IMSI numbers, interception of communications, and potential misuse of subscriber information. It also highlights the potential consequences of IMSI catcher deployments, including the compromise of user privacy, financial fraud, and unauthorized surveillance. The paper further reviews the countermeasures that can be employed to mitigate the risks posed by IMSI catchers. These countermeasures include network-based solutions such as signal analysis, encryption, and authentication mechanisms, as well as user-based solutions such as mobile applications and device settings. The paper also discusses the limitations and effectiveness of these countermeasures in the context of 4G and 5G networks. Finally, the paper identifies research gaps and future directions for enhancing cellular network security against IMSI catchers in the era of 4G and 5G networks. This includes the need for improved encryption algorithms, authentication mechanisms, and detection techniques to effectively detect and prevent IMSI catcher deployments. The paper also emphasizes the importance of regulatory and policy measures to govern the deployment and use of IMSI catchers to protect user privacy and security.","sentences":["IMSI (International Mobile Subscriber Identity) catchers, also known as \"Stingrays\" or \"cell site simulators,\" are rogue devices that pose a significant threat to cellular network security [1].","IMSI catchers can intercept and manipulate cellular communications, compromising the privacy and security of mobile devices and their users.","With the advent of 4G and 5G networks, IMSI catchers have become more sophisticated and pose new challenges to cellular network security [2].","This paper provides an overview of the impact of IMSI catcher deployments on cellular network security in the context of 4G and 5G networks.","It discusses the challenges posed by IMSI catchers, including the unauthorized collection of IMSI numbers, interception of communications, and potential misuse of subscriber information.","It also highlights the potential consequences of IMSI catcher deployments, including the compromise of user privacy, financial fraud, and unauthorized surveillance.","The paper further reviews the countermeasures that can be employed to mitigate the risks posed by IMSI catchers.","These countermeasures include network-based solutions such as signal analysis, encryption, and authentication mechanisms, as well as user-based solutions such as mobile applications and device settings.","The paper also discusses the limitations and effectiveness of these countermeasures in the context of 4G and 5G networks.","Finally, the paper identifies research gaps and future directions for enhancing cellular network security against IMSI catchers in the era of 4G and 5G networks.","This includes the need for improved encryption algorithms, authentication mechanisms, and detection techniques to effectively detect and prevent IMSI catcher deployments.","The paper also emphasizes the importance of regulatory and policy measures to govern the deployment and use of IMSI catchers to protect user privacy and security."],"url":"http://arxiv.org/abs/2405.00793v1","category":"cs.CR"}
{"created":"2024-05-01 18:07:48","title":"Obtaining Favorable Layouts for Multiple Object Generation","abstract":"Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts.","sentences":["Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success.","These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal.","However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects.","When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together.","To address this challenge, we propose a novel approach based on a guiding principle.","We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid.","This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us.","We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks.","We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts."],"url":"http://arxiv.org/abs/2405.00791v1","category":"cs.CV"}
{"created":"2024-05-01 18:02:25","title":"SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators","abstract":"Emerging multi-model workloads with heavy models like recent large language models significantly increased the compute and memory demands on hardware. To address such increasing demands, designing a scalable hardware architecture became a key problem. Among recent solutions, the 2.5D silicon interposer multi-chip module (MCM)-based AI accelerator has been actively explored as a promising scalable solution due to their significant benefits in the low engineering cost and composability. However, previous MCM accelerators are based on homogeneous architectures with fixed dataflow, which encounter major challenges from highly heterogeneous multi-model workloads due to their limited workload adaptivity. Therefore, in this work, we explore the opportunity in the heterogeneous dataflow MCM AI accelerators. We identify the scheduling of multi-model workload on heterogeneous dataflow MCM AI accelerator is an important and challenging problem due to its significance and scale, which reaches O(10^18) scale even for a single model case on 6x6 chiplets. We develop a set of heuristics to navigate the huge scheduling space and codify them into a scheduler with advanced techniques such as inter-chiplet pipelining. Our evaluation on ten multi-model workload scenarios for datacenter multitenancy and AR/VR use-cases has shown the efficacy of our approach, achieving on average 35.3% and 31.4% less energy-delay product (EDP) for the respective applications settings compared to homogeneous baselines.","sentences":["Emerging multi-model workloads with heavy models like recent large language models significantly increased the compute and memory demands on hardware.","To address such increasing demands, designing a scalable hardware architecture became a key problem.","Among recent solutions, the 2.5D silicon interposer multi-chip module (MCM)-based AI accelerator has been actively explored as a promising scalable solution due to their significant benefits in the low engineering cost and composability.","However, previous MCM accelerators are based on homogeneous architectures with fixed dataflow, which encounter major challenges from highly heterogeneous multi-model workloads due to their limited workload adaptivity.","Therefore, in this work, we explore the opportunity in the heterogeneous dataflow MCM AI accelerators.","We identify the scheduling of multi-model workload on heterogeneous dataflow MCM AI accelerator is an important and challenging problem due to its significance and scale, which reaches O(10^18) scale even for a single model case on 6x6 chiplets.","We develop a set of heuristics to navigate the huge scheduling space and codify them into a scheduler with advanced techniques such as inter-chiplet pipelining.","Our evaluation on ten multi-model workload scenarios for datacenter multitenancy and AR/VR use-cases has shown the efficacy of our approach, achieving on average 35.3% and 31.4% less energy-delay product (EDP) for the respective applications settings compared to homogeneous baselines."],"url":"http://arxiv.org/abs/2405.00790v1","category":"cs.AR"}
{"created":"2024-05-01 18:00:48","title":"Quasi-Nambu-Goldstone modes in many-body scar models","abstract":"From the quasisymmetry-group perspective [Phys. Rev. Lett. 126, 120604 (2021)], we show the universal existence of collective, coherent modes of excitations with small momenta in many-body scar models in the degenerate limit, where the energy spacing in the scar tower vanishes. The number of these modes, as well as the quantum numbers carried by them, are given, not by the symmetry of the Hamiltonian, but by the quasisymmetry of the scar tower: hence the name quasi-Goldstone modes. Based on this, we draw a concrete analogy between the paradigm of spontaneous symmetry breaking and the many-body scar physics in the degenerate limit.","sentences":["From the quasisymmetry-group perspective [Phys. Rev. Lett.","126, 120604 (2021)], we show the universal existence of collective, coherent modes of excitations with small momenta in many-body scar models in the degenerate limit, where the energy spacing in the scar tower vanishes.","The number of these modes, as well as the quantum numbers carried by them, are given, not by the symmetry of the Hamiltonian, but by the quasisymmetry of the scar tower:","hence the name quasi-Goldstone modes.","Based on this, we draw a concrete analogy between the paradigm of spontaneous symmetry breaking and the many-body scar physics in the degenerate limit."],"url":"http://arxiv.org/abs/2405.00785v1","category":"cond-mat.str-el"}
{"created":"2024-05-02 17:59:24","title":"OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning","abstract":"The advances in multimodal large language models (MLLMs) have led to growing interests in LLM-based autonomous driving agents to leverage their strong reasoning capabilities. However, capitalizing on MLLMs' strong reasoning capabilities for improved planning behavior is challenging since planning requires full 3D situational awareness beyond 2D reasoning. To address this challenge, our work proposes a holistic framework for strong alignment between agent models and 3D driving tasks. Our framework starts with a novel 3D MLLM architecture that uses sparse queries to lift and compress visual representations into 3D before feeding them into an LLM. This query-based representation allows us to jointly encode dynamic objects and static map elements (e.g., traffic lanes), providing a condensed world model for perception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new visual question-answering dataset challenging the true 3D situational awareness of a model with comprehensive visual question-answering (VQA) tasks, including scene description, traffic regulation, 3D grounding, counterfactual reasoning, decision making and planning. Extensive studies show the effectiveness of the proposed architecture as well as the importance of the VQA tasks for reasoning and planning in complex 3D scenes.","sentences":["The advances in multimodal large language models (MLLMs) have led to growing interests in LLM-based autonomous driving agents to leverage their strong reasoning capabilities.","However, capitalizing on MLLMs' strong reasoning capabilities for improved planning behavior is challenging since planning requires full 3D situational awareness beyond 2D reasoning.","To address this challenge, our work proposes a holistic framework for strong alignment between agent models and 3D driving tasks.","Our framework starts with a novel 3D MLLM architecture that uses sparse queries to lift and compress visual representations into 3D before feeding them into an LLM.","This query-based representation allows us to jointly encode dynamic objects and static map elements (e.g., traffic lanes), providing a condensed world model for perception-action alignment in 3D.","We further propose OmniDrive-nuScenes, a new visual question-answering dataset challenging the true 3D situational awareness of a model with comprehensive visual question-answering (VQA) tasks, including scene description, traffic regulation, 3D grounding, counterfactual reasoning, decision making and planning.","Extensive studies show the effectiveness of the proposed architecture as well as the importance of the VQA tasks for reasoning and planning in complex 3D scenes."],"url":"http://arxiv.org/abs/2405.01533v1","category":"cs.CV"}
{"created":"2024-05-02 17:51:17","title":"Implementation of time-dependent Hartree Fock in real space","abstract":"Time-dependent Hartree-Fock (TDHF) is one of the fundamental post-Hartree-Fock (HF) methods to describe excited states. In its Tamm-Dancoff form, equivalent to Configuration Interaction Singles, it is still widely used and particularly applicable to big molecules where more accurate methods may be unfeasibly expensive. However, it is rarely implemented in real space, mostly because of the expensive nature of the exact-exchange potential in real space. Compared to widely used Gaussian-type orbitals (GTO) basis sets, real space often offers easier implementation of equations and more systematic convergence of Rydberg states, as well as favorable scaling, effective domain parallelization, flexible boundary conditions, and ability to treat model systems. We implemented TDHF in the Octopus real-space code as a step toward linear-response hybrid time-dependent density-functional theory (TDDFT), other post-HF methods, and ensemble density-functional theory methods involving exact exchange. Calculation of HF's non-local exact exchange is very expensive in real space. We overcome this limitation with Octopus' implementation of Adaptively Compressed Exchange (ACE), and find the appropriate mixing and starting point to complete the ground-state calculation in a practical amount of time, to enable TDHF. We compared our results to those from GTOs on a set of small molecules and confirmed close agreement of results, though with larger deviations than in the case of semi-local TDDFT. We find that convergence of TDHF demands a finer real-space grid than semi-local TDDFT. We also present the subtleties in benchmarking a real-space calculation against GTOs, relating to Rydberg and vacuum states.","sentences":["Time-dependent Hartree-Fock (TDHF) is one of the fundamental post-Hartree-Fock (HF) methods to describe excited states.","In its Tamm-Dancoff form, equivalent to Configuration Interaction Singles, it is still widely used and particularly applicable to big molecules where more accurate methods may be unfeasibly expensive.","However, it is rarely implemented in real space, mostly because of the expensive nature of the exact-exchange potential in real space.","Compared to widely used Gaussian-type orbitals (GTO) basis sets, real space often offers easier implementation of equations and more systematic convergence of Rydberg states, as well as favorable scaling, effective domain parallelization, flexible boundary conditions, and ability to treat model systems.","We implemented TDHF in the Octopus real-space code as a step toward linear-response hybrid time-dependent density-functional theory (TDDFT), other post-HF methods, and ensemble density-functional theory methods involving exact exchange.","Calculation of HF's non-local exact exchange is very expensive in real space.","We overcome this limitation with Octopus' implementation of Adaptively Compressed Exchange (ACE), and find the appropriate mixing and starting point to complete the ground-state calculation in a practical amount of time, to enable TDHF.","We compared our results to those from GTOs on a set of small molecules and confirmed close agreement of results, though with larger deviations than in the case of semi-local TDDFT.","We find that convergence of TDHF demands a finer real-space grid than semi-local TDDFT.","We also present the subtleties in benchmarking a real-space calculation against GTOs, relating to Rydberg and vacuum states."],"url":"http://arxiv.org/abs/2405.01522v1","category":"physics.chem-ph"}
{"created":"2024-05-02 17:50:53","title":"Transformer-Aided Semantic Communications","abstract":"The transformer structure employed in large language models (LLMs), as a specialized category of deep neural networks (DNNs) featuring attention mechanisms, stands out for their ability to identify and highlight the most relevant aspects of input data. Such a capability is particularly beneficial in addressing a variety of communication challenges, notably in the realm of semantic communication where proper encoding of the relevant data is critical especially in systems with limited bandwidth. In this work, we employ vision transformers specifically for the purpose of compression and compact representation of the input image, with the goal of preserving semantic information throughout the transmission process. Through the use of the attention mechanism inherent in transformers, we create an attention mask. This mask effectively prioritizes critical segments of images for transmission, ensuring that the reconstruction phase focuses on key objects highlighted by the mask. Our methodology significantly improves the quality of semantic communication and optimizes bandwidth usage by encoding different parts of the data in accordance with their semantic information content, thus enhancing overall efficiency. We evaluate the effectiveness of our proposed framework using the TinyImageNet dataset, focusing on both reconstruction quality and accuracy. Our evaluation results demonstrate that our framework successfully preserves semantic information, even when only a fraction of the encoded data is transmitted, according to the intended compression rates.","sentences":["The transformer structure employed in large language models (LLMs), as a specialized category of deep neural networks (DNNs) featuring attention mechanisms, stands out for their ability to identify and highlight the most relevant aspects of input data.","Such a capability is particularly beneficial in addressing a variety of communication challenges, notably in the realm of semantic communication where proper encoding of the relevant data is critical especially in systems with limited bandwidth.","In this work, we employ vision transformers specifically for the purpose of compression and compact representation of the input image, with the goal of preserving semantic information throughout the transmission process.","Through the use of the attention mechanism inherent in transformers, we create an attention mask.","This mask effectively prioritizes critical segments of images for transmission, ensuring that the reconstruction phase focuses on key objects highlighted by the mask.","Our methodology significantly improves the quality of semantic communication and optimizes bandwidth usage by encoding different parts of the data in accordance with their semantic information content, thus enhancing overall efficiency.","We evaluate the effectiveness of our proposed framework using the TinyImageNet dataset, focusing on both reconstruction quality and accuracy.","Our evaluation results demonstrate that our framework successfully preserves semantic information, even when only a fraction of the encoded data is transmitted, according to the intended compression rates."],"url":"http://arxiv.org/abs/2405.01521v1","category":"cs.CV"}
{"created":"2024-05-02 17:46:42","title":"Model-based Deep Learning for Rate Split Multiple Access in Vehicular Communications","abstract":"Rate split multiple access (RSMA) has been proven as an effective communication scheme for 5G and beyond, especially in vehicular scenarios. However, RSMA requires complicated iterative algorithms for proper resource allocation, which cannot fulfill the stringent latency requirement in resource constrained vehicles. Although data driven approaches can alleviate this issue, they suffer from poor generalizability and scarce training data. In this paper, we propose a fractional programming (FP) based deep unfolding (DU) approach to address resource allocation problem for a weighted sum rate optimization in RSMA. By carefully designing the penalty function, we couple the variable update with projected gradient descent algorithm (PGD). Following the structure of PGD, we embed few learnable parameters in each layer of the DU network. Through extensive simulation, we have shown that the proposed model-based neural networks has similar performance as optimal results given by traditional algorithm but with much lower computational complexity, less training data, and higher resilience to test set data and out-of-distribution (OOD) data.","sentences":["Rate split multiple access (RSMA) has been proven as an effective communication scheme for 5G and beyond, especially in vehicular scenarios.","However, RSMA requires complicated iterative algorithms for proper resource allocation, which cannot fulfill the stringent latency requirement in resource constrained vehicles.","Although data driven approaches can alleviate this issue, they suffer from poor generalizability and scarce training data.","In this paper, we propose a fractional programming (FP) based deep unfolding (DU) approach to address resource allocation problem for a weighted sum rate optimization in RSMA.","By carefully designing the penalty function, we couple the variable update with projected gradient descent algorithm (PGD).","Following the structure of PGD, we embed few learnable parameters in each layer of the DU network.","Through extensive simulation, we have shown that the proposed model-based neural networks has similar performance as optimal results given by traditional algorithm but with much lower computational complexity, less training data, and higher resilience to test set data and out-of-distribution (OOD) data."],"url":"http://arxiv.org/abs/2405.01515v1","category":"cs.IT"}
{"created":"2024-05-02 17:46:13","title":"Valuing maintenance strategies for fusion plants as part of a future electricity grid","abstract":"Scheduled maintenance is likely to be lengthy and therefore consequential for the economics of fusion power plants. The maintenance strategy that maximizes the economic value of a plant depends on internal factors such as the cost and durability of the replaceable components, the frequency and duration of the maintenance blocks, and the external factors of the electricity system in which the plant operates. This paper examines the value of fusion power plants with various maintenance properties in a decarbonized United States Eastern Interconnection circa 2050. Seasonal variations in electricity supply and demand mean that certain times of year, particularly spring to early summer, are best for scheduled maintenance. Seasonality has two important consequences. First, the value of a plant can be 15% higher than what one would naively expect if value were directly proportional to its availability. Second, in some cases, replacing fractions of a component in shorter maintenance blocks spread over multiple years is better than replacing it all at once during a longer outage, even through the overall availability of the plant is lower in the former scenario.","sentences":["Scheduled maintenance is likely to be lengthy and therefore consequential for the economics of fusion power plants.","The maintenance strategy that maximizes the economic value of a plant depends on internal factors such as the cost and durability of the replaceable components, the frequency and duration of the maintenance blocks, and the external factors of the electricity system in which the plant operates.","This paper examines the value of fusion power plants with various maintenance properties in a decarbonized United States Eastern Interconnection circa 2050.","Seasonal variations in electricity supply and demand mean that certain times of year, particularly spring to early summer, are best for scheduled maintenance.","Seasonality has two important consequences.","First, the value of a plant can be 15% higher than what one would naively expect if value were directly proportional to its availability.","Second, in some cases, replacing fractions of a component in shorter maintenance blocks spread over multiple years is better than replacing it all at once during a longer outage, even through the overall availability of the plant is lower in the former scenario."],"url":"http://arxiv.org/abs/2405.01514v1","category":"physics.soc-ph"}
{"created":"2024-05-02 17:34:23","title":"Evaluation and Optimization of Adaptive Cruise Control in Autonomous Vehicles using the CARLA Simulator: A Study on Performance under Wet and Dry Weather Conditions","abstract":"Adaptive Cruise Control ACC can change the speed of the ego vehicle to maintain a safe distance from the following vehicle automatically. The primary purpose of this research is to use cutting-edge computing approaches to locate and track vehicles in real time under various conditions to achieve a safe ACC. The paper examines the extension of ACC employing depth cameras and radar sensors within Autonomous Vehicles AVs to respond in real time by changing weather conditions using the Car Learning to Act CARLA simulation platform at noon. The ego vehicle controller's decision to accelerate or decelerate depends on the speed of the leading ahead vehicle and the safe distance from that vehicle. Simulation results show that a Proportional Integral Derivative PID control of autonomous vehicles using a depth camera and radar sensors reduces the speed of the leading vehicle and the ego vehicle when it rains. In addition, longer travel time was observed for both vehicles in rainy conditions than in dry conditions. Also, PID control prevents the leading vehicle from rear collisions","sentences":["Adaptive Cruise Control ACC can change the speed of the ego vehicle to maintain a safe distance from the following vehicle automatically.","The primary purpose of this research is to use cutting-edge computing approaches to locate and track vehicles in real time under various conditions to achieve a safe ACC.","The paper examines the extension of ACC employing depth cameras and radar sensors within Autonomous Vehicles AVs to respond in real time by changing weather conditions using the Car Learning to Act CARLA simulation platform at noon.","The ego vehicle controller's decision to accelerate or decelerate depends on the speed of the leading ahead vehicle and the safe distance from that vehicle.","Simulation results show that a Proportional Integral Derivative PID control of autonomous vehicles using a depth camera and radar sensors reduces the speed of the leading vehicle and the ego vehicle when it rains.","In addition, longer travel time was observed for both vehicles in rainy conditions than in dry conditions.","Also, PID control prevents the leading vehicle from rear collisions"],"url":"http://arxiv.org/abs/2405.01504v1","category":"cs.RO"}
{"created":"2024-05-02 17:29:53","title":"Optical Manipulation of Spin States in Ultracold Magnetic Atoms via an Inner-Shell Hz Transition","abstract":"Lanthanides, like erbium and dysprosium, have emerged as powerful platforms for quantum-gas research due to their diverse properties, including a significant large spin manifold in their absolute ground state. However, effectively exploiting the spin richness necessitates precise manipulation of spin populations, a challenge yet to be fully addressed in this class of atomic species. In this work, we present an all-optical method for deterministically controlling the spin composition of a dipolar bosonic erbium gas, based on a clock-like transition in the telecom window at 1299 nm. The atoms can be prepared in just a few tens of microseconds in any spin-state composition using a sequence of Rabi-pulse pairs, selectively coupling Zeeman sublevels of the ground state with those of the long-lived clock-like state. Finally, we demonstrate that this transition can also be used to create spin-selective light shifts, thus fully suppressing spin-exchange collisions. These experimental results unlock exciting possibilities for implementing advanced spin models in isolated, clean and fully controllable lattice systems.","sentences":["Lanthanides, like erbium and dysprosium, have emerged as powerful platforms for quantum-gas research due to their diverse properties, including a significant large spin manifold in their absolute ground state.","However, effectively exploiting the spin richness necessitates precise manipulation of spin populations, a challenge yet to be fully addressed in this class of atomic species.","In this work, we present an all-optical method for deterministically controlling the spin composition of a dipolar bosonic erbium gas, based on a clock-like transition in the telecom window at 1299 nm.","The atoms can be prepared in just a few tens of microseconds in any spin-state composition using a sequence of Rabi-pulse pairs, selectively coupling Zeeman sublevels of the ground state with those of the long-lived clock-like state.","Finally, we demonstrate that this transition can also be used to create spin-selective light shifts, thus fully suppressing spin-exchange collisions.","These experimental results unlock exciting possibilities for implementing advanced spin models in isolated, clean and fully controllable lattice systems."],"url":"http://arxiv.org/abs/2405.01499v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-02 17:28:01","title":"Insight-HXMT View of the BHC Swift J1727.8-1613 during its outburst in 2023","abstract":"The transient Galactic black hole candidate Swift J1727.8-1613 went through an outburst for the very first time that started in August 2023 and lasted for almost 6 months. We study the timing and spectral properties of this source using publicly available archival Insight-HXMT data for the first 10 observation IDs that last from MJD 60181 to 60198 with a total of 92 exposures for all three energy bands. We extracted the quasi-periodic oscillation properties by model fitting the power density spectrum and from those properties we designate that the QPOs are type-C in nature. We also conclude that the origin of the QPOs could be the shock instabilities, formed in the transonic accretion flow around black holes. The spectral analysis was performed using simultaneous data from the three on-board instruments LE, ME, and HE of HXMT in the broad energy band of $2-150 $ keV. To achieve the best fit, we needed a combination of interstellar absorption, power-law, multi-color disk-blackbody continuum, gaussian emission/absorption, and power-law reflection by neutral material. From the spectral properties, we found that the source was in an intermediate state at the start of the analysis period and was making a transition toward the softer states. The shock (the boundary layer of the corona) moved inward in progressive days in accordance with the spectral nature. We found that the source is present in a high-inclination binary system. The hydrogen column density was found with an average value of $0.27_{-0.17}^{+0.08}\\times10^{22}$ cm$^{-2}$.","sentences":["The transient Galactic black hole candidate Swift J1727.8-1613 went through an outburst for the very first time that started in August 2023 and lasted for almost 6 months.","We study the timing and spectral properties of this source using publicly available archival Insight-HXMT data for the first 10 observation IDs that last from MJD 60181 to 60198 with a total of 92 exposures for all three energy bands.","We extracted the quasi-periodic oscillation properties by model fitting the power density spectrum and from those properties we designate that the QPOs are type-C in nature.","We also conclude that the origin of the QPOs could be the shock instabilities, formed in the transonic accretion flow around black holes.","The spectral analysis was performed using simultaneous data from the three on-board instruments LE, ME, and HE of HXMT in the broad energy band of $2-150 $ keV. To achieve the best fit, we needed a combination of interstellar absorption, power-law, multi-color disk-blackbody continuum, gaussian emission/absorption, and power-law reflection by neutral material.","From the spectral properties, we found that the source was in an intermediate state at the start of the analysis period and was making a transition toward the softer states.","The shock (the boundary layer of the corona) moved inward in progressive days in accordance with the spectral nature.","We found that the source is present in a high-inclination binary system.","The hydrogen column density was found with an average value of $0.27_{-0.17}^{+0.08}\\times10^{22}$ cm$^{-2}$."],"url":"http://arxiv.org/abs/2405.01498v1","category":"astro-ph.HE"}
{"created":"2024-05-02 17:25:32","title":"FeNNol: an Efficient and Flexible Library for Building Force-field-enhanced Neural Network Potentials","abstract":"Neural network interatomic potentials (NNPs) have recently proven to be powerful tools to accurately model complex molecular systems while bypassing the high numerical cost of ab-initio molecular dynamics simulations. In recent years, numerous advances in model architectures as well as the development of hybrid models combining machine-learning (ML) with more traditional, physically-motivated, force-field interactions have considerably increased the design space of ML potentials. In this paper, we present FeNNol, a new library for building, training and running force-field-enhanced neural network potentials. It provides a flexible and modular system for building hybrid models, allowing to easily combine state-of-the-art embeddings with ML-parameterized physical interaction terms without the need for explicit programming. Furthermore, FeNNol leverages the automatic differentiation and just-in-time compilation features of the Jax Python library to enable fast evaluation of NNPs, shrinking the performance gap between ML potentials and standard force-fields. This is demonstrated with the popular ANI-2x model reaching simulation speeds nearly on par with the AMOEBA polarizable force-field on commodity GPUs (GPU=Graphics processing unit). We hope that FeNNol will facilitate the development and application of new hybrid NNP architectures for a wide range of molecular simulation problems.","sentences":["Neural network interatomic potentials (NNPs) have recently proven to be powerful tools to accurately model complex molecular systems while bypassing the high numerical cost of ab-initio molecular dynamics simulations.","In recent years, numerous advances in model architectures as well as the development of hybrid models combining machine-learning (ML) with more traditional, physically-motivated, force-field interactions have considerably increased the design space of ML potentials.","In this paper, we present FeNNol, a new library for building, training and running force-field-enhanced neural network potentials.","It provides a flexible and modular system for building hybrid models, allowing to easily combine state-of-the-art embeddings with ML-parameterized physical interaction terms without the need for explicit programming.","Furthermore, FeNNol leverages the automatic differentiation and just-in-time compilation features of the Jax Python library to enable fast evaluation of NNPs, shrinking the performance gap between ML potentials and standard force-fields.","This is demonstrated with the popular ANI-2x model reaching simulation speeds nearly on par with the AMOEBA polarizable force-field on commodity GPUs (GPU=Graphics processing unit).","We hope that FeNNol will facilitate the development and application of new hybrid NNP architectures for a wide range of molecular simulation problems."],"url":"http://arxiv.org/abs/2405.01491v1","category":"physics.chem-ph"}
{"created":"2024-05-02 17:14:17","title":"The continuous extension of the logarithmic double layer potential to the Ahlfors-regular boundary","abstract":"For the real part of the Cauchy-type integral that is known to be the logarithmic potential of the double layer, a necessary and sufficient condition for the continuous extension to the Ahlfors-regular boundary is established.","sentences":["For the real part of the Cauchy-type integral that is known to be the logarithmic potential of the double layer, a necessary and sufficient condition for the continuous extension to the Ahlfors-regular boundary is established."],"url":"http://arxiv.org/abs/2405.01482v1","category":"math.CV"}
{"created":"2024-05-02 17:12:25","title":"Common pitfalls to avoid while using multiobjective optimization in machine learning","abstract":"Recently, there has been an increasing interest in exploring the application of multiobjective optimization (MOO) in machine learning (ML). The interest is driven by the numerous situations in real-life applications where multiple objectives need to be optimized simultaneously. A key aspect of MOO is the existence of a Pareto set, rather than a single optimal solution, which illustrates the inherent trade-offs between objectives. Despite its potential, there is a noticeable lack of satisfactory literature that could serve as an entry-level guide for ML practitioners who want to use MOO. Hence, our goal in this paper is to produce such a resource. We critically review previous studies, particularly those involving MOO in deep learning (using Physics-Informed Neural Networks (PINNs) as a guiding example), and identify misconceptions that highlight the need for a better grasp of MOO principles in ML. Using MOO of PINNs as a case study, we demonstrate the interplay between the data loss and the physics loss terms. We highlight the most common pitfalls one should avoid while using MOO techniques in ML. We begin by establishing the groundwork for MOO, focusing on well-known approaches such as the weighted sum (WS) method, alongside more complex techniques like the multiobjective gradient descent algorithm (MGDA). Additionally, we compare the results obtained from the WS and MGDA with one of the most common evolutionary algorithms, NSGA-II. We emphasize the importance of understanding the specific problem, the objective space, and the selected MOO method, while also noting that neglecting factors such as convergence can result in inaccurate outcomes and, consequently, a non-optimal solution. Our goal is to offer a clear and practical guide for ML practitioners to effectively apply MOO, particularly in the context of DL.","sentences":["Recently, there has been an increasing interest in exploring the application of multiobjective optimization (MOO) in machine learning (ML).","The interest is driven by the numerous situations in real-life applications where multiple objectives need to be optimized simultaneously.","A key aspect of MOO is the existence of a Pareto set, rather than a single optimal solution, which illustrates the inherent trade-offs between objectives.","Despite its potential, there is a noticeable lack of satisfactory literature that could serve as an entry-level guide for ML practitioners who want to use MOO.","Hence, our goal in this paper is to produce such a resource.","We critically review previous studies, particularly those involving MOO in deep learning (using Physics-Informed Neural Networks (PINNs) as a guiding example), and identify misconceptions that highlight the need for a better grasp of MOO principles in ML.","Using MOO of PINNs as a case study, we demonstrate the interplay between the data loss and the physics loss terms.","We highlight the most common pitfalls one should avoid while using MOO techniques in ML.","We begin by establishing the groundwork for MOO, focusing on well-known approaches such as the weighted sum (WS) method, alongside more complex techniques like the multiobjective gradient descent algorithm (MGDA).","Additionally, we compare the results obtained from the WS and MGDA with one of the most common evolutionary algorithms, NSGA-II.","We emphasize the importance of understanding the specific problem, the objective space, and the selected MOO method, while also noting that neglecting factors such as convergence can result in inaccurate outcomes and, consequently, a non-optimal solution.","Our goal is to offer a clear and practical guide for ML practitioners to effectively apply MOO, particularly in the context of DL."],"url":"http://arxiv.org/abs/2405.01480v1","category":"cs.LG"}
{"created":"2024-05-02 17:08:11","title":"Experimental demonstration of frequency downconverted arm length stabilization for a future upgraded gravitational wave detector","abstract":"Ground-based laser interferometric gravitational wave detectors consist of complex multiple optical cavity systems. An arm-length stabilization (ALS) system has played an important role in bringing such complex detector into operational state and enhance the duty cycle. The sensitivity of these detectors can be improved if the thermal noise of their test mass mirror coatings is reduced. Crystalline AlGaAs coatings are a promising candidate for this. However, traditional ALS system with frequency-doubled 532 nm light is no longer an option with AlGaAs coatings due to the narrow bandgap of GaAs, thus alternative locking schemes must be developed. In this letter, we describe an experimental demonstration of a novel ALS scheme which is compatible with AlGaAs coatings. This ALS scheme will enable the use of AlGaAs coatings and contribute to improved sensitivity of future detectors.","sentences":["Ground-based laser interferometric gravitational wave detectors consist of complex multiple optical cavity systems.","An arm-length stabilization (ALS) system has played an important role in bringing such complex detector into operational state and enhance the duty cycle.","The sensitivity of these detectors can be improved if the thermal noise of their test mass mirror coatings is reduced.","Crystalline AlGaAs coatings are a promising candidate for this.","However, traditional ALS system with frequency-doubled 532 nm light is no longer an option with AlGaAs coatings due to the narrow bandgap of GaAs, thus alternative locking schemes must be developed.","In this letter, we describe an experimental demonstration of a novel ALS scheme which is compatible with AlGaAs coatings.","This ALS scheme will enable the use of AlGaAs coatings and contribute to improved sensitivity of future detectors."],"url":"http://arxiv.org/abs/2405.01475v1","category":"physics.optics"}
{"created":"2024-05-02 17:06:30","title":"RUBIES: Evolved Stellar Populations with Extended Formation Histories at $z \\sim 7-8$ in Candidate Massive Galaxies Identified with JWST/NIRSpec","abstract":"The identification of red, apparently massive galaxies at $z>7$ in early JWST photometry suggests a strongly accelerated timeline compared to standard models of galaxy growth. A major uncertainty in the interpretation is whether the red colors are caused by evolved stellar populations, dust, or other effects such as emission lines or AGN. Here we show that three of the massive galaxy candidates at $z=6.7-8.4$ have prominent Balmer breaks in JWST/NIRSpec spectroscopy from the RUBIES program. The Balmer breaks demonstrate unambiguously that stellar emission dominates at $\\lambda_{\\rm rest} = 0.4\\,\\mu$m, and require formation histories extending hundreds of Myr into the past in galaxies only 600--800 Myr after the Big Bang. Two of the three galaxies also show broad Balmer lines, with H$\\beta$ FWHM $>2500~{\\rm km\\,s^{-1}}$, suggesting that dust-reddened AGN contribute to, or even dominate, the SEDs of these galaxies at $\\lambda_{\\rm rest}\\gtrsim 0.6\\,\\mu$m. All three galaxies have relatively narrow [O III] lines, seemingly ruling out a high-mass interpretation if the lines arise in dynamically-relaxed, inclined disks. Yet, the inferred masses also remain highly uncertain. We model the high-quality spectra using Prospector to decompose the continuum into stellar and AGN components, and explore limiting cases in stellar/AGN contribution. This produces a wide range of possible stellar masses, spanning $M_\\star \\sim 10^9 - 10^{11}\\,{\\rm M_{\\odot}}$. Nevertheless, all fits suggest a very early and rapid formation, most of which follow with a truncation in star formation. Potential origins and evolutionary tracks for these objects are discussed, from the cores of massive galaxies to low-mass galaxies with over-massive black holes. Intriguingly, we find all of these explanations to be incomplete; deeper and redder data are needed to understand the physics of these systems.","sentences":["The identification of red, apparently massive galaxies at $z>7$ in early JWST photometry suggests a strongly accelerated timeline compared to standard models of galaxy growth.","A major uncertainty in the interpretation is whether the red colors are caused by evolved stellar populations, dust, or other effects such as emission lines or AGN.","Here we show that three of the massive galaxy candidates at $z=6.7-8.4$ have prominent Balmer breaks in JWST/NIRSpec spectroscopy from the RUBIES program.","The Balmer breaks demonstrate unambiguously that stellar emission dominates at $\\lambda_{\\rm rest} = 0.4\\,\\mu$m, and require formation histories extending hundreds of Myr into the past in galaxies only 600--800 Myr after the Big Bang.","Two of the three galaxies also show broad Balmer lines, with H$\\beta$ FWHM $>2500~{\\rm km\\,s^{-1}}$, suggesting that dust-reddened AGN contribute to, or even dominate, the SEDs of these galaxies at $\\lambda_{\\rm rest}\\gtrsim 0.6\\,\\mu$m.","All three galaxies have relatively narrow [O III] lines, seemingly ruling out a high-mass interpretation if the lines arise in dynamically-relaxed, inclined disks.","Yet, the inferred masses also remain highly uncertain.","We model the high-quality spectra using Prospector to decompose the continuum into stellar and AGN components, and explore limiting cases in stellar/AGN contribution.","This produces a wide range of possible stellar masses, spanning $M_\\star \\sim 10^9 - 10^{11}\\,{\\rm M_{\\odot}}$.","Nevertheless, all fits suggest a very early and rapid formation, most of which follow with a truncation in star formation.","Potential origins and evolutionary tracks for these objects are discussed, from the cores of massive galaxies to low-mass galaxies with over-massive black holes.","Intriguingly, we find all of these explanations to be incomplete; deeper and redder data are needed to understand the physics of these systems."],"url":"http://arxiv.org/abs/2405.01473v1","category":"astro-ph.GA"}
{"created":"2024-05-02 16:37:10","title":"Anomalous phonon Gr\u00fcneisen parameters in semiconductor Ta$_2$NiS$_5$","abstract":"Strain tuning is a powerful experimental method in probing correlated electron systems. Here we study the strain response of the lattice dynamics and electronic structure in semiconductor Ta$_2$NiS$_5$ by polarization-resolved Raman spectroscopy. We observe an increase of the size of the direct semiconducting band gap. Although the majority of the optical phonons show only marginal dependence to applied strain, the frequency of the two B$_{2g}$ phonon modes, which have quadrupolar symmetry and already anomalously soften on cooling under zero strain, increases significantly with tensile strain along the $a$ axis. The corresponding Gr\\\"uneisen parameters are unusually large in magnitude and negative in sign. These effects are well captured by first-principles density functional theory calculations and indicate close proximity of Ta$_2$NiS$_5$ to a structural instability, similar to that encountered in excitonic insulator candidate Ta$_2$NiSe$_5$.","sentences":["Strain tuning is a powerful experimental method in probing correlated electron systems.","Here we study the strain response of the lattice dynamics and electronic structure in semiconductor Ta$_2$NiS$_5$ by polarization-resolved Raman spectroscopy.","We observe an increase of the size of the direct semiconducting band gap.","Although the majority of the optical phonons show only marginal dependence to applied strain, the frequency of the two B$_{2g}$ phonon modes, which have quadrupolar symmetry and already anomalously soften on cooling under zero strain, increases significantly with tensile strain along the $a$ axis.","The corresponding Gr\\\"uneisen parameters are unusually large in magnitude and negative in sign.","These effects are well captured by first-principles density functional theory calculations and indicate close proximity of Ta$_2$NiS$_5$ to a structural instability, similar to that encountered in excitonic insulator candidate Ta$_2$NiSe$_5$."],"url":"http://arxiv.org/abs/2405.01455v1","category":"cond-mat.str-el"}
{"created":"2024-05-02 16:32:37","title":"GTX: A Transactional Graph Data System For HTAP Workloads","abstract":"Processing, managing, and analyzing dynamic graphs are the cornerstone in multiple application domains including fraud detection, recommendation system, graph neural network training, etc. This demo presents GTX, a latch-free write-optimized transactional graph data system that supports high throughput read-write transactions while maintaining competitive graph analytics. GTX has a unique latch-free graph storage and a transaction and concurrency control protocol for dynamic power-law graphs. GTX leverages atomic operations to eliminate latches, proposes a delta-based multi-version storage, and designs a hybrid transaction commit protocol to reduce interference between concurrent operations. To further improve its throughput, we design a delta-chains index to support efficient edge lookups. GTX manages concurrency control at delta-chain level, and provides adaptive concurrency according to the workload. Real-world graph access and updates exhibit temporal localities and hotspots. Unlike other transactional graph systems that experience significant performance degradation, GTX is the only system that can adapt to temporal localities and hotspots in graph updates and maintain million-transactions-per-second throughput. GTX is prototyped as a graph library and is evaluated using a graph library evaluation tool using real and synthetic datasets.","sentences":["Processing, managing, and analyzing dynamic graphs are the cornerstone in multiple application domains including fraud detection, recommendation system, graph neural network training, etc.","This demo presents GTX, a latch-free write-optimized transactional graph data system that supports high throughput read-write transactions while maintaining competitive graph analytics.","GTX has a unique latch-free graph storage and a transaction and concurrency control protocol for dynamic power-law graphs.","GTX leverages atomic operations to eliminate latches, proposes a delta-based multi-version storage, and designs a hybrid transaction commit protocol to reduce interference between concurrent operations.","To further improve its throughput, we design a delta-chains index to support efficient edge lookups.","GTX manages concurrency control at delta-chain level, and provides adaptive concurrency according to the workload.","Real-world graph access and updates exhibit temporal localities and hotspots.","Unlike other transactional graph systems that experience significant performance degradation, GTX is the only system that can adapt to temporal localities and hotspots in graph updates and maintain million-transactions-per-second throughput.","GTX is prototyped as a graph library and is evaluated using a graph library evaluation tool using real and synthetic datasets."],"url":"http://arxiv.org/abs/2405.01448v1","category":"cs.DB"}
{"created":"2024-05-02 16:26:07","title":"Two competing populations with a common environmental resource","abstract":"Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state. It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations. However, it has primarily focused on the dynamics of a single population of agents. In this paper, we consider the impact of two populations of agents that share a common environmental resource. We focus on a scenario where individuals in one population are governed by an environmentally \"responsible\" incentive policy, and individuals in the other population are environmentally \"irresponsible\". An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified. We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis.","sentences":["Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state.","It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations.","However, it has primarily focused on the dynamics of a single population of agents.","In this paper, we consider the impact of two populations of agents that share a common environmental resource.","We focus on a scenario where individuals in one population are governed by an environmentally \"responsible\" incentive policy, and individuals in the other population are environmentally \"irresponsible\".","An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified.","We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis."],"url":"http://arxiv.org/abs/2405.01437v1","category":"cs.GT"}
{"created":"2024-05-02 16:19:55","title":"Learning quantum states of continuous variable systems","abstract":"Quantum state tomography with rigorous guarantees with respect to the trace distance, the most operationally meaningful metric for distinguishing quantum states, has been studied extensively for finite-dimensional systems; however, it remains almost unexplored for continuous variable systems. This work fills this gap. We prove that learning energy-constrained $n$-mode states without any additional prior assumption is extremely inefficient: The minimum number of copies needed for achieving an $\\varepsilon$-approximation in trace distance scales as $\\sim \\varepsilon^{-2n}$, in stark contrast to the $n$-qudit case, where the $\\varepsilon$-scaling is $\\sim \\varepsilon^{-2}$. Specifically, we find the optimal sample complexity of tomography of energy-constrained pure states, thereby establishing the ultimate achievable performance of tomography of continuous variable systems. Given such an extreme inefficiency, we then investigate whether more structured, yet still physically interesting, classes of quantum states can be efficiently tomographed. We rigorously prove that this is indeed the case for Gaussian states, a result previously assumed but never proved in the literature. To accomplish this, we establish bounds on the trace distance between two Gaussian states in terms of the norm distance of their first and second moments, which constitute technical tools of independent interest. This allows us to answer a fundamental question for the field of Gaussian quantum information: by estimating the first and second moments of an unknown Gaussian state with precision $\\varepsilon$, what is the resulting trace distance error on the state? Lastly, we show how to efficiently learn $t$-doped Gaussian states, i.e., states prepared by Gaussian unitaries and at most $t$ local non-Gaussian evolutions, unveiling more of the structure of these slightly-perturbed Gaussian systems.","sentences":["Quantum state tomography with rigorous guarantees with respect to the trace distance, the most operationally meaningful metric for distinguishing quantum states, has been studied extensively for finite-dimensional systems; however, it remains almost unexplored for continuous variable systems.","This work fills this gap.","We prove that learning energy-constrained $n$-mode states without any additional prior assumption is extremely inefficient: The minimum number of copies needed for achieving an $\\varepsilon$-approximation in trace distance scales as $\\sim \\varepsilon^{-2n}$, in stark contrast to the $n$-qudit case, where the $\\varepsilon$-scaling is $\\sim \\varepsilon^{-2}$. Specifically, we find the optimal sample complexity of tomography of energy-constrained pure states, thereby establishing the ultimate achievable performance of tomography of continuous variable systems.","Given such an extreme inefficiency, we then investigate whether more structured, yet still physically interesting, classes of quantum states can be efficiently tomographed.","We rigorously prove that this is indeed the case for Gaussian states, a result previously assumed but never proved in the literature.","To accomplish this, we establish bounds on the trace distance between two Gaussian states in terms of the norm distance of their first and second moments, which constitute technical tools of independent interest.","This allows us to answer a fundamental question for the field of Gaussian quantum information: by estimating the first and second moments of an unknown Gaussian state with precision $\\varepsilon$, what is the resulting trace distance error on the state?","Lastly, we show how to efficiently learn $t$-doped Gaussian states, i.e., states prepared by Gaussian unitaries and at most $t$ local non-Gaussian evolutions, unveiling more of the structure of these slightly-perturbed Gaussian systems."],"url":"http://arxiv.org/abs/2405.01431v1","category":"quant-ph"}
{"created":"2024-05-02 16:18:22","title":"Co-rank $1$ Arithmetic Siegel--Weil IV: Analytic local-to-global","abstract":"This is the fourth in a sequence of four papers, where we prove the arithmetic Siegel--Weil formula in co-rank $1$ for Kudla--Rapoport special cycles on exotic smooth integral models of unitary Shimura varieties of arbitrarily large even arithmetic dimension. Our arithmetic Siegel--Weil formula implies that degrees of Kudla--Rapoport arithmetic special $1$-cycles are encoded in the first derivatives of unitary Eisenstein series Fourier coefficients.   In this paper, we pin down precise normalizations for some $U(m,m)$ Siegel Eisenstein series, give local Siegel--Weil special value formulas with explicit constants, and record a geometric Siegel--Weil result for degrees of complex $0$-cycles. Using this, we complete the proof of our arithmetic Siegel--Weil results by patching together the local main theorems from our companion papers.","sentences":["This is the fourth in a sequence of four papers, where we prove the arithmetic Siegel--Weil formula in co-rank $1$ for Kudla--Rapoport special cycles on exotic smooth integral models of unitary Shimura varieties of arbitrarily large even arithmetic dimension.","Our arithmetic Siegel--Weil formula implies that degrees of Kudla--Rapoport arithmetic special $1$-cycles are encoded in the first derivatives of unitary Eisenstein series Fourier coefficients.   ","In this paper, we pin down precise normalizations for some $U(m,m)$ Siegel Eisenstein series, give local Siegel--Weil special value formulas with explicit constants, and record a geometric Siegel--Weil result for degrees of complex $0$-cycles.","Using this, we complete the proof of our arithmetic Siegel--Weil results by patching together the local main theorems from our companion papers."],"url":"http://arxiv.org/abs/2405.01429v1","category":"math.NT"}
{"created":"2024-05-02 16:15:46","title":"In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies","abstract":"We present a new random walk for uniformly sampling high-dimensional convex bodies. It achieves state-of-the-art runtime complexity with stronger guarantees on the output than previously known, namely in R\\'enyi divergence (which implies TV, $\\mathcal{W}_2$, KL, $\\chi^2$). The proof departs from known approaches for polytime algorithms for the problem -- we utilize a stochastic diffusion perspective to show contraction to the target distribution with the rate of convergence determined by functional isoperimetric constants of the stationary density.","sentences":["We present a new random walk for uniformly sampling high-dimensional convex bodies.","It achieves state-of-the-art runtime complexity with stronger guarantees on the output than previously known, namely in R\\'enyi divergence (which implies TV, $\\mathcal{W}_2$, KL, $\\chi^2$).","The proof departs from known approaches for polytime algorithms for the problem -- we utilize a stochastic diffusion perspective to show contraction to the target distribution with the rate of convergence determined by functional isoperimetric constants of the stationary density."],"url":"http://arxiv.org/abs/2405.01425v1","category":"cs.DS"}
{"created":"2024-05-02 16:08:03","title":"Modeling Activity-Driven Music Listening with PACE","abstract":"While the topic of listening context is widely studied in the literature of music recommender systems, the integration of regular user behavior is often omitted. In this paper, we propose PACE (PAttern-based user Consumption Embedding), a framework for building user embeddings that takes advantage of periodic listening behaviors. PACE leverages users' multichannel time-series consumption patterns to build understandable user vectors. We believe the embeddings learned with PACE unveil much about the repetitive nature of user listening dynamics. By applying this framework on long-term user histories, we evaluate the embeddings through a predictive task of activities performed while listening to music. The validation task's interest is two-fold, while it shows the relevance of our approach, it also offers an insightful way of understanding users' musical consumption habits.","sentences":["While the topic of listening context is widely studied in the literature of music recommender systems, the integration of regular user behavior is often omitted.","In this paper, we propose PACE (PAttern-based user Consumption Embedding), a framework for building user embeddings that takes advantage of periodic listening behaviors.","PACE leverages users' multichannel time-series consumption patterns to build understandable user vectors.","We believe the embeddings learned with PACE unveil much about the repetitive nature of user listening dynamics.","By applying this framework on long-term user histories, we evaluate the embeddings through a predictive task of activities performed while listening to music.","The validation task's interest is two-fold, while it shows the relevance of our approach, it also offers an insightful way of understanding users' musical consumption habits."],"url":"http://arxiv.org/abs/2405.01417v1","category":"cs.IR"}
{"created":"2024-05-02 16:02:04","title":"Staggered Routing in Autonomous Mobility-on-Demand Systems","abstract":"In autonomous mobility-on-demand systems, effectively managing vehicle flows to mitigate induced congestion and ensure efficient operations is imperative for system performance and positive customer experience. Against this background, we study the potential of staggered routing, i.e., purposely delaying trip departures from a system perspective, in order to reduce congestion and ensure efficient operations while still meeting customer time windows. We formalize the underlying planning problem and show how to efficiently model it as a mixed integer linear program. Moreover, we present a matheuristic that allows us to efficiently solve large-scale real-world instances both in an offline full-information setting and its online rolling horizon counterpart. We conduct a numerical study for Manhattan, New York City, focusing on low- and highly-congested scenarios. Our results show that in low-congestion scenarios, staggering trip departures allows mitigating, on average, 94% of the induced congestion in a full information setting. In a rolling horizon setting, our algorithm allows us to reduce 90% of the induced congestion. In high-congestion scenarios, we observe an average reduction of 66% as the full information bound and an average reduction of 56% in our online setting. Surprisingly, we show that these reductions can be reached by shifting trip departures by a maximum of six minutes in both the low and high-congestion scenarios.","sentences":["In autonomous mobility-on-demand systems, effectively managing vehicle flows to mitigate induced congestion and ensure efficient operations is imperative for system performance and positive customer experience.","Against this background, we study the potential of staggered routing, i.e., purposely delaying trip departures from a system perspective, in order to reduce congestion and ensure efficient operations while still meeting customer time windows.","We formalize the underlying planning problem and show how to efficiently model it as a mixed integer linear program.","Moreover, we present a matheuristic that allows us to efficiently solve large-scale real-world instances both in an offline full-information setting and its online rolling horizon counterpart.","We conduct a numerical study for Manhattan, New York City, focusing on low- and highly-congested scenarios.","Our results show that in low-congestion scenarios, staggering trip departures allows mitigating, on average, 94% of the induced congestion in a full information setting.","In a rolling horizon setting, our algorithm allows us to reduce 90% of the induced congestion.","In high-congestion scenarios, we observe an average reduction of 66% as the full information bound and an average reduction of 56% in our online setting.","Surprisingly, we show that these reductions can be reached by shifting trip departures by a maximum of six minutes in both the low and high-congestion scenarios."],"url":"http://arxiv.org/abs/2405.01410v1","category":"math.OC"}
{"created":"2024-05-02 15:39:09","title":"Cutting corners: Hypersphere sampling as a new standard for cosmological emulators","abstract":"Cosmological emulators of observables such as the Cosmic Microwave Background (CMB) spectra and matter power spectra commonly use training data sampled from a Latin hypercube. This method often incurs high computational costs by covering less relevant parts of the parameter space, especially in high dimensions where only a small fraction of the parameter space yields a significant likelihood.   In this paper, we introduce hypersphere sampling, which instead concentrates sample points in regions with higher likelihoods, significantly enhancing the efficiency and accuracy of emulators. A novel algorithm for sampling within a high-dimensional hyperellipsoid aligned with axes of correlation in the cosmological parameters is presented. This method focuses the distribution of training data points on areas of the parameter space that are most relevant to the models being tested, thereby avoiding the computational redundancies common in Latin hypercube approaches.   Comparative analysis using the \\textsc{connect} emulation tool demonstrates that hypersphere sampling can achieve similar or improved emulation precision with more than an order of magnitude fewer data points and thus less computational effort than traditional methods. This was tested for both the $\\Lambda$CDM model and a 5-parameter extension including Early Dark Energy, massive neutrinos, and additional ultra-relativistic degrees of freedom. Our results suggest that hypersphere sampling holds potential as a more efficient approach for cosmological emulation, particularly suitable for complex, high-dimensional models.","sentences":["Cosmological emulators of observables such as the Cosmic Microwave Background (CMB) spectra and matter power spectra commonly use training data sampled from a Latin hypercube.","This method often incurs high computational costs by covering less relevant parts of the parameter space, especially in high dimensions where only a small fraction of the parameter space yields a significant likelihood.   ","In this paper, we introduce hypersphere sampling, which instead concentrates sample points in regions with higher likelihoods, significantly enhancing the efficiency and accuracy of emulators.","A novel algorithm for sampling within a high-dimensional hyperellipsoid aligned with axes of correlation in the cosmological parameters is presented.","This method focuses the distribution of training data points on areas of the parameter space that are most relevant to the models being tested, thereby avoiding the computational redundancies common in Latin hypercube approaches.   ","Comparative analysis using the \\textsc{connect} emulation tool demonstrates that hypersphere sampling can achieve similar or improved emulation precision with more than an order of magnitude fewer data points and thus less computational effort than traditional methods.","This was tested for both the $\\Lambda$CDM model and a 5-parameter extension including Early Dark Energy, massive neutrinos, and additional ultra-relativistic degrees of freedom.","Our results suggest that hypersphere sampling holds potential as a more efficient approach for cosmological emulation, particularly suitable for complex, high-dimensional models."],"url":"http://arxiv.org/abs/2405.01396v1","category":"astro-ph.CO"}
{"created":"2024-05-02 15:37:02","title":"Development of the strip LGAD detector with double-end readout for future colliders","abstract":"The Low-Gain Avalanche Diode (LGAD) is a new silicon detector and holds wide application prospects in particle physics experiments due to its excellent timing resolution.   The LGAD with a pixel size of 1.3 mm $\\times$ 1.3 mm was used to construct a High Granularity Timing Detector (HGTD) in ATLAS experiments to solve the pile-up problem.   Meanwhile, the Circular Electron Positron Collider (CEPC) also proposes detectors using the LGAD. However, pixel LGAD exhibits higher readout electronics density and cost, which somewhat limits the application of LGADs. To decrease the readout electronics density, the Institute of High Energy Physics (IHEP) of the Chinese Academy of Sciences has designed strip LGADs with larger areas. These strip LGADs are all 19 mm in length but with different widths of 1.0 mm, 0.5 mm, and 0.3 mm.   This article provides a detailed introduction to the design parameters of these strip LGADs and tests their electrical characteristics, including leakage current, break-down voltage, depletion capacitance, etc.   The timing resolution and signal-to-noise ratio of the three strip LGAD sensors were investigated using a beta source test system.   The position resolution parallel to the strip direction was tested and analyzed for the first time using a pico-second laser test system.   Tests have demonstrated that the timing resolution of strip LGADs can reach about 37.5 ps, and position resolution parallel to the strip direction is better than 1 mm.","sentences":["The Low-Gain Avalanche Diode (LGAD) is a new silicon detector and holds wide application prospects in particle physics experiments due to its excellent timing resolution.   ","The LGAD with a pixel size of 1.3 mm $\\times$ 1.3 mm was used to construct a High Granularity Timing Detector (HGTD) in ATLAS experiments to solve the pile-up problem.   ","Meanwhile, the Circular Electron Positron Collider (CEPC) also proposes detectors using the LGAD.","However, pixel LGAD exhibits higher readout electronics density and cost, which somewhat limits the application of LGADs.","To decrease the readout electronics density, the Institute of High Energy Physics (IHEP) of the Chinese Academy of Sciences has designed strip LGADs with larger areas.","These strip LGADs are all 19 mm in length but with different widths of 1.0 mm, 0.5 mm, and 0.3 mm.   ","This article provides a detailed introduction to the design parameters of these strip LGADs and tests their electrical characteristics, including leakage current, break-down voltage, depletion capacitance, etc.   ","The timing resolution and signal-to-noise ratio of the three strip LGAD sensors were investigated using a beta source test system.   ","The position resolution parallel to the strip direction was tested and analyzed for the first time using a pico-second laser test system.   ","Tests have demonstrated that the timing resolution of strip LGADs can reach about 37.5 ps, and position resolution parallel to the strip direction is better than 1 mm."],"url":"http://arxiv.org/abs/2405.01393v1","category":"physics.ins-det"}
{"created":"2024-05-02 15:35:26","title":"The Sustainability Assessment Framework Toolkit: A Decade of Modeling Experience","abstract":"Software intensive systems play a crucial role in most, if not all, aspects of modern society. As such, both their sustainability and their role in supporting sustainable processes, must be realized by design. To this aim, the architecture of software intensive systems should be designed to support sustainability goals; and measured to understand how effectively they do so. In this paper, we present the Sustainability Assessment Framework (SAF) Toolkit -- a set of instruments that support architects and design decision makers in modeling sustainability as a software quality property. The SAF Toolkit is the result of our experience gained in over a decade of cases in collaboration with industrial partners. We illustrate the toolkit with examples stemming from various cases. We extract our lessons learned, and our current research and future plans to extend the SAF Toolkit for further architecture modeling and measurement.","sentences":["Software intensive systems play a crucial role in most, if not all, aspects of modern society.","As such, both their sustainability and their role in supporting sustainable processes, must be realized by design.","To this aim, the architecture of software intensive systems should be designed to support sustainability goals; and measured to understand how effectively they do so.","In this paper, we present the Sustainability Assessment Framework (SAF) Toolkit -- a set of instruments that support architects and design decision makers in modeling sustainability as a software quality property.","The SAF Toolkit is the result of our experience gained in over a decade of cases in collaboration with industrial partners.","We illustrate the toolkit with examples stemming from various cases.","We extract our lessons learned, and our current research and future plans to extend the SAF Toolkit for further architecture modeling and measurement."],"url":"http://arxiv.org/abs/2405.01391v1","category":"cs.SE"}
{"created":"2024-05-02 15:23:22","title":"High-frequency tails in spectral densities","abstract":"Recent developments in numerically exact quantum dynamics methods have brought the dream of calculating the dynamics of chemically complex open systems closer to reality. Path-integral-based methods, hierarchical equations of motion (HEOM) and quantum analog simulators all require the spectral density of the environment to describe its effect on the system. Here we find that the rate of slow population relaxation is sensitive to the precise functional form used to describe the spectral density peaks. This finding highlights yet another challenge to obtaining accurate spectral densities. In the context of quantum information science, we give a simple recipe to adjust the results of analog simulation for this difference assuming both the simulator and the target spectral densities are known.","sentences":["Recent developments in numerically exact quantum dynamics methods have brought the dream of calculating the dynamics of chemically complex open systems closer to reality.","Path-integral-based methods, hierarchical equations of motion (HEOM) and quantum analog simulators all require the spectral density of the environment to describe its effect on the system.","Here we find that the rate of slow population relaxation is sensitive to the precise functional form used to describe the spectral density peaks.","This finding highlights yet another challenge to obtaining accurate spectral densities.","In the context of quantum information science, we give a simple recipe to adjust the results of analog simulation for this difference assuming both the simulator and the target spectral densities are known."],"url":"http://arxiv.org/abs/2405.01381v1","category":"physics.chem-ph"}
{"created":"2024-05-02 15:18:46","title":"Nonlinearity-induced symmetry breaking in a system of two parametrically driven Kerr-Duffing oscillators","abstract":"We study the classical dynamics of a system of a pair of Kerr-Duffing nonlinear oscillators coupled by a nonlinear interaction and subject to a parametric drive. Within a rotating wave approximation (RWA), we analyze the steady-state solutions for the oscillation amplitude of the two oscillators. In the most relevant case of identical oscillators, we separately investigate configurations in which only one oscillator is parametrically driven, or both of them are simultaneously driven. In the latter regime, special attention is paid to the symmetric case where the parametric drives acting on the two oscillators are equal: for an increasing value of the detuning of the parametric drive, a transition to a multi-stable, symmetry-breaking regime is found, where the two oscillators display different oscillation amplitudes and phases.","sentences":["We study the classical dynamics of a system of a pair of Kerr-Duffing nonlinear oscillators coupled by a nonlinear interaction and subject to a parametric drive.","Within a rotating wave approximation (RWA), we analyze the steady-state solutions for the oscillation amplitude of the two oscillators.","In the most relevant case of identical oscillators, we separately investigate configurations in which only one oscillator is parametrically driven, or both of them are simultaneously driven.","In the latter regime, special attention is paid to the symmetric case where the parametric drives acting on the two oscillators are equal: for an increasing value of the detuning of the parametric drive, a transition to a multi-stable, symmetry-breaking regime is found, where the two oscillators display different oscillation amplitudes and phases."],"url":"http://arxiv.org/abs/2405.01377v1","category":"physics.class-ph"}
{"created":"2024-05-02 15:12:48","title":"Completing the Node-Averaged Complexity Landscape of LCLs on Trees","abstract":"The node-averaged complexity of a problem captures the number of rounds nodes of a graph have to spend on average to solve the problem in the LOCAL model. A challenging line of research with regards to this new complexity measure is to understand the complexity landscape of locally checkable labelings (LCLs) on families of bounded-degree graphs. Particularly interesting in this context is the family of bounded-degree trees as there, for the worst-case complexity, we know a complete characterization of the possible complexities and structures of LCL problems. A first step for the node-averaged complexity case has been achieved recently [DISC '23], where the authors in particular showed that in bounded-degree trees, there is a large complexity gap: There are no LCL problems with a deterministic node-averaged complexity between $\\omega(\\log^* n)$ and $n^{o(1)}$. For randomized algorithms, they even showed that the node-averaged complexity is either $O(1)$ or $n^{\\Omega(1)}$. In this work we fill in the remaining gaps and give a complete description of the node-averaged complexity landscape of LCLs on bounded-degree trees. Our contributions are threefold.   - On bounded-degree trees, there is no LCL with a node-averaged complexity between $\\omega(1)$ and $(\\log^*n)^{o(1)}$.   - For any constants $0<r_1 < r_2 \\leq 1$ and $\\varepsilon>0$, there exists a constant $c$ and an LCL problem with node-averaged complexity between $\\Omega((\\log^* n)^c)$ and $O((\\log^* n)^{c+\\varepsilon})$.   - For any constants $0<\\alpha\\leq 1/2$ and $\\varepsilon>0$, there exists an LCL problem with node-averaged complexity $\\Theta(n^x)$ for some $x\\in [\\alpha, \\alpha+\\varepsilon]$.","sentences":["The node-averaged complexity of a problem captures the number of rounds nodes of a graph have to spend on average to solve the problem in the LOCAL model.","A challenging line of research with regards to this new complexity measure is to understand the complexity landscape of locally checkable labelings (LCLs) on families of bounded-degree graphs.","Particularly interesting in this context is the family of bounded-degree trees as there, for the worst-case complexity, we know a complete characterization of the possible complexities and structures of LCL problems.","A first step for the node-averaged complexity case has been achieved recently","[DISC '23], where the authors in particular showed that in bounded-degree trees, there is a large complexity gap: There are no LCL problems with a deterministic node-averaged complexity between $\\omega(\\log^*","n)$ and $n^{o(1)}$.","For randomized algorithms, they even showed that the node-averaged complexity is either $O(1)$ or $n^{\\Omega(1)}$. In this work we fill in the remaining gaps and give a complete description of the node-averaged complexity landscape of LCLs on bounded-degree trees.","Our contributions are threefold.   ","- On bounded-degree trees, there is no LCL with a node-averaged complexity between $\\omega(1)$ and $(\\log^*n)^{o(1)}$.   - For any constants $0<r_1 < r_2 \\leq 1$ and $\\varepsilon>0$, there exists a constant $c$ and an LCL problem with node-averaged complexity between $\\Omega((\\log^* n)^c)$ and $O((\\log^* n)^{c+\\varepsilon})$.   - For any constants $0<\\alpha\\leq 1/2$ and $\\varepsilon>0$, there exists an LCL problem with node-averaged complexity $\\Theta(n^x)$ for some $x\\in [\\alpha, \\alpha+\\varepsilon]$."],"url":"http://arxiv.org/abs/2405.01366v1","category":"cs.DC"}
{"created":"2024-05-02 15:09:02","title":"Wideband Penetration Loss through Building Materials and Partitions at 6.75 GHz in FR1(C) and 16.95 GHz in the FR3 Upper Mid-band spectrum","abstract":"The 4--8 GHz FR1(C) and 7--24 GHz upper mid-band FR3 spectrum are promising new 6G spectrum allocations being considered by the International Telecommunications Union (ITU) and major governments around the world. There is an urgent need to understand the propagation behavior and radio coverage, outage, and material penetration for the global mobile wireless industry in both indoor and outdoor environments in these emerging frequency bands. This work presents measurements and models that describe the penetration loss in co-polarized and cross-polarized antenna configurations, exhibited by common materials found inside buildings and on building perimeters, including concrete, low-emissivity glass, wood, doors, drywall, and whiteboard at 6.75 GHz and 16.95 GHz. Measurement results show consistent lower penetration loss at 6.75 GHz compared to 16.95 GHz for all ten materials measured for co and cross-polarized antennas at incidence. For instance, the low-emissivity glass wall presents 33.7 dB loss at 6.75 GHz, while presenting 42.3 dB loss at 16.95 GHz. Penetration loss at these frequencies is contrasted with measurements at sub-6 GHz, mmWave and sub-THz frequencies along with 3GPP material penetration loss models. The results provide critical knowledge for future 5G and 6G cellular system deployments as well as refinements for the 3GPP material penetration models.","sentences":["The 4--8 GHz FR1(C) and 7--24 GHz upper mid-band FR3 spectrum are promising new 6G spectrum allocations being considered by the International Telecommunications Union (ITU) and major governments around the world.","There is an urgent need to understand the propagation behavior and radio coverage, outage, and material penetration for the global mobile wireless industry in both indoor and outdoor environments in these emerging frequency bands.","This work presents measurements and models that describe the penetration loss in co-polarized and cross-polarized antenna configurations, exhibited by common materials found inside buildings and on building perimeters, including concrete, low-emissivity glass, wood, doors, drywall, and whiteboard at 6.75 GHz and 16.95 GHz.","Measurement results show consistent lower penetration loss at 6.75 GHz compared to 16.95 GHz for all ten materials measured for co and cross-polarized antennas at incidence.","For instance, the low-emissivity glass wall presents 33.7 dB loss at 6.75 GHz, while presenting 42.3 dB loss at 16.95 GHz.","Penetration loss at these frequencies is contrasted with measurements at sub-6 GHz, mmWave and sub-THz frequencies along with 3GPP material penetration loss models.","The results provide critical knowledge for future 5G and 6G cellular system deployments as well as refinements for the 3GPP material penetration models."],"url":"http://arxiv.org/abs/2405.01362v1","category":"eess.SP"}
{"created":"2024-05-02 15:04:19","title":"Propagation measurements and channel models in Indoor Environment at 6.75 GHz FR1(C) and 16.95 GHz FR3 Upper-mid band Spectrum for 5G and 6G","abstract":"New spectrum allocations in the 4--8 GHz FR1(C) and 7--24 GHz FR3 mid-band frequency spectrum are being considered for 5G/6G cellular deployments. This paper presents results from the world's first comprehensive indoor hotspot (InH) propagation measurement campaign at 6.75 GHz and 16.95 GHz in the NYU WIRELESS Research Center using a 1 GHz wideband channel sounder system over distances from 11 to 97 m in line-of-sight (LOS) and non-LOS (NLOS). Analysis of directional and omnidirectional path loss (PL) using the close-in free space 1 m reference distance model shows a familiar waveguiding effect in LOS with an omnidirectional path loss exponent (PLE) of 1.40 at 6.75 GHz and 1.32 at 16.95 GHz. Compared to mmWave frequencies, the directional NLOS PLEs are lower at FR3 and FR1(C), while omnidirectional NLOS PLEs are similar, suggesting better propagation distances at lower frequencies for links with omnidirectional antennas at both ends of the links, but also, importantly, showing that higher gain antennas will offer better coverage at higher frequencies when antenna apertures are kept same over all frequencies. Comparison of the omnidirectional and directional RMS delay spread (DS) at FR1(C) and FR3 with mmWave frequencies indicates a clear decrease with increasing frequency. The mean spatial lobe and omnidirectional RMS angular spread (AS) is found to be wider at 6.75 GHz compared to 16.95 GHz indicating more multipath components are found in the azimuthal spatial domain at lower frequencies.","sentences":["New spectrum allocations in the 4--8 GHz FR1(C) and 7--24 GHz FR3 mid-band frequency spectrum are being considered for 5G/6G cellular deployments.","This paper presents results from the world's first comprehensive indoor hotspot (InH) propagation measurement campaign at 6.75 GHz and 16.95 GHz in the NYU WIRELESS Research Center using a 1 GHz wideband channel sounder system over distances from 11 to 97 m in line-of-sight (LOS) and non-LOS (NLOS).","Analysis of directional and omnidirectional path loss (PL) using the close-in free space 1 m reference distance model shows a familiar waveguiding effect in LOS with an omnidirectional path loss exponent (PLE) of 1.40 at 6.75 GHz and 1.32 at 16.95 GHz.","Compared to mmWave frequencies, the directional NLOS PLEs are lower at FR3 and FR1(C), while omnidirectional NLOS PLEs are similar, suggesting better propagation distances at lower frequencies for links with omnidirectional antennas at both ends of the links, but also, importantly, showing that higher gain antennas will offer better coverage at higher frequencies when antenna apertures are kept same over all frequencies.","Comparison of the omnidirectional and directional RMS delay spread (DS) at FR1(C) and FR3 with mmWave frequencies indicates a clear decrease with increasing frequency.","The mean spatial lobe and omnidirectional RMS angular spread (AS) is found to be wider at 6.75 GHz compared to 16.95 GHz indicating more multipath components are found in the azimuthal spatial domain at lower frequencies."],"url":"http://arxiv.org/abs/2405.01358v1","category":"eess.SP"}
{"created":"2024-05-02 15:00:55","title":"Using Waste Factor to Optimize Energy Efficiency in Multiple-Input Single-Output (MISO) and Multiple-Input Multiple-Output (MIMO) Systems","abstract":"This paper introduces Waste Factor (W) and Waste Figure (WF) to assess power efficiency in any multiple-input multiple-output (MIMO) or single-input multiple-output (SIMO) or multiple-input single-output (MISO) cascaded communication system. This paper builds upon the new theory of Waste Factor, which systematically models added wasted power in any cascade for parallel systems such as MISO, SIMO, and MIMO systems, which are prevalent in current wireless networks. Here, we also show the advantage of W compared to conventional metrics for quantifying and analyzing energy efficiency. This work explores the utility of W in assessing energy efficiency in communication channels, within Radio Access Networks (RANs).","sentences":["This paper introduces Waste Factor (W) and Waste Figure (WF) to assess power efficiency in any multiple-input multiple-output (MIMO) or single-input multiple-output (SIMO) or multiple-input single-output (MISO) cascaded communication system.","This paper builds upon the new theory of Waste Factor, which systematically models added wasted power in any cascade for parallel systems such as MISO, SIMO, and MIMO systems, which are prevalent in current wireless networks.","Here, we also show the advantage of W compared to conventional metrics for quantifying and analyzing energy efficiency.","This work explores the utility of W in assessing energy efficiency in communication channels, within Radio Access Networks (RANs)."],"url":"http://arxiv.org/abs/2405.01352v1","category":"cs.IT"}
{"created":"2024-05-02 14:50:53","title":"Improved weak convergence for the long time simulation of Mean-field Langevin equations","abstract":"We study the weak convergence behaviour of the Leimkuhler--Matthews method, a non-Markovian Euler-type scheme with the same computational cost as the Euler scheme, for the approximation of the stationary distribution of a one-dimensional McKean--Vlasov Stochastic Differential Equation (MV-SDE). The particular class under study is known as mean-field (overdamped) Langevin equations (MFL). We provide weak and strong error results for the scheme in both finite and infinite time. We work under a strong convexity assumption.   Based on a careful analysis of the variation processes and the Kolmogorov backward equation for the particle system associated with the MV-SDE, we show that the method attains a higher-order approximation accuracy in the long-time limit (of weak order convergence rate $3/2$) than the standard Euler method (of weak order $1$). While we use an interacting particle system (IPS) to approximate the MV-SDE, we show the convergence rate is independent of the dimension of the IPS and this includes establishing uniform-in-time decay estimates for moments of the IPS, the Kolmogorov backward equation and their derivatives. The theoretical findings are supported by numerical tests.","sentences":["We study the weak convergence behaviour of the Leimkuhler--Matthews method, a non-Markovian Euler-type scheme with the same computational cost as the Euler scheme, for the approximation of the stationary distribution of a one-dimensional McKean--Vlasov Stochastic Differential Equation (MV-SDE).","The particular class under study is known as mean-field (overdamped) Langevin equations (MFL).","We provide weak and strong error results for the scheme in both finite and infinite time.","We work under a strong convexity assumption.   ","Based on a careful analysis of the variation processes and the Kolmogorov backward equation for the particle system associated with the MV-SDE, we show that the method attains a higher-order approximation accuracy in the long-time limit (of weak order convergence rate $3/2$) than the standard Euler method (of weak order $1$).","While we use an interacting particle system (IPS) to approximate the MV-SDE, we show the convergence rate is independent of the dimension of the IPS and this includes establishing uniform-in-time decay estimates for moments of the IPS, the Kolmogorov backward equation and their derivatives.","The theoretical findings are supported by numerical tests."],"url":"http://arxiv.org/abs/2405.01346v1","category":"math.NA"}
{"created":"2024-05-02 14:47:19","title":"Metric Dimension and Geodetic Set Parameterized by Vertex Cover","abstract":"For a graph $G$, a subset $S\\subseteq V(G)$ is called a resolving set of $G$ if, for any two vertices $u,v\\in V(G)$, there exists a vertex $w\\in S$ such that $d(w,u)\\neq d(w,v)$. The Metric Dimension problem takes as input a graph $G$ on $n$ vertices and a positive integer $k$, and asks whether there exists a resolving set of size at most $k$. In another metric-based graph problem, Geodetic Set, the input is a graph $G$ and an integer $k$, and the objective is to determine whether there exists a subset $S\\subseteq V(G)$ of size at most $k$ such that, for any vertex $u \\in V(G)$, there are two vertices $s_1, s_2 \\in S$ such that $u$ lies on a shortest path from $s_1$ to $s_2$.   These two classical problems turn out to be intractable with respect to the natural parameter, i.e., the solution size, as well as most structural parameters, including the feedback vertex set number and pathwidth. Some of the very few existing tractable results state that they are both FPT with respect to the vertex cover number $vc$.   More precisely, we observe that both problems admit an FPT algorithm running in time $2^{\\mathcal{O}(vc^2)}\\cdot n^{\\mathcal{O}(1)}$, and a kernelization algorithm that outputs a kernel with $2^{\\mathcal{O}(vc)}$ vertices. We prove that unless the Exponential Time Hypothesis fails, Metric Dimension and Geodetic Set, even on graphs of bounded diameter, neither admit an FPT algorithm running in time $2^{o(vc^2)}\\cdot n^{\\mathcal(1)}$, nor a kernelization algorithm that reduces the solution size and outputs a kernel with $2^{o(vc)}$ vertices. The versatility of our technique enables us to apply it to both these problems.   We only know of one other problem in the literature that admits such a tight lower bound. Similarly, the list of known problems with exponential lower bounds on the number of vertices in kernelized instances is very short.","sentences":["For a graph $G$, a subset $S\\subseteq V(G)$ is called a resolving set of $G$ if, for any two vertices $u,v\\in V(G)$, there exists a vertex $w\\in S$ such that $d(w,u)\\neq d(w,v)$.","The Metric Dimension problem takes as input a graph $G$ on $n$ vertices and a positive integer $k$, and asks whether there exists a resolving set of size at most $k$. In another metric-based graph problem, Geodetic Set, the input is a graph $G$ and an integer $k$, and the objective is to determine whether there exists a subset $S\\subseteq V(G)$ of size at most $k$ such that, for any vertex $u \\in V(G)$, there are two vertices $s_1, s_2 \\in S$ such that $u$ lies on a shortest path from $s_1$ to $s_2$.   These two classical problems turn out to be intractable with respect to the natural parameter, i.e., the solution size, as well as most structural parameters, including the feedback vertex set number and pathwidth.","Some of the very few existing tractable results state that they are both FPT with respect to the vertex cover number $vc$.   More precisely, we observe that both problems admit an FPT algorithm running in time $2^{\\mathcal{O}(vc^2)}\\cdot n^{\\mathcal{O}(1)}$, and a kernelization algorithm that outputs a kernel with $2^{\\mathcal{O}(vc)}$ vertices.","We prove that unless the Exponential Time Hypothesis fails, Metric Dimension and Geodetic Set, even on graphs of bounded diameter, neither admit an FPT algorithm running in time $2^{o(vc^2)}\\cdot n^{\\mathcal(1)}$, nor a kernelization algorithm that reduces the solution size and outputs a kernel with $2^{o(vc)}$ vertices.","The versatility of our technique enables us to apply it to both these problems.   ","We only know of one other problem in the literature that admits such a tight lower bound.","Similarly, the list of known problems with exponential lower bounds on the number of vertices in kernelized instances is very short."],"url":"http://arxiv.org/abs/2405.01344v1","category":"cs.DS"}
{"created":"2024-05-02 14:43:44","title":"The rotation-tunneling spectrum of 3-hydroxypropenal and confirmation of its detection toward IRAS 16293$-$2422 B","abstract":"3-Hydroxypropenal (HOCHCHCHO) is the lower energy tautomer of malonaldehyde which displays a complex rotation-tunneling spectrum. It was detected tentatively toward the solar-type protostar IRAS 16293$-$2422 B with ALMA in the framework of the Protostellar Interferometric Line Survey (PILS). Several transitions, however, had large residuals, preventing not only their detection, but also the excitation temperature of the species from being determined unambiguously. We want to extend the existing rotational line list of 3-hydroxypropenal to shed more light on the recent observational results and to facilitate additional radio astronomical searches for this molecule. We analyzed the rotation-tunneling spectrum of 3-hydroxypropenal in the frequency regions between 150 and 330 GHz and between 400 and 660 GHz. Transitions were searched for in the PILS observations of IRAS 16293$-$2422. Local thermodynamic equilibrium (LTE) models were carried out and compared to the observations to constrain the excitation temperature. Additional transitions were searched for in other ALMA archival data of the same source to confirm the presence of 3-hydroxypropenal. More than 11500 transitions were assigned in the course of our investigation with quantum numbers $2 \\le J \\le 100$, $K_a \\le 59$, and $K_c \\le 97$, resulting in a greatly improved set of spectroscopic parameters. The comparison between the LTE models and the observations yields an excitation temperature of 125 K with a column density $N = 1.0 \\times 10^{15}$ cm$^{-2}$ for this species. We identified seven additional lines of 3-hydroxypropenal that show a good agreement with the model in the ALMA archive data. The calculated rotation-tunneling spectrum of 3-hydroxypropenal has sufficient accuracy for radio astronomical searches. The detection of 3-hydroxypropenal toward IRAS 16293$-$2422 B is now secure.","sentences":["3-Hydroxypropenal (HOCHCHCHO) is the lower energy tautomer of malonaldehyde which displays a complex rotation-tunneling spectrum.","It was detected tentatively toward the solar-type protostar IRAS 16293$-$2422 B with ALMA in the framework of the Protostellar Interferometric Line Survey (PILS).","Several transitions, however, had large residuals, preventing not only their detection, but also the excitation temperature of the species from being determined unambiguously.","We want to extend the existing rotational line list of 3-hydroxypropenal to shed more light on the recent observational results and to facilitate additional radio astronomical searches for this molecule.","We analyzed the rotation-tunneling spectrum of 3-hydroxypropenal in the frequency regions between 150 and 330 GHz and between 400 and 660 GHz.","Transitions were searched for in the PILS observations of IRAS 16293$-$2422.","Local thermodynamic equilibrium (LTE) models were carried out and compared to the observations to constrain the excitation temperature.","Additional transitions were searched for in other ALMA archival data of the same source to confirm the presence of 3-hydroxypropenal.","More than 11500 transitions were assigned in the course of our investigation with quantum numbers $2 \\le J \\le 100$, $K_a \\le 59$, and $K_c \\le 97$, resulting in a greatly improved set of spectroscopic parameters.","The comparison between the LTE models and the observations yields an excitation temperature of 125 K with a column density $N = 1.0 \\times 10^{15}$ cm$^{-2}$ for this species.","We identified seven additional lines of 3-hydroxypropenal that show a good agreement with the model in the ALMA archive data.","The calculated rotation-tunneling spectrum of 3-hydroxypropenal has sufficient accuracy for radio astronomical searches.","The detection of 3-hydroxypropenal toward IRAS 16293$-$2422 B is now secure."],"url":"http://arxiv.org/abs/2405.01338v1","category":"astro-ph.GA"}
{"created":"2024-05-02 14:40:28","title":"Nontopological Electromagnetic Hedgehogs","abstract":"We study classical localised configurations - solitons - in a theory of self-interacting complex Proca field with the global $U(1)$ symmetry. We focus on spherically-symmetric solitons near the nonrelativistic limit, which are supported by the quartic interactions of the neutral Proca field. Such solitons can source the radial electric (magnetic) field if one introduces a parity-even (parity-odd) coupling of the Proca field to the electromagnetic field tensor. We discuss the conditions of existence of such nontopological ''electromagnetic hedgehogs'' and their properties.","sentences":["We study classical localised configurations - solitons - in a theory of self-interacting complex Proca field with the global $U(1)$ symmetry.","We focus on spherically-symmetric solitons near the nonrelativistic limit, which are supported by the quartic interactions of the neutral Proca field.","Such solitons can source the radial electric (magnetic) field if one introduces a parity-even (parity-odd) coupling of the Proca field to the electromagnetic field tensor.","We discuss the conditions of existence of such nontopological ''electromagnetic hedgehogs'' and their properties."],"url":"http://arxiv.org/abs/2405.01335v1","category":"hep-th"}
{"created":"2024-05-02 14:32:21","title":"Decentralization of Ethereum's Builder Market","abstract":"Blockchains protect an ecosystem worth more than $500bn with their strong security properties derived from the principle of decentralization. Is today's blockchain really decentralized? In this paper, we empirically studied one of the {\\em least decentralized} parts of Ethereum -- the most used blockchain system in practice -- and shed light on the decentralization issue from a new perspective.   To avoid centralization caused by Maximal Extractable Value (MEV), Ethereum adopts a novel mechanism that produces blocks through a {\\em builder market}. After two years in operation, however, the builder market has evolved to a highly centralized one with three builders producing more than 90% of blocks. {\\em Why does the builder market centralize, given that it is permissionless and anyone can join?} Moreover, {\\em what are the security implications of a centralized builder market to MEV-Boost auctions?} Through a rigorous empirical study of the builder market's core mechanism, MEV-Boost auctions, we answered these two questions using a large-scale auction dataset we curated since 2022.   Unlike previous works that focus on {\\em who} wins the auctions, we focus on {\\em why} they win, to shed light on the {openness, competitiveness, and efficiency} of MEV-Boost auctions. Our findings also help identify directions for improving the decentralization of builder markets.","sentences":["Blockchains protect an ecosystem worth more than $500bn with their strong security properties derived from the principle of decentralization.","Is today's blockchain really decentralized?","In this paper, we empirically studied one of the {\\em least decentralized} parts of Ethereum -- the most used blockchain system in practice -- and shed light on the decentralization issue from a new perspective.   ","To avoid centralization caused by Maximal Extractable Value (MEV), Ethereum adopts a novel mechanism that produces blocks through a {\\em builder market}.","After two years in operation, however, the builder market has evolved to a highly centralized one with three builders producing more than 90% of blocks.","{\\em Why does the builder market centralize, given that it is permissionless and anyone can join?}","Moreover, {\\em what are the security implications of a centralized builder market to MEV-Boost auctions?}","Through a rigorous empirical study of the builder market's core mechanism, MEV-Boost auctions, we answered these two questions using a large-scale auction dataset we curated since 2022.   ","Unlike previous works that focus on {\\em who} wins the auctions, we focus on {\\em why} they win, to shed light on the {openness, competitiveness, and efficiency} of MEV-Boost auctions.","Our findings also help identify directions for improving the decentralization of builder markets."],"url":"http://arxiv.org/abs/2405.01329v1","category":"cs.CR"}
{"created":"2024-05-02 14:29:42","title":"A Framework for the Systematic Assessment of Anomaly Detectors in Time-Sensitive Automotive Networks","abstract":"Connected cars are susceptible to cyberattacks. Security and safety of future vehicles highly depend on a holistic protection of automotive components, of which the time-sensitive backbone network takes a significant role. These onboard Time-Sensitive Networks (TSNs) require monitoring for safety and -- as versatile platforms to host Network Anomaly Detection Systems (NADSs) -- for security. Still a thorough evaluation of anomaly detection methods in the context of hard real-time operations, automotive protocol stacks, and domain specific attack vectors is missing along with appropriate input datasets. In this paper, we present an assessment framework that allows for reproducible, comparable, and rapid evaluation of detection algorithms. It is based on a simulation toolchain, which contributes configurable topologies, traffic streams, anomalies, attacks, and detectors. We demonstrate the assessment of NADSs in a comprehensive in-vehicular network with its communication flows, on which we model traffic anomalies. We evaluate exemplary detection mechanisms and reveal how the detection performance is influenced by different combinations of TSN traffic flows and anomaly types. Our approach translates to other real-time Ethernet domains, such as industrial facilities, airplanes, and UAVs.","sentences":["Connected cars are susceptible to cyberattacks.","Security and safety of future vehicles highly depend on a holistic protection of automotive components, of which the time-sensitive backbone network takes a significant role.","These onboard Time-Sensitive Networks (TSNs) require monitoring for safety and -- as versatile platforms to host Network Anomaly Detection Systems (NADSs) -- for security.","Still a thorough evaluation of anomaly detection methods in the context of hard real-time operations, automotive protocol stacks, and domain specific attack vectors is missing along with appropriate input datasets.","In this paper, we present an assessment framework that allows for reproducible, comparable, and rapid evaluation of detection algorithms.","It is based on a simulation toolchain, which contributes configurable topologies, traffic streams, anomalies, attacks, and detectors.","We demonstrate the assessment of NADSs in a comprehensive in-vehicular network with its communication flows, on which we model traffic anomalies.","We evaluate exemplary detection mechanisms and reveal how the detection performance is influenced by different combinations of TSN traffic flows and anomaly types.","Our approach translates to other real-time Ethernet domains, such as industrial facilities, airplanes, and UAVs."],"url":"http://arxiv.org/abs/2405.01324v1","category":"cs.NI"}
{"created":"2024-05-02 14:28:01","title":"Reasoning About Group Polarization: From Semantic Games to Sequent Systems","abstract":"Group polarization, the phenomenon where individuals become more extreme after interacting, has been gaining attention, especially with the rise of social media shaping people's opinions. Recent interest has emerged in formal reasoning about group polarization using logical systems. In this work we consider the modal logic PNL that captures the notion of agents agreeing or disagreeing on a given topic. Our contribution involves enhancing PNL with advanced formal reasoning techniques, instead of relying on axiomatic systems for analyzing group polarization. To achieve this, we introduce a semantic game tailored for (hybrid) extensions of PNL. This game fosters dynamic reasoning about concrete network models, aligning with our goal of strengthening PNL's effectiveness in studying group polarization. We show how this semantic game leads to a provability game by systemically exploring the truth in all models. This leads to the first cut-free sequent systems for some variants of PNL. Using polarization of formulas, the proposed calculi can be modularly adapted to consider different frame properties of the underlying model.","sentences":["Group polarization, the phenomenon where individuals become more extreme after interacting, has been gaining attention, especially with the rise of social media shaping people's opinions.","Recent interest has emerged in formal reasoning about group polarization using logical systems.","In this work we consider the modal logic PNL that captures the notion of agents agreeing or disagreeing on a given topic.","Our contribution involves enhancing PNL with advanced formal reasoning techniques, instead of relying on axiomatic systems for analyzing group polarization.","To achieve this, we introduce a semantic game tailored for (hybrid) extensions of PNL.","This game fosters dynamic reasoning about concrete network models, aligning with our goal of strengthening PNL's effectiveness in studying group polarization.","We show how this semantic game leads to a provability game by systemically exploring the truth in all models.","This leads to the first cut-free sequent systems for some variants of PNL.","Using polarization of formulas, the proposed calculi can be modularly adapted to consider different frame properties of the underlying model."],"url":"http://arxiv.org/abs/2405.01322v1","category":"cs.LO"}
{"created":"2024-05-02 14:26:17","title":"Kahan-Hirota-Kimura maps preserving original cubic hamiltonians","abstract":"In this work we investigate the set of cubic Hamiltonian vector fields for which their associated Kahan-Hirota-Kimura maps preserve the original Hamiltonian function. We analyze these fields in $\\mathbb{R}^2$ and $\\mathbb{R}^4$. We also study a family of fields in $\\mathbb{R}^6$. Additionally, we explore several properties like the existence of additional first integrals of specific type, the possibility that the initial Hamiltonian vector field is a Lie Symmetry of the corresponding map, or the symplecticity of the considered maps.","sentences":["In this work we investigate the set of cubic Hamiltonian vector fields for which their associated Kahan-Hirota-Kimura maps preserve the original Hamiltonian function.","We analyze these fields in $\\mathbb{R}^2$ and $\\mathbb{R}^4$. We also study a family of fields in $\\mathbb{R}^6$. Additionally, we explore several properties like the existence of additional first integrals of specific type, the possibility that the initial Hamiltonian vector field is a Lie Symmetry of the corresponding map, or the symplecticity of the considered maps."],"url":"http://arxiv.org/abs/2405.01321v1","category":"nlin.SI"}
{"created":"2024-05-02 14:26:12","title":"Unsupervised identification of local atomic environment from atomistic potential descriptors","abstract":"Analyzing local structures effectively is key to unraveling the origin of many physical phenomena. Unsupervised algorithms offer an effective way of handling systems in which order parameters are unknown or computationally expensive. By combining novel unsupervised algorithm (Pairwise Controlled Manifold Approximation Projection) with atomistic potential descriptors, we distinguish between various chemical environments with minimal computational overhead. In particular, we apply this method to silicon and water systems. The algorithm effectively distinguishes between solid structures and phases of silicon, including solid and liquid phases, and accurately identifies interstitial, monovacancy, and surface atoms in diamond structures. In the case of water, it is capable of identifying an ice nucleus in the liquid phase, demonstrating its applicability in nucleation studies.","sentences":["Analyzing local structures effectively is key to unraveling the origin of many physical phenomena.","Unsupervised algorithms offer an effective way of handling systems in which order parameters are unknown or computationally expensive.","By combining novel unsupervised algorithm (Pairwise Controlled Manifold Approximation Projection) with atomistic potential descriptors, we distinguish between various chemical environments with minimal computational overhead.","In particular, we apply this method to silicon and water systems.","The algorithm effectively distinguishes between solid structures and phases of silicon, including solid and liquid phases, and accurately identifies interstitial, monovacancy, and surface atoms in diamond structures.","In the case of water, it is capable of identifying an ice nucleus in the liquid phase, demonstrating its applicability in nucleation studies."],"url":"http://arxiv.org/abs/2405.01320v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 14:23:07","title":"LOG-LIO2: A LiDAR-Inertial Odometry with Efficient Uncertainty Analysis","abstract":"Uncertainty in LiDAR measurements, stemming from factors such as range sensing, is crucial for LIO (LiDAR-Inertial Odometry) systems as it affects the accurate weighting in the loss function. While recent LIO systems address uncertainty related to range sensing, the impact of incident angle on uncertainty is often overlooked by the community. Moreover, the existing uncertainty propagation methods suffer from computational inefficiency. This paper proposes a comprehensive point uncertainty model that accounts for both the uncertainties from LiDAR measurements and surface characteristics, along with an efficient local uncertainty analytical method for LiDAR-based state estimation problem. We employ a projection operator that separates the uncertainty into the ray direction and its orthogonal plane. Then, we derive incremental Jacobian matrices of eigenvalues and eigenvectors w.r.t. points, which enables a fast approximation of uncertainty propagation. This approach eliminates the requirement for redundant traversal of points, significantly reducing the time complexity of uncertainty propagation from $\\mathcal{O} (n)$ to $\\mathcal{O} (1)$ when a new point is added. Simulations and experiments on public datasets are conducted to validate the accuracy and efficiency of our formulations. The proposed methods have been integrated into a LIO system, which is available at https://github.com/tiev-tongji/LOG-LIO2.","sentences":["Uncertainty in LiDAR measurements, stemming from factors such as range sensing, is crucial for LIO (LiDAR-Inertial Odometry) systems as it affects the accurate weighting in the loss function.","While recent LIO systems address uncertainty related to range sensing, the impact of incident angle on uncertainty is often overlooked by the community.","Moreover, the existing uncertainty propagation methods suffer from computational inefficiency.","This paper proposes a comprehensive point uncertainty model that accounts for both the uncertainties from LiDAR measurements and surface characteristics, along with an efficient local uncertainty analytical method for LiDAR-based state estimation problem.","We employ a projection operator that separates the uncertainty into the ray direction and its orthogonal plane.","Then, we derive incremental Jacobian matrices of eigenvalues and eigenvectors w.r.t. points, which enables a fast approximation of uncertainty propagation.","This approach eliminates the requirement for redundant traversal of points, significantly reducing the time complexity of uncertainty propagation from $\\mathcal{O} (n)$ to $\\mathcal{O} (1)$ when a new point is added.","Simulations and experiments on public datasets are conducted to validate the accuracy and efficiency of our formulations.","The proposed methods have been integrated into a LIO system, which is available at https://github.com/tiev-tongji/LOG-LIO2."],"url":"http://arxiv.org/abs/2405.01316v1","category":"cs.RO"}
{"created":"2024-05-02 14:20:25","title":"Generalising quantum imaginary time evolution to solve linear partial differential equations","abstract":"The quantum imaginary time evolution (QITE) methodology was developed to overcome a critical issue as regards non-unitarity in the implementation of imaginary time evolution on a quantum computer. QITE has since been used to approximate ground states of various physical systems. In this paper, we demonstrate a practical application of QITE as a quantum numerical solver for linear partial differential equations. Our algorithm takes inspiration from QITE in that the quantum state follows the same normalised trajectory in both algorithms. However, it is our QITE methodology's ability to track the scale of the state vector over time that allows our algorithm to solve differential equations. We demonstrate our methodology with numerical simulations and use it to solve the heat equation in one and two dimensions using six and ten qubits, respectively.","sentences":["The quantum imaginary time evolution (QITE) methodology was developed to overcome a critical issue as regards non-unitarity in the implementation of imaginary time evolution on a quantum computer.","QITE has since been used to approximate ground states of various physical systems.","In this paper, we demonstrate a practical application of QITE as a quantum numerical solver for linear partial differential equations.","Our algorithm takes inspiration from QITE in that the quantum state follows the same normalised trajectory in both algorithms.","However, it is our QITE methodology's ability to track the scale of the state vector over time that allows our algorithm to solve differential equations.","We demonstrate our methodology with numerical simulations and use it to solve the heat equation in one and two dimensions using six and ten qubits, respectively."],"url":"http://arxiv.org/abs/2405.01313v1","category":"quant-ph"}
{"created":"2024-05-02 14:11:23","title":"Joint Sequential Fronthaul Quantization and Hardware Complexity Reduction in Uplink Cell-Free Massive MIMO Networks","abstract":"Fronthaul quantization causes a significant distortion in cell-free massive MIMO networks. Due to the limited capacity of fronthaul links, information exchange among access points (APs) must be quantized significantly. Furthermore, the complexity of the multiplication operation in the base-band processing unit increases with the number of bits of the operands. Thus, quantizing the APs' signal vector reduces the complexity of signal estimation in the base-band processing unit. Most recent works consider the direct quantization of the received signal vectors at each AP without any pre-processing. However, the signal vectors received at different APs are correlated mutually (inter-AP correlation) and also have correlated dimensions (intra-AP correlation). Hence, cooperative quantization of APs fronthaul can help to efficiently use the quantization bits at each AP and further reduce the distortion imposed on the quantized vector at the APs. This paper considers a daisy chain fronthaul and three different processing sequences at each AP. We show that 1) de-correlating the received signal vector at each AP from the corresponding vectors of the previous APs (inter-AP de-correlation) and 2) de-correlating the dimensions of the received signal vector at each AP (intra-AP de-correlation) before quantization helps to use the quantization bits at each AP more efficiently than directly quantizing the received signal vector without any pre-processing and consequently, improves the bit error rate (BER) and normalized mean square error (NMSE) of users signal estimation.","sentences":["Fronthaul quantization causes a significant distortion in cell-free massive MIMO networks.","Due to the limited capacity of fronthaul links, information exchange among access points (APs) must be quantized significantly.","Furthermore, the complexity of the multiplication operation in the base-band processing unit increases with the number of bits of the operands.","Thus, quantizing the APs' signal vector reduces the complexity of signal estimation in the base-band processing unit.","Most recent works consider the direct quantization of the received signal vectors at each AP without any pre-processing.","However, the signal vectors received at different APs are correlated mutually (inter-AP correlation) and also have correlated dimensions (intra-AP correlation).","Hence, cooperative quantization of APs fronthaul can help to efficiently use the quantization bits at each AP and further reduce the distortion imposed on the quantized vector at the APs.","This paper considers a daisy chain fronthaul and three different processing sequences at each AP.","We show that 1) de-correlating the received signal vector at each AP from the corresponding vectors of the previous APs (inter-AP de-correlation) and 2) de-correlating the dimensions of the received signal vector at each AP (intra-AP de-correlation) before quantization helps to use the quantization bits at each AP more efficiently than directly quantizing the received signal vector without any pre-processing and consequently, improves the bit error rate (BER) and normalized mean square error (NMSE) of users signal estimation."],"url":"http://arxiv.org/abs/2405.01303v1","category":"eess.SP"}
{"created":"2024-05-02 14:02:38","title":"Partially bonded crystals: a pathway to porosity and polymorphism","abstract":"In recent years, experimental and theoretical investigations have shown that anisotropic colloids can self-organise into ordered porous monolayers, where the interplay of localised bonding sites, so called patches, with the particle's shape is responsible for driving the systems away from close-packing and towards porosity. Until now it has been assumed that patchy particles have to be fully bonded with their neighbouring particles for crystals to form, and that, if full bonding cannot be achieved due to the choice of patch placement, disordered assemblies will form instead. In contrast, we show that by deliberately displacing the patches such that full bonding is disfavored, a different route to porous crystalline monolayers emerges, where geometric frustration and partial bonding are pivotal in the structure formation process. The resulting dangling bonds lead to the emergence of effectively chiral units which then act as building blocks for energetically equivalent crystal polymorphs.","sentences":["In recent years, experimental and theoretical investigations have shown that anisotropic colloids can self-organise into ordered porous monolayers, where the interplay of localised bonding sites, so called patches, with the particle's shape is responsible for driving the systems away from close-packing and towards porosity.","Until now it has been assumed that patchy particles have to be fully bonded with their neighbouring particles for crystals to form, and that, if full bonding cannot be achieved due to the choice of patch placement, disordered assemblies will form instead.","In contrast, we show that by deliberately displacing the patches such that full bonding is disfavored, a different route to porous crystalline monolayers emerges, where geometric frustration and partial bonding are pivotal in the structure formation process.","The resulting dangling bonds lead to the emergence of effectively chiral units which then act as building blocks for energetically equivalent crystal polymorphs."],"url":"http://arxiv.org/abs/2405.01300v1","category":"cond-mat.soft"}
{"created":"2024-05-02 13:54:21","title":"Koopman Data-Driven Predictive Control with Robust Stability and Recursive Feasibility Guarantees","abstract":"In this paper, we consider the design of data-driven predictive controllers for nonlinear systems from input-output data via linear-in-control input Koopman lifted models. Instead of identifying and simulating a Koopman model to predict future outputs, we design a subspace predictive controller in the Koopman space. This allows us to learn the observables minimizing the multi-step output prediction error of the Koopman subspace predictor, preventing the propagation of prediction errors. To avoid losing feasibility of our predictive control scheme due to prediction errors, we compute a terminal cost and terminal set in the Koopman space and we obtain recursive feasibility guarantees through an interpolated initial state. As a third contribution, we introduce a novel regularization cost yielding input-to-state stability guarantees with respect to the prediction error for the resulting closed-loop system. The performance of the developed Koopman data-driven predictive control methodology is illustrated on a nonlinear benchmark example from the literature.","sentences":["In this paper, we consider the design of data-driven predictive controllers for nonlinear systems from input-output data via linear-in-control input Koopman lifted models.","Instead of identifying and simulating a Koopman model to predict future outputs, we design a subspace predictive controller in the Koopman space.","This allows us to learn the observables minimizing the multi-step output prediction error of the Koopman subspace predictor, preventing the propagation of prediction errors.","To avoid losing feasibility of our predictive control scheme due to prediction errors, we compute a terminal cost and terminal set in the Koopman space and we obtain recursive feasibility guarantees through an interpolated initial state.","As a third contribution, we introduce a novel regularization cost yielding input-to-state stability guarantees with respect to the prediction error for the resulting closed-loop system.","The performance of the developed Koopman data-driven predictive control methodology is illustrated on a nonlinear benchmark example from the literature."],"url":"http://arxiv.org/abs/2405.01292v1","category":"math.OC"}
{"created":"2024-05-02 13:53:55","title":"On Hodge structures of compact complex manifolds with semistable degenerations","abstract":"Compact K\\\"{a}hler manifolds satisfy several nice Hodge-theoretic properties such as the Hodge symmetry, the Hard Lefschetz property and the Hodge--Riemann bilinear relations, etc. In this note, we investigate when such nice properties hold on compact complex manifolds with semistable degenerations.   For compact complex manifolds which can be obtained as smoothings of SNC varieties without triple intersection locus, we show the Hodge symmetry when the monodromy logarithm induces isomorphisms on the associated graded. We also show the Hodge--Riemann relations on $H^3$ of non-K\\\"{a}hler Calabi--Yau 3-folds with such semistable degenerations.","sentences":["Compact K\\\"{a}hler manifolds satisfy several nice Hodge-theoretic properties such as the Hodge symmetry, the Hard Lefschetz property and the Hodge--Riemann bilinear relations, etc.","In this note, we investigate when such nice properties hold on compact complex manifolds with semistable degenerations.   ","For compact complex manifolds which can be obtained as smoothings of SNC varieties without triple intersection locus, we show the Hodge symmetry when the monodromy logarithm induces isomorphisms on the associated graded.","We also show the Hodge--Riemann relations on $H^3$ of non-K\\\"{a}hler Calabi--Yau 3-folds with such semistable degenerations."],"url":"http://arxiv.org/abs/2405.01291v1","category":"math.AG"}
{"created":"2024-05-02 13:46:08","title":"Novel method for in-situ drift velocity measurement in large volume TPCs: the Geometry Reference Chamber of the NA61/SHINE experiment at CERN","abstract":"This paper presents a novel method for low maintenance, low ambiguity in-situ drift velocity monitoring in large volume Time Projection Chambers (TPCs). The method was developed and deployed for the 40m^3 TPC tracker system of the NA61/SHINE experiment at CERN, which has a one meter of drift length. The method relies on a low-cost multi-wire proportional chamber (MWPC) placed downstream of the TPCs to be monitored. The drift velocity is then determined by matching the reconstructed tracks in the TPC to the hits of the pertinent monitoring chamber, called Geometry Reference Chamber (GRC), which is then used as a differential length scale. An important design requirement on the GRC was minimal added complexity to the existing system, in particular, compatibility with Front-End Electronics (FEE) cards already used to read out the TPCs. Moreover, the GRC system was designed to operate both in large and small particle flux. The system is capable of monitoring the evolution of the in-situ drift velocity down to a one permil precision, with a few minutes of time sampling.","sentences":["This paper presents a novel method for low maintenance, low ambiguity in-situ drift velocity monitoring in large volume Time Projection Chambers (TPCs).","The method was developed and deployed for the 40m^3 TPC tracker system of the NA61/SHINE experiment at CERN, which has a one meter of drift length.","The method relies on a low-cost multi-wire proportional chamber (MWPC) placed downstream of the TPCs to be monitored.","The drift velocity is then determined by matching the reconstructed tracks in the TPC to the hits of the pertinent monitoring chamber, called Geometry Reference Chamber (GRC), which is then used as a differential length scale.","An important design requirement on the GRC was minimal added complexity to the existing system, in particular, compatibility with Front-End Electronics (FEE) cards already used to read out the TPCs.","Moreover, the GRC system was designed to operate both in large and small particle flux.","The system is capable of monitoring the evolution of the in-situ drift velocity down to a one permil precision, with a few minutes of time sampling."],"url":"http://arxiv.org/abs/2405.01285v1","category":"physics.ins-det"}
{"created":"2024-05-02 13:42:20","title":"Fractional Bloom boundedness of commutators in spaces of homogeneous type","abstract":"We aim to characterise boundedness of commutators $[b,T]$ of singular integrals $T$. Boundedness is studied between weighted Lebesgue spaces $L^p(X)$ and $L^q(X)$, $p\\leq q$, when the underlying space $X$ is a space of homogeneous type. Commutator theory in spaces of homogeneous type already exist in literature, in particular boundedness results in the setting $p=q$. The purpose here is to extend the earlier results to the setting of $p< q$. Our methods extend those of Duong et al. and Hyt\\\"onen et al. A novelty here is that in order to show the lower bound of the commutator norm, we demonstrate that the approximate weak factorisation of Hyt\\\"onen can be used when the underlying setting is a space of homogeneous type and not only in the Euclidean setting. The strength of the approximate weak factorisation is that (when compared to the so-called median method) it readily allows complex-valued $b$ in addition to real-valued ones. However, the median method has been previously successfully applied to iterated commutators and thus has its own strengths. We also present a proof based on that method.","sentences":["We aim to characterise boundedness of commutators $[b,T]$ of singular integrals $T$. Boundedness is studied between weighted Lebesgue spaces $L^p(X)$ and $L^q(X)$, $p\\leq q$, when the underlying space $X$ is a space of homogeneous type.","Commutator theory in spaces of homogeneous type already exist in literature, in particular boundedness results in the setting $p=q$. The purpose here is to extend the earlier results to the setting of $p< q$. Our methods extend those of Duong et al. and Hyt\\\"onen et al.","A novelty here is that in order to show the lower bound of the commutator norm, we demonstrate that the approximate weak factorisation of Hyt\\\"onen can be used when the underlying setting is a space of homogeneous type and not only in the Euclidean setting.","The strength of the approximate weak factorisation is that (when compared to the so-called median method) it readily allows complex-valued $b$ in addition to real-valued ones.","However, the median method has been previously successfully applied to iterated commutators and thus has its own strengths.","We also present a proof based on that method."],"url":"http://arxiv.org/abs/2405.01283v1","category":"math.CA"}
{"created":"2024-05-02 13:30:58","title":"The role of the Allee effect in common-pool resource and its sustainability","abstract":"The management of common-pool resources is a complex challenge due to the risk of overexploitation and the tragedy of the commons. A novel framework has been introduced to address this issue, focusing on the coevolutionary relationship between human behavior and common-pool resources within a human-environment system. However, the impact of the Allee effect on the coevolution and its resource sustainability is still unexplored. The Allee effect, a biological phenomenon characterized by a correlation between resource availability and growth rate, is a fundamental attribute of numerous natural resources. In this paper, we introduce two coevolutionary models of resource and strategy under replicator dynamics and knowledge feedback by applying the Allee effect to the common-pool resources within human-environment system. These models encapsulate various facets of resource dynamics and the players' behavior, such as resource growth function, the extraction rates, and the strategy update rules. We find that the Allee effect can induce bi-stability and critical transition, leading to either sustainable or unsustainable outcomes depending on the initial condition and parameter configuration. We demonstrate that knowledge feedback enhances the resilience and sustainability of the coevolving system, and these results advances the understanding of human-environment system and management of common-pool resources.","sentences":["The management of common-pool resources is a complex challenge due to the risk of overexploitation and the tragedy of the commons.","A novel framework has been introduced to address this issue, focusing on the coevolutionary relationship between human behavior and common-pool resources within a human-environment system.","However, the impact of the Allee effect on the coevolution and its resource sustainability is still unexplored.","The Allee effect, a biological phenomenon characterized by a correlation between resource availability and growth rate, is a fundamental attribute of numerous natural resources.","In this paper, we introduce two coevolutionary models of resource and strategy under replicator dynamics and knowledge feedback by applying the Allee effect to the common-pool resources within human-environment system.","These models encapsulate various facets of resource dynamics and the players' behavior, such as resource growth function, the extraction rates, and the strategy update rules.","We find that the Allee effect can induce bi-stability and critical transition, leading to either sustainable or unsustainable outcomes depending on the initial condition and parameter configuration.","We demonstrate that knowledge feedback enhances the resilience and sustainability of the coevolving system, and these results advances the understanding of human-environment system and management of common-pool resources."],"url":"http://arxiv.org/abs/2405.01271v1","category":"econ.TH"}
{"created":"2024-05-02 13:21:37","title":"Towards Optimising EEG Decoding using Post-hoc Explanations and Domain Knowledge","abstract":"Decoding EEG during motor imagery is pivotal for the Brain-Computer Interface (BCI) system, influencing its overall performance significantly. As end-to-end data-driven learning methods advance, the challenge lies in balancing model complexity with the need for human interpretability and trust. Despite strides in EEG-based BCIs, challenges like artefacts and low signal-to-noise ratio emphasise the ongoing importance of model transparency. This work proposes using post-hoc explanations to interpret model outcomes and validate them against domain knowledge. Leveraging the GradCAM post-hoc explanation technique on the motor imagery dataset, this work demonstrates that relying solely on accuracy metrics may be inadequate to ensure BCI performance and acceptability. A model trained using all EEG channels of the dataset achieves 72.60% accuracy, while a model trained with motor-imagery/movement-relevant channel data has a statistically insignificant decrease of 1.75%. However, the relevant features for both are very different based on neurophysiological facts. This work demonstrates that integrating domain-specific knowledge with XAI techniques emerges as a promising paradigm for validating the neurophysiological basis of model outcomes in BCIs. Our results reveal the significance of neurophysiological validation in evaluating BCI performance, highlighting the potential risks of exclusively relying on performance metrics when selecting models for dependable and transparent BCIs.","sentences":["Decoding EEG during motor imagery is pivotal for the Brain-Computer Interface (BCI) system, influencing its overall performance significantly.","As end-to-end data-driven learning methods advance, the challenge lies in balancing model complexity with the need for human interpretability and trust.","Despite strides in EEG-based BCIs, challenges like artefacts and low signal-to-noise ratio emphasise the ongoing importance of model transparency.","This work proposes using post-hoc explanations to interpret model outcomes and validate them against domain knowledge.","Leveraging the GradCAM post-hoc explanation technique on the motor imagery dataset, this work demonstrates that relying solely on accuracy metrics may be inadequate to ensure BCI performance and acceptability.","A model trained using all EEG channels of the dataset achieves 72.60% accuracy, while a model trained with motor-imagery/movement-relevant channel data has a statistically insignificant decrease of 1.75%.","However, the relevant features for both are very different based on neurophysiological facts.","This work demonstrates that integrating domain-specific knowledge with XAI techniques emerges as a promising paradigm for validating the neurophysiological basis of model outcomes in BCIs.","Our results reveal the significance of neurophysiological validation in evaluating BCI performance, highlighting the potential risks of exclusively relying on performance metrics when selecting models for dependable and transparent BCIs."],"url":"http://arxiv.org/abs/2405.01269v1","category":"cs.HC"}
{"created":"2024-05-02 13:13:45","title":"Derivation of Dirac Exchange Interaction Potential from Quantum Plasma Kinetic Theory","abstract":"The Dirac exchange interaction is derived from recent quantum kinetic theory for collisionless plasmas. For this purpose, the kinetic equation is written in the semiclassical and long wavelength approximations. The validity of the model for real systems is worked out, in terms of temperature and density parameters. Within the region of applicability, the correlation potential energy is shown to be always smaller than the exchange contribution. From the moments of the quantum kinetic equations, macroscopic, hydrodynamic equations are found, for an electron-ion plasma. The Dirac exchange term is explicitly derived, in the case of a completely degenerate electron gas. These results show, within quantum kinetic theory for charged particle systems, a new view of the Dirac exchange interaction frequently used in density functional theory parametrization. Finally, a simpler form of the quantum plasma exchange kinetic theory is also found.","sentences":["The Dirac exchange interaction is derived from recent quantum kinetic theory for collisionless plasmas.","For this purpose, the kinetic equation is written in the semiclassical and long wavelength approximations.","The validity of the model for real systems is worked out, in terms of temperature and density parameters.","Within the region of applicability, the correlation potential energy is shown to be always smaller than the exchange contribution.","From the moments of the quantum kinetic equations, macroscopic, hydrodynamic equations are found, for an electron-ion plasma.","The Dirac exchange term is explicitly derived, in the case of a completely degenerate electron gas.","These results show, within quantum kinetic theory for charged particle systems, a new view of the Dirac exchange interaction frequently used in density functional theory parametrization.","Finally, a simpler form of the quantum plasma exchange kinetic theory is also found."],"url":"http://arxiv.org/abs/2405.01265v1","category":"physics.plasm-ph"}
{"created":"2024-05-02 13:13:35","title":"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch Vehicles","abstract":"This paper introduces a landing guidance strategy for reusable launch vehicles (RLVs) using a model predictive approach based on sequential convex programming (SCP). The proposed approach devises two distinct optimal control problems (OCPs): planning a fuel-optimal landing trajectory that accommodates practical path constraints specific to RLVs, and determining real-time optimal tracking commands. This dual optimization strategy allows for reduced computational load through adjustable prediction horizon lengths in the tracking task, achieving near closed-loop performance. Enhancements in model fidelity for the tracking task are achieved through an alternative rotational dynamics representation, enabling a more stable numerical solution of the OCP and accounting for vehicle transient dynamics. Furthermore, modifications of aerodynamic force in both planning and tracking phases are proposed, tailored for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding computational complexity. Extensive 6-DOF simulation experiments validate the effectiveness and improved guidance performance of the proposed algorithm.","sentences":["This paper introduces a landing guidance strategy for reusable launch vehicles (RLVs) using a model predictive approach based on sequential convex programming (SCP).","The proposed approach devises two distinct optimal control problems (OCPs): planning a fuel-optimal landing trajectory that accommodates practical path constraints specific to RLVs, and determining real-time optimal tracking commands.","This dual optimization strategy allows for reduced computational load through adjustable prediction horizon lengths in the tracking task, achieving near closed-loop performance.","Enhancements in model fidelity for the tracking task are achieved through an alternative rotational dynamics representation, enabling a more stable numerical solution of the OCP and accounting for vehicle transient dynamics.","Furthermore, modifications of aerodynamic force in both planning and tracking phases are proposed, tailored for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding computational complexity.","Extensive 6-DOF simulation experiments validate the effectiveness and improved guidance performance of the proposed algorithm."],"url":"http://arxiv.org/abs/2405.01264v1","category":"eess.SY"}
{"created":"2024-05-02 13:11:53","title":"An Online Gradient-Based Caching Policy with Logarithmic Complexity and Regret Guarantees","abstract":"The commonly used caching policies, such as LRU or LFU, exhibit optimal performance only for specific traffic patterns. Even advanced Machine Learning-based methods, which detect patterns in historical request data, struggle when future requests deviate from past trends. Recently, a new class of policies has emerged that makes no assumptions about the request arrival process. These algorithms solve an online optimization problem, enabling continuous adaptation to the context. They offer theoretical guarantees on the regret metric, which is the gap between the gain of the online policy and the gain of the optimal static cache allocation in hindsight. Nevertheless, the high computational complexity of these solutions hinders their practical adoption. In this study, we introduce a groundbreaking gradient-based online caching policy, the first to achieve logarithmic computational complexity relative to catalog size along with regret guarantees. This means our algorithm can efficiently handle large-scale data while minimizing the performance gap between real-time decisions and optimal hindsight choices. As requests arrive, our policy dynamically adjusts the probabilities of including items in the cache, which drive cache update decisions. Our algorithm's streamlined complexity is a key advantage, enabling its application to real-world traces featuring millions of requests and items. This is a significant achievement, as traces of this scale have been out of reach for existing policies with regret guarantees. To the best of our knowledge, our experimental results show for the first time that the regret guarantees of gradient-based caching policies bring significant benefits in scenarios of practical interest.","sentences":["The commonly used caching policies, such as LRU or LFU, exhibit optimal performance only for specific traffic patterns.","Even advanced Machine Learning-based methods, which detect patterns in historical request data, struggle when future requests deviate from past trends.","Recently, a new class of policies has emerged that makes no assumptions about the request arrival process.","These algorithms solve an online optimization problem, enabling continuous adaptation to the context.","They offer theoretical guarantees on the regret metric, which is the gap between the gain of the online policy and the gain of the optimal static cache allocation in hindsight.","Nevertheless, the high computational complexity of these solutions hinders their practical adoption.","In this study, we introduce a groundbreaking gradient-based online caching policy, the first to achieve logarithmic computational complexity relative to catalog size along with regret guarantees.","This means our algorithm can efficiently handle large-scale data while minimizing the performance gap between real-time decisions and optimal hindsight choices.","As requests arrive, our policy dynamically adjusts the probabilities of including items in the cache, which drive cache update decisions.","Our algorithm's streamlined complexity is a key advantage, enabling its application to real-world traces featuring millions of requests and items.","This is a significant achievement, as traces of this scale have been out of reach for existing policies with regret guarantees.","To the best of our knowledge, our experimental results show for the first time that the regret guarantees of gradient-based caching policies bring significant benefits in scenarios of practical interest."],"url":"http://arxiv.org/abs/2405.01263v1","category":"cs.LG"}
{"created":"2024-05-02 13:07:56","title":"Continuously evolving rewards in an open-ended environment","abstract":"Unambiguous identification of the rewards driving behaviours of entities operating in complex open-ended real-world environments is difficult, partly because goals and associated behaviours emerge endogenously and are dynamically updated as environments change. Reproducing such dynamics in models would be useful in many domains, particularly where fixed reward functions limit the adaptive capabilities of agents. Simulation experiments described assess a candidate algorithm for the dynamic updating of rewards, RULE: Reward Updating through Learning and Expectation. The approach is tested in a simplified ecosystem-like setting where experiments challenge entities' survival, calling for significant behavioural change. The population of entities successfully demonstrate the abandonment of an initially rewarded but ultimately detrimental behaviour, amplification of beneficial behaviour, and appropriate responses to novel items added to their environment. These adjustment happen through endogenous modification of the entities' underlying reward function, during continuous learning, without external intervention.","sentences":["Unambiguous identification of the rewards driving behaviours of entities operating in complex open-ended real-world environments is difficult, partly because goals and associated behaviours emerge endogenously and are dynamically updated as environments change.","Reproducing such dynamics in models would be useful in many domains, particularly where fixed reward functions limit the adaptive capabilities of agents.","Simulation experiments described assess a candidate algorithm for the dynamic updating of rewards, RULE:","Reward Updating through Learning and Expectation.","The approach is tested in a simplified ecosystem-like setting where experiments challenge entities' survival, calling for significant behavioural change.","The population of entities successfully demonstrate the abandonment of an initially rewarded but ultimately detrimental behaviour, amplification of beneficial behaviour, and appropriate responses to novel items added to their environment.","These adjustment happen through endogenous modification of the entities' underlying reward function, during continuous learning, without external intervention."],"url":"http://arxiv.org/abs/2405.01261v1","category":"cs.LG"}
{"created":"2024-05-02 13:06:50","title":"Causal Influence in Federated Edge Inference","abstract":"In this paper, we consider a setting where heterogeneous agents with connectivity are performing inference using unlabeled streaming data. Observed data are only partially informative about the target variable of interest. In order to overcome the uncertainty, agents cooperate with each other by exchanging their local inferences with and through a fusion center. To evaluate how each agent influences the overall decision, we adopt a causal framework in order to distinguish the actual influence of agents from mere correlations within the decision-making process. Various scenarios reflecting different agent participation patterns and fusion center policies are investigated. We derive expressions to quantify the causal impact of each agent on the joint decision, which could be beneficial for anticipating and addressing atypical scenarios, such as adversarial attacks or system malfunctions. We validate our theoretical results with numerical simulations and a real-world application of multi-camera crowd counting.","sentences":["In this paper, we consider a setting where heterogeneous agents with connectivity are performing inference using unlabeled streaming data.","Observed data are only partially informative about the target variable of interest.","In order to overcome the uncertainty, agents cooperate with each other by exchanging their local inferences with and through a fusion center.","To evaluate how each agent influences the overall decision, we adopt a causal framework in order to distinguish the actual influence of agents from mere correlations within the decision-making process.","Various scenarios reflecting different agent participation patterns and fusion center policies are investigated.","We derive expressions to quantify the causal impact of each agent on the joint decision, which could be beneficial for anticipating and addressing atypical scenarios, such as adversarial attacks or system malfunctions.","We validate our theoretical results with numerical simulations and a real-world application of multi-camera crowd counting."],"url":"http://arxiv.org/abs/2405.01260v1","category":"cs.LG"}
{"created":"2024-05-02 12:54:25","title":"Global existence and blow-up for the Euler-Poincar\u00e9 equations with a class of initial data","abstract":"In this paper we investigate the Cauchy problem of d-dimensional Euler-Poincar\\'{e} equations. By choosing a class of new and special initial data, we can transform this d-dimensional Euler-Poincar\\'{e} equations into the Camassa-Holm type equation in the real line. We first obtain some global existence results and then present a new blow-up result to the system under some different assumptions on this special class of initial data.","sentences":["In this paper we investigate the Cauchy problem of d-dimensional Euler-Poincar\\'{e} equations.","By choosing a class of new and special initial data, we can transform this d-dimensional Euler-Poincar\\'{e} equations into the Camassa-Holm type equation in the real line.","We first obtain some global existence results and then present a new blow-up result to the system under some different assumptions on this special class of initial data."],"url":"http://arxiv.org/abs/2405.01252v1","category":"math.AP"}
{"created":"2024-05-02 12:51:01","title":"Lying Graph Convolution: Learning to Lie for Node Classification Tasks","abstract":"In the context of machine learning for graphs, many researchers have empirically observed that Deep Graph Networks (DGNs) perform favourably on node classification tasks when the graph structure is homophilic (\\ie adjacent nodes are similar). In this paper, we introduce Lying-GCN, a new DGN inspired by opinion dynamics that can adaptively work in both the heterophilic and the homophilic setting. At each layer, each agent (node) shares its own opinions (node embeddings) with its neighbours. Instead of sharing its opinion directly as in GCN, we introduce a mechanism which allows agents to lie. Such a mechanism is adaptive, thus the agents learn how and when to lie according to the task that should be solved. We provide a characterisation of our proposal in terms of dynamical systems, by studying the spectral property of the coefficient matrix of the system. While the steady state of the system collapses to zero, we believe the lying mechanism is still usable to solve node classification tasks. We empirically prove our belief on both synthetic and real-world datasets, by showing that the lying mechanism allows to increase the performances in the heterophilic setting without harming the results in the homophilic one.","sentences":["In the context of machine learning for graphs, many researchers have empirically observed that Deep Graph Networks (DGNs) perform favourably on node classification tasks when the graph structure is homophilic (\\ie adjacent nodes are similar).","In this paper, we introduce Lying-GCN, a new DGN inspired by opinion dynamics that can adaptively work in both the heterophilic and the homophilic setting.","At each layer, each agent (node) shares its own opinions (node embeddings) with its neighbours.","Instead of sharing its opinion directly as in GCN, we introduce a mechanism which allows agents to lie.","Such a mechanism is adaptive, thus the agents learn how and when to lie according to the task that should be solved.","We provide a characterisation of our proposal in terms of dynamical systems, by studying the spectral property of the coefficient matrix of the system.","While the steady state of the system collapses to zero, we believe the lying mechanism is still usable to solve node classification tasks.","We empirically prove our belief on both synthetic and real-world datasets, by showing that the lying mechanism allows to increase the performances in the heterophilic setting without harming the results in the homophilic one."],"url":"http://arxiv.org/abs/2405.01247v1","category":"cs.LG"}
{"created":"2024-05-02 12:22:06","title":"Modeling the Trade-off between Throughput and Reliability in a Bluetooth Low Energy Connection","abstract":"The use of Bluetooth Low Energy in low-range Internet of Things systems is growing exponentially. Similar to other wireless communication protocols, throughput and reliability are two key performance metrics in Bluetooth Low Energy communications. However, electromagnetic interference from various sources can heavily affect the performance of wireless devices, leading to dropped throughput and unreliable communication. Therefore, there is a need for both theoretical and practical studies capable of quantifying the BLE communication performance, e.g. throughput and reliability, subject to interference. In this paper, a mathematical model to predict throughput of a BLE connection under interference is derived first, and linked to the reliability model we developed in [1]. After that, extensive practical experiments are performed in various scenarios to sufficiently validate the theoretical results from both models. Finally, the trade-off between throughput and reliability is investigated through the validated models to give some inside properties of BLE communications. The similarity between the theoretical results and the experimental ones highlights the accuracy of the proposed throughput and reliability models. Hence, the two models can be used to explore the performance of various BLE designs or deployments from diverse perspectives.","sentences":["The use of Bluetooth Low Energy in low-range Internet of Things systems is growing exponentially.","Similar to other wireless communication protocols, throughput and reliability are two key performance metrics in Bluetooth Low Energy communications.","However, electromagnetic interference from various sources can heavily affect the performance of wireless devices, leading to dropped throughput and unreliable communication.","Therefore, there is a need for both theoretical and practical studies capable of quantifying the BLE communication performance, e.g. throughput and reliability, subject to interference.","In this paper, a mathematical model to predict throughput of a BLE connection under interference is derived first, and linked to the reliability model we developed in [1].","After that, extensive practical experiments are performed in various scenarios to sufficiently validate the theoretical results from both models.","Finally, the trade-off between throughput and reliability is investigated through the validated models to give some inside properties of BLE communications.","The similarity between the theoretical results and the experimental ones highlights the accuracy of the proposed throughput and reliability models.","Hence, the two models can be used to explore the performance of various BLE designs or deployments from diverse perspectives."],"url":"http://arxiv.org/abs/2405.01231v1","category":"cs.NI"}
{"created":"2024-05-02 12:21:51","title":"Evaluation of Video-Based rPPG in Challenging Environments: Artifact Mitigation and Network Resilience","abstract":"Video-based remote photoplethysmography (rPPG) has emerged as a promising technology for non-contact vital sign monitoring, especially under controlled conditions. However, the accurate measurement of vital signs in real-world scenarios faces several challenges, including artifacts induced by videocodecs, low-light noise, degradation, low dynamic range, occlusions, and hardware and network constraints. In this article, we systematically investigate comprehensive investigate these issues, measuring their detrimental effects on the quality of rPPG measurements. Additionally, we propose practical strategies for mitigating these challenges to improve the dependability and resilience of video-based rPPG systems. We detail methods for effective biosignal recovery in the presence of network limitations and present denoising and inpainting techniques aimed at preserving video frame integrity. Through extensive evaluations and direct comparisons, we demonstrate the effectiveness of the approaches in enhancing rPPG measurements under challenging environments, contributing to the development of more reliable and effective remote vital sign monitoring technologies.","sentences":["Video-based remote photoplethysmography (rPPG) has emerged as a promising technology for non-contact vital sign monitoring, especially under controlled conditions.","However, the accurate measurement of vital signs in real-world scenarios faces several challenges, including artifacts induced by videocodecs, low-light noise, degradation, low dynamic range, occlusions, and hardware and network constraints.","In this article, we systematically investigate comprehensive investigate these issues, measuring their detrimental effects on the quality of rPPG measurements.","Additionally, we propose practical strategies for mitigating these challenges to improve the dependability and resilience of video-based rPPG systems.","We detail methods for effective biosignal recovery in the presence of network limitations and present denoising and inpainting techniques aimed at preserving video frame integrity.","Through extensive evaluations and direct comparisons, we demonstrate the effectiveness of the approaches in enhancing rPPG measurements under challenging environments, contributing to the development of more reliable and effective remote vital sign monitoring technologies."],"url":"http://arxiv.org/abs/2405.01230v1","category":"cs.CV"}
{"created":"2024-05-02 11:58:43","title":"Attention and Sensory Processing in Augmented Reality: Empowering ADHD population","abstract":"The brain's attention system is a complex and adaptive network of brain regions that enables individuals to interact effectively with their surroundings and perform complex tasks. This system involves the coordination of various brain regions, including the prefrontal cortex and the parietal lobes, to process and prioritize sensory information, manage tasks, and maintain focus. In this study, we investigate the intricate mechanisms underpinning the brain's attention system, followed by an exploration within the context of augmented reality (AR) settings. AR emerges as a viable technological intervention to address the multifaceted challenges faced by individuals with Attention Deficit Hyperactivity Disorder (ADHD). Given that the primary characteristics of ADHD include difficulties related to inattention, hyperactivity, and impulsivity, AR offers tailor-made solutions specifically designed to mitigate these challenges and enhance cognitive functioning. On the other hand, if these ADHD-related issues are not adequately addressed, it could lead to a worsening of their condition in AR. This underscores the importance of employing effective interventions such as AR to support individuals with ADHD in managing their symptoms. We examine the attentional mechanisms within AR environments and the sensory processing dynamics prevalent among the ADHD population. Our objective is to comprehensively address the attentional needs of this population in AR settings and offer a framework for designing cognitively accessible AR applications.","sentences":["The brain's attention system is a complex and adaptive network of brain regions that enables individuals to interact effectively with their surroundings and perform complex tasks.","This system involves the coordination of various brain regions, including the prefrontal cortex and the parietal lobes, to process and prioritize sensory information, manage tasks, and maintain focus.","In this study, we investigate the intricate mechanisms underpinning the brain's attention system, followed by an exploration within the context of augmented reality (AR) settings.","AR emerges as a viable technological intervention to address the multifaceted challenges faced by individuals with Attention Deficit Hyperactivity Disorder (ADHD).","Given that the primary characteristics of ADHD include difficulties related to inattention, hyperactivity, and impulsivity, AR offers tailor-made solutions specifically designed to mitigate these challenges and enhance cognitive functioning.","On the other hand, if these ADHD-related issues are not adequately addressed, it could lead to a worsening of their condition in AR.","This underscores the importance of employing effective interventions such as AR to support individuals with ADHD in managing their symptoms.","We examine the attentional mechanisms within AR environments and the sensory processing dynamics prevalent among the ADHD population.","Our objective is to comprehensively address the attentional needs of this population in AR settings and offer a framework for designing cognitively accessible AR applications."],"url":"http://arxiv.org/abs/2405.01218v1","category":"cs.HC"}
{"created":"2024-05-02 11:54:41","title":"Movable Antenna Enhanced Wireless Sensing Via Antenna Position Optimization","abstract":"In this paper, we propose a new wireless sensing system equipped with the movable-antenna (MA) array, which can flexibly adjust the positions of antenna elements for improving the sensing performance over conventional antenna arrays with fixed-position antennas (FPAs). First, we show that the angle estimation performance in wireless sensing is fundamentally determined by the array geometry, where the Cramer-Rao bound (CRB) of the mean square error (MSE) for angle of arrival (AoA) estimation is derived as a function of the antennas' positions for both one-dimensional (1D) and two-dimensional (2D) MA arrays. Then, for the case of 1D MA array, we obtain a globally optimal solution for the MAs' positions in closed form to minimize the CRB of AoA estimation MSE. While in the case of 2D MA array, we aim to achieve the minimum of maximum (min-max) CRBs of estimation MSE for the two AoAs with respect to the horizontal and vertical axes, respectively. In particular, for the special case of circular antenna movement region, an optimal solution for the MAs' positions is derived under certain numbers of MAs and circle radii. Thereby, both the lower- and upper-bounds of the min-max CRB are obtained for the antenna movement region with arbitrary shapes. Moreover, we develop an efficient alternating optimization algorithm to obtain a locally optimal solution for MAs' positions by iteratively optimizing one between their horizontal and vertical coordinates with the other being fixed. Numerical results demonstrate that our proposed 1D/2D MA arrays can significantly decrease the CRB of AoA estimation MSE as well as the actual MSE compared to conventional uniform linear arrays (ULAs)/uniform planar arrays (UPAs) with different values of uniform inter-antenna spacing.","sentences":["In this paper, we propose a new wireless sensing system equipped with the movable-antenna (MA) array, which can flexibly adjust the positions of antenna elements for improving the sensing performance over conventional antenna arrays with fixed-position antennas (FPAs).","First, we show that the angle estimation performance in wireless sensing is fundamentally determined by the array geometry, where the Cramer-Rao bound (CRB) of the mean square error (MSE) for angle of arrival (AoA) estimation is derived as a function of the antennas' positions for both one-dimensional (1D) and two-dimensional (2D) MA arrays.","Then, for the case of 1D MA array, we obtain a globally optimal solution for the MAs' positions in closed form to minimize the CRB of AoA estimation MSE.","While in the case of 2D MA array, we aim to achieve the minimum of maximum (min-max) CRBs of estimation MSE for the two AoAs with respect to the horizontal and vertical axes, respectively.","In particular, for the special case of circular antenna movement region, an optimal solution for the MAs' positions is derived under certain numbers of MAs and circle radii.","Thereby, both the lower- and upper-bounds of the min-max CRB are obtained for the antenna movement region with arbitrary shapes.","Moreover, we develop an efficient alternating optimization algorithm to obtain a locally optimal solution for MAs' positions by iteratively optimizing one between their horizontal and vertical coordinates with the other being fixed.","Numerical results demonstrate that our proposed 1D/2D MA arrays can significantly decrease the CRB of AoA estimation MSE as well as the actual MSE compared to conventional uniform linear arrays (ULAs)/uniform planar arrays (UPAs) with different values of uniform inter-antenna spacing."],"url":"http://arxiv.org/abs/2405.01215v1","category":"cs.IT"}
{"created":"2024-05-02 11:52:01","title":"$Q$-Boson model and relations with integrable hierarchies","abstract":"This work investigates the intricate relationship between the q-boson model, a quantum integrable system, and classical integrable systems such as the Toda and KP hierarchies. Initially, we analyze scalar products of off-shell Bethe states and explore their connections to tau functions of integrable hierarchies. Furthermore, we discuss correlation functions within this formalism, examining their representations in terms of tau functions, as well as their Schur polynomial expansions.","sentences":["This work investigates the intricate relationship between the q-boson model, a quantum integrable system, and classical integrable systems such as the Toda and KP hierarchies.","Initially, we analyze scalar products of off-shell Bethe states and explore their connections to tau functions of integrable hierarchies.","Furthermore, we discuss correlation functions within this formalism, examining their representations in terms of tau functions, as well as their Schur polynomial expansions."],"url":"http://arxiv.org/abs/2405.01213v1","category":"math-ph"}
{"created":"2024-05-02 11:48:30","title":"Improving Membership Inference in ASR Model Auditing with Perturbed Loss Features","abstract":"Membership Inference (MI) poses a substantial privacy threat to the training data of Automatic Speech Recognition (ASR) systems, while also offering an opportunity to audit these models with regard to user data. This paper explores the effectiveness of loss-based features in combination with Gaussian and adversarial perturbations to perform MI in ASR models. To the best of our knowledge, this approach has not yet been investigated. We compare our proposed features with commonly used error-based features and find that the proposed features greatly enhance performance for sample-level MI. For speaker-level MI, these features improve results, though by a smaller margin, as error-based features already obtained a high performance for this task. Our findings emphasise the importance of considering different feature sets and levels of access to target models for effective MI in ASR systems, providing valuable insights for auditing such models.","sentences":["Membership Inference (MI) poses a substantial privacy threat to the training data of Automatic Speech Recognition (ASR) systems, while also offering an opportunity to audit these models with regard to user data.","This paper explores the effectiveness of loss-based features in combination with Gaussian and adversarial perturbations to perform MI in ASR models.","To the best of our knowledge, this approach has not yet been investigated.","We compare our proposed features with commonly used error-based features and find that the proposed features greatly enhance performance for sample-level MI.","For speaker-level MI, these features improve results, though by a smaller margin, as error-based features already obtained a high performance for this task.","Our findings emphasise the importance of considering different feature sets and levels of access to target models for effective MI in ASR systems, providing valuable insights for auditing such models."],"url":"http://arxiv.org/abs/2405.01207v1","category":"cs.LG"}
{"created":"2024-05-02 11:43:04","title":"Learning-to-solve unit commitment based on few-shot physics-guided spatial-temporal graph convolution network","abstract":"This letter proposes a few-shot physics-guided spatial temporal graph convolutional network (FPG-STGCN) to fast solve unit commitment (UC). Firstly, STGCN is tailored to parameterize UC. Then, few-shot physics-guided learning scheme is proposed. It exploits few typical UC solutions yielded via commercial optimizer to escape from local minimum, and leverages the augmented Lagrangian method for constraint satisfaction. To further enable both feasibility and continuous relaxation for integers in learning process, straight-through estimator for Tanh-Sign composition is proposed to fully differentiate the mixed integer solution space. Case study on the IEEE benchmark justifies that, our method bests mainstream learning ways on UC feasibility, and surpasses traditional solver on efficiency.","sentences":["This letter proposes a few-shot physics-guided spatial temporal graph convolutional network (FPG-STGCN) to fast solve unit commitment (UC).","Firstly, STGCN is tailored to parameterize UC.","Then, few-shot physics-guided learning scheme is proposed.","It exploits few typical UC solutions yielded via commercial optimizer to escape from local minimum, and leverages the augmented Lagrangian method for constraint satisfaction.","To further enable both feasibility and continuous relaxation for integers in learning process, straight-through estimator for Tanh-Sign composition is proposed to fully differentiate the mixed integer solution space.","Case study on the IEEE benchmark justifies that, our method bests mainstream learning ways on UC feasibility, and surpasses traditional solver on efficiency."],"url":"http://arxiv.org/abs/2405.01200v1","category":"eess.SY"}
{"created":"2024-05-02 11:37:36","title":"Optimal Beamforming for Bistatic MIMO Sensing","abstract":"This paper considers the beamforming optimization for sensing a point-like scatterer using a bistatic multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) radar, which could be part of a joint communication and sensing system. The goal is to minimize the Cram\\'er-Rao bound on the target position's estimation error, where the radar already knows an approximate position that is taken into account in the optimization. The optimization allows for beamforming with more than one beam per subcarrier. Optimal solutions for the beamforming are discussed for known and unknown channel gain. Numerical results show that beamforming with at most one beam per subcarrier is optimal for certain parameters, but for other parameters, optimal solutions need two beams on some subcarriers. In addition, the degree of freedom in selecting which end of the bistatic radar should transmit and receive is considered.","sentences":["This paper considers the beamforming optimization for sensing a point-like scatterer using a bistatic multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) radar, which could be part of a joint communication and sensing system.","The goal is to minimize the Cram\\'er-Rao bound on the target position's estimation error, where the radar already knows an approximate position that is taken into account in the optimization.","The optimization allows for beamforming with more than one beam per subcarrier.","Optimal solutions for the beamforming are discussed for known and unknown channel gain.","Numerical results show that beamforming with at most one beam per subcarrier is optimal for certain parameters, but for other parameters, optimal solutions need two beams on some subcarriers.","In addition, the degree of freedom in selecting which end of the bistatic radar should transmit and receive is considered."],"url":"http://arxiv.org/abs/2405.01197v1","category":"eess.SP"}
{"created":"2024-05-02 11:34:58","title":"The Lp Polar bodies of shadow system and related inequalities","abstract":"The $L_p$ versions of the support function and polar body are introduced by Berndtsson, Mastrantonis and Rubinstein in \\cite{Berndtsson-Mastrantonis-Rubinstein-2023} recently. In this paper, we prove that the $L_p$-support function of the shadow system $K_t$ introduced by Rogers and Shephard in \\cite{rogers-1958-02,shephard-1964} is convex and the volume of the section of $L_p$ polar bodies of $K_t$ is $\\frac{1}{n}$-concave with respect to parameter $t$, and obtain some related inequalities. Finally, we present the reverse Rogers-Shephard type inequality for $L_p$-polar bodies.","sentences":["The $L_p$ versions of the support function and polar body are introduced by Berndtsson, Mastrantonis and Rubinstein in \\cite{Berndtsson-Mastrantonis-Rubinstein-2023} recently.","In this paper, we prove that the $L_p$-support function of the shadow system $K_t$ introduced by Rogers and Shephard in \\cite{rogers-1958-02,shephard-1964} is convex and the volume of the section of $L_p$ polar bodies of $K_t$ is $\\frac{1}{n}$-concave with respect to parameter $t$, and obtain some related inequalities.","Finally, we present the reverse Rogers-Shephard type inequality for $L_p$-polar bodies."],"url":"http://arxiv.org/abs/2405.01194v1","category":"math.FA"}
{"created":"2024-05-02 11:34:15","title":"On Hyperbolicity of Spirallike Circularlike domain","abstract":"In this paper, we prove that a spirallike circularlike domain is Kobayashi hyperbolic if and only if its core is empty. In particular, we show that such a domain is Kobayashi hyperbolic if and only if it is (biholomorphic to) a bounded domain. We also propose a problem in this area.","sentences":["In this paper, we prove that a spirallike circularlike domain is Kobayashi hyperbolic if and only if its core is empty.","In particular, we show that such a domain is Kobayashi hyperbolic if and only if it is (biholomorphic to) a bounded domain.","We also propose a problem in this area."],"url":"http://arxiv.org/abs/2405.01193v1","category":"math.CV"}
{"created":"2024-05-02 11:08:33","title":"Multiplicatively Ordered and Directed Hybrid Jordan-Lie Superalgebra","abstract":"A new algebra, hitherto not encountered in the usual Lie algebraic varieties or supervarieties, is introduced. The paper explores the rich and novel structure of the algebra, and it compares it on the one hand with the Jordan-Lie Superalgebras studied by Okubo and Kamiya, and on the other, with the four usual Euclidean division rings of the reals, the complexes, the quaternions and the octonions that the algebra is seen to combine, extend and generalise. A potential physical application of the algebra is briefly alluded to at the end.","sentences":["A new algebra, hitherto not encountered in the usual Lie algebraic varieties or supervarieties, is introduced.","The paper explores the rich and novel structure of the algebra, and it compares it on the one hand with the Jordan-Lie Superalgebras studied by Okubo and Kamiya, and on the other, with the four usual Euclidean division rings of the reals, the complexes, the quaternions and the octonions that the algebra is seen to combine, extend and generalise.","A potential physical application of the algebra is briefly alluded to at the end."],"url":"http://arxiv.org/abs/2405.01181v1","category":"math-ph"}
{"created":"2024-05-02 11:02:29","title":"Handling the asymmetric spectral line profile","abstract":"This paper discusses some features of the spectral line profile theory used in the treatment of measured atomic transitions. It is shown that going beyond the established linear approximation for the spectral line contour in the case of its nonresonant extension, the potential for a more accurate extraction of atomic characteristics from experimental data arises. Using the example of the Lyman-$\\alpha$ (Ly$_\\alpha$) transition in hydrogen, a simple analysis of the observed spectral line distorted by a possible interfering transitions is given. In particular, the results obtained in the present work clearly demonstrate that the processing of the same experimental data at different settings can provide an accurate determination of the transition frequency, the centre of gravity as well as the hyperfine splitting of the ground state in hydrogen-like atomic systems. The latter is especially important for setting up precision spectroscopic experiments on the antihydrogen atom.","sentences":["This paper discusses some features of the spectral line profile theory used in the treatment of measured atomic transitions.","It is shown that going beyond the established linear approximation for the spectral line contour in the case of its nonresonant extension, the potential for a more accurate extraction of atomic characteristics from experimental data arises.","Using the example of the Lyman-$\\alpha$ (Ly$_\\alpha$) transition in hydrogen, a simple analysis of the observed spectral line distorted by a possible interfering transitions is given.","In particular, the results obtained in the present work clearly demonstrate that the processing of the same experimental data at different settings can provide an accurate determination of the transition frequency, the centre of gravity as well as the hyperfine splitting of the ground state in hydrogen-like atomic systems.","The latter is especially important for setting up precision spectroscopic experiments on the antihydrogen atom."],"url":"http://arxiv.org/abs/2405.01177v1","category":"physics.atom-ph"}
{"created":"2024-05-02 11:00:50","title":"Equational Theories and Validity for Logically Constrained Term Rewriting (Full Version)","abstract":"Logically constrained term rewriting is a relatively new formalism where rules are equipped with constraints over some arbitrary theory. Although there are many recent advances with respect to rewriting induction, completion, complexity analysis and confluence analysis for logically constrained term rewriting, these works solely focus on the syntactic side of the formalism lacking detailed investigations on semantics. In this paper, we investigate a semantic side of logically constrained term rewriting. To this end, we first define constrained equations, constrained equational theories and validity of the former based on the latter. After presenting the relationship of validity and conversion of rewriting, we then construct a sound inference system to prove validity of constrained equations in constrained equational theories. Finally, we give an algebraic semantics, which enables one to establish invalidity of constrained equations in constrained equational theories. This algebraic semantics derive a new notion of consistency for constrained equational theories.","sentences":["Logically constrained term rewriting is a relatively new formalism where rules are equipped with constraints over some arbitrary theory.","Although there are many recent advances with respect to rewriting induction, completion, complexity analysis and confluence analysis for logically constrained term rewriting, these works solely focus on the syntactic side of the formalism lacking detailed investigations on semantics.","In this paper, we investigate a semantic side of logically constrained term rewriting.","To this end, we first define constrained equations, constrained equational theories and validity of the former based on the latter.","After presenting the relationship of validity and conversion of rewriting, we then construct a sound inference system to prove validity of constrained equations in constrained equational theories.","Finally, we give an algebraic semantics, which enables one to establish invalidity of constrained equations in constrained equational theories.","This algebraic semantics derive a new notion of consistency for constrained equational theories."],"url":"http://arxiv.org/abs/2405.01174v1","category":"cs.LO"}
{"created":"2024-05-02 10:48:22","title":"GroupedMixer: An Entropy Model with Group-wise Token-Mixers for Learned Image Compression","abstract":"Transformer-based entropy models have gained prominence in recent years due to their superior ability to capture long-range dependencies in probability distribution estimation compared to convolution-based methods. However, previous transformer-based entropy models suffer from a sluggish coding process due to pixel-wise autoregression or duplicated computation during inference. In this paper, we propose a novel transformer-based entropy model called GroupedMixer, which enjoys both faster coding speed and better compression performance than previous transformer-based methods. Specifically, our approach builds upon group-wise autoregression by first partitioning the latent variables into groups along spatial-channel dimensions, and then entropy coding the groups with the proposed transformer-based entropy model. The global causal self-attention is decomposed into more efficient group-wise interactions, implemented using inner-group and cross-group token-mixers. The inner-group token-mixer incorporates contextual elements within a group while the cross-group token-mixer interacts with previously decoded groups. Alternate arrangement of two token-mixers enables global contextual reference. To further expedite the network inference, we introduce context cache optimization to GroupedMixer, which caches attention activation values in cross-group token-mixers and avoids complex and duplicated computation. Experimental results demonstrate that the proposed GroupedMixer yields the state-of-the-art rate-distortion performance with fast compression speed.","sentences":["Transformer-based entropy models have gained prominence in recent years due to their superior ability to capture long-range dependencies in probability distribution estimation compared to convolution-based methods.","However, previous transformer-based entropy models suffer from a sluggish coding process due to pixel-wise autoregression or duplicated computation during inference.","In this paper, we propose a novel transformer-based entropy model called GroupedMixer, which enjoys both faster coding speed and better compression performance than previous transformer-based methods.","Specifically, our approach builds upon group-wise autoregression by first partitioning the latent variables into groups along spatial-channel dimensions, and then entropy coding the groups with the proposed transformer-based entropy model.","The global causal self-attention is decomposed into more efficient group-wise interactions, implemented using inner-group and cross-group token-mixers.","The inner-group token-mixer incorporates contextual elements within a group while the cross-group token-mixer interacts with previously decoded groups.","Alternate arrangement of two token-mixers enables global contextual reference.","To further expedite the network inference, we introduce context cache optimization to GroupedMixer, which caches attention activation values in cross-group token-mixers and avoids complex and duplicated computation.","Experimental results demonstrate that the proposed GroupedMixer yields the state-of-the-art rate-distortion performance with fast compression speed."],"url":"http://arxiv.org/abs/2405.01170v1","category":"cs.CV"}
{"created":"2024-05-02 10:46:20","title":"Fourier-Mukai loci of K3 surfaces of Picard number one","abstract":"In this paper, we describe the Fourier-Mukai locus of the derived category of a complex algebraic K3 surface of Picard number one. We also prove that the Fourier-Mukai locus of the derived category of a complex algebraic K3 surface of Picard number one is strictly smaller than it's Matsui spectrum.","sentences":["In this paper, we describe the Fourier-Mukai locus of the derived category of a complex algebraic K3 surface of Picard number one.","We also prove that the Fourier-Mukai locus of the derived category of a complex algebraic K3 surface of Picard number one is strictly smaller than it's Matsui spectrum."],"url":"http://arxiv.org/abs/2405.01169v1","category":"math.AG"}
{"created":"2024-05-02 10:44:09","title":"Remote Nucleation and Stationary Domain Walls via Transition Waves in Tristable Magnetoelastic Lattices","abstract":"We present a tunable magnetoelastic lattice with a multistable onsite potential, focusing on a tristable potential. Through experimental and numerical analysis, we verify the existence of three types of transition waves with distinct amplitudes and velocities. Additionally, we establish the presence of a scaling law that elucidates various characteristics of these transition waves. By manipulating the onsite potential, we investigate the collision dynamics of two transition waves within the system. In chains featuring an asymmetric potential well, the collision of similar transition waves leads to the remote nucleation of a new phase. In chains with a symmetric potential well, the collision of dissimilar transition waves results in the formation of a stationary domain wall","sentences":["We present a tunable magnetoelastic lattice with a multistable onsite potential, focusing on a tristable potential.","Through experimental and numerical analysis, we verify the existence of three types of transition waves with distinct amplitudes and velocities.","Additionally, we establish the presence of a scaling law that elucidates various characteristics of these transition waves.","By manipulating the onsite potential, we investigate the collision dynamics of two transition waves within the system.","In chains featuring an asymmetric potential well, the collision of similar transition waves leads to the remote nucleation of a new phase.","In chains with a symmetric potential well, the collision of dissimilar transition waves results in the formation of a stationary domain wall"],"url":"http://arxiv.org/abs/2405.01168v1","category":"nlin.PS"}
{"created":"2024-05-02 10:29:06","title":"Quantum algorithms for Hopcroft's problem","abstract":"In this work we study quantum algorithms for Hopcroft's problem which is a fundamental problem in computational geometry. Given $n$ points and $n$ lines in the plane, the task is to determine whether there is a point-line incidence. The classical complexity of this problem is well-studied, with the best known algorithm running in $O(n^{4/3})$ time, with matching lower bounds in some restricted settings. Our results are two different quantum algorithms with time complexity $\\widetilde O(n^{5/6})$. The first algorithm is based on partition trees and the quantum backtracking algorithm. The second algorithm uses a quantum walk together with a history-independent dynamic data structure for storing line arrangement which supports efficient point location queries. In the setting where the number of points and lines differ, the quantum walk-based algorithm is asymptotically faster. The quantum speedups for the aforementioned data structures may be useful for other geometric problems.","sentences":["In this work we study quantum algorithms for Hopcroft's problem which is a fundamental problem in computational geometry.","Given $n$ points and $n$ lines in the plane, the task is to determine whether there is a point-line incidence.","The classical complexity of this problem is well-studied, with the best known algorithm running in $O(n^{4/3})$ time, with matching lower bounds in some restricted settings.","Our results are two different quantum algorithms with time complexity $\\widetilde O(n^{5/6})$.","The first algorithm is based on partition trees and the quantum backtracking algorithm.","The second algorithm uses a quantum walk together with a history-independent dynamic data structure for storing line arrangement which supports efficient point location queries.","In the setting where the number of points and lines differ, the quantum walk-based algorithm is asymptotically faster.","The quantum speedups for the aforementioned data structures may be useful for other geometric problems."],"url":"http://arxiv.org/abs/2405.01160v1","category":"quant-ph"}
{"created":"2024-05-02 10:06:13","title":"Optimizing Satellite Network Infrastructure: A Joint Approach to Gateway Placement and Routing","abstract":"Satellite constellation systems are becoming more attractive to provide communication services worldwide, especially in areas without network connectivity. While optimizing satellite gateway placement is crucial for operators to minimize deployment and operating costs, reducing the number of gateways may require more inter-satellite link hops to reach the ground network, thereby increasing latency. Therefore, it is of significant importance to develop a framework that optimizes gateway placement, dynamic routing, and flow management in inter-satellite links to enhance network performance. To this end, we model an optimization problem as a mixed-integer problem with a cost function combining the number of gateways, flow allocation, and traffic latency, allowing satellite operators to set priorities based on their policies. Our simulation results indicate that the proposed approach effectively reduces the number of active gateways by selecting their most appropriate locations while balancing the trade-off between the number of gateways and traffic latency. Furthermore, we demonstrate the impact of different weights in the cost function on performance through comparative analysis.","sentences":["Satellite constellation systems are becoming more attractive to provide communication services worldwide, especially in areas without network connectivity.","While optimizing satellite gateway placement is crucial for operators to minimize deployment and operating costs, reducing the number of gateways may require more inter-satellite link hops to reach the ground network, thereby increasing latency.","Therefore, it is of significant importance to develop a framework that optimizes gateway placement, dynamic routing, and flow management in inter-satellite links to enhance network performance.","To this end, we model an optimization problem as a mixed-integer problem with a cost function combining the number of gateways, flow allocation, and traffic latency, allowing satellite operators to set priorities based on their policies.","Our simulation results indicate that the proposed approach effectively reduces the number of active gateways by selecting their most appropriate locations while balancing the trade-off between the number of gateways and traffic latency.","Furthermore, we demonstrate the impact of different weights in the cost function on performance through comparative analysis."],"url":"http://arxiv.org/abs/2405.01149v1","category":"eess.SY"}
{"created":"2024-05-02 10:04:12","title":"Energy-Efficient Reconfigurable Holographic Surfaces Operating in the Presence of Realistic Hardware Impairments","abstract":"Reconfigurable holographic surfaces (RHSs) constitute a promising technique of supporting energy-efficient communications. In this paper, we formulate the energy efficiency maximization problem of the switch-controlled RHS-aided beamforming architecture by alternately optimizing the holographic beamformer at the RHS, the digital beamformer, the total transmit power and the power sharing ratio of each user. Specifically, to deal with this challenging non-convex optimization problem, we decouple it into three sub-problems. Firstly, the coefficients of RHS elements responsible for the holographic beamformer are optimized to maximize the sum of the eigen-channel gains of all users by our proposed low-complexity eigen-decomposition (ED) method. Then, the digital beamformer is designed by the singular value decomposition (SVD) method to support multi-user information transfer. Finally, the total transmit power and the power sharing ratio are alternately optimized, while considering the effect of transceiver hardware impairments (HWI). We theoretically derive the spectral efficiency and energy efficiency performance upper bound for the RHS-based beamforming architectures in the presence of HWIs. Our simulation results show that the switch-controlled RHS-aided beamforming architecture achieves higher energy efficiency than the conventional fully digital beamformer and the hybrid beamformer based on phase shift arrays (PSA). Moreover, considering the effect of HWI in the beamforming design can bring about further energy efficiency enhancements.","sentences":["Reconfigurable holographic surfaces (RHSs) constitute a promising technique of supporting energy-efficient communications.","In this paper, we formulate the energy efficiency maximization problem of the switch-controlled RHS-aided beamforming architecture by alternately optimizing the holographic beamformer at the RHS, the digital beamformer, the total transmit power and the power sharing ratio of each user.","Specifically, to deal with this challenging non-convex optimization problem, we decouple it into three sub-problems.","Firstly, the coefficients of RHS elements responsible for the holographic beamformer are optimized to maximize the sum of the eigen-channel gains of all users by our proposed low-complexity eigen-decomposition (ED) method.","Then, the digital beamformer is designed by the singular value decomposition (SVD) method to support multi-user information transfer.","Finally, the total transmit power and the power sharing ratio are alternately optimized, while considering the effect of transceiver hardware impairments (HWI).","We theoretically derive the spectral efficiency and energy efficiency performance upper bound for the RHS-based beamforming architectures in the presence of HWIs.","Our simulation results show that the switch-controlled RHS-aided beamforming architecture achieves higher energy efficiency than the conventional fully digital beamformer and the hybrid beamformer based on phase shift arrays (PSA).","Moreover, considering the effect of HWI in the beamforming design can bring about further energy efficiency enhancements."],"url":"http://arxiv.org/abs/2405.01146v1","category":"cs.IT"}
{"created":"2024-05-02 09:57:08","title":"Tracking and classifying objects with DAS data along railway","abstract":"Distributed acoustic sensing through fiber-optical cables can contribute to traffic monitoring systems. Using data from a day of field testing on a 50 km long fiber-optic cable along a railroad track in Norway, we detect and track cars and trains along a segment of the fiber-optic cable where the road runs parallel to the railroad tracks. We develop a method for automatic detection of events and then use these in a Kalman filter variant known as joint probabilistic data association for object tracking and classification. Model parameters are specified using in-situ log data along with the fiber-optic signals. Running the algorithm over an entire day, we highlight results of counting cars and trains over time and their estimated velocities.","sentences":["Distributed acoustic sensing through fiber-optical cables can contribute to traffic monitoring systems.","Using data from a day of field testing on a 50 km long fiber-optic cable along a railroad track in Norway, we detect and track cars and trains along a segment of the fiber-optic cable where the road runs parallel to the railroad tracks.","We develop a method for automatic detection of events and then use these in a Kalman filter variant known as joint probabilistic data association for object tracking and classification.","Model parameters are specified using in-situ log data along with the fiber-optic signals.","Running the algorithm over an entire day, we highlight results of counting cars and trains over time and their estimated velocities."],"url":"http://arxiv.org/abs/2405.01140v1","category":"stat.AP"}
{"created":"2024-05-02 09:49:52","title":"A missing theorem on dual spaces","abstract":"We answer in the affirmative the surprisingly difficult questions: If a complex Banach space possesses a real predual X, then is X a complex Banach space?   If a complex Banach space possesses a real predual, then does it have a complex predual? We also answer the analogous questions for operator spaces, that is spaces of operators on a Hilbert space, up to complete isometry. Indeed we use operator space methods to solve the Banach space question above.","sentences":["We answer in the affirmative the surprisingly difficult questions: If a complex Banach space possesses a real predual X, then is X a complex Banach space?   ","If a complex Banach space possesses a real predual, then does it have a complex predual?","We also answer the analogous questions for operator spaces, that is spaces of operators on a Hilbert space, up to complete isometry.","Indeed we use operator space methods to solve the Banach space question above."],"url":"http://arxiv.org/abs/2405.01133v1","category":"math.FA"}
{"created":"2024-05-02 09:49:27","title":"The Vector Dark Matter, LHC Constraints Including a 95 GeV Light Higgs Boson","abstract":"We study LHC searches for an extension of the Standard Model (SM) by exploiting an additional Abelian $U_D(1)$ gauge symmetry and a complex scalar Higgs portal. As the scalar is charged under this gauge symmetry, a vector dark matter (VDM) candidate can satisfy the observed relic abundance and limits from direct dark matter (DM) searches. The ATLAS and CMS experiments have developed a broad search program for the DM candidates, including associate production of Higgs boson, Z boson and top quark which couple to DM. In this paper, we perform an extensive analysis to constrain the model by using these experiments at LHC. It can be seen that the LHC results can exclude some parts of the parameter space which are still allowed by relic density and the direct detection searches. Furthermore, exclusion limits on the parameter space of the model by using the new results of CMS and ATLAS collaborations for new light Higgs boson with mass $\\sim95~\\rm GeV$ are provided.","sentences":["We study LHC searches for an extension of the Standard Model (SM) by exploiting an additional Abelian $U_D(1)$ gauge symmetry and a complex scalar Higgs portal.","As the scalar is charged under this gauge symmetry, a vector dark matter (VDM) candidate can satisfy the observed relic abundance and limits from direct dark matter (DM) searches.","The ATLAS and CMS experiments have developed a broad search program for the DM candidates, including associate production of Higgs boson, Z boson and top quark which couple to DM.","In this paper, we perform an extensive analysis to constrain the model by using these experiments at LHC.","It can be seen that the LHC results can exclude some parts of the parameter space which are still allowed by relic density and the direct detection searches.","Furthermore, exclusion limits on the parameter space of the model by using the new results of CMS and ATLAS collaborations for new light Higgs boson with mass $\\sim95~\\rm GeV$ are provided."],"url":"http://arxiv.org/abs/2405.01132v1","category":"hep-ph"}
{"created":"2024-05-02 09:48:52","title":"Optimal-order Trotter-Suzuki decomposition for quantum simulation on noisy quantum computers","abstract":"The potential of employing higher orders of the Trotter-Suzuki decomposition of the evolution operator for more effective simulations of quantum systems on a noisy quantum computer is explored. By examining the transverse-field Ising model and the XY model, it is demonstrated that when the gate error is decreased by approximately an order of magnitude relative to typical modern values, higher-order Trotterization becomes advantageous. This form of Trotterization yields a global minimum of the overall simulation error, comprising both the mathematical error of Trotterization and the physical error arising from gate execution.","sentences":["The potential of employing higher orders of the Trotter-Suzuki decomposition of the evolution operator for more effective simulations of quantum systems on a noisy quantum computer is explored.","By examining the transverse-field Ising model and the XY model, it is demonstrated that when the gate error is decreased by approximately an order of magnitude relative to typical modern values, higher-order Trotterization becomes advantageous.","This form of Trotterization yields a global minimum of the overall simulation error, comprising both the mathematical error of Trotterization and the physical error arising from gate execution."],"url":"http://arxiv.org/abs/2405.01131v1","category":"quant-ph"}
{"created":"2024-05-02 09:23:37","title":"A New Self-Alignment Method without Solving Wahba Problem for SINS in Autonomous Vehicles","abstract":"Initial alignment is one of the key technologies in strapdown inertial navigation system (SINS) to provide initial state information for vehicle attitude and navigation. For some situations, such as the attitude heading reference system, the position is not necessarily required or even available, then the self-alignment that does not rely on any external aid becomes very necessary. This study presents a new self-alignment method under swaying conditions, which can determine the latitude and attitude simultaneously by utilizing all observation vectors without solving the Wahba problem, and it is different from the existing methods. By constructing the dyadic tensor of each observation and reference vector itself, all equations related to observation and reference vectors are accumulated into one equation, where the latitude variable is extracted and solved according to the same eigenvalues of similar matrices on both sides of the equation, meanwhile the attitude is obtained by eigenvalue decomposition. Simulation and experiment tests verify the effectiveness of the proposed methods, and the alignment result is better than TRIAD in convergence speed and stability and comparable with OBA method in alignment accuracy with or without latitude. It is useful for guiding the design of initial alignment in autonomous vehicle applications.","sentences":["Initial alignment is one of the key technologies in strapdown inertial navigation system (SINS) to provide initial state information for vehicle attitude and navigation.","For some situations, such as the attitude heading reference system, the position is not necessarily required or even available, then the self-alignment that does not rely on any external aid becomes very necessary.","This study presents a new self-alignment method under swaying conditions, which can determine the latitude and attitude simultaneously by utilizing all observation vectors without solving the Wahba problem, and it is different from the existing methods.","By constructing the dyadic tensor of each observation and reference vector itself, all equations related to observation and reference vectors are accumulated into one equation, where the latitude variable is extracted and solved according to the same eigenvalues of similar matrices on both sides of the equation, meanwhile the attitude is obtained by eigenvalue decomposition.","Simulation and experiment tests verify the effectiveness of the proposed methods, and the alignment result is better than TRIAD in convergence speed and stability and comparable with OBA method in alignment accuracy with or without latitude.","It is useful for guiding the design of initial alignment in autonomous vehicle applications."],"url":"http://arxiv.org/abs/2405.01115v1","category":"cs.RO"}
{"created":"2024-05-02 09:14:41","title":"CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications","abstract":"Spatial understanding from vision is crucial for robots operating in unstructured environments. In the real world, spatial understanding is often an ill-posed problem. There are a number of powerful classical methods that accurately regress relative pose, however, these approaches often lack the ability to leverage data-derived priors to resolve ambiguities. In multi-robot systems, these challenges are exacerbated by the need for accurate and frequent position estimates of cooperating agents. To this end, we propose CoViS-Net, a cooperative, multi-robot, visual spatial foundation model that learns spatial priors from data. Unlike prior work evaluated primarily on offline datasets, we design our model specifically for online evaluation and real-world deployment on cooperative robots. Our model is completely decentralized, platform agnostic, executable in real-time using onboard compute, and does not require existing network infrastructure. In this work, we focus on relative pose estimation and local Bird's Eye View (BEV) prediction tasks. Unlike classical approaches, we show that our model can accurately predict relative poses without requiring camera overlap, and predict BEVs of regions not visible to the ego-agent. We demonstrate our model on a multi-robot formation control task outside the confines of the laboratory.","sentences":["Spatial understanding from vision is crucial for robots operating in unstructured environments.","In the real world, spatial understanding is often an ill-posed problem.","There are a number of powerful classical methods that accurately regress relative pose, however, these approaches often lack the ability to leverage data-derived priors to resolve ambiguities.","In multi-robot systems, these challenges are exacerbated by the need for accurate and frequent position estimates of cooperating agents.","To this end, we propose CoViS-Net, a cooperative, multi-robot, visual spatial foundation model that learns spatial priors from data.","Unlike prior work evaluated primarily on offline datasets, we design our model specifically for online evaluation and real-world deployment on cooperative robots.","Our model is completely decentralized, platform agnostic, executable in real-time using onboard compute, and does not require existing network infrastructure.","In this work, we focus on relative pose estimation and local Bird's Eye View (BEV) prediction tasks.","Unlike classical approaches, we show that our model can accurately predict relative poses without requiring camera overlap, and predict BEVs of regions not visible to the ego-agent.","We demonstrate our model on a multi-robot formation control task outside the confines of the laboratory."],"url":"http://arxiv.org/abs/2405.01107v1","category":"cs.RO"}
{"created":"2024-05-02 08:48:16","title":"Closed-Loop Sensitivity Identification for Cross-Directional Systems","abstract":"At Diamond Light Source, the UK's national synchrotron facility, electron beam disturbances are attenuated by the fast orbit feedback (FOFB), which controls a cross-directional (CD) system with hundreds of inputs and outputs. Due to the inability to measure the disturbance spectrum in real-time, the closed-loop sensitivity of the FOFB cannot be evaluated, making it difficult to compare FOFB algorithms and detect faults. Existing methods rely on comparing open-loop with closed-loop measurements, but they are prone to instabilities and actuator saturation because of the system's strong directionality. Here, we introduce a reference signal to estimate the complementary sensitivity in closed loop. By decoupling the system into sets of single-input, single-output (SISO) systems, we design the reference mode-by-mode to accommodate the system's strong directionality. This allows SISO system identification to be used, making our approach suitable for large-scale systems. Additionally, we derive lower bounds on reference amplitudes to achieve a predefined estimation error bound in the presence of disturbances and measurement noise. Our approach not only enables performance estimation of ill-conditioned CD systems in closed-loop but also provides a signal for fault detection. Its potential applications extend to other CD systems, such as papermaking, steel rolling, or battery manufacturing processes.","sentences":["At Diamond Light Source, the UK's national synchrotron facility, electron beam disturbances are attenuated by the fast orbit feedback (FOFB), which controls a cross-directional (CD) system with hundreds of inputs and outputs.","Due to the inability to measure the disturbance spectrum in real-time, the closed-loop sensitivity of the FOFB cannot be evaluated, making it difficult to compare FOFB algorithms and detect faults.","Existing methods rely on comparing open-loop with closed-loop measurements, but they are prone to instabilities and actuator saturation because of the system's strong directionality.","Here, we introduce a reference signal to estimate the complementary sensitivity in closed loop.","By decoupling the system into sets of single-input, single-output (SISO) systems, we design the reference mode-by-mode to accommodate the system's strong directionality.","This allows SISO system identification to be used, making our approach suitable for large-scale systems.","Additionally, we derive lower bounds on reference amplitudes to achieve a predefined estimation error bound in the presence of disturbances and measurement noise.","Our approach not only enables performance estimation of ill-conditioned CD systems in closed-loop but also provides a signal for fault detection.","Its potential applications extend to other CD systems, such as papermaking, steel rolling, or battery manufacturing processes."],"url":"http://arxiv.org/abs/2405.01094v1","category":"eess.SY"}
{"created":"2024-05-02 08:43:47","title":"Maximizing Network Phylogenetic Diversity","abstract":"Network Phylogenetic Diversity (Network-PD) is a measure for the diversity of a set of species based on a rooted phylogenetic network (with branch lengths and inheritance probabilities on the reticulation edges) describing the evolution of those species. We consider the \\textsc{Max-Network-PD} problem: given such a network, find~$k$ species with maximum Network-PD score. We show that this problem is fixed-parameter tractable (FPT) for binary networks, by describing an optimal algorithm running in $\\mathcal{O}(2^r \\log (k)(n+r))$~time, with~$n$ the total number of species in the network and~$r$ its reticulation number. Furthermore, we show that \\textsc{Max-Network-PD} is NP-hard for level-1 networks, proving that, unless P$=$NP, the FPT approach cannot be extended by using the level as parameter instead of the reticulation number.","sentences":["Network Phylogenetic Diversity (Network-PD) is a measure for the diversity of a set of species based on a rooted phylogenetic network (with branch lengths and inheritance probabilities on the reticulation edges) describing the evolution of those species.","We consider the \\textsc{Max-Network-PD} problem: given such a network, find~$k$ species with maximum Network-PD score.","We show that this problem is fixed-parameter tractable (FPT) for binary networks, by describing an optimal algorithm running in $\\mathcal{O}(2^r \\log (k)(n+r))$~time, with~$n$ the total number of species in the network and~$r$ its reticulation number.","Furthermore, we show that \\textsc{Max-Network-PD} is NP-hard for level-1 networks, proving that, unless P$=$NP, the FPT approach cannot be extended by using the level as parameter instead of the reticulation number."],"url":"http://arxiv.org/abs/2405.01091v1","category":"cs.CC"}
{"created":"2024-05-02 08:33:41","title":"Non-overshooting sliding mode for UAV control","abstract":"For a class of uncertain systems, a non-overshooting sliding mode control is presented to make them globally exponentially stable and without overshoot. Even when the unknown stochastic disturbance exists, and the time-variant reference trajectory is required, the strict non-overshooting stabilization is still achieved. The control law design is based on a desired second-order sliding mode (2-sliding mode), which successively includes two bounded-gain subsystems. Non-overshooting stability requires that the system gains depend on the initial values of system variables. In order to obtain the global non-overshooting stability, the first subsystem with non-overshooting reachability compresses the initial values of the second subsystem to a given bounded range. By partitioning these initial values, the bounded system gains are determined to satisfy the robust non-overshooting stability. In order to reject the chattering in the controller output, a tanh-function-based sliding mode is developed for the design of smoothed non-overshooting controller. The proposed method is applied to a UAV trajectory tracking when the disturbances and uncertainties exist. The control laws are designed to implement the non-overshooting stabilization in position and attitude. Finally, the effectiveness of the proposed method is demonstrated by the flying tests.","sentences":["For a class of uncertain systems, a non-overshooting sliding mode control is presented to make them globally exponentially stable and without overshoot.","Even when the unknown stochastic disturbance exists, and the time-variant reference trajectory is required, the strict non-overshooting stabilization is still achieved.","The control law design is based on a desired second-order sliding mode (2-sliding mode), which successively includes two bounded-gain subsystems.","Non-overshooting stability requires that the system gains depend on the initial values of system variables.","In order to obtain the global non-overshooting stability, the first subsystem with non-overshooting reachability compresses the initial values of the second subsystem to a given bounded range.","By partitioning these initial values, the bounded system gains are determined to satisfy the robust non-overshooting stability.","In order to reject the chattering in the controller output, a tanh-function-based sliding mode is developed for the design of smoothed non-overshooting controller.","The proposed method is applied to a UAV trajectory tracking when the disturbances and uncertainties exist.","The control laws are designed to implement the non-overshooting stabilization in position and attitude.","Finally, the effectiveness of the proposed method is demonstrated by the flying tests."],"url":"http://arxiv.org/abs/2405.01087v1","category":"eess.SY"}
{"created":"2024-05-02 08:33:31","title":"Continuous-variable quantum kernel method on a programmable photonic quantum processor","abstract":"Among various quantum machine learning (QML) algorithms, the quantum kernel method has especially attracted attention due to its compatibility with noisy intermediate-scale quantum devices and its potential to achieve quantum advantage. This method performs classification and regression by nonlinearly mapping data into quantum states in a higher dimensional Hilbert space. Thus far, the quantum kernel method has been implemented only on qubit-based systems, but continuous-variable (CV) systems can potentially offer superior computational power by utilizing its infinite-dimensional Hilbert space. Here, we demonstrate the implementation of the classification task with the CV quantum kernel method on a programmable photonic quantum processor. We experimentally prove that the CV quantum kernel method successfully classifies several datasets robustly even under the experimental imperfections, with high accuracies comparable to the classical kernel. This demonstration sheds light on the utility of CV quantum systems for QML and should stimulate further study in other CV QML algorithms.","sentences":["Among various quantum machine learning (QML) algorithms, the quantum kernel method has especially attracted attention due to its compatibility with noisy intermediate-scale quantum devices and its potential to achieve quantum advantage.","This method performs classification and regression by nonlinearly mapping data into quantum states in a higher dimensional Hilbert space.","Thus far, the quantum kernel method has been implemented only on qubit-based systems, but continuous-variable (CV) systems can potentially offer superior computational power by utilizing its infinite-dimensional Hilbert space.","Here, we demonstrate the implementation of the classification task with the CV quantum kernel method on a programmable photonic quantum processor.","We experimentally prove that the CV quantum kernel method successfully classifies several datasets robustly even under the experimental imperfections, with high accuracies comparable to the classical kernel.","This demonstration sheds light on the utility of CV quantum systems for QML and should stimulate further study in other CV QML algorithms."],"url":"http://arxiv.org/abs/2405.01086v1","category":"quant-ph"}
{"created":"2024-05-02 08:29:05","title":"Single Image Super-Resolution Based on Global-Local Information Synergy","abstract":"Although several image super-resolution solutions exist, they still face many challenges. CNN-based algorithms, despite the reduction in computational complexity, still need to improve their accuracy. While Transformer-based algorithms have higher accuracy, their ultra-high computational complexity makes them difficult to be accepted in practical applications. To overcome the existing challenges, a novel super-resolution reconstruction algorithm is proposed in this paper. The algorithm achieves a significant increase in accuracy through a unique design while maintaining a low complexity. The core of the algorithm lies in its cleverly designed Global-Local Information Extraction Module and Basic Block Module. By combining global and local information, the Global-Local Information Extraction Module aims to understand the image content more comprehensively so as to recover the global structure and local details in the image more accurately, which provides rich information support for the subsequent reconstruction process. Experimental results show that the comprehensive performance of the algorithm proposed in this paper is optimal, providing an efficient and practical new solution in the field of super-resolution reconstruction.","sentences":["Although several image super-resolution solutions exist, they still face many challenges.","CNN-based algorithms, despite the reduction in computational complexity, still need to improve their accuracy.","While Transformer-based algorithms have higher accuracy, their ultra-high computational complexity makes them difficult to be accepted in practical applications.","To overcome the existing challenges, a novel super-resolution reconstruction algorithm is proposed in this paper.","The algorithm achieves a significant increase in accuracy through a unique design while maintaining a low complexity.","The core of the algorithm lies in its cleverly designed Global-Local Information Extraction Module and Basic Block Module.","By combining global and local information, the Global-Local Information Extraction Module aims to understand the image content more comprehensively so as to recover the global structure and local details in the image more accurately, which provides rich information support for the subsequent reconstruction process.","Experimental results show that the comprehensive performance of the algorithm proposed in this paper is optimal, providing an efficient and practical new solution in the field of super-resolution reconstruction."],"url":"http://arxiv.org/abs/2405.01085v1","category":"cs.CV"}
{"created":"2024-05-02 08:22:20","title":"A reduced scalar potential approach for magnetostatics avoiding the coenergy","abstract":"The numerical solution of problems in nonlinear magnetostatics is typically based on a variational formulation in terms of magnetic potentials, the discretization by finite elements, and iterative solvers like the Newton method. The vector potential approach aims at minimizing a certain energy functional and, in three dimensions, requires the use of edge elements and appropriate gauging conditions. The scalar potential approach, on the other hand, seeks to maximize the negative coenergy and can be realized by standard Lagrange finite elements, thus reducing the number of degrees of freedom and simplifying the implementation. The number of Newton iterations required to solve the governing nonlinear system, however, has been observed to be usually higher than for the vector potential formulation. In this paper, we propose a method that combines the advantages of both approaches, i.e., it requires as few Newton iterations as the vector potential formulation while involving the magnetic scalar potential as the primary unknown. We discuss the variational background of the method, its well-posedness, and its efficient implementation. Numerical examples are presented for illustration of the accuracy and the gain in efficiency compared to other approaches.","sentences":["The numerical solution of problems in nonlinear magnetostatics is typically based on a variational formulation in terms of magnetic potentials, the discretization by finite elements, and iterative solvers like the Newton method.","The vector potential approach aims at minimizing a certain energy functional and, in three dimensions, requires the use of edge elements and appropriate gauging conditions.","The scalar potential approach, on the other hand, seeks to maximize the negative coenergy and can be realized by standard Lagrange finite elements, thus reducing the number of degrees of freedom and simplifying the implementation.","The number of Newton iterations required to solve the governing nonlinear system, however, has been observed to be usually higher than for the vector potential formulation.","In this paper, we propose a method that combines the advantages of both approaches, i.e., it requires as few Newton iterations as the vector potential formulation while involving the magnetic scalar potential as the primary unknown.","We discuss the variational background of the method, its well-posedness, and its efficient implementation.","Numerical examples are presented for illustration of the accuracy and the gain in efficiency compared to other approaches."],"url":"http://arxiv.org/abs/2405.01082v1","category":"math.NA"}
{"created":"2024-05-02 08:15:18","title":"Singular Value and Frame Decomposition-based Reconstruction for Atmospheric Tomography","abstract":"Atmospheric tomography, the problem of reconstructing atmospheric turbulence profiles from wavefront sensor measurements, is an integral part of many adaptive optics systems used for enhancing the image quality of ground-based telescopes. Singular-value and frame decompositions of the underlying atmospheric tomography operator can reveal useful analytical information on this inverse problem, as well as serve as the basis of efficient numerical reconstruction algorithms. In this paper, we extend existing singular value decompositions to more realistic Sobolev settings including weighted inner products, and derive an explicit representation of a frame-based (approximate) solution operator. These investigations form the basis of efficient numerical solution methods, which we analyze via numerical simulations for the challenging, real-world Adaptive Optics system of the Extremely Large Telescope using the entirely MATLAB-based simulation tool MOST.","sentences":["Atmospheric tomography, the problem of reconstructing atmospheric turbulence profiles from wavefront sensor measurements, is an integral part of many adaptive optics systems used for enhancing the image quality of ground-based telescopes.","Singular-value and frame decompositions of the underlying atmospheric tomography operator can reveal useful analytical information on this inverse problem, as well as serve as the basis of efficient numerical reconstruction algorithms.","In this paper, we extend existing singular value decompositions to more realistic Sobolev settings including weighted inner products, and derive an explicit representation of a frame-based (approximate) solution operator.","These investigations form the basis of efficient numerical solution methods, which we analyze via numerical simulations for the challenging, real-world Adaptive Optics system of the Extremely Large Telescope using the entirely MATLAB-based simulation tool MOST."],"url":"http://arxiv.org/abs/2405.01079v1","category":"math.NA"}
{"created":"2024-05-02 08:10:08","title":"Stability Analysis of Interacting Wireless Repeaters","abstract":"We consider a wireless network with multiple single-antenna repeaters that amplify and instantaneously re-transmit the signals they receive to improve the channel rank and system coverage. Due to the positive feedback formed by inter-repeater interference, stability could become a critical issue. We investigate the problem of determining the maximum amplification gain that the repeaters can use without breaking the system stability. Specifically, we obtain a bound by using the Gershgorin disc theorem, which reveals that the maximum amplification gain is restricted by the sum of channel amplitude gains. We show by case studies the usefulness of the so-obtained bound and provide insights on how the repeaters should be deployed.","sentences":["We consider a wireless network with multiple single-antenna repeaters that amplify and instantaneously re-transmit the signals they receive to improve the channel rank and system coverage.","Due to the positive feedback formed by inter-repeater interference, stability could become a critical issue.","We investigate the problem of determining the maximum amplification gain that the repeaters can use without breaking the system stability.","Specifically, we obtain a bound by using the Gershgorin disc theorem, which reveals that the maximum amplification gain is restricted by the sum of channel amplitude gains.","We show by case studies the usefulness of the so-obtained bound and provide insights on how the repeaters should be deployed."],"url":"http://arxiv.org/abs/2405.01074v1","category":"cs.IT"}
{"created":"2024-05-02 07:44:11","title":"MFDS-Net: Multi-Scale Feature Depth-Supervised Network for Remote Sensing Change Detection with Global Semantic and Detail Information","abstract":"Change detection as an interdisciplinary discipline in the field of computer vision and remote sensing at present has been receiving extensive attention and research. Due to the rapid development of society, the geographic information captured by remote sensing satellites is changing faster and more complex, which undoubtedly poses a higher challenge and highlights the value of change detection tasks. We propose MFDS-Net: Multi-Scale Feature Depth-Supervised Network for Remote Sensing Change Detection with Global Semantic and Detail Information (MFDS-Net) with the aim of achieving a more refined description of changing buildings as well as geographic information, enhancing the localisation of changing targets and the acquisition of weak features. To achieve the research objectives, we use a modified ResNet_34 as backbone network to perform feature extraction and DO-Conv as an alternative to traditional convolution to better focus on the association between feature information and to obtain better training results. We propose the Global Semantic Enhancement Module (GSEM) to enhance the processing of high-level semantic information from a global perspective. The Differential Feature Integration Module (DFIM) is proposed to strengthen the fusion of different depth feature information, achieving learning and extraction of differential features. The entire network is trained and optimized using a deep supervision mechanism.   The experimental outcomes of MFDS-Net surpass those of current mainstream change detection networks. On the LEVIR dataset, it achieved an F1 score of 91.589 and IoU of 84.483, on the WHU dataset, the scores were F1: 92.384 and IoU: 86.807, and on the GZ-CD dataset, the scores were F1: 86.377 and IoU: 76.021. The code is available at https://github.com/AOZAKIiii/MFDS-Net","sentences":["Change detection as an interdisciplinary discipline in the field of computer vision and remote sensing at present has been receiving extensive attention and research.","Due to the rapid development of society, the geographic information captured by remote sensing satellites is changing faster and more complex, which undoubtedly poses a higher challenge and highlights the value of change detection tasks.","We propose MFDS-Net: Multi-Scale Feature Depth-Supervised Network for Remote Sensing Change Detection with Global Semantic and Detail Information (MFDS-Net) with the aim of achieving a more refined description of changing buildings as well as geographic information, enhancing the localisation of changing targets and the acquisition of weak features.","To achieve the research objectives, we use a modified ResNet_34 as backbone network to perform feature extraction and DO-Conv as an alternative to traditional convolution to better focus on the association between feature information and to obtain better training results.","We propose the Global Semantic Enhancement Module (GSEM) to enhance the processing of high-level semantic information from a global perspective.","The Differential Feature Integration Module (DFIM) is proposed to strengthen the fusion of different depth feature information, achieving learning and extraction of differential features.","The entire network is trained and optimized using a deep supervision mechanism.   ","The experimental outcomes of MFDS-Net surpass those of current mainstream change detection networks.","On the LEVIR dataset, it achieved an F1 score of 91.589 and IoU of 84.483, on the WHU dataset, the scores were F1: 92.384 and IoU: 86.807, and on the GZ-CD dataset, the scores were F1: 86.377 and IoU: 76.021.","The code is available at https://github.com/AOZAKIiii/MFDS-Net"],"url":"http://arxiv.org/abs/2405.01065v1","category":"cs.CV"}
{"created":"2024-05-02 07:42:33","title":"A Roadmap for Simulation-Based Testing of Autonomous Cyber-Physical Systems: Challenges and Future Direction","abstract":"As the era of autonomous cyber-physical systems (ACPSs), such as unmanned aerial vehicles and self-driving cars, unfolds, the demand for robust testing methodologies is key to realizing the adoption of such systems in real-world scenarios. However, traditional software testing paradigms face unprecedented challenges in ensuring the safety and reliability of these systems. In response, this paper pioneers a strategic roadmap for simulation-based testing of ACPSs, specifically focusing on autonomous systems. Our paper discusses the relevant challenges and obstacles of ACPSs, focusing on test automation and quality assurance, hence advocating for tailored solutions to address the unique demands of autonomous systems. While providing concrete definitions of test cases within simulation environments, we also accentuate the need to create new benchmark assets and the development of automated tools tailored explicitly for autonomous systems in the software engineering community. This paper not only highlights the relevant, pressing issues the software engineering community should focus on (in terms of practices, expected automation, and paradigms), but it also outlines ways to tackle them. By outlining the various domains and challenges of simulation-based testing/development for ACPSs, we provide directions for future research efforts.","sentences":["As the era of autonomous cyber-physical systems (ACPSs), such as unmanned aerial vehicles and self-driving cars, unfolds, the demand for robust testing methodologies is key to realizing the adoption of such systems in real-world scenarios.","However, traditional software testing paradigms face unprecedented challenges in ensuring the safety and reliability of these systems.","In response, this paper pioneers a strategic roadmap for simulation-based testing of ACPSs, specifically focusing on autonomous systems.","Our paper discusses the relevant challenges and obstacles of ACPSs, focusing on test automation and quality assurance, hence advocating for tailored solutions to address the unique demands of autonomous systems.","While providing concrete definitions of test cases within simulation environments, we also accentuate the need to create new benchmark assets and the development of automated tools tailored explicitly for autonomous systems in the software engineering community.","This paper not only highlights the relevant, pressing issues the software engineering community should focus on (in terms of practices, expected automation, and paradigms), but it also outlines ways to tackle them.","By outlining the various domains and challenges of simulation-based testing/development for ACPSs, we provide directions for future research efforts."],"url":"http://arxiv.org/abs/2405.01064v1","category":"cs.SE"}
{"created":"2024-05-02 07:40:51","title":"Fair Recommendations with Limited Sensitive Attributes: A Distributionally Robust Optimization Approach","abstract":"As recommender systems are indispensable in various domains such as job searching and e-commerce, providing equitable recommendations to users with different sensitive attributes becomes an imperative requirement. Prior approaches for enhancing fairness in recommender systems presume the availability of all sensitive attributes, which can be difficult to obtain due to privacy concerns or inadequate means of capturing these attributes. In practice, the efficacy of these approaches is limited, pushing us to investigate ways of promoting fairness with limited sensitive attribute information.   Toward this goal, it is important to reconstruct missing sensitive attributes. Nevertheless, reconstruction errors are inevitable due to the complexity of real-world sensitive attribute reconstruction problems and legal regulations. Thus, we pursue fair learning methods that are robust to reconstruction errors. To this end, we propose Distributionally Robust Fair Optimization (DRFO), which minimizes the worst-case unfairness over all potential probability distributions of missing sensitive attributes instead of the reconstructed one to account for the impact of the reconstruction errors. We provide theoretical and empirical evidence to demonstrate that our method can effectively ensure fairness in recommender systems when only limited sensitive attributes are accessible.","sentences":["As recommender systems are indispensable in various domains such as job searching and e-commerce, providing equitable recommendations to users with different sensitive attributes becomes an imperative requirement.","Prior approaches for enhancing fairness in recommender systems presume the availability of all sensitive attributes, which can be difficult to obtain due to privacy concerns or inadequate means of capturing these attributes.","In practice, the efficacy of these approaches is limited, pushing us to investigate ways of promoting fairness with limited sensitive attribute information.   ","Toward this goal, it is important to reconstruct missing sensitive attributes.","Nevertheless, reconstruction errors are inevitable due to the complexity of real-world sensitive attribute reconstruction problems and legal regulations.","Thus, we pursue fair learning methods that are robust to reconstruction errors.","To this end, we propose Distributionally Robust Fair Optimization (DRFO), which minimizes the worst-case unfairness over all potential probability distributions of missing sensitive attributes instead of the reconstructed one to account for the impact of the reconstruction errors.","We provide theoretical and empirical evidence to demonstrate that our method can effectively ensure fairness in recommender systems when only limited sensitive attributes are accessible."],"url":"http://arxiv.org/abs/2405.01063v1","category":"cs.IR"}
{"created":"2024-05-02 07:34:48","title":"Femtoscopy with L\u00e9vy sources at NA61/SHINE","abstract":"In the recent decades of high-energy physics research, it was demonstrated that strongly interacting quark-gluon plasma (sQGP) is created in ultra-relativistic nucleus-nucleus collisions. Investigation and understanding of the properties of the hadronic matter are among the most important goals of the NA61/SHINE collaboration at CERN SPS. Mapping of the phase diagram is achieved by varying the collision energy (5 GeV $\\sqrt{s_{NN}}<17$ GeV) and by changing the collision system ($p$+$p$, $p$+Pb, Be+Be, Ar+Sc, Xe+La, Pb+Pb). Femtoscopic correlations reveal the space-time structure of the hadron emitting source.   In this article, we report on the measurement of femtoscopic correlations in small to intermediate systems. Comparing the measurements to calculations based on symmetric L\\'evy sources, we discuss the results on L\\'evy source parameters as a function of average pair transverse mass. One of the physical parameters is of particular importance, the L\\'evy exponent $\\alpha$, which describes the shape of the source and may be related to the critical exponent $\\eta$ in the proximity of the critical point. Therefore, measuring it may shed light on the location of the critical endpoint of the QCD phase diagram.","sentences":["In the recent decades of high-energy physics research, it was demonstrated that strongly interacting quark-gluon plasma (sQGP) is created in ultra-relativistic nucleus-nucleus collisions.","Investigation and understanding of the properties of the hadronic matter are among the most important goals of the NA61/SHINE collaboration at CERN SPS.","Mapping of the phase diagram is achieved by varying the collision energy (5 GeV $\\sqrt{s_{NN}}<17$ GeV) and by changing the collision system ($p$+$p$, $p$+Pb, Be+Be, Ar+Sc, Xe+La, Pb+Pb).","Femtoscopic correlations reveal the space-time structure of the hadron emitting source.   ","In this article, we report on the measurement of femtoscopic correlations in small to intermediate systems.","Comparing the measurements to calculations based on symmetric L\\'evy sources, we discuss the results on L\\'evy source parameters as a function of average pair transverse mass.","One of the physical parameters is of particular importance, the L\\'evy exponent $\\alpha$, which describes the shape of the source and may be related to the critical exponent $\\eta$ in the proximity of the critical point.","Therefore, measuring it may shed light on the location of the critical endpoint of the QCD phase diagram."],"url":"http://arxiv.org/abs/2405.01061v1","category":"nucl-ex"}
{"created":"2024-05-02 07:28:39","title":"Fuzzy Q-Learning-Based Opportunistic Communication for MEC-Enhanced Vehicular Crowdsensing","abstract":"This study focuses on MEC-enhanced, vehicle-based crowdsensing systems that rely on devices installed on automobiles. We investigate an opportunistic communication paradigm in which devices can transmit measured data directly to a crowdsensing server over a 4G communication channel or to nearby devices or so-called Road Side Units positioned along the road via Wi-Fi. We tackle a new problem that is how to reduce the cost of 4G while preserving the latency. We propose an offloading strategy that combines a reinforcement learning technique known as Q-learning with Fuzzy logic to accomplish the purpose. Q-learning assists devices in learning to decide the communication channel. Meanwhile, Fuzzy logic is used to optimize the reward function in Q-learning. The experiment results show that our offloading method significantly cuts down around 30-40% of the 4G communication cost while keeping the latency of 99% packets below the required threshold.","sentences":["This study focuses on MEC-enhanced, vehicle-based crowdsensing systems that rely on devices installed on automobiles.","We investigate an opportunistic communication paradigm in which devices can transmit measured data directly to a crowdsensing server over a 4G communication channel or to nearby devices or so-called Road Side Units positioned along the road via Wi-Fi.","We tackle a new problem that is how to reduce the cost of 4G while preserving the latency.","We propose an offloading strategy that combines a reinforcement learning technique known as Q-learning with Fuzzy logic to accomplish the purpose.","Q-learning assists devices in learning to decide the communication channel.","Meanwhile, Fuzzy logic is used to optimize the reward function in Q-learning.","The experiment results show that our offloading method significantly cuts down around 30-40% of the 4G communication cost while keeping the latency of 99% packets below the required threshold."],"url":"http://arxiv.org/abs/2405.01057v1","category":"cs.NI"}
{"created":"2024-05-02 07:11:05","title":"Polynomial Chaos Expanded Gaussian Process","abstract":"In complex and unknown processes, global models are initially generated over the entire experimental space, but they often fail to provide accurate predictions in local areas. Recognizing this limitation, this study addresses the need for models that effectively represent both global and local experimental spaces. It introduces a novel machine learning (ML) approach: Polynomial Chaos Expanded Gaussian Process (PCEGP), leveraging polynomial chaos expansion (PCE) to calculate input-dependent hyperparameters of the Gaussian process (GP). This approach provides a mathematically interpretable method that incorporates non-stationary covariance functions and heteroscedastic noise estimation to generate locally adapted models. The model performance is compared to different algorithms in benchmark tests for regression tasks. The results demonstrate low prediction errors of the PCEGP in these benchmark applications, highlighting model performance that is often competitive with or superior to previous methods. A key advantage of the presented model is the transparency and traceability in the calculation of hyperparameters and model predictions.","sentences":["In complex and unknown processes, global models are initially generated over the entire experimental space, but they often fail to provide accurate predictions in local areas.","Recognizing this limitation, this study addresses the need for models that effectively represent both global and local experimental spaces.","It introduces a novel machine learning (ML) approach: Polynomial Chaos Expanded Gaussian Process (PCEGP), leveraging polynomial chaos expansion (PCE) to calculate input-dependent hyperparameters of the Gaussian process (GP).","This approach provides a mathematically interpretable method that incorporates non-stationary covariance functions and heteroscedastic noise estimation to generate locally adapted models.","The model performance is compared to different algorithms in benchmark tests for regression tasks.","The results demonstrate low prediction errors of the PCEGP in these benchmark applications, highlighting model performance that is often competitive with or superior to previous methods.","A key advantage of the presented model is the transparency and traceability in the calculation of hyperparameters and model predictions."],"url":"http://arxiv.org/abs/2405.01052v1","category":"cs.LG"}
{"created":"2024-05-02 06:58:59","title":"Optimal Pricing for Linear-Quadratic Games with Nonlinear Interaction Between Agents","abstract":"This paper studies a class of network games with linear-quadratic payoffs and externalities exerted through a strictly concave interaction function. This class of game is motivated by the diminishing marginal effects with peer influences. We analyze the optimal pricing strategy for this class of network game. First, we prove the existence of a unique Nash Equilibrium (NE). Second, we study the optimal pricing strategy of a monopolist selling a divisible good to agents. We show that the optimal pricing strategy, found by solving a bilevel optimization problem, is strictly better when the monopolist knows the network structure as opposed to the best strategy agnostic to network structure. Numerical experiments demonstrate that in most cases, the maximum revenue is achieved with an asymmetric network. These results contrast with the previously studied case of linear interaction function, where a network-independent price is proven optimal with symmetric networks. Lastly, we describe an efficient algorithm to find the optimal pricing strategy.","sentences":["This paper studies a class of network games with linear-quadratic payoffs and externalities exerted through a strictly concave interaction function.","This class of game is motivated by the diminishing marginal effects with peer influences.","We analyze the optimal pricing strategy for this class of network game.","First, we prove the existence of a unique Nash Equilibrium (NE).","Second, we study the optimal pricing strategy of a monopolist selling a divisible good to agents.","We show that the optimal pricing strategy, found by solving a bilevel optimization problem, is strictly better when the monopolist knows the network structure as opposed to the best strategy agnostic to network structure.","Numerical experiments demonstrate that in most cases, the maximum revenue is achieved with an asymmetric network.","These results contrast with the previously studied case of linear interaction function, where a network-independent price is proven optimal with symmetric networks.","Lastly, we describe an efficient algorithm to find the optimal pricing strategy."],"url":"http://arxiv.org/abs/2405.01047v1","category":"math.OC"}
{"created":"2024-05-02 06:58:46","title":"Development of Cybersecurity Simulator-Based Platform for the Protection of Critical Infrastructures","abstract":"Critical infrastructures (CNI) are vulnerable to cyberattacks due to their interconnected communication systems. We are developing a platform using real-time simulation of cyber-physical systems to enhance CNI resilience and security. The platform, initiated in the Vaasa Harbor Microgrid, allows creation of a digital twin and real-time execution of its functions. It provides a co-simulation environment for simulating cyberattack scenarios, aiding in the design of a cybersecurity simulator-based platform and offering services for CNI stakeholders.","sentences":["Critical infrastructures (CNI) are vulnerable to cyberattacks due to their interconnected communication systems.","We are developing a platform using real-time simulation of cyber-physical systems to enhance CNI resilience and security.","The platform, initiated in the Vaasa Harbor Microgrid, allows creation of a digital twin and real-time execution of its functions.","It provides a co-simulation environment for simulating cyberattack scenarios, aiding in the design of a cybersecurity simulator-based platform and offering services for CNI stakeholders."],"url":"http://arxiv.org/abs/2405.01046v1","category":"cs.CR"}
{"created":"2024-05-02 06:54:59","title":"Reed-Solomon Codes over Cyclic Polynomial Ring with Lower Encoding/Decoding Complexity","abstract":"Reed-Solomon (RS) codes are constructed over a finite field that have been widely employed in storage and communication systems. Many fast encoding/decoding algorithms such as fast Fourier transform (FFT) and modular approach are designed for RS codes to reduce the encoding/decoding complexity defined as the number of XORs involved in the encoding/decoding procedure. In this paper, we present the construction of RS codes over the cyclic polynomial ring $ \\mathbb{F}_2[x]/(1+x+\\ldots+x^{p-1})$ and show that our codes are maximum distance separable (MDS) codes. Moreover, we propose the FFT and modular approach over the ring that can be employed in our codes for encoding/decoding complexity reduction. We show that our codes have 17.9\\% encoding complexity reduction and 7.5\\% decoding complexity reduction compared with RS codes over finite field, for $(n,k)=(2048,1984)$.","sentences":["Reed-Solomon (RS) codes are constructed over a finite field that have been widely employed in storage and communication systems.","Many fast encoding/decoding algorithms such as fast Fourier transform (FFT) and modular approach are designed for RS codes to reduce the encoding/decoding complexity defined as the number of XORs involved in the encoding/decoding procedure.","In this paper, we present the construction of RS codes over the cyclic polynomial ring $ \\mathbb{F}_2[x]/(1+x+\\ldots+x^{p-1})$ and show that our codes are maximum distance separable (MDS) codes.","Moreover, we propose the FFT and modular approach over the ring that can be employed in our codes for encoding/decoding complexity reduction.","We show that our codes have 17.9\\% encoding complexity reduction and","7.5\\% decoding complexity reduction compared with RS codes over finite field, for $(n,k)=(2048,1984)$."],"url":"http://arxiv.org/abs/2405.01043v1","category":"cs.IT"}
{"created":"2024-05-02 06:53:40","title":"Efficient and Flexible Method for Reducing Moderate-size Deep Neural Networks with Condensation","abstract":"Neural networks have been extensively applied to a variety of tasks, achieving astounding results. Applying neural networks in the scientific field is an important research direction that is gaining increasing attention. In scientific applications, the scale of neural networks is generally moderate-size, mainly to ensure the speed of inference during application. Additionally, comparing neural networks to traditional algorithms in scientific applications is inevitable. These applications often require rapid computations, making the reduction of neural network sizes increasingly important. Existing work has found that the powerful capabilities of neural networks are primarily due to their non-linearity. Theoretical work has discovered that under strong non-linearity, neurons in the same layer tend to behave similarly, a phenomenon known as condensation. Condensation offers an opportunity to reduce the scale of neural networks to a smaller subnetwork with similar performance. In this article, we propose a condensation reduction algorithm to verify the feasibility of this idea in practical problems. Our reduction method can currently be applied to both fully connected networks and convolutional networks, achieving positive results. In complex combustion acceleration tasks, we reduced the size of the neural network to 41.7% of its original scale while maintaining prediction accuracy. In the CIFAR10 image classification task, we reduced the network size to 11.5% of the original scale, still maintaining a satisfactory validation accuracy. Our method can be applied to most trained neural networks, reducing computational pressure and improving inference speed.","sentences":["Neural networks have been extensively applied to a variety of tasks, achieving astounding results.","Applying neural networks in the scientific field is an important research direction that is gaining increasing attention.","In scientific applications, the scale of neural networks is generally moderate-size, mainly to ensure the speed of inference during application.","Additionally, comparing neural networks to traditional algorithms in scientific applications is inevitable.","These applications often require rapid computations, making the reduction of neural network sizes increasingly important.","Existing work has found that the powerful capabilities of neural networks are primarily due to their non-linearity.","Theoretical work has discovered that under strong non-linearity, neurons in the same layer tend to behave similarly, a phenomenon known as condensation.","Condensation offers an opportunity to reduce the scale of neural networks to a smaller subnetwork with similar performance.","In this article, we propose a condensation reduction algorithm to verify the feasibility of this idea in practical problems.","Our reduction method can currently be applied to both fully connected networks and convolutional networks, achieving positive results.","In complex combustion acceleration tasks, we reduced the size of the neural network to 41.7% of its original scale while maintaining prediction accuracy.","In the CIFAR10 image classification task, we reduced the network size to 11.5% of the original scale, still maintaining a satisfactory validation accuracy.","Our method can be applied to most trained neural networks, reducing computational pressure and improving inference speed."],"url":"http://arxiv.org/abs/2405.01041v1","category":"cs.LG"}
{"created":"2024-05-02 17:43:45","title":"Reverse Influential Community Search Over Social Networks (Technical Report)","abstract":"As an important fundamental task of numerous real-world applications such as social network analysis and online advertising/marketing, several prior works studied influential community search, which retrieves a community with high structural cohesiveness and maximum influences on other users in social networks. However, previous works usually considered the influences of the community on arbitrary users in social networks, rather than specific groups (e.g., customer groups, or senior communities). Inspired by this, we propose a novel Reverse Influential Community Search (RICS) problem, which obtains a seed community with the maximum influence on a user-specified target community, satisfying both structural and keyword constraints. To efficiently tackle the RICS problem, we design effective pruning strategies to filter out false alarms of candidate seed communities, and propose an effective index mechanism to facilitate the community retrieval. We also formulate and tackle an RICS variant, named Relaxed Reverse Influential Community Search (R2ICS), which returns a subgraph with the relaxed structural constraints and having the maximum influence on a user-specified target community. Comprehensive experiments have been conducted to verify the efficiency and effectiveness of our RICS and R2ICS approaches on both real-world and synthetic social networks under various parameter settings.","sentences":["As an important fundamental task of numerous real-world applications such as social network analysis and online advertising/marketing, several prior works studied influential community search, which retrieves a community with high structural cohesiveness and maximum influences on other users in social networks.","However, previous works usually considered the influences of the community on arbitrary users in social networks, rather than specific groups (e.g., customer groups, or senior communities).","Inspired by this, we propose a novel Reverse Influential Community Search (RICS) problem, which obtains a seed community with the maximum influence on a user-specified target community, satisfying both structural and keyword constraints.","To efficiently tackle the RICS problem, we design effective pruning strategies to filter out false alarms of candidate seed communities, and propose an effective index mechanism to facilitate the community retrieval.","We also formulate and tackle an RICS variant, named Relaxed Reverse Influential Community Search (R2ICS), which returns a subgraph with the relaxed structural constraints and having the maximum influence on a user-specified target community.","Comprehensive experiments have been conducted to verify the efficiency and effectiveness of our RICS and R2ICS approaches on both real-world and synthetic social networks under various parameter settings."],"url":"http://arxiv.org/abs/2405.01510v1","category":"cs.SI"}
{"created":"2024-05-02 17:37:39","title":"Accelerating Convergence in Bayesian Few-Shot Classification","abstract":"Bayesian few-shot classification has been a focal point in the field of few-shot learning. This paper seamlessly integrates mirror descent-based variational inference into Gaussian process-based few-shot classification, addressing the challenge of non-conjugate inference. By leveraging non-Euclidean geometry, mirror descent achieves accelerated convergence by providing the steepest descent direction along the corresponding manifold. It also exhibits the parameterization invariance property concerning the variational distribution. Experimental results demonstrate competitive classification accuracy, improved uncertainty quantification, and faster convergence compared to baseline models. Additionally, we investigate the impact of hyperparameters and components. Code is publicly available at https://github.com/keanson/MD-BSFC.","sentences":["Bayesian few-shot classification has been a focal point in the field of few-shot learning.","This paper seamlessly integrates mirror descent-based variational inference into Gaussian process-based few-shot classification, addressing the challenge of non-conjugate inference.","By leveraging non-Euclidean geometry, mirror descent achieves accelerated convergence by providing the steepest descent direction along the corresponding manifold.","It also exhibits the parameterization invariance property concerning the variational distribution.","Experimental results demonstrate competitive classification accuracy, improved uncertainty quantification, and faster convergence compared to baseline models.","Additionally, we investigate the impact of hyperparameters and components.","Code is publicly available at https://github.com/keanson/MD-BSFC."],"url":"http://arxiv.org/abs/2405.01507v1","category":"cs.LG"}
{"created":"2024-05-02 17:15:30","title":"Designing Algorithmic Recommendations to Achieve Human-AI Complementarity","abstract":"Algorithms frequently assist, rather than replace, human decision-makers. However, the design and analysis of algorithms often focus on predicting outcomes and do not explicitly model their effect on human decisions. This discrepancy between the design and role of algorithmic assistants becomes of particular concern in light of empirical evidence that suggests that algorithmic assistants again and again fail to improve human decisions. In this article, we formalize the design of recommendation algorithms that assist human decision-makers without making restrictive ex-ante assumptions about how recommendations affect decisions. We formulate an algorithmic-design problem that leverages the potential-outcomes framework from causal inference to model the effect of recommendations on a human decision-maker's binary treatment choice. Within this model, we introduce a monotonicity assumption that leads to an intuitive classification of human responses to the algorithm. Under this monotonicity assumption, we can express the human's response to algorithmic recommendations in terms of their compliance with the algorithm and the decision they would take if the algorithm sends no recommendation. We showcase the utility of our framework using an online experiment that simulates a hiring task. We argue that our approach explains the relative performance of different recommendation algorithms in the experiment, and can help design solutions that realize human-AI complementarity.","sentences":["Algorithms frequently assist, rather than replace, human decision-makers.","However, the design and analysis of algorithms often focus on predicting outcomes and do not explicitly model their effect on human decisions.","This discrepancy between the design and role of algorithmic assistants becomes of particular concern in light of empirical evidence that suggests that algorithmic assistants again and again fail to improve human decisions.","In this article, we formalize the design of recommendation algorithms that assist human decision-makers without making restrictive ex-ante assumptions about how recommendations affect decisions.","We formulate an algorithmic-design problem that leverages the potential-outcomes framework from causal inference to model the effect of recommendations on a human decision-maker's binary treatment choice.","Within this model, we introduce a monotonicity assumption that leads to an intuitive classification of human responses to the algorithm.","Under this monotonicity assumption, we can express the human's response to algorithmic recommendations in terms of their compliance with the algorithm and the decision they would take if the algorithm sends no recommendation.","We showcase the utility of our framework using an online experiment that simulates a hiring task.","We argue that our approach explains the relative performance of different recommendation algorithms in the experiment, and can help design solutions that realize human-AI complementarity."],"url":"http://arxiv.org/abs/2405.01484v1","category":"cs.HC"}
{"created":"2024-05-02 17:11:55","title":"On Quantum Ambiguity and Potential Exponential Computational Speed-Ups to Solving","abstract":"We formulate quantum computing solutions to a large class of dynamic nonlinear asset pricing models using algorithms, in theory exponentially more efficient than classical ones, which leverage the quantum properties of superposition and entanglement. The equilibrium asset pricing solution is a quantum state. We introduce quantum decision-theoretic foundations of ambiguity and model/parameter uncertainty to deal with model selection.","sentences":["We formulate quantum computing solutions to a large class of dynamic nonlinear asset pricing models using algorithms, in theory exponentially more efficient than classical ones, which leverage the quantum properties of superposition and entanglement.","The equilibrium asset pricing solution is a quantum state.","We introduce quantum decision-theoretic foundations of ambiguity and model/parameter uncertainty to deal with model selection."],"url":"http://arxiv.org/abs/2405.01479v1","category":"q-fin.PR"}
{"created":"2024-05-02 16:28:18","title":"Data-driven analysis of the beauty hadron production in p+p collisions at the LHC with Bayesian unfolding","abstract":"Heavy flavour production in proton-proton (pp) collisions provides insights into the fundamental properties of Quantum Chromodynamics (QCD). Beauty hadron production measurements are widely performed through indirect approaches based on their inclusive decay modes. A Bayesian unfolding data-driven analysis of the ALICE and LHCb data was performed in this study, which recovers the full kinematic information of the beauty hadrons via different inclusive decay channels. The corresponding beauty hadron production cross sections obtained after the Bayesian unfolding are found to be consistent within their uncertainties. The weighted average open beauty production cross sections are presented as a function of the transverse momentum and rapidity in pp collisions at $\\sqrt{s}$ = 5.02 TeV and $\\sqrt{s}$ = 13 TeV, respectively. The $p_T$-integrated open beauty production $\\mathrm{d}\\sigma/\\mathrm{d}y$ and the total $\\mathrm{b}\\rm\\overline{b}$ cross section $\\sigma_{\\rm \\mathrm{b}\\rm\\overline{b}}$ are also reported. The precision of these results significantly improves upon worldwide measurements, providing valuable validation and constraints on mechanisms of heavy flavour production in pp collisions at the LHC energies.","sentences":["Heavy flavour production in proton-proton (pp) collisions provides insights into the fundamental properties of Quantum Chromodynamics (QCD).","Beauty hadron production measurements are widely performed through indirect approaches based on their inclusive decay modes.","A Bayesian unfolding data-driven analysis of the ALICE and LHCb data was performed in this study, which recovers the full kinematic information of the beauty hadrons via different inclusive decay channels.","The corresponding beauty hadron production cross sections obtained after the Bayesian unfolding are found to be consistent within their uncertainties.","The weighted average open beauty production cross sections are presented as a function of the transverse momentum and rapidity in pp collisions at $\\sqrt{s}$ = 5.02 TeV and $\\sqrt{s}$ = 13 TeV, respectively.","The $p_T$-integrated open beauty production $\\mathrm{d}\\sigma/\\mathrm{d}y$ and the total $\\mathrm{b}\\rm\\overline{b}$ cross section $\\sigma_{\\rm \\mathrm{b}\\rm\\overline{b}}$ are also reported.","The precision of these results significantly improves upon worldwide measurements, providing valuable validation and constraints on mechanisms of heavy flavour production in pp collisions at the LHC energies."],"url":"http://arxiv.org/abs/2405.01444v1","category":"nucl-ex"}
{"created":"2024-05-02 16:27:49","title":"Stability of the Poincar\u00e9-Korn inequality","abstract":"We resolve a question of Carrapatoso et al. on Gaussian optimality for the sharp constant in Poincar\\'e-Korn inequalities, under a moment constraint. We also prove stability, showing that measures with near-optimal constant are quantitatively close to standard Gaussian.","sentences":["We resolve a question of Carrapatoso et al. on Gaussian optimality for the sharp constant in Poincar\\'e-Korn inequalities, under a moment constraint.","We also prove stability, showing that measures with near-optimal constant are quantitatively close to standard Gaussian."],"url":"http://arxiv.org/abs/2405.01441v1","category":"math.AP"}
{"created":"2024-05-02 16:07:30","title":"New upper bound of muon neutrino mass in a short-baseline experiment","abstract":"In the paper Int.J.Mod.Phys.E 23 (2014) 1450004, the potential of short-baseline experiments was proposed to measure the mass (and parameters of Lorentz-violating effects) of the muon neutrino, where a roughly estimated upper bound of 420 eV was given as a possibility with large unknown uncertainties. In the present work, we improve upon this study by focusing on a feasible and improved experimental setup with today's technology, eliminating most large uncertainties, with the use of the Geant4 simulation toolkit. High-energy protons collide with a tungsten target, producing a variety of particles, most importantly pions that decay into muon neutrinos. The detector records the time of flight for both muon and anti-muon neutrinos, utilizing light as a reference signal. Additionally, it captures the energy deposited by neutrinos. By applying the dispersion relation, we determine the muon and/or anti-muon neutrino mass. Our improved results reveal a less optimistic but more accurate and realistic estimated upper bound of the muon neutrino mass, providing a new limit of about 150 keV. Notably, this finding is a factor of three lower than the best upper bound previously established in the literature originating from pion decay in flight.","sentences":["In the paper Int.","J.Mod.","Phys.","E 23 (2014) 1450004, the potential of short-baseline experiments was proposed to measure the mass (and parameters of Lorentz-violating effects) of the muon neutrino, where a roughly estimated upper bound of 420 eV was given as a possibility with large unknown uncertainties.","In the present work, we improve upon this study by focusing on a feasible and improved experimental setup with today's technology, eliminating most large uncertainties, with the use of the Geant4 simulation toolkit.","High-energy protons collide with a tungsten target, producing a variety of particles, most importantly pions that decay into muon neutrinos.","The detector records the time of flight for both muon and anti-muon neutrinos, utilizing light as a reference signal.","Additionally, it captures the energy deposited by neutrinos.","By applying the dispersion relation, we determine the muon and/or anti-muon neutrino mass.","Our improved results reveal a less optimistic but more accurate and realistic estimated upper bound of the muon neutrino mass, providing a new limit of about 150 keV.","Notably, this finding is a factor of three lower than the best upper bound previously established in the literature originating from pion decay in flight."],"url":"http://arxiv.org/abs/2405.01416v1","category":"hep-ph"}
{"created":"2024-05-02 15:54:46","title":"Random Pareto front surfaces","abstract":"The Pareto front of a set of vectors is the subset which is comprised solely of all of the best trade-off points. By interpolating this subset, we obtain the optimal trade-off surface. In this work, we prove a very useful result which states that all Pareto front surfaces can be explicitly parametrised using polar coordinates. In particular, our polar parametrisation result tells us that we can fully characterise any Pareto front surface using the length function, which is a scalar-valued function that returns the projected length along any positive radial direction. Consequently, by exploiting this representation, we show how it is possible to generalise many useful concepts from linear algebra, probability and statistics, and decision theory to function over the space of Pareto front surfaces. Notably, we focus our attention on the stochastic setting where the Pareto front surface itself is a stochastic process. Among other things, we showcase how it is possible to define and estimate many statistical quantities of interest such as the expectation, covariance and quantile of any Pareto front surface distribution. As a motivating example, we investigate how these statistics can be used within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Besides this, we also illustrate how these Pareto front ideas can be used within the context of extreme value theory. Finally, as a numerical example, we applied some of our new methodology on a real-world air pollution data set.","sentences":["The Pareto front of a set of vectors is the subset which is comprised solely of all of the best trade-off points.","By interpolating this subset, we obtain the optimal trade-off surface.","In this work, we prove a very useful result which states that all Pareto front surfaces can be explicitly parametrised using polar coordinates.","In particular, our polar parametrisation result tells us that we can fully characterise any Pareto front surface using the length function, which is a scalar-valued function that returns the projected length along any positive radial direction.","Consequently, by exploiting this representation, we show how it is possible to generalise many useful concepts from linear algebra, probability and statistics, and decision theory to function over the space of Pareto front surfaces.","Notably, we focus our attention on the stochastic setting where the Pareto front surface itself is a stochastic process.","Among other things, we showcase how it is possible to define and estimate many statistical quantities of interest such as the expectation, covariance and quantile of any Pareto front surface distribution.","As a motivating example, we investigate how these statistics can be used within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions.","Besides this, we also illustrate how these Pareto front ideas can be used within the context of extreme value theory.","Finally, as a numerical example, we applied some of our new methodology on a real-world air pollution data set."],"url":"http://arxiv.org/abs/2405.01404v1","category":"stat.ML"}
{"created":"2024-05-02 15:49:40","title":"Scalable Ab Initio Electronic Structure Methods with Near Chemical Accuracy for Main Group Chemistry","abstract":"This study evaluates the precision of widely recognized quantum chemical methodologies, CCSD(T), DLPNO-CCSD(T) and localized ph-AFQMC, for determining the thermochemistry of main group elements. DLPNO-CCSD(T) and localized ph-AFQMC, which offer greater scalability compared to canonical CCSD(T), have emerged over the last decade as pivotal in producing precise benchmark chemical data. Our investigation includes closed-shell, neutral molecules, focusing on their heat of formation and atomization energy sourced from four specific small molecule datasets. Firstly, we selected molecules from the G2 and G3 datasets, noted for their reliable experimental heat of formation data. Additionally, we incorporate molecules from the W4-11 and W4-17 sets, which provide high-level theoretical reference values for atomization energy at 0 K. Our findings reveal that both DLPNO-CCSD(T) and ph-AFQMC methods are capable of achieving a root-mean-square deviation (RMSD) of less than 1 kcal/mol across the combined dataset, aligning with the threshold for chemical accuracy. Moreover, we make efforts to confine the maximum deviations within 2 kcal/mol, a degree of precision that significantly broadens the applicability of these methods in fields such as biology and materials science.","sentences":["This study evaluates the precision of widely recognized quantum chemical methodologies, CCSD(T), DLPNO-CCSD(T) and localized ph-AFQMC, for determining the thermochemistry of main group elements.","DLPNO-CCSD(T) and localized ph-AFQMC, which offer greater scalability compared to canonical CCSD(T), have emerged over the last decade as pivotal in producing precise benchmark chemical data.","Our investigation includes closed-shell, neutral molecules, focusing on their heat of formation and atomization energy sourced from four specific small molecule datasets.","Firstly, we selected molecules from the G2 and G3 datasets, noted for their reliable experimental heat of formation data.","Additionally, we incorporate molecules from the W4-11 and W4-17 sets, which provide high-level theoretical reference values for atomization energy at 0","K. Our findings reveal that both DLPNO-CCSD(T) and ph-AFQMC methods are capable of achieving a root-mean-square deviation (RMSD) of less than 1 kcal/mol across the combined dataset, aligning with the threshold for chemical accuracy.","Moreover, we make efforts to confine the maximum deviations within 2 kcal/mol, a degree of precision that significantly broadens the applicability of these methods in fields such as biology and materials science."],"url":"http://arxiv.org/abs/2405.01400v1","category":"physics.chem-ph"}
{"created":"2024-05-02 15:42:02","title":"A Basic Overview of Various Stochastic Approaches to Financial Modeling With Examples","abstract":"This paper explores stochastic modeling approaches to elucidate the intricate dynamics of stock prices and volatility in financial markets. Beginning with an overview of Brownian motion and its historical significance in finance, we delve into various stochastic models, including the classic Black-Scholes framework, the Heston model, fractional Brownian motion, GARCH models, and Levy processes. Through a thorough investigation, we analyze the strengths and limitations of each model in capturing key features of financial time series data. Our empirical analysis focuses on parameter estimation and model calibration using Levy processes, demonstrating their effectiveness in predicting stock returns. However, we acknowledge the need for further refinement and exploration, suggesting potential avenues for future research, such as hybrid modeling approaches. Overall, this study underscores the importance of stochastic modeling in understanding market dynamics and informs decision-making in the financial industry.","sentences":["This paper explores stochastic modeling approaches to elucidate the intricate dynamics of stock prices and volatility in financial markets.","Beginning with an overview of Brownian motion and its historical significance in finance, we delve into various stochastic models, including the classic Black-Scholes framework, the Heston model, fractional Brownian motion, GARCH models, and Levy processes.","Through a thorough investigation, we analyze the strengths and limitations of each model in capturing key features of financial time series data.","Our empirical analysis focuses on parameter estimation and model calibration using Levy processes, demonstrating their effectiveness in predicting stock returns.","However, we acknowledge the need for further refinement and exploration, suggesting potential avenues for future research, such as hybrid modeling approaches.","Overall, this study underscores the importance of stochastic modeling in understanding market dynamics and informs decision-making in the financial industry."],"url":"http://arxiv.org/abs/2405.01397v1","category":"math.HO"}
{"created":"2024-05-02 15:14:16","title":"Statistical algorithms for low-frequency diffusion data: A PDE approach","abstract":"We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors. Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.","sentences":["We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data.","Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques.","In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation.","Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood.","Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods.","This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates.","We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors.","Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups.","The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion."],"url":"http://arxiv.org/abs/2405.01372v1","category":"stat.ME"}
{"created":"2024-05-02 14:45:04","title":"Dynamic opinion updating with endogenous networks","abstract":"Polarization is a well-documented phenomenon across a wide range of social issues. However, prevailing theories often compartmentalize the examination of herding behavior and opinion convergence within different contexts. In this study, we delve into the micro-foundations of how individuals strategically select reference groups, offering insight into a dynamic process where both individual opinions and the network evolve simultaneously. We base our model on two parameters: people's direct benefit from connections and their adaptability in adjusting their opinions. Our research highlights which conditions impede the network from achieving complete connectivity, resulting in enduring polarization. Notably, our model also reveals that polarization can transiently emerge during the transition towards consensus. We explore the connection between these scenarios and a critical network metric: the initial diameter, under specific conditions related to the initial distribution of opinions.","sentences":["Polarization is a well-documented phenomenon across a wide range of social issues.","However, prevailing theories often compartmentalize the examination of herding behavior and opinion convergence within different contexts.","In this study, we delve into the micro-foundations of how individuals strategically select reference groups, offering insight into a dynamic process where both individual opinions and the network evolve simultaneously.","We base our model on two parameters: people's direct benefit from connections and their adaptability in adjusting their opinions.","Our research highlights which conditions impede the network from achieving complete connectivity, resulting in enduring polarization.","Notably, our model also reveals that polarization can transiently emerge during the transition towards consensus.","We explore the connection between these scenarios and a critical network metric: the initial diameter, under specific conditions related to the initial distribution of opinions."],"url":"http://arxiv.org/abs/2405.01341v1","category":"econ.TH"}
{"created":"2024-05-02 14:43:21","title":"Multi-view Action Recognition via Directed Gromov-Wasserstein Discrepancy","abstract":"Action recognition has become one of the popular research topics in computer vision. There are various methods based on Convolutional Networks and self-attention mechanisms as Transformers to solve both spatial and temporal dimensions problems of action recognition tasks that achieve competitive performances. However, these methods lack a guarantee of the correctness of the action subject that the models give attention to, i.e., how to ensure an action recognition model focuses on the proper action subject to make a reasonable action prediction. In this paper, we propose a multi-view attention consistency method that computes the similarity between two attentions from two different views of the action videos using Directed Gromov-Wasserstein Discrepancy. Furthermore, our approach applies the idea of Neural Radiance Field to implicitly render the features from novel views when training on single-view datasets. Therefore, the contributions in this work are three-fold. Firstly, we introduce the multi-view attention consistency to solve the problem of reasonable prediction in action recognition. Secondly, we define a new metric for multi-view consistent attention using Directed Gromov-Wasserstein Discrepancy. Thirdly, we built an action recognition model based on Video Transformers and Neural Radiance Fields. Compared to the recent action recognition methods, the proposed approach achieves state-of-the-art results on three large-scale datasets, i.e., Jester, Something-Something V2, and Kinetics-400.","sentences":["Action recognition has become one of the popular research topics in computer vision.","There are various methods based on Convolutional Networks and self-attention mechanisms as Transformers to solve both spatial and temporal dimensions problems of action recognition tasks that achieve competitive performances.","However, these methods lack a guarantee of the correctness of the action subject that the models give attention to, i.e., how to ensure an action recognition model focuses on the proper action subject to make a reasonable action prediction.","In this paper, we propose a multi-view attention consistency method that computes the similarity between two attentions from two different views of the action videos using Directed Gromov-Wasserstein Discrepancy.","Furthermore, our approach applies the idea of Neural Radiance Field to implicitly render the features from novel views when training on single-view datasets.","Therefore, the contributions in this work are three-fold.","Firstly, we introduce the multi-view attention consistency to solve the problem of reasonable prediction in action recognition.","Secondly, we define a new metric for multi-view consistent attention using Directed Gromov-Wasserstein Discrepancy.","Thirdly, we built an action recognition model based on Video Transformers and Neural Radiance Fields.","Compared to the recent action recognition methods, the proposed approach achieves state-of-the-art results on three large-scale datasets, i.e., Jester, Something-Something V2, and Kinetics-400."],"url":"http://arxiv.org/abs/2405.01337v1","category":"cs.CV"}
{"created":"2024-05-02 14:31:52","title":"Constrained Reinforcement Learning Under Model Mismatch","abstract":"Existing studies on constrained reinforcement learning (RL) may obtain a well-performing policy in the training environment. However, when deployed in a real environment, it may easily violate constraints that were originally satisfied during training because there might be model mismatch between the training and real environments. To address the above challenge, we formulate the problem as constrained RL under model uncertainty, where the goal is to learn a good policy that optimizes the reward and at the same time satisfy the constraint under model mismatch. We develop a Robust Constrained Policy Optimization (RCPO) algorithm, which is the first algorithm that applies to large/continuous state space and has theoretical guarantees on worst-case reward improvement and constraint violation at each iteration during the training. We demonstrate the effectiveness of our algorithm on a set of RL tasks with constraints.","sentences":["Existing studies on constrained reinforcement learning (RL) may obtain a well-performing policy in the training environment.","However, when deployed in a real environment, it may easily violate constraints that were originally satisfied during training because there might be model mismatch between the training and real environments.","To address the above challenge, we formulate the problem as constrained RL under model uncertainty, where the goal is to learn a good policy that optimizes the reward and at the same time satisfy the constraint under model mismatch.","We develop a Robust Constrained Policy Optimization (RCPO) algorithm, which is the first algorithm that applies to large/continuous state space and has theoretical guarantees on worst-case reward improvement and constraint violation at each iteration during the training.","We demonstrate the effectiveness of our algorithm on a set of RL tasks with constraints."],"url":"http://arxiv.org/abs/2405.01327v1","category":"cs.LG"}
{"created":"2024-05-02 14:23:08","title":"The odd-even differences in stability peninsula for $106 \\leqslant Z \\leqslant 112$ region with the deformed relativistic Hartree-Bogoliubov theory in continuum","abstract":"The predictive power of the deformed relativistic Hartree-Bogoliubov theory in continuum (DRHBc) with density functional PC-PK1 is demonstrated for superheavy region ($101 \\leqslant Z \\leqslant 120$) by comparing with available experimental and empirical mass data in the AME2020. The DRHBc theory predicts 93 bound nuclei beyond the drip line $N = 258$ in the region of $106 \\leqslant Z \\leqslant 112$, which form a stability peninsula. The odd-even differences between odd-$N$ and even-$N$ nuclei are remarkable in the stability peninsula; the number of bound odd-$N$ nuclei is less than that of bound even-$N$ nuclei, and the one-neutron separation energy of an odd-$N$ nucleus is smaller than those of its neighboring even-$N$ nuclei due to the blocking effect. The deformation effect is indispensable for the reentrant stability beyond the drip line by significantly affecting the structure of single-particle levels around the Fermi energy. \\textbf{\\textcolor{red}{The interplay between deformation and pairing effects affects the position where the odd-$N$ nucleus becomes bound in the stability peninsula.}} By examining the deformation effect at different orders, it is found that \\textbf{\\textcolor{red}{quadrupole deformation makes leading contribution to the appearance of stability peninsula and the effects of hexadecapole and hexacontatetrapole deformations are nonnegligible.","sentences":["The predictive power of the deformed relativistic Hartree-Bogoliubov theory in continuum (DRHBc) with density functional PC-PK1 is demonstrated for superheavy region ($101 \\leqslant Z \\leqslant 120$) by comparing with available experimental and empirical mass data in the AME2020.","The DRHBc theory predicts 93 bound nuclei beyond the drip line $N = 258$ in the region of $106 \\leqslant Z \\leqslant 112$, which form a stability peninsula.","The odd-even differences between odd-$N$ and even-$N$ nuclei are remarkable in the stability peninsula; the number of bound odd-$N$ nuclei is less than that of bound even-$N$ nuclei, and the one-neutron separation energy of an odd-$N$ nucleus is smaller than those of its neighboring even-$N$ nuclei due to the blocking effect.","The deformation effect is indispensable for the reentrant stability beyond the drip line by significantly affecting the structure of single-particle levels around the Fermi energy.","\\textbf{\\textcolor{red}{The interplay between deformation and pairing effects affects the position where the odd-$N$ nucleus becomes bound in the stability peninsula.}}","By examining the deformation effect at different orders, it is found that \\textbf{\\textcolor{red}{quadrupole deformation makes leading contribution to the appearance of stability peninsula and the effects of hexadecapole and hexacontatetrapole deformations are nonnegligible."],"url":"http://arxiv.org/abs/2405.01317v1","category":"nucl-th"}
{"created":"2024-05-02 14:06:31","title":"Controlling Communications Quality in V2V Platooning: a TSN-like Slot-Based Scheduler Approach","abstract":"Connected vehicles, facilitated by Vehicle-to-Vehicle (V2V) communications, play a key role in enhancing road safety and traffic efficiency. However, V2V communications primarily rely on wireless protocols, such as Wi-Fi, that require additional collision avoidance mechanisms to better ensure bounded latency and reliability in critical scenarios. In this paper, we introduce a novel approach to address the challenge of message collision in V2V platooning through a slotted-based solution inspired by Time-Sensitive Networking (TSN), which is gaining momentum for in-vehicle networks. To this end, we present a controller, named TSNCtl, operating at the application level of the vehicular communications stack. TSNCtl employs a finite state machine (FSM) to manage platoon formation and slot-based scheduling for message dissemination. The reported evaluation results, based on the OMNeT++ simulation framework and INET library, demonstrate the effectiveness of TSNCtl in reducing packet collisions across various scenarios. Specifically, our experiments reveal a significant reduction in packet collisions compared to the CSMA-CA baseline used in traditional Wi-Fi-based protocols (e.g., IEEE 802.11p): for instance, with slot lengths of 2 ms, our solution achieves an average collision rate under 1%, compared to up to 50% for the baseline case.","sentences":["Connected vehicles, facilitated by Vehicle-to-Vehicle (V2V) communications, play a key role in enhancing road safety and traffic efficiency.","However, V2V communications primarily rely on wireless protocols, such as Wi-Fi, that require additional collision avoidance mechanisms to better ensure bounded latency and reliability in critical scenarios.","In this paper, we introduce a novel approach to address the challenge of message collision in V2V platooning through a slotted-based solution inspired by Time-Sensitive Networking (TSN), which is gaining momentum for in-vehicle networks.","To this end, we present a controller, named TSNCtl, operating at the application level of the vehicular communications stack.","TSNCtl employs a finite state machine (FSM) to manage platoon formation and slot-based scheduling for message dissemination.","The reported evaluation results, based on the OMNeT++ simulation framework and INET library, demonstrate the effectiveness of TSNCtl in reducing packet collisions across various scenarios.","Specifically, our experiments reveal a significant reduction in packet collisions compared to the CSMA-CA baseline used in traditional Wi-Fi-based protocols (e.g., IEEE 802.11p): for instance, with slot lengths of 2 ms, our solution achieves an average collision rate under 1%, compared to up to 50% for the baseline case."],"url":"http://arxiv.org/abs/2405.01301v1","category":"cs.NI"}
{"created":"2024-05-02 13:56:11","title":"Merging N-hyperideals and J-hyperideals in one frame","abstract":"The notions of N-hyperideals and J-hyperideals as two classes of hyperideals were recently defined in the context of Krasner (m,n)-hyperrings. These concepts are created on the basis of the intersection of all n-ary prime hyperideals and the intersection of all maximal hyperideals, respectively. Despite being vastly different in many aspects, they shar numerous similar properties. The aim of this research work is to merge them under one frame called n-ary delta(0)-hyperideals where the function delta assigns to each hyperideals of a Krasner (m,n)-hyperring a hyperideal of the same hyperring. We give various properties of n-ary delta(0)-hyperideals and use them to characerize certain classes of hyperring such as hyperintegral domains and local hyperrings. Moreover, we introduce the notions of (s,n)-absorbing delta(0)-hyperideals and weakly (s,n)-absorbing delta(0)-hyperideals.","sentences":["The notions of N-hyperideals and J-hyperideals as two classes of hyperideals were recently defined in the context of Krasner (m,n)-hyperrings.","These concepts are created on the basis of the intersection of all n-ary prime hyperideals and the intersection of all maximal hyperideals, respectively.","Despite being vastly different in many aspects, they shar numerous similar properties.","The aim of this research work is to merge them under one frame called n-ary delta(0)-hyperideals where the function delta assigns to each hyperideals of a Krasner (m,n)-hyperring a hyperideal of the same hyperring.","We give various properties of n-ary delta(0)-hyperideals and use them to characerize certain classes of hyperring such as hyperintegral domains and local hyperrings.","Moreover, we introduce the notions of (s,n)-absorbing delta(0)-hyperideals and weakly (s,n)-absorbing delta(0)-hyperideals."],"url":"http://arxiv.org/abs/2405.01294v1","category":"math.AC"}
{"created":"2024-05-02 13:49:43","title":"Proposal for PAC 52: Measurement of $\u03b1_-$ for $\u039b\\rightarrow p\u03c0^-$","abstract":"We propose to measure the weak decay constant $\\alpha_-$ for the decay $\\Lambda\\rightarrow p\\pi^-$ using a both circularly and linearly polarized photon beam with the GlueX spectrometer in Hall D. The measurement will take advantage of the fact that a measurement with both linear and circular photon beam polarization results in an over-constrained set of amplitudes which can be fitted to data and used to extract $\\alpha_-$ which will be left as a free parameter in the fit. We expect to determine $\\alpha_-$ with statistical uncertainties comparable to existing measurements and independent systematic uncertainties. This measurement can be performed alongside GlueX-II running and requires no new hardware or new beam time. The measurement requires that a sufficient fraction of the electron beam polarization be longitudinal in the Hall D tagger.","sentences":["We propose to measure the weak decay constant $\\alpha_-$ for the decay $\\Lambda\\rightarrow p\\pi^-$ using a both circularly and linearly polarized photon beam with the GlueX spectrometer in Hall D.","The measurement will take advantage of the fact that a measurement with both linear and circular photon beam polarization results in an over-constrained set of amplitudes which can be fitted to data and used to extract $\\alpha_-$ which will be left as a free parameter in the fit.","We expect to determine $\\alpha_-$ with statistical uncertainties comparable to existing measurements and independent systematic uncertainties.","This measurement can be performed alongside GlueX-II running and requires no new hardware or new beam time.","The measurement requires that a sufficient fraction of the electron beam polarization be longitudinal in the Hall D tagger."],"url":"http://arxiv.org/abs/2405.01288v1","category":"nucl-ex"}
{"created":"2024-05-02 13:32:55","title":"Variable Selection in Ultra-high Dimensional Feature Space for the Cox Model with Interval-Censored Data","abstract":"We develop a set of variable selection methods for the Cox model under interval censoring, in the ultra-high dimensional setting where the dimensionality can grow exponentially with the sample size. The methods select covariates via a penalized nonparametric maximum likelihood estimation with some popular penalty functions, including lasso, adaptive lasso, SCAD, and MCP. We prove that our penalized variable selection methods with folded concave penalties or adaptive lasso penalty enjoy the oracle property. Extensive numerical experiments show that the proposed methods have satisfactory empirical performance under various scenarios. The utility of the methods is illustrated through an application to a genome-wide association study of age to early childhood caries.","sentences":["We develop a set of variable selection methods for the Cox model under interval censoring, in the ultra-high dimensional setting where the dimensionality can grow exponentially with the sample size.","The methods select covariates via a penalized nonparametric maximum likelihood estimation with some popular penalty functions, including lasso, adaptive lasso, SCAD, and MCP.","We prove that our penalized variable selection methods with folded concave penalties or adaptive lasso penalty enjoy the oracle property.","Extensive numerical experiments show that the proposed methods have satisfactory empirical performance under various scenarios.","The utility of the methods is illustrated through an application to a genome-wide association study of age to early childhood caries."],"url":"http://arxiv.org/abs/2405.01275v1","category":"stat.ME"}
{"created":"2024-05-02 12:27:56","title":"Solar-Blind QKD over Simplified Short-Range FSO Link","abstract":"We demonstrate QKD and data communication over an out-door free-space link where large-core fiber substitutes active alignment. We further prove E-band QKD as stable and robust under full daylight, despite the loss of spatial filtering.","sentences":["We demonstrate QKD and data communication over an out-door free-space link where large-core fiber substitutes active alignment.","We further prove E-band QKD as stable and robust under full daylight, despite the loss of spatial filtering."],"url":"http://arxiv.org/abs/2405.01236v1","category":"quant-ph"}
{"created":"2024-05-02 12:03:57","title":"Misspecification of Multiple Scattering in Scalar Wave Fields and its Impact in Ultrasound Tomography","abstract":"In this work, we investigate the localization of targets in the presence of multiple scattering. We focus on the often omitted scenario in which measurement data is affected by multiple scattering, and a simpler model is employed in the estimation. We study the impact of such model mismatch by means of the Misspecified Cram\\'er-Rao Bound (MCRB). In numerical simulations inspired by tomographic inspection in ultrasound nondestructive testing, the MCRB is shown to correctly describe the estimation variance of localization parameters under misspecification of the wave propagation model. We provide extensive discussion on the utility of the MCRB in the practical task of verifying whether a chosen misspecified model is suitable for localization based on the properties of the maximum likelihood estimator and the nuanced distinction between bias and parameter space differences. Finally, we highlight that careful interpretation is needed whenever employing the classical CRB in the presence of mismatch through numerical examples based on the Born approximation and other simplified propagation models stemming from it.","sentences":["In this work, we investigate the localization of targets in the presence of multiple scattering.","We focus on the often omitted scenario in which measurement data is affected by multiple scattering, and a simpler model is employed in the estimation.","We study the impact of such model mismatch by means of the Misspecified Cram\\'er-Rao Bound (MCRB).","In numerical simulations inspired by tomographic inspection in ultrasound nondestructive testing, the MCRB is shown to correctly describe the estimation variance of localization parameters under misspecification of the wave propagation model.","We provide extensive discussion on the utility of the MCRB in the practical task of verifying whether a chosen misspecified model is suitable for localization based on the properties of the maximum likelihood estimator and the nuanced distinction between bias and parameter space differences.","Finally, we highlight that careful interpretation is needed whenever employing the classical CRB in the presence of mismatch through numerical examples based on the Born approximation and other simplified propagation models stemming from it."],"url":"http://arxiv.org/abs/2405.01220v1","category":"eess.SP"}
{"created":"2024-05-02 11:48:16","title":"Posterior contraction rates in a sparse non-linear mixed-effects model","abstract":"Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints. However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting. The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model. In this model, the residual variance is assumed to be known, while the covariance matrix of the random effects and the regression vector are unknown and must be estimated. The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects. First, the effective dimension of this model is bounded with high posterior probabilities. Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector. Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at the same rate as observed in the linear case.","sentences":["Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints.","However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting.","The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model.","In this model, the residual variance is assumed to be known, while the covariance matrix of the random effects and the regression vector are unknown and must be estimated.","The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects.","First, the effective dimension of this model is bounded with high posterior probabilities.","Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector.","Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at the same rate as observed in the linear case."],"url":"http://arxiv.org/abs/2405.01206v1","category":"math.ST"}
{"created":"2024-05-02 11:48:14","title":"Error-Driven Uncertainty Aware Training","abstract":"Neural networks are often overconfident about their predictions, which undermines their reliability and trustworthiness. In this work, we present a novel technique, named Error-Driven Uncertainty Aware Training (EUAT), which aims to enhance the ability of neural models to estimate their uncertainty correctly, namely to be highly uncertain when they output inaccurate predictions and low uncertain when their output is accurate. The EUAT approach operates during the model's training phase by selectively employing two loss functions depending on whether the training examples are correctly or incorrectly predicted by the model. This allows for pursuing the twofold goal of i) minimizing model uncertainty for correctly predicted inputs and ii) maximizing uncertainty for mispredicted inputs, while preserving the model's misprediction rate. We evaluate EUAT using diverse neural models and datasets in the image recognition domains considering both non-adversarial and adversarial settings. The results show that EUAT outperforms existing approaches for uncertainty estimation (including other uncertainty-aware training techniques, calibration, ensembles, and DEUP) by providing uncertainty estimates that not only have higher quality when evaluated via statistical metrics (e.g., correlation with residuals) but also when employed to build binary classifiers that decide whether the model's output can be trusted or not and under distributional data shifts.","sentences":["Neural networks are often overconfident about their predictions, which undermines their reliability and trustworthiness.","In this work, we present a novel technique, named Error-Driven Uncertainty Aware Training (EUAT), which aims to enhance the ability of neural models to estimate their uncertainty correctly, namely to be highly uncertain when they output inaccurate predictions and low uncertain when their output is accurate.","The EUAT approach operates during the model's training phase by selectively employing two loss functions depending on whether the training examples are correctly or incorrectly predicted by the model.","This allows for pursuing the twofold goal of i) minimizing model uncertainty for correctly predicted inputs and ii) maximizing uncertainty for mispredicted inputs, while preserving the model's misprediction rate.","We evaluate EUAT using diverse neural models and datasets in the image recognition domains considering both non-adversarial and adversarial settings.","The results show that EUAT outperforms existing approaches for uncertainty estimation (including other uncertainty-aware training techniques, calibration, ensembles, and DEUP) by providing uncertainty estimates that not only have higher quality when evaluated via statistical metrics (e.g., correlation with residuals) but also when employed to build binary classifiers that decide whether the model's output can be trusted or not and under distributional data shifts."],"url":"http://arxiv.org/abs/2405.01205v1","category":"cs.LG"}
{"created":"2024-05-02 11:45:34","title":"Optimization of reactively sputtered Mn3GaN films based on resistivity measurements","abstract":"Mn-based nitrides with antiperovskite structures have several properties that can be utilised for antiferromagnetic spintronics. Their magnetic properties depend on the structural quality, composition and doping of the cubic antiperovskite structure. Such nitride thin films are usually produced by reactive physical vapour deposition, where the deposition rate of N can only be controlled by the N2 gas flow. We show that the tuning of the N content can be optimised using low temperature resistivity measurements, which serve as an indicator of the degree of structural disorder. Several Mn3GaNx films were prepared by reactive magnetron sputtering under different N2 gas flows. Under optimised conditions, we obtain films that exhibit a metal-like temperature dependence, a vanishing logarithmic increase in resistivity towards zero, the highest resistivity ratio and a lattice contraction of 0.4 % along the growth direction when heated above that of the N\\'eel temperature in agreement with the bulk samples.","sentences":["Mn-based nitrides with antiperovskite structures have several properties that can be utilised for antiferromagnetic spintronics.","Their magnetic properties depend on the structural quality, composition and doping of the cubic antiperovskite structure.","Such nitride thin films are usually produced by reactive physical vapour deposition, where the deposition rate of N can only be controlled by the N2 gas flow.","We show that the tuning of the N content can be optimised using low temperature resistivity measurements, which serve as an indicator of the degree of structural disorder.","Several Mn3GaNx films were prepared by reactive magnetron sputtering under different N2 gas flows.","Under optimised conditions, we obtain films that exhibit a metal-like temperature dependence, a vanishing logarithmic increase in resistivity towards zero, the highest resistivity ratio and a lattice contraction of 0.4 % along the growth direction when heated above that of the N\\'eel temperature in agreement with the bulk samples."],"url":"http://arxiv.org/abs/2405.01203v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 11:00:23","title":"Signature decay modes of the compact doubly-heavy tetraquarks $T_{bb\\bar{u} \\bar{d}}$ and $T_{bc\\bar{u} \\bar{d}}$","abstract":"Based on the expectations that the lowest-lying double-bottom tetraquark $T_{bb\\bar u \\bar d}$ ($J^P=1^+$) and the bottom-charm tetraquark $T_{bc\\bar u \\bar d}$ ($J^P=0^+$) are stable against strong and electromagnetic decays, we work out a number of semileptonic and non-leptonic weak decays of these hadrons, making use of the heavy quark symmetry. In doing this, we concentrate on the exclusive decays involving also tetraquarks in the final states, i.e., transitions such as $T_{bb\\bar u \\bar d} \\to T_{bc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$ and $T_{bc\\bar u \\bar d} \\to T_{cc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$, where $h^-=\\pi^-,\\rho^-,a_1^-$. So far, only the $J^P=1^+$ tetraquark $T_{cc\\bar u \\bar d}$ has been discovered, which we identify with the $I=0$ $T_{cc}^{+}$ object, compatible with $J^P=1^+$ and having the mass and decay widths $\\delta m =M(T_{cc}^+) -(M(D^{*+}) - M(D^0))= -360 \\pm 40 ^{+4}_{-0}$ keV and $\\Gamma(T_{cc}^+)=48^{+2}_{-14}$ keV. Experimental discoveries of the transitions worked out here will go a long way in establishing the nature of these tetraquarks as (mainly) compact four-quark objects.","sentences":["Based on the expectations that the lowest-lying double-bottom tetraquark $T_{bb\\bar u \\bar d}$ ($J^P=1^+$) and the bottom-charm tetraquark $T_{bc\\bar u \\bar d}$ ($J^P=0^+$) are stable against strong and electromagnetic decays, we work out a number of semileptonic and non-leptonic weak decays of these hadrons, making use of the heavy quark symmetry.","In doing this, we concentrate on the exclusive decays involving also tetraquarks in the final states, i.e., transitions such as $T_{bb\\bar u \\bar d} \\to T_{bc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$ and $T_{bc\\bar u \\bar d} \\to T_{cc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$, where $h^-=\\pi^-,\\rho^-,a_1^-$.","So far, only the $J^P=1^+$ tetraquark $T_{cc\\bar u \\bar d}$ has been discovered, which we identify with the $I=0$ $T_{cc}^{+}$ object, compatible with $J^P=1^+$ and having the mass and decay widths $\\delta m =M(T_{cc}^+) -(M(D^{*+}) -","M(D^0))= -360 \\pm 40 ^{+4}_{-0}$ keV and $\\Gamma(T_{cc}^+)=48^{+2}_{-14}$ keV.","Experimental discoveries of the transitions worked out here will go a long way in establishing the nature of these tetraquarks as (mainly) compact four-quark objects."],"url":"http://arxiv.org/abs/2405.01173v1","category":"hep-ph"}
{"created":"2024-05-02 10:07:50","title":"Performance Analysis of Reconfigurable Holographic Surfaces in the Near-Field Scenario of Cell-Free Networks Under Hardware Impairments","abstract":"We propose a hybrid beamforming architecture for near-field reconfigurable holographic surfaces (RHS) harnessed in cell-free networks. Specifically, the holographic beamformer of each base station (BS) is designed for maximizing the channel gain based on the local channel state information (CSI). By contrast, the digital beamformer at the central processing unit is designed based on the minimum mean squared error criterion. Furthermore, the near-field spectral efficiency of the RHS in cell-free networks is derived theoretically by harnessing the popular stochastic geometry approach. We consider both the phase shift error (PSE) at the RHS elements and the hardware impairment (HWI) at the radio frequency (RF) chains of the transceivers. Furthermore, we theoretically derive the asymptotic capacity bound, when considering an infinite physical size for the RHS in the near-field channel model. The theoretical analysis and simulation results show that the PSE at the RHS elements and the HWI at the RF chains of transceivers limit the spectral efficiency in the high signal-to-noise ratio region. Moreover, we show that the PSE at the RHS elements and the HWI at the RF chains of BSs can be compensated by increasing the number of BSs. Finally, we also demonstrate that the ergodic spectral efficiency based on the near-field channel model is higher than that based on the far-field channel model assumption.","sentences":["We propose a hybrid beamforming architecture for near-field reconfigurable holographic surfaces (RHS) harnessed in cell-free networks.","Specifically, the holographic beamformer of each base station (BS) is designed for maximizing the channel gain based on the local channel state information (CSI).","By contrast, the digital beamformer at the central processing unit is designed based on the minimum mean squared error criterion.","Furthermore, the near-field spectral efficiency of the RHS in cell-free networks is derived theoretically by harnessing the popular stochastic geometry approach.","We consider both the phase shift error (PSE) at the RHS elements and the hardware impairment (HWI) at the radio frequency (RF) chains of the transceivers.","Furthermore, we theoretically derive the asymptotic capacity bound, when considering an infinite physical size for the RHS in the near-field channel model.","The theoretical analysis and simulation results show that the PSE at the RHS elements and the HWI at the RF chains of transceivers limit the spectral efficiency in the high signal-to-noise ratio region.","Moreover, we show that the PSE at the RHS elements and the HWI at the RF chains of BSs can be compensated by increasing the number of BSs.","Finally, we also demonstrate that the ergodic spectral efficiency based on the near-field channel model is higher than that based on the far-field channel model assumption."],"url":"http://arxiv.org/abs/2405.01150v1","category":"cs.IT"}
{"created":"2024-05-02 09:41:31","title":"Detecting and clustering swallow events in esophageal long-term high-resolution manometry","abstract":"High-resolution manometry (HRM) is the gold standard in diagnosing esophageal motility disorders. As HRM is typically conducted under short-term laboratory settings, intermittently occurring disorders are likely to be missed. Therefore, long-term (up to 24h) HRM (LTHRM) is used to gain detailed insights into the swallowing behavior. However, analyzing the extensive data from LTHRM is challenging and time consuming as medical experts have to analyze the data manually, which is slow and prone to errors. To address this challenge, we propose a Deep Learning based swallowing detection method to accurately identify swallowing events and secondary non-deglutitive-induced esophageal motility disorders in LTHRM data. We then proceed with clustering the identified swallows into distinct classes, which are analyzed by highly experienced clinicians to validate the different swallowing patterns. We evaluate our computational pipeline on a total of 25 LTHRMs, which were meticulously annotated by medical experts. By detecting more than 94% of all relevant swallow events and providing all relevant clusters for a more reliable diagnostic process among experienced clinicians, we are able to demonstrate the effectiveness as well as positive clinical impact of our approach to make LTHRM feasible in clinical care.","sentences":["High-resolution manometry (HRM) is the gold standard in diagnosing esophageal motility disorders.","As HRM is typically conducted under short-term laboratory settings, intermittently occurring disorders are likely to be missed.","Therefore, long-term (up to 24h) HRM (LTHRM) is used to gain detailed insights into the swallowing behavior.","However, analyzing the extensive data from LTHRM is challenging and time consuming as medical experts have to analyze the data manually, which is slow and prone to errors.","To address this challenge, we propose a Deep Learning based swallowing detection method to accurately identify swallowing events and secondary non-deglutitive-induced esophageal motility disorders in LTHRM data.","We then proceed with clustering the identified swallows into distinct classes, which are analyzed by highly experienced clinicians to validate the different swallowing patterns.","We evaluate our computational pipeline on a total of 25 LTHRMs, which were meticulously annotated by medical experts.","By detecting more than 94% of all relevant swallow events and providing all relevant clusters for a more reliable diagnostic process among experienced clinicians, we are able to demonstrate the effectiveness as well as positive clinical impact of our approach to make LTHRM feasible in clinical care."],"url":"http://arxiv.org/abs/2405.01126v1","category":"cs.CV"}
{"created":"2024-05-02 09:28:53","title":"A Survey of the Overlooked Dangers of Template Engines","abstract":"Template engines play a pivotal role in modern web application development, facilitating the dynamic rendering of content, products, and user interfaces. Nowadays, template engines are essential in any website that deals with dynamic data, from e-commerce platforms to social media. However, their widespread use also makes them attractive targets for attackers seeking to exploit vulnerabilities and gain unauthorized access to web servers. This paper presents a comprehensive survey of template engines, focusing on their susceptibility to Remote Code Execution (RCE) attacks, a critical security concern in web application development.","sentences":["Template engines play a pivotal role in modern web application development, facilitating the dynamic rendering of content, products, and user interfaces.","Nowadays, template engines are essential in any website that deals with dynamic data, from e-commerce platforms to social media.","However, their widespread use also makes them attractive targets for attackers seeking to exploit vulnerabilities and gain unauthorized access to web servers.","This paper presents a comprehensive survey of template engines, focusing on their susceptibility to Remote Code Execution (RCE) attacks, a critical security concern in web application development."],"url":"http://arxiv.org/abs/2405.01118v1","category":"cs.CR"}
{"created":"2024-05-02 09:22:54","title":"Continual Imitation Learning for Prosthetic Limbs","abstract":"Lower limb amputations and neuromuscular impairments severely restrict mobility, necessitating advancements beyond conventional prosthetics. Motorized bionic limbs offer promise, but their utility depends on mimicking the evolving synergy of human movement in various settings. In this context, we present a novel model for bionic prostheses' application that leverages camera-based motion capture and wearable sensor data, to learn the synergistic coupling of the lower limbs during human locomotion, empowering it to infer the kinematic behavior of a missing lower limb across varied tasks, such as climbing inclines and stairs. We propose a model that can multitask, adapt continually, anticipate movements, and refine. The core of our method lies in an approach which we call -- multitask prospective rehearsal -- that anticipates and synthesizes future movements based on the previous prediction and employs a corrective mechanism for subsequent predictions. We design an evolving architecture that merges lightweight, task-specific modules on a shared backbone, ensuring both specificity and scalability. We empirically validate our model against various baselines using real-world human gait datasets, including experiments with transtibial amputees, which encompass a broad spectrum of locomotion tasks. The results show that our approach consistently outperforms baseline models, particularly under scenarios affected by distributional shifts, adversarial perturbations, and noise.","sentences":["Lower limb amputations and neuromuscular impairments severely restrict mobility, necessitating advancements beyond conventional prosthetics.","Motorized bionic limbs offer promise, but their utility depends on mimicking the evolving synergy of human movement in various settings.","In this context, we present a novel model for bionic prostheses' application that leverages camera-based motion capture and wearable sensor data, to learn the synergistic coupling of the lower limbs during human locomotion, empowering it to infer the kinematic behavior of a missing lower limb across varied tasks, such as climbing inclines and stairs.","We propose a model that can multitask, adapt continually, anticipate movements, and refine.","The core of our method lies in an approach which we call -- multitask prospective rehearsal -- that anticipates and synthesizes future movements based on the previous prediction and employs a corrective mechanism for subsequent predictions.","We design an evolving architecture that merges lightweight, task-specific modules on a shared backbone, ensuring both specificity and scalability.","We empirically validate our model against various baselines using real-world human gait datasets, including experiments with transtibial amputees, which encompass a broad spectrum of locomotion tasks.","The results show that our approach consistently outperforms baseline models, particularly under scenarios affected by distributional shifts, adversarial perturbations, and noise."],"url":"http://arxiv.org/abs/2405.01114v1","category":"cs.LG"}
{"created":"2024-05-02 07:21:12","title":"Continual Learning for Robust Gate Detection under Dynamic Lighting in Autonomous Drone Racing","abstract":"In autonomous and mobile robotics, a principal challenge is resilient real-time environmental perception, particularly in situations characterized by unknown and dynamic elements, as exemplified in the context of autonomous drone racing. This study introduces a perception technique for detecting drone racing gates under illumination variations, which is common during high-speed drone flights. The proposed technique relies upon a lightweight neural network backbone augmented with capabilities for continual learning. The envisaged approach amalgamates predictions of the gates' positional coordinates, distance, and orientation, encapsulating them into a cohesive pose tuple. A comprehensive number of tests serve to underscore the efficacy of this approach in confronting diverse and challenging scenarios, specifically those involving variable lighting conditions. The proposed methodology exhibits notable robustness in the face of illumination variations, thereby substantiating its effectiveness.","sentences":["In autonomous and mobile robotics, a principal challenge is resilient real-time environmental perception, particularly in situations characterized by unknown and dynamic elements, as exemplified in the context of autonomous drone racing.","This study introduces a perception technique for detecting drone racing gates under illumination variations, which is common during high-speed drone flights.","The proposed technique relies upon a lightweight neural network backbone augmented with capabilities for continual learning.","The envisaged approach amalgamates predictions of the gates' positional coordinates, distance, and orientation, encapsulating them into a cohesive pose tuple.","A comprehensive number of tests serve to underscore the efficacy of this approach in confronting diverse and challenging scenarios, specifically those involving variable lighting conditions.","The proposed methodology exhibits notable robustness in the face of illumination variations, thereby substantiating its effectiveness."],"url":"http://arxiv.org/abs/2405.01054v1","category":"cs.RO"}
{"created":"2024-05-02 06:59:17","title":"Evolution of Interface Magnetism in Fe/Alq3 Bilayer Structure; Thickness-Dependent Interface Resolved Studies Under X-Ray Standing Wave","abstract":"In the present work, interfacial magnetism at metal organic interface is probed using an isotope sensitive interface resolved nuclear resonance scattering technique which is made depth selective under x-rays standing wave conditions. Using GIWAXS and GINRS measurements, this study evidences the presence of symmetry-based PMA which appears at a lower thickness of Fe having distortion in cubic symmetry and disappears at a higher thickness of Fe as its cubic symmetry retains. The non-zero value of quadrupole splitting evidences the strain at the interfacial region which on increasing thickness of Fe relaxes. The diffusion of Fe is traced using XRF and NRR, deep penetration of Fe in Alq3 layer due to soft nature of the organic film is obtained. This thickness-dependent study enables us to understand the magnetic behavior of buried ferromagnetic metal in the vicinity of organic molecules.","sentences":["In the present work, interfacial magnetism at metal organic interface is probed using an isotope sensitive interface resolved nuclear resonance scattering technique which is made depth selective under x-rays standing wave conditions.","Using GIWAXS and GINRS measurements, this study evidences the presence of symmetry-based PMA which appears at a lower thickness of Fe having distortion in cubic symmetry and disappears at a higher thickness of Fe as its cubic symmetry retains.","The non-zero value of quadrupole splitting evidences the strain at the interfacial region which on increasing thickness of Fe relaxes.","The diffusion of Fe is traced using XRF and NRR, deep penetration of Fe in Alq3 layer due to soft nature of the organic film is obtained.","This thickness-dependent study enables us to understand the magnetic behavior of buried ferromagnetic metal in the vicinity of organic molecules."],"url":"http://arxiv.org/abs/2405.01048v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 06:52:49","title":"Few Shot Class Incremental Learning using Vision-Language models","abstract":"Recent advancements in deep learning have demonstrated remarkable performance comparable to human capabilities across various supervised computer vision tasks. However, the prevalent assumption of having an extensive pool of training data encompassing all classes prior to model training often diverges from real-world scenarios, where limited data availability for novel classes is the norm. The challenge emerges in seamlessly integrating new classes with few samples into the training data, demanding the model to adeptly accommodate these additions without compromising its performance on base classes. To address this exigency, the research community has introduced several solutions under the realm of few-shot class incremental learning (FSCIL).   In this study, we introduce an innovative FSCIL framework that utilizes language regularizer and subspace regularizer. During base training, the language regularizer helps incorporate semantic information extracted from a Vision-Language model. The subspace regularizer helps in facilitating the model's acquisition of nuanced connections between image and text semantics inherent to base classes during incremental training. Our proposed framework not only empowers the model to embrace novel classes with limited data, but also ensures the preservation of performance on base classes. To substantiate the efficacy of our approach, we conduct comprehensive experiments on three distinct FSCIL benchmarks, where our framework attains state-of-the-art performance.","sentences":["Recent advancements in deep learning have demonstrated remarkable performance comparable to human capabilities across various supervised computer vision tasks.","However, the prevalent assumption of having an extensive pool of training data encompassing all classes prior to model training often diverges from real-world scenarios, where limited data availability for novel classes is the norm.","The challenge emerges in seamlessly integrating new classes with few samples into the training data, demanding the model to adeptly accommodate these additions without compromising its performance on base classes.","To address this exigency, the research community has introduced several solutions under the realm of few-shot class incremental learning (FSCIL).   ","In this study, we introduce an innovative FSCIL framework that utilizes language regularizer and subspace regularizer.","During base training, the language regularizer helps incorporate semantic information extracted from a Vision-Language model.","The subspace regularizer helps in facilitating the model's acquisition of nuanced connections between image and text semantics inherent to base classes during incremental training.","Our proposed framework not only empowers the model to embrace novel classes with limited data, but also ensures the preservation of performance on base classes.","To substantiate the efficacy of our approach, we conduct comprehensive experiments on three distinct FSCIL benchmarks, where our framework attains state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.01040v1","category":"cs.CV"}
{"created":"2024-05-02 06:39:58","title":"Existence of normalized solutions of a Hartree-Fock system with mass subcritical growth","abstract":"In this paper, we are concerned with normalized solutions in $H_{r}^{1}(\\mathbb{R}^{3}) \\times H_{r}^{1}(\\mathbb{R}^{3})$ for Hartree-Fock type systems with the form \\be\\lab{ Hartree-Fock} \\left\\{ \\begin{array}{ll} -\\Delta u +\\alpha \\phi _{u,v} u=\\lambda _{1} u+\\left | u \\right | ^{2q-2} u+\\beta \\left | v \\right | ^{q} \\left | u \\right | ^{q-2} u , \\\\ -\\Delta v +\\alpha \\phi _{u,v} v=\\lambda _{2} v+\\left | v\\right | ^{2q-2} v+\\beta \\left | u \\right | ^{q} \\left | v \\right | ^{q-2} v , \\\\ \\int_{\\mathbb{R}^{3}}\\left | u \\right | ^{2} {\\rm d}x=a_{1} , \\quad \\int_{\\mathbb{R}^{3}}\\left | v \\right | ^{2} {\\rm d}x=a_{2} , \\nonumber\\\\ \\end{array} where $$ \\phi_{u, v}\\left(x\\right):=\\int_{\\mathbb{R}^{3}} \\frac{u^{2}(y)+v^{2}(y)}{|x-y|} {\\rm d}y \\in D^{1,2}\\left(\\mathbb{R}^{3}\\right). $$ Here $\\alpha,\\beta>0, a_1,a_2>0$ and $1<q<\\frac{5}{3}$. By seeking the constrained global minimizers of the corresponding functional, we prove that the existence of normalized solutions to the system above for any $a_1,a_2>0$ when $1<q<\\frac{4}{3}$ and for $a_1,a_2>0$ small when $\\frac{4}{3}\\le q < \\frac{3}{2}$. The nonexistence of normalized solutions is also considered for $\\frac{3}{2}\\le q < \\frac{5}{3}$. Also, the orbital stability of standing waves is obtained under local well-posedness assumptions of the evolution problem.","sentences":["In this paper, we are concerned with normalized solutions in $H_{r}^{1}(\\mathbb{R}^{3})","\\times H_{r}^{1}(\\mathbb{R}^{3})$ for Hartree-Fock type systems with the form \\be\\lab{ Hartree-Fock} \\left\\{ \\begin{array}{ll} -\\Delta u +\\alpha \\phi _{u,v} u=\\lambda _{1} u+\\left | u \\right | ^{2q-2} u+\\beta \\left | v \\right | ^{q} \\left | u \\right | ^{q-2} u , \\\\ -\\Delta v +\\alpha \\phi _{u,v} v=\\lambda _{2} v+\\left | v\\right | ^{2q-2} v+\\beta \\left |","u","\\right | ^{q} \\left | v \\right | ^{q-2} v , \\\\ \\int_{\\mathbb{R}^{3}}\\left | u \\right | ^{2} {\\rm d}x=a_{1} , \\quad \\int_{\\mathbb{R}^{3}}\\left | v \\right | ^{2} {\\rm d}x=a_{2} , \\nonumber\\\\ \\end{array} where $$ \\phi_{u, v}\\left(x\\right):=\\int_{\\mathbb{R}^{3}} \\frac{u^{2}(y)+v^{2}(y)}{|x-y|} {\\rm d}y \\in D^{1,2}\\left(\\mathbb{R}^{3}\\right).","$$ Here $\\alpha,\\beta>0, a_1,a_2>0$ and $1<q<\\frac{5}{3}$. By seeking the constrained global minimizers of the corresponding functional, we prove that the existence of normalized solutions to the system above for any $a_1,a_2>0$ when $1<q<\\frac{4}{3}$ and for $a_1,a_2>0$ small when $\\frac{4}{3}\\le q < \\frac{3}{2}$.","The nonexistence of normalized solutions is also considered for $\\frac{3}{2}\\le q < \\frac{5}{3}$.","Also, the orbital stability of standing waves is obtained under local well-posedness assumptions of the evolution problem."],"url":"http://arxiv.org/abs/2405.01036v1","category":"math.AP"}
{"created":"2024-05-02 06:28:06","title":"Approaching the conformal limit of quark matter with different chemical potentials","abstract":"We study in detail the influence of different chemical potentials (baryon, charged, strange, and neutrino) on how and how fast a free gas of quarks in the zero-temperature limit reaches the conformal limit. We discuss the influence of non-zero masses, the inclusion of leptons, and different constraints, such as charge neutrality, zero-net strangeness, and fixed lepton fraction. We also investigate for the first time how the symmetry energy of the system under some of these conditions approaches the conformal limit. Finally, we briefly discuss what kind of corrections are expected from perturbative QCD as one goes away from the conformal limit.","sentences":["We study in detail the influence of different chemical potentials (baryon, charged, strange, and neutrino) on how and how fast a free gas of quarks in the zero-temperature limit reaches the conformal limit.","We discuss the influence of non-zero masses, the inclusion of leptons, and different constraints, such as charge neutrality, zero-net strangeness, and fixed lepton fraction.","We also investigate for the first time how the symmetry energy of the system under some of these conditions approaches the conformal limit.","Finally, we briefly discuss what kind of corrections are expected from perturbative QCD as one goes away from the conformal limit."],"url":"http://arxiv.org/abs/2405.01032v1","category":"hep-ph"}
{"created":"2024-05-02 06:14:56","title":"The Privacy Power of Correlated Noise in Decentralized Learning","abstract":"Decentralized learning is appealing as it enables the scalable usage of large amounts of distributed data and resources (without resorting to any central entity), while promoting privacy since every user minimizes the direct exposure of their data. Yet, without additional precautions, curious users can still leverage models obtained from their peers to violate privacy. In this paper, we propose Decor, a variant of decentralized SGD with differential privacy (DP) guarantees. Essentially, in Decor, users securely exchange randomness seeds in one communication round to generate pairwise-canceling correlated Gaussian noises, which are injected to protect local models at every communication round. We theoretically and empirically show that, for arbitrary connected graphs, Decor matches the central DP optimal privacy-utility trade-off. We do so under SecLDP, our new relaxation of local DP, which protects all user communications against an external eavesdropper and curious users, assuming that every pair of connected users shares a secret, i.e., an information hidden to all others. The main theoretical challenge is to control the accumulation of non-canceling correlated noise due to network sparsity. We also propose a companion SecLDP privacy accountant for public use.","sentences":["Decentralized learning is appealing as it enables the scalable usage of large amounts of distributed data and resources (without resorting to any central entity), while promoting privacy since every user minimizes the direct exposure of their data.","Yet, without additional precautions, curious users can still leverage models obtained from their peers to violate privacy.","In this paper, we propose Decor, a variant of decentralized SGD with differential privacy (DP) guarantees.","Essentially, in Decor, users securely exchange randomness seeds in one communication round to generate pairwise-canceling correlated Gaussian noises, which are injected to protect local models at every communication round.","We theoretically and empirically show that, for arbitrary connected graphs, Decor matches the central DP optimal privacy-utility trade-off.","We do so under SecLDP, our new relaxation of local DP, which protects all user communications against an external eavesdropper and curious users, assuming that every pair of connected users shares a secret, i.e., an information hidden to all others.","The main theoretical challenge is to control the accumulation of non-canceling correlated noise due to network sparsity.","We also propose a companion SecLDP privacy accountant for public use."],"url":"http://arxiv.org/abs/2405.01031v1","category":"cs.LG"}
{"created":"2024-05-02 05:56:54","title":"Asymptotic Results for Penalized Quasi-Likelihood Estimation in Generalized Linear Mixed Models","abstract":"Generalized Linear Mixed Models (GLMMs) are widely used for analysing clustered data. One well-established method of overcoming the integral in the marginal likelihood function for GLMMs is penalized quasi-likelihood (PQL) estimation, although to date there are few asymptotic distribution results relating to PQL estimation for GLMMs in the literature. In this paper, we establish large sample results for PQL estimators of the parameters and random effects in independent-cluster GLMMs, when both the number of clusters and the cluster sizes go to infinity. This is done under two distinct regimes: conditional on the random effects (essentially treating them as fixed effects) and unconditionally (treating the random effects as random). Under the conditional regime, we show the PQL estimators are asymptotically normal around the true fixed and random effects. Unconditionally, we prove that while the estimator of the fixed effects is asymptotically normally distributed, the correct asymptotic distribution of the so-called prediction gap of the random effects may in fact be a normal scale-mixture distribution under certain relative rates of growth. A simulation study is used to verify the finite sample performance of our theoretical results.","sentences":["Generalized Linear Mixed Models (GLMMs) are widely used for analysing clustered data.","One well-established method of overcoming the integral in the marginal likelihood function for GLMMs is penalized quasi-likelihood (PQL) estimation, although to date there are few asymptotic distribution results relating to PQL estimation for GLMMs in the literature.","In this paper, we establish large sample results for PQL estimators of the parameters and random effects in independent-cluster GLMMs, when both the number of clusters and the cluster sizes go to infinity.","This is done under two distinct regimes: conditional on the random effects (essentially treating them as fixed effects) and unconditionally (treating the random effects as random).","Under the conditional regime, we show the PQL estimators are asymptotically normal around the true fixed and random effects.","Unconditionally, we prove that while the estimator of the fixed effects is asymptotically normally distributed, the correct asymptotic distribution of the so-called prediction gap of the random effects may in fact be a normal scale-mixture distribution under certain relative rates of growth.","A simulation study is used to verify the finite sample performance of our theoretical results."],"url":"http://arxiv.org/abs/2405.01026v1","category":"math.ST"}
{"created":"2024-05-02 05:56:34","title":"Density Matrix Realism","abstract":"Realism about quantum theory naturally leads to realism about the quantum state of the universe. It leaves open whether it is a pure state represented by a wave function, or an impure one represented by a density matrix. I characterize and elaborate on Density Matrix Realism, the thesis that the universal quantum state is objective but can be impure. To clarify the thesis, I compare it with Wave Function Realism, explain the conditions under which they are empirically equivalent, consider two generalizations of Density Matrix Realism, and answer some frequently asked questions. I end by highlighting an implication for scientific realism.","sentences":["Realism about quantum theory naturally leads to realism about the quantum state of the universe.","It leaves open whether it is a pure state represented by a wave function, or an impure one represented by a density matrix.","I characterize and elaborate on Density Matrix Realism, the thesis that the universal quantum state is objective but can be impure.","To clarify the thesis, I compare it with Wave Function Realism, explain the conditions under which they are empirically equivalent, consider two generalizations of Density Matrix Realism, and answer some frequently asked questions.","I end by highlighting an implication for scientific realism."],"url":"http://arxiv.org/abs/2405.01025v1","category":"quant-ph"}
{"created":"2024-05-02 05:39:57","title":"Investigating the relationship between empathy and attribution of mental states to robots","abstract":"This paper describes an experimental evaluation aimed at detecting the users' perception of the robot's empathic abilities during a conversation. The results have been then analyzed to search for a possible relationship between the perceived empathy and the attribution of mental states to the robot, namely the user's perception of the robot's mental qualities as compared to humans. The involved sample consisted of 68 subjects, including 34 adults and 34 between teenagers and children. By conducting the experiment with both adult and child participants, make possible to compare the results obtained from each group and identify any differences in perception between the various age groups.","sentences":["This paper describes an experimental evaluation aimed at detecting the users' perception of the robot's empathic abilities during a conversation.","The results have been then analyzed to search for a possible relationship between the perceived empathy and the attribution of mental states to the robot, namely the user's perception of the robot's mental qualities as compared to humans.","The involved sample consisted of 68 subjects, including 34 adults and 34 between teenagers and children.","By conducting the experiment with both adult and child participants, make possible to compare the results obtained from each group and identify any differences in perception between the various age groups."],"url":"http://arxiv.org/abs/2405.01019v1","category":"cs.RO"}
{"created":"2024-05-02 05:25:55","title":"Rare Collision Risk Estimation of Autonomous Vehicles with Multi-Agent Situation Awareness","abstract":"This paper offers a formal framework for the rare collision risk estimation of autonomous vehicles (AVs) with multi-agent situation awareness, affected by different sources of noise in a complex dynamic environment. In our proposed setting, the situation awareness is considered for one of the ego vehicles by aggregating a range of diverse information gathered from other vehicles into a vector. We model AVs equipped with the situation awareness as general stochastic hybrid systems (GSHS) and assess the probability of collision in a lane-change scenario where two self-driving vehicles simultaneously intend to switch lanes into a shared one, while utilizing the time-to-collision measure for decision-making as required. Due to the substantial data requirements of simulation-based methods for the rare collision risk estimation, we leverage a multi-level importance splitting technique, known as interacting particle system-based estimation with fixed assignment splitting (IPS-FAS). This approach allows us to estimate the probability of a rare event by employing a group of interacting particles. Specifically, each particle embodies a system trajectory and engages with others through resampling and branching, focusing computational resources on trajectories with the highest probability of encountering the rare event. The effectiveness of our proposed approach is demonstrated through an extensive simulation of a lane-change scenario.","sentences":["This paper offers a formal framework for the rare collision risk estimation of autonomous vehicles (AVs) with multi-agent situation awareness, affected by different sources of noise in a complex dynamic environment.","In our proposed setting, the situation awareness is considered for one of the ego vehicles by aggregating a range of diverse information gathered from other vehicles into a vector.","We model AVs equipped with the situation awareness as general stochastic hybrid systems (GSHS) and assess the probability of collision in a lane-change scenario where two self-driving vehicles simultaneously intend to switch lanes into a shared one, while utilizing the time-to-collision measure for decision-making as required.","Due to the substantial data requirements of simulation-based methods for the rare collision risk estimation, we leverage a multi-level importance splitting technique, known as interacting particle system-based estimation with fixed assignment splitting (IPS-FAS).","This approach allows us to estimate the probability of a rare event by employing a group of interacting particles.","Specifically, each particle embodies a system trajectory and engages with others through resampling and branching, focusing computational resources on trajectories with the highest probability of encountering the rare event.","The effectiveness of our proposed approach is demonstrated through an extensive simulation of a lane-change scenario."],"url":"http://arxiv.org/abs/2405.01011v1","category":"eess.SY"}
{"created":"2024-05-02 05:17:07","title":"Multi-User Multi-Application Packet Scheduling for Application-Specific QoE Enhancement Based on Knowledge-Embedded DDPG in 6G RAN","abstract":"The rapidly growing diversity of concurrent applications from both different users and same devices calls for application-specific Quality of Experience (QoE) enhancement of future wireless communications. Achieving this goal relies on application-specific packet scheduling, as it is vital for achieving tailored QoE enhancement by realizing the application-specific Quality of Service (QoS) requirements and for optimal perceived QoE values. However, the intertwining diversified QoE perception mechanisms, fairness among concurrent applications, and the impact of network dynamics inevitably complicate tailored packet scheduling. To achieve concurrent application-specific QoE enhancement, the problem of multi-user multi-application packet scheduling in downlink 6G radio access network (RAN) is first formulated as a Markov decision process (MDP) problem in this paper. For solving this problem, a deep deterministic policy gradient (DDPG)-based solution is proposed. However, due to the high dimensionalities of both the state and action spaces, the trained DDPG agents might generate decisions causing unnecessary resource waste. Hence, a knowledge embedding method is proposed to adjust the decisions of the DDPG agents according to human insights. Extensive experiments are conducted, which demonstrate the superiority of DDPG-based packet schedulers over baseline algorithms and the effectiveness of the proposed knowledge embedding technique.","sentences":["The rapidly growing diversity of concurrent applications from both different users and same devices calls for application-specific Quality of Experience (QoE) enhancement of future wireless communications.","Achieving this goal relies on application-specific packet scheduling, as it is vital for achieving tailored QoE enhancement by realizing the application-specific Quality of Service (QoS) requirements and for optimal perceived QoE values.","However, the intertwining diversified QoE perception mechanisms, fairness among concurrent applications, and the impact of network dynamics inevitably complicate tailored packet scheduling.","To achieve concurrent application-specific QoE enhancement, the problem of multi-user multi-application packet scheduling in downlink 6G radio access network (RAN) is first formulated as a Markov decision process (MDP) problem in this paper.","For solving this problem, a deep deterministic policy gradient (DDPG)-based solution is proposed.","However, due to the high dimensionalities of both the state and action spaces, the trained DDPG agents might generate decisions causing unnecessary resource waste.","Hence, a knowledge embedding method is proposed to adjust the decisions of the DDPG agents according to human insights.","Extensive experiments are conducted, which demonstrate the superiority of DDPG-based packet schedulers over baseline algorithms and the effectiveness of the proposed knowledge embedding technique."],"url":"http://arxiv.org/abs/2405.01007v1","category":"cs.NI"}
{"created":"2024-05-02 03:38:58","title":"LLM-AD: Large Language Model based Audio Description System","abstract":"The development of Audio Description (AD) has been a pivotal step forward in making video content more accessible and inclusive. Traditionally, AD production has demanded a considerable amount of skilled labor, while existing automated approaches still necessitate extensive training to integrate multimodal inputs and tailor the output from a captioning style to an AD style. In this paper, we introduce an automated AD generation pipeline that harnesses the potent multimodal and instruction-following capacities of GPT-4V(ision). Notably, our methodology employs readily available components, eliminating the need for additional training. It produces ADs that not only comply with established natural language AD production standards but also maintain contextually consistent character information across frames, courtesy of a tracking-based character recognition module. A thorough analysis on the MAD dataset reveals that our approach achieves a performance on par with learning-based methods in automated AD production, as substantiated by a CIDEr score of 20.5.","sentences":["The development of Audio Description (AD) has been a pivotal step forward in making video content more accessible and inclusive.","Traditionally, AD production has demanded a considerable amount of skilled labor, while existing automated approaches still necessitate extensive training to integrate multimodal inputs and tailor the output from a captioning style to an AD style.","In this paper, we introduce an automated AD generation pipeline that harnesses the potent multimodal and instruction-following capacities of GPT-4V(ision).","Notably, our methodology employs readily available components, eliminating the need for additional training.","It produces ADs that not only comply with established natural language AD production standards but also maintain contextually consistent character information across frames, courtesy of a tracking-based character recognition module.","A thorough analysis on the MAD dataset reveals that our approach achieves a performance on par with learning-based methods in automated AD production, as substantiated by a CIDEr score of 20.5."],"url":"http://arxiv.org/abs/2405.00983v1","category":"cs.CV"}
{"created":"2024-05-02 03:25:39","title":"On Ridge Estimation in High-dimensional Rotationally Sparse Linear Regression","abstract":"Recently, deep neural networks have been found to nearly interpolate training data but still generalize well in various applications. To help understand such a phenomenon, it has been of interest to analyze the ridge estimator and its interpolation limit in high-dimensional regression models. For this motivation, we study the ridge estimator in a rotationally sparse setting of high-dimensional linear regression, where the signal of a response is aligned with a small number, $d$, of covariates with large or spiked variances, compared with the remaining covariates with small or tail variances, \\textit{after} an orthogonal transformation of the covariate vector. We establish high-probability upper and lower bounds on the out-sample and in-sample prediction errors in two distinct regimes depending on the ratio of the effective rank of tail variances over the sample size $n$. The separation of the two regimes enables us to exploit relevant concentration inequalities and derive concrete error bounds without making any oracle assumption or independent components assumption on covariate vectors. Moreover, we derive sufficient and necessary conditions which indicate that the prediction errors of ridge estimation can be of the order $O(\\frac{d}{n})$ if and only if the gap between the spiked and tail variances are sufficiently large. We also compare the orders of optimal out-sample and in-sample prediction errors and find that, remarkably, the optimal out-sample prediction error may be significantly smaller than the optimal in-sample one. Finally, we present numerical experiments which empirically confirm our theoretical findings.","sentences":["Recently, deep neural networks have been found to nearly interpolate training data but still generalize well in various applications.","To help understand such a phenomenon, it has been of interest to analyze the ridge estimator and its interpolation limit in high-dimensional regression models.","For this motivation, we study the ridge estimator in a rotationally sparse setting of high-dimensional linear regression, where the signal of a response is aligned with a small number, $d$, of covariates with large or spiked variances, compared with the remaining covariates with small or tail variances, \\textit{after} an orthogonal transformation of the covariate vector.","We establish high-probability upper and lower bounds on the out-sample and in-sample prediction errors in two distinct regimes depending on the ratio of the effective rank of tail variances over the sample size $n$. The separation of the two regimes enables us to exploit relevant concentration inequalities and derive concrete error bounds without making any oracle assumption or independent components assumption on covariate vectors.","Moreover, we derive sufficient and necessary conditions which indicate that the prediction errors of ridge estimation can be of the order $O(\\frac{d}{n})$ if and only if the gap between the spiked and tail variances are sufficiently large.","We also compare the orders of optimal out-sample and in-sample prediction errors and find that, remarkably, the optimal out-sample prediction error may be significantly smaller than the optimal in-sample one.","Finally, we present numerical experiments which empirically confirm our theoretical findings."],"url":"http://arxiv.org/abs/2405.00974v1","category":"math.ST"}
{"created":"2024-05-02 03:22:24","title":"Active Cell Balancing for Extended Operational Time of Lithium-Ion Battery Systems in Energy Storage Applications","abstract":"Cell inconsistency within a lithium-ion battery system poses a significant challenge in maximizing the system operational time. This study presents an optimization-driven active balancing method to minimize the effects of cell inconsistency on the system operational time while simultaneously satisfying the system output power demand and prolonging the system operational time in energy storage applications. The proposed method utilizes a fractional order model to forecast the terminal voltage dynamics of each cell within a battery system, enhanced with a particle-swarm-optimisation-genetic algorithm for precise parameter identification. It is implemented under two distinct cell-level balancing topologies: independent cell balancing and differential cell balancing. Subsequently, the current distribution for each topology is determined by resolving two optimization control problems constrained by the battery's operational specifications and power demands. The effectiveness of the proposed method is validated by extensive experiments based on the two balancing topologies. The results demonstrate that the proposed method increases the operational time by 3.2%.","sentences":["Cell inconsistency within a lithium-ion battery system poses a significant challenge in maximizing the system operational time.","This study presents an optimization-driven active balancing method to minimize the effects of cell inconsistency on the system operational time while simultaneously satisfying the system output power demand and prolonging the system operational time in energy storage applications.","The proposed method utilizes a fractional order model to forecast the terminal voltage dynamics of each cell within a battery system, enhanced with a particle-swarm-optimisation-genetic algorithm for precise parameter identification.","It is implemented under two distinct cell-level balancing topologies: independent cell balancing and differential cell balancing.","Subsequently, the current distribution for each topology is determined by resolving two optimization control problems constrained by the battery's operational specifications and power demands.","The effectiveness of the proposed method is validated by extensive experiments based on the two balancing topologies.","The results demonstrate that the proposed method increases the operational time by 3.2%."],"url":"http://arxiv.org/abs/2405.00973v1","category":"eess.SY"}
{"created":"2024-05-02 02:33:15","title":"Recovering Labels from Local Updates in Federated Learning","abstract":"Gradient inversion (GI) attacks present a threat to the privacy of clients in federated learning (FL) by aiming to enable reconstruction of the clients' data from communicated model updates. A number of such techniques attempts to accelerate data recovery by first reconstructing labels of the samples used in local training. However, existing label extraction methods make strong assumptions that typically do not hold in realistic FL settings. In this paper we present a novel label recovery scheme, Recovering Labels from Local Updates (RLU), which provides near-perfect accuracy when attacking untrained (most vulnerable) models. More significantly, RLU achieves high performance even in realistic real-world settings where the clients in an FL system run multiple local epochs, train on heterogeneous data, and deploy various optimizers to minimize different objective functions. Specifically, RLU estimates labels by solving a least-square problem that emerges from the analysis of the correlation between labels of the data points used in a training round and the resulting update of the output layer. The experimental results on several datasets, architectures, and data heterogeneity scenarios demonstrate that the proposed method consistently outperforms existing baselines, and helps improve quality of the reconstructed images in GI attacks in terms of both PSNR and LPIPS.","sentences":["Gradient inversion (GI) attacks present a threat to the privacy of clients in federated learning (FL) by aiming to enable reconstruction of the clients' data from communicated model updates.","A number of such techniques attempts to accelerate data recovery by first reconstructing labels of the samples used in local training.","However, existing label extraction methods make strong assumptions that typically do not hold in realistic FL settings.","In this paper we present a novel label recovery scheme, Recovering Labels from Local Updates (RLU), which provides near-perfect accuracy when attacking untrained (most vulnerable) models.","More significantly, RLU achieves high performance even in realistic real-world settings where the clients in an FL system run multiple local epochs, train on heterogeneous data, and deploy various optimizers to minimize different objective functions.","Specifically, RLU estimates labels by solving a least-square problem that emerges from the analysis of the correlation between labels of the data points used in a training round and the resulting update of the output layer.","The experimental results on several datasets, architectures, and data heterogeneity scenarios demonstrate that the proposed method consistently outperforms existing baselines, and helps improve quality of the reconstructed images in GI attacks in terms of both PSNR and LPIPS."],"url":"http://arxiv.org/abs/2405.00955v1","category":"cs.LG"}
{"created":"2024-05-02 02:15:23","title":"SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters","abstract":"This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than 1k parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is available at: https://github.com/lss-1138/SparseTSF.","sentences":["This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources.","At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data.","This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count.","Based on this technique, the SparseTSF model uses fewer than 1k parameters to achieve competitive or superior performance compared to state-of-the-art models.","Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data.","The code is available at: https://github.com/lss-1138/SparseTSF."],"url":"http://arxiv.org/abs/2405.00946v1","category":"cs.LG"}
{"created":"2024-05-02 01:55:48","title":"Computing Threshold Circuits with Bimolecular Void Reactions in Step Chemical Reaction Networks","abstract":"Step Chemical Reaction Networks (step CRNs) are an augmentation of the Chemical Reaction Network (CRN) model where additional species may be introduced to the system in a sequence of ``steps.'' We study step CRN systems using a weak subset of reaction rules, \\emph{void} rules, in which molecular species can only be deleted. We demonstrate that step CRNs with only void rules of size (2,0) can simulate threshold formulas (TFs) under linear resources. These limited systems can also simulate threshold \\emph{circuits} (TCs) by modifying the volume of the system to be exponential. We then prove a matching exponential lower bound on the required volume for simulating threshold circuits in a step CRN with (2,0)-size rules under a restricted \\emph{gate-wise} simulation, thus showing our construction is optimal for simulating circuits in this way.","sentences":["Step Chemical Reaction Networks (step CRNs) are an augmentation of the Chemical Reaction Network (CRN) model where additional species may be introduced to the system in a sequence of ``steps.''","We study step CRN systems using a weak subset of reaction rules, \\emph{void} rules, in which molecular species can only be deleted.","We demonstrate that step CRNs with only void rules of size (2,0) can simulate threshold formulas (TFs) under linear resources.","These limited systems can also simulate threshold \\emph{circuits} (TCs) by modifying the volume of the system to be exponential.","We then prove a matching exponential lower bound on the required volume for simulating threshold circuits in a step CRN with (2,0)-size rules under a restricted \\emph{gate-wise} simulation, thus showing our construction is optimal for simulating circuits in this way."],"url":"http://arxiv.org/abs/2405.00940v1","category":"cs.DC"}
{"created":"2024-05-02 01:55:34","title":"On the solitary wave configurations of nonlinear Schr\u00f6dinger equation under the effect of L\u00e9vy noise","abstract":"This study aims to examine the effect of L\\'evy noise on the solutions of the nonlinear Schr\\\"odinger equation. An improved diversity of stochastic solutions is instinctively located discretely on certain conditions by applying the generalized Kudryashov method. Moreover, the dynamical behaviors of these exact results of the nonlinear Schr\\\"odinger equation are interpreted in the context of the effect of L\\'evy noise. Even mathematical evaluations have been conducted and presented.","sentences":["This study aims to examine the effect of L\\'evy noise on the solutions of the nonlinear Schr\\\"odinger equation.","An improved diversity of stochastic solutions is instinctively located discretely on certain conditions by applying the generalized Kudryashov method.","Moreover, the dynamical behaviors of these exact results of the nonlinear Schr\\\"odinger equation are interpreted in the context of the effect of L\\'evy noise.","Even mathematical evaluations have been conducted and presented."],"url":"http://arxiv.org/abs/2405.00939v1","category":"math.DS"}
{"created":"2024-05-02 01:28:52","title":"Efficient Internal Strategies in Quantum Relaxation based Branch-and-Bound","abstract":"A combinatorial optimization problem is to find an optimal solution under the constraints. This is one of the potential applications for quantum computers. Quantum Random Access Optimization (QRAO) is the quantum optimization algorithm that encodes multiple classical variables into a single qubit to construct a quantum Hamiltonian, thereby reducing the number of qubits required. The ground energy of the QRAO Hamiltonian provides a lower bound on the original problem's optimal value before encoding. This property allows the QRAO Hamiltonian to be used as a relaxation of the original problem, and it is thus referred to as a quantum relaxed Hamiltonian. In the Branch-and-Bound method, solving the relaxation problem plays a significant role. In this study, we developed Quantum Relaxation based Branch-and-Bound (QR-BnB), a method incorporating quantum relaxation into the Branch-and-Bound framework. We solved the MaxCut Problem and the Travelling Salesman Problem in our experiments. In all instances in this study, we obtained the optimal solution whenever we successfully computed the exact lower bound through quantum relaxation. Internal strategies, such as relaxation methods and variable selection, influence the convergence of the Branch-and-Bound. Thus, we have further developed the internal strategies for QR-BnB and examined how these strategies influence its convergence. We show that our variable selection strategy via the expectation value of the Pauli operators gives better convergence than the naive random choice. QRAO deals with only unconstrained optimization problems, but QR-BnB can handle constraints more flexibly because of the Branch-and-Bound processes on the classical computing part. We demonstrate that in our experiments with the Travelling Salesman Problem, the convergence of QR-BnB became more than three times faster by using the information in the constraints.","sentences":["A combinatorial optimization problem is to find an optimal solution under the constraints.","This is one of the potential applications for quantum computers.","Quantum Random Access Optimization (QRAO) is the quantum optimization algorithm that encodes multiple classical variables into a single qubit to construct a quantum Hamiltonian, thereby reducing the number of qubits required.","The ground energy of the QRAO Hamiltonian provides a lower bound on the original problem's optimal value before encoding.","This property allows the QRAO Hamiltonian to be used as a relaxation of the original problem, and it is thus referred to as a quantum relaxed Hamiltonian.","In the Branch-and-Bound method, solving the relaxation problem plays a significant role.","In this study, we developed Quantum Relaxation based Branch-and-Bound (QR-BnB), a method incorporating quantum relaxation into the Branch-and-Bound framework.","We solved the MaxCut Problem and the Travelling Salesman Problem in our experiments.","In all instances in this study, we obtained the optimal solution whenever we successfully computed the exact lower bound through quantum relaxation.","Internal strategies, such as relaxation methods and variable selection, influence the convergence of the Branch-and-Bound.","Thus, we have further developed the internal strategies for QR-BnB and examined how these strategies influence its convergence.","We show that our variable selection strategy via the expectation value of the Pauli operators gives better convergence than the naive random choice.","QRAO deals with only unconstrained optimization problems, but QR-BnB can handle constraints more flexibly because of the Branch-and-Bound processes on the classical computing part.","We demonstrate that in our experiments with the Travelling Salesman Problem, the convergence of QR-BnB became more than three times faster by using the information in the constraints."],"url":"http://arxiv.org/abs/2405.00935v1","category":"quant-ph"}
{"created":"2024-05-02 00:55:15","title":"Nearly Optimum Properties of Certain Multi-Decision Sequential Rules for General Non-i.i.d. Stochastic Models","abstract":"Dedicated to the memory of Professor Tze Leung Lai, this paper introduces three multi-hypothesis sequential tests. These tests are derived from one-sided versions of the sequential probability ratio test and its modifications. They are proven to be first-order asymptotically optimal for testing simple and parametric composite hypotheses when error probabilities are small. These tests exhibit near optimality properties not only in classical i.i.d. observation models but also in general non-i.i.d. models, provided that the log-likelihood ratios between hypotheses converge r-completely to positive and finite numbers. These findings extend the seminal work of Lai (1981) on two hypotheses.","sentences":["Dedicated to the memory of Professor Tze Leung Lai, this paper introduces three multi-hypothesis sequential tests.","These tests are derived from one-sided versions of the sequential probability ratio test and its modifications.","They are proven to be first-order asymptotically optimal for testing simple and parametric composite hypotheses when error probabilities are small.","These tests exhibit near optimality properties not only in classical i.i.d. observation models but also in general non-i.i.d. models, provided that the log-likelihood ratios between hypotheses converge r-completely to positive and finite numbers.","These findings extend the seminal work of Lai (1981) on two hypotheses."],"url":"http://arxiv.org/abs/2405.00928v1","category":"math.ST"}
{"created":"2024-05-02 00:44:59","title":"Zonotope-based Symbolic Controller Synthesis for Linear Temporal Logic Specifications","abstract":"This paper studies the controller synthesis problem for nonlinear control systems under linear temporal logic (LTL) specifications using zonotope techniques. A local-to-global control strategy is proposed for the desired specification expressed as an LTL formula. First, a novel approach is developed to divide the state space into finite zonotopes and constrained zonotopes, which are called cells and allowed to intersect with the neighbor cells. Second, from the intersection relation, a graph among all cells is generated to verify the realization of the accepting path for the LTL formula. The realization verification determines if there is a need for the control design, and also results in finite local LTL formulas. Third, once the accepting path is realized, a novel abstraction-based method is derived for the controller design. In particular, we only focus on the cells from the realization verification and approximate each cell thanks to properties of zonotopes. Based on local symbolic models and local LTL formulas, an iterative synthesis algorithm is proposed to design all local abstract controllers, whose existence and combination establish the global controller for the LTL formula. Finally, the proposed framework is illustrated via a path planning problem of mobile robots.","sentences":["This paper studies the controller synthesis problem for nonlinear control systems under linear temporal logic (LTL) specifications using zonotope techniques.","A local-to-global control strategy is proposed for the desired specification expressed as an LTL formula.","First, a novel approach is developed to divide the state space into finite zonotopes and constrained zonotopes, which are called cells and allowed to intersect with the neighbor cells.","Second, from the intersection relation, a graph among all cells is generated to verify the realization of the accepting path for the LTL formula.","The realization verification determines if there is a need for the control design, and also results in finite local LTL formulas.","Third, once the accepting path is realized, a novel abstraction-based method is derived for the controller design.","In particular, we only focus on the cells from the realization verification and approximate each cell thanks to properties of zonotopes.","Based on local symbolic models and local LTL formulas, an iterative synthesis algorithm is proposed to design all local abstract controllers, whose existence and combination establish the global controller for the LTL formula.","Finally, the proposed framework is illustrated via a path planning problem of mobile robots."],"url":"http://arxiv.org/abs/2405.00924v1","category":"eess.SY"}
{"created":"2024-05-02 00:14:16","title":"Semiparametric mean and variance joint models with clipped-Laplace link functions for bounded integer-valued time series","abstract":"We present a novel approach for modeling bounded count time series data, by deriving accurate upper and lower bounds for the variance of a bounded count random variable while maintaining a fixed mean. Leveraging these bounds, we propose semiparametric mean and variance joint (MVJ) models utilizing a clipped-Laplace link function. These models offer a flexible and feasible structure for both mean and variance, accommodating various scenarios of under-dispersion, equi-dispersion, or over-dispersion in bounded time series. The proposed MVJ models feature a linear mean structure with positive regression coefficients summing to one and allow for negative regression cefficients and autocorrelations. We demonstrate that the autocorrelation structure of MVJ models mirrors that of an autoregressive moving-average (ARMA) process, provided the proposed clipped-Laplace link functions with nonnegative regression coefficients summing to one are utilized. We establish conditions ensuring the stationarity and ergodicity properties of the MVJ process, along with demonstrating the consistency and asymptotic normality of the conditional least squares estimators. To aid model selection and diagnostics, we introduce two model selection criteria and apply two model diagnostics statistics. Finally, we conduct simulations and real data analyses to investigate the finite-sample properties of the proposed MVJ models, providing insights into their efficacy and applicability in practical scenarios.","sentences":["We present a novel approach for modeling bounded count time series data, by deriving accurate upper and lower bounds for the variance of a bounded count random variable while maintaining a fixed mean.","Leveraging these bounds, we propose semiparametric mean and variance joint (MVJ) models utilizing a clipped-Laplace link function.","These models offer a flexible and feasible structure for both mean and variance, accommodating various scenarios of under-dispersion, equi-dispersion, or over-dispersion in bounded time series.","The proposed MVJ models feature a linear mean structure with positive regression coefficients summing to one and allow for negative regression cefficients and autocorrelations.","We demonstrate that the autocorrelation structure of MVJ models mirrors that of an autoregressive moving-average (ARMA) process, provided the proposed clipped-Laplace link functions with nonnegative regression coefficients summing to one are utilized.","We establish conditions ensuring the stationarity and ergodicity properties of the MVJ process, along with demonstrating the consistency and asymptotic normality of the conditional least squares estimators.","To aid model selection and diagnostics, we introduce two model selection criteria and apply two model diagnostics statistics.","Finally, we conduct simulations and real data analyses to investigate the finite-sample properties of the proposed MVJ models, providing insights into their efficacy and applicability in practical scenarios."],"url":"http://arxiv.org/abs/2405.00917v1","category":"stat.ME"}
{"created":"2024-05-01 23:49:15","title":"Stabilization of infinite-dimensional systems under quantization and packet loss","abstract":"We study the problem of stabilizing infinite-dimensional systems with input and output quantization. The closed-loop system we consider is subject to packet loss in the sensor-to-controller channels, whose duration is assumed to be averagely bounded. Given a bound on the initial state, we propose design methods for dynamic quantizers with zoom parameters. We show that the closed-loop state staring in a given region exponentially converges to zero if the bounds of quantization errors and packet-loss duration satisfy suitable conditions. Since the norms of the operators representing the system dynamics are used in the proposed quantization schemes, we also present methods for approximately computing the operator norms.","sentences":["We study the problem of stabilizing infinite-dimensional systems with input and output quantization.","The closed-loop system we consider is subject to packet loss in the sensor-to-controller channels, whose duration is assumed to be averagely bounded.","Given a bound on the initial state, we propose design methods for dynamic quantizers with zoom parameters.","We show that the closed-loop state staring in a given region exponentially converges to zero if the bounds of quantization errors and packet-loss duration satisfy suitable conditions.","Since the norms of the operators representing the system dynamics are used in the proposed quantization schemes, we also present methods for approximately computing the operator norms."],"url":"http://arxiv.org/abs/2405.00911v1","category":"math.OC"}
{"created":"2024-05-01 23:46:44","title":"De-Biasing Models of Biased Decisions: A Comparison of Methods Using Mortgage Application Data","abstract":"Prediction models can improve efficiency by automating decisions such as the approval of loan applications. However, they may inherit bias against protected groups from the data they are trained on. This paper adds counterfactual (simulated) ethnic bias to real data on mortgage application decisions, and shows that this bias is replicated by a machine learning model (XGBoost) even when ethnicity is not used as a predictive variable. Next, several other de-biasing methods are compared: averaging over prohibited variables, taking the most favorable prediction over prohibited variables (a novel method), and jointly minimizing errors as well as the association between predictions and prohibited variables. De-biasing can recover some of the original decisions, but the results are sensitive to whether the bias is effected through a proxy.","sentences":["Prediction models can improve efficiency by automating decisions such as the approval of loan applications.","However, they may inherit bias against protected groups from the data they are trained on.","This paper adds counterfactual (simulated) ethnic bias to real data on mortgage application decisions, and shows that this bias is replicated by a machine learning model (XGBoost) even when ethnicity is not used as a predictive variable.","Next, several other de-biasing methods are compared: averaging over prohibited variables, taking the most favorable prediction over prohibited variables (a novel method), and jointly minimizing errors as well as the association between predictions and prohibited variables.","De-biasing can recover some of the original decisions, but the results are sensitive to whether the bias is effected through a proxy."],"url":"http://arxiv.org/abs/2405.00910v1","category":"cs.LG"}
{"created":"2024-05-01 23:33:52","title":"Angular momentum gain by electrons under action of intense structured light","abstract":"The problem of light waves interaction with charged particles becomes more and more complex starting with the case of plane waves, where the analytical solution is well known, to more natural, though more complicated situations which include focused or structured laser beams. Internal structure may introduce a new degree of freedom and qualitatively change the dynamics of interacting particles. For certain conditions, namely for the dilute plasma, description of single-particle dynamics in the focused structured laser beams is the first step and may serve as a good approximation on the way of understanding the global plasma response. Moreover, the general problem of integrability in complex systems starts from consideration of the integrals of motion for a single particle. The primary goal of this work is an understanding of the physics of the orbital angular momentum (OAM) absorption by a single particle in a focused structured light. A theoretical model of the process, including solutions of Maxwell equations with the required accuracy and a high-order perturbative approach to electron motion in external electromagnetic fields, is developed and its predictions are examined with numerical simulations for several exemplary electromagnetic field configurations. In particular, it was found that for the particles distributed initially with the azimuthal symmetry around the beam propagation direction, the transferred OAM has a smallness of the fourth order of the applied field amplitude, and requires an accurate consideration of the temporal laser pulse envelope.","sentences":["The problem of light waves interaction with charged particles becomes more and more complex starting with the case of plane waves, where the analytical solution is well known, to more natural, though more complicated situations which include focused or structured laser beams.","Internal structure may introduce a new degree of freedom and qualitatively change the dynamics of interacting particles.","For certain conditions, namely for the dilute plasma, description of single-particle dynamics in the focused structured laser beams is the first step and may serve as a good approximation on the way of understanding the global plasma response.","Moreover, the general problem of integrability in complex systems starts from consideration of the integrals of motion for a single particle.","The primary goal of this work is an understanding of the physics of the orbital angular momentum (OAM) absorption by a single particle in a focused structured light.","A theoretical model of the process, including solutions of Maxwell equations with the required accuracy and a high-order perturbative approach to electron motion in external electromagnetic fields, is developed and its predictions are examined with numerical simulations for several exemplary electromagnetic field configurations.","In particular, it was found that for the particles distributed initially with the azimuthal symmetry around the beam propagation direction, the transferred OAM has a smallness of the fourth order of the applied field amplitude, and requires an accurate consideration of the temporal laser pulse envelope."],"url":"http://arxiv.org/abs/2405.00907v1","category":"physics.plasm-ph"}
{"created":"2024-05-01 23:19:51","title":"Adjoint-based goal-oriented implicit shock tracking using full space mesh optimization","abstract":"Solutions to the governing partial differential equations obtained from a discrete numerical scheme can have significant errors, especially near shocks when the discrete representation of the solution cannot fully capture the discontinuity in the solution. A recent approach to shock tracking [1, 2] has been to implicitly align the faces of mesh elements with the shock, yielding accurate solutions on coarse meshes. In engineering applications, the solution field is often used to evaluate a scalar functional of interest, such as lift or drag over an airfoil. While functionals are sensitive to errors in the flow solution, certain regions in the domain are more important for accurate evaluation of the functional than the rest. Using this fact, we formulate a goal-oriented implicit shock tracking approach that captures a segment of the shock that is important for evaluating the functional. Shock tracking is achieved using Lagrange-Newton-Krylov-Schur (LNKS) full space optimizer, with the objective of minimizing the adjoint-weighted residual error indicator. We also present a method to evaluate the sensitivity and the Hessian of the functional error. Using available block preconditioners for LNKS [3, 4] makes the full space approach scalable. The method is applied to test cases of two-dimensional advection and inviscid compressible flows to demonstrate functional-dependent shock tracking. Tracking the entire shock without using artificial dissipation results in the error converging at the orders of $\\mathcal{O}(h^{p+1})$.","sentences":["Solutions to the governing partial differential equations obtained from a discrete numerical scheme can have significant errors, especially near shocks when the discrete representation of the solution cannot fully capture the discontinuity in the solution.","A recent approach to shock tracking [1, 2] has been to implicitly align the faces of mesh elements with the shock, yielding accurate solutions on coarse meshes.","In engineering applications, the solution field is often used to evaluate a scalar functional of interest, such as lift or drag over an airfoil.","While functionals are sensitive to errors in the flow solution, certain regions in the domain are more important for accurate evaluation of the functional than the rest.","Using this fact, we formulate a goal-oriented implicit shock tracking approach that captures a segment of the shock that is important for evaluating the functional.","Shock tracking is achieved using Lagrange-Newton-Krylov-Schur (LNKS) full space optimizer, with the objective of minimizing the adjoint-weighted residual error indicator.","We also present a method to evaluate the sensitivity and the Hessian of the functional error.","Using available block preconditioners for LNKS [3, 4] makes the full space approach scalable.","The method is applied to test cases of two-dimensional advection and inviscid compressible flows to demonstrate functional-dependent shock tracking.","Tracking the entire shock without using artificial dissipation results in the error converging at the orders of $\\mathcal{O}(h^{p+1})$."],"url":"http://arxiv.org/abs/2405.00904v1","category":"math.NA"}
{"created":"2024-05-01 23:07:12","title":"DiL-NeRF: Delving into Lidar for Neural Radiance Field on Street Scenes","abstract":"Photorealistic simulation plays a crucial role in applications such as autonomous driving, where advances in neural radiance fields (NeRFs) may allow better scalability through the automatic creation of digital 3D assets. However, reconstruction quality suffers on street scenes due to largely collinear camera motions and sparser samplings at higher speeds. On the other hand, the application often demands rendering from camera views that deviate from the inputs to accurately simulate behaviors like lane changes. In this paper, we propose several insights that allow a better utilization of Lidar data to improve NeRF quality on street scenes. First, our framework learns a geometric scene representation from Lidar, which is fused with the implicit grid-based representation for radiance decoding, thereby supplying stronger geometric information offered by explicit point cloud. Second, we put forth a robust occlusion-aware depth supervision scheme, which allows utilizing densified Lidar points by accumulation. Third, we generate augmented training views from Lidar points for further improvement. Our insights translate to largely improved novel view synthesis under real driving scenes.","sentences":["Photorealistic simulation plays a crucial role in applications such as autonomous driving, where advances in neural radiance fields (NeRFs) may allow better scalability through the automatic creation of digital 3D assets.","However, reconstruction quality suffers on street scenes due to largely collinear camera motions and sparser samplings at higher speeds.","On the other hand, the application often demands rendering from camera views that deviate from the inputs to accurately simulate behaviors like lane changes.","In this paper, we propose several insights that allow a better utilization of Lidar data to improve NeRF quality on street scenes.","First, our framework learns a geometric scene representation from Lidar, which is fused with the implicit grid-based representation for radiance decoding, thereby supplying stronger geometric information offered by explicit point cloud.","Second, we put forth a robust occlusion-aware depth supervision scheme, which allows utilizing densified Lidar points by accumulation.","Third, we generate augmented training views from Lidar points for further improvement.","Our insights translate to largely improved novel view synthesis under real driving scenes."],"url":"http://arxiv.org/abs/2405.00900v1","category":"cs.CV"}
{"created":"2024-05-01 22:46:34","title":"AT2018fyk: Candidate Tidal Disruption Event by a (Super)massive Black Hole Binary","abstract":"The tidal disruption event (TDE) AT2018fyk has unusual X-ray, UV, and optical light curves that decay over the first $\\sim$600d, rebrighten, and decay again around 1200d. We explain this behavior as a one-off TDE associated with a massive black hole (BH) \\emph{binary}. The sharp drop-offs from $t^{-5/3}$ power laws at around 600d naturally arise when one BH interrupts the debris fallback onto the other BH. The BH mass $M_\\bullet$ derived from fitting X-ray spectra with a slim disk accretion model and, independently, from fitting the early UV/optical light curves, is smaller by two orders of magnitude than predicted from the $M_\\bullet$--$\\sigma_*$ host galaxy relation, suggesting that the debris is accreted onto the secondary, with fallback cut off by the primary. Furthermore, if the rebrightening were associated with the primary, it should occur around 5000d, not the observed 1200d. The secondary's mass and dimensionless spin is $M_{\\bullet,{\\rm s}}=2.7^{+0.5}_{-1.5} \\times 10^5 M_\\odot$ and $a_{\\bullet,{\\rm s}}>0.3$ (X-ray spectral fitting), while the primary's mass is $M_{\\bullet,{\\rm p}}=10^{7.7\\pm0.4}M_\\odot$ ($M_\\bullet$-$\\sigma_*$ relation). An intermediate mass BH secondary is consistent with the observed UV/optical light curve decay, i.e., the secondary's outer accretion disk is too faint to produce a detectable emission floor. The time of the first accretion cutoff constrains the binary separation to be $(6.7\\pm 1.2) \\times 10^{-3}~{\\rm pc}$. X-ray spectral fitting and timing analysis indicate that the hard X-rays arise from a corona above the secondary's disk. The early UV/optical emission, suggesting a super-Eddington phase for the secondary, possibly originates from shocks arising from debris circularization.","sentences":["The tidal disruption event (TDE) AT2018fyk has unusual X-ray, UV, and optical light curves that decay over the first $\\sim$600d, rebrighten, and decay again around 1200d.","We explain this behavior as a one-off TDE associated with a massive black hole (BH) \\emph{binary}.","The sharp drop-offs from $t^{-5/3}$ power laws at around 600d naturally arise when one BH interrupts the debris fallback onto the other BH.","The BH mass $M_\\bullet$ derived from fitting X-ray spectra with a slim disk accretion model and, independently, from fitting the early UV/optical light curves, is smaller by two orders of magnitude than predicted from the $M_\\bullet$--$\\sigma_*$ host galaxy relation, suggesting that the debris is accreted onto the secondary, with fallback cut off by the primary.","Furthermore, if the rebrightening were associated with the primary, it should occur around 5000d, not the observed 1200d.","The secondary's mass and dimensionless spin is $M_{\\bullet,{\\rm s}}=2.7^{+0.5}_{-1.5} \\times 10^5 M_\\odot$ and $a_{\\bullet,{\\rm s}}>0.3$ (X-ray spectral fitting), while the primary's mass is $M_{\\bullet,{\\rm p}}=10^{7.7\\pm0.4}M_\\odot$ ($M_\\bullet$-$\\sigma_*$ relation).","An intermediate mass BH secondary is consistent with the observed UV/optical light curve decay, i.e., the secondary's outer accretion disk is too faint to produce a detectable emission floor.","The time of the first accretion cutoff constrains the binary separation to be $(6.7\\pm 1.2)","\\times 10^{-3}~{\\rm pc}$. X-ray spectral fitting and timing analysis indicate that the hard X-rays arise from a corona above the secondary's disk.","The early UV/optical emission, suggesting a super-Eddington phase for the secondary, possibly originates from shocks arising from debris circularization."],"url":"http://arxiv.org/abs/2405.00894v1","category":"astro-ph.HE"}
{"created":"2024-05-01 21:11:29","title":"Learning to Boost the Performance of Stable Nonlinear Systems","abstract":"The growing scale and complexity of safety-critical control systems underscore the need to evolve current control architectures aiming for the unparalleled performances achievable through state-of-the-art optimization and machine learning algorithms. However, maintaining closed-loop stability while boosting the performance of nonlinear control systems using data-driven and deep-learning approaches stands as an important unsolved challenge. In this paper, we tackle the performance-boosting problem with closed-loop stability guarantees. Specifically, we establish a synergy between the Internal Model Control (IMC) principle for nonlinear systems and state-of-the-art unconstrained optimization approaches for learning stable dynamics. Our methods enable learning over arbitrarily deep neural network classes of performance-boosting controllers for stable nonlinear systems; crucially, we guarantee Lp closed-loop stability even if optimization is halted prematurely, and even when the ground-truth dynamics are unknown, with vanishing conservatism in the class of stabilizing policies as the model uncertainty is reduced to zero. We discuss the implementation details of the proposed control schemes, including distributed ones, along with the corresponding optimization procedures, demonstrating the potential of freely shaping the cost functions through several numerical experiments.","sentences":["The growing scale and complexity of safety-critical control systems underscore the need to evolve current control architectures aiming for the unparalleled performances achievable through state-of-the-art optimization and machine learning algorithms.","However, maintaining closed-loop stability while boosting the performance of nonlinear control systems using data-driven and deep-learning approaches stands as an important unsolved challenge.","In this paper, we tackle the performance-boosting problem with closed-loop stability guarantees.","Specifically, we establish a synergy between the Internal Model Control (IMC) principle for nonlinear systems and state-of-the-art unconstrained optimization approaches for learning stable dynamics.","Our methods enable learning over arbitrarily deep neural network classes of performance-boosting controllers for stable nonlinear systems; crucially, we guarantee Lp closed-loop stability even if optimization is halted prematurely, and even when the ground-truth dynamics are unknown, with vanishing conservatism in the class of stabilizing policies as the model uncertainty is reduced to zero.","We discuss the implementation details of the proposed control schemes, including distributed ones, along with the corresponding optimization procedures, demonstrating the potential of freely shaping the cost functions through several numerical experiments."],"url":"http://arxiv.org/abs/2405.00871v1","category":"eess.SY"}
{"created":"2024-05-01 21:09:26","title":"Two variational problems in K\u00e4hler geometry","abstract":"On a K\\\"ahler manifold we consider the problems of maximizing/minimizing Monge--Amp\\`ere energy over certain subsets of the space of K\\\"ahler potentials. Under suitable assumptions we prove that solutions to these variational problems exist, are unique, and have a simple characterization. We then use the extremals to construct hermitian metrics on holomorphic vector bundles, and investigate their curvature.","sentences":["On a K\\\"ahler manifold we consider the problems of maximizing/minimizing Monge--Amp\\`ere energy over certain subsets of the space of K\\\"ahler potentials.","Under suitable assumptions we prove that solutions to these variational problems exist, are unique, and have a simple characterization.","We then use the extremals to construct hermitian metrics on holomorphic vector bundles, and investigate their curvature."],"url":"http://arxiv.org/abs/2405.00869v1","category":"math.CV"}
{"created":"2024-05-01 20:58:40","title":"A Convex Formulation of the Soft-Capture Problem","abstract":"We present a fast trajectory optimization algorithm for the soft capture of uncooperative tumbling space objects. Our algorithm generates safe, dynamically feasible, and minimum-fuel trajectories for a six-degree-of-freedom servicing spacecraft to achieve soft capture (near-zero relative velocity at contact) between predefined locations on the servicer spacecraft and target body. We solve a convex problem by enforcing a convex relaxation of the field-of-view constraint, followed by a sequential convex program correcting the trajectory for collision avoidance. The optimization problems can be solved with a standard second-order cone programming solver, making the algorithm both fast and practical for implementation in flight software. We demonstrate the performance and robustness of our algorithm in simulation over a range of object tumble rates up to 10{\\deg}/s.","sentences":["We present a fast trajectory optimization algorithm for the soft capture of uncooperative tumbling space objects.","Our algorithm generates safe, dynamically feasible, and minimum-fuel trajectories for a six-degree-of-freedom servicing spacecraft to achieve soft capture (near-zero relative velocity at contact) between predefined locations on the servicer spacecraft and target body.","We solve a convex problem by enforcing a convex relaxation of the field-of-view constraint, followed by a sequential convex program correcting the trajectory for collision avoidance.","The optimization problems can be solved with a standard second-order cone programming solver, making the algorithm both fast and practical for implementation in flight software.","We demonstrate the performance and robustness of our algorithm in simulation over a range of object tumble rates up to 10{\\deg}/s."],"url":"http://arxiv.org/abs/2405.00867v1","category":"cs.RO"}
{"created":"2024-05-01 20:27:29","title":"Evolution of Flare Activity in GKM Stars Younger than 300 Myr over Five Years of TESS Observations","abstract":"Stellar flares are short-duration ($<$ hours) bursts of radiation associated with surface magnetic reconnection events. Stellar magnetic activity generally decreases as a function of both age and Rossby number, $R_0$, a measure of the relative importance of the convective and rotational dynamos. Young stars ($<300$ Myr) have typically been overlooked in population-level flare studies due to challenges with flare-detection methods. Here, we select a sample of stars that are members of 26 nearby moving groups, clusters, or associations with ages $<$300 Myr that have been observed by the Transiting Exoplanet Survey Satellite at 2-minute cadence. We identified 26,355 flares originating from 3,157 stars and robustly measure the rotation periods of 1,847 stars. We measure and find the flare frequency distribution (FFD) slope, $\\alpha$, saturates for all spectral types at $\\alpha \\sim -0.5$ and is constant over 300 Myr. Additionally, we find that flare rates for stars $t_\\textrm{age} = 50 - 250$ Myr are saturated below $R_0 < 0.14$, which is consistent with other indicators of magnetic activity. We find evidence of annual flare rate variability in eleven stars, potentially correlated with long term stellar activity cycles. Additionally, we cross match our entire sample with GALEX and find no correlation between flare rate and Far- and Near-Ultraviolet flux. Finally, we find the flare rates of planet hosting stars are relatively lower than comparable, larger samples of stars, which may have ramifications for the atmospheric evolution of short-period exoplanets.","sentences":["Stellar flares are short-duration ($<$ hours) bursts of radiation associated with surface magnetic reconnection events.","Stellar magnetic activity generally decreases as a function of both age and Rossby number, $R_0$, a measure of the relative importance of the convective and rotational dynamos.","Young stars ($<300$ Myr) have typically been overlooked in population-level flare studies due to challenges with flare-detection methods.","Here, we select a sample of stars that are members of 26 nearby moving groups, clusters, or associations with ages $<$300 Myr that have been observed by the Transiting Exoplanet Survey Satellite at 2-minute cadence.","We identified 26,355 flares originating from 3,157 stars and robustly measure the rotation periods of 1,847 stars.","We measure and find the flare frequency distribution (FFD) slope, $\\alpha$, saturates for all spectral types at $\\alpha \\sim -0.5$ and is constant over 300 Myr.","Additionally, we find that flare rates for stars $t_\\textrm{age} = 50 - 250$ Myr are saturated below $R_0 < 0.14$, which is consistent with other indicators of magnetic activity.","We find evidence of annual flare rate variability in eleven stars, potentially correlated with long term stellar activity cycles.","Additionally, we cross match our entire sample with GALEX and find no correlation between flare rate and Far- and Near-Ultraviolet flux.","Finally, we find the flare rates of planet hosting stars are relatively lower than comparable, larger samples of stars, which may have ramifications for the atmospheric evolution of short-period exoplanets."],"url":"http://arxiv.org/abs/2405.00850v1","category":"astro-ph.SR"}
{"created":"2024-05-01 20:23:37","title":"Gravitational algebras and the generalized second law","abstract":"We derive the generalized second law (GSL) for arbitrary cuts of Killing horizons from the perspective of crossed-product gravitational algebras, making use of a recent proposal by one of us for the construction of local gravitational algebras. This construction relies on the existence of a state whose modular flow is geometric on the horizon. In both free and interacting quantum field theories, such states are guaranteed to exist by the properties of half-sided translations on the horizon. Using geometric identities derived from the canonical analysis of general relativity on null surfaces, we show that the crossed product entropy agrees with the generalized entropy of the horizon cut in a semiclassical limit, and further reproduce Wall's result relating the GSL to monotonicity of relative entropy of the quantum field algebras. We also give a novel generalization of the GSL for interacting theories in asymptotically flat spacetimes involving the concept of an algebra at infinity for a half-sided translation, which accounts for triviality of the algebra of fields smeared only on the horizon. Going beyond the semiclassical limit, we compute subleading corrections to the crossed product entropy, but are unable to determine if the GSL continues to hold after accounting for these. We speculate that an improved GSL could follow from a hidden subalgebra structure of the crossed products, assuming the existence of an operator-valued weight between horizon cut algebras.","sentences":["We derive the generalized second law (GSL) for arbitrary cuts of Killing horizons from the perspective of crossed-product gravitational algebras, making use of a recent proposal by one of us for the construction of local gravitational algebras.","This construction relies on the existence of a state whose modular flow is geometric on the horizon.","In both free and interacting quantum field theories, such states are guaranteed to exist by the properties of half-sided translations on the horizon.","Using geometric identities derived from the canonical analysis of general relativity on null surfaces, we show that the crossed product entropy agrees with the generalized entropy of the horizon cut in a semiclassical limit, and further reproduce Wall's result relating the GSL to monotonicity of relative entropy of the quantum field algebras.","We also give a novel generalization of the GSL for interacting theories in asymptotically flat spacetimes involving the concept of an algebra at infinity for a half-sided translation, which accounts for triviality of the algebra of fields smeared only on the horizon.","Going beyond the semiclassical limit, we compute subleading corrections to the crossed product entropy, but are unable to determine if the GSL continues to hold after accounting for these.","We speculate that an improved GSL could follow from a hidden subalgebra structure of the crossed products, assuming the existence of an operator-valued weight between horizon cut algebras."],"url":"http://arxiv.org/abs/2405.00847v1","category":"hep-th"}
{"created":"2024-05-01 20:21:44","title":"Gameplay Filters: Safe Robot Walking through Adversarial Imagination","abstract":"Ensuring the safe operation of legged robots in uncertain, novel environments is crucial to their widespread adoption. Despite recent advances in safety filters that can keep arbitrary task-driven policies from incurring safety failures, existing solutions for legged robot locomotion still rely on simplified dynamics and may fail when the robot is perturbed away from predefined stable gaits. This paper presents a general approach that leverages offline game-theoretic reinforcement learning to synthesize a highly robust safety filter for high-order nonlinear dynamics. This gameplay filter then maintains runtime safety by continually simulating adversarial futures and precluding task-driven actions that would cause it to lose future games (and thereby violate safety). Validated on a 36-dimensional quadruped robot locomotion task, the gameplay safety filter exhibits inherent robustness to the sim-to-real gap without manual tuning or heuristic designs. Physical experiments demonstrate the effectiveness of the gameplay safety filter under perturbations, such as tugging and unmodeled irregular terrains, while simulation studies shed light on how to trade off computation and conservativeness without compromising safety.","sentences":["Ensuring the safe operation of legged robots in uncertain, novel environments is crucial to their widespread adoption.","Despite recent advances in safety filters that can keep arbitrary task-driven policies from incurring safety failures, existing solutions for legged robot locomotion still rely on simplified dynamics and may fail when the robot is perturbed away from predefined stable gaits.","This paper presents a general approach that leverages offline game-theoretic reinforcement learning to synthesize a highly robust safety filter for high-order nonlinear dynamics.","This gameplay filter then maintains runtime safety by continually simulating adversarial futures and precluding task-driven actions that would cause it to lose future games (and thereby violate safety).","Validated on a 36-dimensional quadruped robot locomotion task, the gameplay safety filter exhibits inherent robustness to the sim-to-real gap without manual tuning or heuristic designs.","Physical experiments demonstrate the effectiveness of the gameplay safety filter under perturbations, such as tugging and unmodeled irregular terrains, while simulation studies shed light on how to trade off computation and conservativeness without compromising safety."],"url":"http://arxiv.org/abs/2405.00846v1","category":"cs.RO"}
{"created":"2024-05-01 19:56:52","title":"Locality Regularized Reconstruction: Structured Sparsity and Delaunay Triangulations","abstract":"Linear representation learning is widely studied due to its conceptual simplicity and empirical utility in tasks such as compression, classification, and feature extraction. Given a set of points $[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n] = \\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and a vector $\\mathbf{y} \\in \\mathbb{R}^d$, the goal is to find coefficients $\\mathbf{w} \\in \\mathbb{R}^n$ so that $\\mathbf{X} \\mathbf{w} \\approx \\mathbf{y}$, subject to some desired structure on $\\mathbf{w}$. In this work we seek $\\mathbf{w}$ that forms a local reconstruction of $\\mathbf{y}$ by solving a regularized least squares regression problem. We obtain local solutions through a locality function that promotes the use of columns of $\\mathbf{X}$ that are close to $\\mathbf{y}$ when used as a regularization term. We prove that, for all levels of regularization and under a mild condition that the columns of $\\mathbf{X}$ have a unique Delaunay triangulation, the optimal coefficients' number of non-zero entries is upper bounded by $d+1$, thereby providing local sparse solutions when $d \\ll n$. Under the same condition we also show that for any $\\mathbf{y}$ contained in the convex hull of $\\mathbf{X}$ there exists a regime of regularization parameter such that the optimal coefficients are supported on the vertices of the Delaunay simplex containing $\\mathbf{y}$. This provides an interpretation of the sparsity as having structure obtained implicitly from the Delaunay triangulation of $\\mathbf{X}$. We demonstrate that our locality regularized problem can be solved in comparable time to other methods that identify the containing Delaunay simplex.","sentences":["Linear representation learning is widely studied due to its conceptual simplicity and empirical utility in tasks such as compression, classification, and feature extraction.","Given a set of points $[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n] = \\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and a vector $\\mathbf{y} \\in \\mathbb{R}^d$, the goal is to find coefficients $\\mathbf{w} \\in \\mathbb{R}^n$ so that $\\mathbf{X} \\mathbf{w} \\approx \\mathbf{y}$, subject to some desired structure on $\\mathbf{w}$. In this work we seek $\\mathbf{w}$ that forms a local reconstruction of $\\mathbf{y}$ by solving a regularized least squares regression problem.","We obtain local solutions through a locality function that promotes the use of columns of $\\mathbf{X}$ that are close to $\\mathbf{y}$ when used as a regularization term.","We prove that, for all levels of regularization and under a mild condition that the columns of $\\mathbf{X}$ have a unique Delaunay triangulation, the optimal coefficients' number of non-zero entries is upper bounded by $d+1$, thereby providing local sparse solutions when $d \\ll n$. Under the same condition we also show that for any $\\mathbf{y}$ contained in the convex hull of $\\mathbf{X}$ there exists a regime of regularization parameter such that the optimal coefficients are supported on the vertices of the Delaunay simplex containing $\\mathbf{y}$. This provides an interpretation of the sparsity as having structure obtained implicitly from the Delaunay triangulation of $\\mathbf{X}$. We demonstrate that our locality regularized problem can be solved in comparable time to other methods that identify the containing Delaunay simplex."],"url":"http://arxiv.org/abs/2405.00837v1","category":"cs.LG"}
{"created":"2024-05-01 19:48:30","title":"No $\u03bd$s is Good News","abstract":"The baryon acoustic oscillation (BAO) analysis from the first year of data from the Dark Energy Spectroscopic Instrument (DESI), when combined with data from the cosmic microwave background (CMB), has placed an upper-limit on the sum of neutrino masses, $\\sum m_\\nu < 70$ meV (95%). In addition to excluding the minimum sum associated with the inverted hierarchy, the posterior is peaked at $\\sum m_\\nu = 0$ and is close to excluding even the minumum sum, 58 meV at 2$\\sigma$. In this paper, we explore the implications of this data for cosmology and particle physics. The sum of neutrino mass is determined in cosmology from the suppression of clustering in the late universe. Allowing the clustering to be enhanced, we extended the DESI analysis to $\\sum m_\\nu < 0$ and find $\\sum m_\\nu = - 160 \\pm 90$ meV (68%), and that the suppression of power from the minimum sum of neutrino masses is excluded at 99% confidence. We show this preference for negative masses makes it challenging to explain the result by a shift of cosmic parameters, such as the optical depth or matter density. We then show how a result of $\\sum m_\\nu =0$ could arise from new physics in the neutrino sector, including decay, cooling, and/or time-dependent masses. These models are consistent with current observations but imply new physics that is accessible in a wide range of experiments. In addition, we discuss how an apparent signal with $\\sum m_\\nu < 0$ can arise from new long range forces in the dark sector or from a primordial trispectrum that resembles the signal of CMB lensing.","sentences":["The baryon acoustic oscillation (BAO) analysis from the first year of data from the Dark Energy Spectroscopic Instrument (DESI), when combined with data from the cosmic microwave background (CMB), has placed an upper-limit on the sum of neutrino masses, $\\sum m_\\nu < 70$ meV (95%).","In addition to excluding the minimum sum associated with the inverted hierarchy, the posterior is peaked at $\\sum m_\\nu = 0$ and is close to excluding even the minumum sum, 58 meV at 2$\\sigma$. In this paper, we explore the implications of this data for cosmology and particle physics.","The sum of neutrino mass is determined in cosmology from the suppression of clustering in the late universe.","Allowing the clustering to be enhanced, we extended the DESI analysis to $\\sum m_\\nu < 0$ and find $\\sum m_\\nu = - 160 \\pm 90$ meV (68%), and that the suppression of power from the minimum sum of neutrino masses is excluded at 99% confidence.","We show this preference for negative masses makes it challenging to explain the result by a shift of cosmic parameters, such as the optical depth or matter density.","We then show how a result of $\\sum m_\\nu =0$ could arise from new physics in the neutrino sector, including decay, cooling, and/or time-dependent masses.","These models are consistent with current observations but imply new physics that is accessible in a wide range of experiments.","In addition, we discuss how an apparent signal with $\\sum m_\\nu < 0$ can arise from new long range forces in the dark sector or from a primordial trispectrum that resembles the signal of CMB lensing."],"url":"http://arxiv.org/abs/2405.00836v1","category":"astro-ph.CO"}
{"created":"2024-05-01 19:46:58","title":"Individual-level models of disease transmission incorporating piecewise spatial risk functions","abstract":"Modelling epidemics is crucial for understanding the emergence, transmission, impact and control of diseases. Spatial individual-level models (ILMs) that account for population heterogeneity are a useful tool, accounting for factors such as location, vaccination status and genetic information. Parametric forms for spatial risk functions, or kernels, are often used, but rely on strong assumptions about underlying transmission mechanisms. Here, we propose a class of non-parametric spatial disease transmission model, fitted within a Bayesian Markov chain Monte Carlo (MCMC) framework, allowing for more flexible assumptions when estimating the effect on spatial distance and infection risk. We focus upon two specific forms of non-parametric spatial infection kernel: piecewise constant and piecewise linear. Although these are relatively simple forms, we find them effective. The performance of these models is examined using simulated data, including under circumstances of model misspecification, and then applied to data from the UK 2001 foot-and-mouth disease.","sentences":["Modelling epidemics is crucial for understanding the emergence, transmission, impact and control of diseases.","Spatial individual-level models (ILMs) that account for population heterogeneity are a useful tool, accounting for factors such as location, vaccination status and genetic information.","Parametric forms for spatial risk functions, or kernels, are often used, but rely on strong assumptions about underlying transmission mechanisms.","Here, we propose a class of non-parametric spatial disease transmission model, fitted within a Bayesian Markov chain Monte Carlo (MCMC) framework, allowing for more flexible assumptions when estimating the effect on spatial distance and infection risk.","We focus upon two specific forms of non-parametric spatial infection kernel: piecewise constant and piecewise linear.","Although these are relatively simple forms, we find them effective.","The performance of these models is examined using simulated data, including under circumstances of model misspecification, and then applied to data from the UK 2001 foot-and-mouth disease."],"url":"http://arxiv.org/abs/2405.00835v1","category":"stat.CO"}
{"created":"2024-05-01 19:15:10","title":"Overcoming model uncertainty -- how equivalence tests can benefit from model averaging","abstract":"A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them. In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures. In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.","sentences":["A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups.","In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments.","Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold.","Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest.","However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate.","Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them.","In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice.","However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures.","In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty.","Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing.","We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data."],"url":"http://arxiv.org/abs/2405.00827v1","category":"stat.ME"}
{"created":"2024-05-01 19:11:47","title":"Efficient and Responsible Adaptation of Large Language Models for Robust Top-k Recommendations","abstract":"Conventional recommendation systems (RSs) are typically optimized to enhance performance metrics uniformly across all training samples.   This makes it hard for data-driven RSs to cater to a diverse set of users due to the varying properties of these users. The performance disparity among various populations can harm the model's robustness with respect to sub-populations. While recent works have shown promising results in adapting large language models (LLMs) for recommendation to address hard samples, long user queries from millions of users can degrade the performance of LLMs and elevate costs, processing times and inference latency. This challenges the practical applicability of LLMs for recommendations. To address this, we propose a hybrid task allocation framework that utilizes the capabilities of both LLMs and traditional RSs. By adopting a two-phase approach to improve robustness to sub-populations, we promote a strategic assignment of tasks for efficient and responsible adaptation of LLMs. Our strategy works by first identifying the weak and inactive users that receive a suboptimal ranking performance by RSs. Next, we use an in-context learning approach for such users, wherein each user interaction history is contextualized as a distinct ranking task and given to an LLM. We test our hybrid framework by incorporating various recommendation algorithms -- collaborative filtering and learning-to-rank recommendation models -- and two LLMs -- both open and close-sourced. Our results on three real-world datasets show a significant reduction in weak users and improved robustness of RSs to sub-populations $(\\approx12\\%)$ and overall performance without disproportionately escalating costs.","sentences":["Conventional recommendation systems (RSs) are typically optimized to enhance performance metrics uniformly across all training samples.   ","This makes it hard for data-driven RSs to cater to a diverse set of users due to the varying properties of these users.","The performance disparity among various populations can harm the model's robustness with respect to sub-populations.","While recent works have shown promising results in adapting large language models (LLMs) for recommendation to address hard samples, long user queries from millions of users can degrade the performance of LLMs and elevate costs, processing times and inference latency.","This challenges the practical applicability of LLMs for recommendations.","To address this, we propose a hybrid task allocation framework that utilizes the capabilities of both LLMs and traditional RSs.","By adopting a two-phase approach to improve robustness to sub-populations, we promote a strategic assignment of tasks for efficient and responsible adaptation of LLMs.","Our strategy works by first identifying the weak and inactive users that receive a suboptimal ranking performance by RSs.","Next, we use an in-context learning approach for such users, wherein each user interaction history is contextualized as a distinct ranking task and given to an LLM.","We test our hybrid framework by incorporating various recommendation algorithms -- collaborative filtering and learning-to-rank recommendation models -- and two LLMs -- both open and close-sourced.","Our results on three real-world datasets show a significant reduction in weak users and improved robustness of RSs to sub-populations $(\\approx12\\%)$ and overall performance without disproportionately escalating costs."],"url":"http://arxiv.org/abs/2405.00824v1","category":"cs.IR"}
{"created":"2024-05-01 19:02:40","title":"Kernel-based Learning for Safe Control of Discrete-Time Unknown Systems under Incomplete Observations","abstract":"Safe control for dynamical systems is critical, yet the presence of unknown dynamics poses significant challenges. In this paper, we present a learning-based control approach for tracking control of a class of high-order systems, operating under the constraint of partially observable states. The uncertainties inherent within the systems are modeled by kernel ridge regression, leveraging the proposed strategic data acquisition approach with limited state measurements. To achieve accurate trajectory tracking, a state observer that seamlessly integrates with the control law is devised. The analysis of the guaranteed control performance is conducted using Lyapunov theory due to the deterministic prediction error bound of kernel ridge regression, ensuring the adaptability of the approach in safety-critical scenarios. To demonstrate the effectiveness of our proposed approach, numerical simulations are performed, underscoring its contributions to the advancement of control strategies.","sentences":["Safe control for dynamical systems is critical, yet the presence of unknown dynamics poses significant challenges.","In this paper, we present a learning-based control approach for tracking control of a class of high-order systems, operating under the constraint of partially observable states.","The uncertainties inherent within the systems are modeled by kernel ridge regression, leveraging the proposed strategic data acquisition approach with limited state measurements.","To achieve accurate trajectory tracking, a state observer that seamlessly integrates with the control law is devised.","The analysis of the guaranteed control performance is conducted using Lyapunov theory due to the deterministic prediction error bound of kernel ridge regression, ensuring the adaptability of the approach in safety-critical scenarios.","To demonstrate the effectiveness of our proposed approach, numerical simulations are performed, underscoring its contributions to the advancement of control strategies."],"url":"http://arxiv.org/abs/2405.00822v1","category":"eess.SY"}
{"created":"2024-05-01 19:02:35","title":"Uncovering Agendas: A Novel French & English Dataset for Agenda Detection on Social Media","abstract":"The behavior and decision making of groups or communities can be dramatically influenced by individuals pushing particular agendas, e.g., to promote or disparage a person or an activity, to call for action, etc.. In the examination of online influence campaigns, particularly those related to important political and social events, scholars often concentrate on identifying the sources responsible for setting and controlling the agenda (e.g., public media). In this article we present a methodology for detecting specific instances of agenda control through social media where annotated data is limited or non-existent. By using a modest corpus of Twitter messages centered on the 2022 French Presidential Elections, we carry out a comprehensive evaluation of various approaches and techniques that can be applied to this problem. Our findings demonstrate that by treating the task as a textual entailment problem, it is possible to overcome the requirement for a large annotated training dataset.","sentences":["The behavior and decision making of groups or communities can be dramatically influenced by individuals pushing particular agendas, e.g., to promote or disparage a person or an activity, to call for action, etc.. In the examination of online influence campaigns, particularly those related to important political and social events, scholars often concentrate on identifying the sources responsible for setting and controlling the agenda (e.g., public media).","In this article we present a methodology for detecting specific instances of agenda control through social media where annotated data is limited or non-existent.","By using a modest corpus of Twitter messages centered on the 2022 French Presidential Elections, we carry out a comprehensive evaluation of various approaches and techniques that can be applied to this problem.","Our findings demonstrate that by treating the task as a textual entailment problem, it is possible to overcome the requirement for a large annotated training dataset."],"url":"http://arxiv.org/abs/2405.00821v1","category":"cs.CL"}
{"created":"2024-05-01 18:37:16","title":"Sensing Spin Wave Excitations by Spin Defects in Few-Layer Thick Hexagonal Boron Nitride","abstract":"Optically active spin defects in wide band-gap semiconductors serve as a local sensor of multiple degrees of freedom in a variety of \"hard\" and \"soft\" condensed matter systems. Taking advantage of the recent progress on quantum sensing using van der Waals (vdW) quantum materials, here we report direct measurements of spin waves excited in magnetic insulator Y3Fe5O12 (YIG) by boron vacancy $V_B^-$ spin defects contained in few-layer thick hexagonal boron nitride nanoflakes. We show that the ferromagnetic resonance and parametric spin excitations can be effectively detected by $V_B^-$ spin defects under various experimental conditions through optically detected magnetic resonance measurements. The off-resonant dipole interaction between YIG magnons and $V_B^-$ spin defects is mediated by multi-magnon scattering processes, which may find relevant applications in a range of emerging quantum sensing, computing, and metrology technologies. Our results also highlight the opportunities offered by quantum spin defects in layered two-dimensional vdW materials for investigating local spin dynamic behaviors in magnetic solid-state matters.","sentences":["Optically active spin defects in wide band-gap semiconductors serve as a local sensor of multiple degrees of freedom in a variety of \"hard\" and \"soft\" condensed matter systems.","Taking advantage of the recent progress on quantum sensing using van der Waals (vdW) quantum materials, here we report direct measurements of spin waves excited in magnetic insulator Y3Fe5O12 (YIG) by boron vacancy $V_B^-$ spin defects contained in few-layer thick hexagonal boron nitride nanoflakes.","We show that the ferromagnetic resonance and parametric spin excitations can be effectively detected by $V_B^-$ spin defects under various experimental conditions through optically detected magnetic resonance measurements.","The off-resonant dipole interaction between YIG magnons and $V_B^-$ spin defects is mediated by multi-magnon scattering processes, which may find relevant applications in a range of emerging quantum sensing, computing, and metrology technologies.","Our results also highlight the opportunities offered by quantum spin defects in layered two-dimensional vdW materials for investigating local spin dynamic behaviors in magnetic solid-state matters."],"url":"http://arxiv.org/abs/2405.00802v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 18:31:36","title":"\"Ask Me Anything\": How Comcast Uses LLMs to Assist Agents in Real Time","abstract":"Customer service is how companies interface with their customers. It can contribute heavily towards the overall customer satisfaction. However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or \"chat bots\". On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment.   This raises the bar for customer service agents. They need to accurately understand the customer's question or concern, identify a solution that is acceptable yet feasible (and within the company's policy), all while handling multiple conversations at once.   In this work, we introduce \"Ask Me Anything\" (AMA) as an add-on feature to an agent-facing customer service interface. AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs. In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually. Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care.","sentences":["Customer service is how companies interface with their customers.","It can contribute heavily towards the overall customer satisfaction.","However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or \"chat bots\".","On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment.   ","This raises the bar for customer service agents.","They need to accurately understand the customer's question or concern, identify a solution that is acceptable yet feasible (and within the company's policy), all while handling multiple conversations at once.   ","In this work, we introduce \"Ask Me Anything\" (AMA) as an add-on feature to an agent-facing customer service interface.","AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs.","In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually.","Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care."],"url":"http://arxiv.org/abs/2405.00801v1","category":"cs.CL"}
{"created":"2024-05-01 18:16:55","title":"ADM: Accelerated Diffusion Model via Estimated Priors for Robust Motion Prediction under Uncertainties","abstract":"Motion prediction is a challenging problem in autonomous driving as it demands the system to comprehend stochastic dynamics and the multi-modal nature of real-world agent interactions. Diffusion models have recently risen to prominence, and have proven particularly effective in pedestrian motion prediction tasks. However, the significant time consumption and sensitivity to noise have limited the real-time predictive capability of diffusion models. In response to these impediments, we propose a novel diffusion-based, acceleratable framework that adeptly predicts future trajectories of agents with enhanced resistance to noise. The core idea of our model is to learn a coarse-grained prior distribution of trajectory, which can skip a large number of denoise steps. This advancement not only boosts sampling efficiency but also maintains the fidelity of prediction accuracy. Our method meets the rigorous real-time operational standards essential for autonomous vehicles, enabling prompt trajectory generation that is vital for secure and efficient navigation. Through extensive experiments, our method speeds up the inference time to 136ms compared to standard diffusion model, and achieves significant improvement in multi-agent motion prediction on the Argoverse 1 motion forecasting dataset.","sentences":["Motion prediction is a challenging problem in autonomous driving as it demands the system to comprehend stochastic dynamics and the multi-modal nature of real-world agent interactions.","Diffusion models have recently risen to prominence, and have proven particularly effective in pedestrian motion prediction tasks.","However, the significant time consumption and sensitivity to noise have limited the real-time predictive capability of diffusion models.","In response to these impediments, we propose a novel diffusion-based, acceleratable framework that adeptly predicts future trajectories of agents with enhanced resistance to noise.","The core idea of our model is to learn a coarse-grained prior distribution of trajectory, which can skip a large number of denoise steps.","This advancement not only boosts sampling efficiency but also maintains the fidelity of prediction accuracy.","Our method meets the rigorous real-time operational standards essential for autonomous vehicles, enabling prompt trajectory generation that is vital for secure and efficient navigation.","Through extensive experiments, our method speeds up the inference time to 136ms compared to standard diffusion model, and achieves significant improvement in multi-agent motion prediction on the Argoverse 1 motion forecasting dataset."],"url":"http://arxiv.org/abs/2405.00797v1","category":"cs.RO"}
{"created":"2024-05-01 18:08:03","title":"Error Exponent in Agnostic PAC Learning","abstract":"Statistical learning theory and the Probably Approximately Correct (PAC) criterion are the common approach to mathematical learning theory. PAC is widely used to analyze learning problems and algorithms, and have been studied thoroughly. Uniform worst case bounds on the convergence rate have been well established using, e.g., VC theory or Radamacher complexity. However, in a typical scenario the performance could be much better. In this paper, we consider PAC learning using a somewhat different tradeoff, the error exponent - a well established analysis method in Information Theory - which describes the exponential behavior of the probability that the risk will exceed a certain threshold as function of the sample size. We focus on binary classification and find, under some stability assumptions, an improved distribution dependent error exponent for a wide range of problems, establishing the exponential behavior of the PAC error probability in agnostic learning. Interestingly, under these assumptions, agnostic learning may have the same error exponent as realizable learning. The error exponent criterion can be applied to analyze knowledge distillation, a problem that so far lacks a theoretical analysis.","sentences":["Statistical learning theory and the Probably Approximately Correct (PAC) criterion are the common approach to mathematical learning theory.","PAC is widely used to analyze learning problems and algorithms, and have been studied thoroughly.","Uniform worst case bounds on the convergence rate have been well established using, e.g., VC theory or Radamacher complexity.","However, in a typical scenario the performance could be much better.","In this paper, we consider PAC learning using a somewhat different tradeoff, the error exponent - a well established analysis method in Information Theory - which describes the exponential behavior of the probability that the risk will exceed a certain threshold as function of the sample size.","We focus on binary classification and find, under some stability assumptions, an improved distribution dependent error exponent for a wide range of problems, establishing the exponential behavior of the PAC error probability in agnostic learning.","Interestingly, under these assumptions, agnostic learning may have the same error exponent as realizable learning.","The error exponent criterion can be applied to analyze knowledge distillation, a problem that so far lacks a theoretical analysis."],"url":"http://arxiv.org/abs/2405.00792v1","category":"cs.LG"}
{"created":"2024-05-01 18:00:05","title":"Could SBND-PRISM probe Lepton Flavor Violation?","abstract":"We investigate the possibility of using the Short-Baseline Near Detector (SBND) at Fermilab to constrain lepton flavor violating decays of pions and kaons. We study how to leverage SBND-PRISM, the use of the neutrino beam angular spread to mitigate systematic uncertainties, to enhance this analysis. We show that SBND-PRISM can put stringent limits on the flavor violating branching ratios $\\rm{BR}(\\pi^+ \\to \\mu^+ \\nu_e) = 8.9 \\times 10^{-4}$, $\\rm{BR}(K^+ \\to \\mu^+ \\nu_e) = 3.2 \\times 10^{-3}$, improving previous constraints by factors 9 and 1.25, respectively. We also estimate the SBND-PRISM sensitivity to lepton number violating decays, $\\rm{BR}(\\pi^+ \\to \\mu^+ \\overline{\\nu}_e)= 2.1 \\times 10^{-3}$ and $\\rm{BR}(K^+ \\to \\mu^+ \\overline{\\nu}_e) = 7.4 \\times 10^{-3}$, though not reaching previous BEBC limits. Last, we identify several ways how the SBND collaboration could improve this analysis.","sentences":["We investigate the possibility of using the Short-Baseline Near Detector (SBND) at Fermilab to constrain lepton flavor violating decays of pions and kaons.","We study how to leverage SBND-PRISM, the use of the neutrino beam angular spread to mitigate systematic uncertainties, to enhance this analysis.","We show that SBND-PRISM can put stringent limits on the flavor violating branching ratios $\\rm{BR}(\\pi^+ \\to \\mu^+ \\nu_e) = 8.9 \\times 10^{-4}$, $\\rm{BR}(K^+ \\to \\mu^+ \\nu_e) = 3.2 \\times 10^{-3}$, improving previous constraints by factors 9 and 1.25, respectively.","We also estimate the SBND-PRISM sensitivity to lepton number violating decays, $\\rm{BR}(\\pi^+ \\to \\mu^+ \\overline{\\nu}_e)= 2.1 \\times 10^{-3}$ and $\\rm{BR}(K^+ \\to \\mu^+ \\overline{\\nu}_e) = 7.4 \\times 10^{-3}$, though not reaching previous BEBC limits.","Last, we identify several ways how the SBND collaboration could improve this analysis."],"url":"http://arxiv.org/abs/2405.00777v1","category":"hep-ph"}
{"created":"2024-05-01 18:00:02","title":"Abell 0399-Abell 0401 radio bridge spectral index: the first multifrequency detection","abstract":"Recent low-frequency radio observations at 140 MHz discovered a 3 Mpc-long bridge of diffuse emission connecting the galaxy clusters Abell 0399 and Abell 0401. We present follow-up observations at 60 MHz to constrain the spectral index of the bridge, which so far has only been detected at 140 and 144 MHz. We analysed deep (~18 hours) LOw Frequency ARray (LOFAR) Low Band Antenna (LBA) data at 60 MHz to detect the bridge at very low frequencies. We then conducted a multi-frequency study with LOFAR HBA data at 144 MHz and uGMRT data at 400 MHz. Assuming second-order Fermi mechanisms for the re-acceleration of relativistic electrons driven by turbulence in the radio bridge regions, we compare the observed radio spectrum with theoretical synchrotron models. The bridge is detected in the 75'' resolution LOFAR image at 60 MHz and its emission fully connects the region between the two galaxy clusters. Between 60 MHz and 144 MHz we found an integrated spectral index value of -1.44 +\\- 0.16 for the bridge emission. For the first time, we produced spectral index and related uncertainties maps for a radio bridge. We produce a radio spectrum, which show significant steepening between 144 and 400 MHz. This detection at low frequencies provides important information on the models of particle acceleration and magnetic field structure on very extended scales. The spectral index gives important clues to the origin of inter-cluster diffuse emission. The steepening of the spectrum above 144 MHz can be explained in a turbulent re-acceleration framework, assuming that the acceleration timescales are longer than ~200 Myr.","sentences":["Recent low-frequency radio observations at 140 MHz discovered a 3 Mpc-long bridge of diffuse emission connecting the galaxy clusters Abell 0399 and Abell 0401.","We present follow-up observations at 60 MHz to constrain the spectral index of the bridge, which so far has only been detected at 140 and 144 MHz.","We analysed deep (~18 hours) LOw Frequency ARray (LOFAR) Low Band Antenna (LBA) data at 60 MHz to detect the bridge at very low frequencies.","We then conducted a multi-frequency study with LOFAR HBA data at 144 MHz and uGMRT data at 400 MHz.","Assuming second-order Fermi mechanisms for the re-acceleration of relativistic electrons driven by turbulence in the radio bridge regions, we compare the observed radio spectrum with theoretical synchrotron models.","The bridge is detected in the 75'' resolution LOFAR image at 60 MHz and its emission fully connects the region between the two galaxy clusters.","Between 60 MHz and 144 MHz we found an integrated spectral index value of -1.44 +\\- 0.16 for the bridge emission.","For the first time, we produced spectral index and related uncertainties maps for a radio bridge.","We produce a radio spectrum, which show significant steepening between 144 and 400 MHz.","This detection at low frequencies provides important information on the models of particle acceleration and magnetic field structure on very extended scales.","The spectral index gives important clues to the origin of inter-cluster diffuse emission.","The steepening of the spectrum above 144 MHz can be explained in a turbulent re-acceleration framework, assuming that the acceleration timescales are longer than ~200 Myr."],"url":"http://arxiv.org/abs/2405.00772v1","category":"astro-ph.CO"}
{"created":"2024-05-01 18:00:02","title":"Hidden sectors of Chern-Simons Matter theories and Exact Holography","abstract":"Chiral higher-spin gravity is a higher-spin extension of both self-dual Yang-Mills and self-dual gravity and is a unique local higher-spin gravity in four dimensions. Its existence implies that there are two closed subsectors in Chern-Simons matter theories. We make first steps in identifying these (anti-)chiral subsectors directly on the CFT side, which should result in a holographically dual pair where both sides are nontrivial, complete, yet exactly soluble. We also discuss closely related theories: self-dual Yang-Mills (SDYM) and self-dual gravity (SDGR) in the holographic context.","sentences":["Chiral higher-spin gravity is a higher-spin extension of both self-dual Yang-Mills and self-dual gravity and is a unique local higher-spin gravity in four dimensions.","Its existence implies that there are two closed subsectors in Chern-Simons matter theories.","We make first steps in identifying these (anti-)chiral subsectors directly on the CFT side, which should result in a holographically dual pair where both sides are nontrivial, complete, yet exactly soluble.","We also discuss closely related theories: self-dual Yang-Mills (SDYM) and self-dual gravity (SDGR) in the holographic context."],"url":"http://arxiv.org/abs/2405.00773v1","category":"hep-th"}
{"created":"2024-05-01 18:00:01","title":"Quantum-Classical Separations in Shallow-Circuit-Based Learning with and without Noises","abstract":"We study quantum-classical separations between classical and quantum supervised learning models based on constant depth (i.e., shallow) circuits, in scenarios with and without noises. We construct a classification problem defined by a noiseless shallow quantum circuit and rigorously prove that any classical neural network with bounded connectivity requires logarithmic depth to output correctly with a larger-than-exponentially-small probability. This unconditional near-optimal quantum-classical separation originates from the quantum nonlocality property that distinguishes quantum circuits from their classical counterparts. We further derive the noise thresholds for demonstrating such a separation on near-term quantum devices under the depolarization noise model. We prove that this separation will persist if the noise strength is upper bounded by an inverse polynomial with respect to the system size, and vanish if the noise strength is greater than an inverse polylogarithmic function. In addition, for quantum devices with constant noise strength, we prove that no super-polynomial classical-quantum separation exists for any classification task defined by shallow Clifford circuits, independent of the structures of the circuits that specify the learning models.","sentences":["We study quantum-classical separations between classical and quantum supervised learning models based on constant depth (i.e., shallow) circuits, in scenarios with and without noises.","We construct a classification problem defined by a noiseless shallow quantum circuit and rigorously prove that any classical neural network with bounded connectivity requires logarithmic depth to output correctly with a larger-than-exponentially-small probability.","This unconditional near-optimal quantum-classical separation originates from the quantum nonlocality property that distinguishes quantum circuits from their classical counterparts.","We further derive the noise thresholds for demonstrating such a separation on near-term quantum devices under the depolarization noise model.","We prove that this separation will persist if the noise strength is upper bounded by an inverse polynomial with respect to the system size, and vanish if the noise strength is greater than an inverse polylogarithmic function.","In addition, for quantum devices with constant noise strength, we prove that no super-polynomial classical-quantum separation exists for any classification task defined by shallow Clifford circuits, independent of the structures of the circuits that specify the learning models."],"url":"http://arxiv.org/abs/2405.00770v1","category":"quant-ph"}
{"created":"2024-05-01 18:00:00","title":"Introducing the DREAMS Project: DaRk mattEr and Astrophysics with Machine learning and Simulations","abstract":"We introduce the DREAMS project, an innovative approach to understanding the astrophysical implications of alternative dark matter models and their effects on galaxy formation and evolution. The DREAMS project will ultimately comprise thousands of cosmological hydrodynamic simulations that simultaneously vary over dark matter physics, astrophysics, and cosmology in modeling a range of systems -- from galaxy clusters to ultra-faint satellites. Such extensive simulation suites can provide adequate training sets for machine-learning-based analyses. This paper introduces two new cosmological hydrodynamical suites of Warm Dark Matter, each comprised of 1024 simulations generated using the Arepo code. One suite consists of uniform-box simulations covering a $(25~h^{-1}~{\\rm M}_\\odot)^3$ volume, while the other consists of Milky Way zoom-ins with sufficient resolution to capture the properties of classical satellites. For each simulation, the Warm Dark Matter particle mass is varied along with the initial density field and several parameters controlling the strength of baryonic feedback within the IllustrisTNG model. We provide two examples, separately utilizing emulators and Convolutional Neural Networks, to demonstrate how such simulation suites can be used to disentangle the effects of dark matter and baryonic physics on galactic properties. The DREAMS project can be extended further to include different dark matter models, galaxy formation physics, and astrophysical targets. In this way, it will provide an unparalleled opportunity to characterize uncertainties on predictions for small-scale observables, leading to robust predictions for testing the particle physics nature of dark matter on these scales.","sentences":["We introduce the DREAMS project, an innovative approach to understanding the astrophysical implications of alternative dark matter models and their effects on galaxy formation and evolution.","The DREAMS project will ultimately comprise thousands of cosmological hydrodynamic simulations that simultaneously vary over dark matter physics, astrophysics, and cosmology in modeling a range of systems -- from galaxy clusters to ultra-faint satellites.","Such extensive simulation suites can provide adequate training sets for machine-learning-based analyses.","This paper introduces two new cosmological hydrodynamical suites of Warm Dark Matter, each comprised of 1024 simulations generated using the Arepo code.","One suite consists of uniform-box simulations covering a $(25~h^{-1}~{\\rm M}_\\odot)^3$ volume, while the other consists of Milky Way zoom-ins with sufficient resolution to capture the properties of classical satellites.","For each simulation, the Warm Dark Matter particle mass is varied along with the initial density field and several parameters controlling the strength of baryonic feedback within the IllustrisTNG model.","We provide two examples, separately utilizing emulators and Convolutional Neural Networks, to demonstrate how such simulation suites can be used to disentangle the effects of dark matter and baryonic physics on galactic properties.","The DREAMS project can be extended further to include different dark matter models, galaxy formation physics, and astrophysical targets.","In this way, it will provide an unparalleled opportunity to characterize uncertainties on predictions for small-scale observables, leading to robust predictions for testing the particle physics nature of dark matter on these scales."],"url":"http://arxiv.org/abs/2405.00766v1","category":"astro-ph.GA"}
{"created":"2024-05-01 17:42:43","title":"Improving Data Cleaning Using Discrete Optimization","abstract":"One of the most important processing steps in any analysis pipeline is handling missing data. Traditional approaches simply delete any sample or feature with missing elements. Recent imputation methods replace missing data based on assumed relationships between observed data and the missing elements. However, there is a largely under-explored alternative amid these extremes. Partial deletion approaches remove excessive amounts of missing data, as defined by the user. They can be used in place of traditional deletion or as a precursor to imputation. In this manuscript, we expand upon the Mr. Clean suite of algorithms, focusing on the scenario where all missing data is removed. We show that the RowCol Integer Program can be recast as a Linear Program, thereby reducing runtime. Additionally, the Element Integer Program can be reformulated to reduce the number of variables and allow for high levels of parallelization. Using real-world data sets from genetic, gene expression, and single cell RNA-seq experiments we demonstrate that our algorithms outperform existing deletion techniques over several missingness values, balancing runtime and data retention. Our combined greedy algorithm retains the maximum number of valid elements in 126 of 150 scenarios and stays within 1\\% of maximum in 23 of the remaining experiments. The reformulated Element IP complements the greedy algorithm when removing all missing data, boasting a reduced runtime and increase in valid elements in larger data sets, over its generic counterpart. These two programs greatly increase the amount of valid data retained over traditional deletion techniques and further improve on existing partial deletion algorithms.","sentences":["One of the most important processing steps in any analysis pipeline is handling missing data.","Traditional approaches simply delete any sample or feature with missing elements.","Recent imputation methods replace missing data based on assumed relationships between observed data and the missing elements.","However, there is a largely under-explored alternative amid these extremes.","Partial deletion approaches remove excessive amounts of missing data, as defined by the user.","They can be used in place of traditional deletion or as a precursor to imputation.","In this manuscript, we expand upon the Mr. Clean suite of algorithms, focusing on the scenario where all missing data is removed.","We show that the RowCol Integer Program can be recast as a Linear Program, thereby reducing runtime.","Additionally, the Element Integer Program can be reformulated to reduce the number of variables and allow for high levels of parallelization.","Using real-world data sets from genetic, gene expression, and single cell RNA-seq experiments we demonstrate that our algorithms outperform existing deletion techniques over several missingness values, balancing runtime and data retention.","Our combined greedy algorithm retains the maximum number of valid elements in 126 of 150 scenarios and stays within 1\\% of maximum in 23 of the remaining experiments.","The reformulated Element IP complements the greedy algorithm when removing all missing data, boasting a reduced runtime and increase in valid elements in larger data sets, over its generic counterpart.","These two programs greatly increase the amount of valid data retained over traditional deletion techniques and further improve on existing partial deletion algorithms."],"url":"http://arxiv.org/abs/2405.00764v1","category":"cs.DB"}
{"created":"2024-05-02 14:58:44","title":"Position Paper: Beyond Robustness Against Single Attack Types","abstract":"Current research on defending against adversarial examples focuses primarily on achieving robustness against a single attack type such as $\\ell_2$ or $\\ell_{\\infty}$-bounded attacks. However, the space of possible perturbations is much larger and currently cannot be modeled by a single attack type. The discrepancy between the focus of current defenses and the space of attacks of interest calls to question the practicality of existing defenses and the reliability of their evaluation. In this position paper, we argue that the research community should look beyond single attack robustness, and we draw attention to three potential directions involving robustness against multiple attacks: simultaneous multiattack robustness, unforeseen attack robustness, and a newly defined problem setting which we call continual adaptive robustness. We provide a unified framework which rigorously defines these problem settings, synthesize existing research in these fields, and outline open directions. We hope that our position paper inspires more research in simultaneous multiattack, unforeseen attack, and continual adaptive robustness.","sentences":["Current research on defending against adversarial examples focuses primarily on achieving robustness against a single attack type such as $\\ell_2$ or $\\ell_{\\infty}$-bounded attacks.","However, the space of possible perturbations is much larger and currently cannot be modeled by a single attack type.","The discrepancy between the focus of current defenses and the space of attacks of interest calls to question the practicality of existing defenses and the reliability of their evaluation.","In this position paper, we argue that the research community should look beyond single attack robustness, and we draw attention to three potential directions involving robustness against multiple attacks: simultaneous multiattack robustness, unforeseen attack robustness, and a newly defined problem setting which we call continual adaptive robustness.","We provide a unified framework which rigorously defines these problem settings, synthesize existing research in these fields, and outline open directions.","We hope that our position paper inspires more research in simultaneous multiattack, unforeseen attack, and continual adaptive robustness."],"url":"http://arxiv.org/abs/2405.01349v1","category":"cs.LG"}
{"created":"2024-05-02 08:52:29","title":"Silencing the Risk, Not the Whistle: A Semi-automated Text Sanitization Tool for Mitigating the Risk of Whistleblower Re-Identification","abstract":"Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors. However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously. The specific content of their disclosures and their distinct writing style may re-identify them as the source. Legal measures, such as the EU WBD, are limited in their scope and effectiveness. Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward. However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity. They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other NE labels) and combinations thereof with placeholders. Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including writing style. Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility. Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text. We then use a LLM that we fine-tuned for paraphrasing to render this text coherent and style-neutral. We evaluate our tool's effectiveness using court cases from the ECHR and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution (AA) attacks and utility loss statistically using the popular IMDb62 movie reviews dataset. Our method can significantly reduce AA accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content's semantics.","sentences":["Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors.","However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously.","The specific content of their disclosures and their distinct writing style may re-identify them as the source.","Legal measures, such as the EU WBD, are limited in their scope and effectiveness.","Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward.","However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity.","They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other NE labels) and combinations thereof with placeholders.","Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including writing style.","Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility.","Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text.","We then use a LLM that we fine-tuned for paraphrasing to render this text coherent and style-neutral.","We evaluate our tool's effectiveness using court cases from the ECHR and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution (AA) attacks and utility loss statistically using the popular IMDb62 movie reviews dataset.","Our method can significantly reduce AA accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content's semantics."],"url":"http://arxiv.org/abs/2405.01097v1","category":"cs.CY"}
{"created":"2024-05-02 07:03:16","title":"Generating User Experience Based on Personas with AI Assistants","abstract":"Traditional UX development methodologies focus on developing ``one size fits all\" solutions and lack the flexibility to cater to diverse user needs. In response, a growing interest has arisen in developing more dynamic UX frameworks. However, existing approaches often cannot personalise user experiences and adapt to user feedback in real-time. Therefore, my research introduces a novel approach of combining Large Language Models and personas, to address these limitations. The research is structured around three areas: (1) a critical review of existing adaptive UX practices and the potential for their automation; (2) an investigation into the role and effectiveness of personas in enhancing UX adaptability; and (3) the proposal of a theoretical framework that leverages LLM capabilities to create more dynamic and responsive UX designs and guidelines.","sentences":["Traditional UX development methodologies focus on developing ``one size fits all\" solutions and lack the flexibility to cater to diverse user needs.","In response, a growing interest has arisen in developing more dynamic UX frameworks.","However, existing approaches often cannot personalise user experiences and adapt to user feedback in real-time.","Therefore, my research introduces a novel approach of combining Large Language Models and personas, to address these limitations.","The research is structured around three areas: (1) a critical review of existing adaptive UX practices and the potential for their automation; (2) an investigation into the role and effectiveness of personas in enhancing UX adaptability; and (3) the proposal of a theoretical framework that leverages LLM capabilities to create more dynamic and responsive UX designs and guidelines."],"url":"http://arxiv.org/abs/2405.01051v1","category":"cs.SE"}
{"created":"2024-05-02 06:55:21","title":"Differentiable Particles for General-Purpose Deformable Object Manipulation","abstract":"Deformable object manipulation is a long-standing challenge in robotics. While existing approaches often focus narrowly on a specific type of object, we seek a general-purpose algorithm, capable of manipulating many different types of objects: beans, rope, cloth, liquid, . . . . One key difficulty is a suitable representation, rich enough to capture object shape, dynamics for manipulation and yet simple enough to be acquired effectively from sensor data. Specifically, we propose Differentiable Particles (DiPac), a new algorithm for deformable object manipulation. DiPac represents a deformable object as a set of particles and uses a differentiable particle dynamics simulator to reason about robot manipulation. To find the best manipulation action, DiPac combines learning, planning, and trajectory optimization through differentiable trajectory tree optimization. Differentiable dynamics provides significant benefits and enable DiPac to (i) estimate the dynamics parameters efficiently, thereby narrowing the sim-to-real gap, and (ii) choose the best action by backpropagating the gradient along sampled trajectories. Both simulation and real-robot experiments show promising results. DiPac handles a variety of object types. By combining planning and learning, DiPac outperforms both pure model-based planning methods and pure data-driven learning methods. In addition, DiPac is robust and adapts to changes in dynamics, thereby enabling the transfer of an expert policy from one object to another with different physical properties, e.g., from a rigid rod to a deformable rope.","sentences":["Deformable object manipulation is a long-standing challenge in robotics.","While existing approaches often focus narrowly on a specific type of object, we seek a general-purpose algorithm, capable of manipulating many different types of objects: beans, rope, cloth, liquid, . . . .","One key difficulty is a suitable representation, rich enough to capture object shape, dynamics for manipulation and yet simple enough to be acquired effectively from sensor data.","Specifically, we propose Differentiable Particles (DiPac), a new algorithm for deformable object manipulation.","DiPac represents a deformable object as a set of particles and uses a differentiable particle dynamics simulator to reason about robot manipulation.","To find the best manipulation action, DiPac combines learning, planning, and trajectory optimization through differentiable trajectory tree optimization.","Differentiable dynamics provides significant benefits and enable DiPac to (i) estimate the dynamics parameters efficiently, thereby narrowing the sim-to-real gap, and (ii) choose the best action by backpropagating the gradient along sampled trajectories.","Both simulation and real-robot experiments show promising results.","DiPac handles a variety of object types.","By combining planning and learning, DiPac outperforms both pure model-based planning methods and pure data-driven learning methods.","In addition, DiPac is robust and adapts to changes in dynamics, thereby enabling the transfer of an expert policy from one object to another with different physical properties, e.g., from a rigid rod to a deformable rope."],"url":"http://arxiv.org/abs/2405.01044v1","category":"cs.RO"}
{"created":"2024-05-02 05:24:28","title":"Efficient and Adaptive Posterior Sampling Algorithms for Bandits","abstract":"We study Thompson Sampling-based algorithms for stochastic bandits with bounded rewards. As the existing problem-dependent regret bound for Thompson Sampling with Gaussian priors [Agrawal and Goyal, 2017] is vacuous when $T \\le 288 e^{64}$, we derive a more practical bound that tightens the coefficient of the leading term %from $288 e^{64}$ to $1270$. Additionally, motivated by large-scale real-world applications that require scalability, adaptive computational resource allocation, and a balance in utility and computation, we propose two parameterized Thompson Sampling-based algorithms: Thompson Sampling with Model Aggregation (TS-MA-$\\alpha$) and Thompson Sampling with Timestamp Duelling (TS-TD-$\\alpha$), where $\\alpha \\in [0,1]$ controls the trade-off between utility and computation. Both algorithms achieve $O \\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $K$ is the number of arms, $T$ is the finite learning horizon, and $\\Delta$ denotes the single round performance loss when pulling a sub-optimal arm.","sentences":["We study Thompson Sampling-based algorithms for stochastic bandits with bounded rewards.","As the existing problem-dependent regret bound for Thompson Sampling with Gaussian priors [Agrawal and Goyal, 2017] is vacuous when $T \\le 288","e^{64}$",", we derive a more practical bound that tightens the coefficient of the leading term %from $288 e^{64}$ to $1270$. Additionally, motivated by large-scale real-world applications that require scalability, adaptive computational resource allocation, and a balance in utility and computation, we propose two parameterized Thompson Sampling-based algorithms: Thompson Sampling with Model Aggregation (TS-MA-$\\alpha$) and Thompson Sampling with Timestamp Duelling (TS-TD-$\\alpha$), where $\\alpha \\in","[0,1]$ controls the trade-off between utility and computation.","Both algorithms achieve $O \\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $K$ is the number of arms, $T$ is the finite learning horizon, and $\\Delta$ denotes the single round performance loss when pulling a sub-optimal arm."],"url":"http://arxiv.org/abs/2405.01010v1","category":"cs.LG"}
{"created":"2024-05-02 05:23:58","title":"Tackling Graph Oversquashing by Global and Local Non-Dissipativity","abstract":"A common problem in Message-Passing Neural Networks is oversquashing -- the limited ability to facilitate effective information flow between distant nodes. Oversquashing is attributed to the exponential decay in information transmission as node distances increase. This paper introduces a novel perspective to address oversquashing, leveraging properties of global and local non-dissipativity, that enable the maintenance of a constant information flow rate. Namely, we present SWAN, a uniquely parameterized model GNN with antisymmetry both in space and weight domains, as a means to obtain non-dissipativity. Our theoretical analysis asserts that by achieving these properties, SWAN offers an enhanced ability to transmit information over extended distances. Empirical evaluations on synthetic and real-world benchmarks that emphasize long-range interactions validate the theoretical understanding of SWAN, and its ability to mitigate oversquashing.","sentences":["A common problem in Message-Passing Neural Networks is oversquashing -- the limited ability to facilitate effective information flow between distant nodes.","Oversquashing is attributed to the exponential decay in information transmission as node distances increase.","This paper introduces a novel perspective to address oversquashing, leveraging properties of global and local non-dissipativity, that enable the maintenance of a constant information flow rate.","Namely, we present SWAN, a uniquely parameterized model GNN with antisymmetry both in space and weight domains, as a means to obtain non-dissipativity.","Our theoretical analysis asserts that by achieving these properties, SWAN offers an enhanced ability to transmit information over extended distances.","Empirical evaluations on synthetic and real-world benchmarks that emphasize long-range interactions validate the theoretical understanding of SWAN, and its ability to mitigate oversquashing."],"url":"http://arxiv.org/abs/2405.01009v1","category":"cs.LG"}
{"created":"2024-05-02 02:30:39","title":"X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation","abstract":"Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry->Texture->Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: https://xmu-xiaoma666.github.io/Projects/X-Oscar/.","sentences":["Recent advancements in automatic 3D avatar generation guided by text have made significant progress.","However, existing methods have limitations such as oversaturation and low-quality output.","To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts.","It follows a sequential Geometry->Texture->Animation paradigm, simplifying optimization through step-by-step generation.","To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training.","Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization.","Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches.","Our anonymous project page: https://xmu-xiaoma666.github.io/Projects/X-Oscar/."],"url":"http://arxiv.org/abs/2405.00954v1","category":"cs.CV"}
{"created":"2024-05-02 02:20:12","title":"The Role of Model Architecture and Scale in Predicting Molecular Properties: Insights from Fine-Tuning RoBERTa, BART, and LLaMA","abstract":"This study introduces a systematic framework to compare the efficacy of Large Language Models (LLMs) for fine-tuning across various cheminformatics tasks. Employing a uniform training methodology, we assessed three well-known models-RoBERTa, BART, and LLaMA-on their ability to predict molecular properties using the Simplified Molecular Input Line Entry System (SMILES) as a universal molecular representation format. Our comparative analysis involved pre-training 18 configurations of these models, with varying parameter sizes and dataset scales, followed by fine-tuning them on six benchmarking tasks from DeepChem. We maintained consistent training environments across models to ensure reliable comparisons. This approach allowed us to assess the influence of model type, size, and training dataset size on model performance. Specifically, we found that LLaMA-based models generally offered the lowest validation loss, suggesting their superior adaptability across tasks and scales. However, we observed that absolute validation loss is not a definitive indicator of model performance - contradicts previous research - at least for fine-tuning tasks: instead, model size plays a crucial role. Through rigorous replication and validation, involving multiple training and fine-tuning cycles, our study not only delineates the strengths and limitations of each model type but also provides a robust methodology for selecting the most suitable LLM for specific cheminformatics applications. This research underscores the importance of considering model architecture and dataset characteristics in deploying AI for molecular property prediction, paving the way for more informed and effective utilization of AI in drug discovery and related fields.","sentences":["This study introduces a systematic framework to compare the efficacy of Large Language Models (LLMs) for fine-tuning across various cheminformatics tasks.","Employing a uniform training methodology, we assessed three well-known models-RoBERTa, BART, and LLaMA-on their ability to predict molecular properties using the Simplified Molecular Input Line Entry System (SMILES) as a universal molecular representation format.","Our comparative analysis involved pre-training 18 configurations of these models, with varying parameter sizes and dataset scales, followed by fine-tuning them on six benchmarking tasks from DeepChem.","We maintained consistent training environments across models to ensure reliable comparisons.","This approach allowed us to assess the influence of model type, size, and training dataset size on model performance.","Specifically, we found that LLaMA-based models generally offered the lowest validation loss, suggesting their superior adaptability across tasks and scales.","However, we observed that absolute validation loss is not a definitive indicator of model performance - contradicts previous research - at least for fine-tuning tasks: instead, model size plays a crucial role.","Through rigorous replication and validation, involving multiple training and fine-tuning cycles, our study not only delineates the strengths and limitations of each model type but also provides a robust methodology for selecting the most suitable LLM for specific cheminformatics applications.","This research underscores the importance of considering model architecture and dataset characteristics in deploying AI for molecular property prediction, paving the way for more informed and effective utilization of AI in drug discovery and related fields."],"url":"http://arxiv.org/abs/2405.00949v1","category":"cs.LG"}
{"created":"2024-05-02 00:34:10","title":"MTDT: A Multi-Task Deep Learning Digital Twin","abstract":"Traffic congestion has significant impacts on both the economy and the environment. Measures of Effectiveness (MOEs) have long been the standard for evaluating the level of service and operational efficiency of traffic intersections. However, the scarcity of traditional high-resolution loop detector data (ATSPM) presents challenges in accurately measuring MOEs or capturing the intricate temporospatial characteristics inherent in urban intersection traffic. In response to this challenge, we have introduced the Multi-Task Deep Learning Digital Twin (MTDT) as a solution for multifaceted and precise intersection traffic flow simulation. MTDT enables accurate, fine-grained estimation of loop detector waveform time series for each lane of movement, alongside successful estimation of several MOEs for each lane group associated with a traffic phase concurrently and for all approaches of an arbitrary urban intersection. Unlike existing deep learning methodologies, MTDT distinguishes itself through its adaptability to local temporal and spatial features, such as signal timing plans, intersection topology, driving behaviors, and turning movement counts. While maintaining a straightforward design, our model emphasizes the advantages of multi-task learning in traffic modeling. By consolidating the learning process across multiple tasks, MTDT demonstrates reduced overfitting, increased efficiency, and enhanced effectiveness by sharing representations learned by different tasks. Furthermore, our approach facilitates sequential computation and lends itself to complete parallelization through GPU implementation. This not only streamlines the computational process but also enhances scalability and performance.","sentences":["Traffic congestion has significant impacts on both the economy and the environment.","Measures of Effectiveness (MOEs) have long been the standard for evaluating the level of service and operational efficiency of traffic intersections.","However, the scarcity of traditional high-resolution loop detector data (ATSPM) presents challenges in accurately measuring MOEs or capturing the intricate temporospatial characteristics inherent in urban intersection traffic.","In response to this challenge, we have introduced the Multi-Task Deep Learning Digital Twin (MTDT) as a solution for multifaceted and precise intersection traffic flow simulation.","MTDT enables accurate, fine-grained estimation of loop detector waveform time series for each lane of movement, alongside successful estimation of several MOEs for each lane group associated with a traffic phase concurrently and for all approaches of an arbitrary urban intersection.","Unlike existing deep learning methodologies, MTDT distinguishes itself through its adaptability to local temporal and spatial features, such as signal timing plans, intersection topology, driving behaviors, and turning movement counts.","While maintaining a straightforward design, our model emphasizes the advantages of multi-task learning in traffic modeling.","By consolidating the learning process across multiple tasks, MTDT demonstrates reduced overfitting, increased efficiency, and enhanced effectiveness by sharing representations learned by different tasks.","Furthermore, our approach facilitates sequential computation and lends itself to complete parallelization through GPU implementation.","This not only streamlines the computational process but also enhances scalability and performance."],"url":"http://arxiv.org/abs/2405.00922v1","category":"cs.LG"}
{"created":"2024-05-01 22:17:57","title":"DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling","abstract":"Traditional language models operate autoregressively, i.e., they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that reduce net inference times. Our models $\\textit{dynamically}$ predict multiple tokens based on their confidence in the predicted joint probability distribution. We propose a lightweight technique to train these models, leveraging the weights of traditional autoregressive counterparts. Moreover, we propose novel ways to enhance the estimated joint probability to improve text generation quality, namely co-occurrence weighted masking and adaptive thresholding. We also propose systematic qualitative and quantitative methods to rigorously test the quality of generated text for non-autoregressive generation. One of the models in our suite, DynaMo-7.3B-T3, achieves same-quality generated text as the baseline (Pythia-6.9B) while achieving 2.57$\\times$ speed-up with only 5.87% and 2.67% parameter and training time overheads, respectively.","sentences":["Traditional language models operate autoregressively, i.e., they predict one token at a time.","Rapid explosion in model sizes has resulted in high inference times.","In this work, we propose DynaMo, a suite of multi-token prediction language models that reduce net inference times.","Our models $\\textit{dynamically}$ predict multiple tokens based on their confidence in the predicted joint probability distribution.","We propose a lightweight technique to train these models, leveraging the weights of traditional autoregressive counterparts.","Moreover, we propose novel ways to enhance the estimated joint probability to improve text generation quality, namely co-occurrence weighted masking and adaptive thresholding.","We also propose systematic qualitative and quantitative methods to rigorously test the quality of generated text for non-autoregressive generation.","One of the models in our suite, DynaMo-7.3B-T3, achieves same-quality generated text as the baseline (Pythia-6.9B) while achieving 2.57$\\times$ speed-up with only 5.87% and 2.67% parameter and training time overheads, respectively."],"url":"http://arxiv.org/abs/2405.00888v1","category":"cs.CL"}
{"created":"2024-05-01 22:17:16","title":"On the Role of Reflectarrays for Interplanetary Links","abstract":"Interplanetary links (IPL) serve as crucial enablers for space exploration, facilitating secure and adaptable space missions. An integrated IPL with inter-satellite communication (IP-ISL) establishes a unified deep space network, expanding coverage and reducing atmospheric losses. The challenges, including irregularities in charged density, hardware impairments, and hidden celestial body brightness are analyzed with a reflectarray-based IP-ISL between Earth and Moon orbiters. It is observed that $10^{-8}$ order severe hardware impairments with intense solar plasma density drops an ideal system's spectral efficiency (SE) from $\\sim\\!38~\\textrm{(bit/s)/Hz}$ down to $0~\\textrm{(bit/s)/Hz}$. An ideal full angle of arrival fluctuation recovery with full steering range achieves $\\sim\\!20~\\textrm{(bit/s)/Hz}$ gain and a limited beamsteering with a numerical reflectarray design achieves at least $\\sim\\!1~\\textrm{(bit/s)/Hz}$ gain in severe hardware impairment cases.","sentences":["Interplanetary links (IPL) serve as crucial enablers for space exploration, facilitating secure and adaptable space missions.","An integrated IPL with inter-satellite communication (IP-ISL) establishes a unified deep space network, expanding coverage and reducing atmospheric losses.","The challenges, including irregularities in charged density, hardware impairments, and hidden celestial body brightness are analyzed with a reflectarray-based IP-ISL between Earth and Moon orbiters.","It is observed that $10^{-8}$ order severe hardware impairments with intense solar plasma density drops an ideal system's spectral efficiency (SE) from $\\sim\\!38~\\textrm{(bit/s)/Hz}$ down to $0~\\textrm{(bit/s)/Hz}$. An ideal full angle of arrival fluctuation recovery with full steering range achieves $\\sim\\!20~\\textrm{(bit/s)/Hz}$ gain and a limited beamsteering with a numerical reflectarray design achieves at least $\\sim\\!1~\\textrm{(bit/s)/Hz}$ gain in severe hardware impairment cases."],"url":"http://arxiv.org/abs/2405.00887v1","category":"eess.SP"}
{"created":"2024-05-01 22:04:48","title":"Implementation of a Mesh refinement algorithm into the quasi-static PIC code QuickPIC","abstract":"Plasma-based acceleration (PBA) has emerged as a promising candidate for the accelerator technology used to build a future linear collider and/or an advanced light source. In PBA, a trailing or witness particle beam is accelerated in the plasma wave wakefield (WF) created by a laser or particle beam driver. The distance over which the drive beam evolves is several orders of magnitude larger than the wake wavelength. This large disparity in length scales is amenable to the quasi-static approach. Three-dimensional (3D), quasi-static (QS), particle-in-cell (PIC) codes, e.g., QuickPIC, have been shown to provide high fidelity simulation capability with 2-4 orders of magnitude speedup over 3D fully explicit PIC codes. We describe a mesh refinement scheme that has been implemented into the 3D QS PIC code, QuickPIC. We use a very fine (high) resolution in a small spatial region that includes the witness beam and progressively coarser resolutions in the rest of the simulation domain. A fast multigrid Poisson solver has been implemented for the field solve on the refined meshes and a Fast Fourier Transform (FFT) based Poisson solver is used for the coarse mesh. The code has been parallelized with both MPI and OpenMP, and the parallel scalability has also been improved by using pipelining. A preliminary adaptive mesh refinement technique is described to optimize the computational time for simulations with an evolving witness beam size. Several test problems are used to verify that the mesh refinement algorithm provides accurate results. The results are also compared to highly resolved simulations with near azimuthal symmetry using a new hybrid QS PIC code QPAD that uses a PIC description in the coordinates ($r$, $ct-z$) and a gridless description in the azimuthal angle, $\\phi$.","sentences":["Plasma-based acceleration (PBA) has emerged as a promising candidate for the accelerator technology used to build a future linear collider and/or an advanced light source.","In PBA, a trailing or witness particle beam is accelerated in the plasma wave wakefield (WF) created by a laser or particle beam driver.","The distance over which the drive beam evolves is several orders of magnitude larger than the wake wavelength.","This large disparity in length scales is amenable to the quasi-static approach.","Three-dimensional (3D), quasi-static (QS), particle-in-cell (PIC) codes, e.g., QuickPIC, have been shown to provide high fidelity simulation capability with 2-4 orders of magnitude speedup over 3D fully explicit PIC codes.","We describe a mesh refinement scheme that has been implemented into the 3D QS PIC code, QuickPIC.","We use a very fine (high) resolution in a small spatial region that includes the witness beam and progressively coarser resolutions in the rest of the simulation domain.","A fast multigrid Poisson solver has been implemented for the field solve on the refined meshes and a Fast Fourier Transform (FFT) based Poisson solver is used for the coarse mesh.","The code has been parallelized with both MPI and OpenMP, and the parallel scalability has also been improved by using pipelining.","A preliminary adaptive mesh refinement technique is described to optimize the computational time for simulations with an evolving witness beam size.","Several test problems are used to verify that the mesh refinement algorithm provides accurate results.","The results are also compared to highly resolved simulations with near azimuthal symmetry using a new hybrid QS PIC code QPAD that uses a PIC description in the coordinates ($r$, $ct-z$) and a gridless description in the azimuthal angle, $\\phi$."],"url":"http://arxiv.org/abs/2405.00886v1","category":"physics.plasm-ph"}
{"created":"2024-05-01 22:01:40","title":"WHALE-FL: Wireless and Heterogeneity Aware Latency Efficient Federated Learning over Mobile Devices via Adaptive Subnetwork Scheduling","abstract":"As a popular distributed learning paradigm, federated learning (FL) over mobile devices fosters numerous applications, while their practical deployment is hindered by participating devices' computing and communication heterogeneity. Some pioneering research efforts proposed to extract subnetworks from the global model, and assign as large a subnetwork as possible to the device for local training based on its full computing and communications capacity. Although such fixed size subnetwork assignment enables FL training over heterogeneous mobile devices, it is unaware of (i) the dynamic changes of devices' communication and computing conditions and (ii) FL training progress and its dynamic requirements of local training contributions, both of which may cause very long FL training delay. Motivated by those dynamics, in this paper, we develop a wireless and heterogeneity aware latency efficient FL (WHALE-FL) approach to accelerate FL training through adaptive subnetwork scheduling. Instead of sticking to the fixed size subnetwork, WHALE-FL introduces a novel subnetwork selection utility function to capture device and FL training dynamics, and guides the mobile device to adaptively select the subnetwork size for local training based on (a) its computing and communication capacity, (b) its dynamic computing and/or communication conditions, and (c) FL training status and its corresponding requirements for local training contributions. Our evaluation shows that, compared with peer designs, WHALE-FL effectively accelerates FL training without sacrificing learning accuracy.","sentences":["As a popular distributed learning paradigm, federated learning (FL) over mobile devices fosters numerous applications, while their practical deployment is hindered by participating devices' computing and communication heterogeneity.","Some pioneering research efforts proposed to extract subnetworks from the global model, and assign as large a subnetwork as possible to the device for local training based on its full computing and communications capacity.","Although such fixed size subnetwork assignment enables FL training over heterogeneous mobile devices, it is unaware of (i) the dynamic changes of devices' communication and computing conditions and (ii) FL training progress and its dynamic requirements of local training contributions, both of which may cause very long FL training delay.","Motivated by those dynamics, in this paper, we develop a wireless and heterogeneity aware latency efficient FL (WHALE-FL) approach to accelerate FL training through adaptive subnetwork scheduling.","Instead of sticking to the fixed size subnetwork, WHALE-FL introduces a novel subnetwork selection utility function to capture device and FL training dynamics, and guides the mobile device to adaptively select the subnetwork size for local training based on (a) its computing and communication capacity, (b) its dynamic computing and/or communication conditions, and (c) FL training status and its corresponding requirements for local training contributions.","Our evaluation shows that, compared with peer designs, WHALE-FL effectively accelerates FL training without sacrificing learning accuracy."],"url":"http://arxiv.org/abs/2405.00885v1","category":"cs.LG"}
{"created":"2024-05-01 20:45:08","title":"Quantifying small-scale anisotropy in turbulent flows","abstract":"The verification of whether small-scale turbulence is isotropic remains a grand challenge. The difficulty arises because the presence of small-scale anisotropy is tied to the dissipation tensor, whose components require the full three-dimensional information of the flow field in both high spatial and temporal resolution, a condition rarely satisfied in turbulence experiments, especially during field scale measurement of atmospheric turbulence. To circumvent this issue, an \\emph{intermittency-anisotropy} framework is proposed through which we successfully extract the features of small-scale anisotropy from single-point measurements of turbulent time series by exploiting the properties of small-scale intermittency. Specifically, this framework quantifies anisotropy by studying the contrasting effects of burst-like activities on the scale-wise production of turbulence kinetic energy between the horizontal and vertical directions. The veracity of this approach is tested by applying it over a range of datasets covering an unprecedented range in the Reynolds numbers ($Re \\approx 10^{3}$ to $10^{6}$), sampling frequencies (10 kHz to 10 Hz), surface conditions (aerodynamically smooth surfaces to typical grasslands to forest canopies), and flow types (channel flows, boundary layer flows, atmospheric flows, and flows over forest canopies). For these diverse datasets, the findings indicate that the effects of small-scale anisotropy persists up to the integral scales of the streamwise velocity fluctuations and there exists a universal relationship to predict this anisotropy from the two-component state of the Reynolds stress tensor. This relationship is important towards the development of next-generation closure models of wall-turbulence by incorporating the effects of anisotropy at smaller scales of the flow.","sentences":["The verification of whether small-scale turbulence is isotropic remains a grand challenge.","The difficulty arises because the presence of small-scale anisotropy is tied to the dissipation tensor, whose components require the full three-dimensional information of the flow field in both high spatial and temporal resolution, a condition rarely satisfied in turbulence experiments, especially during field scale measurement of atmospheric turbulence.","To circumvent this issue, an \\emph{intermittency-anisotropy} framework is proposed through which we successfully extract the features of small-scale anisotropy from single-point measurements of turbulent time series by exploiting the properties of small-scale intermittency.","Specifically, this framework quantifies anisotropy by studying the contrasting effects of burst-like activities on the scale-wise production of turbulence kinetic energy between the horizontal and vertical directions.","The veracity of this approach is tested by applying it over a range of datasets covering an unprecedented range in the Reynolds numbers ($Re \\approx 10^{3}$ to $10^{6}$), sampling frequencies (10 kHz to 10 Hz), surface conditions (aerodynamically smooth surfaces to typical grasslands to forest canopies), and flow types (channel flows, boundary layer flows, atmospheric flows, and flows over forest canopies).","For these diverse datasets, the findings indicate that the effects of small-scale anisotropy persists up to the integral scales of the streamwise velocity fluctuations and there exists a universal relationship to predict this anisotropy from the two-component state of the Reynolds stress tensor.","This relationship is important towards the development of next-generation closure models of wall-turbulence by incorporating the effects of anisotropy at smaller scales of the flow."],"url":"http://arxiv.org/abs/2405.00856v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 20:38:08","title":"Suppression of temperature-gradient-driven turbulence by sheared flows in fusion plasmas","abstract":"Starting from the assumption that saturation of plasma turbulence driven by temperature-gradient instabilities in fusion plasmas is achieved by a local energy cascade between a long-wavelength outer scale, where energy is injected into the fluctuations, and a small-wavelength dissipation scale, where fluctuation energy is thermalized by particle collisions, we formulate a detailed phenomenological theory for the influence of perpendicular flow shear on magnetized-plasma turbulence. Our theory introduces two distinct regimes, called the weak-shear and strong-shear regimes, each with its own set of scaling laws for the scale and amplitude of the fluctuations and for the level of turbulent heat transport. We discover that the ratio of the typical radial and poloidal wavenumbers of the fluctuations (i.e., their aspect ratio) at the outer scale plays a central role in determining the dependence of the turbulent transport on the imposed flow shear. Our theoretical predictions are found to be in excellent agreement with numerical simulations of two paradigmatic models of fusion-relevant plasma turbulence: (i) an electrostatic fluid model of slab electron-scale turbulence, and (ii) Cyclone-base-case gyrokinetic ion-scale turbulence. Additionally, our theory envisions a potential mechanism for the suppression of electron-scale turbulence by perpendicular ion-scale flows based on the role of the aforementioned aspect ratio of the electron-scale fluctuations.","sentences":["Starting from the assumption that saturation of plasma turbulence driven by temperature-gradient instabilities in fusion plasmas is achieved by a local energy cascade between a long-wavelength outer scale, where energy is injected into the fluctuations, and a small-wavelength dissipation scale, where fluctuation energy is thermalized by particle collisions, we formulate a detailed phenomenological theory for the influence of perpendicular flow shear on magnetized-plasma turbulence.","Our theory introduces two distinct regimes, called the weak-shear and strong-shear regimes, each with its own set of scaling laws for the scale and amplitude of the fluctuations and for the level of turbulent heat transport.","We discover that the ratio of the typical radial and poloidal wavenumbers of the fluctuations (i.e., their aspect ratio) at the outer scale plays a central role in determining the dependence of the turbulent transport on the imposed flow shear.","Our theoretical predictions are found to be in excellent agreement with numerical simulations of two paradigmatic models of fusion-relevant plasma turbulence: (i) an electrostatic fluid model of slab electron-scale turbulence, and (ii) Cyclone-base-case gyrokinetic ion-scale turbulence.","Additionally, our theory envisions a potential mechanism for the suppression of electron-scale turbulence by perpendicular ion-scale flows based on the role of the aforementioned aspect ratio of the electron-scale fluctuations."],"url":"http://arxiv.org/abs/2405.00854v1","category":"physics.plasm-ph"}
{"created":"2024-05-01 18:39:00","title":"Decoherence by warm horizons","abstract":"Recently Danielson, Satishchandran, and Wald (DSW) have shown that quantum superpositions held outside of Killing horizons will decohere at a steady rate. This occurs because of the inevitable radiation of soft photons (gravitons), which imprint a electromagnetic (gravitational) ``which-path'' memory onto the horizon. Rather than appealing to this global description, an experimenter ought to also have a local description for the cause of decoherence. One might intuitively guess that this is just the bombardment of Hawking/Unruh radiation on the system, however simple calculations challenge this idea -- the same superposition held in a finite temperature inertial laboratory does not decohere at the DSW rate. In this work we provide a local description of the decoherence by mapping the DSW set-up onto a worldline-localized model resembling an Unruh-DeWitt particle detector. We present an interpretation in terms of random local forces which do not sufficiently self-average over long times. Using the Rindler horizon as a concrete example we clarify the crucial role of temperature, and show that the Unruh effect is the only quantum mechanical effect underlying these random forces. A general lesson is that for an environment which induces Ohmic friction on the central system (as one gets from the classical Abraham-Lorentz-Dirac force, in an accelerating frame) the fluctuation-dissipation theorem implies that when this environment is at finite temperature it will cause steady decoherence on the central system. Our results agree with DSW and provide the complementary local perspective.","sentences":["Recently Danielson, Satishchandran, and Wald (DSW) have shown that quantum superpositions held outside of Killing horizons will decohere at a steady rate.","This occurs because of the inevitable radiation of soft photons (gravitons), which imprint a electromagnetic (gravitational) ``which-path'' memory onto the horizon.","Rather than appealing to this global description, an experimenter ought to also have a local description for the cause of decoherence.","One might intuitively guess that this is just the bombardment of Hawking/Unruh radiation on the system, however simple calculations challenge this idea -- the same superposition held in a finite temperature inertial laboratory does not decohere at the DSW rate.","In this work we provide a local description of the decoherence by mapping the DSW set-up onto a worldline-localized model resembling an Unruh-DeWitt particle detector.","We present an interpretation in terms of random local forces which do not sufficiently self-average over long times.","Using the Rindler horizon as a concrete example we clarify the crucial role of temperature, and show that the Unruh effect is the only quantum mechanical effect underlying these random forces.","A general lesson is that for an environment which induces Ohmic friction on the central system (as one gets from the classical Abraham-Lorentz-Dirac force, in an accelerating frame) the fluctuation-dissipation theorem implies that when this environment is at finite temperature it will cause steady decoherence on the central system.","Our results agree with DSW and provide the complementary local perspective."],"url":"http://arxiv.org/abs/2405.00804v1","category":"hep-th"}
{"created":"2024-05-01 18:27:10","title":"Irreversibility of mesoscopic processes with hydrodynamic interactions","abstract":"Microscopic colloidal particles are often used as probes to study the non-equilibrium activity of living matter or other complex systems. In many of these contexts hydrodynamic interactions between the probe particle and the system of interest play an important role. However little is known about what effect such interactions could have on the overall non-equilibrium characteristics of the system of interest. In this paper, we study two simple models experimentally and theoretically, which demonstrate that hydrodynamic interactions could either diminish or enhance the total entropy production of the combined system. Importantly, we show that our method of calculating entropy production helps identify heat flows consistently, even in the presence of hydrodynamic interactions. The results indicate that interactions can be finely tuned to optimize not only dynamic properties but also irreversibility and energy dissipation, thereby opening new avenues for tailored control and design of driven mesoscale systems.","sentences":["Microscopic colloidal particles are often used as probes to study the non-equilibrium activity of living matter or other complex systems.","In many of these contexts hydrodynamic interactions between the probe particle and the system of interest play an important role.","However little is known about what effect such interactions could have on the overall non-equilibrium characteristics of the system of interest.","In this paper, we study two simple models experimentally and theoretically, which demonstrate that hydrodynamic interactions could either diminish or enhance the total entropy production of the combined system.","Importantly, we show that our method of calculating entropy production helps identify heat flows consistently, even in the presence of hydrodynamic interactions.","The results indicate that interactions can be finely tuned to optimize not only dynamic properties but also irreversibility and energy dissipation, thereby opening new avenues for tailored control and design of driven mesoscale systems."],"url":"http://arxiv.org/abs/2405.00800v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-01 17:57:08","title":"Schwinger-Keldysh nonequilibrium quantum field theory of open quantum systems beyond the Markovian regime: Application to the spin-boson model","abstract":"We develop a Schwinger-Keldysh field theory (SKFT) for open quantum systems interacting with a dissipative environment and apply it to the spin-boson model as an archetypical example where the environment is composed of a bosonic bath. Prior SKFT developments of this type have been confined to the Markovian regime, as an alternative to a conventional description by the Lindblad quantum master equation (QME) which is a time-local matrix differential equation. Here we combine SKFT with a two-particle irreducible (2PI) action that resums a class of Feynman diagrams to infinite order. We obtain the time-evolution of the spin density matrix in the form of a system of integro-differential equations applicable to both Markovian and non-Markovian regimes. The latter regime--where taking into account memory effects becomes essential--poses a challenge for standard methods when trying to incorporate arbitrary properties of the system, bath, and length of time evolution. The SKFT+2PI-computed time evolution of the spin expectation values in the Markovian regime reproduces the solution of the Lindblad QME, as long as the system-bath coupling in the latter is adjusted by increasing it. In the non-Markovian regime, SKFT+2PI yields a nonperturbative solution that mimics results from both hierarchical equations of motion and tensor networks methods that we employ as benchmarks. Our SKFT+2PI approach can also access challenging cases, such as zero-temperature and sub-Ohmic bath, as well as arbitrary long evolution times. Taking into account favorable numerical cost of solving the integro-differential equations with increasing number of spins, time steps or dimensionality the SKFT+2PI approach offers a promising route for simulation of driven-dissipative systems in quantum computing or quantum magnonics and spintronics in the presence of a variety of (single or multiple) dissipative environments.","sentences":["We develop a Schwinger-Keldysh field theory (SKFT) for open quantum systems interacting with a dissipative environment and apply it to the spin-boson model as an archetypical example where the environment is composed of a bosonic bath.","Prior SKFT developments of this type have been confined to the Markovian regime, as an alternative to a conventional description by the Lindblad quantum master equation (QME) which is a time-local matrix differential equation.","Here we combine SKFT with a two-particle irreducible (2PI) action that resums a class of Feynman diagrams to infinite order.","We obtain the time-evolution of the spin density matrix in the form of a system of integro-differential equations applicable to both Markovian and non-Markovian regimes.","The latter regime--where taking into account memory effects becomes essential--poses a challenge for standard methods when trying to incorporate arbitrary properties of the system, bath, and length of time evolution.","The SKFT+2PI-computed time evolution of the spin expectation values in the Markovian regime reproduces the solution of the Lindblad QME, as long as the system-bath coupling in the latter is adjusted by increasing it.","In the non-Markovian regime, SKFT+2PI yields a nonperturbative solution that mimics results from both hierarchical equations of motion and tensor networks methods that we employ as benchmarks.","Our SKFT+2PI approach can also access challenging cases, such as zero-temperature and sub-Ohmic bath, as well as arbitrary long evolution times.","Taking into account favorable numerical cost of solving the integro-differential equations with increasing number of spins, time steps or dimensionality the SKFT+2PI approach offers a promising route for simulation of driven-dissipative systems in quantum computing or quantum magnonics and spintronics in the presence of a variety of (single or multiple) dissipative environments."],"url":"http://arxiv.org/abs/2405.00765v1","category":"quant-ph"}
{"created":"2024-05-02 17:48:47","title":"The heat equation with time-correlated random potential in d=2: Edwards-Wilkinson fluctuations","abstract":"We consider the stochastic PDE:   $\\partial_tu(t,x)=\\frac{1}{2}\\Delta u(t,x)+{\\beta}{}u(t,x)V(t,x),$   in dimension $d=2$, where the potential V is the space and time mollification of the two-dimensional space-time white noise. We show that after renormalizing, the fluctuations of the solution converge to the Edwards-Wilkinson limit with an explicit effective variance and constant effective diffusivity. Our main tool is a Markov chain on the space of paths which we use to establish an extension of the Kallianpur-Robbins law to a specific regenerative process.","sentences":["We consider the stochastic PDE:   $\\partial_tu(t,x)=\\frac{1}{2}\\Delta","u(t,x)+{\\beta}{}u(t,x)V(t,x),$   in dimension $d=2$, where the potential V is the space and time mollification of the two-dimensional space-time white noise.","We show that after renormalizing, the fluctuations of the solution converge to the Edwards-Wilkinson limit with an explicit effective variance and constant effective diffusivity.","Our main tool is a Markov chain on the space of paths which we use to establish an extension of the Kallianpur-Robbins law to a specific regenerative process."],"url":"http://arxiv.org/abs/2405.01519v1","category":"math.PR"}
{"created":"2024-05-02 17:08:59","title":"Manifold with infinitely many fibrations over the sphere","abstract":"We show that the manifold $X=S^2\\times S^3$ has infinitely many structures of a fiber bundle over the base $B=S^2.$ In fact for every lens space $L(p,1)$ there is a fibration $L(p,1)\\to X\\to B.$","sentences":["We show that the manifold $X=S^2\\times S^3$ has infinitely many structures of a fiber bundle over the base $B=S^2.$","In fact for every lens space $L(p,1)$ there is a fibration $L(p,1)\\to X\\to B.$"],"url":"http://arxiv.org/abs/2405.01476v1","category":"math.DG"}
{"created":"2024-05-02 16:22:40","title":"A criterion for Lie algebroid connections on a compact Riemann surface","abstract":"Let $X$ be a compact connected Riemann surface and $(V, \\phi)$ a holomorphic Lie algebroid on $X$ such that the holomorphic vector bundle $V$ is stable. We give a necessary and sufficient condition on holomorphic vector bundles $E$ on $X$ to admit a Lie algebroid connection.","sentences":["Let $X$ be a compact connected Riemann surface and $(V, \\phi)$ a holomorphic Lie algebroid on $X$ such that the holomorphic vector bundle $V$ is stable.","We give a necessary and sufficient condition on holomorphic vector bundles $E$ on $X$ to admit a Lie algebroid connection."],"url":"http://arxiv.org/abs/2405.01432v1","category":"math.AG"}
{"created":"2024-05-02 15:59:45","title":"Quantitative homogenization of state-constraint Hamilton--Jacobi equations on perforated domains and applications","abstract":"We study the periodic homogenization problem of state-constraint Hamilton--Jacobi equations on perforated domains in the convex setting and obtain the optimal convergence rate. We then consider a dilute situation in which the holes' diameter is much smaller than the microscopic scale. Finally, a homogenization problem with domain defects where some holes are missing is analyzed.","sentences":["We study the periodic homogenization problem of state-constraint Hamilton--Jacobi equations on perforated domains in the convex setting and obtain the optimal convergence rate.","We then consider a dilute situation in which the holes' diameter is much smaller than the microscopic scale.","Finally, a homogenization problem with domain defects where some holes are missing is analyzed."],"url":"http://arxiv.org/abs/2405.01408v1","category":"math.AP"}
{"created":"2024-05-02 15:55:43","title":"Reduced Order Modeling for Real-Time Monitoring of Structural Displacements due to Electromagnetic Forces in Large Scale Tokamaks","abstract":"The real-time monitoring of the structural displacement of the Vacuum Vessel (VV) of thermonuclear fusion devices caused by electromagnetic (EM) loads is of great interest. In this paper, Model Order Reduction (MOR) is applied to the Integral Equation Methods (IEM) and the Finite Elements Method (FEM) to develop Electromagnetic and Structural Reduced Order Models (ROMs) compatible with real-time execution which allows for the real-time monitoring of strain and displacement in critical positions of Tokamaks machines. Low-rank compression techniques based on hierarchical matrices are applied to reduce the computational cost during the offline stage when the ROMs are constructed. Numerical results show the accuracy of the approach and demonstrate the compatibility with real-time execution in standard hardware.","sentences":["The real-time monitoring of the structural displacement of the Vacuum Vessel (VV) of thermonuclear fusion devices caused by electromagnetic (EM) loads is of great interest.","In this paper, Model Order Reduction (MOR) is applied to the Integral Equation Methods (IEM) and the Finite Elements Method (FEM) to develop Electromagnetic and Structural Reduced Order Models (ROMs) compatible with real-time execution which allows for the real-time monitoring of strain and displacement in critical positions of Tokamaks machines.","Low-rank compression techniques based on hierarchical matrices are applied to reduce the computational cost during the offline stage when the ROMs are constructed.","Numerical results show the accuracy of the approach and demonstrate the compatibility with real-time execution in standard hardware."],"url":"http://arxiv.org/abs/2405.01406v1","category":"math.NA"}
{"created":"2024-05-02 14:40:11","title":"Nutation: separating the spin from its magnetic moment","abstract":"For nearly 90 years, precession and relaxation processes have been thought to dominate magnetization dynamics. Only recently has it been considered that, on short time scales, an inertia-driven magnetization dynamics should become relevant, leading to additional nutation of the magnetization vector. Here, we trigger magnetic nutation via a sudden excitation of a thin Ni80Fe20 (Permalloy) film with an ultrashort optical pulse, that leads to an abrupt tilting of the effective field acting on the magnetic moments, separating the dynamics of the magnetization from that of its angular momentum. We investigate the resulting magnetization dynamics in the inertial regime experimentally by the time-resolved magneto optical Kerr effect. We find a characteristic oscillation in the Kerr signal in the range of about 0.1 THz superimposed on the precessional oscillations with GHz frequencies. By comparison with atomistic spin dynamics simulations, we demonstrate that this observation cannot be explained by the well-known Landau-Lifshitz-Gilbert equation of motion but can be attributed to inertial contributions leading to nutation of the magnetization vector around its angular momentum. Hence, an optical and non-resonant excitation of inertial magnetization dynamics can trigger and control different magnetic processes, ranging from demagnetization via nutation to precession in a single device. These findings will have profound implications for the understanding of ultrafast spin dynamics and magnetization switching.","sentences":["For nearly 90 years, precession and relaxation processes have been thought to dominate magnetization dynamics.","Only recently has it been considered that, on short time scales, an inertia-driven magnetization dynamics should become relevant, leading to additional nutation of the magnetization vector.","Here, we trigger magnetic nutation via a sudden excitation of a thin Ni80Fe20 (Permalloy) film with an ultrashort optical pulse, that leads to an abrupt tilting of the effective field acting on the magnetic moments, separating the dynamics of the magnetization from that of its angular momentum.","We investigate the resulting magnetization dynamics in the inertial regime experimentally by the time-resolved magneto optical Kerr effect.","We find a characteristic oscillation in the Kerr signal in the range of about 0.1 THz superimposed on the precessional oscillations with GHz frequencies.","By comparison with atomistic spin dynamics simulations, we demonstrate that this observation cannot be explained by the well-known Landau-Lifshitz-Gilbert equation of motion but can be attributed to inertial contributions leading to nutation of the magnetization vector around its angular momentum.","Hence, an optical and non-resonant excitation of inertial magnetization dynamics can trigger and control different magnetic processes, ranging from demagnetization via nutation to precession in a single device.","These findings will have profound implications for the understanding of ultrafast spin dynamics and magnetization switching."],"url":"http://arxiv.org/abs/2405.01334v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-02 14:38:18","title":"NeRF in Robotics: A Survey","abstract":"Meticulous 3D environment representations have been a longstanding goal in computer vision and robotics fields. The recent emergence of neural implicit representations has introduced radical innovation to this field as implicit representations enable numerous capabilities. Among these, the Neural Radiance Field (NeRF) has sparked a trend because of the huge representational advantages, such as simplified mathematical models, compact environment storage, and continuous scene representations. Apart from computer vision, NeRF has also shown tremendous potential in the field of robotics. Thus, we create this survey to provide a comprehensive understanding of NeRF in the field of robotics. By exploring the advantages and limitations of NeRF, as well as its current applications and future potential, we hope to shed light on this promising area of research. Our survey is divided into two main sections: \\textit{The Application of NeRF in Robotics} and \\textit{The Advance of NeRF in Robotics}, from the perspective of how NeRF enters the field of robotics. In the first section, we introduce and analyze some works that have been or could be used in the field of robotics from the perception and interaction perspectives. In the second section, we show some works related to improving NeRF's own properties, which are essential for deploying NeRF in the field of robotics. In the discussion section of the review, we summarize the existing challenges and provide some valuable future research directions for reference.","sentences":["Meticulous 3D environment representations have been a longstanding goal in computer vision and robotics fields.","The recent emergence of neural implicit representations has introduced radical innovation to this field as implicit representations enable numerous capabilities.","Among these, the Neural Radiance Field (NeRF) has sparked a trend because of the huge representational advantages, such as simplified mathematical models, compact environment storage, and continuous scene representations.","Apart from computer vision, NeRF has also shown tremendous potential in the field of robotics.","Thus, we create this survey to provide a comprehensive understanding of NeRF in the field of robotics.","By exploring the advantages and limitations of NeRF, as well as its current applications and future potential, we hope to shed light on this promising area of research.","Our survey is divided into two main sections: \\textit{The Application of NeRF in Robotics} and \\textit{The Advance of NeRF in Robotics}, from the perspective of how NeRF enters the field of robotics.","In the first section, we introduce and analyze some works that have been or could be used in the field of robotics from the perception and interaction perspectives.","In the second section, we show some works related to improving NeRF's own properties, which are essential for deploying NeRF in the field of robotics.","In the discussion section of the review, we summarize the existing challenges and provide some valuable future research directions for reference."],"url":"http://arxiv.org/abs/2405.01333v1","category":"cs.RO"}
{"created":"2024-05-02 14:35:47","title":"On Nanowire Morphological Instability and Pinch-Off by Surface Electromigration","abstract":"Surface diffusion and surface electromigration may lead to a morphological instability of thin solid films and nanowires. In this paper two nonlinear analyzes of a morphological instability are developed for a single-crystal cylindrical nanowire that is subjected to the axial current. These treatments extend the conventional linear stability analyzes without surface electromigration, that manifest a Rayleigh-Plateau instability. A weakly nonlinear analysis is done slightly above the Rayleigh-Plateau (longwave) instability threshold. It results in a one-dimensional Sivashinsky amplitude equation that describes a blow-up of a surface perturbation amplitude in a finite time. This is a signature of a formation of an axisymmetric spike singularity of a cylinder radius, which leads to a wire pinch-off and separation into a disjoint segments. The scaling analysis of the amplitude spike singularity is performed, and the time-and-electric field-dependent dimensions of the spike are characterized. A weakly nonlinear multi-scale analysis is done at the arbitrary distance above a longwave or a shortwave instability threshold. The time-and-electric field-dependent Fourier amplitudes of the major instability modes are derived and characterized.","sentences":["Surface diffusion and surface electromigration may lead to a morphological instability of thin solid films and nanowires.","In this paper two nonlinear analyzes of a morphological instability are developed for a single-crystal cylindrical nanowire that is subjected to the axial current.","These treatments extend the conventional linear stability analyzes without surface electromigration, that manifest a Rayleigh-Plateau instability.","A weakly nonlinear analysis is done slightly above the Rayleigh-Plateau (longwave) instability threshold.","It results in a one-dimensional Sivashinsky amplitude equation that describes a blow-up of a surface perturbation amplitude in a finite time.","This is a signature of a formation of an axisymmetric spike singularity of a cylinder radius, which leads to a wire pinch-off and separation into a disjoint segments.","The scaling analysis of the amplitude spike singularity is performed, and the time-and-electric field-dependent dimensions of the spike are characterized.","A weakly nonlinear multi-scale analysis is done at the arbitrary distance above a longwave or a shortwave instability threshold.","The time-and-electric field-dependent Fourier amplitudes of the major instability modes are derived and characterized."],"url":"http://arxiv.org/abs/2405.01331v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 13:26:18","title":"The Importance of Model Inspection for Better Understanding Performance Characteristics of Graph Neural Networks","abstract":"This study highlights the importance of conducting comprehensive model inspection as part of comparative performance analyses. Here, we investigate the effect of modelling choices on the feature learning characteristics of graph neural networks applied to a brain shape classification task. Specifically, we analyse the effect of using parameter-efficient, shared graph convolutional submodels compared to structure-specific, non-shared submodels. Further, we assess the effect of mesh registration as part of the data harmonisation pipeline. We find substantial differences in the feature embeddings at different layers of the models. Our results highlight that test accuracy alone is insufficient to identify important model characteristics such as encoded biases related to data source or potentially non-discriminative features learned in submodels. Our model inspection framework offers a valuable tool for practitioners to better understand performance characteristics of deep learning models in medical imaging.","sentences":["This study highlights the importance of conducting comprehensive model inspection as part of comparative performance analyses.","Here, we investigate the effect of modelling choices on the feature learning characteristics of graph neural networks applied to a brain shape classification task.","Specifically, we analyse the effect of using parameter-efficient, shared graph convolutional submodels compared to structure-specific, non-shared submodels.","Further, we assess the effect of mesh registration as part of the data harmonisation pipeline.","We find substantial differences in the feature embeddings at different layers of the models.","Our results highlight that test accuracy alone is insufficient to identify important model characteristics such as encoded biases related to data source or potentially non-discriminative features learned in submodels.","Our model inspection framework offers a valuable tool for practitioners to better understand performance characteristics of deep learning models in medical imaging."],"url":"http://arxiv.org/abs/2405.01270v1","category":"cs.LG"}
{"created":"2024-05-02 12:54:04","title":"Revisiting semi-supervised training objectives for differentiable particle filters","abstract":"Differentiable particle filters combine the flexibility of neural networks with the probabilistic nature of sequential Monte Carlo methods. However, traditional approaches rely on the availability of labelled data, i.e., the ground truth latent state information, which is often difficult to obtain in real-world applications. This paper compares the effectiveness of two semi-supervised training objectives for differentiable particle filters. We present results in two simulated environments where labelled data are scarce.","sentences":["Differentiable particle filters combine the flexibility of neural networks with the probabilistic nature of sequential Monte Carlo methods.","However, traditional approaches rely on the availability of labelled data, i.e., the ground truth latent state information, which is often difficult to obtain in real-world applications.","This paper compares the effectiveness of two semi-supervised training objectives for differentiable particle filters.","We present results in two simulated environments where labelled data are scarce."],"url":"http://arxiv.org/abs/2405.01251v1","category":"cs.LG"}
{"created":"2024-05-02 12:50:54","title":"The nonlinear Schr\u00f6dinger equation with sprinkled nonlinearity","abstract":"We prove global well-posedness for the cubic nonlinear Schr\\\"odinger equation with nonlinearity concentrated on a homogeneous Poisson process.","sentences":["We prove global well-posedness for the cubic nonlinear Schr\\\"odinger equation with nonlinearity concentrated on a homogeneous Poisson process."],"url":"http://arxiv.org/abs/2405.01246v1","category":"math.AP"}
{"created":"2024-05-02 12:25:41","title":"Mathematics of Differential Machine Learning in Derivative Pricing and Hedging","abstract":"This article introduces the groundbreaking concept of the financial differential machine learning algorithm through a rigorous mathematical framework. Diverging from existing literature on financial machine learning, the work highlights the profound implications of theoretical assumptions within financial models on the construction of machine learning algorithms.   This endeavour is particularly timely as the finance landscape witnesses a surge in interest towards data-driven models for the valuation and hedging of derivative products. Notably, the predictive capabilities of neural networks have garnered substantial attention in both academic research and practical financial applications.   The approach offers a unified theoretical foundation that facilitates comprehensive comparisons, both at a theoretical level and in experimental outcomes. Importantly, this theoretical grounding lends substantial weight to the experimental results, affirming the differential machine learning method's optimality within the prevailing context.   By anchoring the insights in rigorous mathematics, the article bridges the gap between abstract financial concepts and practical algorithmic implementations.","sentences":["This article introduces the groundbreaking concept of the financial differential machine learning algorithm through a rigorous mathematical framework.","Diverging from existing literature on financial machine learning, the work highlights the profound implications of theoretical assumptions within financial models on the construction of machine learning algorithms.   ","This endeavour is particularly timely as the finance landscape witnesses a surge in interest towards data-driven models for the valuation and hedging of derivative products.","Notably, the predictive capabilities of neural networks have garnered substantial attention in both academic research and practical financial applications.   ","The approach offers a unified theoretical foundation that facilitates comprehensive comparisons, both at a theoretical level and in experimental outcomes.","Importantly, this theoretical grounding lends substantial weight to the experimental results, affirming the differential machine learning method's optimality within the prevailing context.   ","By anchoring the insights in rigorous mathematics, the article bridges the gap between abstract financial concepts and practical algorithmic implementations."],"url":"http://arxiv.org/abs/2405.01233v1","category":"q-fin.MF"}
{"created":"2024-05-02 12:10:52","title":"Avoiding Redundant Restarts in Multimodal Global Optimization","abstract":"Na\\\"ive restarts of global optimization solvers when operating on multimodal search landscapes may resemble the Coupon's Collector Problem, with a potential to waste significant function evaluations budget on revisiting the same basins of attractions. In this paper, we assess the degree to which such ``duplicate restarts'' occur on standard multimodal benchmark functions, which defines the \\textit{redundancy potential} of each particular landscape. We then propose a repelling mechanism to avoid such wasted restarts with the CMA-ES and investigate its efficacy on test cases with high redundancy potential compared to the standard restart mechanism.","sentences":["Na\\\"ive restarts of global optimization solvers when operating on multimodal search landscapes may resemble the Coupon's Collector Problem, with a potential to waste significant function evaluations budget on revisiting the same basins of attractions.","In this paper, we assess the degree to which such ``duplicate restarts'' occur on standard multimodal benchmark functions, which defines the \\textit{redundancy potential} of each particular landscape.","We then propose a repelling mechanism to avoid such wasted restarts with the CMA-ES and investigate its efficacy on test cases with high redundancy potential compared to the standard restart mechanism."],"url":"http://arxiv.org/abs/2405.01226v1","category":"cs.NE"}
{"created":"2024-05-02 11:36:17","title":"Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks","abstract":"Deep Neural Networks (DNN) have shown great promise in many classification applications, yet are widely known to have poorly calibrated predictions when they are over-parametrized. Improving DNN calibration without comprising on model accuracy is of extreme importance and interest in safety critical applications such as in the health-care sector. In this work, we show that decoupling the training of feature extraction layers and classification layers in over-parametrized DNN architectures such as Wide Residual Networks (WRN) and Visual Transformers (ViT) significantly improves model calibration whilst retaining accuracy, and at a low training cost. In addition, we show that placing a Gaussian prior on the last hidden layer outputs of a DNN, and training the model variationally in the classification training stage, even further improves calibration. We illustrate these methods improve calibration across ViT and WRN architectures for several image classification benchmark datasets.","sentences":["Deep Neural Networks (DNN) have shown great promise in many classification applications, yet are widely known to have poorly calibrated predictions when they are over-parametrized.","Improving DNN calibration without comprising on model accuracy is of extreme importance and interest in safety critical applications such as in the health-care sector.","In this work, we show that decoupling the training of feature extraction layers and classification layers in over-parametrized DNN architectures such as Wide Residual Networks (WRN) and Visual Transformers (ViT) significantly improves model calibration whilst retaining accuracy, and at a low training cost.","In addition, we show that placing a Gaussian prior on the last hidden layer outputs of a DNN, and training the model variationally in the classification training stage, even further improves calibration.","We illustrate these methods improve calibration across ViT and WRN architectures for several image classification benchmark datasets."],"url":"http://arxiv.org/abs/2405.01196v1","category":"cs.LG"}
{"created":"2024-05-02 11:35:34","title":"On the semi-additivity of the $1/2$-symmetric caloric capacity in the plane","abstract":"In this paper we study properties of a variant of the $1/2$-caloric capacity, called $1/2$-symmetric caloric capacity. The latter is associated simultaneously with the $1/2$-fractional heat equation and its conjugate. We establish its semi-additivity in $\\mathbb{R}^2$ and, moreover, we compute explicitly the $1/2$-symmetric caloric capacity of rectangles, which illustrates its anisotropic behaviour.","sentences":["In this paper we study properties of a variant of the $1/2$-caloric capacity, called $1/2$-symmetric caloric capacity.","The latter is associated simultaneously with the $1/2$-fractional heat equation and its conjugate.","We establish its semi-additivity in $\\mathbb{R}^2$ and, moreover, we compute explicitly the $1/2$-symmetric caloric capacity of rectangles, which illustrates its anisotropic behaviour."],"url":"http://arxiv.org/abs/2405.01195v1","category":"math.AP"}
{"created":"2024-05-02 10:34:55","title":"Political Stress Index of Poland","abstract":"We apply the political stress index as introduced by Goldstone (1991) and implemented by Turchin (2013), to the case study of Poland. The approach quantifies political and social unrest as a single quantity based on a multitude of economic and demographic variables. The present-day data allow us to directly apply index without the need of simulating the elite component, as was done previously. Neither model version shows appreciable unrest levels for the present, while the simulated model applied to partial historical data yields the index in remarkable agreement with the fall of communism in Poland. We next analyze the model's sensitive dependence on its parameters (the hallmark of chaos), which limits its utility and application to other countries. The original equations cannot, by construction, describe the elite fraction for longer time-periods; and we propose a modification to remedy this problem. The model still holds some predictive power, but we argue that some components should be reinterpreted if one wants to keep its dynamical equations.","sentences":["We apply the political stress index as introduced by Goldstone (1991) and implemented by Turchin (2013), to the case study of Poland.","The approach quantifies political and social unrest as a single quantity based on a multitude of economic and demographic variables.","The present-day data allow us to directly apply index without the need of simulating the elite component, as was done previously.","Neither model version shows appreciable unrest levels for the present, while the simulated model applied to partial historical data yields the index in remarkable agreement with the fall of communism in Poland.","We next analyze the model's sensitive dependence on its parameters (the hallmark of chaos), which limits its utility and application to other countries.","The original equations cannot, by construction, describe the elite fraction for longer time-periods; and we propose a modification to remedy this problem.","The model still holds some predictive power, but we argue that some components should be reinterpreted if one wants to keep its dynamical equations."],"url":"http://arxiv.org/abs/2405.01163v1","category":"physics.soc-ph"}
{"created":"2024-05-02 10:33:04","title":"Numerical implementation of evolution equations for twist-3 collinear PDFs","abstract":"Twist-3 collinear parton distribution functions (PDFs) are matrix elements of quark-gluon-quark or three-gluons light-cone operators. They depend on three momentum fraction variables, which are restricted to a hexagon region, and the evolution kernels are defined via two-dimensional convolution in these variables. We present the numerical realisation of the twist-3 evolution equations at leading order in the strong coupling for all kinds of twist-3 PDF (quark, gluon, chiral-even/odd, etc). We provide two independent codes (in C and Fortran) that have been extensively cross-checked, and are ready-to-use. We supplement the paper with a review of known properties of twist-3 PDFs.","sentences":["Twist-3 collinear parton distribution functions (PDFs) are matrix elements of quark-gluon-quark or three-gluons light-cone operators.","They depend on three momentum fraction variables, which are restricted to a hexagon region, and the evolution kernels are defined via two-dimensional convolution in these variables.","We present the numerical realisation of the twist-3 evolution equations at leading order in the strong coupling for all kinds of twist-3 PDF (quark, gluon, chiral-even/odd, etc).","We provide two independent codes (in C and Fortran) that have been extensively cross-checked, and are ready-to-use.","We supplement the paper with a review of known properties of twist-3 PDFs."],"url":"http://arxiv.org/abs/2405.01162v1","category":"hep-ph"}
{"created":"2024-05-02 08:40:06","title":"Probing the structure of $X(3872)$ in photoproduction","abstract":"We study the production of $X(3872)$ mesons in photon-induced nuclear reactions near the threshold within the collision model based on the nuclear spectral function. The model accounts for direct photon-nucleon $X(3872)$ production processes as well as five different scenarios for their internal structure. We calculate the absolute and relative excitation functions for $X(3872)$ production off $^{12}$C and $^{184}$W target nuclei at near-threshold incident photon energies of 8--16 GeV, the absolute differential cross sections for their production off these target nuclei at laboratory angles of 0$^{\\circ}$--10$^{\\circ}$ and for incident photon energy of 13 GeV as well as the A dependences of the relative (transparency ratios) cross sections for $X(3872)$ production from ${\\gamma}A$ collisions at photon energies around 13 GeV within the adopted scenarios for the $X(3872)$ meson internal structure. We show that the absolute and relative observables considered reveal distinct sensitivity to these scenarios. Therefore, the measurement of such observables in a dedicated experiment at the CEBAF facility in the near-threshold energy range will allow us to get valuable information on the $X(3872)$ inner structure.","sentences":["We study the production of $X(3872)$ mesons in photon-induced nuclear reactions near the threshold within the collision model based on the nuclear spectral function.","The model accounts for direct photon-nucleon $X(3872)$ production processes as well as five different scenarios for their internal structure.","We calculate the absolute and relative excitation functions for $X(3872)$ production off $^{12}$C and $^{184}$W target nuclei at near-threshold incident photon energies of 8--16 GeV, the absolute differential cross sections for their production off these target nuclei at laboratory angles of 0$^{\\circ}$--10$^{\\circ}$ and for incident photon energy of 13 GeV as well as the A dependences of the relative (transparency ratios) cross sections for $X(3872)$ production from ${\\gamma}A$ collisions at photon energies around 13 GeV within the adopted scenarios for the $X(3872)$ meson internal structure.","We show that the absolute and relative observables considered reveal distinct sensitivity to these scenarios.","Therefore, the measurement of such observables in a dedicated experiment at the CEBAF facility in the near-threshold energy range will allow us to get valuable information on the $X(3872)$ inner structure."],"url":"http://arxiv.org/abs/2405.01089v1","category":"hep-ph"}
{"created":"2024-05-02 08:13:42","title":"Mortar Thin Shell Approximation for Analysis of Superconducting Accelerator Magnets","abstract":"Thin layers can lead to unfavorable meshes in a finite element (FE) analysis. Thin shell approximations (TSAs) avoid this issue by removing the need for a mesh of the thin layer while approximating the physics across the layer by an interface condition. Typically, a TSA requires the mesh of both sides of the TSA interface to be conforming. To alleviate this requirement, we propose to combine mortar methods and TSAs for solving the heat equation. The mortar TSA method's formulation is derived and enables an independent discretization of the subdomains on the two sides of the TSA depending on their accuracy requirements. The method is verified by comparison with a reference FE solution of a thermal model problem of a simplified superconducting accelerator magnet.","sentences":["Thin layers can lead to unfavorable meshes in a finite element (FE) analysis.","Thin shell approximations (TSAs) avoid this issue by removing the need for a mesh of the thin layer while approximating the physics across the layer by an interface condition.","Typically, a TSA requires the mesh of both sides of the TSA interface to be conforming.","To alleviate this requirement, we propose to combine mortar methods and TSAs for solving the heat equation.","The mortar TSA method's formulation is derived and enables an independent discretization of the subdomains on the two sides of the TSA depending on their accuracy requirements.","The method is verified by comparison with a reference FE solution of a thermal model problem of a simplified superconducting accelerator magnet."],"url":"http://arxiv.org/abs/2405.01076v1","category":"cs.CE"}
{"created":"2024-05-02 07:39:50","title":"Ancient mean curvature flows with finite total curvature","abstract":"We construct an $I$-family of ancient graphical mean curvature flows over a minimal hypersurface in $\\mathbb{R}^{n+1}$ of finite total curvature with the Morse index $I$ by establishing exponentially fast convergence in terms of $|x|^2-t$. As a corollary, we show that these ancient flows have finite total curvature and finite mass drop. Moreover, one family of these flows is mean convex by a pointwise estimate.","sentences":["We construct an $I$-family of ancient graphical mean curvature flows over a minimal hypersurface in $\\mathbb{R}^{n+1}$ of finite total curvature with the Morse index $I$ by establishing exponentially fast convergence in terms of $|x|^2-t$. As a corollary, we show that these ancient flows have finite total curvature and finite mass drop.","Moreover, one family of these flows is mean convex by a pointwise estimate."],"url":"http://arxiv.org/abs/2405.01062v1","category":"math.DG"}
{"created":"2024-05-02 06:57:53","title":"Well-posedness of stochastic mSQG equations with Kraichnan noise and $L^p$ data","abstract":"We consider stochastic mSQG (modified Surface Quasi-Geostrophic) equations with multiplicative transport noise of Kraichnan type, and $L^p$-initial conditions. Inspired by the recent work of Coghi and Maurelli [arXiv:2308.03216], we show weak existence and pathwise uniqueness of solutions to the equations for suitable choices of parameters in the nonlinearity, the noise and the integrability of initial data.","sentences":["We consider stochastic mSQG (modified Surface Quasi-Geostrophic) equations with multiplicative transport noise of Kraichnan type, and $L^p$-initial conditions.","Inspired by the recent work of Coghi and Maurelli [arXiv:2308.03216], we show weak existence and pathwise uniqueness of solutions to the equations for suitable choices of parameters in the nonlinearity, the noise and the integrability of initial data."],"url":"http://arxiv.org/abs/2405.01045v1","category":"math.PR"}
{"created":"2024-05-02 06:51:26","title":"Evolution of multiple closed knotted curves in space","abstract":"We investigate a system of geometric evolution equations describing a curvature and torsion driven motion of a family of 3D curves in the normal and binormal directions. We explore the direct Lagrangian approach for treating the geometric flow of such interacting curves. Using the abstract theory of nonlinear analytic semi-flows, we are able to prove local existence, uniqueness, and continuation of classical H\\\"older smooth solutions to the governing system of non-linear parabolic equations modelling $n$ evolving curves with mutual nonlocal interactions. We present several computational studies of the flow that combine the normal or binormal velocity and considering nonlocal interaction.","sentences":["We investigate a system of geometric evolution equations describing a curvature and torsion driven motion of a family of 3D curves in the normal and binormal directions.","We explore the direct Lagrangian approach for treating the geometric flow of such interacting curves.","Using the abstract theory of nonlinear analytic semi-flows, we are able to prove local existence, uniqueness, and continuation of classical H\\\"older smooth solutions to the governing system of non-linear parabolic equations modelling $n$ evolving curves with mutual nonlocal interactions.","We present several computational studies of the flow that combine the normal or binormal velocity and considering nonlocal interaction."],"url":"http://arxiv.org/abs/2405.01038v1","category":"math.AP"}
{"created":"2024-05-02 06:46:05","title":"Euclid -- The Dark Universe detective","abstract":"Euclid is a recently launched medium-class mission by the European Space Agency (ESA) designed to measure cosmological parameters, test the cosmological standard model, and explore the nature of dark matter and dark energy. To this end, Euclid conducts a survey of up to 14000 square degrees of the extra-galactic sky and obtains optical and near-infrared photometric measurements for more than a billion galaxies as well as near-infrared slitless spectroscopy for more than 35 million galaxies. These observations will be used to estimate galaxy clustering and cosmic shear. It is expected that Euclid will achieve percent-level constraints on the Dark Energy equation of state parameter. The survey will also be exploited with a range of other cosmological probes and prove revolutionary for non-cosmological science.","sentences":["Euclid is a recently launched medium-class mission by the European Space Agency (ESA) designed to measure cosmological parameters, test the cosmological standard model, and explore the nature of dark matter and dark energy.","To this end, Euclid conducts a survey of up to 14000 square degrees of the extra-galactic sky and obtains optical and near-infrared photometric measurements for more than a billion galaxies as well as near-infrared slitless spectroscopy for more than 35 million galaxies.","These observations will be used to estimate galaxy clustering and cosmic shear.","It is expected that Euclid will achieve percent-level constraints on the Dark Energy equation of state parameter.","The survey will also be exploited with a range of other cosmological probes and prove revolutionary for non-cosmological science."],"url":"http://arxiv.org/abs/2405.01037v1","category":"astro-ph.CO"}
{"created":"2024-05-02 06:30:52","title":"CrossMPT: Cross-attention Message-Passing Transformer for Error Correcting Codes","abstract":"Error correcting codes~(ECCs) are indispensable for reliable transmission in communication systems. The recent advancements in deep learning have catalyzed the exploration of ECC decoders based on neural networks. Among these, transformer-based neural decoders have achieved state-of-the-art decoding performance. In this paper, we propose a novel Cross-attention Message-Passing Transformer~(CrossMPT). CrossMPT iteratively updates two types of input vectors (i.e., magnitude and syndrome vectors) using two masked cross-attention blocks. The mask matrices in these cross-attention blocks are determined by the code's parity-check matrix that delineates the relationship between magnitude and syndrome vectors. Our experimental results show that CrossMPT significantly outperforms existing neural network-based decoders, particularly in decoding low-density parity-check codes. Notably, CrossMPT also achieves a significant reduction in computational complexity, achieving over a 50\\% decrease in its attention layers compared to the original transformer-based decoder, while retaining the computational complexity of the remaining layers.","sentences":["Error correcting codes~(ECCs) are indispensable for reliable transmission in communication systems.","The recent advancements in deep learning have catalyzed the exploration of ECC decoders based on neural networks.","Among these, transformer-based neural decoders have achieved state-of-the-art decoding performance.","In this paper, we propose a novel Cross-attention Message-Passing Transformer~(CrossMPT).","CrossMPT iteratively updates two types of input vectors (i.e., magnitude and syndrome vectors) using two masked cross-attention blocks.","The mask matrices in these cross-attention blocks are determined by the code's parity-check matrix that delineates the relationship between magnitude and syndrome vectors.","Our experimental results show that CrossMPT significantly outperforms existing neural network-based decoders, particularly in decoding low-density parity-check codes.","Notably, CrossMPT also achieves a significant reduction in computational complexity, achieving over a 50\\% decrease in its attention layers compared to the original transformer-based decoder, while retaining the computational complexity of the remaining layers."],"url":"http://arxiv.org/abs/2405.01033v1","category":"cs.LG"}
{"created":"2024-05-02 05:47:44","title":"Summing Gravitational Effects from Loops of Inflationary Scalars","abstract":"We develop a procedure for re-summing the large logarithms induced in gravity by loops of inflationary scalars. We first show how the scalar can be integrated out of the field equations in the presence of constant graviton field. We then extend this result to a fully conserved form which explains the need for a finite renormalization of the cosmological constant which was previously inferred from explicit computation. A variant of the renormalization group turns out to explain the large logarithmic corrections revealed by explicit computation in the electric field strength of gravitational radiation and in the potentials which characterize the response to a point mass. The implications for graviton loops are discussed.","sentences":["We develop a procedure for re-summing the large logarithms induced in gravity by loops of inflationary scalars.","We first show how the scalar can be integrated out of the field equations in the presence of constant graviton field.","We then extend this result to a fully conserved form which explains the need for a finite renormalization of the cosmological constant which was previously inferred from explicit computation.","A variant of the renormalization group turns out to explain the large logarithmic corrections revealed by explicit computation in the electric field strength of gravitational radiation and in the potentials which characterize the response to a point mass.","The implications for graviton loops are discussed."],"url":"http://arxiv.org/abs/2405.01024v1","category":"gr-qc"}
{"created":"2024-05-02 05:32:19","title":"Proven Runtime Guarantees for How the \\moead Computes the Pareto Front From the Subproblem Solutions","abstract":"The decomposition-based multi-objective evolutionary algorithm (MOEA/D) does not directly optimize a given multi-objective function $f$, but instead optimizes $N + 1$ single-objective subproblems of $f$ in a co-evolutionary manner. It maintains an archive of all non-dominated solutions found and outputs it as approximation to the Pareto front. Once the MOEA/D found all optima of the subproblems (the $g$-optima), it may still miss Pareto optima of $f$. The algorithm is then tasked to find the remaining Pareto optima directly by mutating the $g$-optima.   In this work, we analyze for the first time how the MOEA/D with only standard mutation operators computes the whole Pareto front of the OneMinMax benchmark when the $g$-optima are a strict subset of the Pareto front. For standard bit mutation, we prove an expected runtime of $O(n N \\log n + n^{n/(2N)} N \\log n)$ function evaluations. Especially for the second, more interesting phase when the algorithm start with all $g$-optima, we prove an $\\Omega(n^{(1/2)(n/N + 1)} \\sqrt{N} 2^{-n/N})$ expected runtime. This runtime is super-polynomial if $N = o(n)$, since this leaves large gaps between the $g$-optima, which require costly mutations to cover.   For power-law mutation with exponent $\\beta \\in (1, 2)$, we prove an expected runtime of $O\\left(n N \\log n + n^{\\beta} \\log n\\right)$ function evaluations. The $O\\left(n^{\\beta} \\log n\\right)$ term stems from the second phase of starting with all $g$-optima, and it is independent of the number of subproblems $N$. This leads to a huge speedup compared to the lower bound for standard bit mutation. In general, our overall bound for power-law suggests that the MOEA/D performs best for $N = O(n^{\\beta - 1})$, resulting in an $O(n^\\beta \\log n)$ bound. In contrast to standard bit mutation, smaller values of $N$ are better for power-law mutation, as it is capable of easily creating missing solutions.","sentences":["The decomposition-based multi-objective evolutionary algorithm (MOEA/D) does not directly optimize a given multi-objective function $f$, but instead optimizes $N + 1$ single-objective subproblems of $f$ in a co-evolutionary manner.","It maintains an archive of all non-dominated solutions found and outputs it as approximation to the Pareto front.","Once the MOEA/D found all optima of the subproblems (the $g$-optima), it may still miss Pareto optima of $f$. The algorithm is then tasked to find the remaining Pareto optima directly by mutating the $g$-optima.   ","In this work, we analyze for the first time how the MOEA/D with only standard mutation operators computes the whole Pareto front of the OneMinMax benchmark when the $g$-optima are a strict subset of the Pareto front.","For standard bit mutation, we prove an expected runtime of $O(n N \\log n + n^{n/(2N)} N \\log n)$ function evaluations.","Especially for the second, more interesting phase when the algorithm start with all $g$-optima, we prove an $\\Omega(n^{(1/2)(n/N + 1)} \\sqrt{N} 2^{-n/N})$ expected runtime.","This runtime is super-polynomial if $N =","o(n)$, since this leaves large gaps between the $g$-optima, which require costly mutations to cover.   ","For power-law mutation with exponent $\\beta \\in (1, 2)$, we prove an expected runtime of $O\\left(n N \\log n + n^{\\beta} \\log n\\right)$ function evaluations.","The $O\\left(n^{\\beta} \\log n\\right)$ term stems from the second phase of starting with all $g$-optima, and it is independent of the number of subproblems $N$. This leads to a huge speedup compared to the lower bound for standard bit mutation.","In general, our overall bound for power-law suggests that the MOEA/D performs best for $N = O(n^{\\beta - 1})$, resulting in an $O(n^\\beta \\log n)$ bound.","In contrast to standard bit mutation, smaller values of $N$ are better for power-law mutation, as it is capable of easily creating missing solutions."],"url":"http://arxiv.org/abs/2405.01014v1","category":"cs.NE"}
{"created":"2024-05-02 05:27:12","title":"Correcting Biased Centered Kernel Alignment Measures in Biological and Artificial Neural Networks","abstract":"Centred Kernel Alignment (CKA) has recently emerged as a popular metric to compare activations from biological and artificial neural networks (ANNs) in order to quantify the alignment between internal representations derived from stimuli sets (e.g. images, text, video) that are presented to both systems. In this paper we highlight issues that the community should take into account if using CKA as an alignment metric with neural data. Neural data are in the low-data high-dimensionality domain, which is one of the cases where (biased) CKA results in high similarity scores even for pairs of random matrices. Using fMRI and MEG data from the THINGS project, we show that if biased CKA is applied to representations of different sizes in the low-data high-dimensionality domain, they are not directly comparable due to biased CKA's sensitivity to differing feature-sample ratios and not stimuli-driven responses. This situation can arise both when comparing a pre-selected area of interest (e.g. ROI) to multiple ANN layers, as well as when determining to which ANN layer multiple regions of interest (ROIs) / sensor groups of different dimensionality are most similar. We show that biased CKA can be artificially driven to its maximum value when using independent random data of different sample-feature ratios. We further show that shuffling sample-feature pairs of real neural data does not drastically alter biased CKA similarity in comparison to unshuffled data, indicating an undesirable lack of sensitivity to stimuli-driven neural responses. Positive alignment of true stimuli-driven responses is only achieved by using debiased CKA. Lastly, we report findings that suggest biased CKA is sensitive to the inherent structure of neural data, only differing from shuffled data when debiased CKA detects stimuli-driven alignment.","sentences":["Centred Kernel Alignment (CKA) has recently emerged as a popular metric to compare activations from biological and artificial neural networks (ANNs) in order to quantify the alignment between internal representations derived from stimuli sets (e.g. images, text, video) that are presented to both systems.","In this paper we highlight issues that the community should take into account if using CKA as an alignment metric with neural data.","Neural data are in the low-data high-dimensionality domain, which is one of the cases where (biased) CKA results in high similarity scores even for pairs of random matrices.","Using fMRI and MEG data from the THINGS project, we show that if biased CKA is applied to representations of different sizes in the low-data high-dimensionality domain, they are not directly comparable due to biased CKA's sensitivity to differing feature-sample ratios and not stimuli-driven responses.","This situation can arise both when comparing a pre-selected area of interest (e.g. ROI) to multiple ANN layers, as well as when determining to which ANN layer multiple regions of interest (ROIs) / sensor groups of different dimensionality are most similar.","We show that biased CKA can be artificially driven to its maximum value when using independent random data of different sample-feature ratios.","We further show that shuffling sample-feature pairs of real neural data does not drastically alter biased CKA similarity in comparison to unshuffled data, indicating an undesirable lack of sensitivity to stimuli-driven neural responses.","Positive alignment of true stimuli-driven responses is only achieved by using debiased CKA.","Lastly, we report findings that suggest biased CKA is sensitive to the inherent structure of neural data, only differing from shuffled data when debiased CKA detects stimuli-driven alignment."],"url":"http://arxiv.org/abs/2405.01012v1","category":"q-bio.NC"}
{"created":"2024-05-02 04:31:17","title":"Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel Fields","abstract":"This paper presents a novel latent 3D diffusion model for the generation of neural voxel fields, aiming to achieve accurate part-aware structures. Compared to existing methods, there are two key designs to ensure high-quality and accurate part-aware generation. On one hand, we introduce a latent 3D diffusion process for neural voxel fields, enabling generation at significantly higher resolutions that can accurately capture rich textural and geometric details. On the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding the accurate part decomposition and producing high-quality rendering results. Through extensive experimentation and comparisons with state-of-the-art methods, we evaluate our approach across four different classes of data. The results demonstrate the superior generative capabilities of our proposed method in part-aware shape generation, outperforming existing state-of-the-art methods.","sentences":["This paper presents a novel latent 3D diffusion model for the generation of neural voxel fields, aiming to achieve accurate part-aware structures.","Compared to existing methods, there are two key designs to ensure high-quality and accurate part-aware generation.","On one hand, we introduce a latent 3D diffusion process for neural voxel fields, enabling generation at significantly higher resolutions that can accurately capture rich textural and geometric details.","On the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding the accurate part decomposition and producing high-quality rendering results.","Through extensive experimentation and comparisons with state-of-the-art methods, we evaluate our approach across four different classes of data.","The results demonstrate the superior generative capabilities of our proposed method in part-aware shape generation, outperforming existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.00998v1","category":"cs.CV"}
{"created":"2024-05-02 03:49:46","title":"Multi-intent-aware Session-based Recommendation","abstract":"Session-based recommendation (SBR) aims to predict the following item a user will interact with during an ongoing session. Most existing SBR models focus on designing sophisticated neural-based encoders to learn a session representation, capturing the relationship among session items. However, they tend to focus on the last item, neglecting diverse user intents that may exist within a session. This limitation leads to significant performance drops, especially for longer sessions. To address this issue, we propose a novel SBR model, called Multi-intent-aware Session-based Recommendation Model (MiaSRec). It adopts frequency embedding vectors indicating the item frequency in session to enhance the information about repeated items. MiaSRec represents various user intents by deriving multiple session representations centered on each item and dynamically selecting the important ones. Extensive experimental results show that MiaSRec outperforms existing state-of-the-art SBR models on six datasets, particularly those with longer average session length, achieving up to 6.27% and 24.56% gains for MRR@20 and Recall@20. Our code is available at https://github.com/jin530/MiaSRec.","sentences":["Session-based recommendation (SBR) aims to predict the following item a user will interact with during an ongoing session.","Most existing SBR models focus on designing sophisticated neural-based encoders to learn a session representation, capturing the relationship among session items.","However, they tend to focus on the last item, neglecting diverse user intents that may exist within a session.","This limitation leads to significant performance drops, especially for longer sessions.","To address this issue, we propose a novel SBR model, called Multi-intent-aware Session-based Recommendation Model (MiaSRec).","It adopts frequency embedding vectors indicating the item frequency in session to enhance the information about repeated items.","MiaSRec represents various user intents by deriving multiple session representations centered on each item and dynamically selecting the important ones.","Extensive experimental results show that MiaSRec outperforms existing state-of-the-art SBR models on six datasets, particularly those with longer average session length, achieving up to 6.27% and 24.56% gains for MRR@20 and Recall@20.","Our code is available at https://github.com/jin530/MiaSRec."],"url":"http://arxiv.org/abs/2405.00986v1","category":"cs.IR"}
{"created":"2024-05-02 03:16:52","title":"Elastic electron scattering from Be, Mg, and Ca","abstract":"We present a comprehensive set of theoretical results for differential, integrated, and momentum transfer cross sections for the elastic scattering of electrons by beryllium, magnesium and calcium, at energies below 1 keV. In addition, we provide Sherman function values for elastic electron scattering from calcium in the same energy range. This study extends the application of our method of calculations, already employed for barium and strontium, to all stable alkaline-earth-metal atoms. Our semi-empirical approach to treating target polarization has produced in our earlier work a satisfactory agreement with experimental values and precise theoretical results such as convergent close-coupling calculations for barium. The present data are expected to be of similar high accuracy, based on our previous success in similar calculations for barium and all inert gases.","sentences":["We present a comprehensive set of theoretical results for differential, integrated, and momentum transfer cross sections for the elastic scattering of electrons by beryllium, magnesium and calcium, at energies below 1 keV.","In addition, we provide Sherman function values for elastic electron scattering from calcium in the same energy range.","This study extends the application of our method of calculations, already employed for barium and strontium, to all stable alkaline-earth-metal atoms.","Our semi-empirical approach to treating target polarization has produced in our earlier work a satisfactory agreement with experimental values and precise theoretical results such as convergent close-coupling calculations for barium.","The present data are expected to be of similar high accuracy, based on our previous success in similar calculations for barium and all inert gases."],"url":"http://arxiv.org/abs/2405.00969v1","category":"physics.atom-ph"}
{"created":"2024-05-02 01:22:24","title":"Efficient Computation for Invertibility Sequence of Banded Toeplitz Matrices","abstract":"When solving systems of banded Toeplitz equations or calculating their inverses, it is necessary to determine the invertibility of the matrices beforehand. In this paper, we equate the invertibility of an $n$-order banded Toeplitz matrix with bandwidth $2k+1$ to that of a small $k*k$ matrix. By utilizing a specially designed algorithm, we compute the invertibility sequence of a class of banded Toeplitz matrices with a time complexity of $5k^2n/2+kn$ and a space complexity of $3k^2$ where $n$ is the size of the largest matrix. This enables efficient preprocessing when solving equation systems and inverses of banded Toeplitz matrices.","sentences":["When solving systems of banded Toeplitz equations or calculating their inverses, it is necessary to determine the invertibility of the matrices beforehand.","In this paper, we equate the invertibility of an $n$-order banded Toeplitz matrix with bandwidth $2k+1$ to that of a small $k*k$ matrix.","By utilizing a specially designed algorithm, we compute the invertibility sequence of a class of banded Toeplitz matrices with a time complexity of $5k^2n/2+kn$ and a space complexity of $3k^2$ where $n$ is the size of the largest matrix.","This enables efficient preprocessing when solving equation systems and inverses of banded Toeplitz matrices."],"url":"http://arxiv.org/abs/2405.00933v1","category":"math.NA"}
{"created":"2024-05-02 01:11:15","title":"MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion","abstract":"One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.","sentences":["One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample.","Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement.","In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network.","The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator.","The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks.","The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario."],"url":"http://arxiv.org/abs/2405.00930v1","category":"cs.SD"}
{"created":"2024-05-02 00:47:02","title":"Covariant Schr\u00f6dinger Operator and $L^2$-Vanishing Property on Riemannian Manifolds","abstract":"Let $M$ be a complete Riemannian manifold satisfying a weighted Poincar\\'e inequality, and let $\\mathcal{E}$ be a Hermitian vector bundle over $M$ equipped with a metric covariant derivative $\\nabla$. We consider the operator $H_{X,V}=\\nabla^{\\dagger}\\nabla+\\nabla_{X}+ V$, where $\\nabla^{\\dagger}$ is the formal adjoint of $\\nabla$ with respect to the inner product in the space of square-integrable sections of $\\mathcal{E}$, $X$ is a smooth (real) vector field on $M$, and $V$ is a fiberwise self-adjoint, smooth section of the endomorphism bundle $\\textrm{End }\\mathcal{E}$. We give a sufficient condition for the triviality of the $L^2$-kernel of $H_{X,V}$. As a corollary, putting $X\\equiv 0$ and working in the setting of a Clifford bundle equipped with a Clifford connection $\\nabla$, we obtain the triviality of the $L^2$-kernel of $D^2$, where $D$ is the Dirac operator corresponding to $\\nabla$. In particular, when $\\mathcal{E}=\\Lambda^{k}T^*M$ and $D^2$ is the Hodge--deRham Laplacian on $k$-forms, we recover some recent vanishing results for $L^2$-harmonic $k$-forms.","sentences":["Let $M$ be a complete Riemannian manifold satisfying a weighted Poincar\\'e inequality, and let $\\mathcal{E}$ be a Hermitian vector bundle over $M$ equipped with a metric covariant derivative","$\\nabla$.","We consider the operator $H_{X,V}=\\nabla^{\\dagger}\\nabla+\\nabla_{X}+ V$, where $\\nabla^{\\dagger}$ is the formal adjoint of $\\nabla$ with respect to the inner product in the space of square-integrable sections of $\\mathcal{E}$, $X$ is a smooth (real) vector field on $M$, and $V$ is a fiberwise self-adjoint, smooth section of the endomorphism bundle $\\textrm{End }\\mathcal{E}$. We give a sufficient condition for the triviality of the $L^2$-kernel of $H_{X,V}$.","As a corollary, putting $X\\equiv 0$ and working in the setting of a Clifford bundle equipped with a Clifford connection $\\nabla$, we obtain the triviality of the $L^2$-kernel of $D^2$, where $D$ is the Dirac operator corresponding to $\\nabla$. In particular, when $\\mathcal{E}=\\Lambda^{k}T^*M$ and $D^2$ is the Hodge--deRham Laplacian on $k$-forms, we recover some recent vanishing results for $L^2$-harmonic $k$-forms."],"url":"http://arxiv.org/abs/2405.00926v1","category":"math.DG"}
{"created":"2024-05-01 23:07:50","title":"Using Schema to Inform Method Design Practices","abstract":"There are many different forms of design knowledge that guide and shape a designer's ability to act and realize potential realities. Methods and schemas are examples of design knowledge commonly used by design researchers and designers alike. In this pictorial, we explore, engage, and describe the role of schemas as tools that can support design researchers in formulating methods to support design action, with our framing of method design specifically focused on ethical design complexity. We present four ways for method designers to engage with schema: 1) Systems to operationalize complex design constructs such as ethical design complexity through an A.E.I.O.YOU schema; 2) Classifiers to map existing methods and identify the possibility for new methods through descriptive semantic differentials; 3) Tools that enable the creation of methods that relate to one or more elements of the schema through creative departures from research to design; and 4) Interactive channels to playfully engage potential and new opportunities through schema interactivity.","sentences":["There are many different forms of design knowledge that guide and shape a designer's ability to act and realize potential realities.","Methods and schemas are examples of design knowledge commonly used by design researchers and designers alike.","In this pictorial, we explore, engage, and describe the role of schemas as tools that can support design researchers in formulating methods to support design action, with our framing of method design specifically focused on ethical design complexity.","We present four ways for method designers to engage with schema: 1) Systems to operationalize complex design constructs such as ethical design complexity through an A.E.I.O.YOU schema; 2) Classifiers to map existing methods and identify the possibility for new methods through descriptive semantic differentials; 3) Tools that enable the creation of methods that relate to one or more elements of the schema through creative departures from research to design; and 4) Interactive channels to playfully engage potential and new opportunities through schema interactivity."],"url":"http://arxiv.org/abs/2405.00901v1","category":"cs.HC"}
{"created":"2024-05-01 22:59:42","title":"Higher-order asymptotic profiles of solutions to the Cauchy problem for the convection-diffusion equation with variable diffusion","abstract":"We consider the asymptotic behavior of solutions to the convection-diffusion equation: \\[ \\partial_t u - \\mathrm{div}\\left(a(x)\\nabla u\\right) = d\\cdot\\nabla \\left(\\left\\lvert u\\right\\rvert ^{q-1}u\\right), \\ \\ x \\in \\mathbb{R}^n, \\ t>0 \\] with an integrable initial data $u_{0}(x)$, where $n\\ge1$, $q>1+\\frac{1}{n}$ and $d\\in \\mathbb{R}^{n}$. Moreover, we take $a(x)=1+b(x)>0$, where $b(x)$ is smooth and decays fast enough at spatial infinity. It is known that the asymptotic profile of the solution to this problem can be given by the heat kernel. Moreover, the second asymptotic profile of the solution have already been studied. In particular, the following three cases are distinguished: $1+\\frac{1}{n}<q<1+\\frac{2}{n}$; $q=1+\\frac{2}{n}$; $q>1+\\frac{2}{n}$. More precisely, the second asymptotic profile has different properties in each of these three cases. In this paper, we focus on the critical case of $q=1+\\frac{2}{n}$. By analyzing the corresponding integral equation in details, we have succeeded to give the more higher-order asymptotic expansion of the solution, which generalizes the previous works.","sentences":["We consider the asymptotic behavior of solutions to the convection-diffusion equation: \\[ \\partial_t u - \\mathrm{div}\\left(a(x)\\nabla u\\right) = d\\cdot\\nabla \\left(\\left\\lvert u\\right\\rvert ^{q-1}u\\right), \\ \\ x \\in \\mathbb{R}^n, \\ t>0 \\] with an integrable initial data $u_{0}(x)$, where $n\\ge1$, $q>1+\\frac{1}{n}$ and $d\\in \\mathbb{R}^{n}$. Moreover, we take $a(x)=1+b(x)>0$, where $b(x)$ is smooth and decays fast enough at spatial infinity.","It is known that the asymptotic profile of the solution to this problem can be given by the heat kernel.","Moreover, the second asymptotic profile of the solution have already been studied.","In particular, the following three cases are distinguished: $1+\\frac{1}{n}<q<1+\\frac{2}{n}$; $q=1+\\frac{2}{n}$; $q>1+\\frac{2}{n}$. More precisely, the second asymptotic profile has different properties in each of these three cases.","In this paper, we focus on the critical case of $q=1+\\frac{2}{n}$. By analyzing the corresponding integral equation in details, we have succeeded to give the more higher-order asymptotic expansion of the solution, which generalizes the previous works."],"url":"http://arxiv.org/abs/2405.00896v1","category":"math.AP"}
{"created":"2024-05-01 22:32:03","title":"An interacting particle consensus method for constrained global optimization","abstract":"This paper presents a particle-based optimization method designed for addressing minimization problems with equality constraints, particularly in cases where the loss function exhibits non-differentiability or non-convexity. The proposed method combines components from consensus-based optimization algorithm with a newly introduced forcing term directed at the constraint set. A rigorous mean-field limit of the particle system is derived, and the convergence of the mean-field limit to the constrained minimizer is established. Additionally, we introduce a stable discretized algorithm and conduct various numerical experiments to demonstrate the performance of the proposed method.","sentences":["This paper presents a particle-based optimization method designed for addressing minimization problems with equality constraints, particularly in cases where the loss function exhibits non-differentiability or non-convexity.","The proposed method combines components from consensus-based optimization algorithm with a newly introduced forcing term directed at the constraint set.","A rigorous mean-field limit of the particle system is derived, and the convergence of the mean-field limit to the constrained minimizer is established.","Additionally, we introduce a stable discretized algorithm and conduct various numerical experiments to demonstrate the performance of the proposed method."],"url":"http://arxiv.org/abs/2405.00891v1","category":"math.OC"}
{"created":"2024-05-01 22:27:20","title":"Revisiting relativistic electrically charged polytropic spheres","abstract":"We revisit the problem of the structure and physical properties of electrically charged static spherically symmetric solutions of the Einstein-Maxwell system of equations where the matter model is a polytropic gas. We consider a relativistic polytrope equation of state and take the electric charge density to be proportional to the rest mass density. We construct the families of solutions corresponding to various sets of parameters and analyze their stability and compliance with the causality requirement, with special emphasis on the possibility of constructing black hole mimickers. Concretely, we want to test how much electric charge a given object can hold and how compact it can be. We conclude that there is a microscopic bound on the charge density to rest mass density ratio coincident with the macroscopic bound regarding the extremal Reissner-Nordst\\\"om black hole. The macroscopic charge to mass ratio for the object can exceed the corresponding microscopic ratio if the object is non-extremal. Crucially, the only way to obtain a black hole mimicker is by taking a subtle limit in which an electrically counterpoised dust solution is obtained.","sentences":["We revisit the problem of the structure and physical properties of electrically charged static spherically symmetric solutions of the Einstein-Maxwell system of equations where the matter model is a polytropic gas.","We consider a relativistic polytrope equation of state and take the electric charge density to be proportional to the rest mass density.","We construct the families of solutions corresponding to various sets of parameters and analyze their stability and compliance with the causality requirement, with special emphasis on the possibility of constructing black hole mimickers.","Concretely, we want to test how much electric charge a given object can hold and how compact it can be.","We conclude that there is a microscopic bound on the charge density to rest mass density ratio coincident with the macroscopic bound regarding the extremal Reissner-Nordst\\\"om black hole.","The macroscopic charge to mass ratio for the object can exceed the corresponding microscopic ratio if the object is non-extremal.","Crucially, the only way to obtain a black hole mimicker is by taking a subtle limit in which an electrically counterpoised dust solution is obtained."],"url":"http://arxiv.org/abs/2405.00890v1","category":"gr-qc"}
{"created":"2024-05-01 21:53:44","title":"A Differentiable Dynamic Modeling Approach to Integrated Motion Planning and Actuator Physical Design for Mobile Manipulators","abstract":"This paper investigates the differentiable dynamic modeling of mobile manipulators to facilitate efficient motion planning and physical design of actuators, where the actuator design is parameterized by physically meaningful motor geometry parameters. These parameters impact the manipulator's link mass, inertia, center-of-mass, torque constraints, and angular velocity constraints, influencing control authority in motion planning and trajectory tracking control. A motor's maximum torque/speed and how the design parameters affect the dynamics are modeled analytically, facilitating differentiable and analytical dynamic modeling. Additionally, an integrated locomotion and manipulation planning problem is formulated with direct collocation discretization, using the proposed differentiable dynamics and motor parameterization. Such dynamics are required to capture the dynamic coupling between the base and the manipulator. Numerical experiments demonstrate the effectiveness of differentiable dynamics in speeding up optimization and advantages in task completion time and energy consumption over established sequential motion planning approach. Finally, this paper introduces a simultaneous actuator design and motion planning framework, providing numerical results to validate the proposed differentiable modeling approach for co-design problems.","sentences":["This paper investigates the differentiable dynamic modeling of mobile manipulators to facilitate efficient motion planning and physical design of actuators, where the actuator design is parameterized by physically meaningful motor geometry parameters.","These parameters impact the manipulator's link mass, inertia, center-of-mass, torque constraints, and angular velocity constraints, influencing control authority in motion planning and trajectory tracking control.","A motor's maximum torque/speed and how the design parameters affect the dynamics are modeled analytically, facilitating differentiable and analytical dynamic modeling.","Additionally, an integrated locomotion and manipulation planning problem is formulated with direct collocation discretization, using the proposed differentiable dynamics and motor parameterization.","Such dynamics are required to capture the dynamic coupling between the base and the manipulator.","Numerical experiments demonstrate the effectiveness of differentiable dynamics in speeding up optimization and advantages in task completion time and energy consumption over established sequential motion planning approach.","Finally, this paper introduces a simultaneous actuator design and motion planning framework, providing numerical results to validate the proposed differentiable modeling approach for co-design problems."],"url":"http://arxiv.org/abs/2405.00882v1","category":"cs.RO"}
{"created":"2024-05-01 20:28:37","title":"On manifolds with nonnegative Ricci curvature and the infimum of volume growth order $<2$","abstract":"We prove two rigidity theorems for open (complete and noncompact) $n$-manifolds $M$ with nonnegative Ricci curvature and the infimum of volume growth order $<2$. The first theorem asserts that the Riemannian universal cover of $M$ has Euclidean volume growth if and only if $M$ is flat with an $n-1$ dimensional soul. The second theorem asserts that there exists a nonconstant linear growth harmonic function on $M$ if and only if $M$ is isometric to the metric product $\\mathbb{R}\\times N$ for some compact manifold $N$.","sentences":["We prove two rigidity theorems for open (complete and noncompact) $n$-manifolds $M$ with nonnegative Ricci curvature and the infimum of volume growth order $<2$.","The first theorem asserts that the Riemannian universal cover of $M$ has Euclidean volume growth if and only if $M$ is flat with an $n-1$ dimensional soul.","The second theorem asserts that there exists a nonconstant linear growth harmonic function on $M$ if and only if $M$ is isometric to the metric product $\\mathbb{R}\\times N$ for some compact manifold $N$."],"url":"http://arxiv.org/abs/2405.00852v1","category":"math.DG"}
{"created":"2024-05-01 19:39:33","title":"Modelling the nanopore sequencing process with Helicase HMMs","abstract":"Recent advancements in nanopore sequencing technology, particularly the R10 nanopore from Oxford Nanopore Technology, have necessitated the development of improved data processing methods to utilize their potential for more than 9-mer resolution fully. The processing of the ion currents predominantly utilizes neural network-based methods known for their high basecalling accuracy but face developmental bottlenecks at higher resolutions. In light of this, we introduce the Helicase Hidden Markov Model (HHMM), a novel framework designed to incorporate the dynamics of the helicase motor protein alongside the nucleotide sequence during nanopore sequencing. This model supports the analysis of millions of distinct states, enhancing our understanding of raw ion currents and their alignment with nucleotide sequences. Our findings demonstrate the utility of HHMM not only as a potent visualization tool but also as an effective base for developing advanced basecalling algorithms. This approach offers a promising avenue for leveraging the full capabilities of emerging high-resolution nanopore sequencing technologies.","sentences":["Recent advancements in nanopore sequencing technology, particularly the R10 nanopore from Oxford Nanopore Technology, have necessitated the development of improved data processing methods to utilize their potential for more than 9-mer resolution fully.","The processing of the ion currents predominantly utilizes neural network-based methods known for their high basecalling accuracy but face developmental bottlenecks at higher resolutions.","In light of this, we introduce the Helicase Hidden Markov Model (HHMM), a novel framework designed to incorporate the dynamics of the helicase motor protein alongside the nucleotide sequence during nanopore sequencing.","This model supports the analysis of millions of distinct states, enhancing our understanding of raw ion currents and their alignment with nucleotide sequences.","Our findings demonstrate the utility of HHMM not only as a potent visualization tool but also as an effective base for developing advanced basecalling algorithms.","This approach offers a promising avenue for leveraging the full capabilities of emerging high-resolution nanopore sequencing technologies."],"url":"http://arxiv.org/abs/2405.00833v1","category":"eess.SP"}
{"created":"2024-05-01 18:56:41","title":"Extended Galerkin neural network approximation of singular variational problems with error control","abstract":"We present extended Galerkin neural networks (xGNN), a variational framework for approximating general boundary value problems (BVPs) with error control. The main contributions of this work are (1) a rigorous theory guiding the construction of new weighted least squares variational formulations suitable for use in neural network approximation of general BVPs (2) an ``extended'' feedforward network architecture which incorporates and is even capable of learning singular solution structures, thus greatly improving approximability of singular solutions. Numerical results are presented for several problems including steady Stokes flow around re-entrant corners and in convex corners with Moffatt eddies in order to demonstrate efficacy of the method.","sentences":["We present extended Galerkin neural networks (xGNN), a variational framework for approximating general boundary value problems (BVPs) with error control.","The main contributions of this work are (1) a rigorous theory guiding the construction of new weighted least squares variational formulations suitable for use in neural network approximation of general BVPs (2) an ``extended'' feedforward network architecture which incorporates and is even capable of learning singular solution structures, thus greatly improving approximability of singular solutions.","Numerical results are presented for several problems including steady Stokes flow around re-entrant corners and in convex corners with Moffatt eddies in order to demonstrate efficacy of the method."],"url":"http://arxiv.org/abs/2405.00815v1","category":"math.NA"}
{"created":"2024-05-01 18:54:59","title":"Solving Maxwell's equations with Non-Trainable Graph Neural Network Message Passing","abstract":"Computational electromagnetics (CEM) is employed to numerically solve Maxwell's equations, and it has very important and practical applications across a broad range of disciplines, including biomedical engineering, nanophotonics, wireless communications, and electrodynamics. The main limitation of existing CEM methods is that they are computationally demanding. Our work introduces a leap forward in scientific computing and CEM by proposing an original solution of Maxwell's equations that is grounded on graph neural networks (GNNs) and enables the high-performance numerical resolution of these fundamental mathematical expressions. Specifically, we demonstrate that the update equations derived by discretizing Maxwell's partial differential equations can be innately expressed as a two-layer GNN with static and pre-determined edge weights. Given this intuition, a straightforward way to numerically solve Maxwell's equations entails simple message passing between such a GNN's nodes, yielding a significant computational time gain, while preserving the same accuracy as conventional transient CEM methods. Ultimately, our work supports the efficient and precise emulation of electromagnetic wave propagation with GNNs, and more importantly, we anticipate that applying a similar treatment to systems of partial differential equations arising in other scientific disciplines, e.g., computational fluid dynamics, can benefit computational sciences","sentences":["Computational electromagnetics (CEM) is employed to numerically solve Maxwell's equations, and it has very important and practical applications across a broad range of disciplines, including biomedical engineering, nanophotonics, wireless communications, and electrodynamics.","The main limitation of existing CEM methods is that they are computationally demanding.","Our work introduces a leap forward in scientific computing and CEM by proposing an original solution of Maxwell's equations that is grounded on graph neural networks (GNNs) and enables the high-performance numerical resolution of these fundamental mathematical expressions.","Specifically, we demonstrate that the update equations derived by discretizing Maxwell's partial differential equations can be innately expressed as a two-layer GNN with static and pre-determined edge weights.","Given this intuition, a straightforward way to numerically solve Maxwell's equations entails simple message passing between such a GNN's nodes, yielding a significant computational time gain, while preserving the same accuracy as conventional transient CEM methods.","Ultimately, our work supports the efficient and precise emulation of electromagnetic wave propagation with GNNs, and more importantly, we anticipate that applying a similar treatment to systems of partial differential equations arising in other scientific disciplines, e.g., computational fluid dynamics, can benefit computational sciences"],"url":"http://arxiv.org/abs/2405.00814v1","category":"cs.CE"}
{"created":"2024-05-01 18:50:18","title":"A Simple Comparison of Biochemical Systems Theory and Metabolic Control Analysis","abstract":"This paper explores some basic concepts of Biochemical Systems Theory (BST) and Metabolic Control Analysis (MCA), two frameworks developed to understand the behavior of biochemical networks. Initially introduced by Savageau, BST focuses on system stability and employs power laws in modeling biochemical systems. On the other hand, MCA, pioneered by authors such as Kacser and Burns and Heinrich and Rapoport, emphasizes linearization of the governing equations and describes relationships (known as theorems) between different measures. Despite apparent differences, both frameworks are shown to be equivalent in many respects. Through a simple example of a linear chain, the paper demonstrates how BST and MCA yield identical results when analyzing steady-state behavior and logarithmic gains within biochemical pathways. This comparative analysis highlights the interchangeability of concepts such as kinetic orders, elasticities and other logarithmic gains.","sentences":["This paper explores some basic concepts of Biochemical Systems Theory (BST) and Metabolic Control Analysis (MCA), two frameworks developed to understand the behavior of biochemical networks.","Initially introduced by Savageau, BST focuses on system stability and employs power laws in modeling biochemical systems.","On the other hand, MCA, pioneered by authors such as Kacser and Burns and Heinrich and Rapoport, emphasizes linearization of the governing equations and describes relationships (known as theorems) between different measures.","Despite apparent differences, both frameworks are shown to be equivalent in many respects.","Through a simple example of a linear chain, the paper demonstrates how BST and MCA yield identical results when analyzing steady-state behavior and logarithmic gains within biochemical pathways.","This comparative analysis highlights the interchangeability of concepts such as kinetic orders, elasticities and other logarithmic gains."],"url":"http://arxiv.org/abs/2405.00810v1","category":"q-bio.MN"}
{"created":"2024-05-01 18:22:43","title":"Reverse Lieb--Thirring inequality for the half-line matrix Schr\u00f6dinger operator","abstract":"We prove a reverse Lieb--Thirring inequality with a sharp constant for the matrix Schr\\\"odinger equation on the half-line.","sentences":["We prove a reverse Lieb--Thirring inequality with a sharp constant for the matrix Schr\\\"odinger equation on the half-line."],"url":"http://arxiv.org/abs/2405.00799v1","category":"math-ph"}
{"created":"2024-05-01 18:00:28","title":"A Radio Study of Persistent Radio Sources in Nearby Dwarf Galaxies: Implications for Fast Radio Bursts","abstract":"We present 1 - 12 GHz Karl G. Jansky Very Large Array observations of 9 off-nuclear persistent radio sources (PRSs) in nearby (z < 0.055) dwarf galaxies, along with high-resolution European very-long baseline interferometry (VLBI) Network (EVN) observations for one of them at 1.7GHz. We explore the plausibility that these PRSs are associated with fast radio burst (FRB) sources by examining their properties, physical sizes, host-normalized offsets, spectral energy distributions (SEDs), radio luminosities, and light curves, and compare them to those of the PRSs associated with FRBs 20121102A and 20190520B, two known active galactic nuclei (AGN), and one likely AGN in our sample with comparable data, as well as other radio transients exhibiting characteristics analogous to FRB-PRSs. We identify a single source in our sample, J1136+2643, as the most promising FRB- PRS, based on its compact physical size and host-normalized offset. We further identify two sources, J0019+1507 and J0909+5955, with physical sizes comparable to FRB-PRSs, but which exhibit large offsets and flat spectral indices potentially indicative of a background AGN origin. We test the viability of neutron star wind nebulae and hypernebulae models for J1136+2643, and find that the physical size, luminosity, and SED of J1136+2643 are broadly consistent with these models. Finally, we discuss the alternative interpretation that the radio sources are instead powered by accreting massive black holes and outline future prospects and follow-up observations for differentiating between these scenarios.","sentences":["We present 1 - 12 GHz Karl G. Jansky Very Large Array observations of 9 off-nuclear persistent radio sources (PRSs) in nearby (z < 0.055) dwarf galaxies, along with high-resolution European very-long baseline interferometry (VLBI)","Network (EVN) observations for one of them at 1.7GHz.","We explore the plausibility that these PRSs are associated with fast radio burst (FRB) sources by examining their properties, physical sizes, host-normalized offsets, spectral energy distributions (SEDs), radio luminosities, and light curves, and compare them to those of the PRSs associated with FRBs 20121102A and 20190520B, two known active galactic nuclei (AGN), and one likely AGN in our sample with comparable data, as well as other radio transients exhibiting characteristics analogous to FRB-PRSs.","We identify a single source in our sample, J1136+2643, as the most promising FRB- PRS, based on its compact physical size and host-normalized offset.","We further identify two sources, J0019+1507 and J0909+5955, with physical sizes comparable to FRB-PRSs, but which exhibit large offsets and flat spectral indices potentially indicative of a background AGN origin.","We test the viability of neutron star wind nebulae and hypernebulae models for J1136+2643, and find that the physical size, luminosity, and SED of J1136+2643 are broadly consistent with these models.","Finally, we discuss the alternative interpretation that the radio sources are instead powered by accreting massive black holes and outline future prospects and follow-up observations for differentiating between these scenarios."],"url":"http://arxiv.org/abs/2405.00784v1","category":"astro-ph.HE"}
{"created":"2024-05-01 18:00:04","title":"Higher spins and Finsler geometry","abstract":"Finsler geometry is a natural generalization of (pseudo-)Riemannian geometry, where the line element is not the square root of a quadratic form but a more general homogeneous function. Parameterizing this in terms of symmetric tensors suggests a possible interpretation in terms of higher-spin fields. We will see here that, at linear level in these fields, the Finsler version of the Ricci tensor leads to the curved-space Fronsdal equation for all spins, plus a Stueckelberg-like coupling. Nonlinear terms can also be systematically analyzed, suggesting a possible interacting structure. No particular choice of spacetime dimension is needed. The Stueckelberg mechanism breaks gauge transformations to a redundancy that does not change the geometry. This is however not enough to eliminate non-transverse modes, at least for some versions of Finsler dynamics.","sentences":["Finsler geometry is a natural generalization of (pseudo-)Riemannian geometry, where the line element is not the square root of a quadratic form but a more general homogeneous function.","Parameterizing this in terms of symmetric tensors suggests a possible interpretation in terms of higher-spin fields.","We will see here that, at linear level in these fields, the Finsler version of the Ricci tensor leads to the curved-space Fronsdal equation for all spins, plus a Stueckelberg-like coupling.","Nonlinear terms can also be systematically analyzed, suggesting a possible interacting structure.","No particular choice of spacetime dimension is needed.","The Stueckelberg mechanism breaks gauge transformations to a redundancy that does not change the geometry.","This is however not enough to eliminate non-transverse modes, at least for some versions of Finsler dynamics."],"url":"http://arxiv.org/abs/2405.00776v1","category":"hep-th"}
{"created":"2024-05-01 18:00:00","title":"Simple fits for the neutrino luminosities from protoneutron star cooling","abstract":"We propose a simple fit function, $L_{\\nu_i}(t) = C\\, t^{-\\alpha}\\, e^{-(t/\\tau)^{n}}$, to parametrize the luminosities of neutrinos and antineutrinos of all flavors during the protoneutron star (PNS) cooling phase at post-bounce times $t \\gtrsim 1$ s. This fit is based on results from a set of neutrino-hydrodynamics simulations of core-collapse supernovae in spherical symmetry. The simulations were performed with an energy-dependent transport for six neutrino species and took into account the effects of convection and muons in the dense and hot PNS interior. We provide values of the fit parameters $C$, $\\alpha$, $\\tau$, and $n$ for different neutron star masses and equations of state as well as correlations between these fit parameters. Our functional description is useful for analytic supernova modeling, for characterizing the neutrino light curves in large underground neutrino detectors, and as a tool to extract information from measured signals on the mass and equation of state of the PNS and on secondary signal components on top of the PNS's neutrino emission.","sentences":["We propose a simple fit function, $L_{\\nu_i}(t) = C\\, t^{-\\alpha}\\, e^{-(t/\\tau)^{n}}$, to parametrize the luminosities of neutrinos and antineutrinos of all flavors during the protoneutron star (PNS) cooling phase at post-bounce times $t \\gtrsim 1$ s. This fit is based on results from a set of neutrino-hydrodynamics simulations of core-collapse supernovae in spherical symmetry.","The simulations were performed with an energy-dependent transport for six neutrino species and took into account the effects of convection and muons in the dense and hot PNS interior.","We provide values of the fit parameters $C$, $\\alpha$, $\\tau$, and $n$ for different neutron star masses and equations of state as well as correlations between these fit parameters.","Our functional description is useful for analytic supernova modeling, for characterizing the neutrino light curves in large underground neutrino detectors, and as a tool to extract information from measured signals on the mass and equation of state of the PNS and on secondary signal components on top of the PNS's neutrino emission."],"url":"http://arxiv.org/abs/2405.00769v1","category":"astro-ph.HE"}
{"created":"2024-05-02 17:59:57","title":"Multi-Space Alignments Towards Universal LiDAR Segmentation","abstract":"A unified and versatile LiDAR segmentation model with strong robustness and generalizability is desirable for safe autonomous driving perception. This work presents M3Net, a one-of-a-kind framework for fulfilling multi-task, multi-dataset, multi-modality LiDAR segmentation in a universal manner using just a single set of parameters. To better exploit data volume and diversity, we first combine large-scale driving datasets acquired by different types of sensors from diverse scenes and then conduct alignments in three spaces, namely data, feature, and label spaces, during the training. As a result, M3Net is capable of taming heterogeneous data for training state-of-the-art LiDAR segmentation models. Extensive experiments on twelve LiDAR segmentation datasets verify our effectiveness. Notably, using a shared set of parameters, M3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the official benchmarks of SemanticKITTI, nuScenes, and Waymo Open.","sentences":["A unified and versatile LiDAR segmentation model with strong robustness and generalizability is desirable for safe autonomous driving perception.","This work presents M3Net, a one-of-a-kind framework for fulfilling multi-task, multi-dataset, multi-modality LiDAR segmentation in a universal manner using just a single set of parameters.","To better exploit data volume and diversity, we first combine large-scale driving datasets acquired by different types of sensors from diverse scenes and then conduct alignments in three spaces, namely data, feature, and label spaces, during the training.","As a result, M3Net is capable of taming heterogeneous data for training state-of-the-art LiDAR segmentation models.","Extensive experiments on twelve LiDAR segmentation datasets verify our effectiveness.","Notably, using a shared set of parameters, M3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the official benchmarks of SemanticKITTI, nuScenes, and Waymo Open."],"url":"http://arxiv.org/abs/2405.01538v1","category":"cs.CV"}
{"created":"2024-05-02 16:35:07","title":"Test-time Assessment of a Model's Performance on Unseen Domains via Optimal Transport","abstract":"Gauging the performance of ML models on data from unseen domains at test-time is essential yet a challenging problem due to the lack of labels in this setting. Moreover, the performance of these models on in-distribution data is a poor indicator of their performance on data from unseen domains. Thus, it is essential to develop metrics that can provide insights into the model's performance at test time and can be computed only with the information available at test time (such as their model parameters, the training data or its statistics, and the unlabeled test data). To this end, we propose a metric based on Optimal Transport that is highly correlated with the model's performance on unseen domains and is efficiently computable only using information available at test time. Concretely, our metric characterizes the model's performance on unseen domains using only a small amount of unlabeled data from these domains and data or statistics from the training (source) domain(s). Through extensive empirical evaluation using standard benchmark datasets, and their corruptions, we demonstrate the utility of our metric in estimating the model's performance in various practical applications. These include the problems of selecting the source data and architecture that leads to the best performance on data from an unseen domain and the problem of predicting a deployed model's performance at test time on unseen domains. Our empirical results show that our metric, which uses information from both the source and the unseen domain, is highly correlated with the model's performance, achieving a significantly better correlation than that obtained via the popular prediction entropy-based metric, which is computed solely using the data from the unseen domain.","sentences":["Gauging the performance of ML models on data from unseen domains at test-time is essential yet a challenging problem due to the lack of labels in this setting.","Moreover, the performance of these models on in-distribution data is a poor indicator of their performance on data from unseen domains.","Thus, it is essential to develop metrics that can provide insights into the model's performance at test time and can be computed only with the information available at test time (such as their model parameters, the training data or its statistics, and the unlabeled test data).","To this end, we propose a metric based on Optimal Transport that is highly correlated with the model's performance on unseen domains and is efficiently computable only using information available at test time.","Concretely, our metric characterizes the model's performance on unseen domains using only a small amount of unlabeled data from these domains and data or statistics from the training (source) domain(s).","Through extensive empirical evaluation using standard benchmark datasets, and their corruptions, we demonstrate the utility of our metric in estimating the model's performance in various practical applications.","These include the problems of selecting the source data and architecture that leads to the best performance on data from an unseen domain and the problem of predicting a deployed model's performance at test time on unseen domains.","Our empirical results show that our metric, which uses information from both the source and the unseen domain, is highly correlated with the model's performance, achieving a significantly better correlation than that obtained via the popular prediction entropy-based metric, which is computed solely using the data from the unseen domain."],"url":"http://arxiv.org/abs/2405.01451v1","category":"cs.LG"}
{"created":"2024-05-02 16:12:01","title":"Integrating socioeconomic and geographic data to enhance infectious disease prediction in Brazilian cities","abstract":"Supervised machine learning models and public surveillance data has been employed for infectious disease forecasting in many settings. These models leverage various data sources capturing drivers of disease spread, such as climate conditions or human behavior. However, few models have incorporated the organizational structure of different geographic locations for forecasting. Traveling waves of seasonal outbreaks have been reported for dengue, influenza, and other infectious diseases, and many of the drivers of infectious disease dynamics may be shared across different cities, either due to their geographic or socioeconomic proximity. In this study, we developed a machine learning model to predict case counts of four infectious diseases across Brazilian cities one week ahead by incorporating information from related cities. We compared selecting related cities using both geographic distance and GDP per capita. Incorporating information from geographically proximate cities improved predictive performance for two of the four diseases, specifically COVID-19 and Zika. We also discuss the impact on forecasts in the presence of anomalous contagion patterns and the limitations of the proposed methodology.","sentences":["Supervised machine learning models and public surveillance data has been employed for infectious disease forecasting in many settings.","These models leverage various data sources capturing drivers of disease spread, such as climate conditions or human behavior.","However, few models have incorporated the organizational structure of different geographic locations for forecasting.","Traveling waves of seasonal outbreaks have been reported for dengue, influenza, and other infectious diseases, and many of the drivers of infectious disease dynamics may be shared across different cities, either due to their geographic or socioeconomic proximity.","In this study, we developed a machine learning model to predict case counts of four infectious diseases across Brazilian cities one week ahead by incorporating information from related cities.","We compared selecting related cities using both geographic distance and GDP per capita.","Incorporating information from geographically proximate cities improved predictive performance for two of the four diseases, specifically COVID-19 and Zika.","We also discuss the impact on forecasts in the presence of anomalous contagion patterns and the limitations of the proposed methodology."],"url":"http://arxiv.org/abs/2405.01422v1","category":"stat.AP"}
{"created":"2024-05-02 15:32:20","title":"Lexicographic Optimization: Algorithms and Stability","abstract":"A lexicographic maximum of a set $X \\subseteq \\mathbb{R}^n$ is a vector in $X$ whose smallest component is as large as possible, and subject to that requirement, whose second smallest component is as large as possible, and so on for the third smallest component, etc. Lexicographic maximization has numerous practical and theoretical applications, including fair resource allocation, analyzing the implicit regularization of learning algorithms, and characterizing refinements of game-theoretic equilibria. We prove that a minimizer in $X$ of the exponential loss function $L_c(\\mathbf{x}) = \\sum_i \\exp(-c x_i)$ converges to a lexicographic maximum of $X$ as $c \\rightarrow \\infty$, provided that $X$ is stable in the sense that a well-known iterative method for finding a lexicographic maximum of $X$ cannot be made to fail simply by reducing the required quality of each iterate by an arbitrarily tiny degree. Our result holds for both near and exact minimizers of the exponential loss, while earlier convergence results made much stronger assumptions about the set $X$ and only held for the exact minimizer. We are aware of no previous results showing a connection between the iterative method for computing a lexicographic maximum and exponential loss minimization. We show that every convex polytope is stable, but that there exist compact, convex sets that are not stable. We also provide the first analysis of the convergence rate of an exponential loss minimizer (near or exact) and discover a curious dichotomy: While the two smallest components of the vector converge to the lexicographically maximum values very quickly (at roughly the rate $\\frac{\\log n}{c}$), all other components can converge arbitrarily slowly.","sentences":["A lexicographic maximum of a set $X \\subseteq \\mathbb{R}^n$ is a vector in $X$ whose smallest component is as large as possible, and subject to that requirement, whose second smallest component is as large as possible, and so on for the third smallest component, etc.","Lexicographic maximization has numerous practical and theoretical applications, including fair resource allocation, analyzing the implicit regularization of learning algorithms, and characterizing refinements of game-theoretic equilibria.","We prove that a minimizer in $X$ of the exponential loss function $L_c(\\mathbf{x}) = \\sum_i \\exp(-c x_i)$ converges to a lexicographic maximum of $X$ as $c \\rightarrow \\infty$, provided that $X$ is stable in the sense that a well-known iterative method for finding a lexicographic maximum of $X$ cannot be made to fail simply by reducing the required quality of each iterate by an arbitrarily tiny degree.","Our result holds for both near and exact minimizers of the exponential loss, while earlier convergence results made much stronger assumptions about the set $X$ and only held for the exact minimizer.","We are aware of no previous results showing a connection between the iterative method for computing a lexicographic maximum and exponential loss minimization.","We show that every convex polytope is stable, but that there exist compact, convex sets that are not stable.","We also provide the first analysis of the convergence rate of an exponential loss minimizer (near or exact) and discover a curious dichotomy: While the two smallest components of the vector converge to the lexicographically maximum values very quickly (at roughly the rate $\\frac{\\log n}{c}$), all other components can converge arbitrarily slowly."],"url":"http://arxiv.org/abs/2405.01387v1","category":"math.OC"}
{"created":"2024-05-02 15:18:42","title":"Topics in the Study of the Pragmatic Functions of Phonetic Reduction in Dialog","abstract":"Reduced articulatory precision is common in speech, but for dialog its acoustic properties and pragmatic functions have been little studied. We here try to remedy this gap. This technical report contains content that was omitted from the journal article (Ward et al. 2024, submitted). Specifically, we here report 1) lessons learned about annotating for perceived reduction, 2) the finding that, unlike in read speech, the correlates of reduction in dialog include high pitch, wide pitch range, and intensity, and 3) a baseline model for predicting reduction in dialog, using simple acoustic/prosodic features, that achieves correlations with human perceptions of 0.24 for English, and 0.17 for Spanish. We also provide examples of additional possible pragmatic functions of reduction in English, and various discussion, observations and speculations","sentences":["Reduced articulatory precision is common in speech, but for dialog its acoustic properties and pragmatic functions have been little studied.","We here try to remedy this gap.","This technical report contains content that was omitted from the journal article (Ward et al. 2024, submitted).","Specifically, we here report 1) lessons learned about annotating for perceived reduction, 2) the finding that, unlike in read speech, the correlates of reduction in dialog include high pitch, wide pitch range, and intensity, and 3) a baseline model for predicting reduction in dialog, using simple acoustic/prosodic features, that achieves correlations with human perceptions of 0.24 for English, and 0.17 for Spanish.","We also provide examples of additional possible pragmatic functions of reduction in English, and various discussion, observations and speculations"],"url":"http://arxiv.org/abs/2405.01376v1","category":"cs.CL"}
{"created":"2024-05-02 14:45:37","title":"Strategies for Rare Population Detection and Sampling: A Methodological Approach in Liguria","abstract":"Economic policy sciences are constantly investigating the quality of well-being of broad sections of the population in order to describe the current interdependence between unequal living conditions, low levels of education and a lack of integration into society. Such studies are often carried out in the form of surveys, e.g. as part of the EU-SILC program. If the survey is designed at national or international level, the results of the study are often used as a reference by a broad range of public institutions. However, the sampling strategy per se may not capture enough information to provide an accurate representation of all population strata. Problems might arise from rare, or hard-to-sample, populations and the conclusion of the study may be compromised or unrealistic. We propose here a two-phase methodology to identify rare, poorly sampled populations and then resample the hard-to-sample strata. We focused our attention on the 2019 EU-SILC section concerning the Italian region of Liguria. Methods based on dispersion indices or deep learning were used to detect rare populations. A multi-frame survey was proposed as the sampling design. The results showed that factors such as citizenship, material deprivation and large families are still fundamental characteristics that are difficult to capture.","sentences":["Economic policy sciences are constantly investigating the quality of well-being of broad sections of the population in order to describe the current interdependence between unequal living conditions, low levels of education and a lack of integration into society.","Such studies are often carried out in the form of surveys, e.g. as part of the EU-SILC program.","If the survey is designed at national or international level, the results of the study are often used as a reference by a broad range of public institutions.","However, the sampling strategy per se may not capture enough information to provide an accurate representation of all population strata.","Problems might arise from rare, or hard-to-sample, populations and the conclusion of the study may be compromised or unrealistic.","We propose here a two-phase methodology to identify rare, poorly sampled populations and then resample the hard-to-sample strata.","We focused our attention on the 2019 EU-SILC section concerning the Italian region of Liguria.","Methods based on dispersion indices or deep learning were used to detect rare populations.","A multi-frame survey was proposed as the sampling design.","The results showed that factors such as citizenship, material deprivation and large families are still fundamental characteristics that are difficult to capture."],"url":"http://arxiv.org/abs/2405.01342v1","category":"stat.AP"}
{"created":"2024-05-02 14:11:48","title":"Misclassification bounds for PAC-Bayesian sparse deep learning","abstract":"Recently, there has been a significant focus on exploring the theoretical aspects of deep learning, especially regarding its performance in classification tasks. Bayesian deep learning has emerged as a unified probabilistic framework, seeking to integrate deep learning with Bayesian methodologies seamlessly. However, there exists a gap in the theoretical understanding of Bayesian approaches in deep learning for classification. This study presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds techniques, we present theoretical results on the prediction or misclassification error of a probabilistic approach utilizing Spike-and-Slab priors for sparse deep learning in classification. We establish non-asymptotic results for the prediction error. Additionally, we demonstrate that, by considering different architectures, our results can achieve minimax optimal rates in both low and high-dimensional settings, up to a logarithmic factor. Moreover, our additional logarithmic term yields slight improvements over previous works. Additionally, we propose and analyze an automated model selection approach aimed at optimally choosing a network architecture with guaranteed optimality.","sentences":["Recently, there has been a significant focus on exploring the theoretical aspects of deep learning, especially regarding its performance in classification tasks.","Bayesian deep learning has emerged as a unified probabilistic framework, seeking to integrate deep learning with Bayesian methodologies seamlessly.","However, there exists a gap in the theoretical understanding of Bayesian approaches in deep learning for classification.","This study presents an attempt to bridge that gap.","By leveraging PAC-Bayes bounds techniques, we present theoretical results on the prediction or misclassification error of a probabilistic approach utilizing Spike-and-Slab priors for sparse deep learning in classification.","We establish non-asymptotic results for the prediction error.","Additionally, we demonstrate that, by considering different architectures, our results can achieve minimax optimal rates in both low and high-dimensional settings, up to a logarithmic factor.","Moreover, our additional logarithmic term yields slight improvements over previous works.","Additionally, we propose and analyze an automated model selection approach aimed at optimally choosing a network architecture with guaranteed optimality."],"url":"http://arxiv.org/abs/2405.01304v1","category":"math.ST"}
{"created":"2024-05-02 12:52:23","title":"Prompt engineering paradigms for medical applications: scoping review and recommendations for better practices","abstract":"Prompt engineering is crucial for harnessing the potential of large language models (LLMs), especially in the medical domain where specialized terminology and phrasing is used. However, the efficacy of prompt engineering in the medical domain remains to be explored. In this work, 114 recent studies (2022-2024) applying prompt engineering in medicine, covering prompt learning (PL), prompt tuning (PT), and prompt design (PD) are reviewed. PD is the most prevalent (78 articles). In 12 papers, PD, PL, and PT terms were used interchangeably. ChatGPT is the most commonly used LLM, with seven papers using it for processing sensitive clinical data. Chain-of-Thought emerges as the most common prompt engineering technique. While PL and PT articles typically provide a baseline for evaluating prompt-based approaches, 64% of PD studies lack non-prompt-related baselines. We provide tables and figures summarizing existing work, and reporting recommendations to guide future research contributions.","sentences":["Prompt engineering is crucial for harnessing the potential of large language models (LLMs), especially in the medical domain where specialized terminology and phrasing is used.","However, the efficacy of prompt engineering in the medical domain remains to be explored.","In this work, 114 recent studies (2022-2024) applying prompt engineering in medicine, covering prompt learning (PL), prompt tuning (PT), and prompt design (PD) are reviewed.","PD is the most prevalent (78 articles).","In 12 papers, PD, PL, and PT terms were used interchangeably.","ChatGPT is the most commonly used LLM, with seven papers using it for processing sensitive clinical data.","Chain-of-Thought emerges as the most common prompt engineering technique.","While PL and PT articles typically provide a baseline for evaluating prompt-based approaches, 64% of PD studies lack non-prompt-related baselines.","We provide tables and figures summarizing existing work, and reporting recommendations to guide future research contributions."],"url":"http://arxiv.org/abs/2405.01249v1","category":"cs.CL"}
{"created":"2024-05-02 11:58:06","title":"CromSS: Cross-modal pre-training with noisy labels for remote sensing image segmentation","abstract":"We study the potential of noisy labels y to pretrain semantic segmentation models in a multi-modal learning framework for geospatial applications. Specifically, we propose a novel Cross-modal Sample Selection method (CromSS) that utilizes the class distributions P^{(d)}(x,c) over pixels x and classes c modelled by multiple sensors/modalities d of a given geospatial scene. Consistency of predictions across sensors $d$ is jointly informed by the entropy of P^{(d)}(x,c). Noisy label sampling we determine by the confidence of each sensor d in the noisy class label, P^{(d)}(x,c=y(x)). To verify the performance of our approach, we conduct experiments with Sentinel-1 (radar) and Sentinel-2 (optical) satellite imagery from the globally-sampled SSL4EO-S12 dataset. We pair those scenes with 9-class noisy labels sourced from the Google Dynamic World project for pretraining. Transfer learning evaluations (downstream task) on the DFC2020 dataset confirm the effectiveness of the proposed method for remote sensing image segmentation.","sentences":["We study the potential of noisy labels y to pretrain semantic segmentation models in a multi-modal learning framework for geospatial applications.","Specifically, we propose a novel Cross-modal Sample Selection method (CromSS) that utilizes the class distributions P^{(d)}(x,c) over pixels x and classes c modelled by multiple sensors/modalities d of a given geospatial scene.","Consistency of predictions across sensors $d$ is jointly informed by the entropy of P^{(d)}(x,c).","Noisy label sampling we determine by the confidence of each sensor d in the noisy class label, P^{(d)}(x,c=y(x)).","To verify the performance of our approach, we conduct experiments with Sentinel-1 (radar) and Sentinel-2 (optical) satellite imagery from the globally-sampled SSL4EO-S12 dataset.","We pair those scenes with 9-class noisy labels sourced from the Google Dynamic World project for pretraining.","Transfer learning evaluations (downstream task) on the DFC2020 dataset confirm the effectiveness of the proposed method for remote sensing image segmentation."],"url":"http://arxiv.org/abs/2405.01217v1","category":"cs.CV"}
{"created":"2024-05-02 11:40:44","title":"Latent Fingerprint Matching via Dense Minutia Descriptor","abstract":"Latent fingerprint matching is a daunting task, primarily due to the poor quality of latent fingerprints. In this study, we propose a deep-learning based dense minutia descriptor (DMD) for latent fingerprint matching. A DMD is obtained by extracting the fingerprint patch aligned by its central minutia, capturing detailed minutia information and texture information. Our dense descriptor takes the form of a three-dimensional representation, with two dimensions associated with the original image plane and the other dimension representing the abstract features. Additionally, the extraction process outputs the fingerprint segmentation map, ensuring that the descriptor is only valid in the foreground region. The matching between two descriptors occurs in their overlapping regions, with a score normalization strategy to reduce the impact brought by the differences outside the valid area. Our descriptor achieves state-of-the-art performance on several latent fingerprint datasets. Overall, our DMD is more representative and interpretable compared to previous methods.","sentences":["Latent fingerprint matching is a daunting task, primarily due to the poor quality of latent fingerprints.","In this study, we propose a deep-learning based dense minutia descriptor (DMD) for latent fingerprint matching.","A DMD is obtained by extracting the fingerprint patch aligned by its central minutia, capturing detailed minutia information and texture information.","Our dense descriptor takes the form of a three-dimensional representation, with two dimensions associated with the original image plane and the other dimension representing the abstract features.","Additionally, the extraction process outputs the fingerprint segmentation map, ensuring that the descriptor is only valid in the foreground region.","The matching between two descriptors occurs in their overlapping regions, with a score normalization strategy to reduce the impact brought by the differences outside the valid area.","Our descriptor achieves state-of-the-art performance on several latent fingerprint datasets.","Overall, our DMD is more representative and interpretable compared to previous methods."],"url":"http://arxiv.org/abs/2405.01199v1","category":"cs.CV"}
{"created":"2024-05-02 10:05:16","title":"Why Tabular Foundation Models Should Be a Research Priority","abstract":"Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources. In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data. Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power. We believe the time is now to start developing tabular foundation models, or what we coin a Large Tabular Model (LTM). LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets. The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery. We intend to excite reflections on the modalities we study, and convince some researchers to study large tabular models.","sentences":["Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources.","In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data.","Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power.","We believe the time is now to start developing tabular foundation models, or what we coin a Large Tabular Model (LTM).","LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets.","The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery.","We intend to excite reflections on the modalities we study, and convince some researchers to study large tabular models."],"url":"http://arxiv.org/abs/2405.01147v1","category":"cs.LG"}
{"created":"2024-05-02 09:38:07","title":"Investigating Self-Supervised Image Denoising with Denaturation","abstract":"Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning. However, theoretical understanding of the performance of the approach that uses denatured data is lacking. To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments. Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels. We also conduct several experiments to investigate the performance of an extended algorithm in practice. The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results. These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions.","sentences":["Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning.","However, theoretical understanding of the performance of the approach that uses denatured data is lacking.","To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments.","Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels.","We also conduct several experiments to investigate the performance of an extended algorithm in practice.","The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results.","These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions."],"url":"http://arxiv.org/abs/2405.01124v1","category":"stat.ML"}
{"created":"2024-05-02 08:25:52","title":"MCMS: Multi-Category Information and Multi-Scale Stripe Attention for Blind Motion Deblurring","abstract":"Deep learning-based motion deblurring techniques have advanced significantly in recent years. This class of techniques, however, does not carefully examine the inherent flaws in blurry images. For instance, low edge and structural information are traits of blurry images. The high-frequency component of blurry images is edge information, and the low-frequency component is structure information. A blind motion deblurring network (MCMS) based on multi-category information and multi-scale stripe attention mechanism is proposed. Given the respective characteristics of the high-frequency and low-frequency components, a three-stage encoder-decoder model is designed. Specifically, the first stage focuses on extracting the features of the high-frequency component, the second stage concentrates on extracting the features of the low-frequency component, and the third stage integrates the extracted low-frequency component features, the extracted high-frequency component features, and the original blurred image in order to recover the final clear image. As a result, the model effectively improves motion deblurring by fusing the edge information of the high-frequency component and the structural information of the low-frequency component. In addition, a grouped feature fusion technique is developed so as to achieve richer, more three-dimensional and comprehensive utilization of various types of features at a deep level. Next, a multi-scale stripe attention mechanism (MSSA) is designed, which effectively combines the anisotropy and multi-scale information of the image, a move that significantly enhances the capability of the deep model in feature representation. Large-scale comparative studies on various datasets show that the strategy in this paper works better than the recently published measures.","sentences":["Deep learning-based motion deblurring techniques have advanced significantly in recent years.","This class of techniques, however, does not carefully examine the inherent flaws in blurry images.","For instance, low edge and structural information are traits of blurry images.","The high-frequency component of blurry images is edge information, and the low-frequency component is structure information.","A blind motion deblurring network (MCMS) based on multi-category information and multi-scale stripe attention mechanism is proposed.","Given the respective characteristics of the high-frequency and low-frequency components, a three-stage encoder-decoder model is designed.","Specifically, the first stage focuses on extracting the features of the high-frequency component, the second stage concentrates on extracting the features of the low-frequency component, and the third stage integrates the extracted low-frequency component features, the extracted high-frequency component features, and the original blurred image in order to recover the final clear image.","As a result, the model effectively improves motion deblurring by fusing the edge information of the high-frequency component and the structural information of the low-frequency component.","In addition, a grouped feature fusion technique is developed so as to achieve richer, more three-dimensional and comprehensive utilization of various types of features at a deep level.","Next, a multi-scale stripe attention mechanism (MSSA) is designed, which effectively combines the anisotropy and multi-scale information of the image, a move that significantly enhances the capability of the deep model in feature representation.","Large-scale comparative studies on various datasets show that the strategy in this paper works better than the recently published measures."],"url":"http://arxiv.org/abs/2405.01083v1","category":"cs.CV"}
{"created":"2024-05-02 05:41:42","title":"QSimPy: A Learning-centric Simulation Framework for Quantum Cloud Resource Management","abstract":"Quantum cloud computing is an emerging computing paradigm that allows seamless access to quantum hardware as cloud-based services. However, effective use of quantum resources is challenging and necessitates robust simulation frameworks for effective resource management design and evaluation. To address this need, we proposed QSimPy, a novel discrete-event simulation framework designed with the main focus of facilitating learning-centric approaches for quantum resource management problems in cloud environments. Underpinned by extensibility, compatibility, and reusability principles, QSimPy provides a lightweight simulation environment based on SimPy, a well-known Python-based simulation engine for modeling dynamics of quantum cloud resources and task operations. We integrate the Gymnasium environment into our framework to support the creation of simulated environments for developing and evaluating reinforcement learning-based techniques for optimizing quantum cloud resource management. The QSimPy framework encapsulates the operational intricacies of quantum cloud environments, supporting research in dynamic task allocation and optimization through DRL approaches. We also demonstrate the use of QSimPy in developing reinforcement learning policies for quantum task placement problems, demonstrating its potential as a useful framework for future quantum cloud research.","sentences":["Quantum cloud computing is an emerging computing paradigm that allows seamless access to quantum hardware as cloud-based services.","However, effective use of quantum resources is challenging and necessitates robust simulation frameworks for effective resource management design and evaluation.","To address this need, we proposed QSimPy, a novel discrete-event simulation framework designed with the main focus of facilitating learning-centric approaches for quantum resource management problems in cloud environments.","Underpinned by extensibility, compatibility, and reusability principles, QSimPy provides a lightweight simulation environment based on SimPy, a well-known Python-based simulation engine for modeling dynamics of quantum cloud resources and task operations.","We integrate the Gymnasium environment into our framework to support the creation of simulated environments for developing and evaluating reinforcement learning-based techniques for optimizing quantum cloud resource management.","The QSimPy framework encapsulates the operational intricacies of quantum cloud environments, supporting research in dynamic task allocation and optimization through DRL approaches.","We also demonstrate the use of QSimPy in developing reinforcement learning policies for quantum task placement problems, demonstrating its potential as a useful framework for future quantum cloud research."],"url":"http://arxiv.org/abs/2405.01021v1","category":"cs.ET"}
{"created":"2024-05-02 05:35:09","title":"Network reconstruction via the minimum description length principle","abstract":"A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. As we demonstrate, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight \"shrinkage\". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.","sentences":["A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges.","The status quo in this context is based on $L_{1}$ regularization combined with cross-validation.","As we demonstrate, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight \"shrinkage\".","This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation.","In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity.","Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation.","The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data.","As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance.","We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks.","We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system."],"url":"http://arxiv.org/abs/2405.01015v1","category":"stat.ML"}
{"created":"2024-05-02 04:27:35","title":"The IgboAPI Dataset: Empowering Igbo Language Technologies through Multi-dialectal Enrichment","abstract":"The Igbo language is facing a risk of becoming endangered, as indicated by a 2025 UNESCO study. This highlights the need to develop language technologies for Igbo to foster communication, learning and preservation. To create robust, impactful, and widely adopted language technologies for Igbo, it is essential to incorporate the multi-dialectal nature of the language. The primary obstacle in achieving dialectal-aware language technologies is the lack of comprehensive dialectal datasets. In response, we present the IgboAPI dataset, a multi-dialectal Igbo-English dictionary dataset, developed with the aim of enhancing the representation of Igbo dialects. Furthermore, we illustrate the practicality of the IgboAPI dataset through two distinct studies: one focusing on Igbo semantic lexicon and the other on machine translation. In the semantic lexicon project, we successfully establish an initial Igbo semantic lexicon for the Igbo semantic tagger, while in the machine translation study, we demonstrate that by finetuning existing machine translation systems using the IgboAPI dataset, we significantly improve their ability to handle dialectal variations in sentences.","sentences":["The Igbo language is facing a risk of becoming endangered, as indicated by a 2025 UNESCO study.","This highlights the need to develop language technologies for Igbo to foster communication, learning and preservation.","To create robust, impactful, and widely adopted language technologies for Igbo, it is essential to incorporate the multi-dialectal nature of the language.","The primary obstacle in achieving dialectal-aware language technologies is the lack of comprehensive dialectal datasets.","In response, we present the IgboAPI dataset, a multi-dialectal Igbo-English dictionary dataset, developed with the aim of enhancing the representation of Igbo dialects.","Furthermore, we illustrate the practicality of the IgboAPI dataset through two distinct studies: one focusing on Igbo semantic lexicon and the other on machine translation.","In the semantic lexicon project, we successfully establish an initial Igbo semantic lexicon for the Igbo semantic tagger, while in the machine translation study, we demonstrate that by finetuning existing machine translation systems using the IgboAPI dataset, we significantly improve their ability to handle dialectal variations in sentences."],"url":"http://arxiv.org/abs/2405.00997v1","category":"cs.CL"}
{"created":"2024-05-02 03:49:47","title":"S$^2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic","abstract":"Learning expressive stochastic policies instead of deterministic ones has been proposed to achieve better stability, sample complexity, and robustness. Notably, in Maximum Entropy Reinforcement Learning (MaxEnt RL), the policy is modeled as an expressive Energy-Based Model (EBM) over the Q-values. However, this formulation requires the estimation of the entropy of such EBMs, which is an open problem. To address this, previous MaxEnt RL methods either implicitly estimate the entropy, resulting in high computational complexity and variance (SQL), or follow a variational inference procedure that fits simplified actor distributions (e.g., Gaussian) for tractability (SAC). We propose Stein Soft Actor-Critic (S$^2$AC), a MaxEnt RL algorithm that learns expressive policies without compromising efficiency. Specifically, S$^2$AC uses parameterized Stein Variational Gradient Descent (SVGD) as the underlying policy. We derive a closed-form expression of the entropy of such policies. Our formula is computationally efficient and only depends on first-order derivatives and vector products. Empirical results show that S$^2$AC yields more optimal solutions to the MaxEnt objective than SQL and SAC in the multi-goal environment, and outperforms SAC and SQL on the MuJoCo benchmark. Our code is available at: https://github.com/SafaMessaoud/S2AC-Energy-Based-RL-with-Stein-Soft-Actor-Critic","sentences":["Learning expressive stochastic policies instead of deterministic ones has been proposed to achieve better stability, sample complexity, and robustness.","Notably, in Maximum Entropy Reinforcement Learning (MaxEnt RL), the policy is modeled as an expressive Energy-Based Model (EBM) over the Q-values.","However, this formulation requires the estimation of the entropy of such EBMs, which is an open problem.","To address this, previous MaxEnt RL methods either implicitly estimate the entropy, resulting in high computational complexity and variance (SQL), or follow a variational inference procedure that fits simplified actor distributions (e.g., Gaussian) for tractability (SAC).","We propose Stein Soft Actor-Critic (S$^2$AC), a MaxEnt RL algorithm that learns expressive policies without compromising efficiency.","Specifically, S$^2$AC uses parameterized Stein Variational Gradient Descent (SVGD) as the underlying policy.","We derive a closed-form expression of the entropy of such policies.","Our formula is computationally efficient and only depends on first-order derivatives and vector products.","Empirical results show that S$^2$AC yields more optimal solutions to the MaxEnt objective than SQL and SAC in the multi-goal environment, and outperforms SAC and SQL on the MuJoCo benchmark.","Our code is available at: https://github.com/SafaMessaoud/S2AC-Energy-Based-RL-with-Stein-Soft-Actor-Critic"],"url":"http://arxiv.org/abs/2405.00987v1","category":"cs.LG"}
{"created":"2024-05-02 03:03:34","title":"Robust Decentralized Learning with Local Updates and Gradient Tracking","abstract":"As distributed learning applications such as Federated Learning, the Internet of Things (IoT), and Edge Computing grow, it is critical to address the shortcomings of such technologies from a theoretical perspective. As an abstraction, we consider decentralized learning over a network of communicating clients or nodes and tackle two major challenges: data heterogeneity and adversarial robustness. We propose a decentralized minimax optimization method that employs two important modules: local updates and gradient tracking. Minimax optimization is the key tool to enable adversarial training for ensuring robustness. Having local updates is essential in Federated Learning (FL) applications to mitigate the communication bottleneck, and utilizing gradient tracking is essential to proving convergence in the case of data heterogeneity. We analyze the performance of the proposed algorithm, Dec-FedTrack, in the case of nonconvex-strongly concave minimax optimization, and prove that it converges a stationary point. We also conduct numerical experiments to support our theoretical findings.","sentences":["As distributed learning applications such as Federated Learning, the Internet of Things (IoT), and Edge Computing grow, it is critical to address the shortcomings of such technologies from a theoretical perspective.","As an abstraction, we consider decentralized learning over a network of communicating clients or nodes and tackle two major challenges: data heterogeneity and adversarial robustness.","We propose a decentralized minimax optimization method that employs two important modules: local updates and gradient tracking.","Minimax optimization is the key tool to enable adversarial training for ensuring robustness.","Having local updates is essential in Federated Learning (FL) applications to mitigate the communication bottleneck, and utilizing gradient tracking is essential to proving convergence in the case of data heterogeneity.","We analyze the performance of the proposed algorithm, Dec-FedTrack, in the case of nonconvex-strongly concave minimax optimization, and prove that it converges a stationary point.","We also conduct numerical experiments to support our theoretical findings."],"url":"http://arxiv.org/abs/2405.00965v1","category":"cs.LG"}
{"created":"2024-05-02 03:02:25","title":"Deriving Lehmer and H\u00f6lder means as maximum weighted likelihood estimates for the multivariate exponential family","abstract":"The links between the mean families of Lehmer and H\\\"older and the weighted maximum likelihood estimator have recently been established in the case of a regular univariate exponential family. In this article, we will extend the outcomes obtained to the multivariate case. This extension provides a probabilistic interpretation of these families of means and could therefore broaden their uses in various applications.","sentences":["The links between the mean families of Lehmer and H\\\"older and the weighted maximum likelihood estimator have recently been established in the case of a regular univariate exponential family.","In this article, we will extend the outcomes obtained to the multivariate case.","This extension provides a probabilistic interpretation of these families of means and could therefore broaden their uses in various applications."],"url":"http://arxiv.org/abs/2405.00964v1","category":"math.ST"}
{"created":"2024-05-02 02:58:28","title":"FITA: Fine-grained Image-Text Aligner for Radiology Report Generation","abstract":"Radiology report generation aims to automatically generate detailed and coherent descriptive reports alongside radiology images. Previous work mainly focused on refining fine-grained image features or leveraging external knowledge. However, the precise alignment of fine-grained image features with corresponding text descriptions has not been considered. This paper presents a novel method called Fine-grained Image-Text Aligner (FITA) to construct fine-grained alignment for image and text features. It has three novel designs: Image Feature Refiner (IFR), Text Feature Refiner (TFR) and Contrastive Aligner (CA). IFR and TFR aim to learn fine-grained image and text features, respectively. We achieve this by leveraging saliency maps to effectively fuse symptoms with corresponding abnormal visual regions, and by utilizing a meticulously constructed triplet set for training. Finally, CA module aligns fine-grained image and text features using contrastive loss for precise alignment. Results show that our method surpasses existing methods on the widely used benchmark","sentences":["Radiology report generation aims to automatically generate detailed and coherent descriptive reports alongside radiology images.","Previous work mainly focused on refining fine-grained image features or leveraging external knowledge.","However, the precise alignment of fine-grained image features with corresponding text descriptions has not been considered.","This paper presents a novel method called Fine-grained Image-Text Aligner (FITA) to construct fine-grained alignment for image and text features.","It has three novel designs: Image Feature Refiner (IFR), Text Feature Refiner (TFR) and Contrastive Aligner (CA).","IFR and TFR aim to learn fine-grained image and text features, respectively.","We achieve this by leveraging saliency maps to effectively fuse symptoms with corresponding abnormal visual regions, and by utilizing a meticulously constructed triplet set for training.","Finally, CA module aligns fine-grained image and text features using contrastive loss for precise alignment.","Results show that our method surpasses existing methods on the widely used benchmark"],"url":"http://arxiv.org/abs/2405.00962v1","category":"cs.CV"}
{"created":"2024-05-02 01:39:30","title":"New bounds on the cohesion of complete-link and other linkage methods for agglomeration clustering","abstract":"Linkage methods are among the most popular algorithms for hierarchical clustering. Despite their relevance the current knowledge regarding the quality of the clustering produced by these methods is limited. Here, we improve the currently available bounds on the maximum diameter of the clustering obtained by complete-link for metric spaces.   One of our new bounds, in contrast to the existing ones, allows us to separate complete-link from single-link in terms of approximation for the diameter, which corroborates the common perception that the former is more suitable than the latter when the goal is producing compact clusters.   We also show that our techniques can be employed to derive upper bounds on the cohesion of a class of linkage methods that includes the quite popular average-link.","sentences":["Linkage methods are among the most popular algorithms for hierarchical clustering.","Despite their relevance the current knowledge regarding the quality of the clustering produced by these methods is limited.","Here, we improve the currently available bounds on the maximum diameter of the clustering obtained by complete-link for metric spaces.   ","One of our new bounds, in contrast to the existing ones, allows us to separate complete-link from single-link in terms of approximation for the diameter, which corroborates the common perception that the former is more suitable than the latter when the goal is producing compact clusters.   ","We also show that our techniques can be employed to derive upper bounds on the cohesion of a class of linkage methods that includes the quite popular average-link."],"url":"http://arxiv.org/abs/2405.00937v1","category":"cs.LG"}
{"created":"2024-05-02 01:24:53","title":"Benchmarking Representations for Speech, Music, and Acoustic Events","abstract":"Limited diversity in standardized benchmarks for evaluating audio representation learning (ARL) methods may hinder systematic comparison of current methods' capabilities. We present ARCH, a comprehensive benchmark for evaluating ARL methods on diverse audio classification domains, covering acoustic events, music, and speech. ARCH comprises 12 datasets, that allow us to thoroughly assess pre-trained SSL models of different sizes. ARCH streamlines benchmarking of ARL techniques through its unified access to a wide range of domains and its ability to readily incorporate new datasets and models. To address the current lack of open-source, pre-trained models for non-speech audio, we also release new pre-trained models that demonstrate strong performance on non-speech datasets. We argue that the presented wide-ranging evaluation provides valuable insights into state-of-the-art ARL methods, and is useful to pinpoint promising research directions.","sentences":["Limited diversity in standardized benchmarks for evaluating audio representation learning (ARL) methods may hinder systematic comparison of current methods' capabilities.","We present ARCH, a comprehensive benchmark for evaluating ARL methods on diverse audio classification domains, covering acoustic events, music, and speech.","ARCH comprises 12 datasets, that allow us to thoroughly assess pre-trained SSL models of different sizes.","ARCH streamlines benchmarking of ARL techniques through its unified access to a wide range of domains and its ability to readily incorporate new datasets and models.","To address the current lack of open-source, pre-trained models for non-speech audio, we also release new pre-trained models that demonstrate strong performance on non-speech datasets.","We argue that the presented wide-ranging evaluation provides valuable insights into state-of-the-art ARL methods, and is useful to pinpoint promising research directions."],"url":"http://arxiv.org/abs/2405.00934v1","category":"eess.AS"}
{"created":"2024-05-01 23:59:36","title":"Accelerated Fully First-Order Methods for Bilevel and Minimax Optimization","abstract":"This paper presents a new algorithm member for accelerating first-order methods for bilevel optimization, namely the \\emph{(Perturbed) Restarted Accelerated Fully First-order methods for Bilevel Approximation}, abbreviated as \\texttt{(P)RAF${}^2$BA}. The algorithm leverages \\emph{fully} first-order oracles and seeks approximate stationary points in nonconvex-strongly-convex bilevel optimization, enhancing oracle complexity for efficient optimization. Theoretical guarantees for finding approximate first-order stationary points and second-order stationary points at the state-of-the-art query complexities are established, showcasing their effectiveness in solving complex optimization tasks. Empirical studies for real-world problems are provided to further validate the outperformance of our proposed algorithms. The significance of \\texttt{(P)RAF${}^2$BA} in optimizing nonconvex-strongly-convex bilevel optimization problems is underscored by its state-of-the-art convergence rates and computational efficiency.","sentences":["This paper presents a new algorithm member for accelerating first-order methods for bilevel optimization, namely the \\emph{(Perturbed) Restarted Accelerated Fully First-order methods for Bilevel Approximation}, abbreviated as \\texttt{(P)RAF${}^2$BA}.","The algorithm leverages \\emph{fully} first-order oracles and seeks approximate stationary points in nonconvex-strongly-convex bilevel optimization, enhancing oracle complexity for efficient optimization.","Theoretical guarantees for finding approximate first-order stationary points and second-order stationary points at the state-of-the-art query complexities are established, showcasing their effectiveness in solving complex optimization tasks.","Empirical studies for real-world problems are provided to further validate the outperformance of our proposed algorithms.","The significance of \\texttt{(P)RAF${}^2$BA} in optimizing nonconvex-strongly-convex bilevel optimization problems is underscored by its state-of-the-art convergence rates and computational efficiency."],"url":"http://arxiv.org/abs/2405.00914v1","category":"math.OC"}
{"created":"2024-05-01 23:41:14","title":"Quantum Federated Learning Experiments in the Cloud with Data Encoding","abstract":"Quantum Federated Learning (QFL) is an emerging concept that aims to unfold federated learning (FL) over quantum networks, enabling collaborative quantum model training along with local data privacy. We explore the challenges of deploying QFL on cloud platforms, emphasizing quantum intricacies and platform limitations. The proposed data-encoding-driven QFL, with a proof of concept (GitHub Open Source) using genomic data sets on quantum simulators, shows promising results.","sentences":["Quantum Federated Learning (QFL) is an emerging concept that aims to unfold federated learning (FL) over quantum networks, enabling collaborative quantum model training along with local data privacy.","We explore the challenges of deploying QFL on cloud platforms, emphasizing quantum intricacies and platform limitations.","The proposed data-encoding-driven QFL, with a proof of concept (GitHub Open Source) using genomic data sets on quantum simulators, shows promising results."],"url":"http://arxiv.org/abs/2405.00909v1","category":"cs.LG"}
{"created":"2024-05-01 20:48:34","title":"Public Computing Intellectuals in the Age of AI Crisis","abstract":"The belief that AI technology is on the cusp of causing a generalized social crisis became a popular one in 2023. Interestingly, some of these worries were voiced from within the tech sector itself. While there was no doubt an element of hype and exaggeration to some of these accounts, they do reflect the fact that there are troubling ramifications to this technology stack. This conjunction of shared concerns about social, political, and personal futures presaged by current developments in machine learning and data science presents the academic discipline of computing with a rare opportunity for self-examination and reconfiguration. This position paper endeavors to do so in four sections. The first expands on the nature of the AI crisis for computing. The second articulates possible critical responses to this crisis and advocates for a broader analytic focus on power relations. The third section presents a novel characterization of academic computing's epistemological field, one which includes not only the discipline's usual instrumental forms of knowledge but reflexive knowledge as well. This reflexive dimension integrates both the critical and public functions of the discipline as equal intellectual partners and a necessary component of any contemporary academic field. The final section will advocate for a conceptual archetype--the Public Computer Intellectual--as a way of practically imagining the expanded possibilities of academic practice in our discipline, one that provides both self-critique and an outward-facing orientation towards the public good. It will argue that the computer education research community can play a vital role in this regard.","sentences":["The belief that AI technology is on the cusp of causing a generalized social crisis became a popular one in 2023.","Interestingly, some of these worries were voiced from within the tech sector itself.","While there was no doubt an element of hype and exaggeration to some of these accounts, they do reflect the fact that there are troubling ramifications to this technology stack.","This conjunction of shared concerns about social, political, and personal futures presaged by current developments in machine learning and data science presents the academic discipline of computing with a rare opportunity for self-examination and reconfiguration.","This position paper endeavors to do so in four sections.","The first expands on the nature of the AI crisis for computing.","The second articulates possible critical responses to this crisis and advocates for a broader analytic focus on power relations.","The third section presents a novel characterization of academic computing's epistemological field, one which includes not only the discipline's usual instrumental forms of knowledge but reflexive knowledge as well.","This reflexive dimension integrates both the critical and public functions of the discipline as equal intellectual partners and a necessary component of any contemporary academic field.","The final section will advocate for a conceptual archetype--the Public Computer Intellectual--as a way of practically imagining the expanded possibilities of academic practice in our discipline, one that provides both self-critique and an outward-facing orientation towards the public good.","It will argue that the computer education research community can play a vital role in this regard."],"url":"http://arxiv.org/abs/2405.00860v1","category":"cs.CY"}
{"created":"2024-05-01 20:47:06","title":"Guided Conditional Diffusion Classifier (ConDiff) for Enhanced Prediction of Infection in Diabetic Foot Ulcers","abstract":"To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes.","sentences":["To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations.","Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification.","The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space.","Results:","ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%.","The use of a triplet loss function reduces overfitting in the distance-based classifier.","Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes."],"url":"http://arxiv.org/abs/2405.00858v1","category":"cs.CV"}
{"created":"2024-05-01 20:46:04","title":"Brighteye: Glaucoma Screening with Color Fundus Photographs based on Vision Transformer","abstract":"Differences in image quality, lighting conditions, and patient demographics pose challenges to automated glaucoma detection from color fundus photography. Brighteye, a method based on Vision Transformer, is proposed for glaucoma detection and glaucomatous feature classification. Brighteye learns long-range relationships among pixels within large fundus images using a self-attention mechanism. Prior to being input into Brighteye, the optic disc is localized using YOLOv8, and the region of interest (ROI) around the disc center is cropped to ensure alignment with clinical practice. Optic disc detection improves the sensitivity at 95% specificity from 79.20% to 85.70% for glaucoma detection and the Hamming distance from 0.2470 to 0.1250 for glaucomatous feature classification. In the developmental stage of the Justified Referral in AI Glaucoma Screening (JustRAIGS) challenge, the overall outcome secured the fifth position out of 226 entries.","sentences":["Differences in image quality, lighting conditions, and patient demographics pose challenges to automated glaucoma detection from color fundus photography.","Brighteye, a method based on Vision Transformer, is proposed for glaucoma detection and glaucomatous feature classification.","Brighteye learns long-range relationships among pixels within large fundus images using a self-attention mechanism.","Prior to being input into Brighteye, the optic disc is localized using YOLOv8, and the region of interest (ROI) around the disc center is cropped to ensure alignment with clinical practice.","Optic disc detection improves the sensitivity at 95% specificity from 79.20% to 85.70% for glaucoma detection and the Hamming distance from 0.2470 to 0.1250 for glaucomatous feature classification.","In the developmental stage of the Justified Referral in AI Glaucoma Screening (JustRAIGS) challenge, the overall outcome secured the fifth position out of 226 entries."],"url":"http://arxiv.org/abs/2405.00857v1","category":"cs.CV"}
{"created":"2024-05-01 20:34:12","title":"Efficient Algorithms for Learning Monophonic Halfspaces in Graphs","abstract":"We study the problem of learning a binary classifier on the vertices of a graph. In particular, we consider classifiers given by monophonic halfspaces, partitions of the vertices that are convex in a certain abstract sense. Monophonic halfspaces, and related notions such as geodesic halfspaces,have recently attracted interest, and several connections have been drawn between their properties(e.g., their VC dimension) and the structure of the underlying graph $G$. We prove several novel results for learning monophonic halfspaces in the supervised, online, and active settings. Our main result is that a monophonic halfspace can be learned with near-optimal passive sample complexity in time polynomial in $n = |V(G)|$. This requires us to devise a polynomial-time algorithm for consistent hypothesis checking, based on several structural insights on monophonic halfspaces and on a reduction to $2$-satisfiability. We prove similar results for the online and active settings. We also show that the concept class can be enumerated with delay $\\operatorname{poly}(n)$, and that empirical risk minimization can be performed in time $2^{\\omega(G)}\\operatorname{poly}(n)$ where $\\omega(G)$ is the clique number of $G$. These results answer open questions from the literature (Gonz\\'alez et al., 2020), and show a contrast with geodesic halfspaces, for which some of the said problems are NP-hard (Seiffarth et al., 2023).","sentences":["We study the problem of learning a binary classifier on the vertices of a graph.","In particular, we consider classifiers given by monophonic halfspaces, partitions of the vertices that are convex in a certain abstract sense.","Monophonic halfspaces, and related notions such as geodesic halfspaces,have recently attracted interest, and several connections have been drawn between their properties(e.g., their VC dimension) and the structure of the underlying graph $G$.","We prove several novel results for learning monophonic halfspaces in the supervised, online, and active settings.","Our main result is that a monophonic halfspace can be learned with near-optimal passive sample complexity in time polynomial in $n = |V(G)|$. This requires us to devise a polynomial-time algorithm for consistent hypothesis checking, based on several structural insights on monophonic halfspaces and on a reduction to $2$-satisfiability.","We prove similar results for the online and active settings.","We also show that the concept class can be enumerated with delay $\\operatorname{poly}(n)$, and that empirical risk minimization can be performed in time $2^{\\omega(G)}\\operatorname{poly}(n)$ where $\\omega(G)$ is the clique number of $G$. These results answer open questions from the literature (Gonz\\'alez et al., 2020), and show a contrast with geodesic halfspaces, for which some of the said problems are NP-hard (Seiffarth et al., 2023)."],"url":"http://arxiv.org/abs/2405.00853v1","category":"cs.LG"}
{"created":"2024-05-01 20:10:06","title":"Quickest Change Detection with Confusing Change","abstract":"In the problem of quickest change detection (QCD), a change occurs at some unknown time in the distribution of a sequence of independent observations. This work studies a QCD problem where the change is either a bad change, which we aim to detect, or a confusing change, which is not of our interest. Our objective is to detect a bad change as quickly as possible while avoiding raising a false alarm for pre-change or a confusing change. We identify a specific set of pre-change, bad change, and confusing change distributions that pose challenges beyond the capabilities of standard Cumulative Sum (CuSum) procedures. Proposing novel CuSum-based detection procedures, S-CuSum and J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across all kinds of pre-change, bad change, and confusing change distributions. For both S-CuSum and J-CuSum, we provide analytical performance guarantees and validate them by numerical results. Furthermore, both procedures are computationally efficient as they only require simple recursive updates.","sentences":["In the problem of quickest change detection (QCD), a change occurs at some unknown time in the distribution of a sequence of independent observations.","This work studies a QCD problem where the change is either a bad change, which we aim to detect, or a confusing change, which is not of our interest.","Our objective is to detect a bad change as quickly as possible while avoiding raising a false alarm for pre-change or a confusing change.","We identify a specific set of pre-change, bad change, and confusing change distributions that pose challenges beyond the capabilities of standard Cumulative Sum (CuSum) procedures.","Proposing novel CuSum-based detection procedures, S-CuSum and J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across all kinds of pre-change, bad change, and confusing change distributions.","For both S-CuSum and J-CuSum, we provide analytical performance guarantees and validate them by numerical results.","Furthermore, both procedures are computationally efficient as they only require simple recursive updates."],"url":"http://arxiv.org/abs/2405.00842v1","category":"math.ST"}
{"created":"2024-05-01 19:02:18","title":"HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond","abstract":"Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present challenges. Existing datasets have limitations in terms of benchmark coverage, design space enumeration, vendor extensibility, or lack of reproducible and extensible software for dataset construction. Many works also lack user-friendly ways to add more designs, limiting wider adoption of such datasets.   In response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets. HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage. This tripartite architecture ensures broad design space coverage via design space expansion and supports multiple vendor tools. Users can contribute to each stage with their own HLS designs and synthesis results and extend the framework itself with custom frontends and tool flows. We also include an initial set of built-in designs from common HLS benchmarks curated open-source HLS designs.   We showcase the versatility and multi-functionality of our framework through six case studies: I) Design space sampling; II) Fine-grained parallelism backend speedup; III) Targeting Intel's HLS flow; IV) Adding new auxiliary designs; V) Integrating published HLS data; VI) HLS tool version regression benchmarking.   Code at https://github.com/sharc-lab/HLSFactory.","sentences":["Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE).","Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present challenges.","Existing datasets have limitations in terms of benchmark coverage, design space enumeration, vendor extensibility, or lack of reproducible and extensible software for dataset construction.","Many works also lack user-friendly ways to add more designs, limiting wider adoption of such datasets.   ","In response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets.","HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage.","This tripartite architecture ensures broad design space coverage via design space expansion and supports multiple vendor tools.","Users can contribute to each stage with their own HLS designs and synthesis results and extend the framework itself with custom frontends and tool flows.","We also include an initial set of built-in designs from common HLS benchmarks curated open-source HLS designs.   ","We showcase the versatility and multi-functionality of our framework through six case studies: I) Design space sampling; II) Fine-grained parallelism backend speedup; III)","Targeting Intel's HLS flow; IV) Adding new auxiliary designs; V) Integrating published HLS data; VI) HLS tool version regression benchmarking.   ","Code at https://github.com/sharc-lab/HLSFactory."],"url":"http://arxiv.org/abs/2405.00820v1","category":"cs.AR"}
{"created":"2024-05-01 19:00:30","title":"ICU Bloodstream Infection Prediction: A Transformer-Based Approach for EHR Analysis","abstract":"We introduce RatchetEHR, a novel transformer-based framework designed for the predictive analysis of electronic health records (EHR) data in intensive care unit (ICU) settings, with a specific focus on bloodstream infection (BSI) prediction. Leveraging the MIMIC-IV dataset, RatchetEHR demonstrates superior predictive performance compared to other methods, including RNN, LSTM, and XGBoost, particularly due to its advanced handling of sequential and temporal EHR data. A key innovation in RatchetEHR is the integration of the Graph Convolutional Transformer (GCT) component, which significantly enhances the ability to identify hidden structural relationships within EHR data, resulting in more accurate clinical predictions. Through SHAP value analysis, we provide insights into influential features for BSI prediction. RatchetEHR integrates multiple advancements in deep learning which together provide accurate predictions even with a relatively small sample size and highly imbalanced dataset. This study contributes to medical informatics by showcasing the application of advanced AI techniques in healthcare and sets a foundation for further research to optimize these capabilities in EHR data analysis.","sentences":["We introduce RatchetEHR, a novel transformer-based framework designed for the predictive analysis of electronic health records (EHR) data in intensive care unit (ICU) settings, with a specific focus on bloodstream infection (BSI) prediction.","Leveraging the MIMIC-IV dataset, RatchetEHR demonstrates superior predictive performance compared to other methods, including RNN, LSTM, and XGBoost, particularly due to its advanced handling of sequential and temporal EHR data.","A key innovation in RatchetEHR is the integration of the Graph Convolutional Transformer (GCT) component, which significantly enhances the ability to identify hidden structural relationships within EHR data, resulting in more accurate clinical predictions.","Through SHAP value analysis, we provide insights into influential features for BSI prediction.","RatchetEHR integrates multiple advancements in deep learning which together provide accurate predictions even with a relatively small sample size and highly imbalanced dataset.","This study contributes to medical informatics by showcasing the application of advanced AI techniques in healthcare and sets a foundation for further research to optimize these capabilities in EHR data analysis."],"url":"http://arxiv.org/abs/2405.00819v1","category":"cs.LG"}
{"created":"2024-05-01 18:57:41","title":"Sifting out communities in large sparse networks","abstract":"Research data sets are growing to unprecedented sizes and network modeling is commonly used to extract complex relationships in diverse domains, such as genetic interactions involved in disease, logistics, and social communities. As the number of nodes increases in a network, an increasing sparsity of edges is a practical limitation due to memory restrictions. Moreover, many of these sparse networks exhibit very large numbers of nodes with no adjacent edges, as well as disjoint components of nodes with no edges connecting them. A prevalent aim in network modeling is the identification of clusters, or communities, of nodes that are highly interrelated. Several definitions of strong community structure have been introduced to facilitate this task, each with inherent assumptions and biases. We introduce an intuitive objective function for quantifying the quality of clustering results in large sparse networks. We utilize a two-step method for identifying communities which is especially well-suited for this domain as the first step efficiently divides the network into the disjoint components, while the second step optimizes clustering of the produced components based on the new objective. Using simulated networks, optimization based on the new objective function consistently yields significantly higher accuracy than those based on the modularity function, with the widest gaps appearing for the noisiest networks. Additionally, applications to benchmark problems illustrate the intuitive correctness of our approach. Finally, the practicality of our approach is demonstrated in real-world data in which we identify complex genetic interactions in large-scale networks comprised of tens of thousands of nodes. Based on these three different types of trials, our results clearly demonstrate the usefulness of our two-step procedure and the accuracy of our simple objective.","sentences":["Research data sets are growing to unprecedented sizes and network modeling is commonly used to extract complex relationships in diverse domains, such as genetic interactions involved in disease, logistics, and social communities.","As the number of nodes increases in a network, an increasing sparsity of edges is a practical limitation due to memory restrictions.","Moreover, many of these sparse networks exhibit very large numbers of nodes with no adjacent edges, as well as disjoint components of nodes with no edges connecting them.","A prevalent aim in network modeling is the identification of clusters, or communities, of nodes that are highly interrelated.","Several definitions of strong community structure have been introduced to facilitate this task, each with inherent assumptions and biases.","We introduce an intuitive objective function for quantifying the quality of clustering results in large sparse networks.","We utilize a two-step method for identifying communities which is especially well-suited for this domain as the first step efficiently divides the network into the disjoint components, while the second step optimizes clustering of the produced components based on the new objective.","Using simulated networks, optimization based on the new objective function consistently yields significantly higher accuracy than those based on the modularity function, with the widest gaps appearing for the noisiest networks.","Additionally, applications to benchmark problems illustrate the intuitive correctness of our approach.","Finally, the practicality of our approach is demonstrated in real-world data in which we identify complex genetic interactions in large-scale networks comprised of tens of thousands of nodes.","Based on these three different types of trials, our results clearly demonstrate the usefulness of our two-step procedure and the accuracy of our simple objective."],"url":"http://arxiv.org/abs/2405.00816v1","category":"cs.SI"}
{"created":"2024-05-01 18:51:28","title":"Explosively driven Richtmyer--Meshkov instability jet suppression and enhancement via coupling machine learning and additive manufacturing","abstract":"The ability to control the behavior of fluid instabilities at material interfaces, such as the shock-driven Richtmyer--Meshkov instability, is a grand technological challenge with a broad number of applications ranging from inertial confinement fusion experiments to explosively driven shaped charges. In this work, we use a linear-geometry shaped charge as a means of studying methods for controlling material jetting that results from the Richtmyer--Meshkov instability. A shaped charge produces a high-velocity jet by focusing the energy from the detonation of high explosives. The interaction of the resulting detonation wave with a hollowed cavity lined with a thin metal layer produces the unstable jetting effect. By modifying characteristics of the detonation wave prior to striking the lined cavity, the kinetic energy of the jet can be enhanced or reduced. Modifying the geometry of the liner material can also be used to alter jetting properties. We apply optimization methods to investigate several design parameterizations for both enhancing or suppressing the shaped-charge jet. This is accomplished using 2D and 3D hydrodynamic simulations to investigate the design space that we consider. We also apply new additive manufacturing methods for producing the shaped-charge assemblies, which allow for experimental testing of complicated design geometries obtained through computational optimization. We present a direct comparison of our optimized designs with experimental results carried out at the High Explosives Application Facility at Lawrence Livermore National Laboratory.","sentences":["The ability to control the behavior of fluid instabilities at material interfaces, such as the shock-driven Richtmyer--Meshkov instability, is a grand technological challenge with a broad number of applications ranging from inertial confinement fusion experiments to explosively driven shaped charges.","In this work, we use a linear-geometry shaped charge as a means of studying methods for controlling material jetting that results from the Richtmyer--Meshkov instability.","A shaped charge produces a high-velocity jet by focusing the energy from the detonation of high explosives.","The interaction of the resulting detonation wave with a hollowed cavity lined with a thin metal layer produces the unstable jetting effect.","By modifying characteristics of the detonation wave prior to striking the lined cavity, the kinetic energy of the jet can be enhanced or reduced.","Modifying the geometry of the liner material can also be used to alter jetting properties.","We apply optimization methods to investigate several design parameterizations for both enhancing or suppressing the shaped-charge jet.","This is accomplished using 2D and 3D hydrodynamic simulations to investigate the design space that we consider.","We also apply new additive manufacturing methods for producing the shaped-charge assemblies, which allow for experimental testing of complicated design geometries obtained through computational optimization.","We present a direct comparison of our optimized designs with experimental results carried out at the High Explosives Application Facility at Lawrence Livermore National Laboratory."],"url":"http://arxiv.org/abs/2405.00812v1","category":"physics.app-ph"}
{"created":"2024-05-02 17:31:52","title":"Advancements in Streamlining Time-Domain and Multi-Messenger Astronomy Follow-Up Infrastructure at Keck Observatory","abstract":"With active time-domain surveys like the Zwicky Transient Facility, the anticipated Rubin Observatory's Legacy Survey of Space and Time, and multi-messenger experiments such as LIGO/VIRGO/KANGRA for gravitational wave detection and IceCube for high-energy neutrino events, there is a new era in both time-domain and multi-messenger astronomy. The Astro2020 decadal survey highlights effectively responding to these astronomical alerts in a timely manner as a priority, and thus, there is an urgent need for the development of a seamless follow-up infrastructure at existing facilities that are capable of following up on detections at the survey depths. At the W. M. Keck Observatory (WMKO), we are actively constructing critical infrastructure, aimed at facilitating the Target-of-Opportunity (ToO) trigger, optimizing observational planning, streamlining data acquisition, and enhancing data product accessibility. In this document, we provide an overview of these developing services and place them in context of existing observatory infrastructure like the Keck Observatory Archive (KOA) and Data Services Initiative (DSI).","sentences":["With active time-domain surveys like the Zwicky Transient Facility, the anticipated Rubin Observatory's Legacy Survey of Space and Time, and multi-messenger experiments such as LIGO/VIRGO/KANGRA for gravitational wave detection and IceCube for high-energy neutrino events, there is a new era in both time-domain and multi-messenger astronomy.","The Astro2020 decadal survey highlights effectively responding to these astronomical alerts in a timely manner as a priority, and thus, there is an urgent need for the development of a seamless follow-up infrastructure at existing facilities that are capable of following up on detections at the survey depths.","At the W. M. Keck Observatory (WMKO), we are actively constructing critical infrastructure, aimed at facilitating the Target-of-Opportunity (ToO) trigger, optimizing observational planning, streamlining data acquisition, and enhancing data product accessibility.","In this document, we provide an overview of these developing services and place them in context of existing observatory infrastructure like the Keck Observatory Archive (KOA) and Data Services Initiative (DSI)."],"url":"http://arxiv.org/abs/2405.01500v1","category":"astro-ph.IM"}
{"created":"2024-05-02 16:48:51","title":"Unconditionally Safe Light Client","abstract":"Blockchain applications often rely on lightweight clients to access and verify on-chain data efficiently without the need to run a resource-intensive full node. These light clients must maintain robust security to protect the blockchain's integrity for users of applications built upon it, achieving this with minimal resources and without significant latency. Moreover, different applications have varying security needs. This work focuses on addressing these two key requirements in the context of Proof-of-Stake (PoS) blockchains and identifying the fundamental cost-latency trade-offs to achieve tailored, optimal security for each light client.   The key security guarantee of PoS blockchains is economic (implied by the \"stake\"). In this paper we formalize this cryptoeconomic security to light clients, ensuring that the cost of corrupting the data provided to light clients must outweigh the potential profit, thereby economically deterring malicious actors. We further introduce \"insured\" cryptoeconomic security to light clients, providing unconditional protection via the attribution of adversarial actions and the consequent slashing of stakes. The divisible and fungible nature of stake facilitates programmable security, allowing for customization of the security level and insurance amount according to the specific needs of different applications.   We implemented the protocols in less than 1000 lines of Solidity and TypeScript code and evaluated their gas cost, latency, and the computational overhead. For example, for a transaction with value of \\$32k, the light client can choose between zero cost with a latency of 5 hours or instant confirmation with an insurance cost of \\$7.45. Thus, the client can select the optimal point on the latency-cost trade-off spectrum that best aligns with its needs. Light clients require negligible storage and face minimal computational costs,...","sentences":["Blockchain applications often rely on lightweight clients to access and verify on-chain data efficiently without the need to run a resource-intensive full node.","These light clients must maintain robust security to protect the blockchain's integrity for users of applications built upon it, achieving this with minimal resources and without significant latency.","Moreover, different applications have varying security needs.","This work focuses on addressing these two key requirements in the context of Proof-of-Stake (PoS) blockchains and identifying the fundamental cost-latency trade-offs to achieve tailored, optimal security for each light client.   ","The key security guarantee of PoS blockchains is economic (implied by the \"stake\").","In this paper we formalize this cryptoeconomic security to light clients, ensuring that the cost of corrupting the data provided to light clients must outweigh the potential profit, thereby economically deterring malicious actors.","We further introduce \"insured\" cryptoeconomic security to light clients, providing unconditional protection via the attribution of adversarial actions and the consequent slashing of stakes.","The divisible and fungible nature of stake facilitates programmable security, allowing for customization of the security level and insurance amount according to the specific needs of different applications.   ","We implemented the protocols in less than 1000 lines of Solidity and TypeScript code and evaluated their gas cost, latency, and the computational overhead.","For example, for a transaction with value of \\$32k, the light client can choose between zero cost with a latency of 5 hours or instant confirmation with an insurance cost of \\$7.45.","Thus, the client can select the optimal point on the latency-cost trade-off spectrum that best aligns with its needs.","Light clients require negligible storage and face minimal computational costs,..."],"url":"http://arxiv.org/abs/2405.01459v1","category":"cs.CR"}
{"created":"2024-05-02 16:32:04","title":"Digital-Analog Counterdiabatic Quantum Optimization with Trapped Ions","abstract":"We introduce a hardware-specific, problem-dependent digital-analog quantum algorithm of a counterdiabatic quantum dynamics tailored for optimization problems. Specifically, we focus on trapped-ion architectures, taking advantage from global M{\\o}lmer-S{\\o}rensen gates as the analog interactions complemented by digital gates, both of which are available in the state-of-the-art technologies. We show an optimal configuration of analog blocks and digital steps leading to a substantial reduction in circuit depth compared to the purely digital approach. This implies that, using the proposed encoding, we can address larger optimization problem instances, requiring more qubits, while preserving the coherence time of current devices. Furthermore, we study the minimum gate fidelity required by the analog blocks to outperform the purely digital simulation, finding that it is below the best fidelity reported in the literature. To validate the performance of the digital-analog encoding, we tackle the maximum independent set problem, showing that it requires fewer resources compared to the digital case. This hybrid co-design approach paves the way towards quantum advantage for efficient solutions of quantum optimization problems.","sentences":["We introduce a hardware-specific, problem-dependent digital-analog quantum algorithm of a counterdiabatic quantum dynamics tailored for optimization problems.","Specifically, we focus on trapped-ion architectures, taking advantage from global M{\\o}lmer-S{\\o}rensen gates as the analog interactions complemented by digital gates, both of which are available in the state-of-the-art technologies.","We show an optimal configuration of analog blocks and digital steps leading to a substantial reduction in circuit depth compared to the purely digital approach.","This implies that, using the proposed encoding, we can address larger optimization problem instances, requiring more qubits, while preserving the coherence time of current devices.","Furthermore, we study the minimum gate fidelity required by the analog blocks to outperform the purely digital simulation, finding that it is below the best fidelity reported in the literature.","To validate the performance of the digital-analog encoding, we tackle the maximum independent set problem, showing that it requires fewer resources compared to the digital case.","This hybrid co-design approach paves the way towards quantum advantage for efficient solutions of quantum optimization problems."],"url":"http://arxiv.org/abs/2405.01447v1","category":"quant-ph"}
{"created":"2024-05-02 16:26:22","title":"Solving the train-platforming problem via a two-level Lagrangian Relaxation approach","abstract":"High-speed railway stations are crucial junctions in high-speed railway networks. Compared to operations on the tracks between stations, trains have more routing possibilities within stations. As a result, track allocation at a station is relatively complicated. In this study, we aim to solve the train platforming problem for a busy high-speed railway station by considering comprehensive track resources and interlocking configurations. A two-level space-time network is constructed to capture infrastructure information at various levels of detail from both macroscopic and microscopic perspectives. Additionally, we propose a nonlinear programming model that minimizes a weighted sum of total travel time and total deviation time for trains at the station. We apply a Two-level Lagrangian Relaxation (2-L LR) to a linearized version of the model and demonstrate how this induces a decomposable train-specific path choice problem at the macroscopic level that is guided by Lagrange multipliers associated with microscopic resource capacity violation. As case studies, the proposed model and solution approach are applied to a small virtual railway station and a high-speed railway hub station located on the busiest high-speed railway line in China. Through a comparison of other approaches that include Logic-based Benders Decomposition (LBBD), we highlight the superiority of the proposed method; on realistic instances, the 2-L LR method finds solution that are, on average, approximately 2% from optimality. Finally, we test algorithm performance at the operational level and obtain near-optimal solutions, with optimality gaps of approximately 1%, in a very short time.","sentences":["High-speed railway stations are crucial junctions in high-speed railway networks.","Compared to operations on the tracks between stations, trains have more routing possibilities within stations.","As a result, track allocation at a station is relatively complicated.","In this study, we aim to solve the train platforming problem for a busy high-speed railway station by considering comprehensive track resources and interlocking configurations.","A two-level space-time network is constructed to capture infrastructure information at various levels of detail from both macroscopic and microscopic perspectives.","Additionally, we propose a nonlinear programming model that minimizes a weighted sum of total travel time and total deviation time for trains at the station.","We apply a Two-level Lagrangian Relaxation (2-L LR) to a linearized version of the model and demonstrate how this induces a decomposable train-specific path choice problem at the macroscopic level that is guided by Lagrange multipliers associated with microscopic resource capacity violation.","As case studies, the proposed model and solution approach are applied to a small virtual railway station and a high-speed railway hub station located on the busiest high-speed railway line in China.","Through a comparison of other approaches that include Logic-based Benders Decomposition (LBBD), we highlight the superiority of the proposed method; on realistic instances, the 2-L LR method finds solution that are, on average, approximately 2% from optimality.","Finally, we test algorithm performance at the operational level and obtain near-optimal solutions, with optimality gaps of approximately 1%, in a very short time."],"url":"http://arxiv.org/abs/2405.01438v1","category":"math.OC"}
{"created":"2024-05-02 16:12:23","title":"A Model Problem for First Order Mean Field Games with Discrete Initial Data","abstract":"In this article, we study a simplified version of a density-dependent first-order mean field game, in which the players face a penalization equal to the population density at their final position. We consider the problem of finding an equilibrium when the initial distribution is a discrete measure. We show that the problem becomes finite-dimensional: the final piecewise smooth density is completely determined by the weights and positions of the initial measure. We establish existence and uniqueness of a solution using classical fixed point theorems. Finally, we show that Newton's method provides an effective way to compute the solution. Our numerical simulations provide an illustration of how density penalization in a mean field game tends to the smoothen the initial distribution.","sentences":["In this article, we study a simplified version of a density-dependent first-order mean field game, in which the players face a penalization equal to the population density at their final position.","We consider the problem of finding an equilibrium when the initial distribution is a discrete measure.","We show that the problem becomes finite-dimensional: the final piecewise smooth density is completely determined by the weights and positions of the initial measure.","We establish existence and uniqueness of a solution using classical fixed point theorems.","Finally, we show that Newton's method provides an effective way to compute the solution.","Our numerical simulations provide an illustration of how density penalization in a mean field game tends to the smoothen the initial distribution."],"url":"http://arxiv.org/abs/2405.01424v1","category":"math.OC"}
{"created":"2024-05-02 15:03:41","title":"Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance","abstract":"In subject-driven text-to-image synthesis, the synthesis process tends to be heavily influenced by the reference images provided by users, often overlooking crucial attributes detailed in the text prompt. In this work, we propose Subject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the problem. We show that through constructing a subject-agnostic condition and applying our proposed dual classifier-free guidance, one could obtain outputs consistent with both the given subject and input text prompts. We validate the efficacy of our approach through both optimization-based and encoder-based methods. Additionally, we demonstrate its applicability in second-order customization methods, where an encoder-based model is fine-tuned with DreamBooth. Our approach is conceptually simple and requires only minimal code modifications, but leads to substantial quality improvements, as evidenced by our evaluations and user studies.","sentences":["In subject-driven text-to-image synthesis, the synthesis process tends to be heavily influenced by the reference images provided by users, often overlooking crucial attributes detailed in the text prompt.","In this work, we propose Subject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the problem.","We show that through constructing a subject-agnostic condition and applying our proposed dual classifier-free guidance, one could obtain outputs consistent with both the given subject and input text prompts.","We validate the efficacy of our approach through both optimization-based and encoder-based methods.","Additionally, we demonstrate its applicability in second-order customization methods, where an encoder-based model is fine-tuned with DreamBooth.","Our approach is conceptually simple and requires only minimal code modifications, but leads to substantial quality improvements, as evidenced by our evaluations and user studies."],"url":"http://arxiv.org/abs/2405.01356v1","category":"cs.CV"}
{"created":"2024-05-02 11:07:36","title":"Robust Algorithms for Finding Triangles and Computing the Girth in Unit Disk and Transmission Graphs","abstract":"We describe optimal robust algorithms for finding a triangle and the unweighted girth in a unit disk graph, as well as finding a triangle in a transmission graph.In the robust setting, the input is not given as a set of sites in the plane, but rather as an abstract graph. The input may or may not be realizable as a unit disk graph or a transmission graph. If the graph is realizable, the algorithm is guaranteed to give the correct answer. If not, the algorithm will either give a correct answer or correctly state that the input is not of the required type.","sentences":["We describe optimal robust algorithms for finding a triangle and the unweighted girth in a unit disk graph, as well as finding a triangle in a transmission graph.","In the robust setting, the input is not given as a set of sites in the plane, but rather as an abstract graph.","The input may or may not be realizable as a unit disk graph or a transmission graph.","If the graph is realizable, the algorithm is guaranteed to give the correct answer.","If not, the algorithm will either give a correct answer or correctly state that the input is not of the required type."],"url":"http://arxiv.org/abs/2405.01180v1","category":"cs.CG"}
{"created":"2024-05-02 09:59:35","title":"Are We Really Achieving Better Beyond-Accuracy Performance in Next Basket Recommendation?","abstract":"Next basket recommendation (NBR) is a special type of sequential recommendation that is increasingly receiving attention. So far, most NBR studies have focused on optimizing the accuracy of the recommendation, whereas optimizing for beyond-accuracy metrics, e.g., item fairness and diversity remains largely unexplored. Recent studies into NBR have found a substantial performance difference between recommending repeat items and explore items. Repeat items contribute most of the users' perceived accuracy compared with explore items. Informed by these findings, we identify a potential \"short-cut\" to optimize for beyond-accuracy metrics while maintaining high accuracy. To leverage and verify the existence of such short-cuts, we propose a plug-and-play two-step repetition-exploration (TREx) framework that treats repeat items and explores items separately, where we design a simple yet highly effective repetition module to ensure high accuracy, while two exploration modules target optimizing only beyond-accuracy metrics. Experiments are performed on two widely-used datasets w.r.t. a range of beyond-accuracy metrics, viz. five fairness metrics and three diversity metrics. Our experimental results verify the effectiveness of TREx. Prima facie, this appears to be good news: we can achieve high accuracy and improved beyond-accuracy metrics at the same time. However, we argue that the real-world value of our algorithmic solution, TREx, is likely to be limited and reflect on the reasonableness of the evaluation setup. We end up challenging existing evaluation paradigms, particularly in the context of beyond-accuracy metrics, and provide insights for researchers to navigate potential pitfalls and determine reasonable metrics to consider when optimizing for accuracy and beyond-accuracy metrics.","sentences":["Next basket recommendation (NBR) is a special type of sequential recommendation that is increasingly receiving attention.","So far, most NBR studies have focused on optimizing the accuracy of the recommendation, whereas optimizing for beyond-accuracy metrics, e.g., item fairness and diversity remains largely unexplored.","Recent studies into NBR have found a substantial performance difference between recommending repeat items and explore items.","Repeat items contribute most of the users' perceived accuracy compared with explore items.","Informed by these findings, we identify a potential \"short-cut\" to optimize for beyond-accuracy metrics while maintaining high accuracy.","To leverage and verify the existence of such short-cuts, we propose a plug-and-play two-step repetition-exploration (TREx) framework that treats repeat items and explores items separately, where we design a simple yet highly effective repetition module to ensure high accuracy, while two exploration modules target optimizing only beyond-accuracy metrics.","Experiments are performed on two widely-used datasets w.r.t.","a range of beyond-accuracy metrics, viz.","five fairness metrics and three diversity metrics.","Our experimental results verify the effectiveness of TREx.","Prima facie, this appears to be good news: we can achieve high accuracy and improved beyond-accuracy metrics at the same time.","However, we argue that the real-world value of our algorithmic solution, TREx, is likely to be limited and reflect on the reasonableness of the evaluation setup.","We end up challenging existing evaluation paradigms, particularly in the context of beyond-accuracy metrics, and provide insights for researchers to navigate potential pitfalls and determine reasonable metrics to consider when optimizing for accuracy and beyond-accuracy metrics."],"url":"http://arxiv.org/abs/2405.01143v1","category":"cs.IR"}
{"created":"2024-05-02 09:42:30","title":"Backward Map for Filter Stability Analysis","abstract":"In this paper, a backward map is introduced for the purposes of analysis of the nonlinear (stochastic) filter stability. The backward map is important because the filter-stability in the sense of $\\chisq$-divergence follows from showing a certain variance decay property for the backward map. To show this property requires additional assumptions on the model properties of the hidden Markov model (HMM). The analysis in this paper is based on introducing a Poincar\\'e Inequality (PI) for HMMs with white noise observations. In finite state-space settings, PI is related to both the ergodicity of the Markov process as well as the observability of the HMM. It is shown that the Poincar\\'e constant is positive if and only if the HMM is detectable.","sentences":["In this paper, a backward map is introduced for the purposes of analysis of the nonlinear (stochastic) filter stability.","The backward map is important because the filter-stability in the sense of $\\chisq$-divergence follows from showing a certain variance decay property for the backward map.","To show this property requires additional assumptions on the model properties of the hidden Markov model (HMM).","The analysis in this paper is based on introducing a Poincar\\'e Inequality (PI) for HMMs with white noise observations.","In finite state-space settings, PI is related to both the ergodicity of the Markov process as well as the observability of the HMM.","It is shown that the Poincar\\'e constant is positive if and only if the HMM is detectable."],"url":"http://arxiv.org/abs/2405.01127v1","category":"math.PR"}
{"created":"2024-05-02 09:37:32","title":"On some global implicit function theorems for set-valued inclusions with applications to parametric vector optimization","abstract":"The present paper deals with the perturbation analysis of set-valued inclusion problems, a problem format whose relevance has recently emerged in such contexts as robust and vector optimization as well as in vector equilibrium theory. The set-valued inclusions here considered are parameterized by variables belonging to a topological space, with and without constraints. By proper techniques of variational analysis, some qualitative global implicit function theorems are established, which ensure global solvability of these problems and continuous dependence on the parameter of the related solutions. Applications to parametric vector optimization are discussed, aimed at deriving sufficient conditions for the existence of ideal efficient solutions that depend continuously on the parameter perturbations.","sentences":["The present paper deals with the perturbation analysis of set-valued inclusion problems, a problem format whose relevance has recently emerged in such contexts as robust and vector optimization as well as in vector equilibrium theory.","The set-valued inclusions here considered are parameterized by variables belonging to a topological space, with and without constraints.","By proper techniques of variational analysis, some qualitative global implicit function theorems are established, which ensure global solvability of these problems and continuous dependence on the parameter of the related solutions.","Applications to parametric vector optimization are discussed, aimed at deriving sufficient conditions for the existence of ideal efficient solutions that depend continuously on the parameter perturbations."],"url":"http://arxiv.org/abs/2405.01123v1","category":"math.OC"}
{"created":"2024-05-02 07:34:01","title":"An eco-friendly passivation strategy of resveratrol for highly efficient and antioxidative perovskite solar cells","abstract":"The stability of perovskite solar cells is closely related to the defects in perovskite crystals, and there are a large number of crystal defects in the perovskite thin films prepared by the solution method, which is not conducive to the commercial production of PSCs. In this study, resveratrol(RES), a green natural antioxidant abundant in knotweed and grape leaves, was introduced into perovskite films to passivate the defect. RES achieves defect passivation by interacting with uncoordinated Pb2+ in perovskite films. The results show that the quality of the perovskite film is significantly improved, and the energy level structure of the device is optimized, and the power conversion efficiency of the device is increased from 21.62% to 23.44%. In addition, RES can hinder the degradation of perovskite structures by O2- and CO2- free radicals, and the device retained 88% of its initial PCE after over 1000 hours in pure oxygen environment. The device retains 91% of the initial PCE after more than 1000 hours at 25{\\deg}C and 50+5% relative humidity. This work provides a strategy for the use of natural and environmentally friendly additives to improve the efficiency and stability of devices, and provides an idea for the development of efficient, stable and environmentally friendly PSCs.","sentences":["The stability of perovskite solar cells is closely related to the defects in perovskite crystals, and there are a large number of crystal defects in the perovskite thin films prepared by the solution method, which is not conducive to the commercial production of PSCs.","In this study, resveratrol(RES), a green natural antioxidant abundant in knotweed and grape leaves, was introduced into perovskite films to passivate the defect.","RES achieves defect passivation by interacting with uncoordinated Pb2+ in perovskite films.","The results show that the quality of the perovskite film is significantly improved, and the energy level structure of the device is optimized, and the power conversion efficiency of the device is increased from 21.62% to 23.44%.","In addition, RES can hinder the degradation of perovskite structures by O2- and CO2- free radicals, and the device retained 88% of its initial PCE after over 1000 hours in pure oxygen environment.","The device retains 91% of the initial PCE after more than 1000 hours at 25{\\deg}C and 50+5% relative humidity.","This work provides a strategy for the use of natural and environmentally friendly additives to improve the efficiency and stability of devices, and provides an idea for the development of efficient, stable and environmentally friendly PSCs."],"url":"http://arxiv.org/abs/2405.01058v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 04:14:43","title":"Assignment of charmed-strange $D_{s0}(2590)^+$ and $D_{sJ}(3040)^+$","abstract":"Based on analyses of the mass and the strong decay features, $D_{s0}(2590)^+$ observed by LHCb collaboration is identified as a radial excitation of the pseudoscalar $D_s$, and $D_{sJ}(3040)^+$ observed by BaBar collaboration is identified as a radial excitation of $D_{s1}(2536)^\\pm$. $D_{s0}(2590)^+$ is possibly a pure $D_{s}(2~^1S_0)$ meson, both basic $D_{s1}(2536)^\\pm$ and radially excited $D_{sJ}(3040)^+$ are possibly the mixtures $D_s(nP_1)$ between spin triplet $D_s(n~^3P_1)$ and spin singlet $D_s(n~^1P_1)$. In this arrangement, their masses meet the linear behavior of the radial Regge trajectory very well. In the $^3P_0$ strong decay model, the decay channels of $D_{s0}(2590)^+$ are $D^{*0}K^+$ and $D^{*+}K^0$, the total decay width is predicted with $\\Gamma=76.12$ MeV. The main decay channels of $D_{sJ}(3040)^+$ are $D^{*0}K^+$/$D^{*+}K^0$ and $D^{*0}K^{*+}$/$D^{*+}K^{*0}$, the total decay width is predicted with $\\Gamma=283.46$ MeV. These numerical strong decay results are consistent with the experiment data and support our arrangement. The dimensionless strength creation parameter $\\gamma$ plays an important role in the calculation, and $\\gamma=9.57$ is fixed through a comparison of the predicted strong decay widths of $D^*_{s2}(2573)$ and $D^*_{s3}(2860)^{\\pm}$ with experimental data.","sentences":["Based on analyses of the mass and the strong decay features, $D_{s0}(2590)^+$ observed by LHCb collaboration is identified as a radial excitation of the pseudoscalar $D_s$, and $D_{sJ}(3040)^+$ observed by BaBar collaboration is identified as a radial excitation of $D_{s1}(2536)^\\pm$. $D_{s0}(2590)^+$ is possibly a pure $D_{s}(2~^1S_0)$ meson, both basic $D_{s1}(2536)^\\pm$ and radially excited $D_{sJ}(3040)^+$ are possibly the mixtures $D_s(nP_1)$ between spin triplet $D_s(n~^3P_1)$ and spin singlet $D_s(n~^1P_1)$.","In this arrangement, their masses meet the linear behavior of the radial Regge trajectory very well.","In the $^3P_0$ strong decay model, the decay channels of $D_{s0}(2590)^+$ are $D^{*0}K^+$ and $D^{*+}K^0$, the total decay width is predicted with $\\Gamma=76.12$ MeV.","The main decay channels of $D_{sJ}(3040)^+$ are $D^{*0}K^+$/$D^{*+}K^0$ and $D^{*0}K^{*+}$/$D^{*+}K^{*0}$, the total decay width is predicted with $\\Gamma=283.46$ MeV. These numerical strong decay results are consistent with the experiment data and support our arrangement.","The dimensionless strength creation parameter $\\gamma$ plays an important role in the calculation, and $\\gamma=9.57$ is fixed through a comparison of the predicted strong decay widths of $D^*_{s2}(2573)$ and $D^*_{s3}(2860)^{\\pm}$ with experimental data."],"url":"http://arxiv.org/abs/2405.00992v1","category":"hep-ph"}
{"created":"2024-05-02 02:30:26","title":"Asymptotic Properties of the Distributional Synthetic Controls","abstract":"This paper enhances our comprehension of the Distributional Synthetic Control (DSC) proposed by Gunsilius (2023), focusing on its asymptotic properties. We first establish the DSC estimator's asymptotic optimality. The essence of this optimality lies in the treatment effect estimator given by DSC achieves the lowest possible squared prediction error among all potential treatment effect estimators that depend on an average of quantiles of control units. We also establish the convergence of the DSC weights when some requirements are met, as well as the convergence rate. A significant aspect of our research is that we find DSC synthesis forms an optimal weighted average, particularly in situations where it is impractical to perfectly fit the treated unit's quantiles through the weighted average of the control units' quantiles. To corroborate our theoretical insights, we provide empirical evidence derived from simulations.","sentences":["This paper enhances our comprehension of the Distributional Synthetic Control (DSC) proposed by Gunsilius (2023), focusing on its asymptotic properties.","We first establish the DSC estimator's asymptotic optimality.","The essence of this optimality lies in the treatment effect estimator given by DSC achieves the lowest possible squared prediction error among all potential treatment effect estimators that depend on an average of quantiles of control units.","We also establish the convergence of the DSC weights when some requirements are met, as well as the convergence rate.","A significant aspect of our research is that we find DSC synthesis forms an optimal weighted average, particularly in situations where it is impractical to perfectly fit the treated unit's quantiles through the weighted average of the control units' quantiles.","To corroborate our theoretical insights, we provide empirical evidence derived from simulations."],"url":"http://arxiv.org/abs/2405.00953v1","category":"econ.EM"}
{"created":"2024-05-02 02:23:38","title":"Hyperspectral Band Selection based on Generalized 3DTV and Tensor CUR Decomposition","abstract":"Hyperspectral Imaging (HSI) serves as an important technique in remote sensing. However, high dimensionality and data volume typically pose significant computational challenges. Band selection is essential for reducing spectral redundancy in hyperspectral imagery while retaining intrinsic critical information. In this work, we propose a novel hyperspectral band selection model by decomposing the data into a low-rank and smooth component and a sparse one. In particular, we develop a generalized 3D total variation (G3DTV) by applying the $\\ell_1^p$-norm to derivatives to preserve spatial-spectral smoothness. By employing the alternating direction method of multipliers (ADMM), we derive an efficient algorithm, where the tensor low-rankness is implied by the tensor CUR decomposition. We demonstrate the effectiveness of the proposed approach through comparisons with various other state-of-the-art band selection techniques using two benchmark real-world datasets. In addition, we provide practical guidelines for parameter selection in both noise-free and noisy scenarios.","sentences":["Hyperspectral Imaging (HSI) serves as an important technique in remote sensing.","However, high dimensionality and data volume typically pose significant computational challenges.","Band selection is essential for reducing spectral redundancy in hyperspectral imagery while retaining intrinsic critical information.","In this work, we propose a novel hyperspectral band selection model by decomposing the data into a low-rank and smooth component and a sparse one.","In particular, we develop a generalized 3D total variation (G3DTV) by applying the $\\ell_1^p$-norm to derivatives to preserve spatial-spectral smoothness.","By employing the alternating direction method of multipliers (ADMM), we derive an efficient algorithm, where the tensor low-rankness is implied by the tensor CUR decomposition.","We demonstrate the effectiveness of the proposed approach through comparisons with various other state-of-the-art band selection techniques using two benchmark real-world datasets.","In addition, we provide practical guidelines for parameter selection in both noise-free and noisy scenarios."],"url":"http://arxiv.org/abs/2405.00951v1","category":"cs.CV"}
{"created":"2024-05-01 20:51:12","title":"SHARE: Secure Hardware Allocation and Resource Efficiency in Quantum Systems","abstract":"Quantum computing (QC) is poised to revolutionize problem solving across various fields, with research suggesting that systems with over 50 qubits may achieve quantum advantage surpassing supercomputers in certain optimization tasks. As the hardware size of Noisy Intermediate Scale Quantum (NISQ) computers continues to grow, Multi tenant computing (MTC) has emerged as a viable approach to enhance hardware utilization by allowing shared resource access across multiple quantum programs. However, MTC can also bring challenges and security concerns. This paper focuses on optimizing quantum hardware utilization in shared environments by implementing multi programming strategies that not only enhance hardware utilization but also effectively manage associated risks like crosstalk and fault injection. We propose a novel partitioning and allocation method called Community Based Dynamic Allocation Partitioning (COMDAP) and Secure COMDAP to refine and secure multi programming capabilities in quantum systems. COMDAP ensures equitable and efficient resource distribution, addresses the issues of suboptimal partitioning, and significantly improves hardware utilization. We report a 23 percent average improvement in hardware utilization rate compared to existing greedy heuristics, with rates averaging 92 percent. COMDAP introduces an average increase of approximately 0.05X in delta CX, alongside a 3.5 percent average reduction in PST across benchmarks.","sentences":["Quantum computing (QC) is poised to revolutionize problem solving across various fields, with research suggesting that systems with over 50 qubits may achieve quantum advantage surpassing supercomputers in certain optimization tasks.","As the hardware size of Noisy Intermediate Scale Quantum (NISQ) computers continues to grow, Multi tenant computing (MTC) has emerged as a viable approach to enhance hardware utilization by allowing shared resource access across multiple quantum programs.","However, MTC can also bring challenges and security concerns.","This paper focuses on optimizing quantum hardware utilization in shared environments by implementing multi programming strategies that not only enhance hardware utilization but also effectively manage associated risks like crosstalk and fault injection.","We propose a novel partitioning and allocation method called Community Based Dynamic Allocation Partitioning (COMDAP) and Secure COMDAP to refine and secure multi programming capabilities in quantum systems.","COMDAP ensures equitable and efficient resource distribution, addresses the issues of suboptimal partitioning, and significantly improves hardware utilization.","We report a 23 percent average improvement in hardware utilization rate compared to existing greedy heuristics, with rates averaging 92 percent.","COMDAP introduces an average increase of approximately 0.05X in delta CX, alongside a 3.5 percent average reduction in PST across benchmarks."],"url":"http://arxiv.org/abs/2405.00863v1","category":"quant-ph"}
{"created":"2024-05-01 20:49:50","title":"Scaling Up the Quantum Divide and Conquer Algorithm for Combinatorial Optimization","abstract":"Quantum optimization as a field has largely been restricted by the constraints of current quantum computing hardware, as limitations on size, performance, and fidelity mean most non-trivial problem instances won't fit on quantum devices. Even proposed solutions such as distributed quantum computing systems may struggle to achieve scale due to the high cost of inter-device communication. To address these concerns, we propose Deferred Constraint Quantum Divide and Conquer Algorithm (DC-QDCA), a method for constructing quantum circuits which greatly reduces inter-device communication costs for some quantum graph optimization algorithms. This is achieved by identifying a set of vertices whose removal partitions the input graph, known as a separator; by manipulating the placement of constraints associated with the vertices in the separator, we can greatly simplify the topology of the optimization circuit, reducing the number of required inter-device operations. Furthermore, we introduce an iterative algorithm which builds on these techniques to find solutions for problems with potentially thousands of variables. Our experimental results using quantum simulators have shown that we can construct tractable circuits nearly three times the size of previous QDCA methods while retaining a similar or greater level of quality.","sentences":["Quantum optimization as a field has largely been restricted by the constraints of current quantum computing hardware, as limitations on size, performance, and fidelity mean most non-trivial problem instances won't fit on quantum devices.","Even proposed solutions such as distributed quantum computing systems may struggle to achieve scale due to the high cost of inter-device communication.","To address these concerns, we propose Deferred Constraint Quantum Divide and Conquer Algorithm (DC-QDCA), a method for constructing quantum circuits which greatly reduces inter-device communication costs for some quantum graph optimization algorithms.","This is achieved by identifying a set of vertices whose removal partitions the input graph, known as a separator; by manipulating the placement of constraints associated with the vertices in the separator, we can greatly simplify the topology of the optimization circuit, reducing the number of required inter-device operations.","Furthermore, we introduce an iterative algorithm which builds on these techniques to find solutions for problems with potentially thousands of variables.","Our experimental results using quantum simulators have shown that we can construct tractable circuits nearly three times the size of previous QDCA methods while retaining a similar or greater level of quality."],"url":"http://arxiv.org/abs/2405.00861v1","category":"quant-ph"}
{"created":"2024-05-01 20:25:36","title":"Entanglement Routing using Quantum Error Correction for Distillation","abstract":"Bell-state measurement (BSM) on entangled states shared between quantum repeaters is the fundamental operation used to route entanglement in quantum networks. Performing BSMs on Werner states shared between repeaters leads to exponential decay in the fidelity of the end-to-end Werner state with the number of repeaters, necessitating entanglement distillation. Generally, entanglement routing protocols use \\emph{probabilistic} distillation techniques based on local operations and classical communication. In this work, we use quantum error correcting codes (QECCs) for \\emph{deterministic} entanglement distillation to route Werner states on a chain of repeaters. To maximize the end-to-end distillable entanglement, which depends on the number and fidelity of end-to-end Bell pairs, we utilize global link-state knowledge to determine the optimal policy for scheduling distillation and BSMs at the repeaters. We analyze the effect of the QECC's properties on the entanglement rate and the number of quantum memories. We observe that low-rate codes produce high-fidelity end-to-end states owing to their excellent error-correcting capability, whereas high-rate codes yield a larger number of end-to-end states but of lower fidelity. The number of quantum memories used at repeaters increases with the code rate as well as the classical computation time of the QECC's decoder.","sentences":["Bell-state measurement (BSM) on entangled states shared between quantum repeaters is the fundamental operation used to route entanglement in quantum networks.","Performing BSMs on Werner states shared between repeaters leads to exponential decay in the fidelity of the end-to-end Werner state with the number of repeaters, necessitating entanglement distillation.","Generally, entanglement routing protocols use \\emph{probabilistic} distillation techniques based on local operations and classical communication.","In this work, we use quantum error correcting codes (QECCs) for \\emph{deterministic} entanglement distillation to route Werner states on a chain of repeaters.","To maximize the end-to-end distillable entanglement, which depends on the number and fidelity of end-to-end Bell pairs, we utilize global link-state knowledge to determine the optimal policy for scheduling distillation and BSMs at the repeaters.","We analyze the effect of the QECC's properties on the entanglement rate and the number of quantum memories.","We observe that low-rate codes produce high-fidelity end-to-end states owing to their excellent error-correcting capability, whereas high-rate codes yield a larger number of end-to-end states but of lower fidelity.","The number of quantum memories used at repeaters increases with the code rate as well as the classical computation time of the QECC's decoder."],"url":"http://arxiv.org/abs/2405.00849v1","category":"quant-ph"}
{"created":"2024-05-01 19:59:45","title":"Cross-modality Matching and Prediction of Perturbation Responses with Labeled Gromov-Wasserstein Optimal Transport","abstract":"It is now possible to conduct large scale perturbation screens with complex readout modalities, such as different molecular profiles or high content cell images. While these open the way for systematic dissection of causal cell circuits, integrated such data across screens to maximize our ability to predict circuits poses substantial computational challenges, which have not been addressed. Here, we extend two Gromov-Wasserstein Optimal Transport methods to incorporate the perturbation label for cross-modality alignment. The obtained alignment is then employed to train a predictive model that estimates cellular responses to perturbations observed with only one measurement modality. We validate our method for the tasks of cross-modality alignment and cross-modality prediction in a recent multi-modal single-cell perturbation dataset. Our approach opens the way to unified causal models of cell biology.","sentences":["It is now possible to conduct large scale perturbation screens with complex readout modalities, such as different molecular profiles or high content cell images.","While these open the way for systematic dissection of causal cell circuits, integrated such data across screens to maximize our ability to predict circuits poses substantial computational challenges, which have not been addressed.","Here, we extend two Gromov-Wasserstein Optimal Transport methods to incorporate the perturbation label for cross-modality alignment.","The obtained alignment is then employed to train a predictive model that estimates cellular responses to perturbations observed with only one measurement modality.","We validate our method for the tasks of cross-modality alignment and cross-modality prediction in a recent multi-modal single-cell perturbation dataset.","Our approach opens the way to unified causal models of cell biology."],"url":"http://arxiv.org/abs/2405.00838v1","category":"q-bio.GN"}
{"created":"2024-05-01 18:44:29","title":"ReeSPOT: Reeb Graph Models Semantic Patterns of Normalcy in Human Trajectories","abstract":"This paper introduces ReeSPOT, a novel Reeb graph-based method to model patterns of life in human trajectories (akin to a fingerprint). Human behavior typically follows a pattern of normalcy in day-to-day activities. This is marked by recurring activities within specific time periods. In this paper, we model this behavior using Reeb graphs where any deviation from usual day-to-day activities is encoded as nodes in the Reeb graph. The complexity of the proposed algorithm is linear with respect to the number of time points in a given trajectory. We demonstrate the usage of ReeSPOT and how it captures the critically significant spatial and temporal deviations using the nodes of the Reeb graph. Our case study presented in this paper includes realistic human movement scenarios: visiting uncommon locations, taking odd routes at infrequent times, uncommon time visits, and uncommon stay durations. We analyze the Reeb graph to interpret the topological structure of the GPS trajectories. Potential applications of ReeSPOT include urban planning, security surveillance, and behavioral research.","sentences":["This paper introduces ReeSPOT, a novel Reeb graph-based method to model patterns of life in human trajectories (akin to a fingerprint).","Human behavior typically follows a pattern of normalcy in day-to-day activities.","This is marked by recurring activities within specific time periods.","In this paper, we model this behavior using Reeb graphs where any deviation from usual day-to-day activities is encoded as nodes in the Reeb graph.","The complexity of the proposed algorithm is linear with respect to the number of time points in a given trajectory.","We demonstrate the usage of ReeSPOT and how it captures the critically significant spatial and temporal deviations using the nodes of the Reeb graph.","Our case study presented in this paper includes realistic human movement scenarios: visiting uncommon locations, taking odd routes at infrequent times, uncommon time visits, and uncommon stay durations.","We analyze the Reeb graph to interpret the topological structure of the GPS trajectories.","Potential applications of ReeSPOT include urban planning, security surveillance, and behavioral research."],"url":"http://arxiv.org/abs/2405.00808v1","category":"cs.DS"}
{"created":"2024-05-01 18:44:03","title":"Nearly Optimal List Labeling","abstract":"The list-labeling problem captures the basic task of storing a dynamically changing set of up to $n$ elements in sorted order in an array of size $m = (1 + \\Theta(1))n$. The goal is to support insertions and deletions while moving around elements within the array as little as possible.   Until recently, the best known upper bound stood at $O(\\log^2 n)$ amortized cost. This bound, which was first established in 1981, was finally improved two years ago, when a randomized $O(\\log^{3/2} n)$ expected-cost algorithm was discovered. The best randomized lower bound for this problem remains $\\Omega(\\log n)$, and closing this gap is considered to be a major open problem in data structures.   In this paper, we present the See-Saw Algorithm, a randomized list-labeling solution that achieves a nearly optimal bound of $O(\\log n \\operatorname{polyloglog} n)$ amortized expected cost. This bound is achieved despite at least three lower bounds showing that this type of result is impossible for large classes of solutions.","sentences":["The list-labeling problem captures the basic task of storing a dynamically changing set of up to $n$ elements in sorted order in an array of size $m =","(1 + \\Theta(1))n$. The goal is to support insertions and deletions while moving around elements within the array as little as possible.   ","Until recently, the best known upper bound stood at $O(\\log^2 n)$ amortized cost.","This bound, which was first established in 1981, was finally improved two years ago, when a randomized $O(\\log^{3/2} n)$ expected-cost algorithm was discovered.","The best randomized lower bound for this problem remains $\\Omega(\\log n)$, and closing this gap is considered to be a major open problem in data structures.   ","In this paper, we present the See-Saw Algorithm, a randomized list-labeling solution that achieves a nearly optimal bound of $O(\\log n \\operatorname{polyloglog} n)$ amortized expected cost.","This bound is achieved despite at least three lower bounds showing that this type of result is impossible for large classes of solutions."],"url":"http://arxiv.org/abs/2405.00807v1","category":"cs.DS"}
{"created":"2024-05-01 18:00:18","title":"Rigged Dynamic Mode Decomposition: Data-Driven Generalized Eigenfunction Decompositions for Koopman Operators","abstract":"We introduce the Rigged Dynamic Mode Decomposition (Rigged DMD) algorithm, which computes generalized eigenfunction decompositions of Koopman operators. By considering the evolution of observables, Koopman operators transform complex nonlinear dynamics into a linear framework suitable for spectral analysis. While powerful, traditional Dynamic Mode Decomposition (DMD) techniques often struggle with continuous spectra. Rigged DMD addresses these challenges with a data-driven methodology that approximates the Koopman operator's resolvent and its generalized eigenfunctions using snapshot data from the system's evolution. At its core, Rigged DMD builds wave-packet approximations for generalized Koopman eigenfunctions and modes by integrating Measure-Preserving Extended Dynamic Mode Decomposition with high-order kernels for smoothing. This provides a robust decomposition encompassing both discrete and continuous spectral elements. We derive explicit high-order convergence theorems for generalized eigenfunctions and spectral measures. Additionally, we propose a novel framework for constructing rigged Hilbert spaces using time-delay embedding, significantly extending the algorithm's applicability. We provide examples, including systems with a Lebesgue spectrum, integrable Hamiltonian systems, the Lorenz system, and a high-Reynolds number lid-driven flow in a two-dimensional square cavity, demonstrating Rigged DMD's convergence, efficiency, and versatility. This work paves the way for future research and applications of decompositions with continuous spectra.","sentences":["We introduce the Rigged Dynamic Mode Decomposition (Rigged DMD) algorithm, which computes generalized eigenfunction decompositions of Koopman operators.","By considering the evolution of observables, Koopman operators transform complex nonlinear dynamics into a linear framework suitable for spectral analysis.","While powerful, traditional Dynamic Mode Decomposition (DMD) techniques often struggle with continuous spectra.","Rigged DMD addresses these challenges with a data-driven methodology that approximates the Koopman operator's resolvent and its generalized eigenfunctions using snapshot data from the system's evolution.","At its core, Rigged DMD builds wave-packet approximations for generalized Koopman eigenfunctions and modes by integrating Measure-Preserving Extended Dynamic Mode Decomposition with high-order kernels for smoothing.","This provides a robust decomposition encompassing both discrete and continuous spectral elements.","We derive explicit high-order convergence theorems for generalized eigenfunctions and spectral measures.","Additionally, we propose a novel framework for constructing rigged Hilbert spaces using time-delay embedding, significantly extending the algorithm's applicability.","We provide examples, including systems with a Lebesgue spectrum, integrable Hamiltonian systems, the Lorenz system, and a high-Reynolds number lid-driven flow in a two-dimensional square cavity, demonstrating Rigged DMD's convergence, efficiency, and versatility.","This work paves the way for future research and applications of decompositions with continuous spectra."],"url":"http://arxiv.org/abs/2405.00782v1","category":"math.DS"}
{"created":"2024-05-01 18:00:10","title":"A Review of Barren Plateaus in Variational Quantum Computing","abstract":"Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm -- choices of ansatz, initial state, observable, loss function and hardware noise -- can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.","sentences":["Variational quantum computing offers a flexible computational paradigm with applications in diverse areas.","However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon.","When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases.","Importantly, all the moving pieces of an algorithm -- choices of ansatz, initial state, observable, loss function and hardware noise -- can lead to BPs when ill-suited.","Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects.","As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory.","This article provides a comprehensive review of the current understanding of the BP phenomenon."],"url":"http://arxiv.org/abs/2405.00781v1","category":"quant-ph"}
{"created":"2024-05-02 17:45:35","title":"Geometric Quantization Without Polarizations","abstract":"We expound upon our (polarization-free) definition of the quantization map in geometric quantization, which is justified using the Poisson sigma model and pieces together most known quantization schemes. We use it to obtain the noncommutative torus and a finite dimensional irreducible representation. We discuss invariance of polarization using Schur's lemma.","sentences":["We expound upon our (polarization-free) definition of the quantization map in geometric quantization, which is justified using the Poisson sigma model and pieces together most known quantization schemes.","We use it to obtain the noncommutative torus and a finite dimensional irreducible representation.","We discuss invariance of polarization using Schur's lemma."],"url":"http://arxiv.org/abs/2405.01513v1","category":"math.SG"}
{"created":"2024-05-02 17:36:47","title":"Chirotactic response of microswimmers in fluids with odd viscosity","abstract":"Odd viscosity is a property of chiral active fluids with broken time-reversal and parity symmetries. We show that the flow of such a fluid around a rotating axisymmetric body is exactly solvable and use this solution to determine the orientational dynamics of surface-driven microswimmers. Swimmers with a force-dipole moment exhibit precession around the axis of the odd viscosity. In addition, pushers show bimodal chirotaxis, i.e., alignment parallel or antiparallel to the axis, while pullers orbit in a plane perpendicular to it. A chiral swimmer that itself has a broken parity symmetry can exhibit unimodal chirotaxis and always align in the same direction.","sentences":["Odd viscosity is a property of chiral active fluids with broken time-reversal and parity symmetries.","We show that the flow of such a fluid around a rotating axisymmetric body is exactly solvable and use this solution to determine the orientational dynamics of surface-driven microswimmers.","Swimmers with a force-dipole moment exhibit precession around the axis of the odd viscosity.","In addition, pushers show bimodal chirotaxis, i.e., alignment parallel or antiparallel to the axis, while pullers orbit in a plane perpendicular to it.","A chiral swimmer that itself has a broken parity symmetry can exhibit unimodal chirotaxis and always align in the same direction."],"url":"http://arxiv.org/abs/2405.01506v1","category":"cond-mat.soft"}
{"created":"2024-05-02 16:35:43","title":"Revisiting the Concordance $\u039b$CDM model using Gamma-Ray Bursts together with Supernovae Ia and Planck data","abstract":"The Hubble constant, $H_0$, tension is the tension among the local probes, Supernovae Ia, and the Cosmic Microwave Background Radiation. It has been almost a decade, and this tension still puzzles the community. Here, we add intermediate redshift probes, such as Gamma-Ray Bursts (GRB) and Quasars (QS0s), to check if and to what extent these higher redshift probes can reduce this tension. We use the three-dimensional fundamental plane relation among the prompt peak luminosity, the luminosity at the end of the plateau emission, and its rest frame duration. We find similar trend in GRB intrinsic parameters as previously seen in Pantheon-Plus intrinsic parameters. We find an apparent $3.14\\sigma$ tension for the GRB intrinsic parameter $b$. Indeed, this tension disappears and the parameters are actually compatible within $2.26\\sigma$. Another interesting point is that the 3D relation plays an important role in conjunction with Supernovae data with Pantheon Plus and that this apparent discrepancy show how it is important the correction for selection biases and redshift evolution. The incorporation of redshift evolution correction results in a reduction of the GRB tension to $2.26\\sigma$ when adjusting correction parameters. We envision that with more data this indication of tension will possibly disappear when the evolutionary parameters of GRBs are computed with increased precision.","sentences":["The Hubble constant, $H_0$, tension is the tension among the local probes, Supernovae Ia, and the Cosmic Microwave Background Radiation.","It has been almost a decade, and this tension still puzzles the community.","Here, we add intermediate redshift probes, such as Gamma-Ray Bursts (GRB) and Quasars (QS0s), to check if and to what extent these higher redshift probes can reduce this tension.","We use the three-dimensional fundamental plane relation among the prompt peak luminosity, the luminosity at the end of the plateau emission, and its rest frame duration.","We find similar trend in GRB intrinsic parameters as previously seen in Pantheon-Plus intrinsic parameters.","We find an apparent $3.14\\sigma$ tension for the GRB intrinsic parameter $b$. Indeed, this tension disappears and the parameters are actually compatible within $2.26\\sigma$. Another interesting point is that the 3D relation plays an important role in conjunction with Supernovae data with Pantheon Plus and that this apparent discrepancy show how it is important the correction for selection biases and redshift evolution.","The incorporation of redshift evolution correction results in a reduction of the GRB tension to $2.26\\sigma$ when adjusting correction parameters.","We envision that with more data this indication of tension will possibly disappear when the evolutionary parameters of GRBs are computed with increased precision."],"url":"http://arxiv.org/abs/2405.01452v1","category":"astro-ph.HE"}
{"created":"2024-05-02 16:08:58","title":"GROMACS on AMD GPU-Based HPC Platforms: Using SYCL for Performance and Portability","abstract":"GROMACS is a widely-used molecular dynamics software package with a focus on performance, portability, and maintainability across a broad range of platforms. Thanks to its early algorithmic redesign and flexible heterogeneous parallelization, GROMACS has successfully harnessed GPU accelerators for more than a decade. With the diversification of accelerator platforms in HPC and no obvious choice for a multi-vendor programming model, the GROMACS project found itself at a crossroads. The performance and portability requirements, and a strong preference for a standards-based solution, motivated our choice to use SYCL on both new HPC GPU platforms: AMD and Intel. Since the GROMACS 2022 release, the SYCL backend has been the primary means to target AMD GPUs in preparation for exascale HPC architectures like LUMI and Frontier. SYCL is a cross-platform, royalty-free, C++17-based standard for programming hardware accelerators. It allows using the same code to target GPUs from all three major vendors with minimal specialization. While SYCL implementations build on native toolchains, performance of such an approach is not immediately evident. Biomolecular simulations have challenging performance characteristics: latency sensitivity, the need for strong scaling, and typical iteration times as short as hundreds of microseconds. Hence, obtaining good performance across the range of problem sizes and scaling regimes is particularly challenging. Here, we share the results of our work on readying GROMACS for AMD GPU platforms using SYCL, and demonstrate performance on Cray EX235a machines with MI250X accelerators. Our findings illustrate that portability is possible without major performance compromises. We provide a detailed analysis of node-level kernel and runtime performance with the aim of sharing best practices with the HPC community on using SYCL as a performance-portable GPU framework.","sentences":["GROMACS is a widely-used molecular dynamics software package with a focus on performance, portability, and maintainability across a broad range of platforms.","Thanks to its early algorithmic redesign and flexible heterogeneous parallelization, GROMACS has successfully harnessed GPU accelerators for more than a decade.","With the diversification of accelerator platforms in HPC and no obvious choice for a multi-vendor programming model, the GROMACS project found itself at a crossroads.","The performance and portability requirements, and a strong preference for a standards-based solution, motivated our choice to use SYCL on both new HPC GPU platforms: AMD and Intel.","Since the GROMACS 2022 release, the SYCL backend has been the primary means to target AMD GPUs in preparation for exascale HPC architectures like LUMI and Frontier.","SYCL is a cross-platform, royalty-free, C++17-based standard for programming hardware accelerators.","It allows using the same code to target GPUs from all three major vendors with minimal specialization.","While SYCL implementations build on native toolchains, performance of such an approach is not immediately evident.","Biomolecular simulations have challenging performance characteristics: latency sensitivity, the need for strong scaling, and typical iteration times as short as hundreds of microseconds.","Hence, obtaining good performance across the range of problem sizes and scaling regimes is particularly challenging.","Here, we share the results of our work on readying GROMACS for AMD GPU platforms using SYCL, and demonstrate performance on Cray EX235a machines with MI250X accelerators.","Our findings illustrate that portability is possible without major performance compromises.","We provide a detailed analysis of node-level kernel and runtime performance with the aim of sharing best practices with the HPC community on using SYCL as a performance-portable GPU framework."],"url":"http://arxiv.org/abs/2405.01420v1","category":"cs.DC"}
{"created":"2024-05-02 16:05:13","title":"Dimension-free estimates for positivity-preserving Riesz transforms related to Schr\u00f6dinger operators with certain potentials","abstract":"We study the $L^\\infty(\\mathbb{R}^d)$ boundedness for Riesz transforms of the form ${V^{a}(-\\frac12\\Delta+V)^{-a}},$ where $a > 0$ and $V$ is a non-negative potential with power growth acting independently on each coordinate. We factorize the semigroup $e^{-tL}$ into one-dimensional factors, estimate them separately and combine the results to estimate the original semigroup. Similar results with additional assumption $a \\leqslant 1$ are obtained on $L^1(\\mathbb{R}^d)$.","sentences":["We study the $L^\\infty(\\mathbb{R}^d)$ boundedness for Riesz transforms of the form ${V^{a}(-\\frac12\\Delta+V)^{-a}},$ where $a > 0$ and $V$ is a non-negative potential with power growth acting independently on each coordinate.","We factorize the semigroup $e^{-tL}$ into one-dimensional factors, estimate them separately and combine the results to estimate the original semigroup.","Similar results with additional assumption $a \\leqslant 1$ are obtained on $L^1(\\mathbb{R}^d)$."],"url":"http://arxiv.org/abs/2405.01415v1","category":"math.FA"}
{"created":"2024-05-02 16:02:40","title":"Applying Transparent Shaping for Zero Trust Architecture Implementation in AWS: A Case Study","abstract":"This study introduces a methodology integrating Zero Trust Architecture (ZTA) principles and Transparent Shaping into an AWS-hosted Online File Manager (OFM) application, enhancing security without substantial code modifications. We evaluate our approach with the Mozilla Observatory, highlighting significant security improvements and outlining a promising direction for applying Transparent Shaping and ZTA in cloud environments.","sentences":["This study introduces a methodology integrating Zero Trust Architecture (ZTA) principles and Transparent Shaping into an AWS-hosted Online File Manager (OFM) application, enhancing security without substantial code modifications.","We evaluate our approach with the Mozilla Observatory, highlighting significant security improvements and outlining a promising direction for applying Transparent Shaping and ZTA in cloud environments."],"url":"http://arxiv.org/abs/2405.01412v1","category":"cs.CR"}
{"created":"2024-05-02 15:50:44","title":"Breaking and trapping Cooper pairs by Rydberg-molecule spectroscopy in atomic Fermi superfluids","abstract":"We propose a spectroscopic probe of the breaking and localization of Cooper pairs in an atomic Fermi superfluid interacting with a Rydberg impurity. This is achieved by monitoring the formation of diatomic and triatomic ultralong-range molecular species in the superfluid across the BCS - Bose Einstein condensation (BEC) crossover. The triatomic Rydberg molecule in the BEC regime heralds the trapping of a tightly-bound Cooper pair, reminiscent of pion capture in nuclear matter, while the breaking of a Cooper pair on the BCS side by a diatomic Rydberg molecule is evocative of binary-star tidal disruption by a black hole. Spectroscopy of the Fermi superfluid and Rydberg molecules allows for an estimation of the Cooper-pair size while the Rydberg molecule binding energies discern many-body pairing effects.","sentences":["We propose a spectroscopic probe of the breaking and localization of Cooper pairs in an atomic Fermi superfluid interacting with a Rydberg impurity.","This is achieved by monitoring the formation of diatomic and triatomic ultralong-range molecular species in the superfluid across the BCS - Bose Einstein condensation (BEC) crossover.","The triatomic Rydberg molecule in the BEC regime heralds the trapping of a tightly-bound Cooper pair, reminiscent of pion capture in nuclear matter, while the breaking of a Cooper pair on the BCS side by a diatomic Rydberg molecule is evocative of binary-star tidal disruption by a black hole.","Spectroscopy of the Fermi superfluid and Rydberg molecules allows for an estimation of the Cooper-pair size while the Rydberg molecule binding energies discern many-body pairing effects."],"url":"http://arxiv.org/abs/2405.01401v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-02 15:29:51","title":"The Correlation Energy of the Electron Gas in the Mean-Field Regime","abstract":"We prove a rigorous lower bound on the correlation energy of interacting fermions in the mean-field regime for a wide class of singular interactions, including the Coulomb potential. Combined with the upper bound obtained in \\cite{ChrHaiNam-23b}, our result establishes an analogue of the Gell-Mann--Brueckner formula $c_{1}\\rho\\log\\left(\\rho\\right)+c_{2}\\rho$ for the correlation energy of the electron gas in the high-density limit. Moreover, our analysis allows us to go beyond mean-field scaling while still covering the same class of potentials.","sentences":["We prove a rigorous lower bound on the correlation energy of interacting fermions in the mean-field regime for a wide class of singular interactions, including the Coulomb potential.","Combined with the upper bound obtained in \\cite{ChrHaiNam-23b}, our result establishes an analogue of the Gell-Mann--Brueckner formula $c_{1}\\rho\\log\\left(\\rho\\right)+c_{2}\\rho$ for the correlation energy of the electron gas in the high-density limit.","Moreover, our analysis allows us to go beyond mean-field scaling while still covering the same class of potentials."],"url":"http://arxiv.org/abs/2405.01386v1","category":"math-ph"}
{"created":"2024-05-02 14:35:07","title":"High-precision prediction for multi-scale processes at the LHC","abstract":"Comparisons of higher-order predictions within the Standard Model of Particle Physics (SM) to data are central to high-energy collider experiments like the Large Hadron Collider (LHC). Processes with multiple kinematic scales, such as multi-jet and prompt photon production, provide a unique possibility for probing Quantum Chromodynamics (QCD). These processes directly test perturbative QCD and can be used to extract fundamental parameters like the strong coupling constant and to search for BSM physics. Recent developments enabled lifting three-jet, photon plus two-jet, photon-pair plus jet, and three-photon cross-sections to QCD's next-to-next-to-leading order (NNLO). This contribution presents phenomenological results at NNLO QCD for three-jet and photon plus two-jet production.","sentences":["Comparisons of higher-order predictions within the Standard Model of Particle Physics (SM) to data are central to high-energy collider experiments like the Large Hadron Collider (LHC).","Processes with multiple kinematic scales, such as multi-jet and prompt photon production, provide a unique possibility for probing Quantum Chromodynamics (QCD).","These processes directly test perturbative QCD and can be used to extract fundamental parameters like the strong coupling constant and to search for BSM physics.","Recent developments enabled lifting three-jet, photon plus two-jet, photon-pair plus jet, and three-photon cross-sections to QCD's next-to-next-to-leading order (NNLO).","This contribution presents phenomenological results at NNLO QCD for three-jet and photon plus two-jet production."],"url":"http://arxiv.org/abs/2405.01330v1","category":"hep-ph"}
{"created":"2024-05-02 14:18:40","title":"Execution-free Program Repair","abstract":"Automatic program repair usually relies heavily on test cases for both bug identification and fix validation. The issue is that writing test cases is tedious, running them takes much time, and validating a fix through tests does not guarantee its correctness. The novel idea in the Proof2Fix methodology and tool presented here is to rely instead on a program prover, without the need to run tests or to run the program at all. Results show that Proof2Fix finds and fixes significant historical bugs.","sentences":["Automatic program repair usually relies heavily on test cases for both bug identification and fix validation.","The issue is that writing test cases is tedious, running them takes much time, and validating a fix through tests does not guarantee its correctness.","The novel idea in the Proof2Fix methodology and tool presented here is to rely instead on a program prover, without the need to run tests or to run the program at all.","Results show that Proof2Fix finds and fixes significant historical bugs."],"url":"http://arxiv.org/abs/2405.01309v1","category":"cs.SE"}
{"created":"2024-05-02 14:16:06","title":"Spectral and Imaging Observations of a C2.3 White-Light Flare from the Advanced Space-Based Solar Observatory (ASO-S) and the Chinese H$\u03b1$ Solar Explorer (CHASE)","abstract":"Solar white-light flares are characterized by an enhancement in the optical continuum, which are usually large flares (say X- and M-class flares). Here we report a small C2.3 white-light flare (SOL2022-12-20T04:10) observed by the \\emph{Advanced Space-based Solar Observatory} and the \\emph{Chinese H$\\alpha$ Solar Explorer}. This flare exhibits an increase of $\\approx$6.4\\% in the photospheric Fe \\textsc{i} line at 6569.2\\,\\AA\\ and {$\\approx$3.2\\%} in the nearby continuum. The continuum at 3600\\,\\AA\\ also shows an enhancement of $\\approx$4.7\\%. The white-light brightening kernels are mainly located at the flare ribbons and co-spatial with nonthermal hard X-ray sources, which implies that the enhanced white-light emissions are related to nonthermal electron-beam heating. At the brightening kernels, the Fe \\textsc{i} line displays an absorption profile that has a good Gaussian shape, with a redshift up to $\\approx$1.7 km s$^{-1}$, while the H$\\alpha$ line shows an emission profile though having a central reversal. The H$\\alpha$ line profile also shows a red or blue asymmetry caused by plasma flows with a velocity of several to tens of km s$^{-1}$. It is interesting to find that the H$\\alpha$ asymmetry is opposite at the conjugate footpoints. It is also found that the CHASE continuum increase seems to be related to the change of photospheric magnetic field. Our study provides comprehensive characteristics of a small white-light flare that help understand the energy release process of white-light flares.","sentences":["Solar white-light flares are characterized by an enhancement in the optical continuum, which are usually large flares (say X- and M-class flares).","Here we report a small C2.3 white-light flare (SOL2022-12-20T04:10) observed by the \\emph{Advanced Space-based Solar Observatory} and the \\emph{Chinese H$\\alpha$ Solar Explorer}.","This flare exhibits an increase of $\\approx$6.4\\% in the photospheric Fe \\textsc{i} line at 6569.2\\,\\AA\\ and {$\\approx$3.2\\%} in the nearby continuum.","The continuum at 3600\\,\\AA\\ also shows an enhancement of $\\approx$4.7\\%.","The white-light brightening kernels are mainly located at the flare ribbons and co-spatial with nonthermal hard X-ray sources, which implies that the enhanced white-light emissions are related to nonthermal electron-beam heating.","At the brightening kernels, the Fe \\textsc{i} line displays an absorption profile that has a good Gaussian shape, with a redshift up to $\\approx$1.7 km s$^{-1}$, while the H$\\alpha$ line shows an emission profile though having a central reversal.","The H$\\alpha$ line profile also shows a red or blue asymmetry caused by plasma flows with a velocity of several to tens of km s$^{-1}$.","It is interesting to find that the H$\\alpha$ asymmetry is opposite at the conjugate footpoints.","It is also found that the CHASE continuum increase seems to be related to the change of photospheric magnetic field.","Our study provides comprehensive characteristics of a small white-light flare that help understand the energy release process of white-light flares."],"url":"http://arxiv.org/abs/2405.01308v1","category":"astro-ph.SR"}
{"created":"2024-05-02 13:31:27","title":"Satellite lines from autoionizing states of Fe XVI and the problems with the X-ray Fe XVII lines","abstract":"We present new calculations of atomic data needed to model autoionizing states of Fe XVI. We compare the state energies, radiative and excitation data with a sample of results from previous literature. We find a large scatter of results, the most significant ones in the autoionization rates, which are very sensitive to the configuration interaction and state mixing. We find relatively good agreement between the autoionization rates and the collisional excitation rates calculated with the R-matrix suite of programs and autostructure. The largest model, which includes J-resolved states up to n=10, produces ab-initio wavelengths and intensities of the satellite lines which agree well with solar high-resolution spectra of active regions, with few minor wavelength adjustements. We review previous literature, finding many incorrect identifications, most notably those in the NIST database. We provide several new tentative identifications in the 15-15.7 A range, and several new ones at shorter wavelengths, where previous lines were unidentified. Compared to the previous CHIANTI model, the present one has an increased flux in the 15--15.7 A range at 2 MK of a factor of 1.9, resolving the discrepancies found in the analysis of the Marshall Grazing Incidence X-Ray Spectrometer (MaGIXS) observation. It appears that the satellite lines also resolve the long-standing discrepancy in the intensity of the important Fe XVII 3D line at 15.26 A.","sentences":["We present new calculations of atomic data needed to model autoionizing states of Fe XVI.","We compare the state energies, radiative and excitation data with a sample of results from previous literature.","We find a large scatter of results, the most significant ones in the autoionization rates, which are very sensitive to the configuration interaction and state mixing.","We find relatively good agreement between the autoionization rates and the collisional excitation rates calculated with the R-matrix suite of programs and autostructure.","The largest model, which includes J-resolved states up to n=10, produces ab-initio wavelengths and intensities of the satellite lines which agree well with solar high-resolution spectra of active regions, with few minor wavelength adjustements.","We review previous literature, finding many incorrect identifications, most notably those in the NIST database.","We provide several new tentative identifications in the 15-15.7 A range, and several new ones at shorter wavelengths, where previous lines were unidentified.","Compared to the previous CHIANTI model, the present one has an increased flux in the 15--15.7 A range at 2 MK of a factor of 1.9, resolving the discrepancies found in the analysis of the Marshall Grazing Incidence X-Ray Spectrometer (MaGIXS) observation.","It appears that the satellite lines also resolve the long-standing discrepancy in the intensity of the important Fe XVII 3D line at 15.26 A."],"url":"http://arxiv.org/abs/2405.01274v1","category":"astro-ph.SR"}
{"created":"2024-05-02 13:04:26","title":"Towards Consistent Object Detection via LiDAR-Camera Synergy","abstract":"As human-machine interaction continues to evolve, the capacity for environmental perception is becoming increasingly crucial. Integrating the two most common types of sensory data, images, and point clouds, can enhance detection accuracy. However, currently, no model exists that can simultaneously detect an object's position in both point clouds and images and ascertain their corresponding relationship. This information is invaluable for human-machine interactions, offering new possibilities for their enhancement. In light of this, this paper introduces an end-to-end Consistency Object Detection (COD) algorithm framework that requires only a single forward inference to simultaneously obtain an object's position in both point clouds and images and establish their correlation. Furthermore, to assess the accuracy of the object correlation between point clouds and images, this paper proposes a new evaluation metric, Consistency Precision (CP). To verify the effectiveness of the proposed framework, an extensive set of experiments has been conducted on the KITTI and DAIR-V2X datasets. The study also explored how the proposed consistency detection method performs on images when the calibration parameters between images and point clouds are disturbed, compared to existing post-processing methods. The experimental results demonstrate that the proposed method exhibits excellent detection performance and robustness, achieving end-to-end consistency detection. The source code will be made publicly available at https://github.com/xifen523/COD.","sentences":["As human-machine interaction continues to evolve, the capacity for environmental perception is becoming increasingly crucial.","Integrating the two most common types of sensory data, images, and point clouds, can enhance detection accuracy.","However, currently, no model exists that can simultaneously detect an object's position in both point clouds and images and ascertain their corresponding relationship.","This information is invaluable for human-machine interactions, offering new possibilities for their enhancement.","In light of this, this paper introduces an end-to-end Consistency Object Detection (COD) algorithm framework that requires only a single forward inference to simultaneously obtain an object's position in both point clouds and images and establish their correlation.","Furthermore, to assess the accuracy of the object correlation between point clouds and images, this paper proposes a new evaluation metric, Consistency Precision (CP).","To verify the effectiveness of the proposed framework, an extensive set of experiments has been conducted on the KITTI and DAIR-V2X datasets.","The study also explored how the proposed consistency detection method performs on images when the calibration parameters between images and point clouds are disturbed, compared to existing post-processing methods.","The experimental results demonstrate that the proposed method exhibits excellent detection performance and robustness, achieving end-to-end consistency detection.","The source code will be made publicly available at https://github.com/xifen523/COD."],"url":"http://arxiv.org/abs/2405.01258v1","category":"cs.CV"}
{"created":"2024-05-02 13:03:45","title":"Probing the nova shock physics with future gamma-ray observations of the upcoming outburst from T Coronae Borealis","abstract":"Nova shocks behave like scaled-down supernova remnant shocks with a lifetime of only a few weeks or months, thereby providing a unique opportunity to study the dynamics of non-relativistic shocks as well as the shock acceleration physics.Recently, GeV and TeV gamma-ray emissions from an outburst of the recurrent nova RS Ophiuchi have been observed. The light curves of the gamma-ray emissions suggest that they arise from an external shock, which is formed as the nova ejecta interacts with the ambient medium. The shock is thought to transit from an adiabatic shock to a radiative one at later times, but no such later observations are available for RS Ophiuchi. In addition, the spectral evolution of the gamma-ray outburst of RS Ophiuchi was not well measured, and hence the related particle acceleration mechanisms are not well understood. T Coronae Borealis (T CrB) is another recurrent nova in Milky Way and its last outburst was nearly ten times brighter than RS Ophiuichi. Recently the optical light curve of T CrB displayed a state transition behavior before the eruption, and it was predicted that T CrB will undergo an outburst in the near future. By performing a theoretical investigation, we find that Fermi-LAT could capture the transition of the shock from the adiabatic phase to the radiative phase at the GeV band, thanks to a longer detectable time than that of RS Ophiuchi.Due to its higher brightness, we also find that imaging atmospheric Cherenkov telescopes (IACTs) such as MAGIC and VERITAS, and extensive air shower experiments such as LHAASO could detect the nova outburst and measure the gamma-ray spectrum in the very-high-energy (VHE, energy above 0.1 TeV) band more precisely. This can be used to constrain the high-energy cutoff index in the accelerated proton spectrum and the acceleration efficiency, which will shed light on the particle acceleration physics in nova shocks.","sentences":["Nova shocks behave like scaled-down supernova remnant shocks with a lifetime of only a few weeks or months, thereby providing a unique opportunity to study the dynamics of non-relativistic shocks as well as the shock acceleration physics.","Recently, GeV and TeV gamma-ray emissions from an outburst of the recurrent nova RS Ophiuchi have been observed.","The light curves of the gamma-ray emissions suggest that they arise from an external shock, which is formed as the nova ejecta interacts with the ambient medium.","The shock is thought to transit from an adiabatic shock to a radiative one at later times, but no such later observations are available for RS Ophiuchi.","In addition, the spectral evolution of the gamma-ray outburst of RS Ophiuchi was not well measured, and hence the related particle acceleration mechanisms are not well understood.","T Coronae Borealis (T CrB) is another recurrent nova in Milky Way and its last outburst was nearly ten times brighter than RS Ophiuichi.","Recently the optical light curve of T CrB displayed a state transition behavior before the eruption, and it was predicted that T CrB will undergo an outburst in the near future.","By performing a theoretical investigation, we find that Fermi-LAT could capture the transition of the shock from the adiabatic phase to the radiative phase at the GeV band, thanks to a longer detectable time than that of RS Ophiuchi.","Due to its higher brightness, we also find that imaging atmospheric Cherenkov telescopes (IACTs) such as MAGIC and VERITAS, and extensive air shower experiments such as LHAASO could detect the nova outburst and measure the gamma-ray spectrum in the very-high-energy (VHE, energy above 0.1 TeV) band more precisely.","This can be used to constrain the high-energy cutoff index in the accelerated proton spectrum and the acceleration efficiency, which will shed light on the particle acceleration physics in nova shocks."],"url":"http://arxiv.org/abs/2405.01257v1","category":"astro-ph.HE"}
{"created":"2024-05-02 12:34:11","title":"All-Optical Noise Quenching of An Integrated Frequency Comb","abstract":"Integrated frequency combs promise transformation of lab-based metrology into disruptive real-world applications. These microcombs are, however, sensitive to stochastic thermal fluctuations of the integrated cavity refractive index, with its impact becoming more significant as the cavity size becomes smaller. This tradeoff between microcomb noise performance and footprint stands as a prominent obstacle to realizing applications beyond a controlled lab environment. Here, we demonstrate that small footprint and low noise become compatible through the all-optical Kerr-induced synchronization (KIS) method. Our study unveils that the phase-locking nature of the synchronization between the cavity soliton and the injected reference pump laser enables the microcomb to no longer be limited by internal noise sources. Instead, the microcomb noise is mostly limited by external sources, namely, the frequency noise of the two pumps that doubly pin the microcomb. First, we theoretically and experimentally show that the individual comb tooth linewidths of an octave-spanning microcomb remain within the same order-of-magnitude as the pump lasers, contrary to the single-pumped case that exhibits a more than two order-of-magnitude increase from the pump to the comb edge. Second, we theoretically show that intrinsic noise sources such as thermorefractive noise in KIS are quenched at the cavity decay rate, greatly decreasing its impact. Experimentally, we show that even with free-running lasers, the KIS microcomb can exhibit better repetition rate noise performance than the predicted thermorefractive noise limitation in absence of KIS.","sentences":["Integrated frequency combs promise transformation of lab-based metrology into disruptive real-world applications.","These microcombs are, however, sensitive to stochastic thermal fluctuations of the integrated cavity refractive index, with its impact becoming more significant as the cavity size becomes smaller.","This tradeoff between microcomb noise performance and footprint stands as a prominent obstacle to realizing applications beyond a controlled lab environment.","Here, we demonstrate that small footprint and low noise become compatible through the all-optical Kerr-induced synchronization (KIS) method.","Our study unveils that the phase-locking nature of the synchronization between the cavity soliton and the injected reference pump laser enables the microcomb to no longer be limited by internal noise sources.","Instead, the microcomb noise is mostly limited by external sources, namely, the frequency noise of the two pumps that doubly pin the microcomb.","First, we theoretically and experimentally show that the individual comb tooth linewidths of an octave-spanning microcomb remain within the same order-of-magnitude as the pump lasers, contrary to the single-pumped case that exhibits a more than two order-of-magnitude increase from the pump to the comb edge.","Second, we theoretically show that intrinsic noise sources such as thermorefractive noise in KIS are quenched at the cavity decay rate, greatly decreasing its impact.","Experimentally, we show that even with free-running lasers, the KIS microcomb can exhibit better repetition rate noise performance than the predicted thermorefractive noise limitation in absence of KIS."],"url":"http://arxiv.org/abs/2405.01238v1","category":"physics.optics"}
{"created":"2024-05-02 12:09:08","title":"CoolWalks: Assessing the potential of shaded routing for active mobility in urban street networks","abstract":"Walking is the most sustainable form of urban mobility, but is compromised by uncomfortable or unhealthy sun exposure, which is an increasing problem due to global warming. Shade from buildings can provide cooling and protection for pedestrians, but the extent of this potential benefit is unknown. Here we explore the potential for shaded walking, using building footprints and street networks from both synthetic and real cities. We introduce a route choice model with a sun avoidance parameter $\\alpha$ and define the CoolWalkability metric to measure opportunities for walking in shade. We derive analytically that on a regular grid with constant building heights, CoolWalkability is independent of $\\alpha$, and that the grid provides no CoolWalkability benefit for shade-seeking individuals compared to the shortest path. However, variations in street geometry and building heights create such benefits. We further uncover that the potential for shaded routing differs between grid-like and irregular street networks, forms local clusters, and is sensitive to the mapped network geometry. Our research identifies the limitations and potential of shade for cool, active travel, and is a first step towards a rigorous understanding of shade provision for sustainable mobility in cities.","sentences":["Walking is the most sustainable form of urban mobility, but is compromised by uncomfortable or unhealthy sun exposure, which is an increasing problem due to global warming.","Shade from buildings can provide cooling and protection for pedestrians, but the extent of this potential benefit is unknown.","Here we explore the potential for shaded walking, using building footprints and street networks from both synthetic and real cities.","We introduce a route choice model with a sun avoidance parameter $\\alpha$ and define the CoolWalkability metric to measure opportunities for walking in shade.","We derive analytically that on a regular grid with constant building heights, CoolWalkability is independent of $\\alpha$, and that the grid provides no CoolWalkability benefit for shade-seeking individuals compared to the shortest path.","However, variations in street geometry and building heights create such benefits.","We further uncover that the potential for shaded routing differs between grid-like and irregular street networks, forms local clusters, and is sensitive to the mapped network geometry.","Our research identifies the limitations and potential of shade for cool, active travel, and is a first step towards a rigorous understanding of shade provision for sustainable mobility in cities."],"url":"http://arxiv.org/abs/2405.01225v1","category":"physics.soc-ph"}
{"created":"2024-05-02 11:49:22","title":"Recurrent Nova V2487 Oph Had Superflares in 1941 and 1942 With Radiant Energies 10$^{42.5\\pm1.6}$ Ergs","abstract":"V2487 Ophiuchi (V2487 Oph) is a recurrent nova with classical nova eruptions in 1900 and 1998, and it is also the most extreme known superflare star. These superflares are roughly-hour-long flares with amplitudes and optical energies reaching up to 1.10 mag and $10^{39.21}$ ergs, with the superflares recurring once-a-day. The V2487 Oph superflares are certainly operating with the same mechanism as all the other types of superflare stars, where magnetic loops are twisted and stretched until reconnection occurs, whereupon ambient electrons are accelerated to relativistic energies and then emitted bremsstrahlung radiation from X-ray to radio. V2487 Oph is unique among known superflare stars in that one of the loop footprints is in an accretion disk. This exact mechanism was theoretically predicted by M. R. Hayashi and colleagues in 1996. Now, I have found two superflares recorded on Harvard archival photographs from the years 1941 and 1942. These two superflares have $B$ magnitude amplitudes of $>$1.83 and $>$2.00 mag and total radiated energies of $10^{42.4}$ and $10^{42.5}$ ergs with bolometric corrections. Each has emitted energies of $\\sim$30-billion Carringtons, in units of the most energetic solar flare. Further, I find superflares in the Zwicky Transient Factory light curves, so V2487 Oph has been superflaring from 1941 to 2023. For the observed number distribution of $dN/dE$=$4E^{-2}$ superflares per year, for $E$ in units of $10^{41}$ ergs, the emitted energy in superflare light is $10^{42.1}$ erg in each year, or $10^{44.1}$ ergs from 1941 to 2023.","sentences":["V2487 Ophiuchi (V2487 Oph) is a recurrent nova with classical nova eruptions in 1900 and 1998, and it is also the most extreme known superflare star.","These superflares are roughly-hour-long flares with amplitudes and optical energies reaching up to 1.10 mag and $10^{39.21}$ ergs, with the superflares recurring once-a-day.","The V2487","Oph superflares are certainly operating with the same mechanism as all the other types of superflare stars, where magnetic loops are twisted and stretched until reconnection occurs, whereupon ambient electrons are accelerated to relativistic energies and then emitted bremsstrahlung radiation from X-ray to radio.","V2487","Oph is unique among known superflare stars in that one of the loop footprints is in an accretion disk.","This exact mechanism was theoretically predicted by M. R. Hayashi and colleagues in 1996.","Now, I have found two superflares recorded on Harvard archival photographs from the years 1941 and 1942.","These two superflares have $B$ magnitude amplitudes of $>$1.83 and $>$2.00 mag and total radiated energies of $10^{42.4}$ and $10^{42.5}$ ergs with bolometric corrections.","Each has emitted energies of $\\sim$30-billion Carringtons, in units of the most energetic solar flare.","Further, I find superflares in the Zwicky Transient Factory light curves, so V2487","Oph has been superflaring from 1941 to 2023.","For the observed number distribution of $dN/dE$=$4E^{-2}$ superflares per year, for $E$ in units of $10^{41}$ ergs, the emitted energy in superflare light is $10^{42.1}$ erg in each year, or $10^{44.1}$ ergs from 1941 to 2023."],"url":"http://arxiv.org/abs/2405.01210v1","category":"astro-ph.SR"}
{"created":"2024-05-02 11:32:02","title":"Nonperturbative study of the electroweak phase transition in the real scalar singlet extended Standard Model","abstract":"We perform a nonperturbative lattice study of the electroweak phase transition in the real singlet scalar extension of the Standard Model.We consider both the heavy and light singlet-like scalar regimes at non-zero singlet-doublet mixing angle. After reviewing features of the lattice method relevant for phase transition studies, we analyze the dependence of phase transition thermodynamics on phenomenologically relevant parameters. In the heavy singlet-like scalar regime, we find that the transition is crossover for small doublet-singlet mixing angles, despite the presence of an energy barrier in the tree-level potential. The transition becomes first order for sufficiently large mixing angles. We find two-loop perturbation theory to agree closely with the lattice results for all thermodynamical quantities considered here (critical temperature, order parameter discontinuity, latent heat) when the transition is strongly first order. For the light singlet-like scalar regime relevant to exotic Higgs decays, we update previous one-loop perturbative results using the two-loop loop dimensionally reduced effective field theory and assess the nature of the transition with lattice simulations at set of benchmark parameter points. For fixed singlet-like scalar mass the transition becomes crossover when the magnitude of the Higgs-singlet portal coupling is small. We perform our simulations in the high-temperature effective theory, which we briefly review, and present analytic expressions for the relevant lattice-continuum relations.","sentences":["We perform a nonperturbative lattice study of the electroweak phase transition in the real singlet scalar extension of the Standard Model.","We consider both the heavy and light singlet-like scalar regimes at non-zero singlet-doublet mixing angle.","After reviewing features of the lattice method relevant for phase transition studies, we analyze the dependence of phase transition thermodynamics on phenomenologically relevant parameters.","In the heavy singlet-like scalar regime, we find that the transition is crossover for small doublet-singlet mixing angles, despite the presence of an energy barrier in the tree-level potential.","The transition becomes first order for sufficiently large mixing angles.","We find two-loop perturbation theory to agree closely with the lattice results for all thermodynamical quantities considered here (critical temperature, order parameter discontinuity, latent heat) when the transition is strongly first order.","For the light singlet-like scalar regime relevant to exotic Higgs decays, we update previous one-loop perturbative results using the two-loop loop dimensionally reduced effective field theory and assess the nature of the transition with lattice simulations at set of benchmark parameter points.","For fixed singlet-like scalar mass the transition becomes crossover when the magnitude of the Higgs-singlet portal coupling is small.","We perform our simulations in the high-temperature effective theory, which we briefly review, and present analytic expressions for the relevant lattice-continuum relations."],"url":"http://arxiv.org/abs/2405.01191v1","category":"hep-ph"}
{"created":"2024-05-02 11:30:38","title":"Stochastic Geometry Analysis of EMF Exposure of Idle Users and Network Performance with Dynamic Beamforming","abstract":"This paper presents a novel mathematical framework based on stochastic geometry to investigate the electromagnetic field exposure of idle and active users in cellular networks implementing dynamic beamforming. Accurate modeling of antenna gain becomes crucial in this context, encompassing both the main and the side lobes. The marginal distribution of EMF exposure for each type of users is initially derived. Subsequently, network performance is scrutinized by introducing a new metric aimed at ensuring minimal downlink coverage while simultaneously maintaining EMF exposure below distinct thresholds for both idle and active users. The metrics exhibit a high dependency on various parameters, such as the distance between active and idle users and the number of antenna elements.","sentences":["This paper presents a novel mathematical framework based on stochastic geometry to investigate the electromagnetic field exposure of idle and active users in cellular networks implementing dynamic beamforming.","Accurate modeling of antenna gain becomes crucial in this context, encompassing both the main and the side lobes.","The marginal distribution of EMF exposure for each type of users is initially derived.","Subsequently, network performance is scrutinized by introducing a new metric aimed at ensuring minimal downlink coverage while simultaneously maintaining EMF exposure below distinct thresholds for both idle and active users.","The metrics exhibit a high dependency on various parameters, such as the distance between active and idle users and the number of antenna elements."],"url":"http://arxiv.org/abs/2405.01190v1","category":"cs.IT"}
{"created":"2024-05-02 11:02:23","title":"SOPA: A Framework for Sustainability-Oriented Process Analysis and Re-design in Business Process Management","abstract":"Given the continuous global degradation of the Earth's ecosystem due to unsustainable human activity, it is increasingly important for enterprises to evaluate the effects they have on the environment. Consequently, assessing the impact of business processes on sustainability is becoming an important consideration in the discipline of Business Process Management (BPM). However, existing practical approaches that aim at a sustainability-oriented analysis of business processes provide only a limited perspective on the environmental impact caused. Further, they provide no clear and practically applicable mechanism for sustainability-driven process analysis and re-design. Following a design science methodology, we here propose and study SOPA, a framework for sustainability-oriented process analysis and re-design. SOPA extends the BPM life cycle by use of Life Cycle Assessment (LCA) for sustainability analysis in combination with Activity-based Costing (ABC). We evaluate SOPA and its usefulness with a case study, by means of an implementation to support the approach, thereby also illustrating the practical applicability of this work.","sentences":["Given the continuous global degradation of the Earth's ecosystem due to unsustainable human activity, it is increasingly important for enterprises to evaluate the effects they have on the environment.","Consequently, assessing the impact of business processes on sustainability is becoming an important consideration in the discipline of Business Process Management (BPM).","However, existing practical approaches that aim at a sustainability-oriented analysis of business processes provide only a limited perspective on the environmental impact caused.","Further, they provide no clear and practically applicable mechanism for sustainability-driven process analysis and re-design.","Following a design science methodology, we here propose and study SOPA, a framework for sustainability-oriented process analysis and re-design.","SOPA extends the BPM life cycle by use of Life Cycle Assessment (LCA) for sustainability analysis in combination with Activity-based Costing (ABC).","We evaluate SOPA and its usefulness with a case study, by means of an implementation to support the approach, thereby also illustrating the practical applicability of this work."],"url":"http://arxiv.org/abs/2405.01176v1","category":"cs.SE"}
{"created":"2024-05-02 10:13:46","title":"The science and technology of liquid argon detectors","abstract":"Liquid argon detectors are ubiquitous in particle, astroparticle, and applied physics. They reached an unprecedented level of maturity thanks to more than 20 years of R&D and the operation of large-scale facilities at CERN, Fermilab, and the Gran Sasso laboratories. This article reviews such an impressive advance - from the grounding of the experimental technique up to cutting-edge applications. We commence the review by describing the physical and chemical properties of liquid argon as an active and target medium for particle detection, together with advantages and limitations compared with other liquefied noble gases. We examine the opportunities and challenges of liquid argon detectors operated as calorimeters, scintillators, and time projection chambers. We then delve into the core applications of liquid argon detectors at colliders (ATLAS), accelerator neutrino beams (SBN, DUNE), and underground laboratories (DarkSide, DEAP, ICARUS) for the observation of rare events. We complete the review by looking at unconventional developments (pixelization, combined light-charge readout, Xe-doped devices, all-optical readout) and applications in medical and applied physics to extend this technology's scope toward novel research fields.","sentences":["Liquid argon detectors are ubiquitous in particle, astroparticle, and applied physics.","They reached an unprecedented level of maturity thanks to more than 20 years of R&D and the operation of large-scale facilities at CERN, Fermilab, and the Gran Sasso laboratories.","This article reviews such an impressive advance - from the grounding of the experimental technique up to cutting-edge applications.","We commence the review by describing the physical and chemical properties of liquid argon as an active and target medium for particle detection, together with advantages and limitations compared with other liquefied noble gases.","We examine the opportunities and challenges of liquid argon detectors operated as calorimeters, scintillators, and time projection chambers.","We then delve into the core applications of liquid argon detectors at colliders (ATLAS), accelerator neutrino beams (SBN, DUNE), and underground laboratories (DarkSide, DEAP, ICARUS) for the observation of rare events.","We complete the review by looking at unconventional developments (pixelization, combined light-charge readout, Xe-doped devices, all-optical readout) and applications in medical and applied physics to extend this technology's scope toward novel research fields."],"url":"http://arxiv.org/abs/2405.01153v1","category":"physics.ins-det"}
{"created":"2024-05-02 09:58:17","title":"Particle-theory input for neutron-star physics","abstract":"Understanding the properties and physical phase of the dense strongly interacting matter present in the cores of neutron stars or created in their binary mergers remains one of the most prominent open problems in nuclear astrophysics. While most microscopic analyses have historically relied on solvable phenomenological models of nuclear and quark matter, in recent years a model-independent approach utilizing only controlled ab-initio calculations and astrophysical observations has emerged as a viable alternative.   In these lecture notes, I review recent progress in first-principles weak-coupling calculations within high-density quark matter, shedding light on its thermodynamic and transport properties. I cover the most important technical tools used in such calculations, introduce selected highlight results, and explain how this information can be used in phenomenological studies of neutron-star physics. The notes do not offer a self-consistent treatment of the topics covered, but rather aim at filling gaps in existing textbooks on thermal field theory and at connecting the dots in a story developed in several recent research articles, to which the interested reader is directed for further technical details.","sentences":["Understanding the properties and physical phase of the dense strongly interacting matter present in the cores of neutron stars or created in their binary mergers remains one of the most prominent open problems in nuclear astrophysics.","While most microscopic analyses have historically relied on solvable phenomenological models of nuclear and quark matter, in recent years a model-independent approach utilizing only controlled ab-initio calculations and astrophysical observations has emerged as a viable alternative.   ","In these lecture notes, I review recent progress in first-principles weak-coupling calculations within high-density quark matter, shedding light on its thermodynamic and transport properties.","I cover the most important technical tools used in such calculations, introduce selected highlight results, and explain how this information can be used in phenomenological studies of neutron-star physics.","The notes do not offer a self-consistent treatment of the topics covered, but rather aim at filling gaps in existing textbooks on thermal field theory and at connecting the dots in a story developed in several recent research articles, to which the interested reader is directed for further technical details."],"url":"http://arxiv.org/abs/2405.01141v1","category":"hep-ph"}
{"created":"2024-05-02 09:19:03","title":"Investigating the causal effects of multiple treatments using longitudinal data: a simulation study","abstract":"Many clinical questions involve estimating the effects of multiple treatments using observational data. When using longitudinal data, the interest is often in the effect of treatment strategies that involve sustaining treatment over time. This requires causal inference methods appropriate for handling multiple treatments and time-dependent confounding. Robins Generalised methods (g-methods) are a family of methods which can deal with time-dependent confounding and some of these have been extended to situations with multiple treatments, although there are currently no studies comparing different methods in this setting. We show how five g-methods (inverse-probability-of-treatment weighted estimation of marginal structural models, g-formula, g-estimation, censoring and weighting, and a sequential trials approach) can be extended to situations with multiple treatments, compare their performances in a simulation study, and demonstrate their application with an example using data from the UK CF Registry.","sentences":["Many clinical questions involve estimating the effects of multiple treatments using observational data.","When using longitudinal data, the interest is often in the effect of treatment strategies that involve sustaining treatment over time.","This requires causal inference methods appropriate for handling multiple treatments and time-dependent confounding.","Robins Generalised methods (g-methods) are a family of methods which can deal with time-dependent confounding and some of these have been extended to situations with multiple treatments, although there are currently no studies comparing different methods in this setting.","We show how five g-methods (inverse-probability-of-treatment weighted estimation of marginal structural models, g-formula, g-estimation, censoring and weighting, and a sequential trials approach) can be extended to situations with multiple treatments, compare their performances in a simulation study, and demonstrate their application with an example using data from the UK CF Registry."],"url":"http://arxiv.org/abs/2405.01110v1","category":"stat.ME"}
{"created":"2024-05-02 08:51:28","title":"Echo-free quality factor of a multilayer axion haloscope","abstract":"We report a methodology to determine the quality factor ($Q$) in implementations of the so-called dielectric haloscope, a new concept of wavy dark matter detector equipped with a multilayered resonator. An anechoic chamber enables the observation of the resonance frequency and its amplitude for an unlimited series of layers for the first time, which is conveniently filtered. The frequency-normalized power enhancement measured in a Dark-photons \\& Axion-Like particles Interferometer (DALI) prototype is a few hundred per layer over a sweep bandwidth of half a hundred MHz. In light of this result, this scaled-down prototype is sensitive to axions saturating the local dark matter density with a coupling to photons between $g_{a\\gamma\\gamma}\\gtrsim10^{-12}$ GeV$^{-1}$ and $g_{a\\gamma\\gamma}\\gtrsim$ few $\\times 10^{-14}$ GeV$^{-1}$ at frequencies of several dozens of GHz once cooled down to the different working temperatures of the experiment and immersed in magnetic fields ranging from 1 T to 10 T; while the sensitivity of the full-scale DALI is projected at $g_{a\\gamma\\gamma}\\gtrsim\\mathrm{few}\\times10^{-15}$ GeV$^{-1}$ over the entire 25--250 {\\mu}eV range since $Q\\gtrsim10^4$ is expected.","sentences":["We report a methodology to determine the quality factor ($Q$) in implementations of the so-called dielectric haloscope, a new concept of wavy dark matter detector equipped with a multilayered resonator.","An anechoic chamber enables the observation of the resonance frequency and its amplitude for an unlimited series of layers for the first time, which is conveniently filtered.","The frequency-normalized power enhancement measured in a Dark-photons \\& Axion-Like particles Interferometer (DALI) prototype is a few hundred per layer over a sweep bandwidth of half a hundred MHz.","In light of this result, this scaled-down prototype is sensitive to axions saturating the local dark matter density with a coupling to photons between $g_{a\\gamma\\gamma}\\gtrsim10^{-12}$ GeV$^{-1}$ and $g_{a\\gamma\\gamma}\\gtrsim$ few $\\times 10^{-14}$ GeV$^{-1}$ at frequencies of several dozens of GHz once cooled down to the different working temperatures of the experiment and immersed in magnetic fields ranging from 1 T to 10 T; while the sensitivity of the full-scale DALI is projected at $g_{a\\gamma\\gamma}\\gtrsim\\mathrm{few}\\times10^{-15}$ GeV$^{-1}$ over the entire 25--250 {\\mu}eV range since $Q\\gtrsim10^4$ is expected."],"url":"http://arxiv.org/abs/2405.01096v1","category":"hep-ex"}
{"created":"2024-05-02 08:27:13","title":"A positive metric over DGKT vacua","abstract":"We study the notion of a metric over the space of AdS solution in string theory, leading to an associated distance between them. Such a distance is the idea underlying the AdS distance conjecture. We utilise the previously developed prescription for extracting such a metric: taking an off-shell quadratic variation of the string theory effective action and then evaluating it over the space of on-shell solutions. It was shown that this prescription leads to a well-defined positive metric over M-theory Freud-Rubin vacua. In this work, we use the same prescription to calculate the metric over type IIA DGKT vacua. These are much more involved, they have multiple flux parameters and exhibit scale separation. While it remains an open question whether these vacua exist as fully localised solutions of string theory, they are well-defined within the four-dimensional effective theory, which is all that is required for the calculation. We find that they also have a positive metric over them. Interestingly, this metric turns out to be independent of the many flux parameters in the solution, similarly to what happens for metrics over scalar field spaces. This non-trivial flux cancellation, as well as results from explicit vacua, lead us to propose a Swampland condition: that the metric over the space of vacua in quantum gravity, as defined by the above prescription, is always positive.","sentences":["We study the notion of a metric over the space of AdS solution in string theory, leading to an associated distance between them.","Such a distance is the idea underlying the AdS distance conjecture.","We utilise the previously developed prescription for extracting such a metric: taking an off-shell quadratic variation of the string theory effective action and then evaluating it over the space of on-shell solutions.","It was shown that this prescription leads to a well-defined positive metric over M-theory Freud-Rubin vacua.","In this work, we use the same prescription to calculate the metric over type IIA DGKT vacua.","These are much more involved, they have multiple flux parameters and exhibit scale separation.","While it remains an open question whether these vacua exist as fully localised solutions of string theory, they are well-defined within the four-dimensional effective theory, which is all that is required for the calculation.","We find that they also have a positive metric over them.","Interestingly, this metric turns out to be independent of the many flux parameters in the solution, similarly to what happens for metrics over scalar field spaces.","This non-trivial flux cancellation, as well as results from explicit vacua, lead us to propose a Swampland condition: that the metric over the space of vacua in quantum gravity, as defined by the above prescription, is always positive."],"url":"http://arxiv.org/abs/2405.01084v1","category":"hep-th"}
{"created":"2024-05-02 08:02:58","title":"Dynamics for a diffusive epidemic model with a free boundary: spreading-vanishing dichotomy","abstract":"This paper involves a diffusive epidemic model whose domain has one free boundary with the Stefan boundary condition, and one fixed boundary subject to the usual homogeneous Dirichlet or Neumann condition. By using the standard upper and lower solutions method and the regularity theory, we first study some related steady state problems which help us obtain the exact longtime behaviors of solution component $(u,v)$. Then we prove there exists the unique classical solution whose longtime behaviors are governed by a spreading-vanishing dichotomy. Lastly, the criteria determining when spreading or vanishing happens are given with respect to the basic reproduction number $\\mathcal{R}_0$, the initial habitat $[0,h_0]$, the expanding rates $\\mu_1$ and $\\mu_2$ as well as the initial function $(u_0,v_0)$. The criteria reveal the effect of the cooperative behaviors of agents and humans on spreading and vanishing.","sentences":["This paper involves a diffusive epidemic model whose domain has one free boundary with the Stefan boundary condition, and one fixed boundary subject to the usual homogeneous Dirichlet or Neumann condition.","By using the standard upper and lower solutions method and the regularity theory, we first study some related steady state problems which help us obtain the exact longtime behaviors of solution component $(u,v)$. Then we prove there exists the unique classical solution whose longtime behaviors are governed by a spreading-vanishing dichotomy.","Lastly, the criteria determining when spreading or vanishing happens are given with respect to the basic reproduction number $\\mathcal{R}_0$, the initial habitat $[0,h_0]$, the expanding rates $\\mu_1$ and $\\mu_2$ as well as the initial function $(u_0,v_0)$. The criteria reveal the effect of the cooperative behaviors of agents and humans on spreading and vanishing."],"url":"http://arxiv.org/abs/2405.01070v1","category":"math.AP"}
{"created":"2024-05-02 05:13:50","title":"Understanding the phase stability in multi-principal-component AlCuFeMn alloy","abstract":"Method(s) that can reliably predict phase evolution across thermodynamic parameter space, especially in complex systems are of critical significance in academia as well as in the manufacturing industry. In the present work, phase stability in equimolar AlCuFeMn multi-principal-component alloy (MPCA) was predicted using complementary first-principles density functional theory (DFT) calculations, and ab-initio molecular dynamics (AIMD) simulations. Temperature evolution of completely disordered, partially ordered, and completely ordered phases was examined based on Gibbs free energy. Configurational, electronic, vibrational, and lattice mismatch entropies were considered to compute the Gibbs free energy of the competing phases. Additionally, elemental segregation was studied using ab-initio molecular dynamics (AIMD). The predicted results at 300K align well with room-temperature experimental observations using x-ray diffraction, scanning and transmission electron microscopy on a sample prepared using commercially available pure elements. The adopted method could help in predicting plausible phases in other MPCA systems with complex phase stability.","sentences":["Method(s) that can reliably predict phase evolution across thermodynamic parameter space, especially in complex systems are of critical significance in academia as well as in the manufacturing industry.","In the present work, phase stability in equimolar AlCuFeMn multi-principal-component alloy (MPCA) was predicted using complementary first-principles density functional theory (DFT) calculations, and ab-initio molecular dynamics (AIMD) simulations.","Temperature evolution of completely disordered, partially ordered, and completely ordered phases was examined based on Gibbs free energy.","Configurational, electronic, vibrational, and lattice mismatch entropies were considered to compute the Gibbs free energy of the competing phases.","Additionally, elemental segregation was studied using ab-initio molecular dynamics (AIMD).","The predicted results at 300K align well with room-temperature experimental observations using x-ray diffraction, scanning and transmission electron microscopy on a sample prepared using commercially available pure elements.","The adopted method could help in predicting plausible phases in other MPCA systems with complex phase stability."],"url":"http://arxiv.org/abs/2405.01006v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 05:11:40","title":"Hundness and band renormalization in the kagome antiferromagnets Mn$_3X$","abstract":"The interplay of topological band structures and electronic correlations may lead to novel exotic quantum phenomena with potential applications. First-principles calculations are critical for guiding the experimental discoveries and interpretations, but often fail if electronic correlations cannot be properly treated. Here we show that this issue occurs also in the antiferromagnetic kagome lattice Mn$_3X$ ($X=$ Sn, Ge), which exhibit a large anomalous Hall effect due to topological band structures with Weyl nodes near the Fermi energy. Our systematic investigations reveal a crucial role of the Hund's rule coupling on three key aspects of their magnetic, electronic, and topological properties: (1) the establishment of noncollinear antiferromagnetic orders, (2) the weakly renormalized bands in excellent agreement with ARPES, and (3) a sensitive tuning of the Weyl nodes beyond previous expectations. Our work provides a basis for understanding the topological properties of Mn$_3X$ and challenges previous experimental interpretations based on incorrect band structures.","sentences":["The interplay of topological band structures and electronic correlations may lead to novel exotic quantum phenomena with potential applications.","First-principles calculations are critical for guiding the experimental discoveries and interpretations, but often fail if electronic correlations cannot be properly treated.","Here we show that this issue occurs also in the antiferromagnetic kagome lattice Mn$_3X$ ($X=$ Sn, Ge), which exhibit a large anomalous Hall effect due to topological band structures with Weyl nodes near the Fermi energy.","Our systematic investigations reveal a crucial role of the Hund's rule coupling on three key aspects of their magnetic, electronic, and topological properties: (1) the establishment of noncollinear antiferromagnetic orders, (2) the weakly renormalized bands in excellent agreement with ARPES, and (3) a sensitive tuning of the Weyl nodes beyond previous expectations.","Our work provides a basis for understanding the topological properties of Mn$_3X$ and challenges previous experimental interpretations based on incorrect band structures."],"url":"http://arxiv.org/abs/2405.01005v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-02 05:02:13","title":"Bounds on the mass of superradiantly unstable scalar fields around Kerr black holes","abstract":"In this work we compute numerical bounds on the mass $\\mu$ of superradiantly unstable scalar fields in a Kerr black hole background using the continued fraction method. We show that the normalized upper bound on the mass $\\mu$ increases with the angular momentum number $\\ell$ and the azimuthal number $m$, approaching the most stringent analytical bound known to date when $\\ell=m \\gg 1$. We also provide an analytical fit to the numerically determined mass bound as a function of the dimensionless spin parameter $a/M$ of the black hole with an accuracy of the order $0.1\\%$ for the fundamental mode with $\\ell=m=1$, and of the order $1\\%$ for higher-order modes (up to $\\ell=m=20$). We argue that this analytical fit is particularly useful in astrophysical scenarios, since the lowest $\\ell=m$ modes are capable of producing the strongest observable imprints of superradiance.","sentences":["In this work we compute numerical bounds on the mass $\\mu$ of superradiantly unstable scalar fields in a Kerr black hole background using the continued fraction method.","We show that the normalized upper bound on the mass $\\mu$ increases with the angular momentum number $\\ell$ and the azimuthal number $m$, approaching the most stringent analytical bound known to date when $\\ell=m \\gg 1$.","We also provide an analytical fit to the numerically determined mass bound as a function of the dimensionless spin parameter $a/M$ of the black hole with an accuracy of the order $0.1\\%$ for the fundamental mode with $\\ell=m=1$, and of the order $1\\%$ for higher-order modes (up to $\\ell=m=20$).","We argue that this analytical fit is particularly useful in astrophysical scenarios, since the lowest $\\ell=m$ modes are capable of producing the strongest observable imprints of superradiance."],"url":"http://arxiv.org/abs/2405.01003v1","category":"gr-qc"}
{"created":"2024-05-02 04:57:32","title":"Continuous extension of the discrete shift translations on one-dimensional quantum lattice systems","abstract":"We investigate the continuous extension of discrete shift translations on one dimensional quantum lattice systems. We focus on a specific construction provided by a quasi-free C*-flow on the one-dimensional fermion lattice system. This quasi-free C*-flow is heuristically generated by a long-range Hamiltonian consisting of two-body translation invariant interactions with $\\frac{1}{r}$-decay. We explore statistical-mechanical interpretation of such a long-range Hamiltonian, which may be more naturally associated with a momentum operator rather than the Hamiltonian. Through its explicit dynamical formulas, wherein notably the sinc function appears, the continuous shift translations reveal violations of causality and locality. Furthermore, we demonstrate that this quasi-free C*-flow, implementing the shift translations, cannot be extended to the one-dimensional quantum spin system via the Jordan-Wigner transformation.","sentences":["We investigate the continuous extension of discrete shift translations on one dimensional quantum lattice systems.","We focus on a specific construction provided by a quasi-free C*-flow on the one-dimensional fermion lattice system.","This quasi-free C*-flow is heuristically generated by a long-range Hamiltonian consisting of two-body translation invariant interactions with $\\frac{1}{r}$-decay.","We explore statistical-mechanical interpretation of such a long-range Hamiltonian, which may be more naturally associated with a momentum operator rather than the Hamiltonian.","Through its explicit dynamical formulas, wherein notably the sinc function appears, the continuous shift translations reveal violations of causality and locality.","Furthermore, we demonstrate that this quasi-free C*-flow, implementing the shift translations, cannot be extended to the one-dimensional quantum spin system via the Jordan-Wigner transformation."],"url":"http://arxiv.org/abs/2405.01001v1","category":"math-ph"}
{"created":"2024-05-02 03:15:02","title":"A multidimensional approach to quantum state tomography of photoelectron wavepackets","abstract":"There is a growing interest in reconstructing the density matrix of photoionized electrons, in particular in complex systems where decoherence can be introduced either by a partial measurement of the system or through coupling with a stochastic environment. To this, end, several methods to reconstruct the density matrix, quantum state tomography protocols, have been developed and tested on photoelectrons ejected from noble gases following absorption of XUV photons from attosecond pulses. It remains a challenge to obtain model-free, single scan protocols that can reconstruct the density matrix with high fidelities. Current methods require extensive measurements or involve complex fitting of the signal. Faithful single-scan reconstructions would be of great help to increase the number of systems that can be studied. We propose a new and more efficient protocol - rainbow-KRAKEN - that is able to reconstruct the continuous variable density matrix of a photoelectron in a single time delay scan. It is based on measuring the coherences of a photoelectron created by absorption of an XUV pulse using a broadband IR probe that is scanned in time and a narrowband IR reference that is temporally fixed to the XUV pulse. We illustrate its performance for a Fano resonance in He as well as mixed states in Ar arising from spin-orbit splitting. We show that the protocol results in excellent fidelities and near-perfect estimation of the purity.","sentences":["There is a growing interest in reconstructing the density matrix of photoionized electrons, in particular in complex systems where decoherence can be introduced either by a partial measurement of the system or through coupling with a stochastic environment.","To this, end, several methods to reconstruct the density matrix, quantum state tomography protocols, have been developed and tested on photoelectrons ejected from noble gases following absorption of XUV photons from attosecond pulses.","It remains a challenge to obtain model-free, single scan protocols that can reconstruct the density matrix with high fidelities.","Current methods require extensive measurements or involve complex fitting of the signal.","Faithful single-scan reconstructions would be of great help to increase the number of systems that can be studied.","We propose a new and more efficient protocol - rainbow-KRAKEN - that is able to reconstruct the continuous variable density matrix of a photoelectron in a single time delay scan.","It is based on measuring the coherences of a photoelectron created by absorption of an XUV pulse using a broadband IR probe that is scanned in time and a narrowband IR reference that is temporally fixed to the XUV pulse.","We illustrate its performance for a Fano resonance in He as well as mixed states in Ar arising from spin-orbit splitting.","We show that the protocol results in excellent fidelities and near-perfect estimation of the purity."],"url":"http://arxiv.org/abs/2405.00968v1","category":"quant-ph"}
{"created":"2024-05-02 03:13:06","title":"Mass generation via nonlinear massive solution in Higgs potential and particle creations","abstract":"The nonlinear massive plane wave solution of the classical scalar field in the Higgs potential is revisited to study the mass generation and particle creation. In particular, by assuming that the Higgs system is in the slightly excited state in early universe and it is described by the nonlinear solution, we study the mass generation mechanism for massive vector bosons and a heavy fermion in the quantum field theory around the nonlinear massive classical field. The nonlinear massive classical solution gives the transition from the vacuum to a pair of vector bosons and fermions. We present the new formulae of the probability density of the production process for particles in the standard model of elementary particle physics. The probability densities of the particle productions vanish when the nonlinear massive solution reduces to the constant solution (the classical vacuum expectation value); while the probability densities are expressed as the function of the free parameter in the classical solution in general case. We discuss the behavior of the probability densities for the three oscillating modes in the classical solution.","sentences":["The nonlinear massive plane wave solution of the classical scalar field in the Higgs potential is revisited to study the mass generation and particle creation.","In particular, by assuming that the Higgs system is in the slightly excited state in early universe and it is described by the nonlinear solution, we study the mass generation mechanism for massive vector bosons and a heavy fermion in the quantum field theory around the nonlinear massive classical field.","The nonlinear massive classical solution gives the transition from the vacuum to a pair of vector bosons and fermions.","We present the new formulae of the probability density of the production process for particles in the standard model of elementary particle physics.","The probability densities of the particle productions vanish when the nonlinear massive solution reduces to the constant solution (the classical vacuum expectation value); while the probability densities are expressed as the function of the free parameter in the classical solution in general case.","We discuss the behavior of the probability densities for the three oscillating modes in the classical solution."],"url":"http://arxiv.org/abs/2405.00967v1","category":"hep-ph"}
{"created":"2024-05-02 02:54:37","title":"Right-handed neutrino dark matter in $U(1)_X$SSM","abstract":"There is strong evidence for the existence of dark matter in a number of current experiments. We study dark matter using the $U(1)_X$SSM obtained from the $U(1)_X$ extension of the minimal supersymmetric standard model (MSSM). In the $U(1)_X$SSM, we use the right-handed neutrino as a dark matter candidate, whose lightest mass eigenstate has cold dark matter features. In this paper, the relic density of right-handed neutrino as dark matter is investigated. For dark matter scattering, both spin-independent and spin-dependent cross sections are studied. In the final numerical results obtained, some parameter spaces can satisfy the constraints of the relic density and dark matter direct detection experiments.","sentences":["There is strong evidence for the existence of dark matter in a number of current experiments.","We study dark matter using the $U(1)_X$SSM obtained from the $U(1)_X$ extension of the minimal supersymmetric standard model (MSSM).","In the $U(1)_X$SSM, we use the right-handed neutrino as a dark matter candidate, whose lightest mass eigenstate has cold dark matter features.","In this paper, the relic density of right-handed neutrino as dark matter is investigated.","For dark matter scattering, both spin-independent and spin-dependent cross sections are studied.","In the final numerical results obtained, some parameter spaces can satisfy the constraints of the relic density and dark matter direct detection experiments."],"url":"http://arxiv.org/abs/2405.00961v1","category":"hep-ph"}
{"created":"2024-05-02 02:28:48","title":"Free group of Hamel bijections of big size","abstract":"A $f\\colon\\mathbb{R}\\to\\mathbb{R}$ is called Hamel function if its graph is a Hamel basis of the linear space $\\mathbb{R}^2$ over rationals. We construct, assuming CH, a free group of the size $2^\\mathfrak{c}$ contained in the class of all Hamel functions, with the indentity function included. This answers, consistently, a question posed recently by M. Lichman, M. Pawlikowski, S. Smolarek, and J. Swaczyna.","sentences":["A $f\\colon\\mathbb{R}\\to\\mathbb{R}$ is called Hamel function if its graph is a Hamel basis of the linear space $\\mathbb{R}^2$ over rationals.","We construct, assuming CH, a free group of the size $2^\\mathfrak{c}$ contained in the class of all Hamel functions, with the indentity function included.","This answers, consistently, a question posed recently by M. Lichman, M. Pawlikowski, S. Smolarek, and J. Swaczyna."],"url":"http://arxiv.org/abs/2405.00952v1","category":"math.GR"}
{"created":"2024-05-02 01:34:50","title":"Gravitational collapse and gravitational wave in Einstein--Gauss-Bonnet theory with two scalar fields","abstract":"In this paper, we investigate the gravitational collapse to form the black hole in the acceleratingly expanding universe in the frame of Einstein--Gauss-Bonnet theory having two scalar fields and we study the propagation of the gravitational wave (GW). The collapsing spacetime can be obtained by using the formulation of the ``reconstruction'', that is, we find a model that realises the desired or given geometry. In the reconstructed models, ghosts often appear, which could be eliminated by imposing constraints. We show that the standard cosmological solutions or self-gravitating objects such as a planet, the Sun, various types of stars, etc., in Einstein's gravity, are also solutions in this model. Using the dynamical value of Gauss-Bonnet coupling, the propagation of the high-frequency GW is investigated. The propagating speed changes due to the coupling during the period of the black hole formation. The speed at which the GW propagates The speed at which the GW propagates going into the black hole is different from that of the wave going out.","sentences":["In this paper, we investigate the gravitational collapse to form the black hole in the acceleratingly expanding universe in the frame of Einstein--Gauss-Bonnet theory having two scalar fields and we study the propagation of the gravitational wave (GW).","The collapsing spacetime can be obtained by using the formulation of the ``reconstruction'', that is, we find a model that realises the desired or given geometry.","In the reconstructed models, ghosts often appear, which could be eliminated by imposing constraints.","We show that the standard cosmological solutions or self-gravitating objects such as a planet, the Sun, various types of stars, etc., in Einstein's gravity, are also solutions in this model.","Using the dynamical value of Gauss-Bonnet coupling, the propagation of the high-frequency GW is investigated.","The propagating speed changes due to the coupling during the period of the black hole formation.","The speed at which the GW propagates The speed at which the GW propagates going into the black hole is different from that of the wave going out."],"url":"http://arxiv.org/abs/2405.00936v1","category":"gr-qc"}
{"created":"2024-05-02 00:22:33","title":"Confronting MSSM flat direction inflation with Planck/BICEP data","abstract":"We study the scenario of inflection point inflation where a flat direction of the minimal supersymmetric standard model (MSSM) is identified with the inflaton. Specifically, we consider in full generality the cases where a MSSM flat direction is lifted by a higher-dimensional superpotential whose dimension is n = 4, 5, 6, 7, 9. We confront the inflection point inflation scenarios with various n with the Planck and BICEP data, and thereby constrain the soft SUSY breaking mass and the coefficient of the higher-dimensional operator that lifts the flat direction.","sentences":["We study the scenario of inflection point inflation where a flat direction of the minimal supersymmetric standard model (MSSM) is identified with the inflaton.","Specifically, we consider in full generality the cases where a MSSM flat direction is lifted by a higher-dimensional superpotential whose dimension is n = 4, 5, 6, 7, 9.","We confront the inflection point inflation scenarios with various n with the Planck and BICEP data, and thereby constrain the soft SUSY breaking mass and the coefficient of the higher-dimensional operator that lifts the flat direction."],"url":"http://arxiv.org/abs/2405.00918v1","category":"hep-ph"}
{"created":"2024-05-01 23:55:17","title":"Laser Pulse Induced Second- and Third-Harmonic Generation of Gold Nanorods with Real-Time Time-Dependent Density Functional Tight Binding (RT-TDDFTB) Method","abstract":"In this study, we investigate second- and third-harmonic generation processes in Au nanorod systems using the real-time time-dependent density functional tight binding (RT-TDDFTB) method. Our study focuses on computation of nonlinear signals based on the time dependent dipole response induced by linearly polarized laser pulses interacting with nanoparticles. We systematically explore the influence of various laser parameters, including pump intensity, duration, frequency, and polarization directions, on the harmonic generation. We demonstrate all the results using Au nanorod dimer systems arranged in end-to-end configurations, and disrupting the spatial symmetry of regular single nanorod systems crucial for second harmonic generation processes. Furthermore, we study the impact of nanorod lengths, which lead to variable plasmon energies, on the harmonic generation, and estimates of polarizabilities and hyper-polarizabilities are provided.","sentences":["In this study, we investigate second- and third-harmonic generation processes in Au nanorod systems using the real-time time-dependent density functional tight binding (RT-TDDFTB) method.","Our study focuses on computation of nonlinear signals based on the time dependent dipole response induced by linearly polarized laser pulses interacting with nanoparticles.","We systematically explore the influence of various laser parameters, including pump intensity, duration, frequency, and polarization directions, on the harmonic generation.","We demonstrate all the results using Au nanorod dimer systems arranged in end-to-end configurations, and disrupting the spatial symmetry of regular single nanorod systems crucial for second harmonic generation processes.","Furthermore, we study the impact of nanorod lengths, which lead to variable plasmon energies, on the harmonic generation, and estimates of polarizabilities and hyper-polarizabilities are provided."],"url":"http://arxiv.org/abs/2405.00913v1","category":"physics.optics"}
{"created":"2024-05-01 22:36:26","title":"Stepwise ionization of Mo$^{14+}$ ions in EBIT: The importance of the metastable level","abstract":"The visible spectrum of Mo$^{15+}$ ions was measured using a high-temperature superconducting electron-beam ion trap at the Shanghai EBIT Laboratory, with an electron beam energy $E_{e}$=400 eV, significantly lower than the ionization potential (IP=544.0 eV) of Mo$^{14+}$ ions in the ground state. To expound on the experiment, the energy level structure, radiative transition properties, electron-impact excitation, and electron-impact ionization cross section for both the ground state and low-lying excited state of the Mo$^{14+}$ ions were calculated using Dirac-Fock-Slater method with a local central potential and distorted wave approximation. The results demonstrated reasonable agreement with both available experimental and theoretical data. Through an analysis of the related atomic processes of Mo$^{14+}$ ion, a scenario involving the stepwise ionization of the metastable state 3p$^{6}$3d$^{9}$4s was proposed to explain the presence of the Mo$^{15+}$ ions with a lower energy of the incident electron. Finally, the significance of the metastable levels in ionizing Mo$^{14+}$ ions is highlighted.","sentences":["The visible spectrum of Mo$^{15+}$ ions was measured using a high-temperature superconducting electron-beam ion trap at the Shanghai EBIT Laboratory, with an electron beam energy $E_{e}$=400 eV, significantly lower than the ionization potential (IP=544.0 eV) of Mo$^{14+}$ ions in the ground state.","To expound on the experiment, the energy level structure, radiative transition properties, electron-impact excitation, and electron-impact ionization cross section for both the ground state and low-lying excited state of the Mo$^{14+}$ ions were calculated using Dirac-Fock-Slater method with a local central potential and distorted wave approximation.","The results demonstrated reasonable agreement with both available experimental and theoretical data.","Through an analysis of the related atomic processes of Mo$^{14+}$ ion, a scenario involving the stepwise ionization of the metastable state 3p$^{6}$3d$^{9}$4s was proposed to explain the presence of the Mo$^{15+}$ ions with a lower energy of the incident electron.","Finally, the significance of the metastable levels in ionizing Mo$^{14+}$ ions is highlighted."],"url":"http://arxiv.org/abs/2405.00893v1","category":"physics.atom-ph"}
{"created":"2024-05-01 21:44:53","title":"$U_A(1)$ symmetry breaking quark interactions from vacuum polarization","abstract":"By considering the one loop background field method for a quark-antiquark interaction mediated by one (non perturbative) gluon exchange, sixth order quark effective interactions are derived and investigated in the limit of zero momentum transfer. They extend fourth order quark interactions worked out in previous works of the author. These interactions break $U_A(1)$ symmetry and may be either momentum independent or dependent. Part of these $U_A(1)$ breaking interactions vanish in the limit of massless quarks and several other - involving vector and/or axial quark currents - survive even in the absence of Dynamical Symmetry Breaking. By means of the auxiliary field method, these interactions give rise to three meson interactions whose values are compared to phenomenological values found in the literature. By restricting to the up and down quark sector, some three-meson couplings correspond to the strong decay of specific light axial-vector mesons. These (strong) decays are shown to yield an asymmetry in the production rate of positive and negative pions by means of interference effects that arise due to neutral meson mixings.","sentences":["By considering the one loop background field method for a quark-antiquark interaction mediated by one (non perturbative) gluon exchange, sixth order quark effective interactions are derived and investigated in the limit of zero momentum transfer.","They extend fourth order quark interactions worked out in previous works of the author.","These interactions break $U_A(1)$ symmetry and may be either momentum independent or dependent.","Part of these $U_A(1)$ breaking interactions vanish in the limit of massless quarks and several other - involving vector and/or axial quark currents - survive even in the absence of Dynamical Symmetry Breaking.","By means of the auxiliary field method, these interactions give rise to three meson interactions whose values are compared to phenomenological values found in the literature.","By restricting to the up and down quark sector, some three-meson couplings correspond to the strong decay of specific light axial-vector mesons.","These (strong) decays are shown to yield an asymmetry in the production rate of positive and negative pions by means of interference effects that arise due to neutral meson mixings."],"url":"http://arxiv.org/abs/2405.00880v1","category":"hep-ph"}
{"created":"2024-05-01 21:18:05","title":"Beyond the Standard Model: An overview","abstract":"At present, the Standard Model (SM) agrees with almost all collider data. Yet, three finetuning issues -- the Higgs mass problem, the strong CP problem and the cosmological constant problem -- all call for new physics. The most plausible solutions at present are weak scale SUSY, the PQWW axion and the string landscape. A re-evaluation of EW finetuning in SUSY allows for a higgsino-like LSP and naturalness upper bounds well beyond LHC limits. Rather general arguments from string theory allow for statistical predictions that m_h~ 125 GeV with sparticles beyond present LHC limits. The most lucrative LHC search channel may be for light higgsino pair production. Dark matter turns out to be a SUSY DFSZ axion along with a diminished abundance of higgsino-like WIMPs.","sentences":["At present, the Standard Model (SM) agrees with almost all collider data.","Yet, three finetuning issues -- the Higgs mass problem, the strong CP problem and the cosmological constant problem -- all call for new physics.","The most plausible solutions at present are weak scale SUSY, the PQWW axion and the string landscape.","A re-evaluation of EW finetuning in SUSY allows for a higgsino-like LSP and naturalness upper bounds well beyond LHC limits.","Rather general arguments from string theory allow for statistical predictions that m_h~ 125 GeV with sparticles beyond present LHC limits.","The most lucrative LHC search channel may be for light higgsino pair production.","Dark matter turns out to be a SUSY DFSZ axion along with a diminished abundance of higgsino-like WIMPs."],"url":"http://arxiv.org/abs/2405.00872v1","category":"hep-ph"}
{"created":"2024-05-01 21:10:32","title":"Aspects of Modular Flavor Symmetries","abstract":"Modular flavor symmetries refers to scenarios in which fermion masses respect modular symmetries. Such scenarios have been studied in the bottom-up approach and have an explicit realization in string theory. They rely on the remarkable properties of vector-valued modular forms.","sentences":["Modular flavor symmetries refers to scenarios in which fermion masses respect modular symmetries.","Such scenarios have been studied in the bottom-up approach and have an explicit realization in string theory.","They rely on the remarkable properties of vector-valued modular forms."],"url":"http://arxiv.org/abs/2405.00870v1","category":"hep-ph"}
{"created":"2024-05-01 20:57:00","title":"IR-fixed Euclidean vacuum for linearized gravity on de Sitter space","abstract":"We consider the Euclidean vacuum for linearized gravity on the global de Sitter space, obtained from the Euclidean Green's function on the 4-sphere. We use the notion of Calder\\'on projectors to recover a quantum state for the Lorentzian theory on de Sitter space. We show that while the state is gauge invariant and Hadamard, it is not positive on the whole of the phase space. We show however that a suitable modification at low energies yields a well-defined Hadamard state on global de Sitter space.","sentences":["We consider the Euclidean vacuum for linearized gravity on the global de Sitter space, obtained from the Euclidean Green's function on the 4-sphere.","We use the notion of Calder\\'on projectors to recover a quantum state for the Lorentzian theory on de Sitter space.","We show that while the state is gauge invariant and Hadamard, it is not positive on the whole of the phase space.","We show however that a suitable modification at low energies yields a well-defined Hadamard state on global de Sitter space."],"url":"http://arxiv.org/abs/2405.00866v1","category":"math-ph"}
