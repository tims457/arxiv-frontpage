{"created":"2024-03-25 12:58:28","title":"On Stability Behaviors of 5D M-theory Black Objects","abstract":"Using $N = 2$ supergravity formalism, we investigate certain behaviors of five dimensional black objects from the compactification of M-theory on a Calabi-Yau three-fold. The manifold has been constructed as the intersection of two homogeneous polynomials of degrees $ (\\omega+2,1)$ and $ (2,1) $ in a product of two weighted projective spaces given by $ \\mathbb{WP}^{4}(\\omega,1,1,1,1) \\times\\mathbb{P}^{1}$. First, we determine the allowed electric charge regions of the BPS and non BPS black holes obtained by wrapping M2-branes on appropriate two cycles in such a proposed Calabi-Yau three-fold. After that, we calculate the entropy of these solutions which takes a maximal value corresponding to $\\omega=1$ defining the ordinary projective space $\\mathbb{P}^{4}$. For generic values of $\\omega$, we show that the non BPS states are unstable. Then, we conduct a similar study of five dimensional black strings. Concerning the allowed magnetic charge regions of the BPS and non BPS black stringy solutions derived from M5-branes on dual divisors, we calculate the tension taking a minimal value for $\\mathbb{P}^{4}$. By determining the recombination factor, we show that the non-BPS black string states are stable in the allowed regions in the magnetic charge space.","sentences":["Using $N = 2$ supergravity formalism, we investigate certain behaviors of five dimensional black objects from the compactification of M-theory on a Calabi-Yau three-fold.","The manifold has been constructed as the intersection of two homogeneous polynomials of degrees $ (\\omega+2,1)$ and $ (2,1) $ in a product of two weighted projective spaces given by $ \\mathbb{WP}^{4}(\\omega,1,1,1,1) \\times\\mathbb{P}^{1}$.","First, we determine the allowed electric charge regions of the BPS and non BPS black holes obtained by wrapping M2-branes on appropriate two cycles in such a proposed Calabi-Yau three-fold.","After that, we calculate the entropy of these solutions which takes a maximal value corresponding to $\\omega=1$ defining the ordinary projective space $\\mathbb{P}^{4}$. For generic values of $\\omega$, we show that the non BPS states are unstable.","Then, we conduct a similar study of five dimensional black strings.","Concerning the allowed magnetic charge regions of the BPS and non BPS black stringy solutions derived from M5-branes on dual divisors, we calculate the tension taking a minimal value for $\\mathbb{P}^{4}$. By determining the recombination factor, we show that the non-BPS black string states are stable in the allowed regions in the magnetic charge space."],"url":"http://arxiv.org/abs/2403.16724v1","category":"hep-th"}
{"created":"2024-03-25 12:57:41","title":"Static equilibrium of multi-black holes in expanding bubbles in five dimensions","abstract":"We investigate possible configurations for vacuum multi-black holes that maintain static equilibrium in expanding bubbles. Our analysis assumes a five-dimensional Weyl metric to describe the spacetime, facilitating the derivation of solutions based on the provided rod structure. We consider a spacetime having expanding bubbles caused by one or two acceleration horizons, and show that various configurations such as two bubbles, four bubbles devoid of horizons, a black saturn, a black di-ring, a bicycling black ring (orthogonal black di-ring), and a five-dimensional black hole binary can achieve equilibrium within expanding bubbles. Specifically, we demonstrate that equilibrium requires two acceleration horizons on both sides for the bicycling ring and the five-dimensional black hole binary. However, only one acceleration horizon is necessary for achieving equilibrium in the case of the black saturn and the black di-ring.","sentences":["We investigate possible configurations for vacuum multi-black holes that maintain static equilibrium in expanding bubbles.","Our analysis assumes a five-dimensional Weyl metric to describe the spacetime, facilitating the derivation of solutions based on the provided rod structure.","We consider a spacetime having expanding bubbles caused by one or two acceleration horizons, and show that various configurations such as two bubbles, four bubbles devoid of horizons, a black saturn, a black di-ring, a bicycling black ring (orthogonal black di-ring), and a five-dimensional black hole binary can achieve equilibrium within expanding bubbles.","Specifically, we demonstrate that equilibrium requires two acceleration horizons on both sides for the bicycling ring and the five-dimensional black hole binary.","However, only one acceleration horizon is necessary for achieving equilibrium in the case of the black saturn and the black di-ring."],"url":"http://arxiv.org/abs/2403.16723v1","category":"hep-th"}
{"created":"2024-03-25 12:57:16","title":"Anti-de Sitter Momentum Space in 3D and 4D Quantum Gravity","abstract":"There has been strong interest in the possibility that in the quantum-gravity realm momentum space might be curved, mainly focusing, especially for what concerns phenomenological implications, on the case of a de Sitter momentum space. We here take as starting point the known fact that quantum gravity coupled to matter in $2+1$ spacetime dimensions gives rise to an effective picture characterized by a momentum space with anti-de Sitter geometry, and we point out some key properties of $2+1$-dimensional anti-de Sitter momentum space. We observe that it is impossible to implement all of these properties in theories with a $3+1$-dimensional anti-de Sitter momentum space, and we then investigate, with the aim of providing guidance to the relevant phenomenology focusing on possible modified laws of conservation of momenta, the implications of giving up, in the $3+1$-dimensional case, some of the properties of the $2+1$-dimensional case.","sentences":["There has been strong interest in the possibility that in the quantum-gravity realm momentum space might be curved, mainly focusing, especially for what concerns phenomenological implications, on the case of a de Sitter momentum space.","We here take as starting point the known fact that quantum gravity coupled to matter in $2+1$ spacetime dimensions gives rise to an effective picture characterized by a momentum space with anti-de Sitter geometry, and we point out some key properties of $2+1$-dimensional anti-de Sitter momentum space.","We observe that it is impossible to implement all of these properties in theories with a $3+1$-dimensional anti-de Sitter momentum space, and we then investigate, with the aim of providing guidance to the relevant phenomenology focusing on possible modified laws of conservation of momenta, the implications of giving up, in the $3+1$-dimensional case, some of the properties of the $2+1$-dimensional case."],"url":"http://arxiv.org/abs/2403.16721v1","category":"gr-qc"}
{"created":"2024-03-25 12:57:03","title":"Derivation of Jacobian matrices for the error propagation of charged particles traversing magnetic fields and materials","abstract":"In high-energy physics experiments, the trajectories of charged particles are reconstructed using track reconstruction algorithms. Such algorithms need to both identify the set of measurements from a single charged particle and to fit the parameters by propagating tracks along the measurements. The propagation of the track parameter uncertainties is an important component in the track fitting to get the optimal precision in the fitted parameters. The error propagation is performed at the surface intersections by calculating a Jacobian matrix corresponding to the surface-to-surface transport. This paper derives the Jacobian matrix in a general manner to harmonize with semi-analytical numerical integration methods developed for inhomogeneous magnetic fields and materials. The Jacobian and transported covariance matrices are validated by simulating the charged particles between two surfaces and comparing with the results of numerical methods.","sentences":["In high-energy physics experiments, the trajectories of charged particles are reconstructed using track reconstruction algorithms.","Such algorithms need to both identify the set of measurements from a single charged particle and to fit the parameters by propagating tracks along the measurements.","The propagation of the track parameter uncertainties is an important component in the track fitting to get the optimal precision in the fitted parameters.","The error propagation is performed at the surface intersections by calculating a Jacobian matrix corresponding to the surface-to-surface transport.","This paper derives the Jacobian matrix in a general manner to harmonize with semi-analytical numerical integration methods developed for inhomogeneous magnetic fields and materials.","The Jacobian and transported covariance matrices are validated by simulating the charged particles between two surfaces and comparing with the results of numerical methods."],"url":"http://arxiv.org/abs/2403.16720v1","category":"hep-ex"}
{"created":"2024-03-25 12:56:48","title":"Towards a Formalisation of Value-based Actions and Consequentialist Ethics","abstract":"Agents act to bring about a state of the world that is more compatible with their personal or institutional values. To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation. Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile. Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential.","sentences":["Agents act to bring about a state of the world that is more compatible with their personal or institutional values.","To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation.","Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile.","Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential."],"url":"http://arxiv.org/abs/2403.16719v1","category":"cs.MA"}
{"created":"2024-03-25 12:53:42","title":"Superconformal Symmetry and Index Theory","abstract":"Formulation and supersymmetry localization of superconformal indices for $\\mathcal{N}=2B$ superconformal quantum mechanics are reviewed by providing a generalization to fixed point submanifolds of resolved target space geometries, and future applications to gauged scaling quivers are discussed.","sentences":["Formulation and supersymmetry localization of superconformal indices for $\\mathcal{N}=2B$ superconformal quantum mechanics are reviewed by providing a generalization to fixed point submanifolds of resolved target space geometries, and future applications to gauged scaling quivers are discussed."],"url":"http://arxiv.org/abs/2403.16716v1","category":"hep-th"}
{"created":"2024-03-25 12:52:51","title":"Modulational electrostatic wave-wave interactions in plasma fluids modeled by asymmetric coupled nonlinear Schr\u00f6dinger (CNLS) equations","abstract":"The interaction between two co-propagating electrostatic wavepackets characterized by arbitrary carrier wavenumber is considered. A one-dimensional (1D) non-magnetized plasma model is adopted, consisting of a cold inertial ion fluid evolving against a thermalized (Maxwell-Boltzmann distributed) electron background. A multiple-scale perturbation method is employed to reduce the original model equations to a pair of coupled nonlinear Schr\\\"odinger (CNLS) equations governing the dynamics of the wavepacket amplitudes (envelopes). The CNLS equations are in general asymmetric for arbitrary carrier wabvenumbers. Similar CNLS systems have been derived in the past in various physical contexts, and were found to support soliton, breather, and rogue wave solutions, among others. A detailed stability analysis reveals that modulational instability (MI) is possible in a wide range of values in the parameter space. The instability window and the corresponding growth rate are determined, considering different case studies, and their dependence on the carrier and the perturbation wavenumber is investigated from first principles. Wave-wave coupling is shown to favor MI occurrence by extending its range of occurrence and by enhancing its growth rate. Our findings generalize previously known results usually associated with symmetric NLS equations in nonlinear optics, though taking into account the difference between the different envelope wavenumbers and thus group velocities.","sentences":["The interaction between two co-propagating electrostatic wavepackets characterized by arbitrary carrier wavenumber is considered.","A one-dimensional (1D) non-magnetized plasma model is adopted, consisting of a cold inertial ion fluid evolving against a thermalized (Maxwell-Boltzmann distributed) electron background.","A multiple-scale perturbation method is employed to reduce the original model equations to a pair of coupled nonlinear Schr\\\"odinger (CNLS) equations governing the dynamics of the wavepacket amplitudes (envelopes).","The CNLS equations are in general asymmetric for arbitrary carrier wabvenumbers.","Similar CNLS systems have been derived in the past in various physical contexts, and were found to support soliton, breather, and rogue wave solutions, among others.","A detailed stability analysis reveals that modulational instability (MI) is possible in a wide range of values in the parameter space.","The instability window and the corresponding growth rate are determined, considering different case studies, and their dependence on the carrier and the perturbation wavenumber is investigated from first principles.","Wave-wave coupling is shown to favor MI occurrence by extending its range of occurrence and by enhancing its growth rate.","Our findings generalize previously known results usually associated with symmetric NLS equations in nonlinear optics, though taking into account the difference between the different envelope wavenumbers and thus group velocities."],"url":"http://arxiv.org/abs/2403.16715v1","category":"physics.plasm-ph"}
{"created":"2024-03-25 12:51:49","title":"A Mixed Multiscale Spectral Generalized Finite Element Method","abstract":"We present a multiscale mixed finite element method for solving second order elliptic equations with general $L^{\\infty}$-coefficients arising from flow in highly heterogeneous porous media. Our approach is based on a multiscale spectral generalized finite element method (MS-GFEM) and exploits the superior local mass conservation properties of mixed finite elements. Following the MS-GFEM framework, optimal local approximation spaces are built for the velocity field by solving local eigenvalue problems over generalized harmonic spaces. The resulting global velocity space is then enriched suitably to ensure inf-sup stability. We develop the mixed MS-GFEM for both continuous and discrete formulations, with Raviart-Thomas based mixed finite elements underlying the discrete method. Exponential convergence with respect to local degrees of freedom is proven at both the continuous and discrete levels. Numerical results are presented to support the theory and to validate the proposed method.","sentences":["We present a multiscale mixed finite element method for solving second order elliptic equations with general $L^{\\infty}$-coefficients arising from flow in highly heterogeneous porous media.","Our approach is based on a multiscale spectral generalized finite element method (MS-GFEM) and exploits the superior local mass conservation properties of mixed finite elements.","Following the MS-GFEM framework, optimal local approximation spaces are built for the velocity field by solving local eigenvalue problems over generalized harmonic spaces.","The resulting global velocity space is then enriched suitably to ensure inf-sup stability.","We develop the mixed MS-GFEM for both continuous and discrete formulations, with Raviart-Thomas based mixed finite elements underlying the discrete method.","Exponential convergence with respect to local degrees of freedom is proven at both the continuous and discrete levels.","Numerical results are presented to support the theory and to validate the proposed method."],"url":"http://arxiv.org/abs/2403.16714v1","category":"math.NA"}
{"created":"2024-03-25 12:51:22","title":"Design Patterns for Multilevel Modeling and Simulation","abstract":"Multilevel modeling and simulation (M&S) is becoming increasingly relevant due to the benefits that this methodology offers. Multilevel models allow users to describe a system at multiple levels of detail. From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required. From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time. A downside of multilevel M&S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on. In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models. The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on. Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&S application area.","sentences":["Multilevel modeling and simulation (M&S) is becoming increasingly relevant due to the benefits that this methodology offers.","Multilevel models allow users to describe a system at multiple levels of detail.","From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required.","From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time.","A downside of multilevel M&S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on.","In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models.","The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on.","Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&S application area."],"url":"http://arxiv.org/abs/2403.16713v1","category":"cs.SE"}
{"created":"2024-03-25 12:49:40","title":"Chase Termination Beyond Polynomial Time","abstract":"The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering. However, it is merely a semi-decision procedure, which may fail to terminate. Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of \"cycle\" in the chase process. We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions. This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions. Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k-ExpTime. We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k-ExpSpace, respectively.","sentences":["The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering.","However, it is merely a semi-decision procedure, which may fail to terminate.","Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of \"cycle\" in the chase process.","We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions.","This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions.","Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k-ExpTime.","We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k-ExpSpace, respectively."],"url":"http://arxiv.org/abs/2403.16712v1","category":"cs.DB"}
{"created":"2024-03-25 12:48:27","title":"A Gauss-Bonnet formula for the renormalized area of minimal submanifolds of Poincar\u00e9-Einstein manifolds","abstract":"Assuming the extrinsic $Q$-curvature admits a decomposition into the Pfaffian, a scalar conformal submanifold invariant, and a tangential divergence, we prove that the renormalized area of an even-dimensional minimal submanifold of a Poincar\\'e-Einstein manifold can be expressed as a linear combination of its Euler characteristic and the integral of a scalar conformal submanifold invariant. We derive such a decomposition of the extrinsic $Q$-curvature in dimensions two and four, thereby recovering and generalizing results of Alexakis-Mazzeo and Tyrrell, respectively. We also conjecture such a decomposition for general natural submanifold scalars whose integral over compact submanifolds is conformally invariant, and verify our conjecture in dimensions two and four. Our results also apply to the area of a compact even-dimensional minimal submanifold of an Einstein manifold.","sentences":["Assuming the extrinsic $Q$-curvature admits a decomposition into the Pfaffian, a scalar conformal submanifold invariant, and a tangential divergence, we prove that the renormalized area of an even-dimensional minimal submanifold of a Poincar\\'e-Einstein manifold can be expressed as a linear combination of its Euler characteristic and the integral of a scalar conformal submanifold invariant.","We derive such a decomposition of the extrinsic $Q$-curvature in dimensions two and four, thereby recovering and generalizing results of Alexakis-Mazzeo and Tyrrell, respectively.","We also conjecture such a decomposition for general natural submanifold scalars whose integral over compact submanifolds is conformally invariant, and verify our conjecture in dimensions two and four.","Our results also apply to the area of a compact even-dimensional minimal submanifold of an Einstein manifold."],"url":"http://arxiv.org/abs/2403.16710v1","category":"math.DG"}
{"created":"2024-03-25 12:46:15","title":"Facile synthesis of CoSi alloy with rich vacancy for base- and solvent-free aerobic oxidation of aromatic alcohols","abstract":"Rational design and green synthesis of low-cost and robust catalysts efficient for the selective oxidation of various alcohols are full of challenges. Herein, we report a fast and solvent-free arc-melting (AM) method to controllably synthesize semimetal CoSi alloy (abbreviated as AM-CoSi) that is efficient for the base- and solvent-free oxidation of six types of aromatic alcohols. X-ray absorption fine structure (XAFS), electron paramagnetic resonance (EPR), and aberration corrected high angle annular dark field scanning transmission electron microscope (AC HAADF-STEM) confirmed the successful synthesis of AM-CoSi with rich Si vacancy (Siv). The as-prepared CoSi alloy catalysts exhibit an order of magnitude activity enhancement in the oxidation of model reactant benzyl alcohol (BAL) to benzyl benzoate (BBE) compared with its mono counterparts, whereas 70 % yield of BBE which is the highest yield to date. Experimental results and DFT calculations well verify that the CoSi alloy structure improves the BAL conversion and Si vacancy mainly contributes to the generation of BBE. After that, CoSi alloy maintains high stability and a potential pathway is rationally proposed. Besides, CoSi alloy also efficiently works for the selective oxidation of various alcohols with different groups. This work demonstrates for the first time that semimetal CoSi alloy is robust for the green oxidation of various alcohols and provides a vast opportunity for reasonable design and application of other semimetal alloy catalysts.","sentences":["Rational design and green synthesis of low-cost and robust catalysts efficient for the selective oxidation of various alcohols are full of challenges.","Herein, we report a fast and solvent-free arc-melting (AM) method to controllably synthesize semimetal CoSi alloy (abbreviated as AM-CoSi) that is efficient for the base- and solvent-free oxidation of six types of aromatic alcohols.","X-ray absorption fine structure (XAFS), electron paramagnetic resonance (EPR), and aberration corrected high angle annular dark field scanning transmission electron microscope (AC HAADF-STEM) confirmed the successful synthesis of AM-CoSi with rich Si vacancy (Siv).","The as-prepared CoSi alloy catalysts exhibit an order of magnitude activity enhancement in the oxidation of model reactant benzyl alcohol (BAL) to benzyl benzoate (BBE) compared with its mono counterparts, whereas 70 % yield of BBE which is the highest yield to date.","Experimental results and DFT calculations well verify that the CoSi alloy structure improves the BAL conversion and Si vacancy mainly contributes to the generation of BBE.","After that, CoSi alloy maintains high stability and a potential pathway is rationally proposed.","Besides, CoSi alloy also efficiently works for the selective oxidation of various alcohols with different groups.","This work demonstrates for the first time that semimetal CoSi alloy is robust for the green oxidation of various alcohols and provides a vast opportunity for reasonable design and application of other semimetal alloy catalysts."],"url":"http://arxiv.org/abs/2403.16708v1","category":"physics.chem-ph"}
{"created":"2024-03-25 12:44:52","title":"One-Shot Domain Incremental Learning","abstract":"Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our technique through experiments on open datasets.","sentences":["Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification.","In DIL, we assume that samples on new domains are observed over time.","The models must classify inputs on all domains.","In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently.","Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL.","We first empirically show that existing DIL methods do not work well in one-shot DIL.","We have analyzed the reason for this failure through various investigations.","According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers.","Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our technique through experiments on open datasets."],"url":"http://arxiv.org/abs/2403.16707v1","category":"cs.LG"}
{"created":"2024-03-25 12:33:10","title":"Resonant Beam Communications: A New Design Paradigm and Challenges","abstract":"Resonant beam communications (RBCom), which adopt oscillating photons between two separate retroreflectors for information transmission, exhibit potential advantages over other types of wireless optical communications (WOC). However, echo interference generated by the modulated beam reflected from the receiver affects the transmission of the desired information. To tackle this challenge, a synchronization-based point-to-point RBCom system is proposed to eliminate the echo interference, and the design for the transmitter and receiver is discussed. Subsequently, the performance of the proposed RBCom is evaluated and compared with that of visible light communications (VLC) and free space optical communications (FOC). Finally, future research directions are outlined and several implementation challenges of RBCom systems are highlighted.","sentences":["Resonant beam communications (RBCom), which adopt oscillating photons between two separate retroreflectors for information transmission, exhibit potential advantages over other types of wireless optical communications (WOC).","However, echo interference generated by the modulated beam reflected from the receiver affects the transmission of the desired information.","To tackle this challenge, a synchronization-based point-to-point RBCom system is proposed to eliminate the echo interference, and the design for the transmitter and receiver is discussed.","Subsequently, the performance of the proposed RBCom is evaluated and compared with that of visible light communications (VLC) and free space optical communications (FOC).","Finally, future research directions are outlined and several implementation challenges of RBCom systems are highlighted."],"url":"http://arxiv.org/abs/2403.16699v1","category":"cs.IT"}
{"created":"2024-03-25 12:31:01","title":"DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization","abstract":"Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain. Recent work, PromptStyler, employs text prompts to simulate different distribution shifts in the joint vision-language space, allowing the model to generalize effectively to unseen domains without using any images. However, 1) PromptStyler's style generation strategy has limitations, as all style patterns are fixed after the first training phase. This leads to the training set in the second training phase being restricted to a limited set of styles. Additionally, 2) the frozen text encoder in PromptStyler result in the encoder's output varying with the style of the input text prompts, making it difficult for the model to learn domain-invariant features. In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues. The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles. Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets.","sentences":["Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain.","Recent work, PromptStyler, employs text prompts to simulate different distribution shifts in the joint vision-language space, allowing the model to generalize effectively to unseen domains without using any images.","However, 1) PromptStyler's style generation strategy has limitations, as all style patterns are fixed after the first training phase.","This leads to the training set in the second training phase being restricted to a limited set of styles.","Additionally, 2) the frozen text encoder in PromptStyler result in the encoder's output varying with the style of the input text prompts, making it difficult for the model to learn domain-invariant features.","In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues.","The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles.","Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity.","Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets."],"url":"http://arxiv.org/abs/2403.16697v1","category":"cs.CV"}
{"created":"2024-03-25 12:26:32","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading in Prostate Cancer","abstract":"Prostate cancer is a dominant health concern calling for advanced diagnostic tools. Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures. A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles. The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance. Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades. The ConvNeXt model was capable of learning a balance between complexity and generalizability. Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially improving diagnostic efficiency for prostate cancer.","sentences":["Prostate cancer is a dominant health concern calling for advanced diagnostic tools.","Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures.","A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles.","The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance.","Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades.","The ConvNeXt model was capable of learning a balance between complexity and generalizability.","Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially improving diagnostic efficiency for prostate cancer."],"url":"http://arxiv.org/abs/2403.16695v1","category":"eess.IV"}
{"created":"2024-03-25 12:23:12","title":"Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography","abstract":"In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.","sentences":["In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education.","LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students.","Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic.","This research recruited 34 undergraduate students as participants, who were randomly divided into two groups.","The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers.","Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\".","The research findings show comparable scores between the two groups on the retention test.","However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test.","Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity.","However, its strengths on promoting students.","knowledge application and creativity were insignificant.","Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses.","Combining ChatGPT with traditional human teachers might be a more ideal approach.","The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching."],"url":"http://arxiv.org/abs/2403.16687v1","category":"cs.CY"}
{"created":"2024-03-25 12:21:38","title":"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation","abstract":"The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.","sentences":["The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups.","While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language.","Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity.","This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech.","Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem.","Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem.","Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task.","To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech.","Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation.","ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly."],"url":"http://arxiv.org/abs/2403.16685v1","category":"cs.CL"}
{"created":"2024-03-25 12:19:11","title":"CHANG-ES. XXX. 10 kpc Radio Lobes in The Sombrero Galaxy","abstract":"We report the discovery of the 10 kilo-parsec (kpc) scale radio lobes in the Sombrero galaxy (NGC 4594), using data from the Continuum Halos in Nearby Galaxies - an Expanded Very Large Array (VLA) Survey (CHANG-ES) project. We further examine the balance between the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas. At the radii $r$ of ~(1-10) kpc, the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas are generally in balance. This implies that the jets could expand into the surroundings at least to r ~ 10 kpc. The feedback from the active galactic nucleus (AGN) jet responsible for the large-scale lobes may help to explain the unusually high X-ray luminosity of this massive quiescent isolated disk galaxy, although more theoretical work is needed to further examine this possibility.","sentences":["We report the discovery of the 10 kilo-parsec (kpc) scale radio lobes in the Sombrero galaxy (NGC 4594), using data from the Continuum Halos in Nearby Galaxies - an Expanded Very Large Array (VLA) Survey (CHANG-ES) project.","We further examine the balance between the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas.","At the radii $r$ of ~(1-10) kpc, the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas are generally in balance.","This implies that the jets could expand into the surroundings at least to r ~ 10 kpc.","The feedback from the active galactic nucleus (AGN) jet responsible for the large-scale lobes may help to explain the unusually high X-ray luminosity of this massive quiescent isolated disk galaxy, although more theoretical work is needed to further examine this possibility."],"url":"http://arxiv.org/abs/2403.16682v1","category":"astro-ph.GA"}
{"created":"2024-03-25 12:15:55","title":"A note on generalization bounds for losses with finite moments","abstract":"This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.","sentences":["This paper studies the truncation method from Alquier","[1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails.","Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \\to \\infty$ and the loss is essentially bounded.","Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance.","This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature.","Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes.","In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings."],"url":"http://arxiv.org/abs/2403.16681v1","category":"stat.ML"}
{"created":"2024-03-25 12:15:47","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","abstract":"Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization. Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary. An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/tum-pbs/SFBC.","sentences":["Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics.","Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning.","We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation.","We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy.","Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization.","Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary.","An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/tum-pbs/SFBC."],"url":"http://arxiv.org/abs/2403.16680v1","category":"cs.LG"}
{"created":"2024-03-25 12:15:42","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks","abstract":"Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows. Previous studies already demonstrated AI's potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability. To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections. Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing fine-tuned image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures. The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides. We demonstrated that DeepGleason is capable of highly accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806, AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison revealed that the ConvNeXt model was superior performance-wise on our dataset to established and other modern architectures like transformers. Furthermore, we were able to outperform the current state-of-the-art in tile-wise fine-classification with a sensitivity and specificity of 0.94 and 0.98 for benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs Gleason 4 & 5 classification, respectively. Our tool contributes to the wider adoption of AI-based Gleason grading within the research community and paves the way for broader clinical application of deep learning models in digital pathology. DeepGleason is open-source and publicly available for research application in the following Git repository: https://github.com/frankkramer-lab/DeepGleason.","sentences":["Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows.","Previous studies already demonstrated AI's potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability.","To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections.","Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing fine-tuned image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures.","The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides.","We demonstrated that DeepGleason is capable of highly accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806, AUC of 0.991, and Accuracy of 0.974.","The internal architecture comparison revealed that the ConvNeXt model was superior performance-wise on our dataset to established and other modern architectures like transformers.","Furthermore, we were able to outperform the current state-of-the-art in tile-wise fine-classification with a sensitivity and specificity of 0.94 and 0.98 for benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs Gleason 4 & 5 classification, respectively.","Our tool contributes to the wider adoption of AI-based Gleason grading within the research community and paves the way for broader clinical application of deep learning models in digital pathology.","DeepGleason is open-source and publicly available for research application in the following Git repository: https://github.com/frankkramer-lab/DeepGleason."],"url":"http://arxiv.org/abs/2403.16678v1","category":"eess.IV"}
{"created":"2024-03-25 12:14:34","title":"Design and Performance of Resonant Beam Communications -- Part I: Quasi-Static Scenario","abstract":"This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios. Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed. Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam. With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel. Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds. Finally, numerical results validate our analysis. Part II of this paper discusses the performance of the RBCom system under the mobile scenario.","sentences":["This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios.","Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed.","Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam.","With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel.","Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds.","Finally, numerical results validate our analysis.","Part II of this paper discusses the performance of the RBCom system under the mobile scenario."],"url":"http://arxiv.org/abs/2403.16676v1","category":"cs.IT"}
{"created":"2024-03-25 12:13:20","title":"Understanding the Functional Roles of Modelling Components in Spiking Neural Networks","abstract":"Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation. With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios. This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models.","sentences":["Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity.","Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear.","By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs.","Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs.","Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation.","With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios.","This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models."],"url":"http://arxiv.org/abs/2403.16674v1","category":"cs.NE"}
{"created":"2024-03-25 12:07:45","title":"Probabilistic bivariate Bell polynomials","abstract":"Let Y be a random variable whose moment generating function exists in some neighborhood of the origin. We consider the probabilistic bivariate Bell polynomials associated with Y and the probabilistic bivariate r-Bell polynomials associated with Y. For those polynomials, we derive the recurrence relations corresponding to the ones found by Zheng and Li for the bivariate Bell polynomials and the bivariate r-Bell polynomials.","sentences":["Let Y be a random variable whose moment generating function exists in some neighborhood of the origin.","We consider the probabilistic bivariate Bell polynomials associated with Y and the probabilistic bivariate r-Bell polynomials associated with Y. For those polynomials, we derive the recurrence relations corresponding to the ones found by Zheng and Li for the bivariate Bell polynomials and the bivariate r-Bell polynomials."],"url":"http://arxiv.org/abs/2403.16670v1","category":"math.NT"}
{"created":"2024-03-25 12:04:03","title":"Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization","abstract":"Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective. Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored. Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed. In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches. Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives.","sentences":["Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective.","Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored.","Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed.","In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches.","Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives."],"url":"http://arxiv.org/abs/2403.16667v1","category":"cs.AI"}
{"created":"2024-03-25 12:01:27","title":"Revisiting the Sleeping Beauty problem","abstract":"The Sleeping Beauty problem is a probability riddle with no definite solution for more than two decades and its solution is of great interest in many fields of knowledge. There are two main competing solutions to the problem: the halfer approach, and the thirder approach. The main reason for disagreement in the literature is connected to the use of different probability spaces to represent the same probabilistic riddle. In this work, we analyse the problem from a mathematical perspective, identifying probability distributions induced directly from the thought experiment's rules. The precise choices of probability spaces provide both halfer and thirder solutions to the problem. To try and decide on which approach to follow, a criterion involving the information available to Sleeping Beauty is proposed.","sentences":["The Sleeping Beauty problem is a probability riddle with no definite solution for more than two decades and its solution is of great interest in many fields of knowledge.","There are two main competing solutions to the problem: the halfer approach, and the thirder approach.","The main reason for disagreement in the literature is connected to the use of different probability spaces to represent the same probabilistic riddle.","In this work, we analyse the problem from a mathematical perspective, identifying probability distributions induced directly from the thought experiment's rules.","The precise choices of probability spaces provide both halfer and thirder solutions to the problem.","To try and decide on which approach to follow, a criterion involving the information available to Sleeping Beauty is proposed."],"url":"http://arxiv.org/abs/2403.16666v1","category":"math.HO"}
{"created":"2024-03-25 11:56:30","title":"Phase separation dynamics in a symmetric binary mixture of ultrasoft particles","abstract":"Phase separation plays an role in determining the self-assembly of biological and soft-matter systems. In biological systems, liquid-liquid phase separation inside a cell leads to the formation of various macromolecular aggregates. The interaction among these aggregates is soft, i.e., these can significantly overlap at a small energy cost. From the computer simulation point of view, these complex macromolecular aggregates are generally modeled by the so-called soft particles. The effective interaction between two particles is defined via the generalized exponential potential (GEM-n) with n = 4. Here, using molecular dynamics simulations, we study the phase separation dynamics of a size-symmetric binary mixture of ultrasoft particles. We find that when the mixture is quenched to a lower temperature below the critical temperature, the two components spontaneously start to separate. Domains of the two components form, and the equal-time order parameter reveals that the domains grow in a power-law manner with exponent 1/3, which is consistent with the Lifshitz-Slyozov law for conserved systems. Further, the static structure factor shows a power-law decay with exponent 4 consistent with the Porod law.","sentences":["Phase separation plays an role in determining the self-assembly of biological and soft-matter systems.","In biological systems, liquid-liquid phase separation inside a cell leads to the formation of various macromolecular aggregates.","The interaction among these aggregates is soft, i.e., these can significantly overlap at a small energy cost.","From the computer simulation point of view, these complex macromolecular aggregates are generally modeled by the so-called soft particles.","The effective interaction between two particles is defined via the generalized exponential potential (GEM-n) with n = 4.","Here, using molecular dynamics simulations, we study the phase separation dynamics of a size-symmetric binary mixture of ultrasoft particles.","We find that when the mixture is quenched to a lower temperature below the critical temperature, the two components spontaneously start to separate.","Domains of the two components form, and the equal-time order parameter reveals that the domains grow in a power-law manner with exponent 1/3, which is consistent with the Lifshitz-Slyozov law for conserved systems.","Further, the static structure factor shows a power-law decay with exponent 4 consistent with the Porod law."],"url":"http://arxiv.org/abs/2403.16663v1","category":"cond-mat.soft"}
{"created":"2024-03-25 11:56:29","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict","abstract":"Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks.","sentences":["Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence.","High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans.","However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge.","To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web.","Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation.","To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations.","Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks."],"url":"http://arxiv.org/abs/2403.16662v1","category":"cs.CL"}
{"created":"2024-03-25 11:54:35","title":"Dynamics of Cayley Forms","abstract":"The most natural first-order PDE's to be imposed on a Cayley 4-form in eight dimensions is the condition that it is closed. As is known, this implies integrability of the Spin(7) structure defined by the Cayley form, as well as Ricci-flatness of the associated metric. We address the question as to what the most natural second-order in derivatives set of conditions is. We start by constructing the most general diffeomorphism invariant second order in derivatives Lagrangian that is quadratic in the perturbations of the Cayley form. We find that there is a one-parameter family of such Lagrangians. We then describe a non-linear completion of the linear story. To this end, we parametrise the intrinsic torsion of a Spin(7) structure by a 3-form, and show that this 3-form is completely determined by the exterior derivative of the Cayley form. We construct an action functional, which depends on the Cayley 4-form and and auxiliary 3-form as independent variables. There is a unique functional whose Euler-Lagrange equation for the auxiliary 3-form states that it is equal to the torsion 3-form. There is, however, a more general one-parameter family of functionals that can be constructed, and we show how the linearisation of these functionals reproduces the linear story. For any member of our family of theories, the Euler-Lagrange equations are written only using the operator of exterior differentiation of forms, and do not require the knowledge of the metric-compatible Levi-Civita connection. Geometrically, there is a preferred member in the family of Lagrangians, and we propose that its Euler-Lagrange equations are the most natural second-order equations to be satisfied by Cayley forms. Our construction also leads to a natural geometric flow in the space of Cayley forms, defined as the gradient flow of our action functional.","sentences":["The most natural first-order PDE's to be imposed on a Cayley 4-form in eight dimensions is the condition that it is closed.","As is known, this implies integrability of the Spin(7) structure defined by the Cayley form, as well as Ricci-flatness of the associated metric.","We address the question as to what the most natural second-order in derivatives set of conditions is.","We start by constructing the most general diffeomorphism invariant second order in derivatives Lagrangian that is quadratic in the perturbations of the Cayley form.","We find that there is a one-parameter family of such Lagrangians.","We then describe a non-linear completion of the linear story.","To this end, we parametrise the intrinsic torsion of a Spin(7) structure by a 3-form, and show that this 3-form is completely determined by the exterior derivative of the Cayley form.","We construct an action functional, which depends on the Cayley 4-form and and auxiliary 3-form as independent variables.","There is a unique functional whose Euler-Lagrange equation for the auxiliary 3-form states that it is equal to the torsion 3-form.","There is, however, a more general one-parameter family of functionals that can be constructed, and we show how the linearisation of these functionals reproduces the linear story.","For any member of our family of theories, the Euler-Lagrange equations are written only using the operator of exterior differentiation of forms, and do not require the knowledge of the metric-compatible Levi-Civita connection.","Geometrically, there is a preferred member in the family of Lagrangians, and we propose that its Euler-Lagrange equations are the most natural second-order equations to be satisfied by Cayley forms.","Our construction also leads to a natural geometric flow in the space of Cayley forms, defined as the gradient flow of our action functional."],"url":"http://arxiv.org/abs/2403.16661v1","category":"math.DG"}
{"created":"2024-03-25 11:51:24","title":"A classical Bousso bound for higher derivative corrections to general relativity","abstract":"We prove the classical version of the covariant entropy bound (also known as the Bousso bound) in arbitrary diffeomorphism invariant gravitational theories. We focus on theories for which the higher derivative terms are considered as small corrections in the Lagrangian to Einstein's two-derivative theory of general relativity (GR). Even if the higher derivative corrections are treated perturbatively, we provide instances of specific configurations for which they can potentially violate the Bousso bound. To tackle this obstruction, we propose a modification in the Bousso bound that incorporates the offending contributions from the higher derivative corrections. Our proposed modifications are equivalent to replacing the Bekenstein-Hawking area term by Wald's definition (with dynamical corrections as suggested by Wall) for the black hole entropy. Hence, the modifications are physically well motivated by results from the laws of black hole mechanics in higher derivative theories.","sentences":["We prove the classical version of the covariant entropy bound (also known as the Bousso bound) in arbitrary diffeomorphism invariant gravitational theories.","We focus on theories for which the higher derivative terms are considered as small corrections in the Lagrangian to Einstein's two-derivative theory of general relativity (GR).","Even if the higher derivative corrections are treated perturbatively, we provide instances of specific configurations for which they can potentially violate the Bousso bound.","To tackle this obstruction, we propose a modification in the Bousso bound that incorporates the offending contributions from the higher derivative corrections.","Our proposed modifications are equivalent to replacing the Bekenstein-Hawking area term by Wald's definition (with dynamical corrections as suggested by Wall) for the black hole entropy.","Hence, the modifications are physically well motivated by results from the laws of black hole mechanics in higher derivative theories."],"url":"http://arxiv.org/abs/2403.16658v1","category":"hep-th"}
{"created":"2024-03-25 11:47:53","title":"Graph Augmentation for Recommendation","abstract":"Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.","sentences":["Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited.","However, directly applying existing GCL models to real-world recommendation environments poses challenges.","There are two primary issues to address.","Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance.","Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing.","To address these challenges, we propose a principled framework called GraphAug.","This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems.","The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation.","Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model.","The outcomes consistently unveil its superiority over existing baseline methods.","The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug."],"url":"http://arxiv.org/abs/2403.16656v1","category":"cs.LG"}
{"created":"2024-03-25 11:45:21","title":"Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT","abstract":"Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text.   This project aims at analyzing different kinds of error that occurs in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%).","sentences":["Text continues to remain a relevant form of representation for information.","Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech.","While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content.","All these variety of mechanisms of text generation also introduce errors into the captured text.   ","This project aims at analyzing different kinds of error that occurs in text documents.","The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text.","Transfer learning of these models with available dataset is performed to finetune their capacity for error correction.","A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories.","It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%)."],"url":"http://arxiv.org/abs/2403.16655v1","category":"cs.CL"}
{"created":"2024-03-25 11:42:01","title":"A Novel Loss Function-based Support Vector Machine for Binary Classification","abstract":"The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM. Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis. The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method.","sentences":["The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin.","This oversight affects the generalization ability of the SVM classifier to some extent.","To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM).","By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM.","Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM.","To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis.","The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.16654v1","category":"cs.LG"}
{"created":"2024-03-25 11:37:15","title":"CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment","abstract":"Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``\\textit{Helpful and Harmless}'' dataset.","sentences":["Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users.","However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.","To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly.","CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process.","Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences.","Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``\\textit{Helpful and Harmless}'' dataset."],"url":"http://arxiv.org/abs/2403.16649v1","category":"cs.AI"}
{"created":"2024-03-25 11:33:39","title":"Effect of Coriolis Force on Electrical Conductivity Tensor for Rotating Hadron Resonance Gas","abstract":"We have investigated the influence of the Coriolis force on the electrical conductivity of hadronic matter formed in relativistic nuclear collisions, employing the Hadron Resonance Gas (HRG) model. A rotating matter in the peripheral heavy ion collisions can be expected from the initial stage of quark matter to late-stage hadronic matter. Present work is focused on rotating hadronic matter, whose medium constituents - hadron resonances can face a non-zero Coriolis force, which can influence the hadronic flow or conductivity. We estimate this conductivity tensor by using the relativistic Boltzmann transport equation. In the absence of Coriolis force, an isotropic conductivity tensor for hadronic matter is expected. However, our study finds that the presence of Coriolis force can generate an anisotropic conductivity tensor with three main conductivity components - parallel, perpendicular, and Hall, similar to the effect of Lorentz force at a finite magnetic field. Our study has indicated that a noticeable anisotropy of conductivity tensor can be found within the phenomenological range of angular velocity $\\Omega= 0.001-0.02$ GeV and hadronic scattering radius $a=0.2-2$ fm.","sentences":["We have investigated the influence of the Coriolis force on the electrical conductivity of hadronic matter formed in relativistic nuclear collisions, employing the Hadron Resonance Gas (HRG) model.","A rotating matter in the peripheral heavy ion collisions can be expected from the initial stage of quark matter to late-stage hadronic matter.","Present work is focused on rotating hadronic matter, whose medium constituents - hadron resonances can face a non-zero Coriolis force, which can influence the hadronic flow or conductivity.","We estimate this conductivity tensor by using the relativistic Boltzmann transport equation.","In the absence of Coriolis force, an isotropic conductivity tensor for hadronic matter is expected.","However, our study finds that the presence of Coriolis force can generate an anisotropic conductivity tensor with three main conductivity components - parallel, perpendicular, and Hall, similar to the effect of Lorentz force at a finite magnetic field.","Our study has indicated that a noticeable anisotropy of conductivity tensor can be found within the phenomenological range of angular velocity $\\Omega= 0.001-0.02$ GeV and hadronic scattering radius $a=0.2-2$ fm."],"url":"http://arxiv.org/abs/2403.16647v1","category":"hep-ph"}
{"created":"2024-03-25 11:31:45","title":"Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations","abstract":"Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation. Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety. This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate. A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM). The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures. The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information. The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%). The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation.","sentences":["Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation.","Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety.","This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate.","A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM).","The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures.","The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information.","The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%).","The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation."],"url":"http://arxiv.org/abs/2403.16645v1","category":"cs.HC"}
{"created":"2024-03-25 11:28:52","title":"Multi-Scale Texture Loss for CT denoising with GANs","abstract":"Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimization. Our approach also introduces a self-attention layer that dynamically aggregates the multi-scale texture information extracted from the images. We validate our approach by carrying out extensive experiments in the context of low-dose CT denoising, a challenging application that aims to enhance the quality of noisy CT scans. We utilize three publicly available datasets, including one simulated and two real datasets. The results are promising as compared to other well-established loss functions, being also consistent across three different GAN architectures. The code is available at: https://github.com/FrancescoDiFeola/DenoTextureLoss","sentences":["Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging.","However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images.","In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image.","To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM).","Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training.","To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimization.","Our approach also introduces a self-attention layer that dynamically aggregates the multi-scale texture information extracted from the images.","We validate our approach by carrying out extensive experiments in the context of low-dose CT denoising, a challenging application that aims to enhance the quality of noisy CT scans.","We utilize three publicly available datasets, including one simulated and two real datasets.","The results are promising as compared to other well-established loss functions, being also consistent across three different GAN architectures.","The code is available at: https://github.com/FrancescoDiFeola/DenoTextureLoss"],"url":"http://arxiv.org/abs/2403.16640v1","category":"eess.IV"}
{"created":"2024-03-25 11:26:18","title":"AI-Generated Video Detection via Spatio-Temporal Anomaly Learning","abstract":"The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos. Malicious users can easily create non-existent videos to spread false information. This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN). Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively. Results of such sub-detectors are fused to further enhance the discrimination ability. A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation. Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme. Code and dataset will be available at https://github.com/multimediaFor/AIGVDet.","sentences":["The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos.","Malicious users can easily create non-existent videos to spread false information.","This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN).","Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively.","Results of such sub-detectors are fused to further enhance the discrimination ability.","A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation.","Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme.","Code and dataset will be available at https://github.com/multimediaFor/AIGVDet."],"url":"http://arxiv.org/abs/2403.16638v1","category":"cs.CV"}
{"created":"2024-03-25 11:24:23","title":"Detecting affine equivalences between rational or non-rational, meromorphic parametric curves in any dimension","abstract":"We generalize previous results by the authors to provide an algorithm for computing the affine equivalences between two parametric curves, either rational or with non-algebraic, meromorphic components under certain conditions, in any dimension. The algorithm completely avoids polynomial system solving, and uses bivariate factoring, instead, as a fundamental tool.","sentences":["We generalize previous results by the authors to provide an algorithm for computing the affine equivalences between two parametric curves, either rational or with non-algebraic, meromorphic components under certain conditions, in any dimension.","The algorithm completely avoids polynomial system solving, and uses bivariate factoring, instead, as a fundamental tool."],"url":"http://arxiv.org/abs/2403.16636v1","category":"math.AG"}
{"created":"2024-03-25 11:21:52","title":"SASHIMI-SIDM: Semi-analytical subhalo modelling for self-interacting dark matter at sub-galactic scales","abstract":"We combine the semi-analytical structure formation model, SASHIMI, which predicts subhalo populations in collisionless, cold dark matter (CDM), with a parametric model that maps CDM halos to self-interacting dark matter (SIDM) halos. The resulting model, SASHIMI-SIDM, generates SIDM subhalo populations down to sub-galactic mass scales, for an arbitrary input cross section, in minutes. We show that SASHIMI-SIDM agrees with SIDM subhalo populations from high-resolution cosmological zoom-in simulations in resolved regimes. Crucially, we predict that the fraction of core-collapsed subhalos peaks at a mass scale determined by the input SIDM cross section and decreases toward higher halo masses, consistent with the predictions of gravothermal models and cosmological simulations. For the first time, we also show that the core-collapsed fraction decreases toward lower halo masses; this result is uniquely enabled by our semi-analytical approach. As a proof of principle, we apply SASHIMI-SIDM to predict the boost to the local dark matter density and annihilation rate from core-collapsed SIDM subhalos, which can be enhanced relative to CDM by an order of magnitude for viable SIDM models. Thus, SASHIMI-SIDM provides an efficient and reliable tool for scanning SIDM parameter space and testing it with astrophysical observations. The code is publicly available at https://github.com/shinichiroando/sashimi-si.","sentences":["We combine the semi-analytical structure formation model, SASHIMI, which predicts subhalo populations in collisionless, cold dark matter (CDM), with a parametric model that maps CDM halos to self-interacting dark matter (SIDM) halos.","The resulting model, SASHIMI-SIDM, generates SIDM subhalo populations down to sub-galactic mass scales, for an arbitrary input cross section, in minutes.","We show that SASHIMI-SIDM agrees with SIDM subhalo populations from high-resolution cosmological zoom-in simulations in resolved regimes.","Crucially, we predict that the fraction of core-collapsed subhalos peaks at a mass scale determined by the input SIDM cross section and decreases toward higher halo masses, consistent with the predictions of gravothermal models and cosmological simulations.","For the first time, we also show that the core-collapsed fraction decreases toward lower halo masses; this result is uniquely enabled by our semi-analytical approach.","As a proof of principle, we apply SASHIMI-SIDM to predict the boost to the local dark matter density and annihilation rate from core-collapsed SIDM subhalos, which can be enhanced relative to CDM by an order of magnitude for viable SIDM models.","Thus, SASHIMI-SIDM provides an efficient and reliable tool for scanning SIDM parameter space and testing it with astrophysical observations.","The code is publicly available at https://github.com/shinichiroando/sashimi-si."],"url":"http://arxiv.org/abs/2403.16633v1","category":"astro-ph.CO"}
{"created":"2024-03-25 11:21:09","title":"Endogenous Fragility in Opaque Supply Chains","abstract":"This paper investigates the role of supply chain unobservability in generating endogenously fragile production networks. In a simple production game, in which firms need to multisource to hedge against suppliers' risk under unobservability, firms underdiversify vis-a-vis the social optimum. The unobservability of suppliers' relations is the driver behind this. In production networks where upstream risk is highly correlated and supplier relationships are not observable, the marginal risk reduction of adding an additional supplier is low, because this additional supplier's risk is likely to be correlated to that of existing suppliers. This channel reduces firm incentives to diversify, which gives rise to inefficiently fragile production networks. By solving the social planner problem, I show that, if the risk reduction experienced downstream resulting from upstream diversification were to be internalised by upstream firms, endogenous production networks would be resilient to most levels of risk. Furthermore, I show that the opaqueness of the supply chain yields less fragile but more inefficient production networks. Despite its stylised form,the model identifies the trade-off firms face when diversifying risk and isolates the mechanism that aggregates these decisions into a production network. Furthermore, it maps the conditions of the trade-off, such as expected profits of the firm or the pairing costs, to the properties of the production network.","sentences":["This paper investigates the role of supply chain unobservability in generating endogenously fragile production networks.","In a simple production game, in which firms need to multisource to hedge against suppliers' risk under unobservability, firms underdiversify vis-a-vis the social optimum.","The unobservability of suppliers' relations is the driver behind this.","In production networks where upstream risk is highly correlated and supplier relationships are not observable, the marginal risk reduction of adding an additional supplier is low, because this additional supplier's risk is likely to be correlated to that of existing suppliers.","This channel reduces firm incentives to diversify, which gives rise to inefficiently fragile production networks.","By solving the social planner problem, I show that, if the risk reduction experienced downstream resulting from upstream diversification were to be internalised by upstream firms, endogenous production networks would be resilient to most levels of risk.","Furthermore, I show that the opaqueness of the supply chain yields less fragile but more inefficient production networks.","Despite its stylised form,the model identifies the trade-off firms face when diversifying risk and isolates the mechanism that aggregates these decisions into a production network.","Furthermore, it maps the conditions of the trade-off, such as expected profits of the firm or the pairing costs, to the properties of the production network."],"url":"http://arxiv.org/abs/2403.16632v1","category":"econ.TH"}
{"created":"2024-03-25 11:16:23","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","abstract":"Recent advancements in diffusion models have positioned them at the forefront of image generation. Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process. To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency. Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation. We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively. Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation.","sentences":["Recent advancements in diffusion models have positioned them at the forefront of image generation.","Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process.","To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency.","Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation.","We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively.","Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation."],"url":"http://arxiv.org/abs/2403.16627v1","category":"cs.CV"}
{"created":"2024-03-25 11:15:02","title":"Presenting Interval Pomsets with Interfaces","abstract":"Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account. In this paper, we develop a presentation of ipomsets as generated by a graph of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators. Using this presentation, we show that also subsumptions are generated by elementary relations. We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages.","sentences":["Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account.","In this paper, we develop a presentation of ipomsets as generated by a graph of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators.","Using this presentation, we show that also subsumptions are generated by elementary relations.","We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages."],"url":"http://arxiv.org/abs/2403.16626v1","category":"cs.FL"}
{"created":"2024-03-25 11:10:17","title":"Stable solution and extremal solution for the fractional $p$-Laplacian equation","abstract":"To our knowledge, this paper is the first attempt to consider the stable solution and extremal solution for the fractional $p$-Laplacian equation: $(-\\Delta)_p^s u= \\lambda f(u),\\; u> 0 ~\\text{in}~\\Omega;\\; u=0\\;\\text{in}~ \\mathbb{R}^N\\setminus\\Omega$, where $p>1$, $s\\in (0,1)$, $N>sp$, $\\lambda>0$ and $\\Omega$ is a bounded domain with continuous boundary. We first construct the notion of stable solution, and then we prove that when $f$ is of class $C^1$, nondecreasing and such that $f(0)>0$ and $\\underset{t\\to \\infty}{\\lim}\\frac{f(t)}{t^{p-1}}=\\infty$, there exists an extremal parameter $\\lambda^*\\in (0, \\infty)$ such that a bounded minimal solution $u_\\lambda$ exists if $\\lambda\\in (0, \\lambda^*)$, and no bounded solution exists if $\\lambda>\\lambda^*$, no $W_0^{s,p}(\\Omega)$ solution exists if in addition $f(t)^{\\frac{1}{p-1}}$ is convex. Moreover, this family of minimal solutions are stable, and nondecreasing in $\\lambda$, therefore the extremal function $u^*:=\\underset{\\lambda\\to\\lambda^*}{\\lim}u_\\lambda$ exists.   For the regularity of the extremal function, we first show the $L^r$-estimates for the equation $(-\\Delta)_p^su=g$ with $g\\in W_0^{s, p}(\\Omega)^*\\cap L^q(\\Omega)$, $q\\ge 1$. When $f$ is a power-like nonlinearity, we derive the $W_0^{s,p}(\\Omega)$ regularity of $u^*$ in all dimension and $L^{\\infty}(\\Omega)$ regularity of $u^*$ in some low dimensions. For more general nonlinearities, when $f$ is class of $C^2$ and such that some convexity assumptions, then $u^*\\in W_0^{s,p}(\\Omega)$ if $N<sp(1+\\frac{p}{p-1})$ and $u^*\\in L^{\\infty}(\\Omega)$ if $N<\\frac{sp^2}{p-1}$. Furthermore, when the limit $\\tau=\\underset{t\\to\\infty}{\\lim}\\frac{f(t)f''(t)}{f'(t)^2}$ exists and $\\tau>\\frac{p-2}{p-1}$, the results above can be improved as: $u^*\\in W_0^{s,p}(\\Omega)$ for all dimensions, and $u^*\\in L^{\\infty}(\\Omega)$ if $N<sp+\\frac{4sp}{p-1}$.","sentences":["To our knowledge, this paper is the first attempt to consider the stable solution and extremal solution for the fractional $p$-Laplacian equation: $(-\\Delta)_p^s u= \\lambda f(u),\\; u> 0 ~\\text{in}~\\Omega;\\; u=0\\;\\text{in}~ \\mathbb{R}^N\\setminus\\Omega$, where $p>1$, $s\\in (0,1)$, $N>sp$, $\\lambda>0$ and $\\Omega$ is a bounded domain with continuous boundary.","We first construct the notion of stable solution, and then we prove that when $f$ is of class $C^1$, nondecreasing and such that $f(0)>0$ and $\\underset{t\\to \\infty}{\\lim}\\frac{f(t)}{t^{p-1}}=\\infty$, there exists an extremal parameter $\\lambda^*\\in (0, \\infty)$ such that a bounded minimal solution $u_\\lambda$ exists if $\\lambda\\in (0, \\lambda^*)$, and no bounded solution exists if $\\lambda>\\lambda^*$, no $W_0^{s,p}(\\Omega)$ solution exists if in addition $f(t)^{\\frac{1}{p-1}}$ is convex.","Moreover, this family of minimal solutions are stable, and nondecreasing in $\\lambda$, therefore the extremal function $u^*:=\\underset{\\lambda\\to\\lambda^*}{\\lim}u_\\lambda$ exists.   ","For the regularity of the extremal function, we first show the $L^r$-estimates for the equation $(-\\Delta)_p^su=g$ with $g\\in W_0^{s, p}(\\Omega)^*\\cap L^q(\\Omega)$, $q\\ge 1$.","When $f$ is a power-like nonlinearity, we derive the $W_0^{s,p}(\\Omega)$ regularity of $u^*$ in all dimension and $L^{\\infty}(\\Omega)$ regularity of $u^*$ in some low dimensions.","For more general nonlinearities, when $f$ is class of $C^2$ and such that some convexity assumptions, then $u^*\\in W_0^{s,p}(\\Omega)$ if $N<sp(1+\\frac{p}{p-1})$ and $u^*\\in L^{\\infty}(\\Omega)$ if $N<\\frac{sp^2}{p-1}$. Furthermore, when the limit $\\tau=\\underset{t\\to\\infty}{\\lim}\\frac{f(t)f''(t)}{f'(t)^2}$ exists and $\\tau>\\frac{p-2}{p-1}$, the results above can be improved as: $u^*\\in W_0^{s,p}(\\Omega)$ for all dimensions, and $u^*\\in L^{\\infty}(\\Omega)$ if $N<sp+\\frac{4sp}{p-1}$."],"url":"http://arxiv.org/abs/2403.16624v1","category":"math.AP"}
{"created":"2024-03-25 10:30:22","title":"SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation","abstract":"In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery. Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts. In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks. The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models. To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation. We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity. Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency. We employ the novel data instances for downstream segmentation, as a form of data augmentation. In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs. We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data.","sentences":["In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery.","Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts.","In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks.","The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models.","To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation.","We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity.","Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency.","We employ the novel data instances for downstream segmentation, as a form of data augmentation.","In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs.","We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data."],"url":"http://arxiv.org/abs/2403.16605v1","category":"cs.CV"}
{"created":"2024-03-25 10:27:42","title":"Finite size effects and optimization of the calculation of the surface tension in surfactant mixtures at liquid/vapour interfaces","abstract":"The surface tension of monolayers with mixtures of anionic and nonionic surfactant at the liquid/vapour interface is studied. Previous works have observed that calculations of the surface tension of simple fluids show artificial oscillations for small interfacial areas, indicating that the surface tension data fluctuate due to the finite size effects and periodic boundary conditions. In the case of simulations of monolayers composed of surfactant mixtures, the surface tension not only oscillates for small areas but can also give non-physical data, such as negative values. Analysis of the monolayers with different surfactant mixtures, ionic (DTAB, CTAB, SDS) and nonionic (SB3-12), was done for density profiles, parameters of order and pair correlation functions for small and large box areas and all of them present similar behaviour. The fluctuations and the non-physical values of the surface tension are corrected when boxes with large interfacial areas are considered. The results indicate that in order to obtain reliable values of the surface tension, in computer simulations, it is important to choose not only the correct force field but also the appropriate size of the simulation box.","sentences":["The surface tension of monolayers with mixtures of anionic and nonionic surfactant at the liquid/vapour interface is studied.","Previous works have observed that calculations of the surface tension of simple fluids show artificial oscillations for small interfacial areas, indicating that the surface tension data fluctuate due to the finite size effects and periodic boundary conditions.","In the case of simulations of monolayers composed of surfactant mixtures, the surface tension not only oscillates for small areas but can also give non-physical data, such as negative values.","Analysis of the monolayers with different surfactant mixtures, ionic (DTAB, CTAB, SDS) and nonionic (SB3-12), was done for density profiles, parameters of order and pair correlation functions for small and large box areas and all of them present similar behaviour.","The fluctuations and the non-physical values of the surface tension are corrected when boxes with large interfacial areas are considered.","The results indicate that in order to obtain reliable values of the surface tension, in computer simulations, it is important to choose not only the correct force field but also the appropriate size of the simulation box."],"url":"http://arxiv.org/abs/2403.16604v1","category":"cond-mat.soft"}
{"created":"2024-03-25 10:22:28","title":"On the $\\mathbb{Z}_p$-extensions of a totally $p$-adic imaginary quadratic field -- With an appendix by Jean-Fran\u00e7ois Jaulent","abstract":"Let $k$ be an imaginary quadratic field, and let $p$ be an odd prime number split in $k$. We analyze some properties of arbitrary $\\mathbb{Z}_p$-extensions K/k. These properties are governed by the Hase norm residue symbol of the fundamental $p$-unit of $k$, in terms of the valuation $\\delta_p(k)$ of a Fermat quotient, which determines the order of the logarithmic class group $\\widetilde{\\mathcal{H}_k}$ (Theorem 2.2) and leads, under some conditions, to generalizations of Gold's criterion characterizing $\\lambda_p(K/k) = 1$ (Theorems 3.3, 5.1, 5.3). This uses the higher rank Chevalley-Herbrand formulas, for the filtrations of the p-class groups in $K$, that we gave in the 1994's, and the theorem of $\\lambda$-stability (2022). This study is in connection with articles of Gold, Sands, Dummit-Ford-Kisilevsky-Sands, Ozaki, Jaulent, Fujii. In Appendix A, is given a general proof, by Jaulent, of the link between $\\widetilde{\\mathcal{H}_k}$ and $\\delta_p(k)$ in a broader context. Numerical illustrations (with pari/gp programs) are given.","sentences":["Let $k$ be an imaginary quadratic field, and let $p$ be an odd prime number split in $k$. We analyze some properties of arbitrary $\\mathbb{Z}_p$-extensions K/k. These properties are governed by the Hase norm residue symbol of the fundamental $p$-unit of $k$, in terms of the valuation $\\delta_p(k)$ of a Fermat quotient, which determines the order of the logarithmic class group $\\widetilde{\\mathcal{H}_k}$ (Theorem 2.2) and leads, under some conditions, to generalizations of Gold's criterion characterizing $\\lambda_p(K/k) = 1$ (Theorems 3.3, 5.1, 5.3).","This uses the higher rank Chevalley-Herbrand formulas, for the filtrations of the p-class groups in $K$, that we gave in the 1994's, and the theorem of $\\lambda$-stability (2022).","This study is in connection with articles of Gold, Sands, Dummit-Ford-Kisilevsky-Sands, Ozaki, Jaulent, Fujii.","In Appendix A, is given a general proof, by Jaulent, of the link between $\\widetilde{\\mathcal{H}_k}$ and $\\delta_p(k)$ in a broader context.","Numerical illustrations (with pari/gp programs) are given."],"url":"http://arxiv.org/abs/2403.16603v1","category":"math.NT"}
{"created":"2024-03-25 10:19:56","title":"Superadiabatic dynamical density functional theory for colloidal suspensions under homogeneous steady-shear","abstract":"The superadiabatic dynamical density functional theory (superadiabatic-DDFT) is a promising new method for the study of colloidal systems out-of-equilibrium. Within this approach the viscous forces arising from interparticle interactions are accounted for in a natural way by treating explicitly the dynamics of the two-body correlations. For bulk systems subject to spatially homogeneous shear we use the superadiabatic-DDFT framework to calculate the steady-state pair distribution function and the corresponding viscosity for low values of the shear-rate. We then consider a variant of the central approximation underlying this superadiabatic theory and obtain an inhomogeneous generalization of a rheological bulk theory due to Russel and Gast. This paper thus establishes for the first time a connection between DDFT approaches, formulated to treat inhomogeneous systems, and existing work addressing nonequilibrium microstructure and rheology in bulk colloidal suspensions.","sentences":["The superadiabatic dynamical density functional theory (superadiabatic-DDFT) is a promising new method for the study of colloidal systems out-of-equilibrium.","Within this approach the viscous forces arising from interparticle interactions are accounted for in a natural way by treating explicitly the dynamics of the two-body correlations.","For bulk systems subject to spatially homogeneous shear we use the superadiabatic-DDFT framework to calculate the steady-state pair distribution function and the corresponding viscosity for low values of the shear-rate.","We then consider a variant of the central approximation underlying this superadiabatic theory and obtain an inhomogeneous generalization of a rheological bulk theory due to Russel and Gast.","This paper thus establishes for the first time a connection between DDFT approaches, formulated to treat inhomogeneous systems, and existing work addressing nonequilibrium microstructure and rheology in bulk colloidal suspensions."],"url":"http://arxiv.org/abs/2403.16599v1","category":"cond-mat.soft"}
{"created":"2024-03-25 10:17:11","title":"GNS-construction for positive C*-valued sesquilinear maps on a quasi *-algebra","abstract":"The GNS construction for positive invariant sesquilinear forms on quasi *-algebras is generalized to a class of positive C*-valued sesquilinear maps on quasi *-algebras. The result is a *-representation taking values in a space of operators acting on a certain quasi-normed C-module.","sentences":["The GNS construction for positive invariant sesquilinear forms on quasi *-algebras is generalized to a class of positive C*-valued sesquilinear maps on quasi *-algebras.","The result is a *-representation taking values in a space of operators acting on a certain quasi-normed C-module."],"url":"http://arxiv.org/abs/2403.16597v1","category":"math.OA"}
{"created":"2024-03-25 10:09:03","title":"TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques","abstract":"The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries. However, this capability has raised concerns regarding misinformation and personal information leakage. In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts. Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches. We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods. Our methods obtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies.","sentences":["The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries.","However, this capability has raised concerns regarding misinformation and personal information leakage.","In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts.","Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches.","We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods.","Our methods obtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies."],"url":"http://arxiv.org/abs/2403.16592v1","category":"cs.CL"}
{"created":"2024-03-25 10:06:45","title":"Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy","abstract":"The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP established under uniform prior distribution. These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\\xi$-LDP also confers $\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions.","sentences":["The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP).","Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge.","In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs.","We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness.","Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP established under uniform prior distribution.","These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\\xi$-LDP also confers $\\xi$-MBP, and vice versa.","Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions."],"url":"http://arxiv.org/abs/2403.16591v1","category":"cs.LG"}
{"created":"2024-03-25 10:03:46","title":"Extremal properties of max-autoregressive moving average processes for modelling extreme river flows","abstract":"Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models. River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay. Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows. This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes. We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process. We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold. We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable. We illustrate results for river flow data from the UK River Thames.","sentences":["Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models.","River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay.","Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows.","This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes.","We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process.","We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold.","We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable.","We illustrate results for river flow data from the UK River Thames."],"url":"http://arxiv.org/abs/2403.16590v1","category":"stat.ME"}
{"created":"2024-03-25 09:56:59","title":"Intrinsic Dipole Hall effect in tMoTe$_2$ moir\u00e9: magnetoelectricity and contact-free signature of topological transitions","abstract":"We discover an intrinsic dipole Hall effect in a variety of magnetic insulating states at integer fillings of twisted MoTe$_2$ moir\\'e superlattice, including topologically trivial and nontrivial ferro-, antiferro-, and ferri-magnetic configurations. The dipole Hall current, in linear response to in-plane electric field, generates an in-plane orbital magnetization $M_{\\parallel}$ along the field, through which an AC field can drive magnetization oscillation up to THz range. Upon the continuous topological phase transitions from trivial to quantum anomalous Hall states in both ferromagnetic and antiferromagnetic configurations, the dipole Hall current and $M_{\\parallel}$ have an abrupt sign change, enabling contact free detection of the transitions through the magnetic stray field. In configurations where the linear response is forbidden by symmetry, the dipole Hall current and $M_{\\parallel}$ appear as a crossed nonlinear response to both in-plane and out-of-plane electric fields. These magnetoelectric phenomena showcase novel functionalities of insulators from the interplay between magnetism, topology and electrical polarization.","sentences":["We discover an intrinsic dipole Hall effect in a variety of magnetic insulating states at integer fillings of twisted MoTe$_2$ moir\\'e superlattice, including topologically trivial and nontrivial ferro-, antiferro-, and ferri-magnetic configurations.","The dipole Hall current, in linear response to in-plane electric field, generates an in-plane orbital magnetization $M_{\\parallel}$ along the field, through which an AC field can drive magnetization oscillation up to THz range.","Upon the continuous topological phase transitions from trivial to quantum anomalous Hall states in both ferromagnetic and antiferromagnetic configurations, the dipole Hall current and $M_{\\parallel}$ have an abrupt sign change, enabling contact free detection of the transitions through the magnetic stray field.","In configurations where the linear response is forbidden by symmetry, the dipole Hall current and $M_{\\parallel}$ appear as a crossed nonlinear response to both in-plane and out-of-plane electric fields.","These magnetoelectric phenomena showcase novel functionalities of insulators from the interplay between magnetism, topology and electrical polarization."],"url":"http://arxiv.org/abs/2403.16586v1","category":"cond-mat.str-el"}
{"created":"2024-03-25 09:50:37","title":"Amino Acids and Their Biological Derivatives Modulate Protein-Protein Interactions In an Additive Way","abstract":"Protein-protein interactions (PPI) differ when measured in test tubes and cells due to the complexity of the intracellular environment. Free amino acids (AAs) and their derivatives constitute a significant fraction of the intracellular volume and mass. Recently, we have found that AAs have a general property of rendering protein dispersions more stable by reducing the net attractive part of PPI. Here, we study the effects on PPI of different AA derivatives, AA mixtures, and short peptides. We find that all the tested AA derivatives modulate PPI in solution as well as AAs. Furthermore, we show that the modulation effect is additive when AAs form mixtures or are bound into short peptides. Therefore, this study demonstrates the universal effect of a class of small molecules (i.e. AAs and their biological derivatives) on the modulation of PPI and provides insights into rationally designing biocompatible molecules for stabilizing protein interactions and consequently tuning protein functions.","sentences":["Protein-protein interactions (PPI) differ when measured in test tubes and cells due to the complexity of the intracellular environment.","Free amino acids (AAs) and their derivatives constitute a significant fraction of the intracellular volume and mass.","Recently, we have found that AAs have a general property of rendering protein dispersions more stable by reducing the net attractive part of PPI.","Here, we study the effects on PPI of different AA derivatives, AA mixtures, and short peptides.","We find that all the tested AA derivatives modulate PPI in solution as well as AAs.","Furthermore, we show that the modulation effect is additive when AAs form mixtures or are bound into short peptides.","Therefore, this study demonstrates the universal effect of a class of small molecules (i.e. AAs and their biological derivatives) on the modulation of PPI and provides insights into rationally designing biocompatible molecules for stabilizing protein interactions and consequently tuning protein functions."],"url":"http://arxiv.org/abs/2403.16583v1","category":"physics.bio-ph"}
{"created":"2024-03-25 09:49:42","title":"In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data","abstract":"Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures (LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data. We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases. Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought. To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task. We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach.","sentences":["Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration.","When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary.","Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction.","However, they face substantial challenges when dealing with multiple input patterns.","The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions.","In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications.","We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures (LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations.","The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data.","We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases.","Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought.","To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task.","We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach."],"url":"http://arxiv.org/abs/2403.16582v1","category":"cs.LG"}
{"created":"2024-03-25 09:46:47","title":"Experimental demonstration of a thermal-EM concentrator for enhancing EM signals and converging heat fluxes simultaneously","abstract":"Simultaneously concentrating EM waves and heat fluxes to the same target region within an on-chip system carries substantial academic research importance and practical application value. Nevertheless, existing researches are primarily aimed at the design and experimentation of concentrators for individual EM waves or temperature fields. In this work, a thermal-EM concentrator, capable of simultaneously concentrating EM waves and heat fluxes, is designed using transformation optics/thermodynamics and fabricated with engineered EM-thermal metamaterials. The concentrating effects of the proposed thermal-EM concentrator on the thermal fluxes and EM waves are verified through numerical simulations and experimental measurements, respectively, which are in good agreement with each other. Both numerically simulated and experimentally measured results demonstrate the concentrating capability of the proposed thermal-EM concentrator, which can concentrate broadband TM-polarized EM waves ranging from 8-12 GHz and heat/cold flows to the same target region within an on-chip operating environment. The thermal-EM concentrator exhibits a thermal focusing efficiency close to 100% and more than three times enhancement of the magnetic field at the designed center frequency of 10 GHz. The proposed thermal-EM concentrator can be utilized for efficient cooling for the specified component and simultaneously enhancing the EM antenna's radiation/reception efficiency within an on-chip system.","sentences":["Simultaneously concentrating EM waves and heat fluxes to the same target region within an on-chip system carries substantial academic research importance and practical application value.","Nevertheless, existing researches are primarily aimed at the design and experimentation of concentrators for individual EM waves or temperature fields.","In this work, a thermal-EM concentrator, capable of simultaneously concentrating EM waves and heat fluxes, is designed using transformation optics/thermodynamics and fabricated with engineered EM-thermal metamaterials.","The concentrating effects of the proposed thermal-EM concentrator on the thermal fluxes and EM waves are verified through numerical simulations and experimental measurements, respectively, which are in good agreement with each other.","Both numerically simulated and experimentally measured results demonstrate the concentrating capability of the proposed thermal-EM concentrator, which can concentrate broadband TM-polarized EM waves ranging from 8-12 GHz and heat/cold flows to the same target region within an on-chip operating environment.","The thermal-EM concentrator exhibits a thermal focusing efficiency close to 100% and more than three times enhancement of the magnetic field at the designed center frequency of 10 GHz.","The proposed thermal-EM concentrator can be utilized for efficient cooling for the specified component and simultaneously enhancing the EM antenna's radiation/reception efficiency within an on-chip system."],"url":"http://arxiv.org/abs/2403.16579v1","category":"physics.gen-ph"}
{"created":"2024-03-25 09:43:56","title":"SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging","abstract":"Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks. This indicates that SegICL effectively address new segmentation tasks based on contextual information. Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks. Our code will be released soon.","sentences":["Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement.","Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance.","For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation.","Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset).","Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks.","This indicates that SegICL effectively address new segmentation tasks based on contextual information.","Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks.","Our code will be released soon."],"url":"http://arxiv.org/abs/2403.16578v1","category":"cs.CV"}
{"created":"2024-03-25 09:43:26","title":"Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems","abstract":"Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things. The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values. Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values. This highly degrades the physical properties of these applications while does not improve their functionality. To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience. The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values. This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block. The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation. The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a face recognition neural network are demonstrated by means of a wide range of simulation and synthesis results.","sentences":["Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things.","The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values.","Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values.","This highly degrades the physical properties of these applications while does not improve their functionality.","To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience.","The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values.","This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block.","The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation.","The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a face recognition neural network are demonstrated by means of a wide range of simulation and synthesis results."],"url":"http://arxiv.org/abs/2403.16577v1","category":"cs.AR"}
{"created":"2024-03-25 09:41:49","title":"Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization","abstract":"Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity, demonstrating the superiority of our approach.","sentences":["Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature.","In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality.","Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens.","Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference.","Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion.","Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity, demonstrating the superiority of our approach."],"url":"http://arxiv.org/abs/2403.16576v1","category":"q-bio.BM"}
{"created":"2024-03-25 09:40:21","title":"Rotating metrics and new multipole moments from scattering amplitudes in arbitrary dimensions","abstract":"We compute the vacuum metric generated by a generic rotating object in arbitrary dimensions up to third post-Minkowskian order by computing the classical contribution of scattering amplitudes describing the graviton emission by massive spin-1 particles up to two loops. The solution depends on the mass, angular momenta, and on up to two parameters related to generic quadrupole moments. In $D=4$ spacetime dimensions, we recover the vacuum Hartle-Thorne solution describing a generic spinning object to second order in the angular momentum, of which the Kerr metric is a particular case obtained for a specific mass quadrupole moment dictated by the uniqueness theorem. At the level of the effective action, the case of minimal couplings corresponds to the Kerr black hole, while any other mass quadrupole moment requires non-minimal couplings. In $D>4$, the absence of black-hole uniqueness theorems implies that there are multiple spinning black hole solutions with different topology. Using scattering amplitudes, we find a generic solution depending on the mass, angular momenta, the mass quadrupole moment, and a new stress quadrupole moment which does not exist in $D=4$. As special cases, we recover the Myers-Perry and the single-angular-momentum black ring solutions, to third and first post-Minkowksian order, respectively. Interestingly, at variance with the four dimensional case, none of these solutions corresponds to the minimal coupling in the effective action. This shows that, from the point of view of scattering amplitudes, black holes are the \"simplest\" General Relativity vacuum solutions only in $D=4$.","sentences":["We compute the vacuum metric generated by a generic rotating object in arbitrary dimensions up to third post-Minkowskian order by computing the classical contribution of scattering amplitudes describing the graviton emission by massive spin-1 particles up to two loops.","The solution depends on the mass, angular momenta, and on up to two parameters related to generic quadrupole moments.","In $D=4$ spacetime dimensions, we recover the vacuum Hartle-Thorne solution describing a generic spinning object to second order in the angular momentum, of which the Kerr metric is a particular case obtained for a specific mass quadrupole moment dictated by the uniqueness theorem.","At the level of the effective action, the case of minimal couplings corresponds to the Kerr black hole, while any other mass quadrupole moment requires non-minimal couplings.","In $D>4$, the absence of black-hole uniqueness theorems implies that there are multiple spinning black hole solutions with different topology.","Using scattering amplitudes, we find a generic solution depending on the mass, angular momenta, the mass quadrupole moment, and a new stress quadrupole moment which does not exist in $D=4$. As special cases, we recover the Myers-Perry and the single-angular-momentum black ring solutions, to third and first post-Minkowksian order, respectively.","Interestingly, at variance with the four dimensional case, none of these solutions corresponds to the minimal coupling in the effective action.","This shows that, from the point of view of scattering amplitudes, black holes are the \"simplest\" General Relativity vacuum solutions only in $D=4$."],"url":"http://arxiv.org/abs/2403.16574v1","category":"hep-th"}
{"created":"2024-03-25 09:36:51","title":"NSINA: A News Corpus for Sinhala","abstract":"The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.","sentences":["The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources.","This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets.","In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation.","The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language.","NSINA is the largest news corpus for Sinhala, available up to date."],"url":"http://arxiv.org/abs/2403.16571v1","category":"cs.CL"}
{"created":"2024-03-25 09:36:20","title":"Spectropolarimetry of Fraunhofer lines in local upper solar atmosphere","abstract":"Spectropolarimetric results of Fraunhofer lines between 516.3nm and 532.6nm are presented in local upper solar chromosphere, transition zone and inner corona below a height of about 0.04 solar radius above the solar limb. The data were acquired on Nov.3, 2013 during a total solar eclipse in Gabon by the prototype Fiber Arrayed Solar Optical Telescope(FASOT). It is found that the polarization amplitudes of the Fraunhofer lines in these layers depend strongly on specific spectral lines. Fraunhofer line at MgI$b_{1}$518.4nm can have a polarization amplitude up to 0.36$\\%$ with respective to the continuum polarization level, while the polarizations of some lines like FeI/CrI524.7nm and FeI525.0nm are often under the detection limit 6.0$\\times 10^{-4}$. The polarizations of the Fraunhofer lines, like the emission lines and the continuum, increase with height as a whole trend. The fractional linear polarization amplitudes of inner F-corona can be close to those of inner E-corona, and in general larger than those of inner K-corona. Rotation of the polarization direction of Fraunhofer line is often accompanied with variations in their polarization amplitudes and profile shapes. It is also judged from these polarimetric properties, along with evidences, that neutral atoms exist in these atmospheric layers. Thus the inner F-corona described here is induced by the neutral atoms, and the entropy of the inner corona evaluated becomes larger than those in the underneath layers due to more microstates found.","sentences":["Spectropolarimetric results of Fraunhofer lines between 516.3nm and 532.6nm are presented in local upper solar chromosphere, transition zone and inner corona below a height of about 0.04 solar radius above the solar limb.","The data were acquired on Nov.3, 2013 during a total solar eclipse in Gabon by the prototype Fiber Arrayed Solar Optical Telescope(FASOT).","It is found that the polarization amplitudes of the Fraunhofer lines in these layers depend strongly on specific spectral lines.","Fraunhofer line at MgI$b_{1}$518.4nm can have a polarization amplitude up to 0.36$\\%$ with respective to the continuum polarization level, while the polarizations of some lines like FeI/CrI524.7nm and FeI525.0nm are often under the detection limit 6.0$\\times 10^{-4}$.","The polarizations of the Fraunhofer lines, like the emission lines and the continuum, increase with height as a whole trend.","The fractional linear polarization amplitudes of inner F-corona can be close to those of inner E-corona, and in general larger than those of inner K-corona.","Rotation of the polarization direction of Fraunhofer line is often accompanied with variations in their polarization amplitudes and profile shapes.","It is also judged from these polarimetric properties, along with evidences, that neutral atoms exist in these atmospheric layers.","Thus the inner F-corona described here is induced by the neutral atoms, and the entropy of the inner corona evaluated becomes larger than those in the underneath layers due to more microstates found."],"url":"http://arxiv.org/abs/2403.16570v1","category":"astro-ph.SR"}
{"created":"2024-03-25 09:36:10","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors","abstract":"Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks.","sentences":["Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks.","Nonetheless, these techniques could potentially generate misleading explanations.","Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy.","It poses a serious challenge in ensuring the reliability of XAI methods.","To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks.","We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training.","The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks."],"url":"http://arxiv.org/abs/2403.16569v1","category":"cs.LG"}
{"created":"2024-03-25 09:32:29","title":"Decoupling parameter variation from noise: Biquadratic Lyapunov forms in data-driven LPV control","abstract":"A promising step from linear towards nonlinear data-driven control is via the design of controllers for linear parameter-varying (LPV) systems, which are linear systems whose parameters are varying along a measurable scheduling signal. However, the interplay between uncertainty arising from corrupted data and the parameter-varying nature of these systems impacts the stability analysis, and limits the generalization of well-understood data-driven methods for linear time-invariant systems. In this work, we decouple this interplay using a recently developed variant of the Fundamental Lemma for LPV systems and the viewpoint of data-informativity, in combination with biquadratic Lyapunov forms. Together, these allow us to develop novel linear matrix inequality conditions for the existence of scheduling-dependent Lyapunov functions, incorporating the intrinsic nonlinearity. Appealingly, these results are stated purely in terms of the collected data and bounds on the noise, and they are computationally favorable to check.","sentences":["A promising step from linear towards nonlinear data-driven control is via the design of controllers for linear parameter-varying (LPV) systems, which are linear systems whose parameters are varying along a measurable scheduling signal.","However, the interplay between uncertainty arising from corrupted data and the parameter-varying nature of these systems impacts the stability analysis, and limits the generalization of well-understood data-driven methods for linear time-invariant systems.","In this work, we decouple this interplay using a recently developed variant of the Fundamental Lemma for LPV systems and the viewpoint of data-informativity, in combination with biquadratic Lyapunov forms.","Together, these allow us to develop novel linear matrix inequality conditions for the existence of scheduling-dependent Lyapunov functions, incorporating the intrinsic nonlinearity.","Appealingly, these results are stated purely in terms of the collected data and bounds on the noise, and they are computationally favorable to check."],"url":"http://arxiv.org/abs/2403.16565v1","category":"eess.SY"}
{"created":"2024-03-25 09:32:09","title":"Molecular Communication-Based Intelligent Dopamine Rate Modulator for Parkinson's Disease Treatment","abstract":"Parkinson's disease (PD) is a progressive neurodegenerative disease, and it is caused by the loss of dopaminergic neurons in the basal ganglia (BG). Currently, there is no definite cure for PD, and available treatments mainly aim to alleviate its symptoms. Due to impaired neurotransmitter-based information transmission in PD, molecular communication-based approaches can be employed as potential solutions to address this issue. Molecular Communications (MC) is a bio-inspired communication method utilizing molecules for carrying information. This mode of communication stands out for developing bio-compatible nanomachines for diagnosing and treating, particularly in addressing neurodegenerative diseases like PD, due to its compatibility with biological systems. This study presents a novel treatment method that introduces an Intelligent Dopamine Rate Modulator (IDRM), which is located in the synaptic gap between the substantia nigra pars compacta (SNc) and striatum to compensate for insufficiency dopamine release in BG caused by PD. For storing dopamine in the IDRM, dopamine compound (DAC) is swallowed and crossed through the digestive system, blood circulatory system, blood-brain barrier (BBB), and brain extracellular matrix uptakes with IDRMs. Here, the DAC concentration is calculated in these regions, revealing that the required exogenous dopamine consistently reaches IDRM. Therefore, the perpetual dopamine insufficiency in BG associated with PD can be compensated. This method reduces drug side effects because dopamine is not released in other brain regions. Unlike other treatments, this approach targets the root cause of PD rather than just reducing symptoms.","sentences":["Parkinson's disease (PD) is a progressive neurodegenerative disease, and it is caused by the loss of dopaminergic neurons in the basal ganglia (BG).","Currently, there is no definite cure for PD, and available treatments mainly aim to alleviate its symptoms.","Due to impaired neurotransmitter-based information transmission in PD, molecular communication-based approaches can be employed as potential solutions to address this issue.","Molecular Communications (MC) is a bio-inspired communication method utilizing molecules for carrying information.","This mode of communication stands out for developing bio-compatible nanomachines for diagnosing and treating, particularly in addressing neurodegenerative diseases like PD, due to its compatibility with biological systems.","This study presents a novel treatment method that introduces an Intelligent Dopamine Rate Modulator (IDRM), which is located in the synaptic gap between the substantia nigra pars compacta (SNc) and striatum to compensate for insufficiency dopamine release in BG caused by PD.","For storing dopamine in the IDRM, dopamine compound (DAC) is swallowed and crossed through the digestive system, blood circulatory system, blood-brain barrier (BBB), and brain extracellular matrix uptakes with IDRMs.","Here, the DAC concentration is calculated in these regions, revealing that the required exogenous dopamine consistently reaches IDRM.","Therefore, the perpetual dopamine insufficiency in BG associated with PD can be compensated.","This method reduces drug side effects because dopamine is not released in other brain regions.","Unlike other treatments, this approach targets the root cause of PD rather than just reducing symptoms."],"url":"http://arxiv.org/abs/2403.16564v1","category":"cs.ET"}
{"created":"2024-03-25 09:24:34","title":"Phantom scalar field cosmologies constrained by early cosmic measurements","abstract":"In this work, we explore new constraints on phantom scalar field cosmologies with a scalar field employing early times catalogues related to CMB measurements, along with the local standard observables, like Supernovae Type Ia (SNIa), $H(z)$ measurements (Cosmic Clocks), and Baryon Acoustic Oscillations (BAO) baselines. In particular, we studied a tracker phantom field with hyperbolic polar coordinates that have been proposed in the literature. The main goal is to obtain precise cosmological constraints for $H_0$ and $\\sigma_8$, in comparison to other constructions that present tension in early cosmological parameters. Our results show that phantom scalar field cosmologies have a reduced statistical tension on $H_0$ that it is less than 3$\\sigma$ using model-independent CMB catalogues as SPT-3G+WMAP9 and ACTPol DR-4+WMAP9 baselines. This suggests these models using a different phantom potential might address the Hubble constant problem and reduce the systematics involved.","sentences":["In this work, we explore new constraints on phantom scalar field cosmologies with a scalar field employing early times catalogues related to CMB measurements, along with the local standard observables, like Supernovae Type Ia (SNIa), $H(z)$ measurements (Cosmic Clocks), and Baryon Acoustic Oscillations (BAO) baselines.","In particular, we studied a tracker phantom field with hyperbolic polar coordinates that have been proposed in the literature.","The main goal is to obtain precise cosmological constraints for $H_0$ and $\\sigma_8$, in comparison to other constructions that present tension in early cosmological parameters.","Our results show that phantom scalar field cosmologies have a reduced statistical tension on $H_0$ that it is less than 3$\\sigma$ using model-independent CMB catalogues as SPT-3G+WMAP9 and ACTPol DR-4+WMAP9 baselines.","This suggests these models using a different phantom potential might address the Hubble constant problem and reduce the systematics involved."],"url":"http://arxiv.org/abs/2403.16562v1","category":"gr-qc"}
{"created":"2024-03-25 09:24:05","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","abstract":"Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.","sentences":["Federated Learning (FL) heavily depends on label quality for its performance.","However, the label distribution among individual clients is always both noisy and heterogeneous.","The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches.","To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples.","In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance.","To mitigate overfitting, we address this concern from two perspectives.","Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise.","Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models.","We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets.","The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios."],"url":"http://arxiv.org/abs/2403.16561v1","category":"cs.LG"}
{"created":"2024-03-25 09:18:48","title":"Active Admittance Control with Iterative Learning for General-Purpose Contact-Rich Manipulation","abstract":"Force interaction is inevitable when robots face multiple operation scenarios. How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem. Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism. The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model. The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility. Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method. Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively.","sentences":["Force interaction is inevitable when robots face multiple operation scenarios.","How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem.","Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism.","The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model.","The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility.","Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method.","Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively."],"url":"http://arxiv.org/abs/2403.16560v1","category":"cs.RO"}
{"created":"2024-03-25 09:17:15","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","abstract":"Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models.","sentences":["Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied.","This lack of exploration is primarily due to two key challenges.","Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships.","Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.","To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG).","ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions.","Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge.","Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models."],"url":"http://arxiv.org/abs/2403.16558v1","category":"cs.CV"}
{"created":"2024-03-25 09:10:57","title":"Hybrid low-dimensional limiting state of charge estimator for multi-cell lithium-ion batteries","abstract":"The state of charge (SOC) of lithium-ion batteries needs to be accurately estimated for safety and reliability purposes. For battery packs made of a large number of cells, it is not always feasible to design one SOC estimator per cell due to limited computational resources. Instead, only the minimum and the maximum SOC need to be estimated. The challenge is that the cells having minimum and maximum SOC typically change over time. In this context, we present a low-dimensional hybrid estimator of the minimum (maximum) SOC, whose convergence is analytically guaranteed. We consider for this purpose a battery consisting of cells interconnected in series, which we model by electric equivalent circuit models. We then present the hybrid estimator, which runs an observer designed for a single cell at any time instant, selected by a switching-like logic mechanism. We establish a practical exponential stability property for the estimation error on the minimum (maximum) SOC thereby guaranteeing the ability of the hybrid scheme to generate accurate estimates of the minimum (maximum) SOC. The analysis relies on non-smooth hybrid Lyapunov techniques. A numerical illustration is provided to showcase the relevance of the proposed approach.","sentences":["The state of charge (SOC) of lithium-ion batteries needs to be accurately estimated for safety and reliability purposes.","For battery packs made of a large number of cells, it is not always feasible to design one SOC estimator per cell due to limited computational resources.","Instead, only the minimum and the maximum SOC need to be estimated.","The challenge is that the cells having minimum and maximum SOC typically change over time.","In this context, we present a low-dimensional hybrid estimator of the minimum (maximum) SOC, whose convergence is analytically guaranteed.","We consider for this purpose a battery consisting of cells interconnected in series, which we model by electric equivalent circuit models.","We then present the hybrid estimator, which runs an observer designed for a single cell at any time instant, selected by a switching-like logic mechanism.","We establish a practical exponential stability property for the estimation error on the minimum (maximum) SOC thereby guaranteeing the ability of the hybrid scheme to generate accurate estimates of the minimum (maximum) SOC.","The analysis relies on non-smooth hybrid Lyapunov techniques.","A numerical illustration is provided to showcase the relevance of the proposed approach."],"url":"http://arxiv.org/abs/2403.16555v1","category":"eess.SY"}
{"created":"2024-03-25 09:04:14","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","abstract":"The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate the effectiveness of our approach.","sentences":["The black-box nature of deep learning models in NLP hinders their widespread application.","The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions.","Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations.","In this work, we introduce a novel method, namely Poincar\\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity.","Inspired by Poincar\\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures.","Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm.","Experimental results demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.16554v1","category":"cs.CL"}
{"created":"2024-03-25 08:57:27","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","abstract":"Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training. QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer","sentences":["Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance.","However, existing models in this domain still suffer from suboptimal performance.","We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity.","ii)","We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation.","iii)","We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers.","Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training.","QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets.","Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%.","To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer"],"url":"http://arxiv.org/abs/2403.16552v1","category":"cs.NE"}
{"created":"2024-03-25 08:54:18","title":"Coupling elastic media to gravitational waves: an effective field theory approach","abstract":"The interaction of a gravitational wave (GW) with an elastic body is usually described in terms of a GW \"force\" driving the oscillations of the body's normal modes. However, this description is only possible for GW frequencies for which the response of the elastic body is dominated by a few normal modes. At higher frequencies the normal modes blend into a quasi-continuum and a field-theoretical description, as pioneered by Dyson already in 1969, becomes necessary. However, since the metric perturbation $h_{\\mu\\nu}$ is an intrinsically relativistic object, a consistent coupling to GWs can only be obtained within a relativistic (and, in fact generally covariant) theory of elasticity. We develop such a formalism using the methods of modern effective field theories, and we use it to provide a derivation of the interaction of elastic bodies with GWs valid also in the high-frequency regime, providing a first-principle derivation of Dyson's result (and partially correcting it). We also stress that the field-theoretical results are obtained working in the TT frame, while the description in terms of a force driving the normal modes is only valid in the proper detector frame. We show how to transform the results between the two frames. Beside an intrinsic conceptual interest, these results are relevant to the computation of the sensitivity of the recently proposed Lunar Gravitational Wave Antenna.","sentences":["The interaction of a gravitational wave (GW) with an elastic body is usually described in terms of a GW \"force\" driving the oscillations of the body's normal modes.","However, this description is only possible for GW frequencies for which the response of the elastic body is dominated by a few normal modes.","At higher frequencies the normal modes blend into a quasi-continuum and a field-theoretical description, as pioneered by Dyson already in 1969, becomes necessary.","However, since the metric perturbation $h_{\\mu\\nu}$ is an intrinsically relativistic object, a consistent coupling to GWs can only be obtained within a relativistic (and, in fact generally covariant) theory of elasticity.","We develop such a formalism using the methods of modern effective field theories, and we use it to provide a derivation of the interaction of elastic bodies with GWs valid also in the high-frequency regime, providing a first-principle derivation of Dyson's result (and partially correcting it).","We also stress that the field-theoretical results are obtained working in the TT frame, while the description in terms of a force driving the normal modes is only valid in the proper detector frame.","We show how to transform the results between the two frames.","Beside an intrinsic conceptual interest, these results are relevant to the computation of the sensitivity of the recently proposed Lunar Gravitational Wave Antenna."],"url":"http://arxiv.org/abs/2403.16550v1","category":"gr-qc"}
{"created":"2024-03-25 08:43:27","title":"Unipolar opical transitons in nanoclusters of ellipsoidal geometry","abstract":"The quantum states of an ellipsoidal nanocluster of a heterophase system InAs / GaAs are studied using an exact analytical approach, in contrast to the generally accepted theoretical model based on the adiabatic approximation. It is shown that the spectrum of a nanoobject is formed from local groups, consisting of discrete levels, separated by terahertz frequency intervals. A double random degeneration of certain spectrum states is revealed. Allowed mid-infrared (IR) optical transitions between different spectral states are analyzed. The role of dimensional parameters and features of the shape of a nanoobject in the characteristics of unipolar transitions is assessed. The absorption spectrum for the transition in the lower part of the substructure spectrum is calculated.","sentences":["The quantum states of an ellipsoidal nanocluster of a heterophase system InAs / GaAs are studied using an exact analytical approach, in contrast to the generally accepted theoretical model based on the adiabatic approximation.","It is shown that the spectrum of a nanoobject is formed from local groups, consisting of discrete levels, separated by terahertz frequency intervals.","A double random degeneration of certain spectrum states is revealed.","Allowed mid-infrared (IR) optical transitions between different spectral states are analyzed.","The role of dimensional parameters and features of the shape of a nanoobject in the characteristics of unipolar transitions is assessed.","The absorption spectrum for the transition in the lower part of the substructure spectrum is calculated."],"url":"http://arxiv.org/abs/2403.16546v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-25 08:38:50","title":"The Role of Mean Absolute Deviation Function in Obtaining Smooth Estimation for Distribution and Density Functions: Beta Regression Approach","abstract":"Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment. This article introduces a simple approach but novel for estimating both functions in one process to have smooth curves for both via left mean absolute deviation (MAD) function and beta regression approach. Our approach explores estimation of both functions by smoothing the first derivative of left MAD function to obtain the final optimal smooth estimates. The derivation for these final smooth estimates under conditions of nondecreasing distribution function and nonnegative density function are performed by applying beta regression of a polynomial degree on the first derivative of left MAD function where the degree of polynomial is chosen among the models that have less mean absolute residuals under the constraint of nonnegativity for the first derivative of regression vector of expected values. A general class of normal, logistic and Gumbel distributions is derived as proposed smooth estimators for the distribution and density functions using logit, probit and cloglog links, respectively. This approach is applied to simulated data from unimodal, bimodal, tri-modal and skew distributions and an application to real data set is given.","sentences":["Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment.","This article introduces a simple approach but novel for estimating both functions in one process to have smooth curves for both via left mean absolute deviation (MAD) function and beta regression approach.","Our approach explores estimation of both functions by smoothing the first derivative of left MAD function to obtain the final optimal smooth estimates.","The derivation for these final smooth estimates under conditions of nondecreasing distribution function and nonnegative density function are performed by applying beta regression of a polynomial degree on the first derivative of left MAD function where the degree of polynomial is chosen among the models that have less mean absolute residuals under the constraint of nonnegativity for the first derivative of regression vector of expected values.","A general class of normal, logistic and Gumbel distributions is derived as proposed smooth estimators for the distribution and density functions using logit, probit and cloglog links, respectively.","This approach is applied to simulated data from unimodal, bimodal, tri-modal and skew distributions and an application to real data set is given."],"url":"http://arxiv.org/abs/2403.16544v1","category":"stat.ME"}
{"created":"2024-03-25 08:36:06","title":"Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning","abstract":"Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where information is scarce. Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.","sentences":["Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification.","Representations of textual data extract rich information spanning the domain, entities, and relations.","In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning.","While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped.","To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.","Our method employs contrastive learning to extract complementary discriminative information from these individual representations.","This is particularly relevant in low-resource settings where information is scarce.","Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available.","We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints."],"url":"http://arxiv.org/abs/2403.16543v1","category":"cs.CL"}
{"created":"2024-03-25 08:19:07","title":"Uncovering faint lensed gravitational-wave signals and reprioritizing their follow-up analysis using galaxy lensing forecasts with detected counterparts","abstract":"Like light, gravitational waves can be gravitationally lensed by massive astrophysical objects. For galaxy and galaxy-cluster lenses, one expects to see strong lensing -- forecasted to become observable in the coming years -- where the original wave is split into multiple copies with the same frequency evolution but different overall arrival times, phases, amplitudes, and signal strengths. Some of these images can be below the detection threshold and require targeted search methods, based on tailor-made template banks. These searches can be made more sensitive by using our knowledge of the typical distribution and morphology of lenses to predict the time delay, magnification, and image-type ordering of the lensed images. Here, we show that when a subset of the images is super-threshold, they can be used to construct a more constrained prediction of the arrival time of the remaining signals, enhancing our ability to identify lensing candidate signals. Our suggested method effectively reduces the list of triggers requiring follow-up and generally re-ranks the genuine counterpart higher in the lensing candidate list. Therefore, in the future, if one observes two or three lensed images, the information they provide can be used to identify their sub-threshold counterparts, thus allowing identification of additional lensed images. Finding such images would also strengthen our evidence for the event being lensed.","sentences":["Like light, gravitational waves can be gravitationally lensed by massive astrophysical objects.","For galaxy and galaxy-cluster lenses, one expects to see strong lensing -- forecasted to become observable in the coming years -- where the original wave is split into multiple copies with the same frequency evolution but different overall arrival times, phases, amplitudes, and signal strengths.","Some of these images can be below the detection threshold and require targeted search methods, based on tailor-made template banks.","These searches can be made more sensitive by using our knowledge of the typical distribution and morphology of lenses to predict the time delay, magnification, and image-type ordering of the lensed images.","Here, we show that when a subset of the images is super-threshold, they can be used to construct a more constrained prediction of the arrival time of the remaining signals, enhancing our ability to identify lensing candidate signals.","Our suggested method effectively reduces the list of triggers requiring follow-up and generally re-ranks the genuine counterpart higher in the lensing candidate list.","Therefore, in the future, if one observes two or three lensed images, the information they provide can be used to identify their sub-threshold counterparts, thus allowing identification of additional lensed images.","Finding such images would also strengthen our evidence for the event being lensed."],"url":"http://arxiv.org/abs/2403.16532v1","category":"gr-qc"}
{"created":"2024-03-25 08:16:06","title":"An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models","abstract":"Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations. We perform experiments using a text-to-image generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fusion mechanism on two common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and 50% increased training speed compared to a strong U-ViT baseline with an early fusion.","sentences":["Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video.","However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc.","We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment.","We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations.","We perform experiments using a text-to-image generation task on the MS-COCO dataset.","We compare our intermediate fusion mechanism with the classic early fusion mechanism on two common conditioning methods on a U-shaped ViT backbone.","Our intermediate fusion model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and 50% increased training speed compared to a strong U-ViT baseline with an early fusion."],"url":"http://arxiv.org/abs/2403.16530v1","category":"cs.CV"}
{"created":"2024-03-25 08:15:08","title":"Exploit High-Dimensional RIS Information to Localization: What Is the Impact of Faulty Element?","abstract":"This paper proposes a novel localization algorithm using the reconfigurable intelligent surface (RIS) received signal, i.e., RIS information. Compared with BS received signal, i.e., BS information, RIS information offers higher dimension and richer feature set, thereby providing an enhanced capacity to distinguish positions of the mobile users (MUs). Additionally, we address a practical scenario where RIS contains some unknown (number and places) faulty elements that cannot receive signals. Initially, we employ transfer learning to design a two-phase transfer learning (TPTL) algorithm, designed for accurate detection of faulty elements. Then our objective is to regain the information lost from the faulty elements and reconstruct the complete high-dimensional RIS information for localization. To this end, we propose a transfer-enhanced dual-stage (TEDS) algorithm. In \\emph{Stage I}, we integrate the CNN and variational autoencoder (VAE) to obtain the RIS information, which in \\emph{Stage II}, is input to the transferred DenseNet 121 to estimate the location of the MU. To gain more insight, we propose an alternative algorithm named transfer-enhanced direct fingerprint (TEDF) algorithm which only requires the BS information. The comparison between TEDS and TEDF reveals the effectiveness of faulty element detection and the benefits of utilizing the high-dimensional RIS information for localization. Besides, our empirical results demonstrate that the performance of the localization algorithm is dominated by the high-dimensional RIS information and is robust to unoptimized phase shifts and signal-to-noise ratio (SNR).","sentences":["This paper proposes a novel localization algorithm using the reconfigurable intelligent surface (RIS) received signal, i.e., RIS information.","Compared with BS received signal, i.e., BS information, RIS information offers higher dimension and richer feature set, thereby providing an enhanced capacity to distinguish positions of the mobile users (MUs).","Additionally, we address a practical scenario where RIS contains some unknown (number and places) faulty elements that cannot receive signals.","Initially, we employ transfer learning to design a two-phase transfer learning (TPTL) algorithm, designed for accurate detection of faulty elements.","Then our objective is to regain the information lost from the faulty elements and reconstruct the complete high-dimensional RIS information for localization.","To this end, we propose a transfer-enhanced dual-stage (TEDS) algorithm.","In \\emph{Stage I}, we integrate the CNN and variational autoencoder (VAE) to obtain the RIS information, which in \\emph{Stage II}, is input to the transferred DenseNet 121 to estimate the location of the MU.","To gain more insight, we propose an alternative algorithm named transfer-enhanced direct fingerprint (TEDF) algorithm which only requires the BS information.","The comparison between TEDS and TEDF reveals the effectiveness of faulty element detection and the benefits of utilizing the high-dimensional RIS information for localization.","Besides, our empirical results demonstrate that the performance of the localization algorithm is dominated by the high-dimensional RIS information and is robust to unoptimized phase shifts and signal-to-noise ratio (SNR)."],"url":"http://arxiv.org/abs/2403.16529v1","category":"eess.SP"}
{"created":"2024-03-25 08:11:02","title":"Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art","abstract":"Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment. While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model's decision, and detect when it may be hallucinating. In this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.","sentences":["Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry.","The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based.","While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time.","The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing.","Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios.","Large language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment.","While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor.","We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model's decision, and detect when it may be hallucinating.","In this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field."],"url":"http://arxiv.org/abs/2403.16527v1","category":"cs.AI"}
{"created":"2024-03-25 08:09:04","title":"Deep Learning Based Measure of Name Concentration Risk","abstract":"We propose a new deep learning approach for the quantification of name concentration risk in loan portfolios. Our approach is tailored for small portfolios and allows for both an actuarial as well as a mark-to-market definition of loss. The training of our neural network relies on Monte Carlo simulations with importance sampling which we explicitly formulate for the CreditRisk${+}$ and the ratings-based CreditMetrics model. Numerical results based on simulated as well as real data demonstrate the accuracy of our new approach and its superior performance compared to existing analytical methods for assessing name concentration risk in small and concentrated portfolios.","sentences":["We propose a new deep learning approach for the quantification of name concentration risk in loan portfolios.","Our approach is tailored for small portfolios and allows for both an actuarial as well as a mark-to-market definition of loss.","The training of our neural network relies on Monte Carlo simulations with importance sampling which we explicitly formulate for the CreditRisk${+}$ and the ratings-based CreditMetrics model.","Numerical results based on simulated as well as real data demonstrate the accuracy of our new approach and its superior performance compared to existing analytical methods for assessing name concentration risk in small and concentrated portfolios."],"url":"http://arxiv.org/abs/2403.16525v1","category":"q-fin.RM"}
{"created":"2024-03-25 08:09:01","title":"Harnessing the power of LLMs for normative reasoning in MASs","abstract":"Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning. The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making. This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research. We present our vision for creating normative LLM agents. In particular, we discuss how the recently proposed \"LLM agent\" approaches can be extended to implement such normative LLM agents. We also highlight challenges in this emerging field. This paper thus aims to foster collaboration between MAS, NLP and LLM researchers in order to advance the field of normative agents.","sentences":["Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals.","In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents.","However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning.","The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making.","This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research.","We present our vision for creating normative LLM agents.","In particular, we discuss how the recently proposed \"LLM agent\" approaches can be extended to implement such normative LLM agents.","We also highlight challenges in this emerging field.","This paper thus aims to foster collaboration between MAS, NLP and LLM researchers in order to advance the field of normative agents."],"url":"http://arxiv.org/abs/2403.16524v1","category":"cs.AI"}
{"created":"2024-03-25 08:06:08","title":"Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis","abstract":"Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and has at least two directed paths to $Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants. Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant. With the path information, causal order is identifiable under some graphical conditions. A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method.","sentences":["Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios.","One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise.","For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution.","However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent.","Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and has at least two directed paths to $Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants.","Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant.","With the path information, causal order is identifiable under some graphical conditions.","A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.16523v1","category":"stat.ML"}
{"created":"2024-03-25 08:05:08","title":"Galactic dynamics in the presence of scalaron: A perspective from $\\boldsymbol{f(R)}$ gravity","abstract":"We consider $f(R)$ modified gravity with the chameleon mechanism as an alternative approach to address the dark matter issue on the galactic scale. Metric formalism of $f(R)$ theory is considered in this study. A mathematical transformation tool called conformal transformation which transforms the action from Jordan to Einstein frame is employed to simplify the fourth-order modified field equation and to describe the extra degree of freedom $f_R$ by using a minimally coupled scalar field (scalaron) showing the chameleonic character. Then, we examine the viability of a newly introduced $f(R)$ gravity model on behalf of the chameleonic behavior of the scalaron. The model analyzes this behavior of the scalaron successfully with the singularity correction. Further, we consider a test particle (star) in a static, spherically symmetric spacetime to investigate the importance of the scalaron in galactic dynamics. In the non-relativistic limit, the rotational velocity equation for the particle with scalaron contribution is derived. This contribution is found to be model dependent. We generate the rotation curves using the velocity equation and fit the predicted curves to observational data of a set of thirty seven sample galaxies of different categories. The curves are fitted based on two fitting parameters $M_0$ and $r_c$. The fitting shows good agreement of the prediction with the observed data.","sentences":["We consider $f(R)$ modified gravity with the chameleon mechanism as an alternative approach to address the dark matter issue on the galactic scale.","Metric formalism of $f(R)$ theory is considered in this study.","A mathematical transformation tool called conformal transformation which transforms the action from Jordan to Einstein frame is employed to simplify the fourth-order modified field equation and to describe the extra degree of freedom $f_R$ by using a minimally coupled scalar field (scalaron) showing the chameleonic character.","Then, we examine the viability of a newly introduced $f(R)$ gravity model on behalf of the chameleonic behavior of the scalaron.","The model analyzes this behavior of the scalaron successfully with the singularity correction.","Further, we consider a test particle (star) in a static, spherically symmetric spacetime to investigate the importance of the scalaron in galactic dynamics.","In the non-relativistic limit, the rotational velocity equation for the particle with scalaron contribution is derived.","This contribution is found to be model dependent.","We generate the rotation curves using the velocity equation and fit the predicted curves to observational data of a set of thirty seven sample galaxies of different categories.","The curves are fitted based on two fitting parameters $M_0$ and $r_c$. The fitting shows good agreement of the prediction with the observed data."],"url":"http://arxiv.org/abs/2403.16522v1","category":"gr-qc"}
{"created":"2024-03-25 08:03:33","title":"Employing High-Dimensional RIS Information for RIS-aided Localization Systems","abstract":"Reconfigurable intelligent surface (RIS)-aided localization systems have attracted extensive research attention due to their accuracy enhancement capabilities. However, most studies primarily utilized the base stations (BS) received signal, i.e., BS information, for localization algorithm design, neglecting the potential of RIS received signal, i.e., RIS information. Compared with BS information, RIS information offers higher dimension and richer feature set, thereby significantly improving the ability to extract positions of the mobile users (MUs). Addressing this oversight, this paper explores the algorithm design based on the high-dimensional RIS information. Specifically, we first propose a RIS information reconstruction (RIS-IR) algorithm to reconstruct the high-dimensional RIS information from the low-dimensional BS information. The proposed RIS-IR algorithm comprises a data processing module for preprocessing BS information, a convolution neural network (CNN) module for feature extraction, and an output module for outputting the reconstructed RIS information. Then, we propose a transfer learning based fingerprint (TFBF) algorithm that employs the reconstructed high-dimensional RIS information for MU localization. This involves adapting a pre-trained DenseNet-121 model to map the reconstructed RIS signal to the MU's three-dimensional (3D) position. Empirical results affirm that the localization performance is significantly influenced by the high-dimensional RIS information and maintains robustness against unoptimized phase shifts.","sentences":["Reconfigurable intelligent surface (RIS)-aided localization systems have attracted extensive research attention due to their accuracy enhancement capabilities.","However, most studies primarily utilized the base stations (BS) received signal, i.e., BS information, for localization algorithm design, neglecting the potential of RIS received signal, i.e., RIS information.","Compared with BS information, RIS information offers higher dimension and richer feature set, thereby significantly improving the ability to extract positions of the mobile users (MUs).","Addressing this oversight, this paper explores the algorithm design based on the high-dimensional RIS information.","Specifically, we first propose a RIS information reconstruction (RIS-IR) algorithm to reconstruct the high-dimensional RIS information from the low-dimensional BS information.","The proposed RIS-IR algorithm comprises a data processing module for preprocessing BS information, a convolution neural network (CNN) module for feature extraction, and an output module for outputting the reconstructed RIS information.","Then, we propose a transfer learning based fingerprint (TFBF) algorithm that employs the reconstructed high-dimensional RIS information for MU localization.","This involves adapting a pre-trained DenseNet-121 model to map the reconstructed RIS signal to the MU's three-dimensional (3D) position.","Empirical results affirm that the localization performance is significantly influenced by the high-dimensional RIS information and maintains robustness against unoptimized phase shifts."],"url":"http://arxiv.org/abs/2403.16521v1","category":"eess.SP"}
{"created":"2024-03-25 08:01:33","title":"Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study","abstract":"Norms are an important component of the social fabric of society by prescribing expected behaviour. In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust. Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning. However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness. In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations. This paper evaluates the capability of LLMs to detecting norm violations. Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated. For our evaluations we first obtained the ground truth from three human evaluators for each story. Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4). Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind. Also, we identify areas where these models perform poorly and discuss implications for future work.","sentences":["Norms are an important component of the social fabric of society by prescribing expected behaviour.","In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust.","Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning.","However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness.","In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations.","This paper evaluates the capability of LLMs to detecting norm violations.","Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated.","For our evaluations we first obtained the ground truth from three human evaluators for each story.","Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4).","Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind.","Also, we identify areas where these models perform poorly and discuss implications for future work."],"url":"http://arxiv.org/abs/2403.16517v1","category":"cs.MA"}
{"created":"2024-03-25 08:00:43","title":"Visually Guided Generative Text-Layout Pre-training for Document Intelligence","abstract":"Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering.","sentences":["Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells).","To this end, we propose visually guided generative text-layout pre-training, named ViTLP.","Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence.","In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length.","ViTLP can function as a native OCR model to localize and recognize texts of document images.","Besides, ViTLP can be effectively applied to various downstream VDU tasks.","Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering."],"url":"http://arxiv.org/abs/2403.16516v1","category":"cs.CL"}
{"created":"2024-03-25 07:58:58","title":"Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models","abstract":"In the last few years, generative models have shown their powerful capabilities in synthesizing realistic images in both quality and diversity (i.e., facial images, and natural subjects). Unfortunately, the artifact patterns in fake images synthesized by different generative models are inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake. In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties. In this paper, we employ natural traces shared only by real images as an additional predictive target in the detector. Specifically, the natural traces are learned from the wild real images and we introduce extended supervised contrastive learning to bring them closer to real images and further away from fake ones. This motivates the detector to make decisions based on the proximity of images to the natural traces. To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising 6 GAN and 6 diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations. Experimental results show that our proposed method gives 96.1% mAP significantly outperforms the baselines. Extensive experiments conducted on the widely recognized platform Midjourney reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment. The source code and partial self-built dataset are available in supplementary material.","sentences":["In the last few years, generative models have shown their powerful capabilities in synthesizing realistic images in both quality and diversity (i.e., facial images, and natural subjects).","Unfortunately, the artifact patterns in fake images synthesized by different generative models are inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake.","In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties.","In this paper, we employ natural traces shared only by real images as an additional predictive target in the detector.","Specifically, the natural traces are learned from the wild real images and we introduce extended supervised contrastive learning to bring them closer to real images and further away from fake ones.","This motivates the detector to make decisions based on the proximity of images to the natural traces.","To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising 6 GAN and 6 diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations.","Experimental results show that our proposed method gives 96.1% mAP significantly outperforms the baselines.","Extensive experiments conducted on the widely recognized platform Midjourney reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment.","The source code and partial self-built dataset are available in supplementary material."],"url":"http://arxiv.org/abs/2403.16513v1","category":"cs.CV"}
{"created":"2024-03-25 07:55:29","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","abstract":"In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages.","sentences":["In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages.","Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish.","In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages.","Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment.","Moreover, we provide valuable insights into various facets of ICL for low-resource languages.","Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in.","Our work highlights the importance of advancing ICL research, particularly for low-resource languages."],"url":"http://arxiv.org/abs/2403.16512v1","category":"cs.CL"}
{"created":"2024-03-25 07:54:38","title":"Extremality of collections of sets with respect to general perturbations","abstract":"The paper proposes another extension of the extremal principle. A new extremality model involving arbitrary families of perturbations (deformations) of the given sets is studied. It generalizes the conventional model based on linear translations of the sets as well as its set-valued extensions. This approach leads to a more general and simpler version of fuzzy separation. We demonstrate the applicability of the new model to set-valued optimization problems, weakening the assumptions of the known results and streamlining their proofs.","sentences":["The paper proposes another extension of the extremal principle.","A new extremality model involving arbitrary families of perturbations (deformations) of the given sets is studied.","It generalizes the conventional model based on linear translations of the sets as well as its set-valued extensions.","This approach leads to a more general and simpler version of fuzzy separation.","We demonstrate the applicability of the new model to set-valued optimization problems, weakening the assumptions of the known results and streamlining their proofs."],"url":"http://arxiv.org/abs/2403.16511v1","category":"math.OC"}
{"created":"2024-03-25 07:54:18","title":"Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework","abstract":"Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \\url{https://github.com/ICTMCG/Make-Your-Anchor}.","sentences":["Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging.","In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements.","Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances.","We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances.","To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference.","Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos.","Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods.","Project page: \\url{https://github.com/ICTMCG/Make-Your-Anchor}."],"url":"http://arxiv.org/abs/2403.16510v1","category":"cs.CV"}
{"created":"2024-03-25 07:48:34","title":"Human Understanding AI Paper Challenge 2024 -- Dataset Design","abstract":"In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life. This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development.","sentences":["In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life.","This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development."],"url":"http://arxiv.org/abs/2403.16509v1","category":"cs.LG"}
{"created":"2024-03-25 07:47:52","title":"Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning","abstract":"Current approaches for learning for planning have yet to achieve competitive performance against classical planners in several domains, and have poor overall performance. In this work, we construct novel graph representations of lifted planning tasks and use the WL algorithm to generate features from them. These features are used with classical machine learning methods which have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster than the state-of-the-art deep learning for planning models. Our novel approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the $h^{\\text{FF}}$ heuristic in a fair competition setting. It also outperforms or ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on plan quality. WL-GOOSE is the first learning for planning model which achieves these feats. Furthermore, we study the connections between our novel WL feature generation method, previous theoretically flavoured learning architectures, and Description Logic Features for planning.","sentences":["Current approaches for learning for planning have yet to achieve competitive performance against classical planners in several domains, and have poor overall performance.","In this work, we construct novel graph representations of lifted planning tasks and use the WL algorithm to generate features from them.","These features are used with classical machine learning methods which have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster than the state-of-the-art deep learning for planning models.","Our novel approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the $h^{\\text{FF}}$ heuristic in a fair competition setting.","It also outperforms or ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on plan quality.","WL-GOOSE is the first learning for planning model which achieves these feats.","Furthermore, we study the connections between our novel WL feature generation method, previous theoretically flavoured learning architectures, and Description Logic Features for planning."],"url":"http://arxiv.org/abs/2403.16508v1","category":"cs.AI"}
{"created":"2024-03-25 07:34:42","title":"Learning To Guide Human Decision Makers With Vision-Language Models","abstract":"There is increasing interest in developing AIs for assisting human decision making in \\textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   %   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   %   This \\textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine's decisions due to \\textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   %   As a remedy, we introduce \\textit{learning to guide} (LTG), an alternative framework in which -- rather than taking control from the human expert -- the machine provides \\textit{guidance} useful for decision making, and the human is entirely responsible for coming up with a decision.   %   In order to ensure guidance is \\textit{interpretable} and \\textit{task-specific}, we develop \\method, an approach for turning \\textit{any} vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback.   %   Our empirical evaluation highlights the promise of \\method on a challenging, real-world medical diagnosis task.","sentences":["There is increasing interest in developing AIs for assisting human decision making in \\textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   ","%   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   ","%   This \\textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios.","On the one hand, the expert may end up over-relying on the machine's decisions due to \\textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI.","On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   ","%   As a remedy, we introduce \\textit{learning to guide} (LTG), an alternative framework in which -- rather than taking control from the human expert -- the machine provides \\textit{guidance} useful for decision making, and the human is entirely responsible for coming up with a decision.   ","%   ","In order to ensure guidance is \\textit{interpretable} and \\textit{task-specific}, we develop \\method, an approach for turning \\textit{any} vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback.   ","%   Our empirical evaluation highlights the promise of \\method on a challenging, real-world medical diagnosis task."],"url":"http://arxiv.org/abs/2403.16501v1","category":"cs.AI"}
{"created":"2024-03-25 07:34:06","title":"Self-Supervised Learning for Medical Image Data with Anatomy-Oriented Imaging Planes","abstract":"Self-supervised learning has emerged as a powerful tool for pretraining deep networks on unlabeled data, prior to transfer learning of target tasks with limited annotation. The relevance between the pretraining pretext and target tasks is crucial to the success of transfer learning. Various pretext tasks have been proposed to utilize properties of medical image data (e.g., three dimensionality), which are more relevant to medical image analysis than generic ones for natural images. However, previous work rarely paid attention to data with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance imaging views. As these imaging planes are defined according to the anatomy of the imaged organ, pretext tasks effectively exploiting this information can pretrain the networks to gain knowledge on the organ of interest. In this work, we propose two complementary pretext tasks for this group of medical image data based on the spatial relationship of the imaging planes. The first is to learn the relative orientation between the imaging planes and implemented as regressing their intersecting lines. The second exploits parallel imaging planes to regress their relative slice locations within a stack. Both pretext tasks are conceptually straightforward and easy to implement, and can be combined in multitask learning for better representation learning. Thorough experiments on two anatomical structures (heart and knee) and representative target tasks (semantic segmentation and classification) demonstrate that the proposed pretext tasks are effective in pretraining deep networks for remarkably boosted performance on the target tasks, and superior to other recent approaches.","sentences":["Self-supervised learning has emerged as a powerful tool for pretraining deep networks on unlabeled data, prior to transfer learning of target tasks with limited annotation.","The relevance between the pretraining pretext and target tasks is crucial to the success of transfer learning.","Various pretext tasks have been proposed to utilize properties of medical image data (e.g., three dimensionality), which are more relevant to medical image analysis than generic ones for natural images.","However, previous work rarely paid attention to data with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance imaging views.","As these imaging planes are defined according to the anatomy of the imaged organ, pretext tasks effectively exploiting this information can pretrain the networks to gain knowledge on the organ of interest.","In this work, we propose two complementary pretext tasks for this group of medical image data based on the spatial relationship of the imaging planes.","The first is to learn the relative orientation between the imaging planes and implemented as regressing their intersecting lines.","The second exploits parallel imaging planes to regress their relative slice locations within a stack.","Both pretext tasks are conceptually straightforward and easy to implement, and can be combined in multitask learning for better representation learning.","Thorough experiments on two anatomical structures (heart and knee) and representative target tasks (semantic segmentation and classification) demonstrate that the proposed pretext tasks are effective in pretraining deep networks for remarkably boosted performance on the target tasks, and superior to other recent approaches."],"url":"http://arxiv.org/abs/2403.16499v1","category":"cs.CV"}
{"created":"2024-03-25 07:29:39","title":"BackCom Assisted Hybrid NOMA Uplink Transmission for Ambient IoT","abstract":"Hybrid non-orthogonal multiple access (H-NOMA) has recently received significant attention as a general framework of multiple access, where both conventional orthogonal multiple access (OMA) and pure NOMA are its special cases. This paper focuses on the application of H-NOMA to ambient Internet of Things (IoT) with energy-constrained devices, where a new backscatter communication (BackCom) assisted H-NOMA uplink scheme is developed. Resource allocation for H-NOMA uplink transmission is also considered, where an overall power minimization problem is formulated. Insightful understandings for the key features of BackCom assisted H-NOMA and its difference from conventional H-NOMA are illustrated by developing analytical results for the two-user special case. For the general multi-user scenario, two algorithms, one based on the branch-bound (BB) principle and the other based on successive convex approximation (SCA), are developed to realize different tradeoffs between the system performance and complexity. The numerical results are also provided to verify the accuracy of the developed analytical results and demonstrate the performance gain of H-NOMA over OMA.","sentences":["Hybrid non-orthogonal multiple access (H-NOMA) has recently received significant attention as a general framework of multiple access, where both conventional orthogonal multiple access (OMA) and pure NOMA are its special cases.","This paper focuses on the application of H-NOMA to ambient Internet of Things (IoT) with energy-constrained devices, where a new backscatter communication (BackCom) assisted H-NOMA uplink scheme is developed.","Resource allocation for H-NOMA uplink transmission is also considered, where an overall power minimization problem is formulated.","Insightful understandings for the key features of BackCom assisted H-NOMA and its difference from conventional H-NOMA are illustrated by developing analytical results for the two-user special case.","For the general multi-user scenario, two algorithms, one based on the branch-bound (BB) principle and the other based on successive convex approximation (SCA), are developed to realize different tradeoffs between the system performance and complexity.","The numerical results are also provided to verify the accuracy of the developed analytical results and demonstrate the performance gain of H-NOMA over OMA."],"url":"http://arxiv.org/abs/2403.16498v1","category":"cs.IT"}
{"created":"2024-03-25 07:23:23","title":"LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting","abstract":"Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series. Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated convolution layers, and periodic features are extracted by dynamic graph convolution layers. For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features. Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results. Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\% and a maximum improvement of 16.78\\% over baseline models. The source code is available at https://github.com/GeoX-Lab/LSTTN.","sentences":["Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models.","However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow.","Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation.","To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow.","First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series.","Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated convolution layers, and periodic features are extracted by dynamic graph convolution layers.","For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features.","Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results.","Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\% and a maximum improvement of 16.78\\% over baseline models.","The source code is available at https://github.com/GeoX-Lab/LSTTN."],"url":"http://arxiv.org/abs/2403.16495v1","category":"cs.LG"}
{"created":"2024-03-25 07:22:22","title":"CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid Convolution and Transformer Neural Networks","abstract":"We present CT-Bound, a fast boundary estimation method for noisy images using a hybrid Convolution and Transformer neural network. The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization of image boundaries. It first estimates a parametric representation of boundary structures only using the input image within a small receptive field and then refines the boundary structure in the parameter domain without accessing the input image. Because of this, a part of the network can be easily trained using naive, synthetic images and still generalized to real images, and the entire architecture is computationally efficient as the boundary refinement is non-iterative and not in the image domain. Compared with the previous highest accuracy methods, our experiment shows that CT-Bound is 100 times faster, producing comparably accurate, high-quality boundary and color maps. We also demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra fine-tuning and real-time boundary map and color map videos at ten frames per second.","sentences":["We present CT-Bound, a fast boundary estimation method for noisy images using a hybrid Convolution and Transformer neural network.","The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization of image boundaries.","It first estimates a parametric representation of boundary structures only using the input image within a small receptive field and then refines the boundary structure in the parameter domain without accessing the input image.","Because of this, a part of the network can be easily trained using naive, synthetic images and still generalized to real images, and the entire architecture is computationally efficient as the boundary refinement is non-iterative and not in the image domain.","Compared with the previous highest accuracy methods, our experiment shows that CT-Bound is 100 times faster, producing comparably accurate, high-quality boundary and color maps.","We also demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra fine-tuning and real-time boundary map and color map videos at ten frames per second."],"url":"http://arxiv.org/abs/2403.16494v1","category":"cs.CV"}
{"created":"2024-03-25 07:16:08","title":"Ensuring Disturbance Rejection Performance by Synthesizing Grid-Following and Grid-Forming Inverters in Power Systems","abstract":"To meet the dynamic requirement of power systems, it is imperative for grid-connected inverters to ensure good disturbance rejection performance (DRP) under variable grid conditions. This letter discovers and theoretically proves that for the general networks, synthesizing grid-following (GFL) inverters and grid-forming (GFM) inverters can more effectively ensure the DRP of multiple inverters, compared to homogeneous inverter-based systems that solely use either GFL or GFM inverters. The combination of GFL inverters and GFM inverters can concurrently increases the smallest eigenvalue and decreases the largest eigenvalue of the grounded network Laplacian matrix. This can be equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM inverters, thereby ensuring the DRP of inverters both in weak and strong grids. The result reveals the necessity of synthesizing diverse inverter control schemes from the network-based perspective. Sensitivity function-based analysis and real-time simulations confirm the effectiveness of our results.","sentences":["To meet the dynamic requirement of power systems, it is imperative for grid-connected inverters to ensure good disturbance rejection performance (DRP) under variable grid conditions.","This letter discovers and theoretically proves that for the general networks, synthesizing grid-following (GFL) inverters and grid-forming (GFM) inverters can more effectively ensure the DRP of multiple inverters, compared to homogeneous inverter-based systems that solely use either GFL or GFM inverters.","The combination of GFL inverters and GFM inverters can concurrently increases the smallest eigenvalue and decreases the largest eigenvalue of the grounded network Laplacian matrix.","This can be equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM inverters, thereby ensuring the DRP of inverters both in weak and strong grids.","The result reveals the necessity of synthesizing diverse inverter control schemes from the network-based perspective.","Sensitivity function-based analysis and real-time simulations confirm the effectiveness of our results."],"url":"http://arxiv.org/abs/2403.16488v1","category":"eess.SY"}
{"created":"2024-03-25 07:12:51","title":"Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation","abstract":"This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities.","sentences":["This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation.","We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning.","Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking.","The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics.","ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN.","ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit.","The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities."],"url":"http://arxiv.org/abs/2403.16485v1","category":"cs.RO"}
{"created":"2024-03-25 07:08:13","title":"Automatic Construction of a Large-Scale Corpus for Geoparsing Using Wikipedia Hyperlinks","abstract":"Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts. Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation. For evaluating geoparsing systems, several corpora have been proposed in previous work. However, these corpora are small-scale and suffer from the coverage of location expressions on general domains. In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates. With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions. 45.6% of location expressions are ambiguous and refer to more than one location with the same notation. In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates. By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts. Experimental results show that there remains room for improvement by disambiguating location expressions.","sentences":["Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts.","Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation.","For evaluating geoparsing systems, several corpora have been proposed in previous work.","However, these corpora are small-scale and suffer from the coverage of location expressions on general domains.","In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles.","WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates.","With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing.","The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions.","45.6% of location expressions are ambiguous and refer to more than one location with the same notation.","In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates.","By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts.","Experimental results show that there remains room for improvement by disambiguating location expressions."],"url":"http://arxiv.org/abs/2403.16483v1","category":"cs.CL"}
{"created":"2024-03-25 07:06:53","title":"Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models","abstract":"Recent studies show that deployed deep learning (DL) models such as those of Tensor Flow Lite (TFLite) can be easily extracted from real-world applications and devices by attackers to generate many kinds of attacks like adversarial attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent the aforementioned threats. Traditional software protection techniques have been widely explored, if on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate the DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract the on-device model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing units, configures the computing units to a graph, and then generates customized code to implement and deploy the ML solution without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively.","sentences":["Recent studies show that deployed deep learning (DL) models such as those of Tensor Flow Lite (TFLite) can be easily extracted from real-world applications and devices by attackers to generate many kinds of attacks like adversarial attacks.","Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent the aforementioned threats.","Traditional software protection techniques have been widely explored, if on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing software protection techniques.","However, due to the complexity of DL models, there is no automatic method that can translate the DL models to pure code.","To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract the on-device model information and synthesize a customized executable program for a wide range of DL models.","CustomDLCoder first parses the DL model, extracts its backend computing units, configures the computing units to a graph, and then generates customized code to implement and deploy the ML solution without explicit model representation.","The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model.","In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process.","Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing.","Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively.","Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively."],"url":"http://arxiv.org/abs/2403.16479v1","category":"cs.SE"}
{"created":"2024-03-25 07:04:24","title":"Towards Cooperative Maneuver Planning in Mixed Traffic at Urban Intersections","abstract":"Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas. Apart from sharing of awareness and perception information over wireless communication links, cooperative maneuver planning may facilitate active guidance of connected automated vehicles at urban intersections. Research in automatic intersection management put forth a large body of works that mostly employ rule-based or optimization-based approaches primarily in fully automated simulated environments. In this work, we present two cooperative planning approaches that are capable of handling mixed traffic, i.e., the road being shared by automated vehicles and regular vehicles driven by humans. Firstly, we propose an optimization-based planner trained on real driving data that cyclically selects the most efficient out of multiple predicted coordinated maneuvers. Additionally, we present a cooperative planning approach based on graph-based reinforcement learning, which conquers the lack of ground truth data for cooperative maneuvers. We present evaluation results of both cooperative planners in high-fidelity simulation and real-world traffic. Simulative experiments in fully automated traffic and mixed traffic show that cooperative maneuver planning leads to less delay due to interaction and a reduced number of stops. In real-world experiments with three prototype connected automated vehicles in public traffic, both planners demonstrate their ability to perform efficient cooperative maneuvers.","sentences":["Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas.","Apart from sharing of awareness and perception information over wireless communication links, cooperative maneuver planning may facilitate active guidance of connected automated vehicles at urban intersections.","Research in automatic intersection management put forth a large body of works that mostly employ rule-based or optimization-based approaches primarily in fully automated simulated environments.","In this work, we present two cooperative planning approaches that are capable of handling mixed traffic, i.e., the road being shared by automated vehicles and regular vehicles driven by humans.","Firstly, we propose an optimization-based planner trained on real driving data that cyclically selects the most efficient out of multiple predicted coordinated maneuvers.","Additionally, we present a cooperative planning approach based on graph-based reinforcement learning, which conquers the lack of ground truth data for cooperative maneuvers.","We present evaluation results of both cooperative planners in high-fidelity simulation and real-world traffic.","Simulative experiments in fully automated traffic and mixed traffic show that cooperative maneuver planning leads to less delay due to interaction and a reduced number of stops.","In real-world experiments with three prototype connected automated vehicles in public traffic, both planners demonstrate their ability to perform efficient cooperative maneuvers."],"url":"http://arxiv.org/abs/2403.16478v1","category":"cs.RO"}
{"created":"2024-03-25 07:04:16","title":"Safeguarding Next Generation Multiple Access Using Physical Layer Security Techniques: A Tutorial","abstract":"Driven by the ever-increasing requirements of ultra-high spectral efficiency, ultra-low latency, and massive connectivity, the forefront of wireless research calls for the design of advanced next generation multiple access schemes to facilitate provisioning of these stringent demands. This inspires the embrace of non-orthogonal multiple access (NOMA) in future wireless communication networks. Nevertheless, the support of massive access via NOMA leads to additional security threats, due to the open nature of the air interface, the broadcast characteristic of radio propagation as well as intertwined relationship among paired NOMA users. To address this specific challenge, the superimposed transmission of NOMA can be explored as new opportunities for security aware design, for example, multiuser interference inherent in NOMA can be constructively engineered to benefit communication secrecy and privacy. The purpose of this tutorial is to provide a comprehensive overview on the state-of-the-art physical layer security techniques that guarantee wireless security and privacy for NOMA networks, along with the opportunities, technical challenges, and future research trends.","sentences":["Driven by the ever-increasing requirements of ultra-high spectral efficiency, ultra-low latency, and massive connectivity, the forefront of wireless research calls for the design of advanced next generation multiple access schemes to facilitate provisioning of these stringent demands.","This inspires the embrace of non-orthogonal multiple access (NOMA) in future wireless communication networks.","Nevertheless, the support of massive access via NOMA leads to additional security threats, due to the open nature of the air interface, the broadcast characteristic of radio propagation as well as intertwined relationship among paired NOMA users.","To address this specific challenge, the superimposed transmission of NOMA can be explored as new opportunities for security aware design, for example, multiuser interference inherent in NOMA can be constructively engineered to benefit communication secrecy and privacy.","The purpose of this tutorial is to provide a comprehensive overview on the state-of-the-art physical layer security techniques that guarantee wireless security and privacy for NOMA networks, along with the opportunities, technical challenges, and future research trends."],"url":"http://arxiv.org/abs/2403.16477v1","category":"cs.IT"}
{"created":"2024-03-25 06:57:30","title":"Sweeping Arrangements of Non-Piercing Curves in Plane","abstract":"Let $\\Gamma$ be a finite set of Jordan curves in the plane. For any curve $\\gamma \\in \\Gamma$, we denote the bounded region enclosed by $\\gamma$ as $\\tilde{\\gamma}$. We say that $\\Gamma$ is a non-piercing family if for any two curves $\\alpha , \\beta \\in \\Gamma$, $\\tilde{\\alpha} \\setminus \\tilde{\\beta}$ is a connected region. A non-piercing family of curves generalizes a family of $2$-intersecting curves in which each pair of curves intersect in at most two points. Snoeyink and Hershberger (``Sweeping Arrangements of Curves'', SoCG '89) proved that if we are given a family $\\mathcal{C}$ of $2$-intersecting curves and a fixed curve $C\\in\\mathcal{C}$, then the arrangement can be \\emph{swept} by $C$, i.e., $C$ can be continuously shrunk to any point $p \\in \\tilde{C}$ in such a way that the we have a family of $2$-intersecting curves throughout the process. In this paper, we generalize the result of Snoeyink and Hershberger to the setting of non-piercing curves. We show that given an arrangement of non-piercing curves $\\Gamma$, and a fixed curve $\\gamma\\in \\Gamma$, the arrangement can be swept by $\\gamma$ so that the arrangement remains non-piercing throughout the process. We also give a shorter and simpler proof of the result of Snoeyink and Hershberger and cite applications of their result, where our result leads to a generalization.","sentences":["Let $\\Gamma$ be a finite set of Jordan curves in the plane.","For any curve $\\gamma \\in \\Gamma$, we denote the bounded region enclosed by $\\gamma$ as $\\tilde{\\gamma}$. We say that $\\Gamma$ is a non-piercing family if for any two curves $\\alpha , \\beta \\in \\Gamma$, $\\tilde{\\alpha} \\setminus \\tilde{\\beta}$ is a connected region.","A non-piercing family of curves generalizes a family of $2$-intersecting curves in which each pair of curves intersect in at most two points.","Snoeyink and Hershberger (``Sweeping Arrangements of Curves'', SoCG '89) proved that if we are given a family $\\mathcal{C}$ of $2$-intersecting curves and a fixed curve $C\\in\\mathcal{C}$, then the arrangement can be \\emph{swept} by $C$, i.e., $C$ can be continuously shrunk to any point $p \\in \\tilde{C}$ in such a way that the we have a family of $2$-intersecting curves throughout the process.","In this paper, we generalize the result of Snoeyink and Hershberger to the setting of non-piercing curves.","We show that given an arrangement of non-piercing curves $\\Gamma$, and a fixed curve $\\gamma\\in \\Gamma$, the arrangement can be swept by $\\gamma$ so that the arrangement remains non-piercing throughout the process.","We also give a shorter and simpler proof of the result of Snoeyink and Hershberger and cite applications of their result, where our result leads to a generalization."],"url":"http://arxiv.org/abs/2403.16474v1","category":"cs.CG"}
{"created":"2024-03-25 06:56:27","title":"Power-Aware Sparse Reflect Beamforming in Active RIS-aided Interference Channels","abstract":"Active reconfigurable intelligent surface (RIS) has attracted significant attention in wireless communications, due to its reflecting elements (REs) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications. In this paper, we are interested in active RIS-aided interference channels in which $K$ user pairs share the same time and frequency resources with the aid of active RIS. Thanks to the promising amplitude amplification capability, activating a moderate number of REs, rather than all of them, is sufficient for the active RIS to mitigate cross-channel interferences. Motivated by this, we propose a power-aware sparse reflect beamforming design for the active RIS-aided interference channels, which allows the active RIS to flexibly adjust the number of activated REs for the sake of reducing hardware and power costs. Specifically, we establish the power consumption model in which only those activated REs consume the biasing and operation power that supports the amplitude amplification, yielding an $\\ell_0$-norm power consumption function. Based on the proposed model, we investigate a sum-rate maximization problem and an active RIS power minimization problem by carefully designing the sparse reflect beamforming vector. To solve these problems, we first replace the nonconvex $\\ell_0$-norm function with an iterative reweighted $\\ell_1$-norm function. Then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (DCA) is used to solve the active RIS power minimization. Numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active RIS in interference channels.","sentences":["Active reconfigurable intelligent surface (RIS) has attracted significant attention in wireless communications, due to its reflecting elements (REs) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications.","In this paper, we are interested in active RIS-aided interference channels in which $K$ user pairs share the same time and frequency resources with the aid of active RIS.","Thanks to the promising amplitude amplification capability, activating a moderate number of REs, rather than all of them, is sufficient for the active RIS to mitigate cross-channel interferences.","Motivated by this, we propose a power-aware sparse reflect beamforming design for the active RIS-aided interference channels, which allows the active RIS to flexibly adjust the number of activated REs for the sake of reducing hardware and power costs.","Specifically, we establish the power consumption model in which only those activated REs consume the biasing and operation power that supports the amplitude amplification, yielding an $\\ell_0$-norm power consumption function.","Based on the proposed model, we investigate a sum-rate maximization problem and an active RIS power minimization problem by carefully designing the sparse reflect beamforming vector.","To solve these problems, we first replace the nonconvex $\\ell_0$-norm function with an iterative reweighted $\\ell_1$-norm function.","Then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (DCA) is used to solve the active RIS power minimization.","Numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active RIS in interference channels."],"url":"http://arxiv.org/abs/2403.16472v1","category":"cs.IT"}
{"created":"2024-03-25 06:46:27","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator","abstract":"A generative adversarial network (GAN)-based vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics. However, this data-driven model requires a large amount of training data incurring high data-collection costs. This fact motivates us to train a GAN-based vocoder on limited data. A promising solution is to augment the training data to avoid overfitting. However, a standard discriminator is unconditional and insensitive to distributional changes caused by data augmentation. Thus, augmented speech (which can be extraordinary) may be considered real speech. To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmented distribution. Experimental results indicate that AugCondD improves speech quality under limited data conditions while achieving comparable speech quality under sufficient data conditions. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.","sentences":["A generative adversarial network (GAN)-based vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics.","However, this data-driven model requires a large amount of training data incurring high data-collection costs.","This fact motivates us to train a GAN-based vocoder on limited data.","A promising solution is to augment the training data to avoid overfitting.","However, a standard discriminator is unconditional and insensitive to distributional changes caused by data augmentation.","Thus, augmented speech (which can be extraordinary) may be considered real speech.","To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmented distribution.","Experimental results indicate that AugCondD improves speech quality under limited data conditions while achieving comparable speech quality under sufficient data conditions.","Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/."],"url":"http://arxiv.org/abs/2403.16464v1","category":"cs.SD"}
{"created":"2024-03-25 06:45:09","title":"Few-shot Named Entity Recognition via Superposition Concept Discrimination","abstract":"Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models. To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases. Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts.","sentences":["Few-shot NER aims to identify entities of target types with only limited number of illustrative instances.","Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency.","In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm.","Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary.","Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus.","Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models.","To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases.","Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts."],"url":"http://arxiv.org/abs/2403.16463v1","category":"cs.CL"}
{"created":"2024-03-25 06:43:28","title":"FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data","abstract":"Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments. Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods.","sentences":["Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training.","However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness.","In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments.","Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods."],"url":"http://arxiv.org/abs/2403.16460v1","category":"cs.LG"}
{"created":"2024-03-25 06:41:25","title":"Next Generation Advanced Transceiver Technologies for 6G","abstract":"To accommodate new applications such as extended reality, fully autonomous vehicular networks and the metaverse, next generation wireless networks are going to be subject to much more stringent performance requirements than the fifth-generation (5G) in terms of data rates, reliability, latency, and connectivity. It is thus necessary to develop next generation advanced transceiver (NGAT) technologies for efficient signal transmission and reception. In this tutorial, we explore the evolution of NGAT from three different perspectives. Specifically, we first provide an overview of new-field NGAT technology, which shifts from conventional far-field channel models to new near-field channel models. Then, three new-form NGAT technologies and their design challenges are presented, including reconfigurable intelligent surfaces, flexible antennas, and holographic multi-input multi-output (MIMO) systems. Subsequently, we discuss recent advances in semantic-aware NGAT technologies, which can utilize new metrics for advanced transceiver designs. Finally, we point out other promising transceiver technologies for future research.","sentences":["To accommodate new applications such as extended reality, fully autonomous vehicular networks and the metaverse, next generation wireless networks are going to be subject to much more stringent performance requirements than the fifth-generation (5G) in terms of data rates, reliability, latency, and connectivity.","It is thus necessary to develop next generation advanced transceiver (NGAT) technologies for efficient signal transmission and reception.","In this tutorial, we explore the evolution of NGAT from three different perspectives.","Specifically, we first provide an overview of new-field NGAT technology, which shifts from conventional far-field channel models to new near-field channel models.","Then, three new-form NGAT technologies and their design challenges are presented, including reconfigurable intelligent surfaces, flexible antennas, and holographic multi-input multi-output (MIMO) systems.","Subsequently, we discuss recent advances in semantic-aware NGAT technologies, which can utilize new metrics for advanced transceiver designs.","Finally, we point out other promising transceiver technologies for future research."],"url":"http://arxiv.org/abs/2403.16458v1","category":"cs.IT"}
{"created":"2024-03-25 06:36:10","title":"Flux Quantization on 11-dimensional Superspace","abstract":"Flux quantization of the C-field in 11d supergravity is arguably necessary for the (UV-)completion of the theory, in that it determines the torsion charges carried by small numbers of M-branes. However, hypotheses about C-field flux-quantization (\"models of the C-field\") have previously been discussed only in the bosonic sector of 11d supergravity and ignoring the supergravity equations of motion. Here we highlight a duality-symmetric formulation of on-shell 11d supergravity on superspace, observe that this naturally lends itself to completion of the theory by flux quantization, and indeed that 11d super-spacetimes are put on-shell by carrying quantizable duality-symmetric super-C-field flux; the proof of which we present in detail.","sentences":["Flux quantization of the C-field in 11d supergravity is arguably necessary for the (UV-)completion of the theory, in that it determines the torsion charges carried by small numbers of M-branes.","However, hypotheses about C-field flux-quantization (\"models of the C-field\") have previously been discussed only in the bosonic sector of 11d supergravity and ignoring the supergravity equations of motion.","Here we highlight a duality-symmetric formulation of on-shell 11d supergravity on superspace, observe that this naturally lends itself to completion of the theory by flux quantization, and indeed that 11d super-spacetimes are put on-shell by carrying quantizable duality-symmetric super-C-field flux; the proof of which we present in detail."],"url":"http://arxiv.org/abs/2403.16456v1","category":"hep-th"}
{"created":"2024-03-25 06:33:23","title":"Photon orbits and phase transition for Letelier AdS black holes immersed in perfect fluid dark matter","abstract":"We obtain an exact solution of spherically symmetric Letelier AdS black holes immersed in perfect fluid dark matter (PFDM). Considering the cosmological constant as the positive pressure of the system and volume as its conjugate variable, we analyse the thermodynamics of our black holes in the extended phase space. Owing to the background clouds of strings parameter ($a$) and the parameter endowed with PFDM ($\\beta$), we analyse the Hawking temperature, entropy and specific heat. We also investigate the relationship between the photon sphere radius and the phase transition for the Letelier AdS black holes immersed in PFDM. Through the analysis, we find with a particular condition, there are non-monotonic behaviours between the photon sphere radius, the impact parameter, the PFDM parameter, temperature, and pressure. We can regard both the changes of photon sphere radius and impact parameter before and after phase transition as the order parameter; their critical exponents near the critical point are equal to the same value 1/2, just like ordinary thermal systems. These indicate that a universal relation of gravity may exist near the critical point for a black hole thermodynamic system.","sentences":["We obtain an exact solution of spherically symmetric Letelier AdS black holes immersed in perfect fluid dark matter (PFDM).","Considering the cosmological constant as the positive pressure of the system and volume as its conjugate variable, we analyse the thermodynamics of our black holes in the extended phase space.","Owing to the background clouds of strings parameter ($a$) and the parameter endowed with PFDM ($\\beta$), we analyse the Hawking temperature, entropy and specific heat.","We also investigate the relationship between the photon sphere radius and the phase transition for the Letelier AdS black holes immersed in PFDM.","Through the analysis, we find with a particular condition, there are non-monotonic behaviours between the photon sphere radius, the impact parameter, the PFDM parameter, temperature, and pressure.","We can regard both the changes of photon sphere radius and impact parameter before and after phase transition as the order parameter; their critical exponents near the critical point are equal to the same value 1/2, just like ordinary thermal systems.","These indicate that a universal relation of gravity may exist near the critical point for a black hole thermodynamic system."],"url":"http://arxiv.org/abs/2403.16454v1","category":"gr-qc"}
{"created":"2024-03-25 06:31:13","title":"Determinants of Uruguay's Real Effective Exchange Rate: A Mundell-Fleming Model Approach","abstract":"This study examines the factors influencing the short-term real effective exchange rate (REER) in Uruguay by applying an extended Mundell-Fleming model. Analyzing the impact of the US lending rate (USLR), money supply (M2), inflation (CPI), and the world interest rate (WIR), the paper uses a linear regression model with Newey-West standard errors. Key findings reveal that an increase in the USLR, CPI, and M2 is associated with a depreciation of the REER. In contrast, WIR shows no significant impact. These findings are consistent with the theoretical expectations of the Mundell-Fleming model regarding open economies under floating exchange rates. Therefore, authorities should tighten monetary policy, control inflation, adjust fiscal strategies, and boost exports in response to Peso depreciation.","sentences":["This study examines the factors influencing the short-term real effective exchange rate (REER) in Uruguay by applying an extended Mundell-Fleming model.","Analyzing the impact of the US lending rate (USLR), money supply (M2), inflation (CPI), and the world interest rate (WIR), the paper uses a linear regression model with Newey-West standard errors.","Key findings reveal that an increase in the USLR, CPI, and M2 is associated with a depreciation of the REER.","In contrast, WIR shows no significant impact.","These findings are consistent with the theoretical expectations of the Mundell-Fleming model regarding open economies under floating exchange rates.","Therefore, authorities should tighten monetary policy, control inflation, adjust fiscal strategies, and boost exports in response to Peso depreciation."],"url":"http://arxiv.org/abs/2403.16452v1","category":"econ.GN"}
{"created":"2024-03-25 06:30:54","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","abstract":"We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.","sentences":["We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations.","We have built and evaluated DeepMachining based on manufacturing data from factories.","Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states.","Then, we fine-tune the pretrained model to adapt to specific machining tasks.","We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools.","To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines."],"url":"http://arxiv.org/abs/2403.16451v1","category":"cs.LG"}
{"created":"2024-03-25 06:22:27","title":"Camera-aware Label Refinement for Unsupervised Person Re-identification","abstract":"Unsupervised person re-identification aims to retrieve images of a specified person without identity labels. Many recent unsupervised Re-ID approaches adopt clustering-based methods to measure cross-camera feature similarity to roughly divide images into clusters. They ignore the feature distribution discrepancy induced by camera domain gap, resulting in the unavoidable performance degradation. Camera information is usually available, and the feature distribution in the single camera usually focuses more on the appearance of the individual and has less intra-identity variance. Inspired by the observation, we introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel \\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by clustering intra-camera similarity. Specifically, we employ intra-camera training to obtain reliable local pseudo labels within each camera, and then refine global labels generated by inter-camera clustering and train the discriminative model using more reliable global pseudo labels in a self-paced manner. Meanwhile, we develop a camera-alignment module to align feature distributions under different cameras, which could help deal with the camera variance further. Extensive experiments validate the superiority of our proposed method over state-of-the-art approaches. The code is accessible at https://github.com/leeBooMla/CALR.","sentences":["Unsupervised person re-identification aims to retrieve images of a specified person without identity labels.","Many recent unsupervised Re-ID approaches adopt clustering-based methods to measure cross-camera feature similarity to roughly divide images into clusters.","They ignore the feature distribution discrepancy induced by camera domain gap, resulting in the unavoidable performance degradation.","Camera information is usually available, and the feature distribution in the single camera usually focuses more on the appearance of the individual and has less intra-identity variance.","Inspired by the observation, we introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel \\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by clustering intra-camera similarity.","Specifically, we employ intra-camera training to obtain reliable local pseudo labels within each camera, and then refine global labels generated by inter-camera clustering and train the discriminative model using more reliable global pseudo labels in a self-paced manner.","Meanwhile, we develop a camera-alignment module to align feature distributions under different cameras, which could help deal with the camera variance further.","Extensive experiments validate the superiority of our proposed method over state-of-the-art approaches.","The code is accessible at https://github.com/leeBooMla/CALR."],"url":"http://arxiv.org/abs/2403.16450v1","category":"cs.CV"}
{"created":"2024-03-25 06:19:47","title":"Multinomial random combinatorial structures and $r$-versions of Stirling, Eulerian and Lah numbers","abstract":"We introduce multinomial and $r$-variants of several classic objects of combinatorial probability, such as the random recursive and Hoppe trees, random set partitions and compositions, the Chinese restaurant process, Feller's coupling, and some others. Just as various classic combinatorial numbers - like Stirling, Eulerian and Lah numbers - emerge as essential ingredients defining the distributions of the mentioned processes, the so-called $r$-versions of these numbers appear in exact distributional formulas for the multinomial and $r$-counterparts. This approach allows us to offer a concise probabilistic interpretation for various identities involving $r$-versions of these combinatorial numbers, which were either unavailable or meaningful only for specific values of the parameter $r$. We analyze the derived distributions for fixed-size structures and establish distributional limit theorems as the size tends to infinity. Utilizing the aforementioned generalized Stirling numbers of both kinds, we define and analyze $(r,s)$-Lah distributions, which have arisen in the existing literature on combinatorial probability in various contexts.","sentences":["We introduce multinomial and $r$-variants of several classic objects of combinatorial probability, such as the random recursive and Hoppe trees, random set partitions and compositions, the Chinese restaurant process, Feller's coupling, and some others.","Just as various classic combinatorial numbers - like Stirling, Eulerian and Lah numbers - emerge as essential ingredients defining the distributions of the mentioned processes, the so-called $r$-versions of these numbers appear in exact distributional formulas for the multinomial and $r$-counterparts.","This approach allows us to offer a concise probabilistic interpretation for various identities involving $r$-versions of these combinatorial numbers, which were either unavailable or meaningful only for specific values of the parameter $r$. We analyze the derived distributions for fixed-size structures and establish distributional limit theorems as the size tends to infinity.","Utilizing the aforementioned generalized Stirling numbers of both kinds, we define and analyze $(r,s)$-Lah distributions, which have arisen in the existing literature on combinatorial probability in various contexts."],"url":"http://arxiv.org/abs/2403.16448v1","category":"math.PR"}
{"created":"2024-03-25 06:09:55","title":"CodeS: Natural Language to Code Repository via Multi-Layer Sketch","abstract":"The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.","sentences":["The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development.","In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo).","This task aims to generate an entire code repository from its natural language requirements.","To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch.","Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller.","RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch.","To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis.","For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU.","For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies.","Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task."],"url":"http://arxiv.org/abs/2403.16443v1","category":"cs.CL"}
{"created":"2024-03-25 06:05:50","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions","abstract":"Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.","sentences":["Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape.","However, it is unclear to what extent VLMs prioritize this information to represent concepts.","We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs.","EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM.","Then, we inspect the descriptions to identify the features that contribute to VLM representations.","We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT.","More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts.","Also, our analysis reveals that different VLMs prioritize different attributes in their representations.","Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations."],"url":"http://arxiv.org/abs/2403.16442v1","category":"cs.CL"}
{"created":"2024-03-25 05:37:16","title":"Evaluating Large Language Models with Runtime Behavior of Program Execution","abstract":"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs.","sentences":["Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities.","To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval).","Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient.","Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning.","To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution.","We utilize existing code benchmarks and adapt them to new benchmarks within our framework.","A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3).","Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs."],"url":"http://arxiv.org/abs/2403.16437v1","category":"cs.SE"}
{"created":"2024-03-25 05:27:35","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models","abstract":"Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by \\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo.","sentences":["Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks.","Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization.","Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm.","Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm.","However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses.","In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens.","Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by \\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo."],"url":"http://arxiv.org/abs/2403.16432v1","category":"cs.CL"}
{"created":"2024-03-25 05:22:34","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding","abstract":"Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously. Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks. This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects. In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner. Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship. In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks. A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training. Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset. Code is available at https://github.com/SAITPublic/DOCTR.","sentences":["Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously.","Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks.","This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects.","In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner.","Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship.","In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks.","A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training.","Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset.","Code is available at https://github.com/SAITPublic/DOCTR."],"url":"http://arxiv.org/abs/2403.16431v1","category":"cs.CV"}
{"created":"2024-03-25 05:12:21","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects","abstract":"We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views. Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects. Our efforts will enrich the community's knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.","sentences":["We interact with the world with our hands and see it through our own (egocentric) perspective.","A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation.","Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement.","To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits.","Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks.","Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views.","Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects.","Our efforts will enrich the community's knowledge foundation and facilitate future hand studies on egocentric hand-object interactions."],"url":"http://arxiv.org/abs/2403.16428v1","category":"cs.CV"}
{"created":"2024-03-25 05:12:18","title":"Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation","abstract":"Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \\underline{Re}flective \\underline{Re}inforcement \\underline{L}arge \\underline{L}anguage \\underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs.   To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors.   To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent.   It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations. Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.","sentences":["Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   ","However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   ","Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   ","To address such issues, we propose a \\underline{Re}flective \\underline{Re}inforcement \\underline{L}arge \\underline{L}anguage \\underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   ","In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs.   ","To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors.   ","To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent.   ","It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations.","Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16427v1","category":"cs.AI"}
{"created":"2024-03-25 05:04:52","title":"An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations","abstract":"This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.","sentences":["This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH).","The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries.","The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness.","The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH.","Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs."],"url":"http://arxiv.org/abs/2403.16424v1","category":"cs.AI"}
{"created":"2024-03-25 05:02:36","title":"Canonical Quantization of the U(1) Gauge Field in the Rindler Coordinates","abstract":"This paper describes the canonical quantization of the U(1) gauge field across all four regions in the Rindler coordinates in the Lorentz covariant gauge. In the four regions (future, past, left and right Rindler-wedges) in the Rindler coordinates, defining the gauge-fixed Lagrangian in the Lorentz covariant gauge, which is composed of the U(1) gauge field, the $B$-field and ghost fields. Since the U(1) gauge and $B$-fields are decoupled from the ghost fields by the property of the U(1) gauge theory, the U(1) gauge field and the $B$-field are examined in this study.   Then, by solving the equations of motion obtained from the gauge-fixed Lagrangian, the solutions of each mode of the U(1) gauge field and the $B$-field can be obtained. Following this, with the Klein-Gordon inner-product defined in the Rindler coordinates, the normalization constants of each of those mode-solutions are determined.   Subsequently, formulating the canonical commutation relations of the U(1) gauge field and its canonical conjugate momentum, the commutation relations of the coefficient of each mode-solution in each direction of the U(1) gauge field in each region of the Rindler coordinates are obtained. From these, it can be seen that those coefficients have physical meaning as creation/annihilation operators. The polarization vectors in each region of the Rindler coordinates are also given in this study.   From these, it is shown that the Minkowski ground state can be given as the outer-product of the quantum states multiplied by the creation operators of the U(1) gauge field in the left and right Rindler-wedge states.   Then, obtaining the density matrix of the U(1) gauge theory in the right Rindler-wedge from that, it is shown that the U(1) gauge field in the constant accelerated system feels the Unruh temperature as well.","sentences":["This paper describes the canonical quantization of the U(1) gauge field across all four regions in the Rindler coordinates in the Lorentz covariant gauge.","In the four regions (future, past, left and right Rindler-wedges) in the Rindler coordinates, defining the gauge-fixed Lagrangian in the Lorentz covariant gauge, which is composed of the U(1) gauge field, the $B$-field and ghost fields.","Since the U(1) gauge and $B$-fields are decoupled from the ghost fields by the property of the U(1) gauge theory, the U(1) gauge field and the $B$-field are examined in this study.   ","Then, by solving the equations of motion obtained from the gauge-fixed Lagrangian, the solutions of each mode of the U(1) gauge field and the $B$-field can be obtained.","Following this, with the Klein-Gordon inner-product defined in the Rindler coordinates, the normalization constants of each of those mode-solutions are determined.   ","Subsequently, formulating the canonical commutation relations of the U(1) gauge field and its canonical conjugate momentum, the commutation relations of the coefficient of each mode-solution in each direction of the U(1) gauge field in each region of the Rindler coordinates are obtained.","From these, it can be seen that those coefficients have physical meaning as creation/annihilation operators.","The polarization vectors in each region of the Rindler coordinates are also given in this study.   ","From these, it is shown that the Minkowski ground state can be given as the outer-product of the quantum states multiplied by the creation operators of the U(1) gauge field in the left and right Rindler-wedge states.   ","Then, obtaining the density matrix of the U(1) gauge theory in the right Rindler-wedge from that, it is shown that the U(1) gauge field in the constant accelerated system feels the Unruh temperature as well."],"url":"http://arxiv.org/abs/2403.16423v1","category":"hep-th"}
{"created":"2024-03-25 04:54:49","title":"Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation","abstract":"Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research. We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches. We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, OCR precision, recall, F1 score, accuracy, and edit distance scores. For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval, respectively. Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature","sentences":["Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention.","However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images.","The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications.","To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout.","Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research.","We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text.","Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches.","We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, OCR precision, recall, F1 score, accuracy, and edit distance scores.","For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval, respectively.","Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature"],"url":"http://arxiv.org/abs/2403.16422v1","category":"cs.CV"}
{"created":"2024-03-25 04:50:02","title":"Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling","abstract":"This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo simulations. We used the LiteX framework to generate a Petitbateau FemtoRV RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board. The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different benchmark applications when compared to GNU Scientific Library software random number generation. These speedups are achievable because the benchmarks spend an average of 90.0 % of their execution time generating random samples. The results of the Monte Carlo benchmark programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times$that of the results produced by the GNU Scientific Library random number generators. The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA. The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC.","sentences":["This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo simulations.","We used the LiteX framework to generate a Petitbateau FemtoRV RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board.","The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different benchmark applications when compared to GNU Scientific Library software random number generation.","These speedups are achievable because the benchmarks spend an average of 90.0 % of their execution time generating random samples.","The results of the Monte Carlo benchmark programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times$that of the results produced by the GNU Scientific Library random number generators.","The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA.","The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC."],"url":"http://arxiv.org/abs/2403.16421v1","category":"cs.AR"}
{"created":"2024-03-25 04:47:44","title":"Real-Time Recognition of Vortex Beams Modes Through Random Diffusive at the Speed of Light","abstract":"Optical vortex beam with orbital angular momentum (OAM) has great potential to increase the capacity of optical communication and information processing in classical and quantum regimes. Nevertheless, important challenges that influence the optical data transmission in free space is the existence of diffusers along the optical path, which causes inevitable information loss during the wave propagation. Numerous algorithms have been proposed successively for identifying the modes of vortex beams propagating through scattering media. However, these methods all require completion on a computer, which is energyintensive and energy consuming. Here, we propose an all-optical regime for identifying the modes of vortex light fields propagating through scattering media. After training by deep learning, our model can recognize the mode of vortex beam through unknown phase diffusers, demonstrating generalization to new random diffusers that have never been encountered before. Once physically deployed, the entire setup will rapidly identify the modes of vortex light propagating through scattering media at the speed of light, and the entire inference process will consume zero energy except for illumination source. Our research represents a significant step towards highly accurate recognition of vortex light modes propagating through complex scattering media, providing significant guidance for the application of optical communication in complex environments.","sentences":["Optical vortex beam with orbital angular momentum (OAM) has great potential to increase the capacity of optical communication and information processing in classical and quantum regimes.","Nevertheless, important challenges that influence the optical data transmission in free space is the existence of diffusers along the optical path, which causes inevitable information loss during the wave propagation.","Numerous algorithms have been proposed successively for identifying the modes of vortex beams propagating through scattering media.","However, these methods all require completion on a computer, which is energyintensive and energy consuming.","Here, we propose an all-optical regime for identifying the modes of vortex light fields propagating through scattering media.","After training by deep learning, our model can recognize the mode of vortex beam through unknown phase diffusers, demonstrating generalization to new random diffusers that have never been encountered before.","Once physically deployed, the entire setup will rapidly identify the modes of vortex light propagating through scattering media at the speed of light, and the entire inference process will consume zero energy except for illumination source.","Our research represents a significant step towards highly accurate recognition of vortex light modes propagating through complex scattering media, providing significant guidance for the application of optical communication in complex environments."],"url":"http://arxiv.org/abs/2403.16420v1","category":"physics.optics"}
{"created":"2024-03-25 04:43:47","title":"An incremental MaxSAT-based model to learn balanced rules","abstract":"The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset. Finally, IMLIB and IMLI are compared using diverse databases. IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes.","sentences":["The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions.","However, in certain cases, accuracy alone may not be sufficient.","Many real-world problems also demand explanations and interpretability behind the predictions.","One of the most popular interpretable models that are classification rules.","This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB.","This new model was based on two other approaches, one based on SAT and the other on MaxSAT.","The one based on SAT limits the size of each generated rule, making it possible to balance them.","We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules.","The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset.","Finally, IMLIB and IMLI are compared using diverse databases.","IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes."],"url":"http://arxiv.org/abs/2403.16418v1","category":"cs.LG"}
{"created":"2024-03-25 04:34:20","title":"Leveraging Large Language Model to Generate a Novel Metaheuristic Algorithm with CRISPE Framework","abstract":"In this paper, we borrow the large language model (LLM) ChatGPT-3.5 to automatically and quickly design a new metaheuristic algorithm (MA) with only a small amount of input. The novel animal-inspired MA named zoological search optimization (ZSO) draws inspiration from the collective behaviors of animals for solving continuous optimization problems. Specifically, the basic ZSO algorithm involves two search operators: the prey-predator interaction operator and the social flocking operator to balance exploration and exploitation well. Besides, the standard prompt engineering framework CRISPE (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment) is responsible for the specific prompt design. Furthermore, we designed four variants of the ZSO algorithm with slight human-interacted adjustment. In numerical experiments, we comprehensively investigate the performance of ZSO-derived algorithms on CEC2014 benchmark functions, CEC2022 benchmark functions, and six engineering optimization problems. 20 popular and state-of-the-art MAs are employed as competitors. The experimental results and statistical analysis confirm the efficiency and effectiveness of ZSO-derived algorithms. At the end of this paper, we explore the prospects for the development of the metaheuristics community under the LLM era.","sentences":["In this paper, we borrow the large language model (LLM) ChatGPT-3.5 to automatically and quickly design a new metaheuristic algorithm (MA) with only a small amount of input.","The novel animal-inspired MA named zoological search optimization (ZSO) draws inspiration from the collective behaviors of animals for solving continuous optimization problems.","Specifically, the basic ZSO algorithm involves two search operators: the prey-predator interaction operator and the social flocking operator to balance exploration and exploitation well.","Besides, the standard prompt engineering framework CRISPE (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment) is responsible for the specific prompt design.","Furthermore, we designed four variants of the ZSO algorithm with slight human-interacted adjustment.","In numerical experiments, we comprehensively investigate the performance of ZSO-derived algorithms on CEC2014 benchmark functions, CEC2022 benchmark functions, and six engineering optimization problems.","20 popular and state-of-the-art MAs are employed as competitors.","The experimental results and statistical analysis confirm the efficiency and effectiveness of ZSO-derived algorithms.","At the end of this paper, we explore the prospects for the development of the metaheuristics community under the LLM era."],"url":"http://arxiv.org/abs/2403.16417v1","category":"cs.NE"}
{"created":"2024-03-25 04:21:06","title":"How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation","abstract":"Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time. CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on LLMs. While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results. (2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single prompt template proves challenging. To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items. Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results.","sentences":["Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time.","CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus.","Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields.","Simultaneously, efforts are underway to construct user simulators based on LLMs.","While these works showcase innovation, they also come with certain limitations that require attention.","In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research.","To achieve this goal, we conduct analytical validation on the notable work, iEvaLM.","Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results.","(2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators.","(3) Controlling the output of the user simulator through a single prompt template proves challenging.","To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items.","Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results."],"url":"http://arxiv.org/abs/2403.16416v1","category":"cs.AI"}
{"created":"2024-03-25 04:17:31","title":"Generation of $\u03b3$-photons and pairs with transverse orbital angular momentum via spatiotemporal optical vortex pulse","abstract":"We present the generation of well-collimated $\\gamma$-photons and pairs with extrinsic transverse orbital angular momentum (TOAM) through the head-on collision of an intense spatiotemporal optical vortex (STOV) pulse carrying intrinsic TOAM with a high-energy electron beam. It is found that the TOAM of STOV pulse remains almost unchanged, and the TOAM is conserved in the center-of-mass frame (CMF). Moreover, there exhibits duality for particles TOAM in the CMF and laboratory frame (LF) when the initial location of high-energy electron beam is different. Furthermore, the TOAM of $\\gamma$-photons in the CMF increases while that of positrons decreases as the topological charge of STOV pulse increases, whereas in the LF, the TOAM of both $\\gamma$-photons and positrons decreases. And the result under the same pulse intensity is better than that under the same pulse energy. The increase in the initial energy of high-energy electrons leads to an enhancement of the TOAM for both $\\gamma$-photons and positrons in both frames. $\\gamma$-photons and electrons/positrons with TOAM as a new degree of freedom maybe have an extensive applications in optical communication, astrophysics and nanomaterials and so on.","sentences":["We present the generation of well-collimated $\\gamma$-photons and pairs with extrinsic transverse orbital angular momentum (TOAM) through the head-on collision of an intense spatiotemporal optical vortex (STOV) pulse carrying intrinsic TOAM with a high-energy electron beam.","It is found that the TOAM of STOV pulse remains almost unchanged, and the TOAM is conserved in the center-of-mass frame (CMF).","Moreover, there exhibits duality for particles TOAM in the CMF and laboratory frame (LF) when the initial location of high-energy electron beam is different.","Furthermore, the TOAM of $\\gamma$-photons in the CMF increases while that of positrons decreases as the topological charge of STOV pulse increases, whereas in the LF, the TOAM of both $\\gamma$-photons and positrons decreases.","And the result under the same pulse intensity is better than that under the same pulse energy.","The increase in the initial energy of high-energy electrons leads to an enhancement of the TOAM for both $\\gamma$-photons and positrons in both frames.","$\\gamma$-photons and electrons/positrons with TOAM as a new degree of freedom maybe have an extensive applications in optical communication, astrophysics and nanomaterials and so on."],"url":"http://arxiv.org/abs/2403.16414v1","category":"physics.optics"}
{"created":"2024-03-25 04:14:07","title":"Unsupervised Template-assisted Point Cloud Shape Correspondence Network","abstract":"Unsupervised point cloud shape correspondence aims to establish point-wise correspondences between source and target point clouds. Existing methods obtain correspondences directly by computing point-wise feature similarity between point clouds. However, non-rigid objects possess strong deformability and unusual shapes, making it a longstanding challenge to directly establish correspondences between point clouds with unconventional shapes. To address this challenge, we propose an unsupervised Template-Assisted point cloud shape correspondence Network, termed TANet, including a template generation module and a template assistance module. The proposed TANet enjoys several merits. Firstly, the template generation module establishes a set of learnable templates with explicit structures. Secondly, we introduce a template assistance module that extensively leverages the generated templates to establish more accurate shape correspondences from multiple perspectives. Extensive experiments on four human and animal datasets demonstrate that TANet achieves favorable performance against state-of-the-art methods.","sentences":["Unsupervised point cloud shape correspondence aims to establish point-wise correspondences between source and target point clouds.","Existing methods obtain correspondences directly by computing point-wise feature similarity between point clouds.","However, non-rigid objects possess strong deformability and unusual shapes, making it a longstanding challenge to directly establish correspondences between point clouds with unconventional shapes.","To address this challenge, we propose an unsupervised Template-Assisted point cloud shape correspondence Network, termed TANet, including a template generation module and a template assistance module.","The proposed TANet enjoys several merits.","Firstly, the template generation module establishes a set of learnable templates with explicit structures.","Secondly, we introduce a template assistance module that extensively leverages the generated templates to establish more accurate shape correspondences from multiple perspectives.","Extensive experiments on four human and animal datasets demonstrate that TANet achieves favorable performance against state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16412v1","category":"cs.CV"}
{"created":"2024-03-25 04:05:23","title":"Spike-NeRF: Neural Radiance Field Based On Spike Camera","abstract":"As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking. Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes. Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time. To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function. We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator. Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes. Our code and data will be released soon.","sentences":["As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking.","Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes.","Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time.","To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function.","We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator.","Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes.","Our code and data will be released soon."],"url":"http://arxiv.org/abs/2403.16410v1","category":"cs.CV"}
{"created":"2024-03-25 03:47:53","title":"A Survey on Long Video Generation: Challenges, Methods, and Prospects","abstract":"Video generation is a rapidly advancing research area, garnering significant attention due to its broad range of applications. One critical aspect of this field is the generation of long-duration videos, which presents unique challenges and opportunities. This paper presents the first survey of recent advancements in long video generation and summarises them into two key paradigms: divide and conquer temporal autoregressive.   We delve into the common models employed in each paradigm, including aspects of network design and conditioning techniques. Furthermore, we offer a comprehensive overview and classification of the datasets and evaluation metrics which are crucial for advancing long video generation research. Concluding with a summary of existing studies, we also discuss the emerging challenges and future directions in this dynamic field. We hope that this survey will serve as an essential reference for researchers and practitioners in the realm of long video generation.","sentences":["Video generation is a rapidly advancing research area, garnering significant attention due to its broad range of applications.","One critical aspect of this field is the generation of long-duration videos, which presents unique challenges and opportunities.","This paper presents the first survey of recent advancements in long video generation and summarises them into two key paradigms: divide and conquer temporal autoregressive.   ","We delve into the common models employed in each paradigm, including aspects of network design and conditioning techniques.","Furthermore, we offer a comprehensive overview and classification of the datasets and evaluation metrics which are crucial for advancing long video generation research.","Concluding with a summary of existing studies, we also discuss the emerging challenges and future directions in this dynamic field.","We hope that this survey will serve as an essential reference for researchers and practitioners in the realm of long video generation."],"url":"http://arxiv.org/abs/2403.16407v1","category":"cs.CV"}
{"created":"2024-03-25 03:38:06","title":"Douglas-Rudin Approximation theorem for operator-valued functions on the unit ball of $\\mathbb{C}^d$","abstract":"Douglas and Rudin proved that any unimodular function on the unit circle $\\T$ can be uniformly approximated by quotients of inner functions. We extend this result to the operator-valued unimodular functions defined on the boundary of the open unit ball of $\\mathbb{C}^d$. Our proof technique combines the spectral theorem for unitary operators with the Douglas-Rudin theorem in the scalar case to bootstrap the result to the operator-valued case. This yields a new proof and a significant generalization of Barclay's result [Proc. Lond. Math. Soc. 2009] on approximation of matrix-valued unimodular functions on $\\T$.","sentences":["Douglas and Rudin proved that any unimodular function on the unit circle $\\T$ can be uniformly approximated by quotients of inner functions.","We extend this result to the operator-valued unimodular functions defined on the boundary of the open unit ball of $\\mathbb{C}^d$. Our proof technique combines the spectral theorem for unitary operators with the Douglas-Rudin theorem in the scalar case to bootstrap the result to the operator-valued case.","This yields a new proof and a significant generalization of Barclay's result [Proc. Lond.","Math.","Soc. 2009] on approximation of matrix-valued unimodular functions on $\\T$."],"url":"http://arxiv.org/abs/2403.16401v1","category":"math.FA"}
{"created":"2024-03-25 03:26:01","title":"Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data","abstract":"Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating. To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.","sentences":["Federated learning achieves effective performance in modeling decentralized data.","In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data.","However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models.","The former indicates that representation collapse in local model will subsequently impact the global model and other local models.","The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals.","In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data.","Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA).","FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating.","To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100."],"url":"http://arxiv.org/abs/2403.16398v1","category":"cs.LG"}
{"created":"2024-03-25 03:23:10","title":"RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks","abstract":"Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding strategy that leverages radio propagation models to capture the spatial-spectral correlation inherent in the data. This graph-based representation not only simplifies data handling but also enables tailored label sampling during training, significantly enhancing the framework's adaptability for deployment. Subsequently, The GAT is employed to generalize the radiomap information across various frequency bands. Extensive experiments using raytracing datasets based on real-world environments have demonstrated RadioGAT's enhanced accuracy in supervised learning settings and its robustness in semi-supervised scenarios. These results underscore RadioGAT's effectiveness and practicality for MB-RMR in environments with limited data availability.","sentences":["Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning.","However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges.","These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements.","To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets.","RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources.","The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding strategy that leverages radio propagation models to capture the spatial-spectral correlation inherent in the data.","This graph-based representation not only simplifies data handling but also enables tailored label sampling during training, significantly enhancing the framework's adaptability for deployment.","Subsequently, The GAT is employed to generalize the radiomap information across various frequency bands.","Extensive experiments using raytracing datasets based on real-world environments have demonstrated RadioGAT's enhanced accuracy in supervised learning settings and its robustness in semi-supervised scenarios.","These results underscore RadioGAT's effectiveness and practicality for MB-RMR in environments with limited data availability."],"url":"http://arxiv.org/abs/2403.16397v1","category":"eess.SP"}
{"created":"2024-03-25 03:18:39","title":"Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation","abstract":"The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations. But there lacks a formal understanding of how entity-relation compositions can be effectively learned. Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining. We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage. We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance. Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size. This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size. Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for reasoning with abstract relations.","sentences":["The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations.","But there lacks a formal understanding of how entity-relation compositions can be effectively learned.","Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining.","We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization.","We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage.","We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance.","Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size.","This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size.","Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for reasoning with abstract relations."],"url":"http://arxiv.org/abs/2403.16394v1","category":"cs.CL"}
{"created":"2024-03-25 03:17:27","title":"Concurrent Linguistic Error Detection (CLED) for Large Language Models","abstract":"The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only relies on the outputs of the model, then it can be used on LLMs in which there is no access to the internal nodes. The proposed CLED scheme has been evaluated on the T5 model when used for news summarization and on the OPUS-MT model when used for translation. In both cases, the same set of linguistic features has been used for error detection to illustrate the applicability of the proposed scheme beyond a specific case. The results show that CLED can detect most of the errors at a low overhead penalty. The use of the concurrent classifier also enables a trade-off between error detection effectiveness and its associated overhead, so providing flexibility to a designer.","sentences":["The wide adoption of Large language models (LLMs) makes their dependability a pressing concern.","Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue.","In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes.","An interesting observation is that the output of LLMs in error-free operation should be valid and normal text.","Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error.","Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors.","Since the proposed error detection mechanism only relies on the outputs of the model, then it can be used on LLMs in which there is no access to the internal nodes.","The proposed CLED scheme has been evaluated on the T5 model when used for news summarization and on the OPUS-MT model when used for translation.","In both cases, the same set of linguistic features has been used for error detection to illustrate the applicability of the proposed scheme beyond a specific case.","The results show that CLED can detect most of the errors at a low overhead penalty.","The use of the concurrent classifier also enables a trade-off between error detection effectiveness and its associated overhead, so providing flexibility to a designer."],"url":"http://arxiv.org/abs/2403.16393v1","category":"cs.AI"}
{"created":"2024-03-25 03:12:51","title":"Illuminating Systematic Trends in Nuclear Data with Generative Machine Learning Models","abstract":"We introduce a novel method for studying systematic trends in nuclear reaction data using generative adversarial networks. Libraries of nuclear cross section evaluations exhibit intricate systematic trends across the nuclear landscape, and predictive models capable of reproducing and analyzing these trends are valuable for many applications. We have developed a predictive model using deep generative adversarial networks to learn trends from the inelastic neutron scattering channel of TENDL for even-even nuclei. The system predicts cross sections based on adding/subtracting particles to/from the target nucleus. It can thus help identify cross sections that break from expected trends and predict beyond the limit of current experiments. Our model can produce good predictions for cross section curves for many nuclides, and it is most robust near the line of stability. We also create an ensemble of predictions to leverage different correlations and estimate model uncertainty. This research marks an important first step in computer generation of nuclear cross-section libraries.","sentences":["We introduce a novel method for studying systematic trends in nuclear reaction data using generative adversarial networks.","Libraries of nuclear cross section evaluations exhibit intricate systematic trends across the nuclear landscape, and predictive models capable of reproducing and analyzing these trends are valuable for many applications.","We have developed a predictive model using deep generative adversarial networks to learn trends from the inelastic neutron scattering channel of TENDL for even-even nuclei.","The system predicts cross sections based on adding/subtracting particles to/from the target nucleus.","It can thus help identify cross sections that break from expected trends and predict beyond the limit of current experiments.","Our model can produce good predictions for cross section curves for many nuclides, and it is most robust near the line of stability.","We also create an ensemble of predictions to leverage different correlations and estimate model uncertainty.","This research marks an important first step in computer generation of nuclear cross-section libraries."],"url":"http://arxiv.org/abs/2403.16389v1","category":"nucl-th"}
{"created":"2024-03-25 03:06:54","title":"On a fibrational construction for optics, lenses, and Dialectica categories","abstract":"Categories of lenses/optics and Dialectica categories are both comprised of bidirectional morphisms of basically the same form. In this work we show how they can be considered a special case of an overarching fibrational construction, generalizing Hofstra's construction of Dialectica fibrations and Spivak's construction of generalized lenses. This construction turns a tower of Grothendieck fibrations into another tower of fibrations by iteratively twisting each of the components, using the opposite fibration construction.","sentences":["Categories of lenses/optics and Dialectica categories are both comprised of bidirectional morphisms of basically the same form.","In this work we show how they can be considered a special case of an overarching fibrational construction, generalizing Hofstra's construction of Dialectica fibrations and Spivak's construction of generalized lenses.","This construction turns a tower of Grothendieck fibrations into another tower of fibrations by iteratively twisting each of the components, using the opposite fibration construction."],"url":"http://arxiv.org/abs/2403.16388v1","category":"math.CT"}
{"created":"2024-03-25 03:02:51","title":"Dia-LLaMA: Towards Large Language Model-driven CT Report Generation","abstract":"Medical report generation has achieved remarkable advancements yet has still been faced with several challenges. First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses. Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information. Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs. Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges. In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance prompts. Considering the high dimension of CT, we leverage a pre-trained ViT3D with perceiver to extract the visual information. To tailor the LLM for report generation and emphasize abnormality, we extract additional diagnostic information by referring to a disease prototype memory bank, which is updated during training to capture common disease representations. Furthermore, we introduce disease-aware attention to enable the model to adjust attention for different diseases. Experiments on the chest CT dataset demonstrated that our proposed method outperformed previous methods and achieved state-of-the-art on both clinical efficacy performance and natural language generation metrics. The code will be made publically available.","sentences":["Medical report generation has achieved remarkable advancements yet has still been faced with several challenges.","First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses.","Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information.","Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs.","Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges.","In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance prompts.","Considering the high dimension of CT, we leverage a pre-trained ViT3D with perceiver to extract the visual information.","To tailor the LLM for report generation and emphasize abnormality, we extract additional diagnostic information by referring to a disease prototype memory bank, which is updated during training to capture common disease representations.","Furthermore, we introduce disease-aware attention to enable the model to adjust attention for different diseases.","Experiments on the chest CT dataset demonstrated that our proposed method outperformed previous methods and achieved state-of-the-art on both clinical efficacy performance and natural language generation metrics.","The code will be made publically available."],"url":"http://arxiv.org/abs/2403.16386v1","category":"cs.CV"}
{"created":"2024-03-25 03:02:27","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA","abstract":"Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight the significance of the proposed step-by-step generation. By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning. We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks.","sentences":["Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics.","Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions.","In this work, we address the lack of reasoning ability by data augmentation.","We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images.","The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python.","This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline.","Experimental results highlight the significance of the proposed step-by-step generation.","By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets.","In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning.","We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks."],"url":"http://arxiv.org/abs/2403.16385v1","category":"cs.CV"}
{"created":"2024-03-25 03:00:34","title":"Nonequilibrium Bound for Canonical Nonlinearity Under Single-Shot Work","abstract":"For classical discrete systems under constant composition (specifically substitutional alloys), canonical average acts as a map from a set of many-body interatomic interactions to a set of configuration in thermodynamic equilibrium, which is generally nonlinear. In terms of the configurational geometry (i.e., information about configurational density of states), the nonlinearity has been measured as special vector on configuration space, which is extended to Kullback-Leibler (KL) divergence on statistical manifold. Although they successfully provide new insight into how the geometry of lattice characterizes the nonlinearity, their application is essentially restricted to thermodynamic equilibrium. Based on the resource theory (especially, thermo-majorization), we here extend the applicability of the nonlinearity to nonequilibrium states obtained through single-shot work on Gibbs state. We derive the bound for the extended nonlinearity in nonequilibrium state, characterized by the nonlinearity in equilibrium state, Renyi alpha-divergence between equilibrium and nonequilibrium distribution, temperature and work.","sentences":["For classical discrete systems under constant composition (specifically substitutional alloys), canonical average acts as a map from a set of many-body interatomic interactions to a set of configuration in thermodynamic equilibrium, which is generally nonlinear.","In terms of the configurational geometry (i.e., information about configurational density of states), the nonlinearity has been measured as special vector on configuration space, which is extended to Kullback-Leibler (KL) divergence on statistical manifold.","Although they successfully provide new insight into how the geometry of lattice characterizes the nonlinearity, their application is essentially restricted to thermodynamic equilibrium.","Based on the resource theory (especially, thermo-majorization), we here extend the applicability of the nonlinearity to nonequilibrium states obtained through single-shot work on Gibbs state.","We derive the bound for the extended nonlinearity in nonequilibrium state, characterized by the nonlinearity in equilibrium state, Renyi alpha-divergence between equilibrium and nonequilibrium distribution, temperature and work."],"url":"http://arxiv.org/abs/2403.16383v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-25 02:53:32","title":"FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models","abstract":"In recent years, there has been significant progress in the development of text-to-image generative models. Evaluating the quality of the generative models is one essential step in the development process. Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical. Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset. We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level). We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection. We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations, including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets. Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup. We release the condensed subset of these commonly used datasets to help facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at https://github.com/thu-nics/FlashEval.","sentences":["In recent years, there has been significant progress in the development of text-to-image generative models.","Evaluating the quality of the generative models is one essential step in the development process.","Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical.","Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset.","We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level).","We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection.","We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations, including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets.","Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup.","We release the condensed subset of these commonly used datasets to help facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at https://github.com/thu-nics/FlashEval."],"url":"http://arxiv.org/abs/2403.16379v1","category":"cs.CV"}
{"created":"2024-03-25 02:53:32","title":"Tensor Neural Network Based Machine Learning Method for Elliptic Multiscale Problems","abstract":"In this paper, we introduce a type of tensor neural network based machine learning method to solve elliptic multiscale problems. Based on the special structure, we can do the direct and highly accurate high dimensional integrations for the tensor neural network functions without Monte Carlo process. Here, with the help of homogenization techniques, the multiscale problem is first transformed to the high dimensional limit problem with reasonable accuracy. Then, based on the tensor neural network, we design a type of machine learning method to solve the derived high dimensional limit problem. The proposed method in this paper brings a new way to design numerical methods for computing more general multiscale problems with high accuracy. Several numerical examples are also provided to validate the accuracy of the proposed numerical methods.","sentences":["In this paper, we introduce a type of tensor neural network based machine learning method to solve elliptic multiscale problems.","Based on the special structure, we can do the direct and highly accurate high dimensional integrations for the tensor neural network functions without Monte Carlo process.","Here, with the help of homogenization techniques, the multiscale problem is first transformed to the high dimensional limit problem with reasonable accuracy.","Then, based on the tensor neural network, we design a type of machine learning method to solve the derived high dimensional limit problem.","The proposed method in this paper brings a new way to design numerical methods for computing more general multiscale problems with high accuracy.","Several numerical examples are also provided to validate the accuracy of the proposed numerical methods."],"url":"http://arxiv.org/abs/2403.16380v1","category":"math.NA"}
{"created":"2024-03-25 02:52:42","title":"Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models","abstract":"The rise of large language models (LLMs) has opened new opportunities in Recommender Systems (RSs) by enhancing user behavior modeling and content understanding. However, current approaches that integrate LLMs into RSs solely utilize either LLM or conventional recommender model (CRM) to generate final recommendations, without considering which data segments LLM or CRM excel in. To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that LLMs excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for LLM, requiring substantial training data and a long training time for comparable performance. This suggests potential synergies in the combination between LLM and CRM. Motivated by these insights, we propose Collaborative Recommendation with conventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}). In this framework, we first jointly train LLM and CRM and address the issue of decision boundary shifts through alignment loss. Then, the resource-efficient CRM, with a shorter inference time, handles simple and moderate samples, while LLM processes the small subset of challenging samples for CRM. Our experimental results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM methods significantly, underscoring its effectiveness in recommendation tasks.","sentences":["The rise of large language models (LLMs) has opened new opportunities in Recommender Systems (RSs) by enhancing user behavior modeling and content understanding.","However, current approaches that integrate LLMs into RSs solely utilize either LLM or conventional recommender model (CRM) to generate final recommendations, without considering which data segments LLM or CRM excel in.","To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an LLM (LLaMA2-7B) on various groups of data samples.","Our findings reveal that LLMs excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for LLM, requiring substantial training data and a long training time for comparable performance.","This suggests potential synergies in the combination between LLM and CRM.","Motivated by these insights, we propose Collaborative Recommendation with conventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}).","In this framework, we first jointly train LLM and CRM and address the issue of decision boundary shifts through alignment loss.","Then, the resource-efficient CRM, with a shorter inference time, handles simple and moderate samples, while LLM processes the small subset of challenging samples for CRM.","Our experimental results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM methods significantly, underscoring its effectiveness in recommendation tasks."],"url":"http://arxiv.org/abs/2403.16378v1","category":"cs.IR"}
{"created":"2024-03-25 02:31:57","title":"Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation","abstract":"Sequential Recommenders have been widely applied in various online services, aiming to model users' dynamic interests from their sequential interactions. With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated. However, existing sequential recommender models often struggle to handle such lifelong sequences. The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence. Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged. In this work, we investigate the performance of Mamba for lifelong sequential recommendation (i.e., length>=2k). More specifically, we leverage the Mamba block to model lifelong user sequences selectively. We conduct extensive experiments to evaluate the performance of representative sequential recommendation models in the setting of lifelong sequences. Experiments on two real-world datasets demonstrate the superiority of Mamba. We found that RecMamba achieves performance comparable to the representative model while significantly reducing training duration by approximately 70% and memory costs by 80%. Codes and data are available at \\url{https://github.com/nancheng58/RecMamba}.","sentences":["Sequential Recommenders have been widely applied in various online services, aiming to model users' dynamic interests from their sequential interactions.","With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated.","However, existing sequential recommender models often struggle to handle such lifelong sequences.","The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence.","Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged.","In this work, we investigate the performance of Mamba for lifelong sequential recommendation (i.e., length>=2k).","More specifically, we leverage the Mamba block to model lifelong user sequences selectively.","We conduct extensive experiments to evaluate the performance of representative sequential recommendation models in the setting of lifelong sequences.","Experiments on two real-world datasets demonstrate the superiority of Mamba.","We found that RecMamba achieves performance comparable to the representative model while significantly reducing training duration by approximately 70% and memory costs by 80%.","Codes and data are available at \\url{https://github.com/nancheng58/RecMamba}."],"url":"http://arxiv.org/abs/2403.16371v1","category":"cs.IR"}
{"created":"2024-03-25 02:30:32","title":"GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation","abstract":"This paper tackles a novel yet challenging problem: how to transfer knowledge from the emerging Segment Anything Model (SAM) -- which reveals impressive zero-shot instance segmentation capacity -- to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data. This poses considerable challenges due to SAM's inability to provide semantic labels and the large capacity gap between SAM and the student. To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve knowledge transfer. Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement. This subtly enhances TA's prediction capacity on panoramic images. DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation (MKA) module to efficiently transfer the multi-level feature knowledge from TA and ensemble logits to learn a compact student model. Extensive experiments on two benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU improvement over the state-of-the-art (SOTA) domain adaptation methods. Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters.","sentences":["This paper tackles a novel yet challenging problem: how to transfer knowledge from the emerging Segment Anything Model (SAM) -- which reveals impressive zero-shot instance segmentation capacity -- to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data.","This poses considerable challenges due to SAM's inability to provide semantic labels and the large capacity gap between SAM and the student.","To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve knowledge transfer.","Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement.","This subtly enhances TA's prediction capacity on panoramic images.","DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits.","Moreover, we introduce a Multi-level Knowledge Adaptation (MKA) module to efficiently transfer the multi-level feature knowledge from TA and ensemble logits to learn a compact student model.","Extensive experiments on two benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU improvement over the state-of-the-art (SOTA) domain adaptation methods.","Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters."],"url":"http://arxiv.org/abs/2403.16370v1","category":"cs.CV"}
{"created":"2024-03-25 02:17:54","title":"Learning Action-based Representations Using Invariance","abstract":"Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts distant state features that are relevant for control. We demonstrate that action-bisimulation pretraining on reward-free, uniformly random data improves sample efficiency in several environments, including a photorealistic 3D simulation domain, Habitat. Additionally, we provide theoretical analysis and qualitative results demonstrating the information captured by action-bisimulation.","sentences":["Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors.","A representation that captures controllability identifies these state elements by determining what affects agent control.","While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem.","Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away.","To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint.","By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts distant state features that are relevant for control.","We demonstrate that action-bisimulation pretraining on reward-free, uniformly random data improves sample efficiency in several environments, including a photorealistic 3D simulation domain, Habitat.","Additionally, we provide theoretical analysis and qualitative results demonstrating the information captured by action-bisimulation."],"url":"http://arxiv.org/abs/2403.16369v1","category":"cs.LG"}
{"created":"2024-03-25 02:17:20","title":"Distilling Semantic Priors from SAM to Efficient Image Restoration Models","abstract":"In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models. Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising.","sentences":["In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance.","The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks.","However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models.","The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency.","To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process.","Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme.","SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image.","SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models.","Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors.","We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising."],"url":"http://arxiv.org/abs/2403.16368v1","category":"cs.CV"}
{"created":"2024-03-25 02:03:38","title":"Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion","abstract":"Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness. Our implementation code is publicly available at: https://github.com/hsouri/GDP .","sentences":["Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection.","As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it.","Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons.","However, some base samples may be significantly more amenable to poisoning than others.","As a result, we may be able to craft more potent poisons by carefully choosing the base samples.","In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks.","Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness.","Our implementation code is publicly available at: https://github.com/hsouri/GDP ."],"url":"http://arxiv.org/abs/2403.16365v1","category":"cs.LG"}
{"created":"2024-03-25 01:58:39","title":"Non-relativistic stellar structure in the Fierz--Pauli theory and generic linear massive gravity","abstract":"We study the structure of static spherical stars composed of non-relativistic matter in linear massive gravity with or without the Fierz-Pauli (FP) tuning. Adopting a polytropic equation of state, we construct master differential equations for the stellar profile function, which is fourth order in the FP theory or sixth order in generic non-FP theories, where the difference in the differential order reflects the presence of a ghost spin-0 graviton in the latter. In both cases, even when the spin-0 ghost is present, we find exact solutions with finite radius for the polytropic indices n = 0 and 1. Analyzing the dependences of the stellar radius, mass, and Yukawa charge on the graviton masses, we observe that a discontinuous behavior arises in the massless limit of the FP theory similarly to the van Dam-Veltman-Zakharov discontinuity, while it is absent in non-FP theories. We discuss rough observational constraints on the graviton masses.","sentences":["We study the structure of static spherical stars composed of non-relativistic matter in linear massive gravity with or without the Fierz-Pauli (FP) tuning.","Adopting a polytropic equation of state, we construct master differential equations for the stellar profile function, which is fourth order in the FP theory or sixth order in generic non-FP theories, where the difference in the differential order reflects the presence of a ghost spin-0 graviton in the latter.","In both cases, even when the spin-0 ghost is present, we find exact solutions with finite radius for the polytropic indices n = 0 and 1.","Analyzing the dependences of the stellar radius, mass, and Yukawa charge on the graviton masses, we observe that a discontinuous behavior arises in the massless limit of the FP theory similarly to the van Dam-Veltman-Zakharov discontinuity, while it is absent in non-FP theories.","We discuss rough observational constraints on the graviton masses."],"url":"http://arxiv.org/abs/2403.16363v1","category":"gr-qc"}
{"created":"2024-03-25 01:47:14","title":"Application of Floquet-Magnus expansion and Fer expansion to investigate the chemical shift anisotropy in solid-state NMR when irradiated with the triple oscillating field technique","abstract":"The Floquet-Magnus and Fer expansion schemes were introduced in solid-state nuclear magnetic resonance (NMR) in 2011 and 2006, respectively. Key features of the Floquet magnus expansion are its ability to account for the calculations developed in a finite-dimensional Hilbert space instead of an infinite-dimensional space within the Floquet theory as well as its use of its distinguishable function from other theories such as average Hamiltonian theory, Floquet theory, and Fer expansion, which facilitates the evaluation of the spin behavior in between the stroboscopic observation points. This article focuses on revisiting the Floquet-Magnus and Fer expansion approaches and applying both methods to calculate the effective Hamiltonians and propagators, which control the spin system evolution during the Triple Oscillating Field Technique radiation experiment (TOFU). The TOFU pulse sequence is an important technique that was shown to avoid the dipolar truncation problem and form a new basis for accurate distance measurement by solid-state NMR. We take advantage of the interaction frequencies and the time modulation arising from the TOFU pulse sequence allowing selective recoupling of specific terms in the Hamiltonian that fulfill determined specific conditions. The work presented unifies and generalizes existing results of the Floquet-Magnus and Fer expansions and delivers illustrations of novel springs that boost previous applications that are based on the classical information. We believe that the generality of this work points to potential applications in problems related to theoretical developments of spectroscopy as well as interdisciplinary research areas whenever they include the spin dynamics concepts.","sentences":["The Floquet-Magnus and Fer expansion schemes were introduced in solid-state nuclear magnetic resonance (NMR) in 2011 and 2006, respectively.","Key features of the Floquet magnus expansion are its ability to account for the calculations developed in a finite-dimensional Hilbert space instead of an infinite-dimensional space within the Floquet theory as well as its use of its distinguishable function from other theories such as average Hamiltonian theory, Floquet theory, and Fer expansion, which facilitates the evaluation of the spin behavior in between the stroboscopic observation points.","This article focuses on revisiting the Floquet-Magnus and Fer expansion approaches and applying both methods to calculate the effective Hamiltonians and propagators, which control the spin system evolution during the Triple Oscillating Field Technique radiation experiment (TOFU).","The TOFU pulse sequence is an important technique that was shown to avoid the dipolar truncation problem and form a new basis for accurate distance measurement by solid-state NMR.","We take advantage of the interaction frequencies and the time modulation arising from the TOFU pulse sequence allowing selective recoupling of specific terms in the Hamiltonian that fulfill determined specific conditions.","The work presented unifies and generalizes existing results of the Floquet-Magnus and Fer expansions and delivers illustrations of novel springs that boost previous applications that are based on the classical information.","We believe that the generality of this work points to potential applications in problems related to theoretical developments of spectroscopy as well as interdisciplinary research areas whenever they include the spin dynamics concepts."],"url":"http://arxiv.org/abs/2403.16359v1","category":"physics.chem-ph"}
{"created":"2024-03-25 01:44:34","title":"ChebMixer: Efficient Graph Representation Learning with MLP Mixer","abstract":"Graph neural networks have achieved remarkable success in learning graph representations, especially graph Transformer, which has recently shown superior performance on various graph mining tasks. However, graph Transformer generally treats nodes as tokens, which results in quadratic complexity regarding the number of nodes during self-attention computation. The graph MLP Mixer addresses this challenge by using the efficient MLP Mixer technique from computer vision. However, the time-consuming process of extracting graph tokens limits its performance. In this paper, we present a novel architecture named ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens. Firstly, we produce multiscale representations of graph nodes via fast Chebyshev polynomial-based spectral filtering. Next, we consider each node's multiscale representations as a sequence of tokens and refine the node representation with an effective MLP Mixer. Finally, we aggregate the multiscale representations of nodes through Chebyshev interpolation. Owing to the powerful representation capabilities and fast computational properties of MLP Mixer, we can quickly extract more informative node representations to improve the performance of downstream tasks. The experimental results prove our significant improvements in a variety of scenarios ranging from graph node classification to medical image segmentation.","sentences":["Graph neural networks have achieved remarkable success in learning graph representations, especially graph Transformer, which has recently shown superior performance on various graph mining tasks.","However, graph Transformer generally treats nodes as tokens, which results in quadratic complexity regarding the number of nodes during self-attention computation.","The graph MLP Mixer addresses this challenge by using the efficient MLP Mixer technique from computer vision.","However, the time-consuming process of extracting graph tokens limits its performance.","In this paper, we present a novel architecture named ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens.","Firstly, we produce multiscale representations of graph nodes via fast Chebyshev polynomial-based spectral filtering.","Next, we consider each node's multiscale representations as a sequence of tokens and refine the node representation with an effective MLP Mixer.","Finally, we aggregate the multiscale representations of nodes through Chebyshev interpolation.","Owing to the powerful representation capabilities and fast computational properties of MLP Mixer, we can quickly extract more informative node representations to improve the performance of downstream tasks.","The experimental results prove our significant improvements in a variety of scenarios ranging from graph node classification to medical image segmentation."],"url":"http://arxiv.org/abs/2403.16358v1","category":"cs.CV"}
{"created":"2024-03-25 01:33:03","title":"Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability","abstract":"We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation. We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics. We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner. The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints. Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints. We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map. We evaluate the efficacy of our planning framework on Digit bipedal robot simulation in MuJoCo.","sentences":["We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain.","In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation.","Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations.","Thus, the problems of terrain mapping and locomotion stability are intertwined.","We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation.","We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics.","We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner.","The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints.","Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints.","We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map.","We evaluate the efficacy of our planning framework on Digit bipedal robot simulation in MuJoCo."],"url":"http://arxiv.org/abs/2403.16356v1","category":"cs.RO"}
{"created":"2024-03-25 01:12:57","title":"ChatDBG: An AI-Powered Debugging Assistant","abstract":"This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times.","sentences":["This paper presents ChatDBG, the first AI-powered debugging assistant.","ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers.","ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\".","To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer.","Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python.","Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors.","For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%.","ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times."],"url":"http://arxiv.org/abs/2403.16354v1","category":"cs.SE"}
{"created":"2024-03-25 00:51:25","title":"A Multivariate Berry--Esseen theorem for time-dependent expanding dynamical systems","abstract":"We adapt Stein's method to obtain Berry--Esseen type error bounds in the multivariate central limit theorem for non-stationary processes generated by time-dependent compositions of uniformly expanding dynamical systems. In a particular case of random dynamical systems with a strongly mixing base transformation, we derive an error estimate of order $O(N^{-1/2})$ in the quenched multivariate CLT, provided that the covariance matrix \\say{grows linearly} with the number of summands $N$. The error in the normal approximation is estimated for the class of all convex sets.","sentences":["We adapt Stein's method to obtain Berry--Esseen type error bounds in the multivariate central limit theorem for non-stationary processes generated by time-dependent compositions of uniformly expanding dynamical systems.","In a particular case of random dynamical systems with a strongly mixing base transformation, we derive an error estimate of order $O(N^{-1/2})$ in the quenched multivariate CLT, provided that the covariance matrix \\say{grows linearly} with the number of summands $N$. The error in the normal approximation is estimated for the class of all convex sets."],"url":"http://arxiv.org/abs/2403.16349v1","category":"math.DS"}
{"created":"2024-03-25 00:51:13","title":"Quadratic embedding constants of fan graphs and graph joins","abstract":"We derive a general formula for the quadratic embedding constant of a graph join $\\bar{K}_m+G$, where $\\bar{K}_m$ is the empty graph on $m\\ge1$ vertices and $G$ is an arbitrary graph. Applying our formula to a fan graph $K_1+P_n$, where $K_1=\\bar{K}_1$ is the singleton graph and $P_n$ is the path on $n\\ge1$ vertices, we show that $\\mathrm{QEC}(K_1+P_n)=-\\tilde{\\alpha}_n-2$, where $\\tilde{\\alpha}_n$ is the minimal zero of a new polynomial $\\Phi_n(x)$ related to Chebyshev polynomials of the second kind. Moreover, for an even $n$ we have $\\tilde{\\alpha}_n=\\min\\mathrm{ev}(A_n)$, where the right-hand side is the An minimal eigenvalue of the adjacency matrix $A_n$ of $P_n$. For an odd $n$ we show that $\\min\\mathrm{ev}(A_{n+1})\\le\\tilde{\\alpha}_n<\\min\\mathrm{ev}(A_n)$.","sentences":["We derive a general formula for the quadratic embedding constant of a graph join $\\bar{K}_m+G$, where $\\bar{K}_m$ is the empty graph on $m\\ge1$ vertices and $G$ is an arbitrary graph.","Applying our formula to a fan graph $K_1+P_n$, where $K_1=\\bar{K}_1$ is the singleton graph and $P_n$ is the path on $n\\ge1$ vertices, we show that $\\mathrm{QEC}(K_1+P_n)=-\\tilde{\\alpha}_n-2$, where $\\tilde{\\alpha}_n$ is the minimal zero of a new polynomial $\\Phi_n(x)$ related to Chebyshev polynomials of the second kind.","Moreover, for an even $n$ we have $\\tilde{\\alpha}_n=\\min\\mathrm{ev}(A_n)$, where the right-hand side is the An minimal eigenvalue of the adjacency matrix $A_n$ of $P_n$. For an odd $n$ we show that $\\min\\mathrm{ev}(A_{n+1})\\le\\tilde{\\alpha}_n<\\min\\mathrm{ev}(A_n)$."],"url":"http://arxiv.org/abs/2403.16348v1","category":"math.CO"}
{"created":"2024-03-25 00:50:27","title":"ChatGPT Incorrectness Detection in Software Reviews","abstract":"We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 - 0.75.","sentences":["We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks.","We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses.","We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses.","CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts).","The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response.","In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 - 0.75."],"url":"http://arxiv.org/abs/2403.16347v1","category":"cs.SE"}
{"created":"2024-03-25 00:43:44","title":"Enhanced Facet Generation with LLM Editing","abstract":"In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine. The first strategy is multi-task learning to predict SERP. By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules. The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model. Overall performance improves when small model and LLM are combined rather than facet generation individually.","sentences":["In information retrieval, facet identification of a user query is an important task.","If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results.","Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine.","However, there are challenges in extending it to other applications when a search engine operates as part of the model.","First, search engines are constantly updated.","Therefore, additional information may change during training and test, which may reduce performance.","The second challenge is that public search engines cannot search for internal documents.","Therefore, a separate search system needs to be built to incorporate documents from private domains within the company.","We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine.","The first strategy is multi-task learning to predict SERP.","By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules.","The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model.","Overall performance improves when small model and LLM are combined rather than facet generation individually."],"url":"http://arxiv.org/abs/2403.16345v1","category":"cs.CL"}
{"created":"2024-03-25 00:42:50","title":"Percentile Optimization in Wireless Networks- Part I: Power Control for Max-Min-Rate to Sum-Rate Maximization (and Everything in Between)","abstract":"Improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks. While a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them. In this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users. Part I focuses on the class of least-percentile throughput maximization through power control. This class subsumes the well-known max-min and max-sum-rate optimization problems as special cases. Apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly NP-hard in general for multiuser interference networks, making optimization extremely challenging. We propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points. Comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster.","sentences":["Improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks.","While a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them.","In this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users.","Part I focuses on the class of least-percentile throughput maximization through power control.","This class subsumes the well-known max-min and max-sum-rate optimization problems as special cases.","Apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly NP-hard in general for multiuser interference networks, making optimization extremely challenging.","We propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points.","Comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster."],"url":"http://arxiv.org/abs/2403.16344v1","category":"cs.IT"}
{"created":"2024-03-25 00:39:45","title":"A generalization of the first Tits construction","abstract":"Let $F$ be a field of characteristic not 2 or 3. The first Tits construction is a well-known tripling process to construct separable cubic Jordan algebras, especially Albert algebras. We generalize the first Tits construction by choosing the scalar employed in the tripling process outside of the base field. This yields a new family of nonassociative unital algebras which carry a cubic map, and maps that can be viewed as generalized adjoint and generalized trace maps. These maps display properties often similar to the ones in the classical setup. In particular, the cubic norm map permits some kind of weak Jordan composition law.","sentences":["Let $F$ be a field of characteristic not 2 or 3.","The first Tits construction is a well-known tripling process to construct separable cubic Jordan algebras, especially Albert algebras.","We generalize the first Tits construction by choosing the scalar employed in the tripling process outside of the base field.","This yields a new family of nonassociative unital algebras which carry a cubic map, and maps that can be viewed as generalized adjoint and generalized trace maps.","These maps display properties often similar to the ones in the classical setup.","In particular, the cubic norm map permits some kind of weak Jordan composition law."],"url":"http://arxiv.org/abs/2403.16342v1","category":"math.RA"}
{"created":"2024-03-25 00:24:10","title":"Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks","abstract":"Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system. The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle's life cycle). Lossless compression doesn't provide sufficient compression ratios, hence, lossy video compression has been explored. It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms. However, there is limited work in this area to provide a solid conclusion. In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts. Fisheye cameras are commonly used in automotive systems for 3D object detection task. In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fisheye camera images. We demonstrate that the achievable compression with negligible impact depends on the dataset and temporal prediction of the video codec. We propose a radial distortion-aware zonal metric to evaluate the performance of artifacts in fisheye images. In addition, we present a novel method for estimating affine mode parameters of the latest VVC codec, and suggest some areas for improvement in video codecs for the application to fisheye imagery.","sentences":["Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system.","The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle's life cycle).","Lossless compression doesn't provide sufficient compression ratios, hence, lossy video compression has been explored.","It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms.","However, there is limited work in this area to provide a solid conclusion.","In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts.","Fisheye cameras are commonly used in automotive systems for 3D object detection task.","In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fisheye camera images.","We demonstrate that the achievable compression with negligible impact depends on the dataset and temporal prediction of the video codec.","We propose a radial distortion-aware zonal metric to evaluate the performance of artifacts in fisheye images.","In addition, we present a novel method for estimating affine mode parameters of the latest VVC codec, and suggest some areas for improvement in video codecs for the application to fisheye imagery."],"url":"http://arxiv.org/abs/2403.16338v1","category":"cs.CV"}
{"created":"2024-03-25 00:23:38","title":"On solution of tropical discrete best approximation problems","abstract":"We consider a discrete best approximation problem formulated in the framework of tropical algebra, which deals with the theory and applications of algebraic systems with idempotent operations. Given a set of samples of input and output of an unknown function, the problem is to construct a generalized tropical Puiseux polynomial that best approximates the function in the sense of a tropical distance function. The construction of an approximate polynomial involves the evaluation of both unknown coefficient and exponent of each monomial in the polynomial. To solve the approximation problem, we first reduce the problem to an equation in unknown vector of coefficients, which is given by a matrix with entries parameterized by unknown exponents. We derive a best approximate solution of the equation, which yields both vector of coefficients and approximation error parameterized by the exponents. Optimal values of exponents are found by minimization of the approximation error, which is reduced to a minimization of a function of exponents over all partitions of a finite set. We solve this minimization problem in terms of max-plus algebra (where addition is defined as maximum and multiplication as arithmetic addition) by using a computational procedure based on the agglomerative clustering technique. This solution is extended to the minimization problem of finding optimal exponents in the polynomial in terms of max-algebra (where addition is defined as maximum). The results obtained are applied to develop new solutions for conventional problems of discrete best approximation of real functions by piecewise linear functions and piecewise Puiseux polynomials. We discuss computational complexity of the proposed solution and estimate upper bounds on the computational time. We demonstrate examples of approximation problems solved in terms of max-plus and max-algebra, and give graphical illustrations.","sentences":["We consider a discrete best approximation problem formulated in the framework of tropical algebra, which deals with the theory and applications of algebraic systems with idempotent operations.","Given a set of samples of input and output of an unknown function, the problem is to construct a generalized tropical Puiseux polynomial that best approximates the function in the sense of a tropical distance function.","The construction of an approximate polynomial involves the evaluation of both unknown coefficient and exponent of each monomial in the polynomial.","To solve the approximation problem, we first reduce the problem to an equation in unknown vector of coefficients, which is given by a matrix with entries parameterized by unknown exponents.","We derive a best approximate solution of the equation, which yields both vector of coefficients and approximation error parameterized by the exponents.","Optimal values of exponents are found by minimization of the approximation error, which is reduced to a minimization of a function of exponents over all partitions of a finite set.","We solve this minimization problem in terms of max-plus algebra (where addition is defined as maximum and multiplication as arithmetic addition) by using a computational procedure based on the agglomerative clustering technique.","This solution is extended to the minimization problem of finding optimal exponents in the polynomial in terms of max-algebra (where addition is defined as maximum).","The results obtained are applied to develop new solutions for conventional problems of discrete best approximation of real functions by piecewise linear functions and piecewise Puiseux polynomials.","We discuss computational complexity of the proposed solution and estimate upper bounds on the computational time.","We demonstrate examples of approximation problems solved in terms of max-plus and max-algebra, and give graphical illustrations."],"url":"http://arxiv.org/abs/2403.16337v1","category":"math.NA"}
{"created":"2024-03-25 00:21:34","title":"Predictive Inference in Multi-environment Scenarios","abstract":"We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.","sentences":["We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments.","We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios.","Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems.","We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets."],"url":"http://arxiv.org/abs/2403.16336v1","category":"stat.ML"}
{"created":"2024-03-25 00:17:43","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline","abstract":"The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy parameters. To overcome this challenge, we introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method tailored specifically for ultrasound applications. USLoRA allows for selective fine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters compared to fully fine-tuning only the UNet portion of SD. To enhance dataset diversity, we incorporate different adjectives into the generation process prompts, thereby desensitizing the classifiers to intensity changes across different images. This approach is inspired by clinicians' decision-making processes regarding breast tumors, where tumor shape often plays a more crucial role than intensity. In conclusion, our pipeline not only outperforms classifiers trained on the original dataset but also demonstrates superior performance when encountering unseen datasets. The source code is available at https://github.com/yasamin-med/MEDDAP.","sentences":["The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data.","However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties.","Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer.","To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples.","Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy parameters.","To overcome this challenge, we introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method tailored specifically for ultrasound applications.","USLoRA allows for selective fine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters compared to fully fine-tuning only the UNet portion of SD.","To enhance dataset diversity, we incorporate different adjectives into the generation process prompts, thereby desensitizing the classifiers to intensity changes across different images.","This approach is inspired by clinicians' decision-making processes regarding breast tumors, where tumor shape often plays a more crucial role than intensity.","In conclusion, our pipeline not only outperforms classifiers trained on the original dataset but also demonstrates superior performance when encountering unseen datasets.","The source code is available at https://github.com/yasamin-med/MEDDAP."],"url":"http://arxiv.org/abs/2403.16335v1","category":"eess.IV"}
{"created":"2024-03-25 00:15:34","title":"Graphs Generalization under Distribution Shifts","abstract":"Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution. To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process. However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology. Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations across domains by modeling the potential seen or unseen variations of attribute distribution and topological structure and (2) minimize the discrepancy of the variation in a representation space where the target is to predict semantic labels. Extensive experiment results indicate that our model outperforms baseline methods on node-level OOD generalization across domains in distribution shift on node features and topological structures simultaneously.","sentences":["Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution.","To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process.","However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges.","Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology.","Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge.","To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER).","The goal is to (1) diversify variations across domains by modeling the potential seen or unseen variations of attribute distribution and topological structure and (2) minimize the discrepancy of the variation in a representation space where the target is to predict semantic labels.","Extensive experiment results indicate that our model outperforms baseline methods on node-level OOD generalization across domains in distribution shift on node features and topological structures simultaneously."],"url":"http://arxiv.org/abs/2403.16334v1","category":"cs.LG"}
{"created":"2024-03-25 00:10:38","title":"Expanding the frontiers of cool-dwarf asteroseismology with ESPRESSO. Detection of solar-like oscillations in the K5 dwarf $\u03b5$ Indi","abstract":"Fuelled by space photometry, asteroseismology is vastly benefitting the study of cool main-sequence stars, which exhibit convection-driven solar-like oscillations. Even so, the tiny oscillation amplitudes in K dwarfs continue to pose a challenge to space-based asteroseismology. A viable alternative is offered by the lower stellar noise over the oscillation timescales in Doppler observations. In this letter we present the definite detection of solar-like oscillations in the bright K5 dwarf $\\epsilon$ Indi based on time-intensive observations collected with the ESPRESSO spectrograph at the VLT, thus making it the coolest seismic dwarf ever observed. We measured the frequencies of a total of 19 modes of degree $\\ell=0$--2 along with $\\nu_{\\rm max}=5305\\pm176\\:{\\rm \\mu Hz}$ and $\\Delta\\nu=201.25\\pm0.16\\:{\\rm \\mu Hz}$. The peak amplitude of radial modes is $2.6\\pm0.5\\:{\\rm cm\\,s^{-1}}$, or a mere ${\\sim} 14\\%$ of the solar value. Measured mode amplitudes are ${\\sim} 2$ times lower than predicted from a nominal $L/M$ scaling relation and favour a scaling closer to $(L/M)^{1.5}$ below ${\\sim} 5500\\:{\\rm K}$, carrying important implications for our understanding of the coupling efficiency between pulsations and near-surface convection in K dwarfs. This detection conclusively shows that precise asteroseismology of cool dwarfs is possible down to at least the mid-K regime using next-generation spectrographs on large-aperture telescopes, effectively opening up a new domain in observational asteroseismology.","sentences":["Fuelled by space photometry, asteroseismology is vastly benefitting the study of cool main-sequence stars, which exhibit convection-driven solar-like oscillations.","Even so, the tiny oscillation amplitudes in K dwarfs continue to pose a challenge to space-based asteroseismology.","A viable alternative is offered by the lower stellar noise over the oscillation timescales in Doppler observations.","In this letter we present the definite detection of solar-like oscillations in the bright K5 dwarf $\\epsilon$ Indi based on time-intensive observations collected with the ESPRESSO spectrograph at the VLT, thus making it the coolest seismic dwarf ever observed.","We measured the frequencies of a total of 19 modes of degree $\\ell=0$--2 along with $\\nu_{\\rm max}=5305\\pm176\\:{\\rm \\mu Hz}$ and","$\\Delta\\nu=201.25\\pm0.16\\:{\\rm \\mu Hz}$. The peak amplitude of radial modes is $2.6\\pm0.5\\:{\\rm cm\\,s^{-1}}$, or a mere ${\\sim} 14\\%$ of the solar value.","Measured mode amplitudes are ${\\sim} 2$ times lower than predicted from a nominal $L/M$ scaling relation and favour a scaling closer to $(L/M)^{1.5}$ below ${\\sim} 5500\\:{\\rm K}$, carrying important implications for our understanding of the coupling efficiency between pulsations and near-surface convection in K dwarfs.","This detection conclusively shows that precise asteroseismology of cool dwarfs is possible down to at least the mid-K regime using next-generation spectrographs on large-aperture telescopes, effectively opening up a new domain in observational asteroseismology."],"url":"http://arxiv.org/abs/2403.16333v1","category":"astro-ph.SR"}
{"created":"2024-03-24 23:22:02","title":"Artificial Neural Microcircuits as Building Blocks: Concept and Challenges","abstract":"Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation. However the current trend is for ANNs to be structurally homogeneous. Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems. How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges uncovered during these efforts and explorations of methods by which they might be overcome.","sentences":["Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation.","However the current trend is for ANNs to be structurally homogeneous.","Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting.","In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems.","How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges uncovered during these efforts and explorations of methods by which they might be overcome."],"url":"http://arxiv.org/abs/2403.16327v1","category":"cs.NE"}
{"created":"2024-03-24 23:16:56","title":"Minimal Cellular Resolutions of Path Ideals","abstract":"In this paper, we prove that the path ideals of both paths and cycles have minimal cellular resolutions. Specifically, these minimal free resolutions coincide with the Barile-Macchia resolutions for paths, and their generalized counterparts for cycles. Our result on cycles marks the first instance in the literature where the minimal free resolution of edge ideals of cycles has been established. Furthermore, edge ideals of cycles represent the first class of ideals that lack a minimal Barile-Macchia resolution, yet have a minimal generalized Barile-Macchia resolution.","sentences":["In this paper, we prove that the path ideals of both paths and cycles have minimal cellular resolutions.","Specifically, these minimal free resolutions coincide with the Barile-Macchia resolutions for paths, and their generalized counterparts for cycles.","Our result on cycles marks the first instance in the literature where the minimal free resolution of edge ideals of cycles has been established.","Furthermore, edge ideals of cycles represent the first class of ideals that lack a minimal Barile-Macchia resolution, yet have a minimal generalized Barile-Macchia resolution."],"url":"http://arxiv.org/abs/2403.16324v1","category":"math.AC"}
{"created":"2024-03-24 22:59:24","title":"Enhancing Quantum Entanglement in Bipartite Systems: Leveraging Optimal Control and Physics-Informed Neural Networks","abstract":"Quantum entanglement stands at the forefront of quantum information science, heralding new paradigms in quantum communication, computation, and cryptography. This paper introduces a quantum optimal control approach by focusing on entanglement measures rather than targeting predefined maximally entangled states. Leveraging the indirect Pontryagin Minimum Principle, we formulate an optimal control problem centered on maximizing an enhanced lower bound of the entanglement measure within a shortest timeframe in the presence of input constraints. We derive optimality conditions based on Pontryagin's Minimum Principle tailored for a matrix-valued dynamic control system and tackle the resulting boundary value problem through a Physics-Informed Neural Network, which is adept at handling differential matrix equations. The proposed strategy not only refines the process of generating entangled states but also introduces a method with increased sensitivity in detecting entangled states, thereby overcoming the limitations of conventional concurrence estimation.","sentences":["Quantum entanglement stands at the forefront of quantum information science, heralding new paradigms in quantum communication, computation, and cryptography.","This paper introduces a quantum optimal control approach by focusing on entanglement measures rather than targeting predefined maximally entangled states.","Leveraging the indirect Pontryagin Minimum Principle, we formulate an optimal control problem centered on maximizing an enhanced lower bound of the entanglement measure within a shortest timeframe in the presence of input constraints.","We derive optimality conditions based on Pontryagin's Minimum Principle tailored for a matrix-valued dynamic control system and tackle the resulting boundary value problem through a Physics-Informed Neural Network, which is adept at handling differential matrix equations.","The proposed strategy not only refines the process of generating entangled states but also introduces a method with increased sensitivity in detecting entangled states, thereby overcoming the limitations of conventional concurrence estimation."],"url":"http://arxiv.org/abs/2403.16321v1","category":"quant-ph"}
{"created":"2024-03-24 22:53:16","title":"AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans","abstract":"Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code will be made publicly available at https://github.com/artonson/autoinst.","sentences":["Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments.","Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations.","Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain.","To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations.","To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals.","To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances.","We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals.","To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations.","Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline.","The code will be made publicly available at https://github.com/artonson/autoinst."],"url":"http://arxiv.org/abs/2403.16318v1","category":"cs.CV"}
{"created":"2024-03-24 22:42:40","title":"Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective","abstract":"We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\\\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\\\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of our results are that: (i) it is possible to obtain complexity results for both convex and nonconvex problems with the (local or global) Lipschitz constant being replaced by a constant of local subgradient variation and (ii) mean width of the subdifferential set around the optima plays a role in the complexity of nonsmooth optimization, particularly in parallel settings. A consequence of (ii) is that for any error parameter $\\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex optimization is lower than its sequential oracle complexity by a factor $\\tilde{\\Omega}\\big(\\frac{1}{\\epsilon}\\big)$ whenever the objective function is piecewise linear with polynomially many pieces in the input size. This is particularly surprising as existing parallel complexity lower bounds are based on such classes of functions. The seeming contradiction is resolved by considering the region in which the algorithm is allowed to query the objective.","sentences":["We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense.","The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\\\"{o}lder/Lipschitz continuity of its gradient.","Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\\\"{o}lder continuous gradient.","When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.''","Some highlights of our results are that: (i) it is possible to obtain complexity results for both convex and nonconvex problems with the (local or global) Lipschitz constant being replaced by a constant of local subgradient variation and (ii) mean width of the subdifferential set around the optima plays a role in the complexity of nonsmooth optimization, particularly in parallel settings.","A consequence of (ii) is that for any error parameter $\\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex optimization is lower than its sequential oracle complexity by a factor $\\tilde{\\Omega}\\big(\\frac{1}{\\epsilon}\\big)$ whenever the objective function is piecewise linear with polynomially many pieces in the input size.","This is particularly surprising as existing parallel complexity lower bounds are based on such classes of functions.","The seeming contradiction is resolved by considering the region in which the algorithm is allowed to query the objective."],"url":"http://arxiv.org/abs/2403.16317v1","category":"math.OC"}
{"created":"2024-03-24 22:40:54","title":"On interpolation categories for the hyperoctahedral group","abstract":"Two different types of Deligne categories have been defined to interpolate the finite dimensional complex representations of the hyperoctahedral group. The first one, initially defined by Knop and then further studied by Likeng and Savage, uses a categorical analogue of the permutation representation as a tensor generator. The second one, due to Flake and Maassen, is tensor generated by a categorical analogue of the reflection representation. We construct a symmetric monoidal functor between the two and show that it is an equivalence of symmetric monoidal categories.","sentences":["Two different types of Deligne categories have been defined to interpolate the finite dimensional complex representations of the hyperoctahedral group.","The first one, initially defined by Knop and then further studied by Likeng and Savage, uses a categorical analogue of the permutation representation as a tensor generator.","The second one, due to Flake and Maassen, is tensor generated by a categorical analogue of the reflection representation.","We construct a symmetric monoidal functor between the two and show that it is an equivalence of symmetric monoidal categories."],"url":"http://arxiv.org/abs/2403.16316v1","category":"math.RT"}
{"created":"2024-03-24 22:26:33","title":"Standard and Non-Standard Aspects of Neutrino Physics","abstract":"This review provides a succinct overview of the basic aspects of neutrino physics. The topics covered include: neutrinos in the standard model and the three-neutrino mixing scheme; the current status of neutrino oscillation measurements and what remains to be determined; the seesaw mechanisms for neutrino mass generation and the associated phenomenology, including the leptogenesis mechanism to explain the observed matter-antimatter asymmetry of the Universe; models for the origin of the pattern of neutrino mixing and lepton masses based on discrete flavour symmetries and modular invariance.","sentences":["This review provides a succinct overview of the basic aspects of neutrino physics.","The topics covered include: neutrinos in the standard model and the three-neutrino mixing scheme; the current status of neutrino oscillation measurements and what remains to be determined; the seesaw mechanisms for neutrino mass generation and the associated phenomenology, including the leptogenesis mechanism to explain the observed matter-antimatter asymmetry of the Universe; models for the origin of the pattern of neutrino mixing and lepton masses based on discrete flavour symmetries and modular invariance."],"url":"http://arxiv.org/abs/2403.16308v1","category":"hep-ph"}
{"created":"2024-03-24 21:41:41","title":"SoK: An Essential Guide For Using Malware Sandboxes In Security Applications: Challenges, Pitfalls, and Lessons Learned","abstract":"Malware sandboxes provide many benefits for security applications, but they are complex. These complexities can overwhelm new users in different research areas and make it difficult to select, configure, and use sandboxes. Even worse, incorrectly using sandboxes can have a negative impact on security applications. In this paper, we address this knowledge gap by systematizing 84 representative papers for using x86/64 malware sandboxes in the academic literature. We propose a novel framework to simplify sandbox components and organize the literature to derive practical guidelines for using sandboxes. We evaluate the proposed guidelines systematically using three common security applications and demonstrate that the choice of different sandboxes can significantly impact the results. Specifically, our results show that the proposed guidelines improve the sandbox observable activities by at least 1.6x and up to 11.3x. Furthermore, we observe a roughly 25% improvement in accuracy, precision, and recall when using the guidelines to help with a malware family classification task. We conclude by affirming that there is no \"silver bullet\" sandbox deployment that generalizes, and we recommend that users apply our framework to define a scope for their analysis, a threat model, and derive context about how the sandbox artifacts will influence their intended use case. Finally, it is important that users document their experiment, limitations, and potential solutions for reproducibility","sentences":["Malware sandboxes provide many benefits for security applications, but they are complex.","These complexities can overwhelm new users in different research areas and make it difficult to select, configure, and use sandboxes.","Even worse, incorrectly using sandboxes can have a negative impact on security applications.","In this paper, we address this knowledge gap by systematizing 84 representative papers for using x86/64 malware sandboxes in the academic literature.","We propose a novel framework to simplify sandbox components and organize the literature to derive practical guidelines for using sandboxes.","We evaluate the proposed guidelines systematically using three common security applications and demonstrate that the choice of different sandboxes can significantly impact the results.","Specifically, our results show that the proposed guidelines improve the sandbox observable activities by at least 1.6x and up to 11.3x.","Furthermore, we observe a roughly 25% improvement in accuracy, precision, and recall when using the guidelines to help with a malware family classification task.","We conclude by affirming that there is no \"silver bullet\" sandbox deployment that generalizes, and we recommend that users apply our framework to define a scope for their analysis, a threat model, and derive context about how the sandbox artifacts will influence their intended use case.","Finally, it is important that users document their experiment, limitations, and potential solutions for reproducibility"],"url":"http://arxiv.org/abs/2403.16304v1","category":"cs.CR"}
{"created":"2024-03-24 21:29:39","title":"Large Language Models in Biomedical and Health Informatics: A Bibliometric Review","abstract":"Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as well as healthcare delivery and patient outcomes. This comprehensive review serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of LLMs in BHI.","sentences":["Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research.","This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023.","It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine.","To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field.","Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations.","Looking ahead, we consider how LLMs could further transform biomedical research as well as healthcare delivery and patient outcomes.","This comprehensive review serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of LLMs in BHI."],"url":"http://arxiv.org/abs/2403.16303v1","category":"cs.DL"}
{"created":"2024-03-24 21:05:36","title":"MRSch: Multi-Resource Scheduling for HPC","abstract":"Emerging workloads in high-performance computing (HPC) are embracing significant changes, such as having diverse resource requirements instead of being CPU-centric. This advancement forces cluster schedulers to consider multiple schedulable resources during decision-making. Existing scheduling studies rely on heuristic or optimization methods, which are limited by an inability to adapt to new scenarios for ensuring long-term scheduling performance. We present an intelligent scheduling agent named MRSch for multi-resource scheduling in HPC that leverages direct future prediction (DFP), an advanced multi-objective reinforcement learning algorithm. While DFP demonstrated outstanding performance in a gaming competition, it has not been previously explored in the context of HPC scheduling. Several key techniques are developed in this study to tackle the challenges involved in multi-resource scheduling. These techniques enable MRSch to learn an appropriate scheduling policy automatically and dynamically adapt its policy in response to workload changes via dynamic resource prioritizing. We compare MRSch with existing scheduling methods through extensive tracebase simulations. Our results demonstrate that MRSch improves scheduling performance by up to 48% compared to the existing scheduling methods.","sentences":["Emerging workloads in high-performance computing (HPC) are embracing significant changes, such as having diverse resource requirements instead of being CPU-centric.","This advancement forces cluster schedulers to consider multiple schedulable resources during decision-making.","Existing scheduling studies rely on heuristic or optimization methods, which are limited by an inability to adapt to new scenarios for ensuring long-term scheduling performance.","We present an intelligent scheduling agent named MRSch for multi-resource scheduling in HPC that leverages direct future prediction (DFP), an advanced multi-objective reinforcement learning algorithm.","While DFP demonstrated outstanding performance in a gaming competition, it has not been previously explored in the context of HPC scheduling.","Several key techniques are developed in this study to tackle the challenges involved in multi-resource scheduling.","These techniques enable MRSch to learn an appropriate scheduling policy automatically and dynamically adapt its policy in response to workload changes via dynamic resource prioritizing.","We compare MRSch with existing scheduling methods through extensive tracebase simulations.","Our results demonstrate that MRSch improves scheduling performance by up to 48% compared to the existing scheduling methods."],"url":"http://arxiv.org/abs/2403.16298v1","category":"cs.DC"}
{"created":"2024-03-24 21:05:28","title":"Round Robin Active Sequential Change Detection for Dependent Multi-Channel Data","abstract":"This paper considers the problem of sequentially detecting a change in the joint distribution of multiple data sources under a sampling constraint. Specifically, the channels or sources generate observations that are independent over time, but not necessarily independent at any given time instant. The sources follow an initial joint distribution, and at an unknown time instant, the joint distribution of an unknown subset of sources changes. Importantly, there is a hard constraint that only a fixed number of sources are allowed to be sampled at each time instant. The goal is to sequentially observe the sources according to the constraint, and stop sampling as quickly as possible after the change while controlling the false alarm rate below a user-specified level. The sources can be selected dynamically based on the already collected data, and thus, a policy for this problem consists of a joint sampling and change-detection rule. A non-randomized policy is studied, and an upper bound is established on its worst-case conditional expected detection delay with respect to both the change point and the observations from the affected sources before the change.","sentences":["This paper considers the problem of sequentially detecting a change in the joint distribution of multiple data sources under a sampling constraint.","Specifically, the channels or sources generate observations that are independent over time, but not necessarily independent at any given time instant.","The sources follow an initial joint distribution, and at an unknown time instant, the joint distribution of an unknown subset of sources changes.","Importantly, there is a hard constraint that only a fixed number of sources are allowed to be sampled at each time instant.","The goal is to sequentially observe the sources according to the constraint, and stop sampling as quickly as possible after the change while controlling the false alarm rate below a user-specified level.","The sources can be selected dynamically based on the already collected data, and thus, a policy for this problem consists of a joint sampling and change-detection rule.","A non-randomized policy is studied, and an upper bound is established on its worst-case conditional expected detection delay with respect to both the change point and the observations from the affected sources before the change."],"url":"http://arxiv.org/abs/2403.16297v1","category":"stat.ME"}
{"created":"2024-03-24 21:02:35","title":"LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation","abstract":"With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well. As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language. Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document's context will support such harmonized legal definitions across different regulations and thus avoid ambiguities. In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using retrieval augmented generation (RAG) and existing term definitions present in different legislative documents. For this, definition elements are built by extracting definitions from existing documents. Using definition elements and RAG, a Definitions article can be suggested on demand for a legislative document that is being drafted. We demonstrate and evaluate the functionality of LexDrafter using a collection of EU documents from the energy domain. The code for LexDrafter framework is available at https://github.com/achouhan93/LexDrafter.","sentences":["With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well.","As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language.","Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document's context will support such harmonized legal definitions across different regulations and thus avoid ambiguities.","In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using retrieval augmented generation (RAG) and existing term definitions present in different legislative documents.","For this, definition elements are built by extracting definitions from existing documents.","Using definition elements and RAG, a Definitions article can be suggested on demand for a legislative document that is being drafted.","We demonstrate and evaluate the functionality of LexDrafter using a collection of EU documents from the energy domain.","The code for LexDrafter framework is available at https://github.com/achouhan93/LexDrafter."],"url":"http://arxiv.org/abs/2403.16295v1","category":"cs.CL"}
{"created":"2024-03-24 20:48:36","title":"latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction","abstract":"We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data.","sentences":["We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture.","Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible.","In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data.","The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians.","From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network.","We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data."],"url":"http://arxiv.org/abs/2403.16292v1","category":"cs.CV"}
{"created":"2024-03-24 20:43:29","title":"Guessing human intentions to avoid dangerous situations in caregiving robots","abstract":"For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been conducted to test the implementation's robustness, precision and real-time response, including a simulated scenario, a human-in-the-loop hybrid configuration and a real-world scenario.","sentences":["For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately.","This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided.","This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions.","We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time.","We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people.","Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations.","The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios.","Three experiments have been conducted to test the implementation's robustness, precision and real-time response, including a simulated scenario, a human-in-the-loop hybrid configuration and a real-world scenario."],"url":"http://arxiv.org/abs/2403.16291v1","category":"cs.RO"}
{"created":"2024-03-24 20:41:19","title":"An Information Theoretic Treatment of Animal Movement Tracks","abstract":"The two-dimensional track of an animal on a landscape has progressed over the past three decades from hourly to second-by-second recordings of locations. Track segmentation methods for analyzing the behavioral information in such relocation data has lagged somewhat behind, with scales of analysis currently at the sub-hourly to minute level. A new approach is needed to bring segmentation analysis down to a second-by-second level. Here, such an approach is presented that rests heavily on concepts from Shannon's Information Theory. In this paper, we first briefly review and update concepts relating to movement path segmentation. We then discuss how cluster analysis can be used to organize the smallest viable statistical movement elements (StaMEs), which are $\\mu$ steps long, and to code the next level of movement elements called ``words'' that are $m \\mu$ steps long. Centroids of these word clusters are identified as canonical activity modes (CAMs). Unlike current segmentation schemes, the approach presented here allows us to provide entropy measures for movement paths, compute the coding efficiencies of derived StaMEs and CAMs, and assess error rates in the allocation of strings of $m$ StaMEs to CAM types. In addition our approach allows us to employ the Jensen-Shannon divergence measure to assess and compare the best choices for the various parameters (number of steps in a StaME, number of StaME types, number of StaMEs in a word, number of CAM types), as well as the best clustering methods for generating segments that can then be used to interpret and predict sequences of higher order segments. The theory presented here provides another tool in our toolbox for dealing with the effects of global change on the movement and redistribution of animals across altered landscapes","sentences":["The two-dimensional track of an animal on a landscape has progressed over the past three decades from hourly to second-by-second recordings of locations.","Track segmentation methods for analyzing the behavioral information in such relocation data has lagged somewhat behind, with scales of analysis currently at the sub-hourly to minute level.","A new approach is needed to bring segmentation analysis down to a second-by-second level.","Here, such an approach is presented that rests heavily on concepts from Shannon's Information Theory.","In this paper, we first briefly review and update concepts relating to movement path segmentation.","We then discuss how cluster analysis can be used to organize the smallest viable statistical movement elements (StaMEs), which are $\\mu$ steps long, and to code the next level of movement elements called ``words'' that are $m \\mu$ steps long.","Centroids of these word clusters are identified as canonical activity modes (CAMs).","Unlike current segmentation schemes, the approach presented here allows us to provide entropy measures for movement paths, compute the coding efficiencies of derived StaMEs and CAMs, and assess error rates in the allocation of strings of $m$ StaMEs to CAM types.","In addition our approach allows us to employ the Jensen-Shannon divergence measure to assess and compare the best choices for the various parameters (number of steps in a StaME, number of StaME types, number of StaMEs in a word, number of CAM types), as well as the best clustering methods for generating segments that can then be used to interpret and predict sequences of higher order segments.","The theory presented here provides another tool in our toolbox for dealing with the effects of global change on the movement and redistribution of animals across altered landscapes"],"url":"http://arxiv.org/abs/2403.16290v1","category":"q-bio.PE"}
{"created":"2024-03-24 20:40:51","title":"Engineering Safety Requirements for Autonomous Driving with Large Language Models","abstract":"Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","sentences":["Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps.","Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update.","In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements.","This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements.","We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria.","We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively.","Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency."],"url":"http://arxiv.org/abs/2403.16289v1","category":"cs.AI"}
{"created":"2024-03-24 20:37:33","title":"Study of Workload Interference with Intelligent Routing on Dragonfly","abstract":"Dragonfly interconnect is a crucial network technology for supercomputers. To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair. While link utilization is increased, workload performance is often offset by network contention. Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency. However, its effectiveness in reducing workload interference is unknown. In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system. We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns. We also present two metrics to characterize application communication intensity. Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics. Several key insights are made from the analysis.","sentences":["Dragonfly interconnect is a crucial network technology for supercomputers.","To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair.","While link utilization is increased, workload performance is often offset by network contention.","Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency.","However, its effectiveness in reducing workload interference is unknown.","In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system.","We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns.","We also present two metrics to characterize application communication intensity.","Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics.","Several key insights are made from the analysis."],"url":"http://arxiv.org/abs/2403.16288v1","category":"cs.NI"}
{"created":"2024-03-24 20:08:16","title":"The Evolution of Football Betting- A Machine Learning Approach to Match Outcome Forecasting and Bookmaker Odds Estimation","abstract":"This paper explores the significant history of professional football and the betting industry, tracing its evolution from clandestine beginnings to a lucrative multi-million-pound enterprise. Initiated by the legalization of gambling in 1960 and complemented by advancements in football data gathering pioneered by Thorold Charles Reep, the symbiotic relationship between these sectors has propelled rapid growth and innovation. Over the past six decades, both industries have undergone radical transformations, with data collection methods evolving from rudimentary notetaking to sophisticated technologies such as high-definition cameras and Artificial Intelligence (AI)-driven analytics. Therefore, the primary aim of this study is to utilize Machine Learning (ML) algorithms to forecast premier league football match outcomes. By analyzing historical data and investigating the significance of various features, the study seeks to identify the most effective predictive models and discern key factors influencing match results. Additionally, the study aims to utilize these forecasting to inform the establishment of bookmaker odds, providing insights into the impact of different variables on match outcomes. By highlighting the potential for informed decision-making in sports forecasting and betting, this study opens up new avenues for research and practical applications in the domain of sports analytics.","sentences":["This paper explores the significant history of professional football and the betting industry, tracing its evolution from clandestine beginnings to a lucrative multi-million-pound enterprise.","Initiated by the legalization of gambling in 1960 and complemented by advancements in football data gathering pioneered by Thorold Charles Reep, the symbiotic relationship between these sectors has propelled rapid growth and innovation.","Over the past six decades, both industries have undergone radical transformations, with data collection methods evolving from rudimentary notetaking to sophisticated technologies such as high-definition cameras and Artificial Intelligence (AI)-driven analytics.","Therefore, the primary aim of this study is to utilize Machine Learning (ML) algorithms to forecast premier league football match outcomes.","By analyzing historical data and investigating the significance of various features, the study seeks to identify the most effective predictive models and discern key factors influencing match results.","Additionally, the study aims to utilize these forecasting to inform the establishment of bookmaker odds, providing insights into the impact of different variables on match outcomes.","By highlighting the potential for informed decision-making in sports forecasting and betting, this study opens up new avenues for research and practical applications in the domain of sports analytics."],"url":"http://arxiv.org/abs/2403.16282v1","category":"cs.LG"}
{"created":"2024-03-24 19:52:53","title":"Combined Task and Motion Planning Via Sketch Decompositions (Extended Version with Supplementary Material)","abstract":"The challenge in combined task and motion planning (TAMP) is the effective integration of a search over a combinatorial space, usually carried out by a task planner, and a search over a continuous configuration space, carried out by a motion planner. Using motion planners for testing the feasibility of task plans and filling out the details is not effective because it makes the geometrical constraints play a passive role. This work introduces a new interleaved approach for integrating the two dimensions of TAMP that makes use of sketches, a recent simple but powerful language for expressing the decomposition of problems into subproblems. A sketch has width 1 if it decomposes the problem into subproblems that can be solved greedily in linear time. In the paper, a general sketch is introduced for several classes of TAMP problems which has width 1 under suitable assumptions. While sketch decompositions have been developed for classical planning, they offer two important benefits in the context of TAMP. First, when a task plan is found to be unfeasible due to the geometric constraints, the combinatorial search resumes in a specific sub-problem. Second, the sampling of object configurations is not done once, globally, at the start of the search, but locally, at the start of each subproblem. Optimizations of this basic setting are also considered and experimental results over existing and new pick-and-place benchmarks are reported.","sentences":["The challenge in combined task and motion planning (TAMP) is the effective integration of a search over a combinatorial space, usually carried out by a task planner, and a search over a continuous configuration space, carried out by a motion planner.","Using motion planners for testing the feasibility of task plans and filling out the details is not effective because it makes the geometrical constraints play a passive role.","This work introduces a new interleaved approach for integrating the two dimensions of TAMP that makes use of sketches, a recent simple but powerful language for expressing the decomposition of problems into subproblems.","A sketch has width 1 if it decomposes the problem into subproblems that can be solved greedily in linear time.","In the paper, a general sketch is introduced for several classes of TAMP problems which has width 1 under suitable assumptions.","While sketch decompositions have been developed for classical planning, they offer two important benefits in the context of TAMP.","First, when a task plan is found to be unfeasible due to the geometric constraints, the combinatorial search resumes in a specific sub-problem.","Second, the sampling of object configurations is not done once, globally, at the start of the search, but locally, at the start of each subproblem.","Optimizations of this basic setting are also considered and experimental results over existing and new pick-and-place benchmarks are reported."],"url":"http://arxiv.org/abs/2403.16277v1","category":"cs.RO"}
{"created":"2024-03-24 19:50:49","title":"AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue","abstract":"In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information. Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments. Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos. We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task.","sentences":["In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD).","While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited.","Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively.","To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations.","We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information.","Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments.","Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos.","We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task."],"url":"http://arxiv.org/abs/2403.16276v1","category":"cs.CV"}
{"created":"2024-03-24 19:47:37","title":"M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and Scheduling","abstract":"In this paper, we present a novel problem coined multi-robot, multi-objective, and multi-mode routing and scheduling (M^3RS). The formulation for M^3RS is introduced for time-bound multi-robot, multi-objective routing and scheduling missions where each task has multiple execution modes. Different execution modes have distinct resource consumption, associated execution time, and quality. M^3RS assigns the optimal sequence of tasks and the execution modes to each agent. The routes and associated modes depend on user preferences for different objective criteria. The need for M^3RS comes from multi-robot applications in which a trade-off between multiple criteria arises from different task execution modes. We use M^3RS for the application of multi-robot disinfection in public locations. The objectives considered for disinfection application are disinfection quality and number of tasks completed. A mixed-integer linear programming model is proposed for M^3RS. Then, a time-efficient column generation scheme is presented to tackle the issue of computation times for larger problem instances. The advantage of using multiple modes over fixed execution mode is demonstrated using experiments on synthetic data. The results suggest that M^3RS provides flexibility to the user in terms of available solutions and performs well in joint performance metrics. The application of the proposed problem is shown for a team of disinfection robots.} The videos for the experiments are available on the project website: https://sites.google.com/view/g-robot/m3rs/ .","sentences":["In this paper, we present a novel problem coined multi-robot, multi-objective, and multi-mode routing and scheduling (M^3RS).","The formulation for M^3RS is introduced for time-bound multi-robot, multi-objective routing and scheduling missions where each task has multiple execution modes.","Different execution modes have distinct resource consumption, associated execution time, and quality.","M^3RS assigns the optimal sequence of tasks and the execution modes to each agent.","The routes and associated modes depend on user preferences for different objective criteria.","The need for M^3RS comes from multi-robot applications in which a trade-off between multiple criteria arises from different task execution modes.","We use M^3RS for the application of multi-robot disinfection in public locations.","The objectives considered for disinfection application are disinfection quality and number of tasks completed.","A mixed-integer linear programming model is proposed for M^3RS.","Then, a time-efficient column generation scheme is presented to tackle the issue of computation times for larger problem instances.","The advantage of using multiple modes over fixed execution mode is demonstrated using experiments on synthetic data.","The results suggest that M^3RS provides flexibility to the user in terms of available solutions and performs well in joint performance metrics.","The application of the proposed problem is shown for a team of disinfection robots.}","The videos for the experiments are available on the project website: https://sites.google.com/view/g-robot/m3rs/ ."],"url":"http://arxiv.org/abs/2403.16275v1","category":"cs.RO"}
{"created":"2024-03-24 19:43:13","title":"Long-lived, pulse-induced absorption in $\\mathrm{LiNb}_{1-x}\\mathrm{Ta}_x\\mathrm{O}_3$ solid solutions: the case of three intrinsic defect sites for electron localization with strong coupling","abstract":"Femto-/nanosecond pulse-induced, red and near-infrared absorption is studied in $\\mathrm{LiNb}_{1-x}\\mathrm{Ta}_{x}\\mathrm{O}_3$ (LNT) solid solutions with the goal to probe the intrinsic defect structure via the formation, transport and recombination of optically generated small bound electron polarons with strong coupling to the lattice. As a result, long-lived transients are uncovered for LNT which exceed lifetimes of LN and LT by a factor of up to 100 over the entire range of investigated compositions. At the same time, the starting amplitude varies in the range of $\\alpha_\\mathrm{li}^0\\approx10-100\\,\\mathrm{m}^{-1}$ as a function of $x$ and exceed the ones of LN and LT by a factor of up to ten. The results are interpreted in the model of three-dimensional small polaron hopping transport considering the simultaneous presence of three different types of small bound polarons, in particular of small electron $\\mathrm{Nb}_\\mathrm{Li}^{4+}$ and $\\mathrm{Ta}_\\mathrm{Li}^{4+}$ antisite polarons, and of small electron $\\mathrm{Ta}_\\mathrm{V}^{4+}$ interstitial polarons. We conclude that the differences between LNT, LN, and LT may point to model systems that consist of one (LN), two (LT) and three (LNT) intrinsic defect centers for electron localization.","sentences":["Femto-/nanosecond pulse-induced, red and near-infrared absorption is studied in $\\mathrm{LiNb}_{1-x}\\mathrm{Ta}_{x}\\mathrm{O}_3$ (LNT) solid solutions with the goal to probe the intrinsic defect structure via the formation, transport and recombination of optically generated small bound electron polarons with strong coupling to the lattice.","As a result, long-lived transients are uncovered for LNT which exceed lifetimes of LN and LT by a factor of up to 100 over the entire range of investigated compositions.","At the same time, the starting amplitude varies in the range of $\\alpha_\\mathrm{li}^0\\approx10-100\\,\\mathrm{m}^{-1}$ as a function of $x$ and exceed the ones of LN and LT by a factor of up to ten.","The results are interpreted in the model of three-dimensional small polaron hopping transport considering the simultaneous presence of three different types of small bound polarons, in particular of small electron $\\mathrm{Nb}_\\mathrm{Li}^{4+}$ and $\\mathrm{Ta}_\\mathrm{Li}^{4+}$ antisite polarons, and of small electron $\\mathrm{Ta}_\\mathrm{V}^{4+}$ interstitial polarons.","We conclude that the differences between LNT, LN, and LT may point to model systems that consist of one (LN), two (LT) and three (LNT) intrinsic defect centers for electron localization."],"url":"http://arxiv.org/abs/2403.16274v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-24 19:34:33","title":"L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction","abstract":"Pre-training strategies based on self-supervised learning (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision. Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging. Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support. In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known Transformer-based MAE. In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking. Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends. The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progression. Using OPHDIAT, a large follow-up screening dataset targeting diabetic retinopathy (DR), we evaluated the pre-trained weights on a longitudinal task, which is to predict the severity label of the next visit within 3 years based on the past time series examinations. Our results demonstrated the relevancy of both time-aware position embedding and masking strategies based on disease progression knowledge. Compared to popular baseline models and standard longitudinal Transformers, these simple yet effective extensions significantly enhance the predictive ability of deep classification models.","sentences":["Pre-training strategies based on self-supervised learning (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision.","Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging.","Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support.","In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known Transformer-based MAE.","In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking.","Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends.","The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progression.","Using OPHDIAT, a large follow-up screening dataset targeting diabetic retinopathy (DR), we evaluated the pre-trained weights on a longitudinal task, which is to predict the severity label of the next visit within 3 years based on the past time series examinations.","Our results demonstrated the relevancy of both time-aware position embedding and masking strategies based on disease progression knowledge.","Compared to popular baseline models and standard longitudinal Transformers, these simple yet effective extensions significantly enhance the predictive ability of deep classification models."],"url":"http://arxiv.org/abs/2403.16272v1","category":"cs.CV"}
{"created":"2024-03-24 18:57:58","title":"An extended scheme of classical special functions","abstract":"A unifying scheme of classical special functions of hypergeometric type obeying orthogonality or biorthogonality relations is described. It expands the Askey scheme of classical orthogonal polynomials and its $q$-analogue based on the Askey--Wilson polynomials. On the top, it has two-index biorthogonal functions formed from elliptic hypergeometric series with the absolutely continuous measure determined by the elliptic beta integral. A new result is an inclusion of complex hypergeometric functions into the scheme. Its further potential generalizations are discussed as well.","sentences":["A unifying scheme of classical special functions of hypergeometric type obeying orthogonality or biorthogonality relations is described.","It expands the Askey scheme of classical orthogonal polynomials and its $q$-analogue based on the Askey--Wilson polynomials.","On the top, it has two-index biorthogonal functions formed from elliptic hypergeometric series with the absolutely continuous measure determined by the elliptic beta integral.","A new result is an inclusion of complex hypergeometric functions into the scheme.","Its further potential generalizations are discussed as well."],"url":"http://arxiv.org/abs/2403.16264v1","category":"math.CA"}
{"created":"2024-03-24 18:49:16","title":"HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under Unknown Vertical Ground Motion","abstract":"This paper presents a hierarchical control framework that enables robust quadrupedal locomotion on a dynamic rigid surface (DRS) with general and unknown vertical motions. The key novelty of the framework lies in its higher layer, which is a discrete-time, provably stabilizing footstep controller. The basis of the footstep controller is a new hybrid, time-varying, linear inverted pendulum (HT-LIP) model that is low-dimensional and accurately captures the essential robot dynamics during DRS locomotion. A new set of sufficient stability conditions are then derived to directly guide the controller design for ensuring the asymptotic stability of the HT-LIP model under general, unknown, vertical DRS motions. Further, the footstep controller is cast as a computationally efficient quadratic program that incorporates the proposed HT-LIP model and stability conditions. The middle layer takes the desired footstep locations generated by the higher layer as input to produce kinematically feasible full-body reference trajectories, which are then accurately tracked by a lower-layer torque controller. Hardware experiments on a Unitree Go1 quadrupedal robot confirm the robustness of the proposed framework under various unknown, aperiodic, vertical DRS motions and uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and sudden pushes).","sentences":["This paper presents a hierarchical control framework that enables robust quadrupedal locomotion on a dynamic rigid surface (DRS) with general and unknown vertical motions.","The key novelty of the framework lies in its higher layer, which is a discrete-time, provably stabilizing footstep controller.","The basis of the footstep controller is a new hybrid, time-varying, linear inverted pendulum (HT-LIP) model that is low-dimensional and accurately captures the essential robot dynamics during DRS locomotion.","A new set of sufficient stability conditions are then derived to directly guide the controller design for ensuring the asymptotic stability of the HT-LIP model under general, unknown, vertical DRS motions.","Further, the footstep controller is cast as a computationally efficient quadratic program that incorporates the proposed HT-LIP model and stability conditions.","The middle layer takes the desired footstep locations generated by the higher layer as input to produce kinematically feasible full-body reference trajectories, which are then accurately tracked by a lower-layer torque controller.","Hardware experiments on a Unitree Go1 quadrupedal robot confirm the robustness of the proposed framework under various unknown, aperiodic, vertical DRS motions and uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and sudden pushes)."],"url":"http://arxiv.org/abs/2403.16262v1","category":"cs.RO"}
{"created":"2024-03-24 18:43:04","title":"Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble","abstract":"Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into distinct supervision tasks. This innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages diverse training tasks to generate distinct comprehensions of the data and labels, thereby extending the feature representation field.   Our experimental results demonstrate the superior performance of the MC Ensemble strategy in OOD detection compared to both the naive Deep Ensemble method and a standalone model of comparable size. This underscores the effectiveness of our proposed approach in enhancing the model's capability to detect instances outside its training distribution.","sentences":["Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection.","Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   ","However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods.","We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   ","To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into distinct supervision tasks.","This innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages diverse training tasks to generate distinct comprehensions of the data and labels, thereby extending the feature representation field.   ","Our experimental results demonstrate the superior performance of the MC Ensemble strategy in OOD detection compared to both the naive Deep Ensemble method and a standalone model of comparable size.","This underscores the effectiveness of our proposed approach in enhancing the model's capability to detect instances outside its training distribution."],"url":"http://arxiv.org/abs/2403.16260v1","category":"cs.LG"}
{"created":"2024-03-24 18:40:37","title":"On the effective generation of direct images of pluricanonical bundles in mixed characteristic","abstract":"We present an effective global generation result for direct images of pluricanonical bundles in mixed characteristic. This is a mixed characteristic analog of Ejiri's theorem in positive characteristic and the theorem of Popa and Schnell regarding their Fujita-type conjecture in characteristic zero. As an application, we establish a weak positivity statement for the relative canonical sheaf of a smooth morphism in mixed characteristic. Using this weak positivity result, we prove that images of Fano schemes under smooth morphisms are again Fano in mixed characteristic.","sentences":["We present an effective global generation result for direct images of pluricanonical bundles in mixed characteristic.","This is a mixed characteristic analog of Ejiri's theorem in positive characteristic and the theorem of Popa and Schnell regarding their Fujita-type conjecture in characteristic zero.","As an application, we establish a weak positivity statement for the relative canonical sheaf of a smooth morphism in mixed characteristic.","Using this weak positivity result, we prove that images of Fano schemes under smooth morphisms are again Fano in mixed characteristic."],"url":"http://arxiv.org/abs/2403.16259v1","category":"math.AG"}
{"created":"2024-03-24 18:33:16","title":"Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis","abstract":"While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.","sentences":["While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels.","To address this limitation, we adopt a non-isotropic diffusion model at the decoder side.","This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images.","Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step.","This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk.","The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks.","The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster.","Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings."],"url":"http://arxiv.org/abs/2403.16258v1","category":"eess.IV"}
{"created":"2024-03-24 18:15:36","title":"Instantaneous control strategies for magnetically confined fusion plasma","abstract":"The principle behind magnetic fusion is to confine high temperature plasma inside a device in such a way that the nuclei of deuterium and tritium joining together can release energy. The high temperatures generated needs the plasma to be isolated from the wall of the device to avoid damages and the scope of external magnetic fields is to achieve this goal. In this paper, to face this challenge from a numerical perspective, we propose an instantaneous control mathematical approach to steer a plasma into a given spatial region. From the modeling point of view, we focus on the Vlasov equation in a bounded domain with self induced electric field and an external strong magnetic field. The main feature of the control strategy employed is that it provides a feedback on the equation of motion based on an instantaneous prediction of the discretized system. This permits to directly embed the minimization of a given cost functional into the particle interactions of the corresponding Vlasov model. The numerical results demonstrate the validity of our control approach and the capability of an external magnetic field, even if in a simplified setting, to lead the plasma far from the boundaries.","sentences":["The principle behind magnetic fusion is to confine high temperature plasma inside a device in such a way that the nuclei of deuterium and tritium joining together can release energy.","The high temperatures generated needs the plasma to be isolated from the wall of the device to avoid damages and the scope of external magnetic fields is to achieve this goal.","In this paper, to face this challenge from a numerical perspective, we propose an instantaneous control mathematical approach to steer a plasma into a given spatial region.","From the modeling point of view, we focus on the Vlasov equation in a bounded domain with self induced electric field and an external strong magnetic field.","The main feature of the control strategy employed is that it provides a feedback on the equation of motion based on an instantaneous prediction of the discretized system.","This permits to directly embed the minimization of a given cost functional into the particle interactions of the corresponding Vlasov model.","The numerical results demonstrate the validity of our control approach and the capability of an external magnetic field, even if in a simplified setting, to lead the plasma far from the boundaries."],"url":"http://arxiv.org/abs/2403.16254v1","category":"math.NA"}
{"created":"2024-03-25 11:53:49","title":"Algorithms and data structures for numerical computations with automatic precision estimation","abstract":"We introduce data structures and algorithms to count numerical inaccuracies arising from usage of floating numbers described in IEEE 754. Here we describe how to estimate precision for some collection of functions most commonly used for array manipulations and training of neural networks. For highly optimized functions like matrix multiplication, we provide a fast estimation of precision and some hint how the estimation can be strengthened.","sentences":["We introduce data structures and algorithms to count numerical inaccuracies arising from usage of floating numbers described in IEEE 754.","Here we describe how to estimate precision for some collection of functions most commonly used for array manipulations and training of neural networks.","For highly optimized functions like matrix multiplication, we provide a fast estimation of precision and some hint how the estimation can be strengthened."],"url":"http://arxiv.org/abs/2403.16660v1","category":"math.NA"}
{"created":"2024-03-25 11:49:24","title":"Six new eccentric eclipsing systems with a third body","abstract":"We present the discovery of six new triple stellar system candidates composed of an inner eccentric-orbit eclipsing binary with an apsidal motion. These stars were studied using new, precise TESS light curves and a long-term collection of older photometric ground-based data. These data were used for the monitoring of ETVs (eclipse timing variations) and to detect the slow apsidal movements along with additional periodic signals. The systems analysed were ASASSN-V J012214.37+643943.3 (orbital period 2.01156 d, eccentricity 0.15, third body with 3.3 yr period); ASASSN-V J052227.78+345257.6 (2.42673 d, 0.35, 3.2 yr); ASASSN-V J203158.98+410731.4 (2.53109 d, 0.20, 2.7 yr); ASASSN-V J230945.10+605349.3 (2.08957 d, 0.18, 2.3 yr); ASASSN-V J231028.27+590841.8 (2.41767 d, 0.43, 4.9 yr); and NSV 14698 (3.30047 d, 0.147, 0.5 yr). In the system ASASSN-V J230945.10+605349.3, we detected a second eclipsing pair (per 2.99252 d) and found adequate ETV for the pair B, proving its 2+2 bound quadruple nature. All of these detected systems deserve special attention from long-term studies for their three-body dynamics since their outer orbital periods are not too long and because some dynamical effects should be detectable during the next decades. The system NSV 14698 especially seems to be the most interesting from the dynamical point of view due to it having the shortest outer period of the systems we studied, its fast apsidal motion, and its possible orbital changes during the whole 20th century.","sentences":["We present the discovery of six new triple stellar system candidates composed of an inner eccentric-orbit eclipsing binary with an apsidal motion.","These stars were studied using new, precise TESS light curves and a long-term collection of older photometric ground-based data.","These data were used for the monitoring of ETVs (eclipse timing variations) and to detect the slow apsidal movements along with additional periodic signals.","The systems analysed were ASASSN-V J012214.37+643943.3 (orbital period 2.01156 d, eccentricity 0.15, third body with 3.3 yr period); ASASSN-V J052227.78+345257.6 (2.42673 d, 0.35, 3.2 yr); ASASSN-V J203158.98+410731.4 (2.53109 d, 0.20, 2.7 yr); ASASSN-V J230945.10+605349.3 (2.08957 d, 0.18, 2.3 yr); ASASSN-V J231028.27+590841.8 (2.41767 d, 0.43, 4.9 yr); and NSV 14698 (3.30047 d, 0.147, 0.5 yr).","In the system ASASSN-V J230945.10+605349.3, we detected a second eclipsing pair (per 2.99252 d) and found adequate ETV for the pair B, proving its 2+2 bound quadruple nature.","All of these detected systems deserve special attention from long-term studies for their three-body dynamics since their outer orbital periods are not too long and because some dynamical effects should be detectable during the next decades.","The system NSV 14698 especially seems to be the most interesting from the dynamical point of view due to it having the shortest outer period of the systems we studied, its fast apsidal motion, and its possible orbital changes during the whole 20th century."],"url":"http://arxiv.org/abs/2403.16657v1","category":"astro-ph.SR"}
{"created":"2024-03-25 11:37:39","title":"Probing Stellar Clusters from Gaia DR2 as Galactic PeVatrons: I -- Expected Gamma-ray and Neutrino Emission","abstract":"Young & massive stellar clusters (SCs) are a potential source of galactic cosmic rays up to very high energies as a result of two possible acceleration scenarios. Collective stellar winds from massive member stars form a wind-blown bubble with a termination shock (TS) at which particle acceleration to PeV energies may occur. Furthermore, shock acceleration may occur at SNRs expanding inside the bubble. By applying a model of CR acceleration at both the wind TS and SNR shocks to catalogues of known SCs derived from Gaia DR2, we identify the most promising targets to search for evidence of PeVatron activity. Predictions for the secondary fluxes of gamma-ray and neutrino emission are derived based on particle acceleration at the collective wind TS and the subsequent hadronic interactions with the surrounding medium. Predictions from our modelling under baseline and optimistic scenarios are compared to data, finding consistent results. We estimate the detection prospects for future gamma-ray and neutrino experiments. We find that degree-scale angular sizes of the wind-blown bubbles are typical, that may pose a challenge for experimental detection. A shortlist of the most promising candidates is provided, with an anticipated flux range. Of order 10 SCs may be detectable with future facilities, and 1-5 could be currently operating as PeVatrons. Of these, three gamma-ray detected SCs have data within our predicted range. Our model can consistently describe gamma-ray measurements of SC emission. Several further as-yet-undetected SCs offer promising targets for future observations, although the flux range allowed by our model can be large (> factor 10). The large angular size of the wind-blown bubble may lead to low surface brightness emission, worsening the problem of source confusion. Nevertheless, we discuss how further work will help to constrain SCs as PeVatron candidates. (abridged)","sentences":["Young & massive stellar clusters (SCs) are a potential source of galactic cosmic rays up to very high energies as a result of two possible acceleration scenarios.","Collective stellar winds from massive member stars form a wind-blown bubble with a termination shock (TS) at which particle acceleration to PeV energies may occur.","Furthermore, shock acceleration may occur at SNRs expanding inside the bubble.","By applying a model of CR acceleration at both the wind TS and SNR shocks to catalogues of known SCs derived from Gaia DR2, we identify the most promising targets to search for evidence of PeVatron activity.","Predictions for the secondary fluxes of gamma-ray and neutrino emission are derived based on particle acceleration at the collective wind TS and the subsequent hadronic interactions with the surrounding medium.","Predictions from our modelling under baseline and optimistic scenarios are compared to data, finding consistent results.","We estimate the detection prospects for future gamma-ray and neutrino experiments.","We find that degree-scale angular sizes of the wind-blown bubbles are typical, that may pose a challenge for experimental detection.","A shortlist of the most promising candidates is provided, with an anticipated flux range.","Of order 10 SCs may be detectable with future facilities, and 1-5 could be currently operating as PeVatrons.","Of these, three gamma-ray detected SCs have data within our predicted range.","Our model can consistently describe gamma-ray measurements of SC emission.","Several further as-yet-undetected SCs offer promising targets for future observations, although the flux range allowed by our model can be large (> factor 10).","The large angular size of the wind-blown bubble may lead to low surface brightness emission, worsening the problem of source confusion.","Nevertheless, we discuss how further work will help to constrain SCs as PeVatron candidates.","(abridged)"],"url":"http://arxiv.org/abs/2403.16650v1","category":"astro-ph.HE"}
{"created":"2024-03-25 11:29:19","title":"Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution","abstract":"Artifact-free super-resolution (SR) aims to translate low-resolution images into their high-resolution counterparts with a strict integrity of the original content, eliminating any distortions or synthetic details. While traditional diffusion-based SR techniques have demonstrated remarkable abilities to enhance image detail, they are prone to artifact introduction during iterative procedures. Such artifacts, ranging from trivial noise to unauthentic textures, deviate from the true structure of the source image, thus challenging the integrity of the super-resolution process. In this work, we propose Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that delves into the latent space to effectively identify and mitigate the propagation of artifacts. Our SARGD begins by using an artifact detector to identify implausible pixels, creating a binary mask that highlights artifacts. Following this, the Reality Guidance Refinement (RGR) process refines artifacts by integrating this mask with realistic latent representations, improving alignment with the original image. Nonetheless, initial realistic-latent representations from lower-quality images result in over-smoothing in the final output. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism. It dynamically computes a reality score, enhancing the sharpness of the realistic latent. These alternating mechanisms collectively achieve artifact-free super-resolution. Extensive experiments demonstrate the superiority of our method, delivering detailed artifact-free high-resolution images while reducing sampling steps by 2X. We release our code at https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.","sentences":["Artifact-free super-resolution (SR) aims to translate low-resolution images into their high-resolution counterparts with a strict integrity of the original content, eliminating any distortions or synthetic details.","While traditional diffusion-based SR techniques have demonstrated remarkable abilities to enhance image detail, they are prone to artifact introduction during iterative procedures.","Such artifacts, ranging from trivial noise to unauthentic textures, deviate from the true structure of the source image, thus challenging the integrity of the super-resolution process.","In this work, we propose Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that delves into the latent space to effectively identify and mitigate the propagation of artifacts.","Our SARGD begins by using an artifact detector to identify implausible pixels, creating a binary mask that highlights artifacts.","Following this, the Reality Guidance Refinement (RGR) process refines artifacts by integrating this mask with realistic latent representations, improving alignment with the original image.","Nonetheless, initial realistic-latent representations from lower-quality images result in over-smoothing in the final output.","To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.","It dynamically computes a reality score, enhancing the sharpness of the realistic latent.","These alternating mechanisms collectively achieve artifact-free super-resolution.","Extensive experiments demonstrate the superiority of our method, delivering detailed artifact-free high-resolution images while reducing sampling steps by 2X. We release our code at https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git."],"url":"http://arxiv.org/abs/2403.16643v1","category":"eess.IV"}
{"created":"2024-03-25 11:28:37","title":"Investigating the Readability of Test Code: Combining Scientific and Practical Views","abstract":"The readability of source code is key for understanding and maintaining software systems and tests. Several studies investigate the readability of source code, but there is limited research on the readability of test code and related influence factors. We investigate the factors that influence the readability of test code from an academic perspective complemented by practical views. First, we perform a Systematic Mapping Study (SMS) with a focus on scientific literature. Second, we extend this study by reviewing grey literature sources for practical aspects on test code readability and understandability. Finally, we conduct a controlled experiment on the readability of a selected set of test cases to collect additional knowledge on influence factors discussed in practice. The result set of the SMS includes 19 primary studies from the scientific literature. The grey literature search reveals 62 sources for information on test code readability. Based on an analysis of these sources, we identified a combined set of 14 factors that influence the readability of test code. 7 of these factors were found in scientific and grey literature, while some factors were mainly discussed in academia (2) or industry (5) with limited overlap. The controlled experiment on practically relevant influence factors showed that the investigated factors have a significant impact on readability for half of the selected test cases. Our review of scientific and grey literature showed that test code readability is of interest for academia and industry with a consensus on key influence factors. However, we also found factors only discussed by practitioners. For some of these factors we were able to confirm an impact on readability in a first experiment. Therefore, we see the need to bring together academic and industry viewpoints to achieve a common view on the readability of software test code.","sentences":["The readability of source code is key for understanding and maintaining software systems and tests.","Several studies investigate the readability of source code, but there is limited research on the readability of test code and related influence factors.","We investigate the factors that influence the readability of test code from an academic perspective complemented by practical views.","First, we perform a Systematic Mapping Study (SMS) with a focus on scientific literature.","Second, we extend this study by reviewing grey literature sources for practical aspects on test code readability and understandability.","Finally, we conduct a controlled experiment on the readability of a selected set of test cases to collect additional knowledge on influence factors discussed in practice.","The result set of the SMS includes 19 primary studies from the scientific literature.","The grey literature search reveals 62 sources for information on test code readability.","Based on an analysis of these sources, we identified a combined set of 14 factors that influence the readability of test code.","7 of these factors were found in scientific and grey literature, while some factors were mainly discussed in academia (2) or industry (5) with limited overlap.","The controlled experiment on practically relevant influence factors showed that the investigated factors have a significant impact on readability for half of the selected test cases.","Our review of scientific and grey literature showed that test code readability is of interest for academia and industry with a consensus on key influence factors.","However, we also found factors only discussed by practitioners.","For some of these factors we were able to confirm an impact on readability in a first experiment.","Therefore, we see the need to bring together academic and industry viewpoints to achieve a common view on the readability of software test code."],"url":"http://arxiv.org/abs/2403.16639v1","category":"cs.SE"}
{"created":"2024-03-25 11:22:38","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for Computations in Matlab","abstract":"Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements. This fact has led to an increasing adoption of GA in applied mathematics and engineering problems. However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with. This prevents wider adoption among more applied professionals. To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License. SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations. It supports both numeric and symbolic computations in high-dimensional GAs. Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature. Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics. Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems.","sentences":["Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements.","This fact has led to an increasing adoption of GA in applied mathematics and engineering problems.","However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with.","This prevents wider adoption among more applied professionals.","To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License.","SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations.","It supports both numeric and symbolic computations in high-dimensional GAs.","Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature.","Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics.","Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems."],"url":"http://arxiv.org/abs/2403.16634v1","category":"cs.MS"}
{"created":"2024-03-25 10:50:19","title":"Preparation of tautomer-pure molecular beams by electrostatic deflection","abstract":"Tautomers are ubiquitous throughout chemistry, and typically considered inseparable in solution. Yet (bio)chemical activity is highly tautomer specific, with common examples being the amino and nucleic acids. While tautomers exist in an equilibrium in solution, in the cold environment of a molecular beam the barrier to tautomerization is typically much too high for interconversion, and tautomers can be considered separate species. Here we demonstrate the separation of tautomers and production of tautomerically-pure gas-phase samples. We show this for the 2-pyridone / 2-hydroxypyridine system, an important structural motif in both uracil and cytosine. Spatial separation of the tautomers is achieved via electrostatic deflection in strong inhomogeneous fields. We furthermore collect tautomer-resolved photoelectron spectra using femtosecond multiphoton ionization. This paves the way for studying the structure-function-dynamic relationship on the level of individual tautomers, using approaches that typically lack the resolution to do so, such as ultrafast dynamics experiments.","sentences":["Tautomers are ubiquitous throughout chemistry, and typically considered inseparable in solution.","Yet (bio)chemical activity is highly tautomer specific, with common examples being the amino and nucleic acids.","While tautomers exist in an equilibrium in solution, in the cold environment of a molecular beam the barrier to tautomerization is typically much too high for interconversion, and tautomers can be considered separate species.","Here we demonstrate the separation of tautomers and production of tautomerically-pure gas-phase samples.","We show this for the 2-pyridone / 2-hydroxypyridine system, an important structural motif in both uracil and cytosine.","Spatial separation of the tautomers is achieved via electrostatic deflection in strong inhomogeneous fields.","We furthermore collect tautomer-resolved photoelectron spectra using femtosecond multiphoton ionization.","This paves the way for studying the structure-function-dynamic relationship on the level of individual tautomers, using approaches that typically lack the resolution to do so, such as ultrafast dynamics experiments."],"url":"http://arxiv.org/abs/2403.16617v1","category":"physics.chem-ph"}
{"created":"2024-03-25 09:47:07","title":"Identification of Cyclists' Route Choice Criteria","abstract":"The behavior of cyclists when choosing the path to follow along a road network is not uniform. Some of them are mostly interested in minimizing the travelled distance, but some others may also take into account other features such as safety of the roads or pollution. Individuating the different groups of users, estimating the numerical consistency of each of these groups, and reporting the weights assigned by each group to different characteristics of the road network, is quite relevant. Indeed, when decision makers need to assign some budget for infrastructural interventions, they need to know the impact of their decisions, and this is strictly related to the way users perceive different features of the road network. In this paper, we propose an optimization approach to detect the weights assigned to different road features by various user groups, leveraging knowledge of the true paths followed by them, accessible, for example, through data collected by bike-sharing services.","sentences":["The behavior of cyclists when choosing the path to follow along a road network is not uniform.","Some of them are mostly interested in minimizing the travelled distance, but some others may also take into account other features such as safety of the roads or pollution.","Individuating the different groups of users, estimating the numerical consistency of each of these groups, and reporting the weights assigned by each group to different characteristics of the road network, is quite relevant.","Indeed, when decision makers need to assign some budget for infrastructural interventions, they need to know the impact of their decisions, and this is strictly related to the way users perceive different features of the road network.","In this paper, we propose an optimization approach to detect the weights assigned to different road features by various user groups, leveraging knowledge of the true paths followed by them, accessible, for example, through data collected by bike-sharing services."],"url":"http://arxiv.org/abs/2403.16580v1","category":"math.OC"}
{"created":"2024-03-25 09:33:21","title":"Higher-spin gauge theories in three spacetime dimensions","abstract":"These lecture notes provide an introduction to higher-spin gauge theories in three spacetime dimensions, with a focus on their asymptotic symmetries, their holographic description in terms of conformal field theories with W-symmetries as well as on their couplings to scalar matter.","sentences":["These lecture notes provide an introduction to higher-spin gauge theories in three spacetime dimensions, with a focus on their asymptotic symmetries, their holographic description in terms of conformal field theories with W-symmetries as well as on their couplings to scalar matter."],"url":"http://arxiv.org/abs/2403.16567v1","category":"hep-th"}
{"created":"2024-03-25 08:58:29","title":"Imaging quantum interference in a monolayer Kitaev quantum spin liquid candidate","abstract":"Single atomic defects are prominent windows to look into host quantum states because collective responses from the host states emerge as localized states around the defects. Friedel oscillations and Kondo clouds in Fermi liquids are quintessential examples. However, the situation is quite different for quantum spin liquid (QSL), an exotic state of matter with fractionalized quasiparticles and topological order arising from a profound impact of quantum entanglement. Elucidating the underlying local electronic property has been challenging due to the charge neutrality of fractionalized quasiparticles and the insulating nature of QSLs. Here, using spectroscopic-imaging scanning tunneling microscopy, we report atomically resolved images of monolayer $\\alpha$-RuCl$_3$, the most promising Kitaev QSL candidate, on metallic substrates. We find quantum interference in the insulator manifesting as incommensurate and decaying spatial oscillations of the local density of states around defects with a characteristic bias dependence. The oscillation differs from any known spatial structures in its nature and does not exist in other Mott insulators, implying it is an exotic oscillation involved with excitations unique to $\\alpha$-RuCl$_3$. Numerical simulations can reproduce the observed oscillation by assuming that itinerant Majorana fermions of Kitaev QSL are scattered across the Majorana Fermi surface. The oscillation provides a new approach to exploring Kitaev QSLs through the local response against defects like Friedel oscillations in metals.","sentences":["Single atomic defects are prominent windows to look into host quantum states because collective responses from the host states emerge as localized states around the defects.","Friedel oscillations and Kondo clouds in Fermi liquids are quintessential examples.","However, the situation is quite different for quantum spin liquid (QSL), an exotic state of matter with fractionalized quasiparticles and topological order arising from a profound impact of quantum entanglement.","Elucidating the underlying local electronic property has been challenging due to the charge neutrality of fractionalized quasiparticles and the insulating nature of QSLs.","Here, using spectroscopic-imaging scanning tunneling microscopy, we report atomically resolved images of monolayer $\\alpha$-RuCl$_3$, the most promising Kitaev QSL candidate, on metallic substrates.","We find quantum interference in the insulator manifesting as incommensurate and decaying spatial oscillations of the local density of states around defects with a characteristic bias dependence.","The oscillation differs from any known spatial structures in its nature and does not exist in other Mott insulators, implying it is an exotic oscillation involved with excitations unique to $\\alpha$-RuCl$_3$. Numerical simulations can reproduce the observed oscillation by assuming that itinerant Majorana fermions of Kitaev QSL are scattered across the Majorana Fermi surface.","The oscillation provides a new approach to exploring Kitaev QSLs through the local response against defects like Friedel oscillations in metals."],"url":"http://arxiv.org/abs/2403.16553v1","category":"cond-mat.str-el"}
{"created":"2024-03-25 08:28:39","title":"Fluorophore signal and detection enhancement in nanowire biosensors","abstract":"Semiconductor nanowires have been demonstrated as an efficient platform for fluorescence-based biosensors. Here, we study theoretically how GaP nanowires (i) enhance the excitation intensity at the position of fluorophores attached to the nanowire sidewall, (ii) enhance the probability to collect photons emitted from the fluorophores by directing them preferentially into the numerical aperture of collection objective, and (iii) through the Purcell effect increase the quantum yield of fluorophores. We find that the optimum diameter depends strongly on the fluorophore emission wavelength. In addition to an overall signal-detection scheme, we model imaging-based detection of the fluorescence.","sentences":["Semiconductor nanowires have been demonstrated as an efficient platform for fluorescence-based biosensors.","Here, we study theoretically how GaP nanowires (i) enhance the excitation intensity at the position of fluorophores attached to the nanowire sidewall, (ii) enhance the probability to collect photons emitted from the fluorophores by directing them preferentially into the numerical aperture of collection objective, and (iii) through the Purcell effect increase the quantum yield of fluorophores.","We find that the optimum diameter depends strongly on the fluorophore emission wavelength.","In addition to an overall signal-detection scheme, we model imaging-based detection of the fluorescence."],"url":"http://arxiv.org/abs/2403.16537v1","category":"physics.optics"}
{"created":"2024-03-25 07:20:57","title":"Parity-sensitive inhomogeneous dephasing of macroscopic spin ensembles","abstract":"Spin ensembles play a pivotal role in various quantum applications such as metrology and simulating many-body physics. Recent research has proposed utilizing spin cat states to encode logical quantum information, with potentially logical lifetimes on the order of seconds via enhanced collective interactions that scale with system size. We investigate the dynamics of spin cat states under inhomogeneous broadening, revealing a phenomenon termed `parity-sensitive inhomogeneous dephasing': odd cat states are significantly more susceptible to inhomogeneous dephasing compared to even cat states due to parity symmetry. Additionally, from a mean-field analysis of the driven-dissipative dynamics, we identify a synchronization phase transition wherein the ensemble becomes completely dephased beyond a critical inhomogeneous linewidth. Our findings shed light on the stability of collective spin states, important for advancing quantum technologies.","sentences":["Spin ensembles play a pivotal role in various quantum applications such as metrology and simulating many-body physics.","Recent research has proposed utilizing spin cat states to encode logical quantum information, with potentially logical lifetimes on the order of seconds via enhanced collective interactions that scale with system size.","We investigate the dynamics of spin cat states under inhomogeneous broadening, revealing a phenomenon termed `parity-sensitive inhomogeneous dephasing': odd cat states are significantly more susceptible to inhomogeneous dephasing compared to even cat states due to parity symmetry.","Additionally, from a mean-field analysis of the driven-dissipative dynamics, we identify a synchronization phase transition wherein the ensemble becomes completely dephased beyond a critical inhomogeneous linewidth.","Our findings shed light on the stability of collective spin states, important for advancing quantum technologies."],"url":"http://arxiv.org/abs/2403.16491v1","category":"quant-ph"}
{"created":"2024-03-25 07:15:06","title":"ColonyOS -- A Meta-Operating System for Distributed Computing Across Heterogeneous Platform","abstract":"This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC. Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums. This makes it possible to develop AI workflows and applications that can operate across platforms. At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture. These executors collectively form a colony, serving as a unified computing unit. To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict zero-trust security protocol. Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications. These are sent to a Colonies server, which acts as intermediary between applications and the executors. Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor. Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms. Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum. The paper describes design principles and implementation details of ColonyOS.","sentences":["This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC.","Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums.","This makes it possible to develop AI workflows and applications that can operate across platforms.","At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture.","These executors collectively form a colony, serving as a unified computing unit.","To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict zero-trust security protocol.","Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications.","These are sent to a Colonies server, which acts as intermediary between applications and the executors.","Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor.","Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms.","Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum.","The paper describes design principles and implementation details of ColonyOS."],"url":"http://arxiv.org/abs/2403.16486v1","category":"cs.DC"}
{"created":"2024-03-25 07:08:01","title":"Determined Multi-Label Learning via Similarity-Based Prompt","abstract":"In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data. Additionally, we introduce a similarity-based prompt learning method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental prompt with richer semantic information. Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods.","sentences":["In multi-label classification, each training instance is associated with multiple class labels simultaneously.","Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications.","To alleviate this problem, a novel labeling setting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks.","In this novel labeling setting, each training instance is associated with a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates whether the training instance contains the provided class label.","The provided class label is randomly and uniformly selected from the whole candidate labels set.","Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets.","In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data.","Additionally, we introduce a similarity-based prompt learning method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental prompt with richer semantic information.","Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16482v1","category":"cs.LG"}
{"created":"2024-03-25 06:17:54","title":"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm","abstract":"Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language. Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.","sentences":["Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language.","Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations.","However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements.","To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment.","The evaluation paradigm contains three basic elements: metric, data, and algorithm.","Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess.","Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure.","Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP.","The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities.","Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE.","Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice."],"url":"http://arxiv.org/abs/2403.16446v1","category":"cs.CL"}
{"created":"2024-03-25 05:10:34","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras","abstract":"Event cameras are increasingly popular in robotics due to their beneficial features, such as low latency, energy efficiency, and high dynamic range. Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters. These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion. This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly fast adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (slow adaptation). Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database. We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time. To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information. Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods. Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms.","sentences":["Event cameras are increasingly popular in robotics due to their beneficial features, such as low latency, energy efficiency, and high dynamic range.","Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters.","These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion.","This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly fast adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (slow adaptation).","Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database.","We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time.","To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information.","Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods.","Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms."],"url":"http://arxiv.org/abs/2403.16425v1","category":"cs.RO"}
{"created":"2024-03-25 03:56:19","title":"Large-scale Array for Radio Astronomy on the Farside","abstract":"At the Royal Society meeting in 2023, we have mainly presented our lunar orbit array concept called DSL, and also briefly introduced a concept of a lunar surface array, LARAF. As the DSL concept had been presented before, in this article we introduce the LARAF. We propose to build an array in the far side of the Moon, with a master station which handles the data collection and processing, and 20 stations with maximum baseline of 10 km. Each station consists 12 membrane antenna units, and the stations are connected to the master station by power line and optical fiber. The array will make interferometric observation in the 0.1-50 MHz band during the lunar night, powered by regenerated fuel cells (RFCs). The whole array can be carried to the lunar surface with a heavy rocket mission, and deployed with a rover in 8 months. Such an array would be an important step in the long term development of lunar based ultralong wavelength radio astronomy. It has a sufficiently high sensitivity to observe many radio sources in the sky, though still short of the dark age fluctuations. We discuss the possible options in the power supply, data communication, deployment, etc.","sentences":["At the Royal Society meeting in 2023, we have mainly presented our lunar orbit array concept called DSL, and also briefly introduced a concept of a lunar surface array, LARAF.","As the DSL concept had been presented before, in this article we introduce the LARAF.","We propose to build an array in the far side of the Moon, with a master station which handles the data collection and processing, and 20 stations with maximum baseline of 10 km.","Each station consists 12 membrane antenna units, and the stations are connected to the master station by power line and optical fiber.","The array will make interferometric observation in the 0.1-50 MHz band during the lunar night, powered by regenerated fuel cells (RFCs).","The whole array can be carried to the lunar surface with a heavy rocket mission, and deployed with a rover in 8 months.","Such an array would be an important step in the long term development of lunar based ultralong wavelength radio astronomy.","It has a sufficiently high sensitivity to observe many radio sources in the sky, though still short of the dark age fluctuations.","We discuss the possible options in the power supply, data communication, deployment, etc."],"url":"http://arxiv.org/abs/2403.16409v1","category":"astro-ph.IM"}
{"created":"2024-03-25 02:37:18","title":"A new social welfare function with a number of desirable properties","abstract":"By relaxing the dominating set in three ways (e.g., from \"each member beats every non-member\" to \"each member beats or ties every non-member, with an additional requirement that at least one member beat every non-member\"), we propose a new social welfare function, which satisfies a number of desirable properties including Condorcet winner principle, Condorcet loser principle, strong Gehrlein-stability (hence Smith set principle), anonymity, neutrality, weak Pareto, strong Pareto, non-dictatorship, and [independence of irrelevant alternatives (IIA) when the pairwise majority relation is an ordering on the alternative set]. If the pairwise majority relation is complete and transitive, the proposed method yields a collective preference relation that coincides with the input majority relation. It thus shares the same collective preference function on the dichotomous domain with the approval voting and the majority voting. It runs in polynomial time and thus possesses a competitive advantage over a number of computationally intractable voting rules such as the Dodgson's rule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's tournament equilibrium set (TEQ) rule. When it is used in tournaments, its winner belongs to the uncovered set, the top cycle set, the Smith set, and the Schwartz set. In addition, in a tournament where the number of alternatives is not more than 4, its winner set is a subset, sometimes proper, of the Copeland winner set. Whether this attractive argument is still valid in four-more-alternative tournaments remains an open question.","sentences":["By relaxing the dominating set in three ways (e.g., from \"each member beats every non-member\" to \"each member beats or ties every non-member, with an additional requirement that at least one member beat every non-member\"), we propose a new social welfare function, which satisfies a number of desirable properties including Condorcet winner principle, Condorcet loser principle, strong Gehrlein-stability (hence Smith set principle), anonymity, neutrality, weak Pareto, strong Pareto, non-dictatorship, and","[independence of irrelevant alternatives (IIA) when the pairwise majority relation is an ordering on the alternative set].","If the pairwise majority relation is complete and transitive, the proposed method yields a collective preference relation that coincides with the input majority relation.","It thus shares the same collective preference function on the dichotomous domain with the approval voting and the majority voting.","It runs in polynomial time and thus possesses a competitive advantage over a number of computationally intractable voting rules such as the Dodgson's rule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's tournament equilibrium set (TEQ) rule.","When it is used in tournaments, its winner belongs to the uncovered set, the top cycle set, the Smith set, and the Schwartz set.","In addition, in a tournament where the number of alternatives is not more than 4, its winner set is a subset, sometimes proper, of the Copeland winner set.","Whether this attractive argument is still valid in four-more-alternative tournaments remains an open question."],"url":"http://arxiv.org/abs/2403.16373v1","category":"econ.TH"}
{"created":"2024-03-25 00:59:35","title":"3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical Image Segmentation","abstract":"Medical image segmentation (MIS) aims to finely segment various organs. It requires grasping global information from both parts and the entire image for better segmenting, and clinically there are often certain requirements for segmentation efficiency. Convolutional neural networks (CNNs) have made considerable achievements in MIS. However, they are difficult to fully collect global context information and their pooling layer may cause information loss. Capsule networks, which combine the benefits of CNNs while taking into account additional information such as relative location that CNNs do not, have lately demonstrated some advantages in MIS. Vision Transformer (ViT) employs transformers in visual tasks. Transformer based on attention mechanism has excellent global inductive modeling capabilities and is expected to capture longrange information. Moreover, there have been resent studies on making ViT more lightweight to minimize model complexity and increase efficiency. In this paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps, which combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our encoder uses capsule blocks and EfficientViT blocks to jointly capture local and global semantic information more effectively and efficiently with less information loss, while the decoder employs CNN blocks and EfficientViT blocks to catch ffner details for segmentation. We conduct experiments on various datasets, including iSeg-2017, Hippocampus and Cardiac to verify the performance and efficiency of 3D-EffiViTCaps, which performs better than previous 3D CNN-based, 3D Capsule-based and 3D Transformer-based models. We further implement a series of ablation experiments on the main blocks. Our code is available at: https://github.com/HidNeuron/3D-EffiViTCaps.","sentences":["Medical image segmentation (MIS) aims to finely segment various organs.","It requires grasping global information from both parts and the entire image for better segmenting, and clinically there are often certain requirements for segmentation efficiency.","Convolutional neural networks (CNNs) have made considerable achievements in MIS.","However, they are difficult to fully collect global context information and their pooling layer may cause information loss.","Capsule networks, which combine the benefits of CNNs while taking into account additional information such as relative location that CNNs do not, have lately demonstrated some advantages in MIS.","Vision Transformer (ViT) employs transformers in visual tasks.","Transformer based on attention mechanism has excellent global inductive modeling capabilities and is expected to capture longrange information.","Moreover, there have been resent studies on making ViT more lightweight to minimize model complexity and increase efficiency.","In this paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps, which combines 3D capsule blocks with 3D EfficientViT blocks for MIS.","Our encoder uses capsule blocks and EfficientViT blocks to jointly capture local and global semantic information more effectively and efficiently with less information loss, while the decoder employs CNN blocks and EfficientViT blocks to catch ffner details for segmentation.","We conduct experiments on various datasets, including iSeg-2017, Hippocampus and Cardiac to verify the performance and efficiency of 3D-EffiViTCaps, which performs better than previous 3D CNN-based, 3D Capsule-based and 3D Transformer-based models.","We further implement a series of ablation experiments on the main blocks.","Our code is available at: https://github.com/HidNeuron/3D-EffiViTCaps."],"url":"http://arxiv.org/abs/2403.16350v1","category":"eess.IV"}
{"created":"2024-03-24 22:35:54","title":"Datasets of Great Britain Primary Substations Integrated with Household Heating Information","abstract":"The growing demand for electrified heating, electrified transportation, and power-intensive data centres challenge distribution networks. If electrification projects are carried out without considering electrical distribution infrastructure, there could be unexpected blackouts and financial losses. Datasets containing real-world distribution network information are required to address this. On the other hand, social data, such as household heating composition, are closely coupled with people's lives. Studying the coupling between the energy system and society is important in promoting social welfare. To fill these gaps, this paper introduces two datasets. The first is the main dataset for the distribution networks in Great Britain (GB), collecting information on firm capacity, peak demands, locations, and parent transmission nodes (the Grid Supply Point, namely GSP) for all primary substations (PSs). PSs are a crucial part of the UK distribution network and are at the lowest voltage level (11 kV) with publicly available data for most UK Distribution Network Operators (DNOs). Substation firm capacity and peak demand facilitate an understanding of the remaining room of the existing network. The parent GSP information helps link the dataset of distribution networks to datasets of transmission networks. The second dataset extends the main network dataset, linking each PS to information about the number of households that use different types of central heating recorded in census data. The derivation of the second dataset is based on locations of PSs collected in the main dataset with appropriate assumptions. The derivation process may also be replicated to integrate other social datasets.","sentences":["The growing demand for electrified heating, electrified transportation, and power-intensive data centres challenge distribution networks.","If electrification projects are carried out without considering electrical distribution infrastructure, there could be unexpected blackouts and financial losses.","Datasets containing real-world distribution network information are required to address this.","On the other hand, social data, such as household heating composition, are closely coupled with people's lives.","Studying the coupling between the energy system and society is important in promoting social welfare.","To fill these gaps, this paper introduces two datasets.","The first is the main dataset for the distribution networks in Great Britain (GB), collecting information on firm capacity, peak demands, locations, and parent transmission nodes (the Grid Supply Point, namely GSP) for all primary substations (PSs).","PSs are a crucial part of the UK distribution network and are at the lowest voltage level (11 kV) with publicly available data for most UK Distribution Network Operators (DNOs).","Substation firm capacity and peak demand facilitate an understanding of the remaining room of the existing network.","The parent GSP information helps link the dataset of distribution networks to datasets of transmission networks.","The second dataset extends the main network dataset, linking each PS to information about the number of households that use different types of central heating recorded in census data.","The derivation of the second dataset is based on locations of PSs collected in the main dataset with appropriate assumptions.","The derivation process may also be replicated to integrate other social datasets."],"url":"http://arxiv.org/abs/2403.16313v1","category":"eess.SY"}
{"created":"2024-03-24 19:54:30","title":"A descent basis for the Garsia-Procesi module","abstract":"We assign to each Young diagram $\\lambda$ a subset $\\mathcal{B}_{\\lambda'}$ of the collection of Garsia-Stanton descent monomials, and prove that it determines a basis of the Garsia-Procesi module $R_\\lambda$, whose graded character is the Hall-Littlewood polynomial $\\tilde{H}_{\\lambda}[X;t]$. This basis is a major index analogue of the basis $\\mathcal{B}_\\lambda \\subset R_\\lambda$ defined by certain recursions in due to Garsia and Procesi, in the same way that the descent basis is related to the Artin basis of the coinvariant algebra $R_n$, which in fact corresponds to the case when $\\lambda=1^n$. By anti-symmetrizing a subset of this basis with respect to the corresponding Young subgroup under the Springer action, we obtain a basis in the parabolic case, as well as a corresponding formula for the expansion of $\\tilde{H}_{\\lambda}[X;t]$. Despite a similar appearance, it does not appear obvious how to connect these formulas appear to the specialization of the modified Macdonald formula of Haglund, Haiman and Loehr at $q=0$.","sentences":["We assign to each Young diagram $\\lambda$ a subset $\\mathcal{B}_{\\lambda'}$ of the collection of Garsia-Stanton descent monomials, and prove that it determines a basis of the Garsia-Procesi module $R_\\lambda$, whose graded character is the Hall-Littlewood polynomial $\\tilde{H}_{\\lambda}[X;t]$.","This basis is a major index analogue of the basis $\\mathcal{B}_\\lambda \\subset R_\\lambda$ defined by certain recursions in due to Garsia and Procesi, in the same way that the descent basis is related to the Artin basis of the coinvariant algebra $R_n$, which in fact corresponds to the case when $\\lambda=1^n$. By anti-symmetrizing a subset of this basis with respect to the corresponding Young subgroup under the Springer action, we obtain a basis in the parabolic case, as well as a corresponding formula for the expansion of $\\tilde{H}_{\\lambda}[X;t]$. Despite a similar appearance, it does not appear obvious how to connect these formulas appear to the specialization of the modified Macdonald formula of Haglund, Haiman and Loehr at $q=0$."],"url":"http://arxiv.org/abs/2403.16278v1","category":"math.RT"}
{"created":"2024-03-24 18:53:57","title":"Emotion Recognition from the perspective of Activity Recognition","abstract":"Applications of an efficient emotion recognition system can be found in several domains such as medicine, driver fatigue surveillance, social robotics, and human-computer interaction. Appraising human emotional states, behaviors, and reactions displayed in real-world settings can be accomplished using latent continuous dimensions. Continuous dimensional models of human affect, such as those based on valence and arousal are more accurate in describing a broad range of spontaneous everyday emotions than more traditional models of discrete stereotypical emotion categories (e.g. happiness, surprise). Most of the prior work on estimating valence and arousal considers laboratory settings and acted data. But, for emotion recognition systems to be deployed and integrated into real-world mobile and computing devices, we need to consider data collected in the world. Action recognition is a domain of Computer Vision that involves capturing complementary information on appearance from still frames and motion between frames. In this paper, we treat emotion recognition from the perspective of action recognition by exploring the application of deep learning architectures specifically designed for action recognition, for continuous affect recognition. We propose a novel three-stream end-to-end deep learning regression pipeline with an attention mechanism, which is an ensemble design based on sub-modules of multiple state-of-the-art action recognition systems. The pipeline constitutes a novel data pre-processing approach with a spatial self-attention mechanism to extract keyframes. The optical flow of high-attention regions of the face is extracted to capture temporal context. AFEW-VA in-the-wild dataset has been used to conduct comparative experiments. Quantitative analysis shows that the proposed model outperforms multiple standard baselines of both emotion recognition and action recognition models.","sentences":["Applications of an efficient emotion recognition system can be found in several domains such as medicine, driver fatigue surveillance, social robotics, and human-computer interaction.","Appraising human emotional states, behaviors, and reactions displayed in real-world settings can be accomplished using latent continuous dimensions.","Continuous dimensional models of human affect, such as those based on valence and arousal are more accurate in describing a broad range of spontaneous everyday emotions than more traditional models of discrete stereotypical emotion categories (e.g. happiness, surprise).","Most of the prior work on estimating valence and arousal considers laboratory settings and acted data.","But, for emotion recognition systems to be deployed and integrated into real-world mobile and computing devices, we need to consider data collected in the world.","Action recognition is a domain of Computer Vision that involves capturing complementary information on appearance from still frames and motion between frames.","In this paper, we treat emotion recognition from the perspective of action recognition by exploring the application of deep learning architectures specifically designed for action recognition, for continuous affect recognition.","We propose a novel three-stream end-to-end deep learning regression pipeline with an attention mechanism, which is an ensemble design based on sub-modules of multiple state-of-the-art action recognition systems.","The pipeline constitutes a novel data pre-processing approach with a spatial self-attention mechanism to extract keyframes.","The optical flow of high-attention regions of the face is extracted to capture temporal context.","AFEW-VA in-the-wild dataset has been used to conduct comparative experiments.","Quantitative analysis shows that the proposed model outperforms multiple standard baselines of both emotion recognition and action recognition models."],"url":"http://arxiv.org/abs/2403.16263v1","category":"cs.CV"}
{"created":"2024-03-24 17:21:32","title":"On the Equivalency, Substitutability, and Flexibility of Synthetic Data","abstract":"We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios. Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs. Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection. To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps. Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic data not only enhances model performance but also demonstrates substitutability for real data, with 60% to 80% replacement without performance loss. In addition, our study of the impact of synthetic data distributions on downstream performance reveals the importance of flexible data generators in narrowing domain gaps for improved model adaptability.","sentences":["We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios.","Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs.","Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection.","To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps.","Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17.","Our results suggest that synthetic data not only enhances model performance but also demonstrates substitutability for real data, with 60% to 80% replacement without performance loss.","In addition, our study of the impact of synthetic data distributions on downstream performance reveals the importance of flexible data generators in narrowing domain gaps for improved model adaptability."],"url":"http://arxiv.org/abs/2403.16244v1","category":"cs.LG"}
{"created":"2024-03-24 16:48:10","title":"On machine learning analysis of atomic force microscopy images for image classification, sample surface recognition","abstract":"Atomic force microscopy (AFM or SPM) imaging is one of the best matches with machine learning (ML) analysis among microscopy techniques. The digital format of AFM images allows for direct utilization in ML algorithms without the need for additional processing. Additionally, AFM enables the simultaneous imaging of distributions of over a dozen different physicochemical properties of sample surfaces, a process known as multidimensional imaging. While this wealth of information can be challenging to analyze using traditional methods, ML provides a seamless approach to this task. However, the relatively slow speed of AFM imaging poses a challenge in applying deep learning methods broadly used in image recognition. This Prospective is focused on ML recognition/classification when using a relatively small number of AFM images, small database. We discuss ML methods other than popular deep-learning neural networks. The described approach has already been successfully used to analyze and classify the surfaces of biological cells. It can be applied to recognize medical images, specific material processing, in forensic studies, even to identify the authenticity of arts. A general template for ML analysis specific to AFM is suggested, with a specific example of the identification of cell phenotype. Special attention is given to the analysis of the statistical significance of the obtained results, an important feature that is often overlooked in papers dealing with machine learning. A simple method for finding statistical significance is also described.","sentences":["Atomic force microscopy (AFM or SPM) imaging is one of the best matches with machine learning (ML) analysis among microscopy techniques.","The digital format of AFM images allows for direct utilization in ML algorithms without the need for additional processing.","Additionally, AFM enables the simultaneous imaging of distributions of over a dozen different physicochemical properties of sample surfaces, a process known as multidimensional imaging.","While this wealth of information can be challenging to analyze using traditional methods, ML provides a seamless approach to this task.","However, the relatively slow speed of AFM imaging poses a challenge in applying deep learning methods broadly used in image recognition.","This Prospective is focused on ML recognition/classification when using a relatively small number of AFM images, small database.","We discuss ML methods other than popular deep-learning neural networks.","The described approach has already been successfully used to analyze and classify the surfaces of biological cells.","It can be applied to recognize medical images, specific material processing, in forensic studies, even to identify the authenticity of arts.","A general template for ML analysis specific to AFM is suggested, with a specific example of the identification of cell phenotype.","Special attention is given to the analysis of the statistical significance of the obtained results, an important feature that is often overlooked in papers dealing with machine learning.","A simple method for finding statistical significance is also described."],"url":"http://arxiv.org/abs/2403.16230v1","category":"physics.bio-ph"}
{"created":"2024-03-24 16:41:50","title":"Dual-modal Prior Semantic Guided Infrared and Visible Image Fusion for Intelligent Transportation System","abstract":"Infrared and visible image fusion (IVF) plays an important role in intelligent transportation system (ITS). The early works predominantly focus on boosting the visual appeal of the fused result, and only several recent approaches have tried to combine the high-level vision task with IVF. However, they prioritize the design of cascaded structure to seek unified suitable features and fit different tasks. Thus, they tend to typically bias toward to reconstructing raw pixels without considering the significance of semantic features. Therefore, we propose a novel prior semantic guided image fusion method based on the dual-modality strategy, improving the performance of IVF in ITS. Specifically, to explore the independent significant semantic of each modality, we first design two parallel semantic segmentation branches with a refined feature adaptive-modulation (RFaM) mechanism. RFaM can perceive the features that are semantically distinct enough in each semantic segmentation branch. Then, two pilot experiments based on the two branches are conducted to capture the significant prior semantic of two images, which then is applied to guide the fusion task in the integration of semantic segmentation branches and fusion branches. In addition, to aggregate both high-level semantics and impressive visual effects, we further investigate the frequency response of the prior semantics, and propose a multi-level representation-adaptive fusion (MRaF) module to explicitly integrate the low-frequent prior semantic with the high-frequent details. Extensive experiments on two public datasets demonstrate the superiority of our method over the state-of-the-art image fusion approaches, in terms of either the visual appeal or the high-level semantics.","sentences":["Infrared and visible image fusion (IVF) plays an important role in intelligent transportation system (ITS).","The early works predominantly focus on boosting the visual appeal of the fused result, and only several recent approaches have tried to combine the high-level vision task with IVF.","However, they prioritize the design of cascaded structure to seek unified suitable features and fit different tasks.","Thus, they tend to typically bias toward to reconstructing raw pixels without considering the significance of semantic features.","Therefore, we propose a novel prior semantic guided image fusion method based on the dual-modality strategy, improving the performance of IVF in ITS.","Specifically, to explore the independent significant semantic of each modality, we first design two parallel semantic segmentation branches with a refined feature adaptive-modulation (RFaM) mechanism.","RFaM can perceive the features that are semantically distinct enough in each semantic segmentation branch.","Then, two pilot experiments based on the two branches are conducted to capture the significant prior semantic of two images, which then is applied to guide the fusion task in the integration of semantic segmentation branches and fusion branches.","In addition, to aggregate both high-level semantics and impressive visual effects, we further investigate the frequency response of the prior semantics, and propose a multi-level representation-adaptive fusion (MRaF) module to explicitly integrate the low-frequent prior semantic with the high-frequent details.","Extensive experiments on two public datasets demonstrate the superiority of our method over the state-of-the-art image fusion approaches, in terms of either the visual appeal or the high-level semantics."],"url":"http://arxiv.org/abs/2403.16227v1","category":"cs.CV"}
{"created":"2024-03-24 16:36:12","title":"Bi-Level Control of Weaving Sections in Mixed Traffic Environments with Connected and Automated Vehicles","abstract":"Connected and automated vehicles (CAVs) can be beneficial for improving the operation of highway bottlenecks such as weaving sections. This paper proposes a bi-level control approach based on an upper-level deep reinforcement learning controller and a lower-level model predictive controller to coordinate the lane-changings of a mixed fleet of CAVs and human-driven vehicles (HVs) in weaving sections. The upper level represents a roadside controller that collects vehicular information from the entire weaving section and determines the control weights used in the lower-level controller. The lower level is implemented within each CAV, which takes the control weights from the upper-level controller and generates the acceleration and steering angle for individual CAVs based on the local situation. The lower-level controller further incorporates an HV trajectory predictor, which is capable of handling the dynamic topology of vehicles in weaving scenarios with intensive mandatory lane changes. The case study inspired by a real weaving section in Basel, Switzerland, shows that our method consistently outperforms state-of-the-art benchmarks.","sentences":["Connected and automated vehicles (CAVs) can be beneficial for improving the operation of highway bottlenecks such as weaving sections.","This paper proposes a bi-level control approach based on an upper-level deep reinforcement learning controller and a lower-level model predictive controller to coordinate the lane-changings of a mixed fleet of CAVs and human-driven vehicles (HVs) in weaving sections.","The upper level represents a roadside controller that collects vehicular information from the entire weaving section and determines the control weights used in the lower-level controller.","The lower level is implemented within each CAV, which takes the control weights from the upper-level controller and generates the acceleration and steering angle for individual CAVs based on the local situation.","The lower-level controller further incorporates an HV trajectory predictor, which is capable of handling the dynamic topology of vehicles in weaving scenarios with intensive mandatory lane changes.","The case study inspired by a real weaving section in Basel, Switzerland, shows that our method consistently outperforms state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2403.16225v1","category":"eess.SY"}
{"created":"2024-03-24 16:30:05","title":"Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization","abstract":"Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers. As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets. Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature. One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text. In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers. We demonstrate this concept in the cybersecurity domain. One modality of the KG represents observable information from the papers, such as the categories in which they were published or the authors. The second modality uncovers latent (hidden) patterns of text extracted through hierarchical and semantic non-negative matrix factorization (NMF), such as named entities, topics or clusters, and keywords. We illustrate this concept by consolidating more than two million scientific papers uploaded to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by building a cyber-domain-specific KG.","sentences":["Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers.","As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets.","Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature.","One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text.","In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers.","We demonstrate this concept in the cybersecurity domain.","One modality of the KG represents observable information from the papers, such as the categories in which they were published or the authors.","The second modality uncovers latent (hidden) patterns of text extracted through hierarchical and semantic non-negative matrix factorization (NMF), such as named entities, topics or clusters, and keywords.","We illustrate this concept by consolidating more than two million scientific papers uploaded to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by building a cyber-domain-specific KG."],"url":"http://arxiv.org/abs/2403.16222v1","category":"cs.AI"}
{"created":"2024-03-24 16:18:27","title":"CoverUp: Coverage-Guided LLM-Based Test Generation","abstract":"This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.","sentences":["This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs).","CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches.","The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board.","On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%).","We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes."],"url":"http://arxiv.org/abs/2403.16218v1","category":"cs.SE"}
{"created":"2024-03-24 16:09:21","title":"Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane","abstract":"We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting.","sentences":["We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass.","Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part.","The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes.","During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes.","Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts.","The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting."],"url":"http://arxiv.org/abs/2403.16210v1","category":"cs.CV"}
{"created":"2024-03-24 16:08:10","title":"Image Captioning in news report scenario","abstract":"Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the narrative in news reporting through a more intuitive image captioning framework.","sentences":["Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP).","This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond.","Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images.","However, much of the existing body of work primarily centers around understanding scenes and actions.","In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices.","This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information.","Our endeavor shows a broader horizon, enriching the narrative in news reporting through a more intuitive image captioning framework."],"url":"http://arxiv.org/abs/2403.16209v1","category":"cs.CV"}
{"created":"2024-03-24 15:59:47","title":"Rumor Detection with a novel graph neural network approach","abstract":"The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these two modules to classify the rumors. Since malicious users intend to subvert our model after deployment, we further develop a greedy attack scheme to analyze the cost of three adversarial attacks: graph attack, comment attack, and joint attack. Evaluation results on two public datasets illustrate that the proposed MODEL outperforms the state-of-the-art rumor detection models. We also demonstrate our method performs well for early rumor detection. Moreover, the proposed detection method is more robust to adversarial attacks compared to the best existing method. Importantly, we show that it requires a high cost for attackers to subvert user correlation pattern, demonstrating the importance of considering user correlation for rumor detection.","sentences":["The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public.","How to debunk rumors as early as possible remains a challenging problem.","Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity.","In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media.","Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure.","Then we combine the learned representations from these two modules to classify the rumors.","Since malicious users intend to subvert our model after deployment, we further develop a greedy attack scheme to analyze the cost of three adversarial attacks: graph attack, comment attack, and joint attack.","Evaluation results on two public datasets illustrate that the proposed MODEL outperforms the state-of-the-art rumor detection models.","We also demonstrate our method performs well for early rumor detection.","Moreover, the proposed detection method is more robust to adversarial attacks compared to the best existing method.","Importantly, we show that it requires a high cost for attackers to subvert user correlation pattern, demonstrating the importance of considering user correlation for rumor detection."],"url":"http://arxiv.org/abs/2403.16206v1","category":"cs.AI"}
{"created":"2024-03-24 15:56:03","title":"Maximum Polygon Packing: The CG:SHOP Challenge 2024","abstract":"We give an overview of the 2024 Computational Geometry Challenge targeting the problem \\textsc{Maximum Polygon Packing}: Given a convex region $P$ in the plane, and a collection of simple polygons $Q_1, \\ldots, Q_n$, each $Q_i$ with a respective value $c_i$, find a subset $S \\subseteq \\{1, \\ldots,n\\}$ and a feasible packing within $P$ of the polygons $Q_i$ (without rotation) for $i \\in S$, maximizing $\\sum_{i \\in S} c_i$. Geometric packing problems, such as this, present significant computational challenges and are of substantial practical importance.","sentences":["We give an overview of the 2024 Computational Geometry Challenge targeting the problem \\textsc{Maximum Polygon Packing}: Given a convex region $P$ in the plane, and a collection of simple polygons $Q_1, \\ldots, Q_n$, each $Q_i$ with a respective value $c_i$, find a subset $S \\subseteq \\{1, \\ldots,n\\}$ and a feasible packing within $P$ of the polygons $Q_i$ (without rotation) for $i \\in S$, maximizing $\\sum_{i \\in S} c_i$.","Geometric packing problems, such as this, present significant computational challenges and are of substantial practical importance."],"url":"http://arxiv.org/abs/2403.16203v1","category":"cs.CG"}
{"created":"2024-03-24 15:14:44","title":"Logic-based Explanations for Linear Support Vector Classifiers with Reject Option","abstract":"Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems. It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist. This further increases the confidence of the model. Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results. While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present. We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option. We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations. Obtained results show that our proposed method gives shorter explanations with reduced time cost.","sentences":["Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems.","It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist.","This further increases the confidence of the model.","Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results.","While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present.","We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option.","We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations.","Obtained results show that our proposed method gives shorter explanations with reduced time cost."],"url":"http://arxiv.org/abs/2403.16190v1","category":"cs.AI"}
{"created":"2024-03-24 14:38:18","title":"Mixed-Initiative Human-Robot Teaming under Suboptimality with Online Bayesian Adaptation","abstract":"For effective human-agent teaming, robots and other artificial intelligence (AI) agents must infer their human partner's abilities and behavioral response patterns and adapt accordingly. Most prior works make the unrealistic assumption that one or more teammates can act near-optimally. In real-world collaboration, humans and autonomous agents can be suboptimal, especially when each only has partial domain knowledge. In this work, we develop computational modeling and optimization techniques for enhancing the performance of suboptimal human-agent teams, where the human and the agent have asymmetric capabilities and act suboptimally due to incomplete environmental knowledge. We adopt an online Bayesian approach that enables a robot to infer people's willingness to comply with its assistance in a sequential decision-making game. Our user studies show that user preferences and team performance indeed vary with robot intervention styles, and our approach for mixed-initiative collaborations enhances objective team performance ($p<.001$) and subjective measures, such as user's trust ($p<.001$) and perceived likeability of the robot ($p<.001$).","sentences":["For effective human-agent teaming, robots and other artificial intelligence (AI) agents must infer their human partner's abilities and behavioral response patterns and adapt accordingly.","Most prior works make the unrealistic assumption that one or more teammates can act near-optimally.","In real-world collaboration, humans and autonomous agents can be suboptimal, especially when each only has partial domain knowledge.","In this work, we develop computational modeling and optimization techniques for enhancing the performance of suboptimal human-agent teams, where the human and the agent have asymmetric capabilities and act suboptimally due to incomplete environmental knowledge.","We adopt an online Bayesian approach that enables a robot to infer people's willingness to comply with its assistance in a sequential decision-making game.","Our user studies show that user preferences and team performance indeed vary with robot intervention styles, and our approach for mixed-initiative collaborations enhances objective team performance ($p<.001$) and subjective measures, such as user's trust ($p<.001$) and perceived likeability of the robot ($p<.001$)."],"url":"http://arxiv.org/abs/2403.16178v1","category":"cs.RO"}
{"created":"2024-03-24 14:08:24","title":"An Analytic Solution to Covariance Propagation in Neural Networks","abstract":"Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.","sentences":["Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems.","However, this often involves costly or inaccurate sampling methods and approximations.","This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks.","A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU.","The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks."],"url":"http://arxiv.org/abs/2403.16163v1","category":"cs.LG"}
{"created":"2024-03-24 14:04:40","title":"Multi-Task Learning with Multi-Task Optimization","abstract":"Multi-task learning solves multiple correlated tasks. However, conflicts may exist between them. In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs. To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization. Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems. These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization. A theorem proving faster convergence through the inclusion of such transfers is presented. We investigate the proposed multi-task learning with multi-task optimization for solving various problem settings including image classification, scene understanding, and multi-target regression. Comprehensive experiments confirm that the proposed method significantly advances the state-of-the-art in discovering sets of Pareto-optimized models. Notably, on the large image dataset we tested on, namely NYUv2, the hypervolume convergence achieved by our method was found to be nearly two times faster than the next-best among the state-of-the-art.","sentences":["Multi-task learning solves multiple correlated tasks.","However, conflicts may exist between them.","In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs.","To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization.","Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems.","These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization.","A theorem proving faster convergence through the inclusion of such transfers is presented.","We investigate the proposed multi-task learning with multi-task optimization for solving various problem settings including image classification, scene understanding, and multi-target regression.","Comprehensive experiments confirm that the proposed method significantly advances the state-of-the-art in discovering sets of Pareto-optimized models.","Notably, on the large image dataset we tested on, namely NYUv2, the hypervolume convergence achieved by our method was found to be nearly two times faster than the next-best among the state-of-the-art."],"url":"http://arxiv.org/abs/2403.16162v1","category":"cs.AI"}
{"created":"2024-03-24 13:44:57","title":"One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation","abstract":"Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines. In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning. Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors. During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them. We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor faults. The masked model not only simplifies the overall FDIA pipeline, but also outperforms existing approaches. Our proposed technique has the potential to significantly improve the accuracy and reliability of sensor measurements in complex engineering systems in real-time, and could be applied to other types of sensors and engineering systems in the future. We believe that our proposed framework can contribute to the development of more efficient and effective FDIA techniques for a wide range of applications.","sentences":["Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines.","In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning.","Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors.","During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them.","We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor faults.","The masked model not only simplifies the overall FDIA pipeline, but also outperforms existing approaches.","Our proposed technique has the potential to significantly improve the accuracy and reliability of sensor measurements in complex engineering systems in real-time, and could be applied to other types of sensors and engineering systems in the future.","We believe that our proposed framework can contribute to the development of more efficient and effective FDIA techniques for a wide range of applications."],"url":"http://arxiv.org/abs/2403.16153v1","category":"cs.LG"}
{"created":"2024-03-24 13:43:43","title":"A Survey on Consumer IoT Traffic: Security and Privacy","abstract":"For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious traffic analysis, security analysis, and measurement. At last, we discuss the new challenges and future research directions.","sentences":["For the past few years, the Consumer Internet of Things (CIoT) has entered public lives.","While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns.","In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community.","From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved.","We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified.","Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious traffic analysis, security analysis, and measurement.","At last, we discuss the new challenges and future research directions."],"url":"http://arxiv.org/abs/2403.16149v1","category":"cs.CR"}
{"created":"2024-03-24 13:35:40","title":"Angular constraints on planar frameworks","abstract":"Consider a collection of points and the sets of slopes or directions of the lines between pairs of points. It is known that the algebraic matroid on this set of elements is the well studied 2-dimensional rigidity matroid. This article analyzes a construction on top of the set of slopes given by an angle constraint system of incidences and angles. In this setting we provide a matricial rigidity formulation of the problem for colored graphs, an algebro-geometric reformulation, precise necessary conditions and a combinatorial characterization of the generic behaviour for a special case.","sentences":["Consider a collection of points and the sets of slopes or directions of the lines between pairs of points.","It is known that the algebraic matroid on this set of elements is the well studied 2-dimensional rigidity matroid.","This article analyzes a construction on top of the set of slopes given by an angle constraint system of incidences and angles.","In this setting we provide a matricial rigidity formulation of the problem for colored graphs, an algebro-geometric reformulation, precise necessary conditions and a combinatorial characterization of the generic behaviour for a special case."],"url":"http://arxiv.org/abs/2403.16145v1","category":"math.CO"}
{"created":"2024-03-24 13:28:27","title":"What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?","abstract":"We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method. A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets. After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label. As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method.","sentences":["We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method.","A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets.","After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label.","As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method."],"url":"http://arxiv.org/abs/2403.16142v1","category":"cs.CL"}
{"created":"2024-03-24 13:06:05","title":"Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions","abstract":"In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previous surveys, this paper provides a more updated and comprehensive summary of the research, discusses future research directions, and contributes to the advancement of this field.","sentences":["In recent years, complementary recommendation has received extensive attention in the e-commerce domain.","In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024.","Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products.","Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start.","Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research.","Compared to previous surveys, this paper provides a more updated and comprehensive summary of the research, discusses future research directions, and contributes to the advancement of this field."],"url":"http://arxiv.org/abs/2403.16135v1","category":"cs.IR"}
{"created":"2024-03-24 13:03:49","title":"Enriching the physics program of the CMS experiment via data scouting and data parking","abstract":"Specialized data-taking and data-processing techniques were introduced by the CMS experiment in Run 1 of the CERN LHC to enhance the sensitivity of searches for new physics and the precision of standard model measurements. These techniques, termed data scouting and data parking, extend the data-taking capabilities of CMS beyond the original design specifications. The novel data-scouting strategy trades complete event information for higher event rates, while keeping the data bandwidth within limits. Data parking involves storing a large amount of raw detector data collected by algorithms with low trigger thresholds to be processed when sufficient computational power is available to handle such data. The research program of the CMS Collaboration is greatly expanded with these techniques. The implementation, performance, and physics results obtained with data scouting and data parking in CMS over the last decade are discussed in this Report, along with new developments aimed at further improving low-mass physics sensitivity over the next years of data taking.","sentences":["Specialized data-taking and data-processing techniques were introduced by the CMS experiment in Run 1 of the CERN LHC to enhance the sensitivity of searches for new physics and the precision of standard model measurements.","These techniques, termed data scouting and data parking, extend the data-taking capabilities of CMS beyond the original design specifications.","The novel data-scouting strategy trades complete event information for higher event rates, while keeping the data bandwidth within limits.","Data parking involves storing a large amount of raw detector data collected by algorithms with low trigger thresholds to be processed when sufficient computational power is available to handle such data.","The research program of the CMS Collaboration is greatly expanded with these techniques.","The implementation, performance, and physics results obtained with data scouting and data parking in CMS over the last decade are discussed in this Report, along with new developments aimed at further improving low-mass physics sensitivity over the next years of data taking."],"url":"http://arxiv.org/abs/2403.16134v1","category":"hep-ex"}
{"created":"2024-03-24 13:03:35","title":"SSHPool: The Separated Subgraph-based Hierarchical Pooling","abstract":"In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsulating rich intrinsic structural characteristics. Furthermore, we develop an end-to-end GNN framework associated with the proposed SSHPool module for graph classification. Experimental results demonstrate the superior performance of the proposed model on real-world datasets, significantly outperforming state-of-the-art GNN methods in terms of the classification accuracies.","sentences":["In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification.","To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs.","We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph.","Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs).","By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsulating rich intrinsic structural characteristics.","Furthermore, we develop an end-to-end GNN framework associated with the proposed SSHPool module for graph classification.","Experimental results demonstrate the superior performance of the proposed model on real-world datasets, significantly outperforming state-of-the-art GNN methods in terms of the classification accuracies."],"url":"http://arxiv.org/abs/2403.16133v1","category":"cs.AI"}
{"created":"2024-03-24 13:01:05","title":"AKBR: Learning Adaptive Kernel-based Representations for Graph Classification","abstract":"In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel matrix can be theoretically seen as the embedding vector of a sample graph, the proposed AKBR model is able to directly employ the resulting kernel matrix as the graph feature matrix and input it into the classifier for classification (i.e., the SoftMax layer), naturally providing an end-to-end learning architecture between the kernel computation as well as the classifier. Experimental results show that the proposed AKBR model outperforms existing state-of-the-art graph kernels and deep learning methods on standard graph benchmarks.","sentences":["In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification.","Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs.","To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs.","The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions.","Since each row of the resulting kernel matrix can be theoretically seen as the embedding vector of a sample graph, the proposed AKBR model is able to directly employ the resulting kernel matrix as the graph feature matrix and input it into the classifier for classification (i.e., the SoftMax layer), naturally providing an end-to-end learning architecture between the kernel computation as well as the classifier.","Experimental results show that the proposed AKBR model outperforms existing state-of-the-art graph kernels and deep learning methods on standard graph benchmarks."],"url":"http://arxiv.org/abs/2403.16130v1","category":"cs.LG"}
{"created":"2024-03-24 12:49:30","title":"WangchanLion and WangchanX MRC Eval","abstract":"This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improve our model in the future. Our code is public at https://github.com/vistec-AI/WangchanLion.","sentences":["This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language.","Our model is based on SEA-LION and a collection of instruction following datasets.","To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license.","To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad.","Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings.","In addition, our evaluation goes beyond the traditional MRC.","We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality.","Evaluation results provide insight into how we can improve our model in the future.","Our code is public at https://github.com/vistec-AI/WangchanLion."],"url":"http://arxiv.org/abs/2403.16127v1","category":"cs.CL"}
{"created":"2024-03-24 12:15:28","title":"Self-Supervised Multi-Frame Neural Scene Flow","abstract":"Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving. Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear. Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds. This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks. Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds. Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theoretical evaluation of its generalization abilities. Our analysis confirms that the proposed method maintains a limited generalization error, suggesting that adding multiple frames to the scene flow optimization process does not detract from its generalizability. Extensive experimental results on large-scale autonomous driving Waymo Open and Argoverse lidar datasets demonstrate that the proposed method achieves state-of-the-art performance.","sentences":["Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving.","Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear.","Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds.","This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks.","Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds.","Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theoretical evaluation of its generalization abilities.","Our analysis confirms that the proposed method maintains a limited generalization error, suggesting that adding multiple frames to the scene flow optimization process does not detract from its generalizability.","Extensive experimental results on large-scale autonomous driving Waymo Open and Argoverse lidar datasets demonstrate that the proposed method achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.16116v1","category":"cs.CV"}
{"created":"2024-03-24 12:05:23","title":"Opportunities and challenges in the application of large artificial intelligence models in radiology","abstract":"Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models. Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology. Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.","sentences":["Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development.","As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field.","This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models.","Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology.","Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography."],"url":"http://arxiv.org/abs/2403.16112v1","category":"cs.CV"}
{"created":"2024-03-24 11:52:39","title":"A Transformer approach for Electricity Price Forecasting","abstract":"This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.","sentences":["This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model.","As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism.","Hence, showing that the attention layer is enough for capturing the temporal patterns.","The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research.","The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation."],"url":"http://arxiv.org/abs/2403.16108v1","category":"cs.LG"}
{"created":"2024-03-24 11:50:49","title":"Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR","abstract":"Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants' abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing. Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR.","sentences":["Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR.","Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space.","However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform.","We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures.","Participants predominantly favored creating gestures with their hands.","The type of tasks and participants' abilities influence their choice of body parts for gesture design.","Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing.","Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR."],"url":"http://arxiv.org/abs/2403.16107v1","category":"cs.HC"}
{"created":"2024-03-24 11:49:53","title":"Viscoelastic material properties determine contact mechanics of hydrogel spheres","abstract":"Granular materials are ubiquitous in nature and industry; their mechanical behavior has been of academic and engineering interest for centuries. One of the reasons for their rather complex mechanical behavior is that stresses exerted on a granular material propagate only through contacts between the grains. These contacts can change as the packing evolves. This makes any deformation and mechanical response from a granular packing a function of the nature of contacts between the grains and the material response of the material the grains are made of. We present a study in which we isolate the role of the grain material in the contact forces acting between two particles sliding past each other. We use hydrogel particles and find that a viscoelastic material model, in which the shear modulus decays with time, coupled with a simple Coulomb friction model captures the experimental results. The results suggest that the particle material evolution itself may play a role in the collective behavior of granular materials.","sentences":["Granular materials are ubiquitous in nature and industry; their mechanical behavior has been of academic and engineering interest for centuries.","One of the reasons for their rather complex mechanical behavior is that stresses exerted on a granular material propagate only through contacts between the grains.","These contacts can change as the packing evolves.","This makes any deformation and mechanical response from a granular packing a function of the nature of contacts between the grains and the material response of the material the grains are made of.","We present a study in which we isolate the role of the grain material in the contact forces acting between two particles sliding past each other.","We use hydrogel particles and find that a viscoelastic material model, in which the shear modulus decays with time, coupled with a simple Coulomb friction model captures the experimental results.","The results suggest that the particle material evolution itself may play a role in the collective behavior of granular materials."],"url":"http://arxiv.org/abs/2403.16105v1","category":"cond-mat.soft"}
{"created":"2024-03-24 11:33:18","title":"Evaluating Fairness Metrics Across Borders from Human Perceptions","abstract":"Which fairness metrics are appropriately applicable in your contexts? There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics. Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness. However, these surveys were limited in scope, including only a few hundred participants within a single country. In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios. We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics. Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case. This investigation explores the relationship between personal attributes and the choice of fairness metrics, uncovering a significant influence of national context on these preferences.","sentences":["Which fairness metrics are appropriately applicable in your contexts?","There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics.","Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness.","However, these surveys were limited in scope, including only a few hundred participants within a single country.","In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios.","We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics.","Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case.","This investigation explores the relationship between personal attributes and the choice of fairness metrics, uncovering a significant influence of national context on these preferences."],"url":"http://arxiv.org/abs/2403.16101v1","category":"cs.AI"}
{"created":"2024-03-24 11:32:43","title":"Specifying Agent Ethics (Blue Sky Ideas)","abstract":"We consider the question of what properties a Machine Ethics system should have. This question is complicated by the existence of ethical dilemmas with no agreed upon solution. We provide an example to motivate why we do not believe falling back on the elicitation of values from stakeholders is sufficient to guarantee correctness of such systems. We go on to define two broad categories of ethical property that have arisen in our own work and present a challenge to the community to approach this question in a more systematic way.","sentences":["We consider the question of what properties a Machine Ethics system should have.","This question is complicated by the existence of ethical dilemmas with no agreed upon solution.","We provide an example to motivate why we do not believe falling back on the elicitation of values from stakeholders is sufficient to guarantee correctness of such systems.","We go on to define two broad categories of ethical property that have arisen in our own work and present a challenge to the community to approach this question in a more systematic way."],"url":"http://arxiv.org/abs/2403.16100v1","category":"cs.AI"}
{"created":"2024-03-24 11:29:55","title":"A Multi-Label Dataset of French Fake News: Human and Machine Insights","abstract":"We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press","sentences":["We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators.","By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers.","We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus.","We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News.","The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press"],"url":"http://arxiv.org/abs/2403.16099v1","category":"cs.CL"}
{"created":"2024-03-24 11:27:16","title":"Can Language Models Pretend Solvers? Logic Code Simulation with LLMs","abstract":"Transformer-based large language models (LLMs) have demonstrated significant potential in addressing logic problems. capitalizing on the great capabilities of LLMs for code-related activities, several frameworks leveraging logical solvers for logic reasoning have been proposed recently. While existing research predominantly focuses on viewing LLMs as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention. This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs. To further investigate this novel task, we formulate our three research questions: Can LLMs efficiently simulate the outputs of logic codes? What strength arises along with logic code simulation? And what pitfalls? To address these inquiries, we curate three novel datasets tailored for the logic code simulation task and undertake thorough experiments to establish the baseline performance of LLMs in code simulation. Subsequently, we introduce a pioneering LLM-based code simulation technique, Dual Chains of Logic (DCoL). This technique advocates a dual-path thinking approach for LLMs, which has demonstrated state-of-the-art performance compared to other LLM prompt strategies, achieving a notable improvement in accuracy by 7.06% with GPT-4-Turbo.","sentences":["Transformer-based large language models (LLMs) have demonstrated significant potential in addressing logic problems.","capitalizing on the great capabilities of LLMs for code-related activities, several frameworks leveraging logical solvers for logic reasoning have been proposed recently.","While existing research predominantly focuses on viewing LLMs as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention.","This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs.","To further investigate this novel task, we formulate our three research questions: Can LLMs efficiently simulate the outputs of logic codes?","What strength arises along with logic code simulation?","And what pitfalls?","To address these inquiries, we curate three novel datasets tailored for the logic code simulation task and undertake thorough experiments to establish the baseline performance of LLMs in code simulation.","Subsequently, we introduce a pioneering LLM-based code simulation technique, Dual Chains of Logic (DCoL).","This technique advocates a dual-path thinking approach for LLMs, which has demonstrated state-of-the-art performance compared to other LLM prompt strategies, achieving a notable improvement in accuracy by 7.06% with GPT-4-Turbo."],"url":"http://arxiv.org/abs/2403.16097v1","category":"cs.AI"}
{"created":"2024-03-24 10:07:46","title":"The Interplay of Learning, Analytics, and Artificial Intelligence in Education","abstract":"This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought processes, and the extension of human cognition via tightly integrated human-AI systems. Examples from current research and practice are examined as instances of the three conceptualisations, highlighting the potential value and limitations of each conceptualisation for education, as well as the perils of overemphasis on externalising human cognition as exemplified in today's hype surrounding generative AI tools. The paper concludes with an advocacy for a broader educational approach that includes educating people about AI and innovating educational systems to remain relevant in an AI enabled world.","sentences":["This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes.","Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI.","I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning.","Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection.","The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought processes, and the extension of human cognition via tightly integrated human-AI systems.","Examples from current research and practice are examined as instances of the three conceptualisations, highlighting the potential value and limitations of each conceptualisation for education, as well as the perils of overemphasis on externalising human cognition as exemplified in today's hype surrounding generative AI tools.","The paper concludes with an advocacy for a broader educational approach that includes educating people about AI and innovating educational systems to remain relevant in an AI enabled world."],"url":"http://arxiv.org/abs/2403.16081v1","category":"cs.CY"}
{"created":"2024-03-24 09:45:31","title":"The high-density regime of dusty plasma: Coulomb plasma","abstract":"It is shown that the dust density regimes in dusty plasma are characterized by two complementary screening processes, (a) the low dust density regime where the Debye screening is the dominant process and (b) the high dust density regime where the Coulomb screening is the dominant process. The Debye regime is characterized by a state where all dust particles carry an equal and constant charge. The high-density regime or the Coulomb plasma regime is characterized by (a) Coulomb screening where the dust charge depends on the spatial location and is screened by other dust particles in the vicinity by charge reduction, (b) quark like asymptotic freedom where dust particles, which on an average carry minimal electric charge (q tends to 0), are asymptotically free, (c) uniform dust charge density and plasma potential, (d) dust charge neutralization by a uniform background of hot ions. Thus, the Coulomb plasma is essentially a one-component plasma (OCP) with screening as opposed to electron plasma which is OCP without screening. Molecular dynamics (MD) simulations verify these properties. The MD simulations are performed, using a recently developed Hamiltonian formalism, to study the dynamics of Yukawa particles carrying variable electric charge. A hydrodynamic model for describing the collective properties of Coulomb plasma and its characteristic acoustic mode called the Coulomb acoustic wave is given.","sentences":["It is shown that the dust density regimes in dusty plasma are characterized by two complementary screening processes, (a) the low dust density regime where the Debye screening is the dominant process and (b) the high dust density regime where the Coulomb screening is the dominant process.","The Debye regime is characterized by a state where all dust particles carry an equal and constant charge.","The high-density regime or the Coulomb plasma regime is characterized by (a) Coulomb screening where the dust charge depends on the spatial location and is screened by other dust particles in the vicinity by charge reduction, (b) quark like asymptotic freedom where dust particles, which on an average carry minimal electric charge (q tends to 0), are asymptotically free, (c) uniform dust charge density and plasma potential, (d) dust charge neutralization by a uniform background of hot ions.","Thus, the Coulomb plasma is essentially a one-component plasma (OCP) with screening as opposed to electron plasma which is OCP without screening.","Molecular dynamics (MD) simulations verify these properties.","The MD simulations are performed, using a recently developed Hamiltonian formalism, to study the dynamics of Yukawa particles carrying variable electric charge.","A hydrodynamic model for describing the collective properties of Coulomb plasma and its characteristic acoustic mode called the Coulomb acoustic wave is given."],"url":"http://arxiv.org/abs/2403.16079v1","category":"physics.plasm-ph"}
{"created":"2024-03-24 09:26:53","title":"Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications","abstract":"Smart contracts are decentralized applications built atop blockchains like Ethereum. Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and justification are correct). This is likely because off-the-shelf LLMs were primarily pre-trained on a general text/code corpus and not fine-tuned on the specific domain of Solidity smart contract auditing.   In this paper, we propose TrustLLM, a general framework that combines fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications. Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause. As such, TrustLLM employs a two-stage fine-tuning approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities. However, fine-tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability. Therefore, we introduce two LLM-based agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine-tuned Reasoner model. To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM. We then compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b). On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%. The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes.","sentences":["Smart contracts are decentralized applications built atop blockchains like Ethereum.","Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and justification are correct).","This is likely because off-the-shelf LLMs were primarily pre-trained on a general text/code corpus and not fine-tuned on the specific domain of Solidity smart contract auditing.   ","In this paper, we propose TrustLLM, a general framework that combines fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.","Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause.","As such, TrustLLM employs a two-stage fine-tuning approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities.","However, fine-tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability.","Therefore, we introduce two LLM-based agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine-tuned Reasoner model.","To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM.","We then compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b).","On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%.","The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes."],"url":"http://arxiv.org/abs/2403.16073v1","category":"cs.SE"}
{"created":"2024-03-24 09:18:21","title":"Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization","abstract":"Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, a max-min mutual information regularization approach is proposed to capture speaker-insensitive latent representations. Experimental evaluations on public lip reading datasets demonstrate the effectiveness of the proposed approach under the intra-speaker and inter-speaker conditions.","sentences":["Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications.","Deep learning approaches greatly improve current lip reading systems.","However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability.","A well-trained lip reading system may perform poorly when handling a brand new speaker.","To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers.","In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics.","Furthermore, a max-min mutual information regularization approach is proposed to capture speaker-insensitive latent representations.","Experimental evaluations on public lip reading datasets demonstrate the effectiveness of the proposed approach under the intra-speaker and inter-speaker conditions."],"url":"http://arxiv.org/abs/2403.16071v1","category":"cs.AI"}
{"created":"2024-03-24 08:34:08","title":"Robust Diffusion Models for Adversarial Purification","abstract":"Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks. Extensive experiments are conducted to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks.","sentences":["Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT).","However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well.","Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy.","To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive.","We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs.","This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks.","Extensive experiments are conducted to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks."],"url":"http://arxiv.org/abs/2403.16067v1","category":"cs.CV"}
{"created":"2024-03-24 08:33:13","title":"A Temporal Graph Network Framework for Dynamic Recommendation","abstract":"Recommender systems, crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users' evolving preferences due to static data reliance. After Temporal Graph Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time. However, despite its promising capabilities, it has not been directly applied in recommender systems to date. Our study bridges this gap by directly implementing Temporal Graph Networks (TGN) in recommender systems, a first in this field. Using real-world datasets and a range of graph and history embedding methods, we show TGN's adaptability, confirming its effectiveness in dynamic recommendation scenarios.","sentences":["Recommender systems, crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users' evolving preferences due to static data reliance.","After Temporal Graph Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time.","However, despite its promising capabilities, it has not been directly applied in recommender systems to date.","Our study bridges this gap by directly implementing Temporal Graph Networks (TGN) in recommender systems, a first in this field.","Using real-world datasets and a range of graph and history embedding methods, we show TGN's adaptability, confirming its effectiveness in dynamic recommendation scenarios."],"url":"http://arxiv.org/abs/2403.16066v1","category":"cs.AI"}
{"created":"2024-03-24 08:22:36","title":"Holography inspired self-controlled reconfigurable intelligent surface","abstract":"Among various promising candidate technologies for the sixth-generation (6G) wireless communications, recent advances in microwave metasurfaces have sparked a new research area of reconfigurable intelligent surfaces (RISs). By controllably reprogramming the wireless propagation channel, RISs are envisioned to achieve low-cost wireless capacity boosting, coverage extension, and enhanced energy efficiency. To reprogram the channel, each meta-atom on RIS needs an external control signal, which is usually generated by base station (BS). However, BS-controlled RISs require complicated control cables, which hamper their massive deployments. Here, we eliminate the need for BS control by proposing a self-controlled RIS (SC-RIS), which is inspired by the optical holography principle. Different from the existing BS-controlled RISs, each meta-atom of SC-RIS is integrated with an additional power detector for holographic recording. By applying the classical Fourier-transform processing to the measured hologram, SC-RIS is capable of retrieving the user's channel state information required for beamforming, thus enabling autonomous RIS beamforming without control cables. Owing to this WiFi-like plug-and-play capability without the BS control, SC-RISs are expected to enable easy and massive deployments in the future 6G systems.","sentences":["Among various promising candidate technologies for the sixth-generation (6G) wireless communications, recent advances in microwave metasurfaces have sparked a new research area of reconfigurable intelligent surfaces (RISs).","By controllably reprogramming the wireless propagation channel, RISs are envisioned to achieve low-cost wireless capacity boosting, coverage extension, and enhanced energy efficiency.","To reprogram the channel, each meta-atom on RIS needs an external control signal, which is usually generated by base station (BS).","However, BS-controlled RISs require complicated control cables, which hamper their massive deployments.","Here, we eliminate the need for BS control by proposing a self-controlled RIS (SC-RIS), which is inspired by the optical holography principle.","Different from the existing BS-controlled RISs, each meta-atom of SC-RIS is integrated with an additional power detector for holographic recording.","By applying the classical Fourier-transform processing to the measured hologram, SC-RIS is capable of retrieving the user's channel state information required for beamforming, thus enabling autonomous RIS beamforming without control cables.","Owing to this WiFi-like plug-and-play capability without the BS control, SC-RISs are expected to enable easy and massive deployments in the future 6G systems."],"url":"http://arxiv.org/abs/2403.16062v1","category":"eess.SP"}
{"created":"2024-03-24 07:48:05","title":"Qibo: A Large Language Model for Traditional Chinese Medicine","abstract":"In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, which is a specialized tool for evaluating the performance of LLMs in the TCM domain. This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine. Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine.","sentences":["In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance.","However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources.","In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT).","Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, which is a specialized tool for evaluating the performance of LLMs in the TCM domain.","This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine.","Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine."],"url":"http://arxiv.org/abs/2403.16056v1","category":"cs.CL"}
{"created":"2024-03-24 07:04:08","title":"Semantic Is Enough: Only Semantic Information For NeRF Reconstruction","abstract":"Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels. This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component. We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach. We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model. Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation. The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding.","sentences":["Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels.","This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component.","We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach.","We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model.","Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation.","The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding."],"url":"http://arxiv.org/abs/2403.16043v1","category":"cs.CV"}
{"created":"2024-03-24 06:30:02","title":"V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception","abstract":"Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability. However, there are no real-world datasets to facilitate the real V2X cooperative perception research -- existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation. In this paper, we propose a dataset that has a mixture of multiple vehicles and smart infrastructure simultaneously to facilitate the V2X cooperative perception development with multi-modality sensing data. Our V2X-Real is collected using two connected automated vehicles and two smart infrastructures, which are all equipped with multi-modal sensors including LiDAR sensors and multi-view cameras. The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios. According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception. Comprehensive multi-class multi-agent benchmarks of SOTA cooperative perception methods are provided. The V2X-Real dataset and benchmark codes will be released.","sentences":["Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability.","However, there are no real-world datasets to facilitate the real V2X cooperative perception research -- existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation.","In this paper, we propose a dataset that has a mixture of multiple vehicles and smart infrastructure simultaneously to facilitate the V2X cooperative perception development with multi-modality sensing data.","Our V2X-Real is collected using two connected automated vehicles and two smart infrastructures, which are all equipped with multi-modal sensors including LiDAR sensors and multi-view cameras.","The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios.","According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception.","Comprehensive multi-class multi-agent benchmarks of SOTA cooperative perception methods are provided.","The V2X-Real dataset and benchmark codes will be released."],"url":"http://arxiv.org/abs/2403.16034v1","category":"cs.CV"}
{"created":"2024-03-24 06:21:35","title":"FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools","abstract":"The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development. Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools. Therefore, previous research efforts have explored learning-based methods to validate the reported warnings. Nevertheless, there are still some limitations. (1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level. These approaches lack specificity regarding individual bugs and warnings. (2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings. (3) The small number of selected projects hinders the comprehensive evaluation of these approaches. In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \\ourtool. Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention. To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA. It is ten times larger than the existing largest dataset. Then, we conducted extensive experiments on the dataset to evaluate FineWAVE. The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines.","sentences":["The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development.","Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools.","Therefore, previous research efforts have explored learning-based methods to validate the reported warnings.","Nevertheless, there are still some limitations.","(1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level.","These approaches lack specificity regarding individual bugs and warnings.","(2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings.","(3) The small number of selected projects hinders the comprehensive evaluation of these approaches.","In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \\ourtool.","Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention.","To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA.","It is ten times larger than the existing largest dataset.","Then, we conducted extensive experiments on the dataset to evaluate FineWAVE.","The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines."],"url":"http://arxiv.org/abs/2403.16032v1","category":"cs.SE"}
{"created":"2024-03-24 06:02:58","title":"Electromagnetic-Field-Based Circuit Theory and Charge-Flux-Flow Diagrams","abstract":"The conventional circuit diagrams and graph-based circuit theory are used for the phase-independent circuits such as resistor-inductor-capacitor (RLC) circuits and semiconductor transistor circuits, rather than the phase-dependent circuits such as Josephson junction circuits and quantum-phase-slip (QPS) junction circuits. in the age of artificial intelligence (AI), we present an electromagnetic-field-based circuit theory to unify the phase-independent and phase-dependent electric circuits. This theory drives two general system models for all electric circuits, and visualizes the dynamics of circuit devices with electric-charge-flow (ECF) diagrams and the magnetic-flux-flow (MFF) diagrams. ECF and MFF diagrams enable electric circuits to be designed and analyzed like the molecules composed of two kinds of atoms; they are promising for the language to train AI-aided electronic-design-automation (EDA) tools.","sentences":["The conventional circuit diagrams and graph-based circuit theory are used for the phase-independent circuits such as resistor-inductor-capacitor (RLC) circuits and semiconductor transistor circuits, rather than the phase-dependent circuits such as Josephson junction circuits and quantum-phase-slip (QPS) junction circuits.","in the age of artificial intelligence (AI), we present an electromagnetic-field-based circuit theory to unify the phase-independent and phase-dependent electric circuits.","This theory drives two general system models for all electric circuits, and visualizes the dynamics of circuit devices with electric-charge-flow (ECF) diagrams and the magnetic-flux-flow (MFF) diagrams.","ECF and MFF diagrams enable electric circuits to be designed and analyzed like the molecules composed of two kinds of atoms; they are promising for the language to train AI-aided electronic-design-automation (EDA) tools."],"url":"http://arxiv.org/abs/2403.16025v1","category":"physics.app-ph"}
{"created":"2024-03-24 05:55:39","title":"RPMArt: Towards Robust Perception and Manipulation for Articulated Objects","abstract":"Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects. Experimental results confirm our approach's effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments. The code and data will be open-sourced for reproduction. More results are published on the project website at https://r-pmart.github.io .","sentences":["Articulated objects are commonly found in daily life.","It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications.","However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios.","To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud.","Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting.","Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer.","Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects.","After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects.","Experimental results confirm our approach's effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments.","The code and data will be open-sourced for reproduction.","More results are published on the project website at https://r-pmart.github.io ."],"url":"http://arxiv.org/abs/2403.16023v1","category":"cs.RO"}
{"created":"2024-03-24 05:50:55","title":"Digital Twin Assisted Intelligent Network Management for Vehicular Applications","abstract":"The emerging data-driven methods based on artificial intelligence (AI) have paved the way for intelligent, flexible, and adaptive network management in vehicular applications. To enhance network management towards network automation, this article presents a digital twin (DT) assisted two-tier learning framework, which facilitates the automated life-cycle management of machine learning based intelligent network management functions (INMFs). Specifically, at a high tier, meta learning is employed to capture different levels of general features for the INMFs under nonstationary network conditions. At a low tier, individual learning models are customized for local networks based on fast model adaptation. Hierarchical DTs are deployed at the edge and cloud servers to assist the two-tier learning process, through closed-loop interactions with the physical network domain. Finally, a case study demonstrates the fast and accurate model adaptation ability of meta learning in comparison with benchmark schemes.","sentences":["The emerging data-driven methods based on artificial intelligence (AI) have paved the way for intelligent, flexible, and adaptive network management in vehicular applications.","To enhance network management towards network automation, this article presents a digital twin (DT) assisted two-tier learning framework, which facilitates the automated life-cycle management of machine learning based intelligent network management functions (INMFs).","Specifically, at a high tier, meta learning is employed to capture different levels of general features for the INMFs under nonstationary network conditions.","At a low tier, individual learning models are customized for local networks based on fast model adaptation.","Hierarchical DTs are deployed at the edge and cloud servers to assist the two-tier learning process, through closed-loop interactions with the physical network domain.","Finally, a case study demonstrates the fast and accurate model adaptation ability of meta learning in comparison with benchmark schemes."],"url":"http://arxiv.org/abs/2403.16021v1","category":"cs.NI"}
{"created":"2024-03-24 05:26:55","title":"Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)","abstract":"Image inpainting is the process of taking an image and generating lost or intentionally occluded portions. Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text. Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions. In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses. A critical gap in these existing models will be addressed, focusing on the ability to prompt and control what exactly is generated. We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality. Finally, we will evaluate the results of our approaches by qualitatively checking whether they generate high-quality images that correctly inpaint regions with the objects that they are instructed to produce.","sentences":["Image inpainting is the process of taking an image and generating lost or intentionally occluded portions.","Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text.","Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions.","In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses.","A critical gap in these existing models will be addressed, focusing on the ability to prompt and control what exactly is generated.","We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality.","Finally, we will evaluate the results of our approaches by qualitatively checking whether they generate high-quality images that correctly inpaint regions with the objects that they are instructed to produce."],"url":"http://arxiv.org/abs/2403.16016v1","category":"cs.CV"}
{"created":"2024-03-24 04:34:34","title":"CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering","abstract":"The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM.","sentences":["The recent advancements in artificial intelligence highlight the potential of language models in psychological health support.","While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques.","To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models.","Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies.","Unlike previous methods, our dataset emphasizes professional and structured response.","Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques.","Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality.","The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM."],"url":"http://arxiv.org/abs/2403.16008v1","category":"cs.CL"}
{"created":"2024-03-24 04:23:43","title":"A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures","abstract":"Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model. The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense experiments. The results show that FLGNN performs good robustness, and the success rate of privacy theft is further reduced by adding differential privacy defense methods.","sentences":["Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy.","However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario.","Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model.","The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets.","Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense experiments.","The results show that FLGNN performs good robustness, and the success rate of privacy theft is further reduced by adding differential privacy defense methods."],"url":"http://arxiv.org/abs/2403.16004v1","category":"cs.LG"}
{"created":"2024-03-24 04:22:37","title":"Diverse Representation Embedding for Lifelong Person Re-Identification","abstract":"Lifelong Person Re-Identification (LReID) aims to continuously learn from successive data streams, matching individuals across multiple cameras. The key challenge for LReID is how to effectively preserve old knowledge while learning new information incrementally. Task-level domain gaps and limited old task datasets are key factors leading to catastrophic forgetting in ReLD, which are overlooked in existing methods. To alleviate this problem, we propose a novel Diverse Representation Embedding (DRE) framework for LReID. The proposed DRE preserves old knowledge while adapting to new information based on instance-level and task-level layout. Concretely, an Adaptive Constraint Module (ACM) is proposed to implement integration and push away operations between multiple representations, obtaining dense embedding subspace for each instance to improve matching ability on limited old task datasets. Based on the processed diverse representation, we interact knowledge between the adjustment model and the learner model through Knowledge Update (KU) and Knowledge Preservation (KP) strategies at the task-level layout, which reduce the task-wise domain gap on both old and new tasks, and exploit diverse representation of each instance in limited datasets from old tasks, improving model performance for extended periods. Extensive experiments were conducted on eleven Re-ID datasets, including five seen datasets for training in order-1 and order-2 orders and six unseen datasets for inference. Compared to state-of-the-art methods, our method achieves significantly improved performance in holistic, large-scale, and occluded datasets.","sentences":["Lifelong Person Re-Identification (LReID) aims to continuously learn from successive data streams, matching individuals across multiple cameras.","The key challenge for LReID is how to effectively preserve old knowledge while learning new information incrementally.","Task-level domain gaps and limited old task datasets are key factors leading to catastrophic forgetting in ReLD, which are overlooked in existing methods.","To alleviate this problem, we propose a novel Diverse Representation Embedding (DRE) framework for LReID.","The proposed DRE preserves old knowledge while adapting to new information based on instance-level and task-level layout.","Concretely, an Adaptive Constraint Module (ACM) is proposed to implement integration and push away operations between multiple representations, obtaining dense embedding subspace for each instance to improve matching ability on limited old task datasets.","Based on the processed diverse representation, we interact knowledge between the adjustment model and the learner model through Knowledge Update (KU) and Knowledge Preservation (KP) strategies at the task-level layout, which reduce the task-wise domain gap on both old and new tasks, and exploit diverse representation of each instance in limited datasets from old tasks, improving model performance for extended periods.","Extensive experiments were conducted on eleven Re-ID datasets, including five seen datasets for training in order-1 and order-2 orders and six unseen datasets for inference.","Compared to state-of-the-art methods, our method achieves significantly improved performance in holistic, large-scale, and occluded datasets."],"url":"http://arxiv.org/abs/2403.16003v1","category":"cs.CV"}
{"created":"2024-03-24 03:10:39","title":"Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting","abstract":"Facial expression spotting is a significant but challenging task in facial expression analysis. The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions. In this paper, we propose a Multi-Scale Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression spotting. To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network. This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement. The subtle motion features are then converted to a facial graph representation, whose spatio-temporal graph patterns are learned by a graph convolutional network. This network learns both local and global features from multiple scales of facial graph structures using our proposed facial local graph pooling (FLGP). Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability of our model for difficult-to-classify frames. The experimental results on the SAMM-LV and CAS(ME)^2 datasets demonstrate that our method achieves state-of-the-art performance, particularly in micro-expression spotting. Ablation studies further verify the effectiveness of our proposed modules.","sentences":["Facial expression spotting is a significant but challenging task in facial expression analysis.","The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions.","In this paper, we propose a Multi-Scale Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression spotting.","To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network.","This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement.","The subtle motion features are then converted to a facial graph representation, whose spatio-temporal graph patterns are learned by a graph convolutional network.","This network learns both local and global features from multiple scales of facial graph structures using our proposed facial local graph pooling (FLGP).","Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability of our model for difficult-to-classify frames.","The experimental results on the SAMM-LV and CAS(ME)^2 datasets demonstrate that our method achieves state-of-the-art performance, particularly in micro-expression spotting.","Ablation studies further verify the effectiveness of our proposed modules."],"url":"http://arxiv.org/abs/2403.15994v1","category":"cs.CV"}
{"created":"2024-03-24 03:10:07","title":"BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval","abstract":"The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions. It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.","sentences":["The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals.","To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution.","While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets.","To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports.","Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder.","This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions.","It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks."],"url":"http://arxiv.org/abs/2403.15992v1","category":"cs.CV"}
{"created":"2024-03-24 02:54:46","title":"Knowledge-guided Machine Learning: Current Trends and Future Prospects","abstract":"This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.","sentences":["This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models.","It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results.","We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML.","We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category."],"url":"http://arxiv.org/abs/2403.15989v1","category":"cs.LG"}
{"created":"2024-03-24 02:15:14","title":"Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance Fields","abstract":"Accurate collection of plant phenotyping is critical to optimising sustainable farming practices in precision agriculture. Traditional phenotyping in controlled laboratory environments, while valuable, falls short in understanding plant growth under real-world conditions. Emerging sensor and digital technologies offer a promising approach for direct phenotyping of plants in farm environments. This study investigates a learning-based phenotyping method using the Neural Radiance Field to achieve accurate in-situ phenotyping of pepper plants in greenhouse environments. To quantitatively evaluate the performance of this method, traditional point cloud registration on 3D scanning data is implemented for comparison. Experimental result shows that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the 3D scanning methods. The mean distance error between the scanner-based method and the NeRF-based method is 0.865mm. This study shows that the learning-based NeRF method achieves similar accuracy to 3D scanning-based methods but with improved scalability and robustness.","sentences":["Accurate collection of plant phenotyping is critical to optimising sustainable farming practices in precision agriculture.","Traditional phenotyping in controlled laboratory environments, while valuable, falls short in understanding plant growth under real-world conditions.","Emerging sensor and digital technologies offer a promising approach for direct phenotyping of plants in farm environments.","This study investigates a learning-based phenotyping method using the Neural Radiance Field to achieve accurate in-situ phenotyping of pepper plants in greenhouse environments.","To quantitatively evaluate the performance of this method, traditional point cloud registration on 3D scanning data is implemented for comparison.","Experimental result shows that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the 3D scanning methods.","The mean distance error between the scanner-based method and the NeRF-based method is 0.865mm.","This study shows that the learning-based NeRF method achieves similar accuracy to 3D scanning-based methods but with improved scalability and robustness."],"url":"http://arxiv.org/abs/2403.15981v1","category":"cs.CV"}
{"created":"2024-03-24 01:20:08","title":"Towards Two-Stream Foveation-based Active Vision Learning","abstract":"Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both \"what object is being observed\" and \"where it is located\". In contrast, the \"two-stream hypothesis\" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the \"two-stream hypothesis\" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.","sentences":["Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both \"what object is being observed\" and \"where it is located\".","In contrast, the \"two-stream hypothesis\" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions.","In this work, we propose a machine learning framework inspired by the \"two-stream hypothesis\" and explore the potential benefits that it offers.","Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches.","The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model.","We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes.","The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box.","We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets."],"url":"http://arxiv.org/abs/2403.15977v1","category":"cs.CV"}
{"created":"2024-03-24 01:19:15","title":"Searches for CE\u03bdNS and Physics beyond the Standard Model using Skipper-CCDs at CONNIE","abstract":"The Coherent Neutrino-Nucleus Interaction Experiment (CONNIE) aims to detect the coherent scattering (CE$\\nu$NS) of reactor antineutrinos off silicon nuclei using thick fully-depleted high-resistivity silicon CCDs. Two Skipper-CCD sensors with sub-electron readout noise capability were installed at the experiment next to the Angra-2 reactor in 2021, making CONNIE the first experiment to employ Skipper-CCDs for reactor neutrino detection. We report on the performance of the Skipper-CCDs, the new data processing and data quality selection techniques and the event selection for CE$\\nu$NS interactions, which enable CONNIE to reach a record low detection threshold of 15 eV. The data were collected over 300 days in 2021-2022 and correspond to exposures of 14.9 g-days with the reactor-on and 3.5 g-days with the reactor-off. The difference between the reactor-on and off event rates shows no excess and yields upper limits at 95% confidence level for the neutrino interaction rates comparable with previous CONNIE limits from standard CCDs and higher exposures. Searches for new neutrino interactions beyond the Standard Model were performed, yielding an improvement on the previous CONNIE limit on a simplified model with light vector mediators. A first dark matter (DM) search by diurnal modulation was performed by CONNIE and the results represent the best limits on the DM-electron scattering cross-section, obtained by a surface-level experiment. These promising results, obtained using a very small-mass sensor, illustrate the potential of Skipper-CCDs to probe rare neutrino interactions and motivate the plans to increase the detector mass in the near future.","sentences":["The Coherent Neutrino-Nucleus Interaction Experiment (CONNIE) aims to detect the coherent scattering (CE$\\nu$NS) of reactor antineutrinos off silicon nuclei using thick fully-depleted high-resistivity silicon CCDs.","Two Skipper-CCD sensors with sub-electron readout noise capability were installed at the experiment next to the Angra-2 reactor in 2021, making CONNIE the first experiment to employ Skipper-CCDs for reactor neutrino detection.","We report on the performance of the Skipper-CCDs, the new data processing and data quality selection techniques and the event selection for CE$\\nu$NS interactions, which enable CONNIE to reach a record low detection threshold of 15 eV. The data were collected over 300 days in 2021-2022 and correspond to exposures of 14.9 g-days with the reactor-on and 3.5 g-days with the reactor-off.","The difference between the reactor-on and off event rates shows no excess and yields upper limits at 95% confidence level for the neutrino interaction rates comparable with previous CONNIE limits from standard CCDs and higher exposures.","Searches for new neutrino interactions beyond the Standard Model were performed, yielding an improvement on the previous CONNIE limit on a simplified model with light vector mediators.","A first dark matter (DM) search by diurnal modulation was performed by CONNIE and the results represent the best limits on the DM-electron scattering cross-section, obtained by a surface-level experiment.","These promising results, obtained using a very small-mass sensor, illustrate the potential of Skipper-CCDs to probe rare neutrino interactions and motivate the plans to increase the detector mass in the near future."],"url":"http://arxiv.org/abs/2403.15976v1","category":"hep-ex"}
{"created":"2024-03-24 00:46:40","title":"CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data","abstract":"This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains. Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data. For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold. We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image. We show that the CBGT-Net provides improved accuracy and robustness compared to models trained to classify from a single patch, and models leveraging an LSTM layer to classify from a fixed sequence length of patches.","sentences":["This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains.","Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data.","For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold.","We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image.","We show that the CBGT-Net provides improved accuracy and robustness compared to models trained to classify from a single patch, and models leveraging an LSTM layer to classify from a fixed sequence length of patches."],"url":"http://arxiv.org/abs/2403.15974v1","category":"cs.NE"}
{"created":"2024-03-23 23:49:01","title":"Detection of Problem Gambling with Less Features Using Machine Learning Methods","abstract":"Analytic features in gambling study are performed based on the amount of data monitoring on user daily actions. While performing the detection of problem gambling, existing datasets provide relatively rich analytic features for building machine learning based model. However, considering the complexity and cost of collecting the analytic features in real applications, conducting precise detection with less features will tremendously reduce the cost of data collection. In this study, we propose a deep neural networks PGN4 that performs well when using limited analytic features. Through the experiment on two datasets, we discover that PGN4 only experiences a mere performance drop when cutting 102 features to 5 features. Besides, we find the commonality within the top 5 features from two datasets.","sentences":["Analytic features in gambling study are performed based on the amount of data monitoring on user daily actions.","While performing the detection of problem gambling, existing datasets provide relatively rich analytic features for building machine learning based model.","However, considering the complexity and cost of collecting the analytic features in real applications, conducting precise detection with less features will tremendously reduce the cost of data collection.","In this study, we propose a deep neural networks PGN4 that performs well when using limited analytic features.","Through the experiment on two datasets, we discover that PGN4 only experiences a mere performance drop when cutting 102 features to 5 features.","Besides, we find the commonality within the top 5 features from two datasets."],"url":"http://arxiv.org/abs/2403.15962v1","category":"cs.LG"}
{"created":"2024-03-23 23:48:41","title":"SAT Encoding of Partial Ordering Models for Graph Coloring Problems","abstract":"In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal \"distance\" between the assigned colors, and the goal is to minimize the \"largest\" color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assignment-based model. Our practical evaluation confirms not only a dominance compared to the assignment-based encodings but also to the state-of-the-art approaches on a set of benchmark instances. Up to our knowledge, we have solved several open instances of the BCP from the literature for the first time.","sentences":["In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP).","The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors.","The BCP is a generalization, where each edge has a weight that enforces a minimal \"distance\" between the assigned colors, and the goal is to minimize the \"largest\" color used.","For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set.","Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances.","For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assignment-based model.","Our practical evaluation confirms not only a dominance compared to the assignment-based encodings but also to the state-of-the-art approaches on a set of benchmark instances.","Up to our knowledge, we have solved several open instances of the BCP from the literature for the first time."],"url":"http://arxiv.org/abs/2403.15961v1","category":"cs.AI"}
{"created":"2024-03-23 23:22:54","title":"Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection","abstract":"In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking methods. As invisible watermarks become increasingly prevalent, while specific decoding techniques remain undisclosed, our approach provides a versatile solution and establishes a path toward increasing accountability, transparency, and trust in our digital visual content.","sentences":["In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting.","WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques.","We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset.","Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5.","In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking methods.","As invisible watermarks become increasingly prevalent, while specific decoding techniques remain undisclosed, our approach provides a versatile solution and establishes a path toward increasing accountability, transparency, and trust in our digital visual content."],"url":"http://arxiv.org/abs/2403.15955v1","category":"cs.CV"}
{"created":"2024-03-23 23:14:37","title":"Understanding The Effectiveness of Lossy Compression in Machine Learning Training Sets","abstract":"Learning and Artificial Intelligence (ML/AI) techniques have become increasingly prevalent in high performance computing (HPC). However, these methods depend on vast volumes of floating point data for training and validation which need methods to share the data on a wide area network (WAN) or to transfer it from edge devices to data centers. Data compression can be a solution to these problems, but an in-depth understanding of how lossy compression affects model quality is needed. Prior work largely considers a single application or compression method. We designed a systematic methodology for evaluating data reduction techniques for ML/AI, and we use it to perform a very comprehensive evaluation with 17 data reduction methods on 7 ML/AI applications to show modern lossy compression methods can achieve a 50-100x compression ratio improvement for a 1% or less loss in quality. We identify critical insights that guide the future use and design of lossy compressors for ML/AI.","sentences":["Learning and Artificial Intelligence (ML/AI) techniques have become increasingly prevalent in high performance computing (HPC).","However, these methods depend on vast volumes of floating point data for training and validation which need methods to share the data on a wide area network (WAN) or to transfer it from edge devices to data centers.","Data compression can be a solution to these problems, but an in-depth understanding of how lossy compression affects model quality is needed.","Prior work largely considers a single application or compression method.","We designed a systematic methodology for evaluating data reduction techniques for ML/AI, and we use it to perform a very comprehensive evaluation with 17 data reduction methods on 7 ML/AI applications to show modern lossy compression methods can achieve a 50-100x compression ratio improvement for a 1% or less loss in quality.","We identify critical insights that guide the future use and design of lossy compressors for ML/AI."],"url":"http://arxiv.org/abs/2403.15953v1","category":"cs.LG"}
{"created":"2024-03-23 22:31:17","title":"Team Coordination on Graphs: Problem, Analysis, and Algorithms","abstract":"Team Coordination on Graphs with Risky Edges (TCGRE) is a recently emerged problem, in which a robot team collectively reduces graph traversal cost through support from one robot to another when the latter traverses a risky edge. Resembling the traditional Multi-Agent Path Finding (MAPF) problem, both classical and learning-based methods have been proposed to solve TCGRE, however, they lacked either computation efficiency or optimality assurance. In this paper, we reformulate TCGRE as a constrained optimization and perform rigorous mathematical analysis. Our theoretical analysis shows the NP-hardness of TCGRE by reduction from the Maximum 3D Matching problem and that efficient decomposition is a key to tackle this combinatorial optimization problem. Further more, we design three classes of algorithms to solve TCGRE, i.e., Joint State Graph (JSG) based, coordination based, and receding-horizon sub-team based solutions. Each of these proposed algorithms enjoy different provable optimality and efficiency characteristics that are demonstrated in our extensive experiments.","sentences":["Team Coordination on Graphs with Risky Edges (TCGRE) is a recently emerged problem, in which a robot team collectively reduces graph traversal cost through support from one robot to another when the latter traverses a risky edge.","Resembling the traditional Multi-Agent Path Finding (MAPF) problem, both classical and learning-based methods have been proposed to solve TCGRE, however, they lacked either computation efficiency or optimality assurance.","In this paper, we reformulate TCGRE as a constrained optimization and perform rigorous mathematical analysis.","Our theoretical analysis shows the NP-hardness of TCGRE by reduction from the Maximum 3D Matching problem and that efficient decomposition is a key to tackle this combinatorial optimization problem.","Further more, we design three classes of algorithms to solve TCGRE, i.e., Joint State Graph (JSG) based, coordination based, and receding-horizon sub-team based solutions.","Each of these proposed algorithms enjoy different provable optimality and efficiency characteristics that are demonstrated in our extensive experiments."],"url":"http://arxiv.org/abs/2403.15946v1","category":"cs.MA"}
{"created":"2024-03-23 22:14:38","title":"Adaptive Super Resolution For One-Shot Talking-Head Generation","abstract":"The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity. Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations. The code and demo video are available on: \\url{https://github.com/Songluchuan/AdaSR-TalkingHead/}.","sentences":["The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video.","Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation.","The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images.","Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution.","In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules.","Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity.","Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations.","The code and demo video are available on: \\url{https://github.com/Songluchuan/AdaSR-TalkingHead/}."],"url":"http://arxiv.org/abs/2403.15944v1","category":"cs.CV"}
{"created":"2024-03-23 22:04:03","title":"Explore until Confident: Efficient Exploration for Embodied Question Answering","abstract":"We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: https://explore-eqa.github.io/","sentences":["We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question.","In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions.","However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore.","We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration.","Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy.","To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D).","Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence.","Webpage with experiment videos and code: https://explore-eqa.github.io/"],"url":"http://arxiv.org/abs/2403.15941v1","category":"cs.RO"}
{"created":"2024-03-23 22:02:56","title":"Geotokens and Geotransformers","abstract":"In transformer architectures, position encoding primarily provides a sense of sequence for input tokens. While the original transformer paper's method has shown satisfactory results in general language processing tasks, there have been new proposals, such as Rotary Position Embedding (RoPE), for further improvement. This paper presents geotokens, input components for transformers, each linked to a specific geological location. Unlike typical language sequences, for these tokens, the order is not as vital as the geographical coordinates themselves. To represent the relative position in this context and to keep a balance between the real world distance and the distance in the embedding space, we design a position encoding approach drawing from the RoPE structure but tailored for spherical coordinates.","sentences":["In transformer architectures, position encoding primarily provides a sense of sequence for input tokens.","While the original transformer paper's method has shown satisfactory results in general language processing tasks, there have been new proposals, such as Rotary Position Embedding (RoPE), for further improvement.","This paper presents geotokens, input components for transformers, each linked to a specific geological location.","Unlike typical language sequences, for these tokens, the order is not as vital as the geographical coordinates themselves.","To represent the relative position in this context and to keep a balance between the real world distance and the distance in the embedding space, we design a position encoding approach drawing from the RoPE structure but tailored for spherical coordinates."],"url":"http://arxiv.org/abs/2403.15940v1","category":"cs.CL"}
{"created":"2024-03-23 21:54:34","title":"LlamBERT: Large-scale low-cost data annotation in NLP","abstract":"Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.","sentences":["Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks.","Despite their effectiveness, the high costs associated with their use pose a challenge.","We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa.","This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus.","Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness."],"url":"http://arxiv.org/abs/2403.15938v1","category":"cs.CL"}
{"created":"2024-03-23 21:16:56","title":"Understanding Domain-Size Generalization in Markov Logic Networks","abstract":"We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the variance of the MLN parameters, like regularization and Domain-Size Aware MLNs, increase the internal consistency of the MLNs. We empirically verify our results on four different datasets, with different methods to control parameter variance, showing that controlling parameter variance leads to better generalization.","sentences":["We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes.","Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes.","This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes.","In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters.","The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes.","We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes.","Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models.","Finally, we observe that solutions known to decrease the variance of the MLN parameters, like regularization and Domain-Size Aware MLNs, increase the internal consistency of the MLNs.","We empirically verify our results on four different datasets, with different methods to control parameter variance, showing that controlling parameter variance leads to better generalization."],"url":"http://arxiv.org/abs/2403.15933v1","category":"cs.AI"}
{"created":"2024-03-23 20:30:28","title":"X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention","abstract":"We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.","sentences":["We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation.","Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements.","As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet.","In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs.","The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions.","Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules.","Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics."],"url":"http://arxiv.org/abs/2403.15931v1","category":"cs.CV"}
{"created":"2024-03-23 19:13:01","title":"Multi-agent transformer-accelerated RL for satisfaction of STL specifications","abstract":"One of the main challenges in multi-agent reinforcement learning is scalability as the number of agents increases. This issue is further exacerbated if the problem considered is temporally dependent. State-of-the-art solutions today mainly follow centralized training with decentralized execution paradigm in order to handle the scalability concerns. In this paper, we propose time-dependent multi-agent transformers which can solve the temporally dependent multi-agent problem efficiently with a centralized approach via the use of transformers that proficiently handle the large input. We highlight the efficacy of this method on two problems and use tools from statistics to verify the probability that the trajectories generated under the policy satisfy the task. The experiments show that our approach has superior performance against the literature baseline algorithms in both cases.","sentences":["One of the main challenges in multi-agent reinforcement learning is scalability as the number of agents increases.","This issue is further exacerbated if the problem considered is temporally dependent.","State-of-the-art solutions today mainly follow centralized training with decentralized execution paradigm in order to handle the scalability concerns.","In this paper, we propose time-dependent multi-agent transformers which can solve the temporally dependent multi-agent problem efficiently with a centralized approach via the use of transformers that proficiently handle the large input.","We highlight the efficacy of this method on two problems and use tools from statistics to verify the probability that the trajectories generated under the policy satisfy the task.","The experiments show that our approach has superior performance against the literature baseline algorithms in both cases."],"url":"http://arxiv.org/abs/2403.15916v1","category":"cs.AI"}
{"created":"2024-03-23 19:02:42","title":"Michell Truss and From 1-beam to k-beam","abstract":"This paper generalizes the Michell Truss problem and Gangbo's paper from 1-dimension to higher dimensions using geometric measure theory.   Given an elastic surface $S$ made of $(k-1)$-beams under an equilibriated system $F$ of external forces, then we ask the following two questions:   1. What are the necessary and sufficient conditions for the existence of an elastic body made of $k$-beams whose forces on the surface balance $F$ and whose surfaces consist of $S$.   2. What is an optimal design so that the total cost is a minimum?   We've solved the existence question completely; and research is still in progress for the minimal question. In particular when $k=1$, it involves a system of beams joining a given finite collection of pointed forces. It was first introduced by A. Michell in 1904, then used in mechanical engineering, and recently popularized in many pure mathematics works by W. Gangbo, Prager, and others. Here we are going to generalize them to higher dimensional cases. We have already found the minimal solutions in terms of the flat chain complex and vector-valued currents. Right now we are studying the Calibration theory for future directions. I appreciate the discussion with Prof. Robert Hardt!","sentences":["This paper generalizes the Michell Truss problem and Gangbo's paper from 1-dimension to higher dimensions using geometric measure theory.   ","Given an elastic surface $S$ made of $(k-1)$-beams under an equilibriated system $F$ of external forces, then we ask the following two questions:   1.","What are the necessary and sufficient conditions for the existence of an elastic body made of $k$-beams whose forces on the surface balance $F$ and whose surfaces consist of $S$.   2.","What is an optimal design so that the total cost is a minimum?   ","We've solved the existence question completely; and research is still in progress for the minimal question.","In particular when $k=1$, it involves a system of beams joining a given finite collection of pointed forces.","It was first introduced by A. Michell in 1904, then used in mechanical engineering, and recently popularized in many pure mathematics works by W. Gangbo, Prager, and others.","Here we are going to generalize them to higher dimensional cases.","We have already found the minimal solutions in terms of the flat chain complex and vector-valued currents.","Right now we are studying the Calibration theory for future directions.","I appreciate the discussion with Prof. Robert Hardt!"],"url":"http://arxiv.org/abs/2403.15915v1","category":"math.OC"}
{"created":"2024-03-23 18:31:18","title":"Balancing art and money in pursuit of a Kelly-type optimality","abstract":"We introduce and study a mathematical model of an art collector. In our model, the collector is a rational agent whose actions in the art market are driven by two competing long-term objectives, namely sustainable financial health and maintaining the collection. Mathematically, our model is a two-dimensional random linear dynamical system with transformation matrix of a peculiar type. In some examples we are able to show that within the Kelly-type optimization paradigm, that is optimizing the system's Lyapunov exponent over a set of policy parameters, the dilemma ``art or money\" can be successfully resolved, namely the optimal policy creates a coexistence equilibrium where the value of both is increasing over the time.","sentences":["We introduce and study a mathematical model of an art collector.","In our model, the collector is a rational agent whose actions in the art market are driven by two competing long-term objectives, namely sustainable financial health and maintaining the collection.","Mathematically, our model is a two-dimensional random linear dynamical system with transformation matrix of a peculiar type.","In some examples we are able to show that within the Kelly-type optimization paradigm, that is optimizing the system's Lyapunov exponent over a set of policy parameters, the dilemma ``art or money\" can be successfully resolved, namely the optimal policy creates a coexistence equilibrium where the value of both is increasing over the time."],"url":"http://arxiv.org/abs/2403.15907v1","category":"math.PR"}
{"created":"2024-03-23 18:04:58","title":"MatchSeg: Towards Better Segmentation via Reference Image Matching","abstract":"Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental results demonstrate superior segmentation performance and powerful domain generalization ability of MatchSeg against existing methods for domain-specific and cross-domain segmentation tasks. Our code is made available at https://github.com/keeplearning-again/MatchSeg","sentences":["Recently, automated medical image segmentation methods based on deep learning have achieved great success.","However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire.","Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set.","Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching.","We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set.","Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets.","We validated our method across four public datasets.","Experimental results demonstrate superior segmentation performance and powerful domain generalization ability of MatchSeg against existing methods for domain-specific and cross-domain segmentation tasks.","Our code is made available at https://github.com/keeplearning-again/MatchSeg"],"url":"http://arxiv.org/abs/2403.15901v1","category":"cs.AI"}
{"created":"2024-03-23 16:51:52","title":"Leveraging Zero-Shot Prompting for Efficient Language Model Distillation","abstract":"This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact of explanation properties on distillation efficiency, demonstrating that minimal performance loss occurs even when rationale augmentation is not applied across the entire dataset, facilitating further reductions of tokens. This research marks a step toward the efficient training of task-specific models with minimal human intervention, offering substantial cost-savings while maintaining, or even enhancing, performance.","sentences":["This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor.","Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data.","Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions.","Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs.","Additionally, the paper investigates the impact of explanation properties on distillation efficiency, demonstrating that minimal performance loss occurs even when rationale augmentation is not applied across the entire dataset, facilitating further reductions of tokens.","This research marks a step toward the efficient training of task-specific models with minimal human intervention, offering substantial cost-savings while maintaining, or even enhancing, performance."],"url":"http://arxiv.org/abs/2403.15886v1","category":"cs.CL"}
{"created":"2024-03-23 16:12:52","title":"TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions","abstract":"Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability detection, SQL generation, and error detection, which are then integrated into a single pipeline; and 2) developing a unified approach that optimizes a single model to address the proposed task. Experimental results using our new reliability score show that addressing this challenge involves many different areas of research and opens new avenues for model development. Nonetheless, none of the methods surpass the reliability performance of the naive baseline, which abstains from answering all questions.","sentences":["Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries.","While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones.","To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings.","The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions.","For model evaluation, we explore various modeling approaches specifically designed for this task.","These include: 1) optimizing separate models for answerability detection, SQL generation, and error detection, which are then integrated into a single pipeline; and 2) developing a unified approach that optimizes a single model to address the proposed task.","Experimental results using our new reliability score show that addressing this challenge involves many different areas of research and opens new avenues for model development.","Nonetheless, none of the methods surpass the reliability performance of the naive baseline, which abstains from answering all questions."],"url":"http://arxiv.org/abs/2403.15879v1","category":"cs.AI"}
{"created":"2024-03-23 15:53:00","title":"Cognitive resilience: Unraveling the proficiency of image-captioning models to interpret masked visual content","abstract":"This study explores the ability of Image Captioning (IC) models to decode masked visual content sourced from diverse datasets. Our findings reveal the IC model's capability to generate captions from masked images, closely resembling the original content. Notably, even in the presence of masks, the model adeptly crafts descriptive textual information that goes beyond what is observable in the original image-generated captions. While the decoding performance of the IC model experiences a decline with an increase in the masked region's area, the model still performs well when important regions of the image are not masked at high coverage.","sentences":["This study explores the ability of Image Captioning (IC) models to decode masked visual content sourced from diverse datasets.","Our findings reveal the IC model's capability to generate captions from masked images, closely resembling the original content.","Notably, even in the presence of masks, the model adeptly crafts descriptive textual information that goes beyond what is observable in the original image-generated captions.","While the decoding performance of the IC model experiences a decline with an increase in the masked region's area, the model still performs well when important regions of the image are not masked at high coverage."],"url":"http://arxiv.org/abs/2403.15876v1","category":"cs.CV"}
{"created":"2024-03-23 15:52:37","title":"LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification","abstract":"This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER) framework, designed to systematically evaluate the adaptability of pre-trained language models (PLMs) in accommodating diverse prompts and their integration in zero-shot time series (TS) classification. We deploy LAMPER in experimental assessments using 128 univariate TS datasets sourced from the UCR archive. Our findings indicate that the feature representation capacity of LAMPER is influenced by the maximum input token threshold imposed by PLMs.","sentences":["This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER) framework, designed to systematically evaluate the adaptability of pre-trained language models (PLMs) in accommodating diverse prompts and their integration in zero-shot time series (TS) classification.","We deploy LAMPER in experimental assessments using 128 univariate TS datasets sourced from the UCR archive.","Our findings indicate that the feature representation capacity of LAMPER is influenced by the maximum input token threshold imposed by PLMs."],"url":"http://arxiv.org/abs/2403.15875v1","category":"cs.AI"}
{"created":"2024-03-23 15:43:30","title":"RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts","abstract":"Move structures have been studied in English for Specific Purposes (ESP) and English for Academic Purposes (EAP) for decades. However, there are few move annotation corpora for Research Article (RA) abstracts. In this paper, we introduce RAAMove, a comprehensive multi-domain corpus dedicated to the annotation of move structures in RA abstracts. The primary objective of RAAMove is to facilitate move analysis and automatic move identification. This paper provides a thorough discussion of the corpus construction process, including the scheme, data collection, annotation guidelines, and annotation procedures. The corpus is constructed through two stages: initially, expert annotators manually annotate high-quality data; subsequently, based on the human-annotated data, a BERT-based model is employed for automatic annotation with the help of experts' modification. The result is a large-scale and high-quality corpus comprising 33,988 annotated instances. We also conduct preliminary move identification experiments using the BERT-based model to verify the effectiveness of the proposed corpus and model. The annotated corpus is available for academic research purposes and can serve as essential resources for move analysis, English language teaching and writing, as well as move/discourse-related tasks in Natural Language Processing (NLP).","sentences":["Move structures have been studied in English for Specific Purposes (ESP) and English for Academic Purposes (EAP) for decades.","However, there are few move annotation corpora for Research Article (RA) abstracts.","In this paper, we introduce RAAMove, a comprehensive multi-domain corpus dedicated to the annotation of move structures in RA abstracts.","The primary objective of RAAMove is to facilitate move analysis and automatic move identification.","This paper provides a thorough discussion of the corpus construction process, including the scheme, data collection, annotation guidelines, and annotation procedures.","The corpus is constructed through two stages: initially, expert annotators manually annotate high-quality data; subsequently, based on the human-annotated data, a BERT-based model is employed for automatic annotation with the help of experts' modification.","The result is a large-scale and high-quality corpus comprising 33,988 annotated instances.","We also conduct preliminary move identification experiments using the BERT-based model to verify the effectiveness of the proposed corpus and model.","The annotated corpus is available for academic research purposes and can serve as essential resources for move analysis, English language teaching and writing, as well as move/discourse-related tasks in Natural Language Processing (NLP)."],"url":"http://arxiv.org/abs/2403.15872v1","category":"cs.CL"}
{"created":"2024-03-23 15:09:50","title":"Using Large Language Models for OntoClean-based Ontology Refinement","abstract":"This paper explores the integration of Large Language Models (LLMs) such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology. OntoClean, critical for assessing the metaphysical quality of ontologies, involves a two-step process of assigning meta-properties to classes and verifying a set of constraints. Manually conducting the first step proves difficult in practice, due to the need for philosophical expertise and lack of consensus among ontologists. By employing LLMs with two prompting strategies, the study demonstrates that high accuracy in the labelling process can be achieved. The findings suggest the potential for LLMs to enhance ontology refinement, proposing the development of plugin software for ontology tools to facilitate this integration.","sentences":["This paper explores the integration of Large Language Models (LLMs) such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology.","OntoClean, critical for assessing the metaphysical quality of ontologies, involves a two-step process of assigning meta-properties to classes and verifying a set of constraints.","Manually conducting the first step proves difficult in practice, due to the need for philosophical expertise and lack of consensus among ontologists.","By employing LLMs with two prompting strategies, the study demonstrates that high accuracy in the labelling process can be achieved.","The findings suggest the potential for LLMs to enhance ontology refinement, proposing the development of plugin software for ontology tools to facilitate this integration."],"url":"http://arxiv.org/abs/2403.15864v1","category":"cs.AI"}
{"created":"2024-03-23 14:57:34","title":"User Experience in Dataset Search Platform Interfaces","abstract":"This research investigates User Experience (UX) issues in dataset search platform interfaces, targeting Google Dataset Search and data.europa.eu. It focuses on 6 areas within UX: Initial Interaction, Search Process, Dataset Exploration, Filtering and Sorting, Dataset Actions, and Assistance and Feedback. The evaluation method combines 'The Pandemic Puzzle' user task, think-aloud methods, and demographic and post-task questionnaires. 29 strengths and 63 weaknesses were collected from 19 participants involved in roles within technology firm or academia. While certain insights are specific to particular platforms, most are derived from features commonly observed in dataset search platforms across a variety of fields, implying that our findings are broadly applicable. Observations from commonly found features in dataset search platforms across various fields have led to the development of 10 new design prototypes. Unlike literature retrieval, dataset retrieval involves a significant focus on metadata accessibility and quality, each element of which can impact decision-making. To address issues like reading fatigue from metadata presentation, inefficient methods for results searching, filtering, and selection, along with other unresolved user-centric issues on current platforms. These prototypes concentrate on enhancing metadata-related features. They include a redesigned homepage, an improved search bar, better sorting options, an enhanced search result display, a metadata comparison tool, and a navigation guide. Our aim is to improve usability for a wide range of users, including both developers and researchers.","sentences":["This research investigates User Experience (UX) issues in dataset search platform interfaces, targeting Google Dataset Search and data.europa.eu.","It focuses on 6 areas within UX: Initial Interaction, Search Process, Dataset Exploration, Filtering and Sorting, Dataset Actions, and Assistance and Feedback.","The evaluation method combines 'The Pandemic Puzzle' user task, think-aloud methods, and demographic and post-task questionnaires.","29 strengths and 63 weaknesses were collected from 19 participants involved in roles within technology firm or academia.","While certain insights are specific to particular platforms, most are derived from features commonly observed in dataset search platforms across a variety of fields, implying that our findings are broadly applicable.","Observations from commonly found features in dataset search platforms across various fields have led to the development of 10 new design prototypes.","Unlike literature retrieval, dataset retrieval involves a significant focus on metadata accessibility and quality, each element of which can impact decision-making.","To address issues like reading fatigue from metadata presentation, inefficient methods for results searching, filtering, and selection, along with other unresolved user-centric issues on current platforms.","These prototypes concentrate on enhancing metadata-related features.","They include a redesigned homepage, an improved search bar, better sorting options, an enhanced search result display, a metadata comparison tool, and a navigation guide.","Our aim is to improve usability for a wide range of users, including both developers and researchers."],"url":"http://arxiv.org/abs/2403.15861v1","category":"cs.HC"}
{"created":"2024-03-23 14:55:41","title":"Self-organised dynamics beyond scaling of avalanches: Cyclic stress fluctuations in critical sandpiles","abstract":"Recognising changes in collective dynamics in complex systems is essential for predicting potential events and their development. Possessing intrinsic attractors with laws associated with scale invariance, self-organised critical dynamics represent a suitable example for quantitatively studying changes in collective behaviour. We consider two prototypal models of self-organised criticality, the sandpile automata with deterministic (Bak-Tang-Wiesenfeld) and probabilistic (Manna model) dynamical rules, focusing on the nature of stress fluctuations induced by driving - adding grains during the avalanche propagation, and dissipation through avalanches that hit the system boundary. Our analysis of stress evolution time series reveals robust cycles modulated by collective fluctuations with dissipative avalanches. These modulated cycles are multifractal within a broad range of time scales. Features of the associated singularity spectra capture the differences in the dynamic rules behind the self-organised critical states and their response to the increased driving rate, altering the process stochasticity and causing a loss of avalanche scaling. In the related sequences of outflow current, the first return distributions are found to follow modified laws that describe different pathways to the gradual loss of cooperative behaviour in these two models. The spontaneous appearance of cycles is another characteristic of self-organised criticality. It can also help identify the prominence of self-organisational phenomenology in an empirical time series when underlying interactions and driving modes remain hidden.","sentences":["Recognising changes in collective dynamics in complex systems is essential for predicting potential events and their development.","Possessing intrinsic attractors with laws associated with scale invariance, self-organised critical dynamics represent a suitable example for quantitatively studying changes in collective behaviour.","We consider two prototypal models of self-organised criticality, the sandpile automata with deterministic (Bak-Tang-Wiesenfeld) and probabilistic (Manna model) dynamical rules, focusing on the nature of stress fluctuations induced by driving - adding grains during the avalanche propagation, and dissipation through avalanches that hit the system boundary.","Our analysis of stress evolution time series reveals robust cycles modulated by collective fluctuations with dissipative avalanches.","These modulated cycles are multifractal within a broad range of time scales.","Features of the associated singularity spectra capture the differences in the dynamic rules behind the self-organised critical states and their response to the increased driving rate, altering the process stochasticity and causing a loss of avalanche scaling.","In the related sequences of outflow current, the first return distributions are found to follow modified laws that describe different pathways to the gradual loss of cooperative behaviour in these two models.","The spontaneous appearance of cycles is another characteristic of self-organised criticality.","It can also help identify the prominence of self-organisational phenomenology in an empirical time series when underlying interactions and driving modes remain hidden."],"url":"http://arxiv.org/abs/2403.15859v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-23 14:47:26","title":"Automated System-level Testing of Unmanned Aerial Systems","abstract":"Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems. The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes. The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings. The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation. In this paper, we propose a novel approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test scenarios are generated on the fly, i.e., during test execution based on the environmental context at runtime. The approach is supported by a toolset. We empirically evaluate the proposed approach on two core components of UAS, an autopilot system of an unmanned aerial vehicle (UAV) and cockpit display systems (CDS) of the ground control station (GCS). The results show that the AITester effectively generates test scenarios causing deviations from the expected behavior of the UAV autopilot and reveals potential flaws in the GCS-CDS.","sentences":["Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical.","A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems.","The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes.","The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings.","The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation.","In this paper, we propose a novel approach to automate the system-level testing of the UAS.","The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios.","The test scenarios are generated on the fly, i.e., during test execution based on the environmental context at runtime.","The approach is supported by a toolset.","We empirically evaluate the proposed approach on two core components of UAS, an autopilot system of an unmanned aerial vehicle (UAV) and cockpit display systems (CDS) of the ground control station (GCS).","The results show that the AITester effectively generates test scenarios causing deviations from the expected behavior of the UAV autopilot and reveals potential flaws in the GCS-CDS."],"url":"http://arxiv.org/abs/2403.15857v1","category":"cs.SE"}
{"created":"2024-03-23 14:33:41","title":"#TeamFollowBack: Detection & Analysis of Follow Back Accounts on Social Media","abstract":"Follow back accounts inflate their follower counts by engaging in reciprocal followings. Such accounts manipulate the public and the algorithms by appearing more popular than they really are. Despite their potential harm, no studies have analyzed such accounts at scale. In this study, we present the first large-scale analysis of follow back accounts. We formally define follow back accounts and employ a honeypot approach to collect a dataset of such accounts on X (formerly Twitter). We discover and describe 12 communities of follow back accounts from 12 different countries, some of which exhibit clear political agenda. We analyze the characteristics of follow back accounts and report that they are newer, more engaging, and have more followings and followers. Finally, we propose a classifier for such accounts and report that models employing profile metadata and the ego network demonstrate promising results, although achieving high recall is challenging. Our study enhances understanding of the follow back accounts and discovering such accounts in the wild.","sentences":["Follow back accounts inflate their follower counts by engaging in reciprocal followings.","Such accounts manipulate the public and the algorithms by appearing more popular than they really are.","Despite their potential harm, no studies have analyzed such accounts at scale.","In this study, we present the first large-scale analysis of follow back accounts.","We formally define follow back accounts and employ a honeypot approach to collect a dataset of such accounts on X (formerly Twitter).","We discover and describe 12 communities of follow back accounts from 12 different countries, some of which exhibit clear political agenda.","We analyze the characteristics of follow back accounts and report that they are newer, more engaging, and have more followings and followers.","Finally, we propose a classifier for such accounts and report that models employing profile metadata and the ego network demonstrate promising results, although achieving high recall is challenging.","Our study enhances understanding of the follow back accounts and discovering such accounts in the wild."],"url":"http://arxiv.org/abs/2403.15856v1","category":"cs.SI"}
{"created":"2024-03-23 14:24:36","title":"Initialisation and Topology Effects in Decentralised Federated Learning","abstract":"Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics.","sentences":["Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised.","This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination.","Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices.","A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency.","Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy.","This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics."],"url":"http://arxiv.org/abs/2403.15855v1","category":"cs.LG"}
{"created":"2024-03-23 14:16:38","title":"A Modular Safety Filter for Safety-Certified Cyber-Physical Systems","abstract":"Nowadays, many control systems are networked and embed communication and computation capabilities. Such control architectures are prone to cyber attacks on the cyberinfrastructure. Consequently, there is an impellent need to develop solutions to preserve the plant's safety against potential attacks. To ensure safety, this paper introduces a modular safety filter approach that is effective for a variety of cyber-attack types. This solution can be implemented in combination with existing control and detection algorithms, effectively separating safety from performance. The safety filter does not require information on the reliability of the received command or the feature of the used anomaly detector. It can be implemented in conjunction with high-performance, resilient controllers, to achieve both high performance during normal operation and safety during an attack. As an illustrative example, we have shown the effectiveness of the proposed design considering a multi-agent formation task involving 20 mobile robots. The simulation results testify that the safety filter operates effectively during false data injection and intelligent attacks.","sentences":["Nowadays, many control systems are networked and embed communication and computation capabilities.","Such control architectures are prone to cyber attacks on the cyberinfrastructure.","Consequently, there is an impellent need to develop solutions to preserve the plant's safety against potential attacks.","To ensure safety, this paper introduces a modular safety filter approach that is effective for a variety of cyber-attack types.","This solution can be implemented in combination with existing control and detection algorithms, effectively separating safety from performance.","The safety filter does not require information on the reliability of the received command or the feature of the used anomaly detector.","It can be implemented in conjunction with high-performance, resilient controllers, to achieve both high performance during normal operation and safety during an attack.","As an illustrative example, we have shown the effectiveness of the proposed design considering a multi-agent formation task involving 20 mobile robots.","The simulation results testify that the safety filter operates effectively during false data injection and intelligent attacks."],"url":"http://arxiv.org/abs/2403.15854v1","category":"eess.SY"}
{"created":"2024-03-23 14:04:48","title":"When LLM-based Code Generation Meets the Software Development Process","abstract":"Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. Through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results indicate LCGScrum outperforms other models, achieving Pass@1 scores of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively - an average 15% improvement over GPT. Analysis reveals distinct impacts of development activities on generated code, with design and code reviews contributing to enhanced exception handling, while design, testing, and code reviews mitigate code smells. Furthermore, temperature values exhibit negligible influence on Pass@1 across all models. However, variations in Pass@1 are notable for different GPT3.5 model versions, ranging from 5 to over 60 in HumanEval, highlighting the stability of LCG across model versions. This stability underscores the importance of adopting software process models to bolster the quality and consistency of LLM-generated code.","sentences":["Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively.","This paper introduces LCG, a code generation framework inspired by established software engineering practices.","LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum.","Each model assigns LLM agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns.","Through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality.","Utilizing GPT3.5 as the underlying LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET.","Results indicate LCGScrum outperforms other models, achieving Pass@1 scores of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively - an average 15% improvement over GPT.","Analysis reveals distinct impacts of development activities on generated code, with design and code reviews contributing to enhanced exception handling, while design, testing, and code reviews mitigate code smells.","Furthermore, temperature values exhibit negligible influence on Pass@1 across all models.","However, variations in Pass@1 are notable for different GPT3.5 model versions, ranging from 5 to over 60 in HumanEval, highlighting the stability of LCG across model versions.","This stability underscores the importance of adopting software process models to bolster the quality and consistency of LLM-generated code."],"url":"http://arxiv.org/abs/2403.15852v1","category":"cs.SE"}
{"created":"2024-03-23 13:47:43","title":"Phylogenetic diversity indices from an affine and projective viewpoint","abstract":"Phylogenetic diversity indices are commonly used to rank the elements in a collection of species or populations for conservation purposes. The derivation of these indices is typically based on some quantitative description of the evolutionary history of the species in question, which is often given in terms of a phylogenetic tree. Both rooted and unrooted phylogenetic trees can be employed, and there are close connections between the indices that are derived in these two different ways. In this paper, we introduce more general phylogenetic diversity indices that can be derived from collections of subsets (clusters) and collections of bipartitions (splits) of the given set of species. Such indices could be useful, for example, in case there is some uncertainty in the topology of the tree being used to derive a phylogenetic diversity index. As well as characterizing some of the indices that we introduce in terms of their special properties, we provide a link between cluster-based and split-based phylogenetic diversity indices that uses a discrete analogue of the classical link between affine and projective geometry. This provides a unified framework for many of the various phylogenetic diversity indices used in the literature based on rooted and unrooted phylogenetic trees, generalizations and new proofs for previous results concerning tree-based indices, and a way to define some new phylogenetic diversity indices that naturally arise as affine or projective variants of each other.","sentences":["Phylogenetic diversity indices are commonly used to rank the elements in a collection of species or populations for conservation purposes.","The derivation of these indices is typically based on some quantitative description of the evolutionary history of the species in question, which is often given in terms of a phylogenetic tree.","Both rooted and unrooted phylogenetic trees can be employed, and there are close connections between the indices that are derived in these two different ways.","In this paper, we introduce more general phylogenetic diversity indices that can be derived from collections of subsets (clusters) and collections of bipartitions (splits) of the given set of species.","Such indices could be useful, for example, in case there is some uncertainty in the topology of the tree being used to derive a phylogenetic diversity index.","As well as characterizing some of the indices that we introduce in terms of their special properties, we provide a link between cluster-based and split-based phylogenetic diversity indices that uses a discrete analogue of the classical link between affine and projective geometry.","This provides a unified framework for many of the various phylogenetic diversity indices used in the literature based on rooted and unrooted phylogenetic trees, generalizations and new proofs for previous results concerning tree-based indices, and a way to define some new phylogenetic diversity indices that naturally arise as affine or projective variants of each other."],"url":"http://arxiv.org/abs/2403.15842v1","category":"q-bio.PE"}
{"created":"2024-03-23 13:21:09","title":"ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning","abstract":"Robotics learning highly relies on human expertise and efforts, such as demonstrations, design of reward functions in reinforcement learning, performance evaluation using human feedback, etc. However, reliance on human assistance can lead to expensive learning costs and make skill learning difficult to scale. In this work, we introduce the Large Language Model Supervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims to replace human participation in the robot skill learning process with large-scale language models that incorporate reward function design and performance evaluation. We provide evidence that our approach enables fully autonomous robot skill learning, capable of completing partial tasks without human intervention. Furthermore, we also analyze the limitations of this approach in task understanding and optimization stability.","sentences":["Robotics learning highly relies on human expertise and efforts, such as demonstrations, design of reward functions in reinforcement learning, performance evaluation using human feedback, etc.","However, reliance on human assistance can lead to expensive learning costs and make skill learning difficult to scale.","In this work, we introduce the Large Language Model Supervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims to replace human participation in the robot skill learning process with large-scale language models that incorporate reward function design and performance evaluation.","We provide evidence that our approach enables fully autonomous robot skill learning, capable of completing partial tasks without human intervention.","Furthermore, we also analyze the limitations of this approach in task understanding and optimization stability."],"url":"http://arxiv.org/abs/2403.15834v1","category":"cs.RO"}
{"created":"2024-03-23 12:53:51","title":"Scaling Learning based Policy Optimization for Temporal Tasks via Dropout","abstract":"This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agent's task objectives. This poses a challenge: RNNs are susceptible to vanishing and exploding gradients, and na\\\"{i}ve gradient descent-based strategies to solve long-horizon task objectives thus suffer from the same problems. To tackle this challenge, we introduce a novel gradient approximation algorithm based on the idea of dropout or gradient sampling. We show that, the existing smooth semantics for robustness are inefficient regarding gradient computation when the specification becomes complex. To address this challenge, we propose a new smooth semantics for DT-STL that under-approximates the robustness value and scales well for backpropagation over a complex specification. We show that our control synthesis methodology, can be quite helpful for stochastic gradient descent to converge with less numerical issues, enabling scalable backpropagation over long time horizons and trajectories over high dimensional state spaces.","sentences":["This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment.","We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL).","One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics.","In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula.","We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers.","We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agent's task objectives.","This poses a challenge: RNNs are susceptible to vanishing and exploding gradients, and na\\\"{i}ve gradient descent-based strategies to solve long-horizon task objectives thus suffer from the same problems.","To tackle this challenge, we introduce a novel gradient approximation algorithm based on the idea of dropout or gradient sampling.","We show that, the existing smooth semantics for robustness are inefficient regarding gradient computation when the specification becomes complex.","To address this challenge, we propose a new smooth semantics for DT-STL that under-approximates the robustness value and scales well for backpropagation over a complex specification.","We show that our control synthesis methodology, can be quite helpful for stochastic gradient descent to converge with less numerical issues, enabling scalable backpropagation over long time horizons and trajectories over high dimensional state spaces."],"url":"http://arxiv.org/abs/2403.15826v1","category":"eess.SY"}
{"created":"2024-03-23 12:33:12","title":"Carbon Intensity-Aware Adaptive Inference of DNNs","abstract":"DNN inference, known for its significant energy consumption and the resulting high carbon footprint, can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day. Our heuristic algorithm uses larger, high-accuracy models during low-intensity periods and smaller, lower-accuracy ones during high-intensity periods. We also introduce a metric, carbon-emission efficiency, which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint. The evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80%.","sentences":["DNN inference, known for its significant energy consumption and the resulting high carbon footprint, can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day.","Our heuristic algorithm uses larger, high-accuracy models during low-intensity periods and smaller, lower-accuracy ones during high-intensity periods.","We also introduce a metric, carbon-emission efficiency, which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint.","The evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80%."],"url":"http://arxiv.org/abs/2403.15824v1","category":"cs.LG"}
{"created":"2024-03-23 11:50:20","title":"The Impact of Evolutionary Computation on Robotic Design: A Case Study with an Underactuated Hand Exoskeleton","abstract":"Robotic exoskeletons can enhance human strength and aid people with physical disabilities. However, designing them to ensure safety and optimal performance presents significant challenges. Developing exoskeletons should incorporate specific optimization algorithms to find the best design. This study investigates the potential of Evolutionary Computation (EC) methods in robotic design optimization, with an underactuated hand exoskeleton (U-HEx) used as a case study. We propose improving the performance and usability of the U-HEx design, which was initially optimized using a naive brute-force approach, by integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch Algorithm. Comparative analysis revealed that EC methods consistently yield more precise and optimal solutions than brute force in a significantly shorter time. This allowed us to improve the optimization by increasing the number of variables in the design, which was impossible with naive methods. The results show significant improvements in terms of the torque magnitude the device transfers to the user, enhancing its efficiency. These findings underline the importance of performing proper optimization while designing exoskeletons, as well as providing a significant improvement to this specific robotic design.","sentences":["Robotic exoskeletons can enhance human strength and aid people with physical disabilities.","However, designing them to ensure safety and optimal performance presents significant challenges.","Developing exoskeletons should incorporate specific optimization algorithms to find the best design.","This study investigates the potential of Evolutionary Computation (EC) methods in robotic design optimization, with an underactuated hand exoskeleton (U-HEx) used as a case study.","We propose improving the performance and usability of the U-HEx design, which was initially optimized using a naive brute-force approach, by integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch Algorithm.","Comparative analysis revealed that EC methods consistently yield more precise and optimal solutions than brute force in a significantly shorter time.","This allowed us to improve the optimization by increasing the number of variables in the design, which was impossible with naive methods.","The results show significant improvements in terms of the torque magnitude the device transfers to the user, enhancing its efficiency.","These findings underline the importance of performing proper optimization while designing exoskeletons, as well as providing a significant improvement to this specific robotic design."],"url":"http://arxiv.org/abs/2403.15812v1","category":"cs.RO"}
{"created":"2024-03-23 11:34:17","title":"Efficient Data Access Paths for Mixed Vector-Relational Search","abstract":"The rapid growth of machine learning capabilities and the adoption of data processing methods using vector embeddings sparked a great interest in creating systems for vector data management. While the predominant approach of vector data management is to use specialized index structures for fast search over the entirety of the vector embeddings, once combined with other (meta)data, the search queries can also become selective on relational attributes - typical for analytical queries. As using vector indexes differs from traditional relational data access, we revisit and analyze alternative access paths for efficient mixed vector-relational search.   We first evaluate the accurate but exhaustive scan-based search and propose hardware optimizations and alternative tensor-based formulation and batching to offset the cost. We outline the complex access-path design space, primarily driven by relational selectivity, and the decisions to consider when selecting an exhaustive scan-based search against an approximate index-based approach. Since the vector index primarily avoids expensive computation across the entire dataset, contrary to the common relational knowledge, it is better to scan at lower selectivity and probe at higher, with a cross-point between the two approaches dictated by data dimensionality and the number of concurrent search queries.","sentences":["The rapid growth of machine learning capabilities and the adoption of data processing methods using vector embeddings sparked a great interest in creating systems for vector data management.","While the predominant approach of vector data management is to use specialized index structures for fast search over the entirety of the vector embeddings, once combined with other (meta)data, the search queries can also become selective on relational attributes - typical for analytical queries.","As using vector indexes differs from traditional relational data access, we revisit and analyze alternative access paths for efficient mixed vector-relational search.   ","We first evaluate the accurate but exhaustive scan-based search and propose hardware optimizations and alternative tensor-based formulation and batching to offset the cost.","We outline the complex access-path design space, primarily driven by relational selectivity, and the decisions to consider when selecting an exhaustive scan-based search against an approximate index-based approach.","Since the vector index primarily avoids expensive computation across the entire dataset, contrary to the common relational knowledge, it is better to scan at lower selectivity and probe at higher, with a cross-point between the two approaches dictated by data dimensionality and the number of concurrent search queries."],"url":"http://arxiv.org/abs/2403.15807v1","category":"cs.DB"}
{"created":"2024-03-23 11:09:41","title":"Vid2Real HRI: Align video-based HRI study designs with real-world settings","abstract":"HRI research using autonomous robots in real-world settings can produce results with the highest ecological validity of any study modality, but many difficulties limit such studies' feasibility and effectiveness. We propose Vid2Real HRI, a research framework to maximize real-world insights offered by video-based studies. The Vid2Real HRI framework was used to design an online study using first-person videos of robots as real-world encounter surrogates. The online study ($n = 385$) distinguished the within-subjects effects of four robot behavioral conditions on perceived social intelligence and human willingness to help the robot enter an exterior door. A real-world, between-subjects replication ($n = 26$) using two conditions confirmed the validity of the online study's findings and the sufficiency of the participant recruitment target ($22$) based on a power analysis of online study results. The Vid2Real HRI framework offers HRI researchers a principled way to take advantage of the efficiency of video-based study modalities while generating directly transferable knowledge of real-world HRI. Code and data from the study are provided at https://vid2real.github.io/vid2realHRI","sentences":["HRI research using autonomous robots in real-world settings can produce results with the highest ecological validity of any study modality, but many difficulties limit such studies' feasibility and effectiveness.","We propose Vid2Real HRI, a research framework to maximize real-world insights offered by video-based studies.","The Vid2Real HRI framework was used to design an online study using first-person videos of robots as real-world encounter surrogates.","The online study ($n = 385$) distinguished the within-subjects effects of four robot behavioral conditions on perceived social intelligence and human willingness to help the robot enter an exterior door.","A real-world, between-subjects replication ($n = 26$) using two conditions confirmed the validity of the online study's findings and the sufficiency of the participant recruitment target ($22$) based on a power analysis of online study results.","The Vid2Real HRI framework offers HRI researchers a principled way to take advantage of the efficiency of video-based study modalities while generating directly transferable knowledge of real-world HRI.","Code and data from the study are provided at https://vid2real.github.io/vid2realHRI"],"url":"http://arxiv.org/abs/2403.15798v1","category":"cs.RO"}
{"created":"2024-03-23 11:03:31","title":"Understanding Emergent Abilities of Language Models from the Loss Perspective","abstract":"Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.","sentences":["Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models.","This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities.","In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute.","We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks.","We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold.","Before reaching this threshold, its performance remains at the level of random guessing.","This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses."],"url":"http://arxiv.org/abs/2403.15796v1","category":"cs.CL"}
{"created":"2024-03-23 09:32:23","title":"A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services","abstract":"As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote. Through strategic rebalancing of vehicle distribution, our approach aims to maximize operator performance while simultaneously upholding fairness principles for users. In addition to theoretical insights, we substantiate our findings with a case study or simulation based on synthetic data, validating the efficacy of our approach. This paper underscores the critical importance of fairness considerations in shaping control strategies for Shared Micromobility Services, offering a pragmatic framework for enhancing equity in urban transportation systems.","sentences":["As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community.","On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited.","Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services.","Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach.","Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote.","Through strategic rebalancing of vehicle distribution, our approach aims to maximize operator performance while simultaneously upholding fairness principles for users.","In addition to theoretical insights, we substantiate our findings with a case study or simulation based on synthetic data, validating the efficacy of our approach.","This paper underscores the critical importance of fairness considerations in shaping control strategies for Shared Micromobility Services, offering a pragmatic framework for enhancing equity in urban transportation systems."],"url":"http://arxiv.org/abs/2403.15780v1","category":"eess.SY"}
{"created":"2024-03-23 09:26:15","title":"The Frontier of Data Erasure: Machine Unlearning for Large Language Models","abstract":"Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.","sentences":["Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation.","Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets.","Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data.","This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining.","It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy.","Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI."],"url":"http://arxiv.org/abs/2403.15779v1","category":"cs.AI"}
{"created":"2024-03-23 09:18:53","title":"Modeling Unified Semantic Discourse Structure for High-quality Headline Generation","abstract":"Headline generation aims to summarize a long document with a short, catchy title that reflects the main idea. This requires accurately capturing the core document semantics, which is challenging due to the lengthy and background information-rich na ture of the texts. In this work, We propose using a unified semantic discourse structure (S3) to represent document semantics, achieved by combining document-level rhetorical structure theory (RST) trees with sentence-level abstract meaning representation (AMR) graphs to construct S3 graphs. The hierarchical composition of sentence, clause, and word intrinsically characterizes the semantic meaning of the overall document. We then develop a headline generation framework, in which the S3 graphs are encoded as contextual features. To consolidate the efficacy of S3 graphs, we further devise a hierarchical structure pruning mechanism to dynamically screen the redundant and nonessential nodes within the graph. Experimental results on two headline generation datasets demonstrate that our method outperforms existing state-of-art methods consistently. Our work can be instructive for a broad range of document modeling tasks, more than headline or summarization generation.","sentences":["Headline generation aims to summarize a long document with a short, catchy title that reflects the main idea.","This requires accurately capturing the core document semantics, which is challenging due to the lengthy and background information-rich na ture of the texts.","In this work, We propose using a unified semantic discourse structure (S3) to represent document semantics, achieved by combining document-level rhetorical structure theory (RST) trees with sentence-level abstract meaning representation (AMR) graphs to construct S3 graphs.","The hierarchical composition of sentence, clause, and word intrinsically characterizes the semantic meaning of the overall document.","We then develop a headline generation framework, in which the S3 graphs are encoded as contextual features.","To consolidate the efficacy of S3 graphs, we further devise a hierarchical structure pruning mechanism to dynamically screen the redundant and nonessential nodes within the graph.","Experimental results on two headline generation datasets demonstrate that our method outperforms existing state-of-art methods consistently.","Our work can be instructive for a broad range of document modeling tasks, more than headline or summarization generation."],"url":"http://arxiv.org/abs/2403.15776v1","category":"cs.CL"}
{"created":"2024-03-23 08:54:03","title":"FusionINN: Invertible Image Fusion for Brain Tumor Monitoring","abstract":"Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as medical image fusion compared to other tasks like multi-focus or multi-exposure image fusion. Our extensive experimentation validates FusionINN over existing discriminative and generative fusion methods, both subjectively and objectively. Moreover, compared to a recent denoising diffusion-based fusion model, our approach offers faster and qualitatively better fusion results. We also exhibit the clinical utility of our results in aiding disease prognosis.","sentences":["Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image.","However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology.","We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process.","FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process.","To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as medical image fusion compared to other tasks like multi-focus or multi-exposure image fusion.","Our extensive experimentation validates FusionINN over existing discriminative and generative fusion methods, both subjectively and objectively.","Moreover, compared to a recent denoising diffusion-based fusion model, our approach offers faster and qualitatively better fusion results.","We also exhibit the clinical utility of our results in aiding disease prognosis."],"url":"http://arxiv.org/abs/2403.15769v1","category":"eess.IV"}
{"created":"2024-03-23 08:40:38","title":"BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion","abstract":"Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance. The performance improvement of bagging mainly relies on the number and diversity of base classifiers. However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset. Recently, diffusion models, which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity. We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network Diffusion (BEND). The originality of BEND comes from the first use of a neural network diffusion model to efficiently build base classifiers for bagging. Our approach is simple but effective, first using multiple trained model weights and biases as inputs to train autoencoder and latent diffusion model to realize a diffusion model from noise to valid neural network parameters. Subsequently, we generate several base classifiers using the trained diffusion model. Finally, we integrate these ba se classifiers for various inference tasks using the Bagging method. Resulting experiments on multiple models and datasets show that our proposed BEND algorithm can consistently outperform the mean and median accuracies of both the original trained model and the diffused model. At the same time, new models diffused using the diffusion model have higher diversity and lower cost than multiple models trained using traditional methods. The BEND approach successfully introduces diffusion models into the new deep learning training domain and provides a new paradigm for future deep learning training and inference.","sentences":["Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance.","The performance improvement of bagging mainly relies on the number and diversity of base classifiers.","However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset.","Recently, diffusion models, which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity.","We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network Diffusion (BEND).","The originality of BEND comes from the first use of a neural network diffusion model to efficiently build base classifiers for bagging.","Our approach is simple but effective, first using multiple trained model weights and biases as inputs to train autoencoder and latent diffusion model to realize a diffusion model from noise to valid neural network parameters.","Subsequently, we generate several base classifiers using the trained diffusion model.","Finally, we integrate these ba se classifiers for various inference tasks using the Bagging method.","Resulting experiments on multiple models and datasets show that our proposed BEND algorithm can consistently outperform the mean and median accuracies of both the original trained model and the diffused model.","At the same time, new models diffused using the diffusion model have higher diversity and lower cost than multiple models trained using traditional methods.","The BEND approach successfully introduces diffusion models into the new deep learning training domain and provides a new paradigm for future deep learning training and inference."],"url":"http://arxiv.org/abs/2403.15766v1","category":"cs.LG"}
{"created":"2024-03-23 08:40:35","title":"Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents","abstract":"Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification techniques. This approach aims to generate relation representations that are more aware of the spatial context and unseen relation in a manner similar to human perception. Experimental results demonstrate the effectiveness of our proposed method by showcasing its ability to outperform existing methods. This study also opens up new possibilities for practical applications.","sentences":["Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles.","These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets.","However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples.","To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs.","Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets.","Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification techniques.","This approach aims to generate relation representations that are more aware of the spatial context and unseen relation in a manner similar to human perception.","Experimental results demonstrate the effectiveness of our proposed method by showcasing its ability to outperform existing methods.","This study also opens up new possibilities for practical applications."],"url":"http://arxiv.org/abs/2403.15765v1","category":"cs.CV"}
{"created":"2024-03-23 08:24:09","title":"An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning","abstract":"Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server. With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover, our knowledge transfer scheme is applicable in scenarios with only one edge client. Code: https://github.com/TsingZ0/FedKTL","sentences":["Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy.","Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity.","To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL).","Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server.","With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task.","We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs.","Results show that our upload-efficient FedKTL surpasses seven state-of-the-art methods by up to 7.31% in accuracy.","Moreover, our knowledge transfer scheme is applicable in scenarios with only one edge client.","Code: https://github.com/TsingZ0/FedKTL"],"url":"http://arxiv.org/abs/2403.15760v1","category":"cs.AI"}
{"created":"2024-03-25 12:57:41","title":"The effect of inter-track coupling on H$_2$O$_2$ productions","abstract":"Background: Lower production of H$_2$O$_2$ in water is a hallmark of ultra-high dose rate (UHDR) compared to the conventional dose rate (CDR). However, the current computational models based on the predicted yield of H$_2$O$_2$ are in opposite of the experimental data. Methods: We construct an analytical model for the rate equation in the production of H$_2$O$_2$ from \\ce{^{.}OH}-radicals and use it as a guide to propose a hypothetical geometrical inhomogeneity in the configuration of particles in the FLASH-UHDR beams. We perform a series of Monte Carlo (MC) simulations of the track structures for a system of charged particles impinging the medium in the form of clusters and/or bunches. Results: We demonstrate the interplay of diffusion, reaction rates, and overlaps in track-spacing attribute to a lower yield of H$_2$O$_2$ at FLASH-UHDR vs. CDR. This trend is reversed if spacing among the tracks becomes larger than a critical value, with a length scale that is proportional to the diffusion length of \\ce{^{.}OH}-radicals modulated by a rate of decay due to recombination with other species, available within a track, and the space among the tracks. The latter is substantial on the suppressing of the H$_2$O$_2$ population at FLASH-UHDR relative to CDR. Conclusions: Based on our analysis of the present work, at FLASH-UHDR, the lower yield in H$_2$O$_2$ can be interpreted as a signature of bunching the particles in beams of ionizing radiation. The beams enter the medium in closely packed clusters and form inhomogeneities in the track-structure distribution. Thus the MC simulations based on the assumption of uniformly distributed tracks are unable to explain the experimental data.","sentences":["Background: Lower production of H$_2$O$_2$ in water is a hallmark of ultra-high dose rate (UHDR) compared to the conventional dose rate (CDR).","However, the current computational models based on the predicted yield of H$_2$O$_2$ are in opposite of the experimental data.","Methods: We construct an analytical model for the rate equation in the production of H$_2$O$_2$ from \\ce{^{.}OH}-radicals and use it as a guide to propose a hypothetical geometrical inhomogeneity in the configuration of particles in the FLASH-UHDR beams.","We perform a series of Monte Carlo (MC) simulations of the track structures for a system of charged particles impinging the medium in the form of clusters and/or bunches.","Results:","We demonstrate the interplay of diffusion, reaction rates, and overlaps in track-spacing attribute to a lower yield of H$_2$O$_2$ at FLASH-UHDR vs. CDR.","This trend is reversed if spacing among the tracks becomes larger than a critical value, with a length scale that is proportional to the diffusion length of \\ce{^{.}OH}-radicals modulated by a rate of decay due to recombination with other species, available within a track, and the space among the tracks.","The latter is substantial on the suppressing of the H$_2$O$_2$ population at FLASH-UHDR relative to CDR.","Conclusions: Based on our analysis of the present work, at FLASH-UHDR, the lower yield in H$_2$O$_2$ can be interpreted as a signature of bunching the particles in beams of ionizing radiation.","The beams enter the medium in closely packed clusters and form inhomogeneities in the track-structure distribution.","Thus the MC simulations based on the assumption of uniformly distributed tracks are unable to explain the experimental data."],"url":"http://arxiv.org/abs/2403.16722v1","category":"physics.med-ph"}
{"created":"2024-03-25 12:56:13","title":"Unveiling clean two-dimensional discrete time quasicrystals on a digital quantum computer","abstract":"In periodically driven (Floquet) systems, evolution typically results in an infinite-temperature thermal state due to continuous energy absorption over time. However, before reaching thermal equilibrium, such systems may transiently pass through a meta-stable state known as a prethermal state. This prethermal state can exhibit phenomena not commonly observed in equilibrium, such as discrete time crystals (DTCs), making it an intriguing platform for exploring out-of-equilibrium dynamics. Here, we investigate the relaxation dynamics of initially prepared product states under periodic driving in a kicked Ising model using the IBM Quantum Heron processor, comprising 133 superconducting qubits arranged on a heavy-hexagonal lattice, over up to $100$ time steps. We identify the presence of a prethermal regime characterised by magnetisation measurements oscillating at twice the period of the Floquet cycle and demonstrate its robustness against perturbations to the transverse field. Our results provide evidence supporting the realisation of a period-doubling DTC in a two-dimensional system. Moreover, we discover that the longitudinal field induces additional amplitude modulations in the magnetisation with a period incommensurate with the driving period, leading to the emergence of discrete time quasicrystals (DTQCs). These observations are further validated through comparison with tensor-network and state-vector simulations. Our findings not only enhance our understanding of clean DTCs in two dimensions but also highlight the utility of digital quantum computers for simulating the dynamics of quantum many-body systems, addressing challenges faced by state-of-the-art classical simulations.","sentences":["In periodically driven (Floquet) systems, evolution typically results in an infinite-temperature thermal state due to continuous energy absorption over time.","However, before reaching thermal equilibrium, such systems may transiently pass through a meta-stable state known as a prethermal state.","This prethermal state can exhibit phenomena not commonly observed in equilibrium, such as discrete time crystals (DTCs), making it an intriguing platform for exploring out-of-equilibrium dynamics.","Here, we investigate the relaxation dynamics of initially prepared product states under periodic driving in a kicked Ising model using the IBM Quantum Heron processor, comprising 133 superconducting qubits arranged on a heavy-hexagonal lattice, over up to $100$ time steps.","We identify the presence of a prethermal regime characterised by magnetisation measurements oscillating at twice the period of the Floquet cycle and demonstrate its robustness against perturbations to the transverse field.","Our results provide evidence supporting the realisation of a period-doubling DTC in a two-dimensional system.","Moreover, we discover that the longitudinal field induces additional amplitude modulations in the magnetisation with a period incommensurate with the driving period, leading to the emergence of discrete time quasicrystals (DTQCs).","These observations are further validated through comparison with tensor-network and state-vector simulations.","Our findings not only enhance our understanding of clean DTCs in two dimensions but also highlight the utility of digital quantum computers for simulating the dynamics of quantum many-body systems, addressing challenges faced by state-of-the-art classical simulations."],"url":"http://arxiv.org/abs/2403.16718v1","category":"quant-ph"}
{"created":"2024-03-25 12:49:09","title":"Predictable Interval MDPs through Entropy Regularization","abstract":"Regularization of control policies using entropy can be instrumental in adjusting predictability of real-world systems. Applications benefiting from such approaches range from, e.g., cybersecurity, which aims at maximal unpredictability, to human-robot interaction, where predictable behavior is highly desirable. In this paper, we consider entropy regularization for interval Markov decision processes (IMDPs). IMDPs are uncertain MDPs, where transition probabilities are only known to belong to intervals. Lately, IMDPs have gained significant popularity in the context of abstracting stochastic systems for control design. In this work, we address robust minimization of the linear combination of entropy and a standard cumulative cost in IMDPs, thereby establishing a trade-off between optimality and predictability. We show that optimal deterministic policies exist, and devise a value-iteration algorithm to compute them. The algorithm solves a number of convex programs at each step. Finally, through an illustrative example we show the benefits of penalizing entropy in IMDPs.","sentences":["Regularization of control policies using entropy can be instrumental in adjusting predictability of real-world systems.","Applications benefiting from such approaches range from, e.g., cybersecurity, which aims at maximal unpredictability, to human-robot interaction, where predictable behavior is highly desirable.","In this paper, we consider entropy regularization for interval Markov decision processes (IMDPs).","IMDPs are uncertain MDPs, where transition probabilities are only known to belong to intervals.","Lately, IMDPs have gained significant popularity in the context of abstracting stochastic systems for control design.","In this work, we address robust minimization of the linear combination of entropy and a standard cumulative cost in IMDPs, thereby establishing a trade-off between optimality and predictability.","We show that optimal deterministic policies exist, and devise a value-iteration algorithm to compute them.","The algorithm solves a number of convex programs at each step.","Finally, through an illustrative example we show the benefits of penalizing entropy in IMDPs."],"url":"http://arxiv.org/abs/2403.16711v1","category":"eess.SY"}
{"created":"2024-03-25 12:32:47","title":"Boson sampling enhanced quantum chemistry","abstract":"In this work, we give a new variational quantum algorithm for solving electronic structure problems of molecules using only linear quantum optical systems. The variational ansatz we proposed is a hybrid of non-interacting Boson dynamics and classical computational chemistry methods, specifically, the Hartree-Fock method and the Configuration Interaction method. The Boson part is built by a linear optical interferometer which is easier to realize compared with the well-known Unitary Coupled Cluster (UCC) ansatz composed of quantum gates in conventional VQE and the classical part is merely classical processing acting on the Hamiltonian. We called such ansatzes Boson Sampling-Classic (BS-C). The appearance of permanents in the Boson part has its physical intuition to provide different kinds of resources from commonly used single-, double-, and higher-excitations in classical methods and the UCC ansatz to exploring chemical quantum states. Such resources can help enhance the accuracy of methods used in the classical parts. We give a scalable hybrid homodyne and photon number measurement procedure for evaluating the energy value which has intrinsic abilities to mitigate photon loss errors and discuss the extra measurement cost induced by the no Pauli exclusion principle for Bosons with its solutions. To demonstrate our proposal, we run numerical experiments on several molecules and obtain their potential energy curves reaching chemical accuracy.","sentences":["In this work, we give a new variational quantum algorithm for solving electronic structure problems of molecules using only linear quantum optical systems.","The variational ansatz we proposed is a hybrid of non-interacting Boson dynamics and classical computational chemistry methods, specifically, the Hartree-Fock method and the Configuration Interaction method.","The Boson part is built by a linear optical interferometer which is easier to realize compared with the well-known Unitary Coupled Cluster (UCC) ansatz composed of quantum gates in conventional VQE and the classical part is merely classical processing acting on the Hamiltonian.","We called such ansatzes Boson Sampling-Classic (BS-C).","The appearance of permanents in the Boson part has its physical intuition to provide different kinds of resources from commonly used single-, double-, and higher-excitations in classical methods and the UCC ansatz to exploring chemical quantum states.","Such resources can help enhance the accuracy of methods used in the classical parts.","We give a scalable hybrid homodyne and photon number measurement procedure for evaluating the energy value which has intrinsic abilities to mitigate photon loss errors and discuss the extra measurement cost induced by the no Pauli exclusion principle for Bosons with its solutions.","To demonstrate our proposal, we run numerical experiments on several molecules and obtain their potential energy curves reaching chemical accuracy."],"url":"http://arxiv.org/abs/2403.16698v1","category":"quant-ph"}
{"created":"2024-03-25 12:27:24","title":"BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based Obstacle Avoidance","abstract":"Nano-drones, distinguished by their agility, minimal weight, and cost-effectiveness, are particularly well-suited for exploration in confined, cluttered and narrow spaces. Recognizing transparent, highly reflective or absorbing materials, such as glass and metallic surfaces is challenging, as classical sensors, such as cameras or laser rangers, often do not detect them. Inspired by bats, which can fly at high speeds in complete darkness with the help of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering sensor-deck employing a lightweight and low-power ultrasonic sensor for nano-drone autonomous navigation. This paper first provides insights about sensor characteristics, highlighting the influence of motor noise on the ultrasound readings, then it introduces the results of extensive experimental tests for obstacle avoidance (OA) in a diverse environment. Results show that \\textit{BatDeck} allows exploration for a flight time of 8 minutes while covering 136m on average before crash in a challenging environment with transparent and reflective obstacles, proving the effectiveness of ultrasonic sensors for OA on nano-drones.","sentences":["Nano-drones, distinguished by their agility, minimal weight, and cost-effectiveness, are particularly well-suited for exploration in confined, cluttered and narrow spaces.","Recognizing transparent, highly reflective or absorbing materials, such as glass and metallic surfaces is challenging, as classical sensors, such as cameras or laser rangers, often do not detect them.","Inspired by bats, which can fly at high speeds in complete darkness with the help of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering sensor-deck employing a lightweight and low-power ultrasonic sensor for nano-drone autonomous navigation.","This paper first provides insights about sensor characteristics, highlighting the influence of motor noise on the ultrasound readings, then it introduces the results of extensive experimental tests for obstacle avoidance (OA) in a diverse environment.","Results show that \\textit{BatDeck} allows exploration for a flight time of 8 minutes while covering 136m on average before crash in a challenging environment with transparent and reflective obstacles, proving the effectiveness of ultrasonic sensors for OA on nano-drones."],"url":"http://arxiv.org/abs/2403.16696v1","category":"cs.RO"}
{"created":"2024-03-25 12:26:18","title":"Design and Performance of Resonant Beam Communications -- Part II: Mobile Scenario","abstract":"This two-part paper focuses on the system design and performance analysis for a point-to-point resonant beam communication (RBCom) system under both the quasi-static and mobile scenarios. Part I of this paper proposes a synchronization-based information transmission scheme and derives the capacity upper and lower bounds for the quasi-static channel case. In Part II, we address the mobile scenario, where the receiver is in relative motion to the transmitter, and derive a mobile RBCom channel model that jointly considers the Doppler effect, channel variation, and echo interference. With the obtained channel model, we prove that the channel gain of the mobile RBCom decreases as the number of transmitted frames increases, and thus show that the considered mobile RBCom terminates after the transmitter sends a certain number of frames without frequency compensation. By deriving an upper bound on the number of successfully transmitted frames, we formulate the throughput maximization problem for the considered mobile RBCom system, and solve it via a sequential parametric convex approximation (SPCA) method. Finally, simulation results validate the analysis of our proposed method in some typical scenarios.","sentences":["This two-part paper focuses on the system design and performance analysis for a point-to-point resonant beam communication (RBCom) system under both the quasi-static and mobile scenarios.","Part I of this paper proposes a synchronization-based information transmission scheme and derives the capacity upper and lower bounds for the quasi-static channel case.","In Part II, we address the mobile scenario, where the receiver is in relative motion to the transmitter, and derive a mobile RBCom channel model that jointly considers the Doppler effect, channel variation, and echo interference.","With the obtained channel model, we prove that the channel gain of the mobile RBCom decreases as the number of transmitted frames increases, and thus show that the considered mobile RBCom terminates after the transmitter sends a certain number of frames without frequency compensation.","By deriving an upper bound on the number of successfully transmitted frames, we formulate the throughput maximization problem for the considered mobile RBCom system, and solve it via a sequential parametric convex approximation (SPCA) method.","Finally, simulation results validate the analysis of our proposed method in some typical scenarios."],"url":"http://arxiv.org/abs/2403.16694v1","category":"cs.IT"}
{"created":"2024-03-25 12:19:12","title":"Optimal Mass Transport of Nonlinear Systems under Input and Density Constraints","abstract":"We investigate optimal mass transport problem of affine-nonlinear dynamical systems with input and density constraints. Three algorithms are proposed to tackle this problem, including two Uzawa-type methods and a splitting algorithm based on the Douglas-Rachford algorithm. Some preliminary simulation results are presented to demonstrate the effectiveness of our approaches.","sentences":["We investigate optimal mass transport problem of affine-nonlinear dynamical systems with input and density constraints.","Three algorithms are proposed to tackle this problem, including two Uzawa-type methods and a splitting algorithm based on the Douglas-Rachford algorithm.","Some preliminary simulation results are presented to demonstrate the effectiveness of our approaches."],"url":"http://arxiv.org/abs/2403.16683v1","category":"math.OC"}
{"created":"2024-03-25 12:15:47","title":"Giant tunability of magnetoelasticity in Fe$_4$N system: Platform for unveiling correlation between magnetostriction and magnetic damping","abstract":"Flexible spintronics has opened new avenue to promising devices and applications in the field of wearable electronics. Particularly, miniaturized strain sensors exploiting the spintronic function have attracted considerable attention, in which the magnetoelasticity linking magnetism and lattice distortion is a vital property for high-sensitive detection of strain. This paper reports the demonstration that the magnetoelastic properties of Fe$_4$N can be significantly varied by partially replacing Fe with Co or Mn. The high quality Fe$_4$N film exhibits large negative magnetostriction along the [100] direction ($\\lambda_{100}$) of -121 ppm while Fe$_{3.2}$Co$_{0.8}$N shows $\\lambda_{100}$ of +46 ppm. This wide-range tunability of $\\lambda_{100}$ from -121 to +46 across 0 allows us to thoroughly examine the correlation between the magnetoelasticity and other magnetic properties. The strong correlation between $\\lambda_{100}$ and magnetic damping ($\\alpha$) is found. The enhanced extrinsic term of $\\alpha$ is attributable to the large two magnon scattering coming from the large magnetostriction. In addition, the density of states at the Fermi level plays a primal role to determine both $\\lambda_{100}$ and the intrinsic term of $\\alpha$. Thanks to the giant tunability and the bipolarity of magnetoelasticity, magnetic nitrides are candidate materials for high-sensitive spintronic strain sensors.","sentences":["Flexible spintronics has opened new avenue to promising devices and applications in the field of wearable electronics.","Particularly, miniaturized strain sensors exploiting the spintronic function have attracted considerable attention, in which the magnetoelasticity linking magnetism and lattice distortion is a vital property for high-sensitive detection of strain.","This paper reports the demonstration that the magnetoelastic properties of Fe$_4$N can be significantly varied by partially replacing Fe with Co or Mn.","The high quality Fe$_4$N film exhibits large negative magnetostriction along the [100] direction ($\\lambda_{100}$) of -121 ppm while Fe$_{3.2}$Co$_{0.8}$N shows $\\lambda_{100}$ of +46 ppm.","This wide-range tunability of $\\lambda_{100}$ from -121 to +46 across 0 allows us to thoroughly examine the correlation between the magnetoelasticity and other magnetic properties.","The strong correlation between $\\lambda_{100}$ and magnetic damping ($\\alpha$) is found.","The enhanced extrinsic term of $\\alpha$ is attributable to the large two magnon scattering coming from the large magnetostriction.","In addition, the density of states at the Fermi level plays a primal role to determine both $\\lambda_{100}$ and the intrinsic term of $\\alpha$. Thanks to the giant tunability and the bipolarity of magnetoelasticity, magnetic nitrides are candidate materials for high-sensitive spintronic strain sensors."],"url":"http://arxiv.org/abs/2403.16679v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-25 12:14:48","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression","abstract":"Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.","sentences":["Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation.","As constellation sizes increase, network contention poses a downlink bottleneck.","Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source.","However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   ","This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance.","FOOL partitions high-resolution satellite imagery to maximize throughput.","Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead.","While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at lower bitrates.","We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit.","Lastly, we test the feasibility of our system for standardized nanosatellite form factors.","We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks."],"url":"http://arxiv.org/abs/2403.16677v1","category":"cs.LG"}
{"created":"2024-03-25 12:11:28","title":"Quasi-randomization tests for network interference","abstract":"Many classical inferential approaches fail to hold when interference exists among the population units. This amounts to the treatment status of one unit affecting the potential outcome of other units in the population. Testing for such spillover effects in this setting makes the null hypothesis non-sharp. An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population. In randomized experiments, conditional randomized tests hold finite sample validity. Such approaches can pose computational challenges as finding these appropriate sub-populations based on experimental design can involve solving an NP-hard problem. In this paper, we view the network amongst the population as a random variable instead of being fixed. We propose a new approach that builds a conditional quasi-randomization test. Our main idea is to build the (non-sharp) null distribution of no spillover effects using random graph null models. We show that our method is exactly valid in finite-samples under mild assumptions. Our method displays enhanced power over other methods, with substantial improvement in complex experimental designs. We highlight that the method reduces to a simple permutation test, making it easy to implement in practice. We conduct a simulation study to verify the finite-sample validity of our approach and illustrate our methodology to test for interference in a weather insurance adoption experiment run in rural China.","sentences":["Many classical inferential approaches fail to hold when interference exists among the population units.","This amounts to the treatment status of one unit affecting the potential outcome of other units in the population.","Testing for such spillover effects in this setting makes the null hypothesis non-sharp.","An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population.","In randomized experiments, conditional randomized tests hold finite sample validity.","Such approaches can pose computational challenges as finding these appropriate sub-populations based on experimental design can involve solving an NP-hard problem.","In this paper, we view the network amongst the population as a random variable instead of being fixed.","We propose a new approach that builds a conditional quasi-randomization test.","Our main idea is to build the (non-sharp) null distribution of no spillover effects using random graph null models.","We show that our method is exactly valid in finite-samples under mild assumptions.","Our method displays enhanced power over other methods, with substantial improvement in complex experimental designs.","We highlight that the method reduces to a simple permutation test, making it easy to implement in practice.","We conduct a simulation study to verify the finite-sample validity of our approach and illustrate our methodology to test for interference in a weather insurance adoption experiment run in rural China."],"url":"http://arxiv.org/abs/2403.16673v1","category":"stat.ME"}
{"created":"2024-03-25 12:09:48","title":"Twisted conjugacy in dihedral Artin groups I: Torus Knot groups","abstract":"In this paper we provide an alternative solution to a result by Juh\\'{a}sz that the twisted conjugacy problem for odd dihedral Artin groups is solvable, that is, groups with presentation $G(m) = \\langle a,b \\; | \\; _{m}(a,b) = {}_{m}(b,a) \\rangle$, where $m\\geq 3$ is odd, and $_{m}(a,b)$ is the word $abab \\dots$ of length $m$, is solvable. Our solution provides an implementable linear time algorithm, by considering an alternative group presentation to that of a torus knot group, and working with geodesic normal forms. An application of this result is that the conjugacy problem is solvable in extensions of odd dihedral Artin groups.","sentences":["In this paper we provide an alternative solution to a result by Juh\\'{a}sz that the twisted conjugacy problem for odd dihedral Artin groups is solvable, that is, groups with presentation $G(m) = \\langle a,b \\; | \\; _{m}(a,b) = {}_{m}(b,a) \\rangle$, where $m\\geq 3$ is odd, and $_{m}(a,b)$ is the word $abab \\dots$ of length $m$, is solvable.","Our solution provides an implementable linear time algorithm, by considering an alternative group presentation to that of a torus knot group, and working with geodesic normal forms.","An application of this result is that the conjugacy problem is solvable in extensions of odd dihedral Artin groups."],"url":"http://arxiv.org/abs/2403.16671v1","category":"math.GR"}
{"created":"2024-03-25 11:57:30","title":"Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments","abstract":"This paper focuses on the acquisition of mapless navigation skills within unknown environments. We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism. Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge. Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments. Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models. Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments.","sentences":["This paper focuses on the acquisition of mapless navigation skills within unknown environments.","We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism.","Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge.","Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments.","Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models.","Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios.","Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments."],"url":"http://arxiv.org/abs/2403.16664v1","category":"cs.RO"}
{"created":"2024-03-25 11:53:00","title":"The Directionality of Gravitational and Thermal Diffusive Transport in Geologic Fluid Storage","abstract":"Diffusive transport has implications for the long-term status of underground storage of hydrogen (H$_2$) fuel and carbon dioxide (CO$_2$), technologies which are being pursued to mitigate climate change and advance the energy transition. Once injected underground, CO$_2$ and H$_2$ will exist in multiphase fluid-water-rock systems: being partially-soluble, injected fluids can flow through the porous rack in a connected plume, become disconnected and trapped as ganglia surrounded by groundwater within the storage rock pore space, and also dissolve and migrate through the aqueous phase. Recent analyses have focused on the concentration gradients induced by differing capillary pressure between fluid ganglia which can drive diffusive transport (\"Ostwald ripening\"). However, studies have neglected or excessively simplified important factors; namely: the non-ideality of gases under geologic conditions, the opposing equilibrium state of dissolved CO$_2$ and H$_2$ driven by the partial molar density of dissolved solutes, and entropic and thermodiffusive effects resulting from geothermal gradients. We conduct an analysis from thermodynamic first principles and use this to provide numerical estimates at conditions relevant to underground storage reservoirs. We show that entropic contributions to the free energy are so significant as to cause a reversal in the direction of diffusive transport in systems with geothermal gradients. For CO$_2$, even geothermal gradients less than 10 C/km induce downwards diffusion at depths relevant to storage. Diffusive transport of H$_2$ is less affected, but still reverses direction under typical gradients. Contrary to previous studies, we find that in diffusion and convection will likely work in concert - both driving CO$_2$ downwards, and both driving H$_2$ upwards - for conditions representative of their respective storage reservoirs.","sentences":["Diffusive transport has implications for the long-term status of underground storage of hydrogen (H$_2$) fuel and carbon dioxide (CO$_2$), technologies which are being pursued to mitigate climate change and advance the energy transition.","Once injected underground, CO$_2$ and H$_2$ will exist in multiphase fluid-water-rock systems: being partially-soluble, injected fluids can flow through the porous rack in a connected plume, become disconnected and trapped as ganglia surrounded by groundwater within the storage rock pore space, and also dissolve and migrate through the aqueous phase.","Recent analyses have focused on the concentration gradients induced by differing capillary pressure between fluid ganglia which can drive diffusive transport (\"Ostwald ripening\").","However, studies have neglected or excessively simplified important factors; namely: the non-ideality of gases under geologic conditions, the opposing equilibrium state of dissolved CO$_2$ and H$_2$ driven by the partial molar density of dissolved solutes, and entropic and thermodiffusive effects resulting from geothermal gradients.","We conduct an analysis from thermodynamic first principles and use this to provide numerical estimates at conditions relevant to underground storage reservoirs.","We show that entropic contributions to the free energy are so significant as to cause a reversal in the direction of diffusive transport in systems with geothermal gradients.","For CO$_2$, even geothermal gradients less than 10 C/km induce downwards diffusion at depths relevant to storage.","Diffusive transport of H$_2$ is less affected, but still reverses direction under typical gradients.","Contrary to previous studies, we find that in diffusion and convection will likely work in concert - both driving CO$_2$ downwards, and both driving H$_2$ upwards - for conditions representative of their respective storage reservoirs."],"url":"http://arxiv.org/abs/2403.16659v1","category":"physics.geo-ph"}
{"created":"2024-03-25 11:40:47","title":"Instantaneous Visual Analysis of Blood Flow in Stenoses Using Morphological Similarity","abstract":"The emergence of computational fluid dynamics (CFD) enabled the simulation of intricate transport processes, including flow in physiological structures, such as blood vessels. While these so-called hemodynamic simulations offer groundbreaking opportunities to solve problems at the clinical forefront, a successful translation of CFD to clinical decision-making is challenging. Hemodynamic simulations are intrinsically complex, time-consuming, and resource-intensive, which conflicts with the time-sensitive nature of clinical workflows and the fact that hospitals usually do not have the necessary resources or infrastructure to support CFD simulations. To address these transfer challenges, we propose a novel visualization system which enables instant flow exploration without performing on-site simulation. To gain insights into the viability of the approach, we focus on hemodynamic simulations of the carotid bifurcation, which is a highly relevant arterial subtree in stroke diagnostics and prevention. We created an initial database of 120 high-resolution carotid bifurcation flow models and developed a set of similarity metrics used to place a new carotid surface model into a neighborhood of simulated cases with the highest geometric similarity. The neighborhood can be immediately explored and the flow fields analyzed. We found that if the artery models are similar enough in the regions of interest, a new simulation leads to coinciding results, allowing the user to circumvent individual flow simulations. We conclude that similarity-based visual analysis is a promising approach toward the usability of CFD in medical practice.","sentences":["The emergence of computational fluid dynamics (CFD) enabled the simulation of intricate transport processes, including flow in physiological structures, such as blood vessels.","While these so-called hemodynamic simulations offer groundbreaking opportunities to solve problems at the clinical forefront, a successful translation of CFD to clinical decision-making is challenging.","Hemodynamic simulations are intrinsically complex, time-consuming, and resource-intensive, which conflicts with the time-sensitive nature of clinical workflows and the fact that hospitals usually do not have the necessary resources or infrastructure to support CFD simulations.","To address these transfer challenges, we propose a novel visualization system which enables instant flow exploration without performing on-site simulation.","To gain insights into the viability of the approach, we focus on hemodynamic simulations of the carotid bifurcation, which is a highly relevant arterial subtree in stroke diagnostics and prevention.","We created an initial database of 120 high-resolution carotid bifurcation flow models and developed a set of similarity metrics used to place a new carotid surface model into a neighborhood of simulated cases with the highest geometric similarity.","The neighborhood can be immediately explored and the flow fields analyzed.","We found that if the artery models are similar enough in the regions of interest, a new simulation leads to coinciding results, allowing the user to circumvent individual flow simulations.","We conclude that similarity-based visual analysis is a promising approach toward the usability of CFD in medical practice."],"url":"http://arxiv.org/abs/2403.16653v1","category":"physics.flu-dyn"}
{"created":"2024-03-25 11:40:32","title":"Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL","abstract":"This study is about the implementation of a reinforcement learning algorithm in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick and place the randomly placed block at a random target point in an unknown environment. The obstacle is randomly moving which creates a hurdle in picking the object. The objective of the robot is to avoid the obstacle and pick the block with constraints to a fixed timestamp. In this literature, we have applied a deep deterministic policy gradient (DDPG) algorithm and compared the model's efficiency with dense and sparse rewards.","sentences":["This study is about the implementation of a reinforcement learning algorithm in the trajectory planning of manipulators.","We have a 7-DOF robotic arm to pick and place the randomly placed block at a random target point in an unknown environment.","The obstacle is randomly moving which creates a hurdle in picking the object.","The objective of the robot is to avoid the obstacle and pick the block with constraints to a fixed timestamp.","In this literature, we have applied a deep deterministic policy gradient (DDPG) algorithm and compared the model's efficiency with dense and sparse rewards."],"url":"http://arxiv.org/abs/2403.16652v1","category":"cs.RO"}
{"created":"2024-03-25 11:35:52","title":"On the Korteweg-de Vries limit for the Boussinesq equation","abstract":"The Korteweg-de Vries (KdV) equation is known as a universal equation describing various long waves in dispersive systems. In this article, we prove that in a certain scaling regime, a large class of rough solutions to the Boussinesq equation are approximated by the sums of two counter-propagating waves solving the KdV equations. It extends the earlier result by \\cite{Schneider1998} to slightly more regular than $L^2$-solutions. Our proof is based on robust Fourier analysis methods developed for the low regularity theory of nonlinear dispersive equations.","sentences":["The Korteweg-de Vries (KdV) equation is known as a universal equation describing various long waves in dispersive systems.","In this article, we prove that in a certain scaling regime, a large class of rough solutions to the Boussinesq equation are approximated by the sums of two counter-propagating waves solving the KdV equations.","It extends the earlier result by \\cite{Schneider1998} to slightly more regular than $L^2$-solutions.","Our proof is based on robust Fourier analysis methods developed for the low regularity theory of nonlinear dispersive equations."],"url":"http://arxiv.org/abs/2403.16648v1","category":"math.AP"}
{"created":"2024-03-25 11:29:32","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","abstract":"We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.","sentences":["We present SIM-FSVGD for learning robot dynamics from data.","As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models.","While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available.","We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification.","We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system.","Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art."],"url":"http://arxiv.org/abs/2403.16644v1","category":"cs.RO"}
{"created":"2024-03-25 11:29:02","title":"Self-duel solution of 3D incompressible Navier-Stokes equations","abstract":"Whether the 3D incompressible Navier-Stokes equations will have a global smooth solution for all smooth, finite energy initial data is a Millennium Prize problem. One of the main difficulties of this problem is that the Navier-Stokes equations are actually a system of semilinear heat equations rather than a single equation. In this paper, we discover a remarkable hidden symmetry of the 3D incompressible Navier-Stokes equations. Under this symmetric reduction, the system reduces to a single scalar semilinear heat equation. The symmetry also holds for the 3D incompressible Euler equations.","sentences":["Whether the 3D incompressible Navier-Stokes equations will have a global smooth solution for all smooth, finite energy initial data is a Millennium Prize problem.","One of the main difficulties of this problem is that the Navier-Stokes equations are actually a system of semilinear heat equations rather than a single equation.","In this paper, we discover a remarkable hidden symmetry of the 3D incompressible Navier-Stokes equations.","Under this symmetric reduction, the system reduces to a single scalar semilinear heat equation.","The symmetry also holds for the 3D incompressible Euler equations."],"url":"http://arxiv.org/abs/2403.16642v1","category":"math.AP"}
{"created":"2024-03-25 11:25:52","title":"Formally Verifying the Safety of Pipelined Moonshot Consensus Protocol","abstract":"Decentralized Finance (DeFi) has emerged as a contemporary competitive as well as complementary to traditional centralized finance systems. As of 23rd January 2024, per Defillama approximately USD 55 billion is the total value locked on the DeFi applications on all blockchains put together.   A Byzantine Fault Tolerant (BFT) State Machine Replication (SMR) protocol, popularly known as the consensus protocol, is the central component of a blockchain. If forks are possible in a consensus protocol, they can be misused to carry out double spending attacks and can be catastrophic given high volumes of finance that are transacted on blockchains. Formal verification of the safety of consensus protocols is the golden standard for guaranteeing that forks are not possible. However, it is considered complex and challenging to do. This is reflected by the fact that not many complex consensus protocols are formally verified except for Tendermint and QBFT.   We focus on Supra's Pipelined Moonshot consensus protocol. Similar to Tendermint's formal verification, we too model Pipelined Moonshot using IVy and formally prove that for all network sizes, as long as the number of Byzantine validators is less than one thirds, the protocol does not allow forks, thus proving that Pipelined Moonshot is safe and double spending cannot be done using forks. The IVy model and proof of safety is available on Github.","sentences":["Decentralized Finance (DeFi) has emerged as a contemporary competitive as well as complementary to traditional centralized finance systems.","As of 23rd January 2024, per Defillama approximately USD 55 billion is the total value locked on the DeFi applications on all blockchains put together.   ","A Byzantine Fault Tolerant (BFT) State Machine Replication (SMR) protocol, popularly known as the consensus protocol, is the central component of a blockchain.","If forks are possible in a consensus protocol, they can be misused to carry out double spending attacks and can be catastrophic given high volumes of finance that are transacted on blockchains.","Formal verification of the safety of consensus protocols is the golden standard for guaranteeing that forks are not possible.","However, it is considered complex and challenging to do.","This is reflected by the fact that not many complex consensus protocols are formally verified except for Tendermint and QBFT.   ","We focus on Supra's Pipelined Moonshot consensus protocol.","Similar to Tendermint's formal verification, we too model Pipelined Moonshot using IVy and formally prove that for all network sizes, as long as the number of Byzantine validators is less than one thirds, the protocol does not allow forks, thus proving that Pipelined Moonshot is safe and double spending cannot be done using forks.","The IVy model and proof of safety is available on Github."],"url":"http://arxiv.org/abs/2403.16637v1","category":"cs.LO"}
{"created":"2024-03-25 11:19:11","title":"Orientation-Driven Large Magnetic Hysteresis of Er(III) Cyclooctatetraenide-Based Single-Ion Magnets Adsorbed on Ag(100)","abstract":"The molecular self-assembly and the magnetic properties of two cyclooctatetraenide (COT) - based single-ion magnets (SIM) adsorbed on Ag(100) in the sub-monolayer range are reported. Our study combines scanning-tunneling microscopy, X-ray photoemission spectroscopy and polarized X-ray absorption spectroscopy to show that Cp*ErCOT (Cp* = 1,2,3,4,5-pentamethylcyclopentadienide anion) SIMs self-assemble as alternating compact parallel rows including standing-up and lying-down conformations, following the main crystallographic directions of the substrate. Conversely, K[Er(COT)$_2$], obtained from subliming the [K(18-c-6)][Er(COT)$_2$]$\\cdot$ 2THF salt, forms uniaxially ordered domains with the (COT)$^{2-}$ rings perpendicular to the substrate plane. The polarization-dependent X-ray absorption spectra reproduced by the multiX simulations suggest that the strong in-plane magnetic anisotropy of K[Er(COT)$_2$]/Ag(100) and the weak out-of-plane anisotropy of Cp*ErCOT/Ag(100) can be attributed to the strikingly different surface ordering of these two complexes. Compared to the bulk phase, surface-supported K[Er(COT)$_2$] exhibits a similarly large hysteresis opening, while the Cp*ErCOT shows a rather small opening. This result reveals that despite structural similarities, the two organometallic SMMs have strongly different magnetic properties when adsorbed on the metal substrate, attributed to the different orientations and the resulting interactions of the ligand rings with the surface.","sentences":["The molecular self-assembly and the magnetic properties of two cyclooctatetraenide (COT) - based single-ion magnets (SIM) adsorbed on Ag(100) in the sub-monolayer range are reported.","Our study combines scanning-tunneling microscopy, X-ray photoemission spectroscopy and polarized X-ray absorption spectroscopy to show that Cp*ErCOT (Cp* = 1,2,3,4,5-pentamethylcyclopentadienide anion) SIMs self-assemble as alternating compact parallel rows including standing-up and lying-down conformations, following the main crystallographic directions of the substrate.","Conversely, K[Er(COT)$_2$], obtained from subliming the [K(18-c-6)][Er(COT)$_2$]$\\cdot$ 2THF salt, forms uniaxially ordered domains with the (COT)$^{2-}$ rings perpendicular to the substrate plane.","The polarization-dependent X-ray absorption spectra reproduced by the multiX simulations suggest that the strong in-plane magnetic anisotropy of K[Er(COT)$_2$]/Ag(100) and the weak out-of-plane anisotropy of Cp*ErCOT/Ag(100) can be attributed to the strikingly different surface ordering of these two complexes.","Compared to the bulk phase, surface-supported K[Er(COT)$_2$] exhibits a similarly large hysteresis opening, while the Cp*ErCOT shows a rather small opening.","This result reveals that despite structural similarities, the two organometallic SMMs have strongly different magnetic properties when adsorbed on the metal substrate, attributed to the different orientations and the resulting interactions of the ligand rings with the surface."],"url":"http://arxiv.org/abs/2403.16629v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-25 11:06:25","title":"Precise characterisation of HD 15337 with CHEOPS: a laboratory for planet formation and evolution","abstract":"We aim to constrain the internal structure and composition of HD 15337 b and c, two short-period planets situated on opposite sides of the radius valley, using new transit photometry and radial velocity data. We acquire 6 new transit visits with the CHaracterising ExOPlanet Satellite (CHEOPS) and 32 new radial velocity measurements from the High Accuracy Radial Velocity Planet Searcher (HARPS) to improve the accuracy of the mass and radius estimates for both planets. We reanalyse light curves from TESS sectors 3 and 4 and analyse new data from sector 30, correcting for long-term stellar activity. Subsequently, we perform a joint fit of the TESS and CHEOPS light curves, and all available RV data from HARPS and the Planet Finder Spectrograph (PFS). Our model fits the planetary signals, the stellar activity signal and the instrumental decorrelation model for the CHEOPS data simultaneously. The stellar activity was modelled using a Gaussian-process regression on both the RV and activity indicators. We finally employ a Bayesian retrieval code to determine the internal composition and structure of the planets. We derive updated and highly precise parameters for the HD 15337 system. Our improved precision on the planetary parameters makes HD 15337 b one of the most precisely characterised rocky exoplanets, with radius and mass measurements achieving a precision better than 2\\% and 7\\%, respectively. We are able to improve the precision of the radius measurement of HD 15337 c to 3\\%. Our results imply that the composition of HD 15337 b is predominantly rocky, while HD 15337 c exhibits a gas envelope with a mass of at least $0.01\\ M_\\oplus$.Our results lay the groundwork for future studies, which can further unravel the atmospheric evolution of these exoplanets and give new insights into their composition and formation history and the causes behind the radius gap.","sentences":["We aim to constrain the internal structure and composition of HD 15337 b and c, two short-period planets situated on opposite sides of the radius valley, using new transit photometry and radial velocity data.","We acquire 6 new transit visits with the CHaracterising ExOPlanet Satellite (CHEOPS) and 32 new radial velocity measurements from the High Accuracy Radial Velocity Planet Searcher (HARPS) to improve the accuracy of the mass and radius estimates for both planets.","We reanalyse light curves from TESS sectors 3 and 4 and analyse new data from sector 30, correcting for long-term stellar activity.","Subsequently, we perform a joint fit of the TESS and CHEOPS light curves, and all available RV data from HARPS and the Planet Finder Spectrograph (PFS).","Our model fits the planetary signals, the stellar activity signal and the instrumental decorrelation model for the CHEOPS data simultaneously.","The stellar activity was modelled using a Gaussian-process regression on both the RV and activity indicators.","We finally employ a Bayesian retrieval code to determine the internal composition and structure of the planets.","We derive updated and highly precise parameters for the HD 15337 system.","Our improved precision on the planetary parameters makes HD 15337 b one of the most precisely characterised rocky exoplanets, with radius and mass measurements achieving a precision better than 2\\% and 7\\%, respectively.","We are able to improve the precision of the radius measurement of HD 15337 c to 3\\%.","Our results imply that the composition of HD 15337 b is predominantly rocky, while HD 15337 c exhibits a gas envelope with a mass of at least $0.01\\ M_\\oplus$.Our results lay the groundwork for future studies, which can further unravel the atmospheric evolution of these exoplanets and give new insights into their composition and formation history and the causes behind the radius gap."],"url":"http://arxiv.org/abs/2403.16621v1","category":"astro-ph.EP"}
{"created":"2024-03-25 11:06:11","title":"Calibration of detector time constant with a thermal source for the POLARBEAR-2A CMB polarization experiment","abstract":"The Simons Array (SA) project is a ground-based Cosmic Microwave Background (CMB) polarization experiment. The SA observes the sky using three telescopes, and POLARBEAR-2A (PB-2A) is the receiver system on the first telescope. For the ground-based experiment, atmospheric fluctuation is the primary noise source that could cause polarization leakage. In the PB-2A receiver system, a continuously rotating half-wave plate (HWP) is used to mitigate the polarization leakage. However, due to the rapid modulation of the polarization signal, the uncertainty in the time constant of the detector results in an uncertainty in the polarization angle. For PB-2A, the time constant of each bolometer needs to be calibrated at the sub-millisecond level to avoid introducing bias to the polarization signal. We have developed a new calibrator system that can be used to calibrate the time constants of the detectors. In this study, we present the design of the calibration system and the preliminary results of the time constant calibration for PB-2A.","sentences":["The Simons Array (SA) project is a ground-based Cosmic Microwave Background (CMB) polarization experiment.","The SA observes the sky using three telescopes, and POLARBEAR-2A (PB-2A) is the receiver system on the first telescope.","For the ground-based experiment, atmospheric fluctuation is the primary noise source that could cause polarization leakage.","In the PB-2A receiver system, a continuously rotating half-wave plate (HWP) is used to mitigate the polarization leakage.","However, due to the rapid modulation of the polarization signal, the uncertainty in the time constant of the detector results in an uncertainty in the polarization angle.","For PB-2A, the time constant of each bolometer needs to be calibrated at the sub-millisecond level to avoid introducing bias to the polarization signal.","We have developed a new calibrator system that can be used to calibrate the time constants of the detectors.","In this study, we present the design of the calibration system and the preliminary results of the time constant calibration for PB-2A."],"url":"http://arxiv.org/abs/2403.16620v1","category":"astro-ph.IM"}
{"created":"2024-03-25 10:51:30","title":"Guided Bayesian Optimization: Data-Efficient Controller Tuning with Digital Twin","abstract":"This article presents the guided Bayesian optimization algorithm as an efficient data-driven method for iteratively tuning closed-loop controller parameters using an event-triggered digital twin of the system based on available closed-loop data. We define a controller tuning framework independent of the controller or the plant structure. Our proposed methodology is model-free, making it suitable for nonlinear and unmodelled plants with measurement noise. The objective function consists of performance metrics modeled by Gaussian processes. We utilize the available information in the closed-loop system to identify and progressively maintain a digital twin that guides the optimizer, improving the data efficiency of our method. Switching the digital twin on and off is triggered by data-driven criteria related to the digital twin's uncertainty estimations in the BO tuning framework. Effectively, it replaces much of the exploration of the real system with exploration performed on the digital twin. We analyze the properties of our method in simulation and demonstrate its performance on two real closed-loop systems with different plant and controller structures. The experimental results show that our method requires fewer experiments on the physical plant than Bayesian optimization to find the optimal controller parameters.","sentences":["This article presents the guided Bayesian optimization algorithm as an efficient data-driven method for iteratively tuning closed-loop controller parameters using an event-triggered digital twin of the system based on available closed-loop data.","We define a controller tuning framework independent of the controller or the plant structure.","Our proposed methodology is model-free, making it suitable for nonlinear and unmodelled plants with measurement noise.","The objective function consists of performance metrics modeled by Gaussian processes.","We utilize the available information in the closed-loop system to identify and progressively maintain a digital twin that guides the optimizer, improving the data efficiency of our method.","Switching the digital twin on and off is triggered by data-driven criteria related to the digital twin's uncertainty estimations in the BO tuning framework.","Effectively, it replaces much of the exploration of the real system with exploration performed on the digital twin.","We analyze the properties of our method in simulation and demonstrate its performance on two real closed-loop systems with different plant and controller structures.","The experimental results show that our method requires fewer experiments on the physical plant than Bayesian optimization to find the optimal controller parameters."],"url":"http://arxiv.org/abs/2403.16619v1","category":"eess.SY"}
{"created":"2024-03-25 10:40:04","title":"Distributed collaborative anomalous sound detection by embedding sharing","abstract":"To develop a machine sound monitoring system, a method for detecting anomalous sound is proposed. In this paper, we explore a method for multiple clients to collaboratively learn an anomalous sound detection model while keeping their raw data private from each other. In the context of industrial machine anomalous sound detection, each client possesses data from different machines or different operational states, making it challenging to learn through federated learning or split learning. In our proposed method, each client calculates embeddings using a common pre-trained model developed for sound data classification, and these calculated embeddings are aggregated on the server to perform anomalous sound detection through outlier exposure. Experiments showed that our proposed method improves the AUC of anomalous sound detection by an average of 6.8%.","sentences":["To develop a machine sound monitoring system, a method for detecting anomalous sound is proposed.","In this paper, we explore a method for multiple clients to collaboratively learn an anomalous sound detection model while keeping their raw data private from each other.","In the context of industrial machine anomalous sound detection, each client possesses data from different machines or different operational states, making it challenging to learn through federated learning or split learning.","In our proposed method, each client calculates embeddings using a common pre-trained model developed for sound data classification, and these calculated embeddings are aggregated on the server to perform anomalous sound detection through outlier exposure.","Experiments showed that our proposed method improves the AUC of anomalous sound detection by an average of 6.8%."],"url":"http://arxiv.org/abs/2403.16610v1","category":"eess.AS"}
{"created":"2024-03-25 10:39:18","title":"Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units","abstract":"Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities. Traum provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding. We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs. Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs.","sentences":["Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared.","This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information.","The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system.","Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities.","Traum provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking.","To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding.","We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs.","Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs."],"url":"http://arxiv.org/abs/2403.16609v1","category":"cs.CL"}
{"created":"2024-03-25 10:38:55","title":"Vector Ising Spin Annealer for Minimizing Ising Hamiltonians","abstract":"We introduce the Vector Ising Spin Annealer (VISA), a framework in gain-based computing that harnesses light-matter interactions to solve complex optimization problems encoded in spin Hamiltonians. Traditional driven-dissipative systems often select excited states due to limitations in spin movement. VISA transcends these constraints by enabling spins to operate in a three-dimensional space, offering a robust solution to minimize Ising Hamiltonians effectively. Our comparative analysis reveals VISA's superior performance over conventional single-dimension spin optimizers, demonstrating its ability to bridge substantial energy barriers in complex landscapes. Through detailed studies on cyclic and random graphs, we show VISA's proficiency in dynamically evolving the energy landscape with time-dependent gain and penalty annealing, illustrating its potential to redefine optimization in physical systems.","sentences":["We introduce the Vector Ising Spin Annealer (VISA), a framework in gain-based computing that harnesses light-matter interactions to solve complex optimization problems encoded in spin Hamiltonians.","Traditional driven-dissipative systems often select excited states due to limitations in spin movement.","VISA transcends these constraints by enabling spins to operate in a three-dimensional space, offering a robust solution to minimize Ising Hamiltonians effectively.","Our comparative analysis reveals VISA's superior performance over conventional single-dimension spin optimizers, demonstrating its ability to bridge substantial energy barriers in complex landscapes.","Through detailed studies on cyclic and random graphs, we show VISA's proficiency in dynamically evolving the energy landscape with time-dependent gain and penalty annealing, illustrating its potential to redefine optimization in physical systems."],"url":"http://arxiv.org/abs/2403.16608v1","category":"quant-ph"}
{"created":"2024-03-25 10:33:20","title":"ROXIE: Defining a Robotic eXplanation and Interpretability Engine","abstract":"In an era where autonomous robots increasingly inhabit public spaces, the imperative for transparency and interpretability in their decision-making processes becomes paramount. This paper presents the overview of a Robotic eXplanation and Interpretability Engine (ROXIE), which addresses this critical need, aiming to demystify the opaque nature of complex robotic behaviors. This paper elucidates the key features and requirements needed for providing information and explanations about robot decision-making processes. It also overviews the suite of software components and libraries available for deployment with ROS 2, empowering users to provide comprehensive explanations and interpretations of robot processes and behaviors, thereby fostering trust and collaboration in human-robot interactions.","sentences":["In an era where autonomous robots increasingly inhabit public spaces, the imperative for transparency and interpretability in their decision-making processes becomes paramount.","This paper presents the overview of a Robotic eXplanation and Interpretability Engine (ROXIE), which addresses this critical need, aiming to demystify the opaque nature of complex robotic behaviors.","This paper elucidates the key features and requirements needed for providing information and explanations about robot decision-making processes.","It also overviews the suite of software components and libraries available for deployment with ROS 2, empowering users to provide comprehensive explanations and interpretations of robot processes and behaviors, thereby fostering trust and collaboration in human-robot interactions."],"url":"http://arxiv.org/abs/2403.16606v1","category":"cs.RO"}
{"created":"2024-03-25 10:19:27","title":"Universal properties of branched copolymers in dilute solutions","abstract":"We analyze the universal conformational properties of complex copolymer macromolecules, based on two topologies: the rosette structure containing $f_c$ linear branches and $f_r$ closed loops grafted to the central core, and the symmetric pom-pom structure, consisting of a backbone linear chain terminated by two branching points with functionalities $f$. We assume that the constituent strands (branches) of these structures can be of two different chemical species $a$ and $b$. Depending on the solvent conditions, the inter- or intrachain interactions of some links may vanish, which corresponds to $\\Theta$-state of the corresponding polymer species. Applying both the analytical approach within the frames of direct polymer renormalization and numerical simulations based on the lattice model of polymer, we evaluated the set of parameters characterizing the size properties of constituent parts of two complex topologies and estimated quantitatively the impact of interactions between constituent parts on these size characteristics.","sentences":["We analyze the universal conformational properties of complex copolymer macromolecules, based on two topologies: the rosette structure containing $f_c$ linear branches and $f_r$ closed loops grafted to the central core, and the symmetric pom-pom structure, consisting of a backbone linear chain terminated by two branching points with functionalities $f$.","We assume that the constituent strands (branches) of these structures can be of two different chemical species $a$ and $b$. Depending on the solvent conditions, the inter- or intrachain interactions of some links may vanish, which corresponds to $\\Theta$-state of the corresponding polymer species.","Applying both the analytical approach within the frames of direct polymer renormalization and numerical simulations based on the lattice model of polymer, we evaluated the set of parameters characterizing the size properties of constituent parts of two complex topologies and estimated quantitatively the impact of interactions between constituent parts on these size characteristics."],"url":"http://arxiv.org/abs/2403.16598v1","category":"cond-mat.soft"}
{"created":"2024-03-25 10:17:08","title":"Limits on an improved action for contact effective field theory in two-body systems","abstract":"We consider a possible resummation of subleading effects in two-body systems with a large scattering length as described by a short-range effective field theory (EFT). In particular, we investigate the consequences of a resummation of part of the range corrections. Explicit calculations of the two-body phase shifts and charge form factor indicate that, except for extreme choices, resummations do not alter the convergence of the EFT expansion and are often beneficial at lowest orders. We have considered the expansion when the regulator cutoff is removed as well as when it is finite, and find that the cutoff is not an important factor for resummations. Our results connect with other works where the partial resummation is induced by potentials with finite cutoffs or interaction ranges.","sentences":["We consider a possible resummation of subleading effects in two-body systems with a large scattering length as described by a short-range effective field theory (EFT).","In particular, we investigate the consequences of a resummation of part of the range corrections.","Explicit calculations of the two-body phase shifts and charge form factor indicate that, except for extreme choices, resummations do not alter the convergence of the EFT expansion and are often beneficial at lowest orders.","We have considered the expansion when the regulator cutoff is removed as well as when it is finite, and find that the cutoff is not an important factor for resummations.","Our results connect with other works where the partial resummation is induced by potentials with finite cutoffs or interaction ranges."],"url":"http://arxiv.org/abs/2403.16596v1","category":"nucl-th"}
{"created":"2024-03-25 10:16:51","title":"The Adaptive Workplace: Orchestrating Architectural Services around the Wellbeing of Individual Occupants","abstract":"As the academic consortia members of the EU Horizon project SONATA (\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the workshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\" by proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to adapt a workplace to the ever-changing needs of individual occupants, instead of that occupants are expected to adapt to their workplace.","sentences":["As the academic consortia members of the EU Horizon project SONATA (\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the workshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\" by proposing the \"Adaptive Workplace\" concept.","In essence, our vision aims to adapt a workplace to the ever-changing needs of individual occupants, instead of that occupants are expected to adapt to their workplace."],"url":"http://arxiv.org/abs/2403.16595v1","category":"cs.HC"}
{"created":"2024-03-25 10:09:42","title":"Counter-example guided Imitation Learning of Feedback Controllers from Temporal Logic Specifications","abstract":"We present a novel method for imitation learning for control requirements expressed using Signal Temporal Logic (STL). More concretely we focus on the problem of training a neural network to imitate a complex controller. The learning process is guided by efficient data aggregation based on counter-examples and a coverage measure. Moreover, we introduce a method to evaluate the performance of the learned controller via parameterization and parameter estimation of the STL requirements. We demonstrate our approach with a flying robot case study.","sentences":["We present a novel method for imitation learning for control requirements expressed using Signal Temporal Logic (STL).","More concretely we focus on the problem of training a neural network to imitate a complex controller.","The learning process is guided by efficient data aggregation based on counter-examples and a coverage measure.","Moreover, we introduce a method to evaluate the performance of the learned controller via parameterization and parameter estimation of the STL requirements.","We demonstrate our approach with a flying robot case study."],"url":"http://arxiv.org/abs/2403.16593v1","category":"cs.RO"}
{"created":"2024-03-25 09:53:59","title":"Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy Approach with Performance Guarantee","abstract":"We study a linear quadratic regulation problem with a constraint where the control input can be nonzero only at a limited number of times. Given that this constraint leads to a combinational optimization problem, we adopt a greedy method to find a suboptimal solution. To quantify the performance of the greedy algorithm, we employ two metrics that reflect the submodularity level of the objective function: The submodularity ratio and curvature. We first present an explicit form of the optimal control input that is amenable to evaluating these metrics. Subsequently, we establish bounds on the submodularity ratio and curvature, which enable us to offer a practical performance guarantee for the greedy algorithm. The effectiveness of our guarantee is further demonstrated through numerical simulations.","sentences":["We study a linear quadratic regulation problem with a constraint where the control input can be nonzero only at a limited number of times.","Given that this constraint leads to a combinational optimization problem, we adopt a greedy method to find a suboptimal solution.","To quantify the performance of the greedy algorithm, we employ two metrics that reflect the submodularity level of the objective function: The submodularity ratio and curvature.","We first present an explicit form of the optimal control input that is amenable to evaluating these metrics.","Subsequently, we establish bounds on the submodularity ratio and curvature, which enable us to offer a practical performance guarantee for the greedy algorithm.","The effectiveness of our guarantee is further demonstrated through numerical simulations."],"url":"http://arxiv.org/abs/2403.16585v1","category":"eess.SY"}
{"created":"2024-03-25 09:39:45","title":"Near-field Beam Steering with Planar Antenna Array","abstract":"Beam steering enables manipulation of the electromagnetic radiation patterns in antenna array systems. A methodology for steering beams in the near field of a planar antenna array with known phase wavefront functions towards arbitrary azimuth and elevation angles is described in this paper. Rotation of the phase wavefront function is used while preserving the shape. The phase shifts for antenna element excitation currents are determined based on the distances from antenna elements to the nearest point on the rotated wavefront. Beam steering utilizing a Gaussian and a Bessel beam is studied. Phase distribution examples for various steering directions are considered. For Bessel beam steering, a discussion on the resulting beam shapes and the steering impact on the polarization mismatch is provided. The results show a non-negligible magnitude of the cross-polarization with values depending on the steering direction.","sentences":["Beam steering enables manipulation of the electromagnetic radiation patterns in antenna array systems.","A methodology for steering beams in the near field of a planar antenna array with known phase wavefront functions towards arbitrary azimuth and elevation angles is described in this paper.","Rotation of the phase wavefront function is used while preserving the shape.","The phase shifts for antenna element excitation currents are determined based on the distances from antenna elements to the nearest point on the rotated wavefront.","Beam steering utilizing a Gaussian and a Bessel beam is studied.","Phase distribution examples for various steering directions are considered.","For Bessel beam steering, a discussion on the resulting beam shapes and the steering impact on the polarization mismatch is provided.","The results show a non-negligible magnitude of the cross-polarization with values depending on the steering direction."],"url":"http://arxiv.org/abs/2403.16573v1","category":"eess.SP"}
{"created":"2024-03-25 09:35:21","title":"Gravitational waves from domain wall collapses and dark matter in the SM with a complex scalar","abstract":"We study domain wall induced by spontaneously broken $\\mathbb{Z}_2$ symmetry and its gravitational wave signature in the standard model with a complex scalar in connection with dark matter physics. In a minimal setup, a linear term of the singlet field is added to the scalar potential as an explicit $\\mathbb{Z}_2$ breaking term to make the domain wall unstable. We obtain its minimal size from cosmological constraints and show that the parameter space that can be probed by current and future pulsar time array experiments requires the vacuum expectation value of the singlet field to be greater than $\\mathcal{O}(10-100)$ TeV, along with a singlet-like Higgs mass of $\\mathcal{O}(1-100)$ TeV. However, such a region is severely restricted by the dark matter relic density, which places an upper bound on the singlet vacuum expectation value at approximately 200 TeV, and limits the dark matter mass to about half of the singlet-like Higgs boson mass.","sentences":["We study domain wall induced by spontaneously broken $\\mathbb{Z}_2$ symmetry and its gravitational wave signature in the standard model with a complex scalar in connection with dark matter physics.","In a minimal setup, a linear term of the singlet field is added to the scalar potential as an explicit $\\mathbb{Z}_2$ breaking term to make the domain wall unstable.","We obtain its minimal size from cosmological constraints and show that the parameter space that can be probed by current and future pulsar time array experiments requires the vacuum expectation value of the singlet field to be greater than $\\mathcal{O}(10-100)$ TeV, along with a singlet-like Higgs mass of $\\mathcal{O}(1-100)$ TeV. However, such a region is severely restricted by the dark matter relic density, which places an upper bound on the singlet vacuum expectation value at approximately 200 TeV, and limits the dark matter mass to about half of the singlet-like Higgs boson mass."],"url":"http://arxiv.org/abs/2403.16568v1","category":"hep-ph"}
{"created":"2024-03-25 09:26:57","title":"Moments of Margulis functions and indefinite ternary quadratic forms","abstract":"In this paper, we study the moments of the Margulis $\\alpha$-function integrating over expanding translates of a unipotent orbit in $\\operatorname{SL}_3(\\mathbb{R})/\\operatorname{SL}_3(\\mathbb{Z})$. We show that for some $\\lambda>1$ the $\\lambda$-moments of the Margulis $\\alpha$-function over expanding translates of a unipotent orbit are uniformly bounded, under suitable Diophantine conditions of the initial unipotent orbit.   As an application, we prove that for any indefinite irrational ternary quadratic form $Q$ with suitable Diophantine conditions and $a<b$ the number of integral vectors of norm at most $T$ satisfying $a<Q(v)<b$ is asymptotically equivalent to $\\mathsf{C}_Q(b-a)T$ as $T$ tends to infinity, where the constant $\\mathsf{C}_Q>0$ depends only on $Q$.","sentences":["In this paper, we study the moments of the Margulis $\\alpha$-function integrating over expanding translates of a unipotent orbit in $\\operatorname{SL}_3(\\mathbb{R})/\\operatorname{SL}_3(\\mathbb{Z})$. We show that for some $\\lambda>1$ the $\\lambda$-moments of the Margulis $\\alpha$-function over expanding translates of a unipotent orbit are uniformly bounded, under suitable Diophantine conditions of the initial unipotent orbit.   ","As an application, we prove that for any indefinite irrational ternary quadratic form $Q$ with suitable Diophantine conditions and $a<b$ the number of integral vectors of norm at most $T$ satisfying $a<Q(v)<b$ is asymptotically equivalent to $\\mathsf{C}_Q(b-a)T$ as $T$ tends to infinity, where the constant $\\mathsf{C}_Q>0$ depends only on $Q$."],"url":"http://arxiv.org/abs/2403.16563v1","category":"math.DS"}
{"created":"2024-03-25 09:17:31","title":"On divergent on average trajectories for higher rank actions","abstract":"For $d\\ge 3$ we first show that the Hausdorff dimension of the set of $A$-divergent on average points in the $(d-1)$-dimensional closed horosphere in the space of $d$-dimensional Euclidean lattices, where $A$ is the group of positive diagonal matrices, is at most $\\frac{d-1}{2}$. In particular, this upper bound is sharp for $d=3$.   We apply this to compute the Hausdorff dimension of the set of exceptions to the inhomogeneous uniform version of Littlewood conjecture. We say that a pair $(\\xi_1,\\xi_2)\\in\\mathbb{R}^2$ satisfies the inhomogeneous Littlewood conjecture if $$\\liminf_{q\\to\\infty}q\\|q\\xi_1-\\theta_1\\|_{\\mathbb{Z}}\\|q\\xi_2-\\theta_2\\|_{\\mathbb{Z}}=0$$ for all $(\\theta_1,\\theta_2)\\in\\mathbb{R}^2$, where $\\|\\cdot\\|_\\mathbb{Z}$ denotes the distance to the nearest integer. We prove that the Hausdorff dimension of the set of pairs $(\\xi_1,\\xi_2)\\in\\mathbb{R}^2$ not satisfying the inhomogeneous Littlewood conjecture is $1$, which is equal to the Hausdorff dimension of the conjectural set of exceptions.","sentences":["For $d\\ge 3$ we first show that the Hausdorff dimension of the set of $A$-divergent on average points in the $(d-1)$-dimensional closed horosphere in the space of $d$-dimensional Euclidean lattices, where $A$ is the group of positive diagonal matrices, is at most $\\frac{d-1}{2}$. In particular, this upper bound is sharp for $d=3$.   We apply this to compute the Hausdorff dimension of the set of exceptions to the inhomogeneous uniform version of Littlewood conjecture.","We say that a pair $(\\xi_1,\\xi_2)\\in\\mathbb{R}^2$ satisfies the inhomogeneous Littlewood conjecture if $$\\liminf_{q\\to\\infty}q\\|q\\xi_1-\\theta_1\\|_{\\mathbb{Z}}\\|q\\xi_2-\\theta_2\\|_{\\mathbb{Z}}=0$$ for all $(\\theta_1,\\theta_2)\\in\\mathbb{R}^2$, where $\\|\\cdot\\|_\\mathbb{Z}$ denotes the distance to the nearest integer.","We prove that the Hausdorff dimension of the set of pairs $(\\xi_1,\\xi_2)\\in\\mathbb{R}^2$ not satisfying the inhomogeneous Littlewood conjecture is $1$, which is equal to the Hausdorff dimension of the conjectural set of exceptions."],"url":"http://arxiv.org/abs/2403.16559v1","category":"math.DS"}
{"created":"2024-03-25 09:16:59","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients","abstract":"Federated Learning (FL) is a distributed machine learning framework in communication network systems. However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global aggregation. We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence.","sentences":["Federated Learning (FL) is a distributed machine learning framework in communication network systems.","However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence.","In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset.","In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model.","Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients.","These top portion of the gradients will be selected and sent to the server for global aggregation.","We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence."],"url":"http://arxiv.org/abs/2403.16557v1","category":"cs.LG"}
{"created":"2024-03-25 09:14:08","title":"Connectivity of Parameter Regions of Multistationarity for Multisite Phosphorylation Networks","abstract":"The parameter region of multistationarity of a reaction network contains all the parameters for which the associated dynamical system exhibits multiple steady states. Describing this region is challenging and remains an active area of research. In this paper, we concentrate on two biologically relevant families of reaction networks that model multisite phosphorylation and dephosphorylation of a substrate at $n$ sites. For small values of $n$, it had previously been shown that the parameter region of multistationarity is connected. Here, we extend these results and provide a proof that applies to all values of $n$. Our techniques are based on the study of the critical polynomial associated with these reaction networks together with polyhedral geometric conditions of the signed support of this polynomial.","sentences":["The parameter region of multistationarity of a reaction network contains all the parameters for which the associated dynamical system exhibits multiple steady states.","Describing this region is challenging and remains an active area of research.","In this paper, we concentrate on two biologically relevant families of reaction networks that model multisite phosphorylation and dephosphorylation of a substrate at $n$ sites.","For small values of $n$, it had previously been shown that the parameter region of multistationarity is connected.","Here, we extend these results and provide a proof that applies to all values of $n$. Our techniques are based on the study of the critical polynomial associated with these reaction networks together with polyhedral geometric conditions of the signed support of this polynomial."],"url":"http://arxiv.org/abs/2403.16556v1","category":"q-bio.MN"}
{"created":"2024-03-25 08:54:46","title":"Droughts in Germany -- Why global climate change amplifies hydrological extremes","abstract":"The warmer temperatures of global climate change strengthen the water cycle, evaporation and precipitation increase. But the extremes of heavy rain, floods, dry periods and droughts will also increase. How does this fit together? Simple physical considerations show which factors mainly regulate the strength of the water cycle in the Earth system, and how this determines water availability on land. This can be used to interpret the observed changes in the water balance in Germany and explain the increasing dryness in Germany.","sentences":["The warmer temperatures of global climate change strengthen the water cycle, evaporation and precipitation increase.","But the extremes of heavy rain, floods, dry periods and droughts will also increase.","How does this fit together?","Simple physical considerations show which factors mainly regulate the strength of the water cycle in the Earth system, and how this determines water availability on land.","This can be used to interpret the observed changes in the water balance in Germany and explain the increasing dryness in Germany."],"url":"http://arxiv.org/abs/2403.16551v1","category":"physics.pop-ph"}
{"created":"2024-03-25 08:54:08","title":"Unfolding numbers for interval maps","abstract":"We propose a new invariant (\\emph{rotation number}) for a \\emph{cycle} of an \\emph{interval map}, called its \\emph{unfolding number} to circumvent some difficulties faced in the \\emph{over-rotation theory} for \\emph{interval maps}. We corroborate the \\emph{completeness} of the theory and exhibit that the set of \\emph{unfolding numbers} of points under a given interval map $f$ forms an \\emph{interval} of the form $[u_f, \\frac{1}{2}]$ where $u_f$ can be \\emph{computed} by a simple \\emph{algorithm} for any \\emph{interval} map $f$. Further, $u_f$ is realized on a \\emph{closed invariant} subset $Z_f \\subset [0,1]$ such that $f|_{Z_f}$ is \\emph{strictly ergodic}. In the end we forge connection between \\emph{unfolding numbers} and \\emph{over-rotation numbers}: we reveal that while disparate in essence, there exists \\emph{special patterns} $\\pi_s$ called \\emph{sheer patterns} for whom \\emph{unfolding numbers} equals \\emph{over-rotation numbers}.","sentences":["We propose a new invariant (\\emph{rotation number}) for a \\emph{cycle} of an \\emph{interval map}, called its \\emph{unfolding number} to circumvent some difficulties faced in the \\emph{over-rotation theory} for \\emph{interval maps}.","We corroborate the \\emph{completeness} of the theory and exhibit that the set of \\emph{unfolding numbers} of points under a given interval map $f$ forms an \\emph{interval} of the form $[u_f, \\frac{1}{2}]$ where $u_f$ can be \\emph{computed} by a simple \\emph{algorithm} for any \\emph{interval} map $f$. Further, $u_f$ is realized on a \\emph{closed invariant} subset $Z_f \\subset","[0,1]$ such that $f|_{Z_f}$ is \\emph{strictly ergodic}.","In the end we forge connection between \\emph{unfolding numbers} and \\emph{over-rotation numbers}: we reveal that while disparate in essence, there exists \\emph{special patterns} $\\pi_s$ called \\emph{sheer patterns} for whom \\emph{unfolding numbers} equals \\emph{over-rotation numbers}."],"url":"http://arxiv.org/abs/2403.16549v1","category":"math.DS"}
{"created":"2024-03-25 08:52:52","title":"The Effect of Light Nuclei on Chemical Freeze-out Parameters at RHIC Energy","abstract":"This study examines the chemical freeze-out of hadrons, encompassing light-flavor, strange-flavor, and light nuclei produced in Au+Au collisions at the Relativistic Heavy Ion Collider (RHIC). By conducting a thermal analysis of hadron yields with and without the inclusion of light nuclei yields, we observe a discernible decrease in the chemical freeze-out temperature $T_{\\textrm{ch}}$ when light nuclei yields are considered. This suggests that light nuclei formation occurs at a later stage of the system evolution. Furthermore, the $T_{\\textrm{ch}}$ associated with strange-flavor particles is found to be approximately $20-30$ MeV higher than that for light-flavor particles and light nuclei, hinting at the existence of multiple freeze-out hyper-surfaces for different hadron types in heavy-ion collisions. We present parameterized formulas that describe the energy dependence of $T_{\\textrm{ch}}$ and $\\mu_B$ for four distinct particle sets in central Au+Au collisions. These formulas allow for the selection of appropriate parameters tailored to specific studies, facilitating a more nuanced understanding of the freeze-out process.","sentences":["This study examines the chemical freeze-out of hadrons, encompassing light-flavor, strange-flavor, and light nuclei produced in Au+Au collisions at the Relativistic Heavy Ion Collider (RHIC).","By conducting a thermal analysis of hadron yields with and without the inclusion of light nuclei yields, we observe a discernible decrease in the chemical freeze-out temperature $T_{\\textrm{ch}}$ when light nuclei yields are considered.","This suggests that light nuclei formation occurs at a later stage of the system evolution.","Furthermore, the $T_{\\textrm{ch}}$ associated with strange-flavor particles is found to be approximately $20-30$ MeV higher than that for light-flavor particles and light nuclei, hinting at the existence of multiple freeze-out hyper-surfaces for different hadron types in heavy-ion collisions.","We present parameterized formulas that describe the energy dependence of $T_{\\textrm{ch}}$ and $\\mu_B$ for four distinct particle sets in central Au+Au collisions.","These formulas allow for the selection of appropriate parameters tailored to specific studies, facilitating a more nuanced understanding of the freeze-out process."],"url":"http://arxiv.org/abs/2403.16548v1","category":"nucl-th"}
{"created":"2024-03-25 08:40:43","title":"Exposing the hidden layers and interplay in the quantum software stack","abstract":"Current and near-future quantum computers face resource limitations due to noise and low qubit counts. Despite this, effective quantum advantage can still be achieved due to the exponential nature of bit-to-qubit conversion. However, optimizing the software architecture of these systems is essential to utilize available resources efficiently. Unfortunately, the focus on user-friendly quantum computers has obscured critical steps in the software stack, leading to ripple effects into the stack's upper layer induced by limitations in current qubit implementations. This paper unveils the hidden interplay among layers of the quantum software stack.","sentences":["Current and near-future quantum computers face resource limitations due to noise and low qubit counts.","Despite this, effective quantum advantage can still be achieved due to the exponential nature of bit-to-qubit conversion.","However, optimizing the software architecture of these systems is essential to utilize available resources efficiently.","Unfortunately, the focus on user-friendly quantum computers has obscured critical steps in the software stack, leading to ripple effects into the stack's upper layer induced by limitations in current qubit implementations.","This paper unveils the hidden interplay among layers of the quantum software stack."],"url":"http://arxiv.org/abs/2403.16545v1","category":"cs.SE"}
{"created":"2024-03-25 08:21:41","title":"XAV: A High-Performance Regular Expression Matching Engine for Packet Processing","abstract":"Regular expression matching is the core function of various network security applications such as network intrusion detection systems. With the network bandwidth increases, it is a great challenge to implement regular expression matching for line rate packet processing. To this end, a novel scheme named XAV targeting high-performance regular expression matching is proposed in this paper. XAV first employs anchor DFA to tackle the state explosion problem of DFA. Then based on anchor DFA, two techniques including pre-filtering and regex decomposition are utilized to improve the average time complexity. Through implementing XAV with an FPGA-CPU architecture, comprehensive experiments show that a high matching throughput of up to 75 Gbps can be achieved for the large and complex Snort rule-set. Compared to state-of-the-art software schemes, XAV achieves two orders of magnitude of performance improvement. While compared to state-of-the-art FPGA-based schemes, XAV achieves more than 2.5x performance improvement with the same hardware resource consumption.","sentences":["Regular expression matching is the core function of various network security applications such as network intrusion detection systems.","With the network bandwidth increases, it is a great challenge to implement regular expression matching for line rate packet processing.","To this end, a novel scheme named XAV targeting high-performance regular expression matching is proposed in this paper.","XAV first employs anchor DFA to tackle the state explosion problem of DFA.","Then based on anchor DFA, two techniques including pre-filtering and regex decomposition are utilized to improve the average time complexity.","Through implementing XAV with an FPGA-CPU architecture, comprehensive experiments show that a high matching throughput of up to 75 Gbps can be achieved for the large and complex Snort rule-set.","Compared to state-of-the-art software schemes, XAV achieves two orders of magnitude of performance improvement.","While compared to state-of-the-art FPGA-based schemes, XAV achieves more than 2.5x performance improvement with the same hardware resource consumption."],"url":"http://arxiv.org/abs/2403.16533v1","category":"cs.NI"}
{"created":"2024-03-25 08:02:41","title":"CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal Representation Learning for AD classification","abstract":"Alzheimer's disease (AD) is an incurable neurodegenerative condition leading to cognitive and functional deterioration. Given the lack of a cure, prompt and precise AD diagnosis is vital, a complex process dependent on multiple factors and multi-modal data. While successful efforts have been made to integrate multi-modal representation learning into medical datasets, scant attention has been given to 3D medical images. In this paper, we propose Contrastive Masked Vim Autoencoder (CMViM), the first efficient representation learning method tailored for 3D multi-modal data. Our proposed framework is built on a masked Vim autoencoder to learn a unified multi-modal representation and long-dependencies contained in 3D medical images. We also introduce an intra-modal contrastive learning module to enhance the capability of the multi-modal Vim encoder for modeling the discriminative features in the same modality, and an inter-modal contrastive learning module to alleviate misaligned representation among modalities. Our framework consists of two main steps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to reconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal representations with contrastive learning mechanisms from both intra-modal and inter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset and validated on the downstream task for AD classification. The proposed CMViM yields 2.7\\% AUC performance improvement compared with other state-of-the-art methods.","sentences":["Alzheimer's disease (AD) is an incurable neurodegenerative condition leading to cognitive and functional deterioration.","Given the lack of a cure, prompt and precise AD diagnosis is vital, a complex process dependent on multiple factors and multi-modal data.","While successful efforts have been made to integrate multi-modal representation learning into medical datasets, scant attention has been given to 3D medical images.","In this paper, we propose Contrastive Masked Vim Autoencoder (CMViM), the first efficient representation learning method tailored for 3D multi-modal data.","Our proposed framework is built on a masked Vim autoencoder to learn a unified multi-modal representation and long-dependencies contained in 3D medical images.","We also introduce an intra-modal contrastive learning module to enhance the capability of the multi-modal Vim encoder for modeling the discriminative features in the same modality, and an inter-modal contrastive learning module to alleviate misaligned representation among modalities.","Our framework consists of two main steps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to reconstruct 3D masked multi-modal data efficiently.","2) align the multi-modal representations with contrastive learning mechanisms from both intra-modal and inter-modal aspects.","Our framework is pre-trained and validated ADNI2 dataset and validated on the downstream task for AD classification.","The proposed CMViM yields 2.7\\% AUC performance improvement compared with other state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16520v1","category":"cs.CV"}
{"created":"2024-03-25 08:02:31","title":"Two Algorithms for Computing Rational Univariate Representations of Zero-Dimensional Ideals with Parameters","abstract":"Two algorithms for computing the rational univariate representation of zero-dimensional ideals with parameters are presented in the paper. Different from the rational univariate representation of zero-dimensional ideals without parameters, the number of zeros of zero-dimensional ideals with parameters under various specializations is different, which leads to choosing and checking the separating element, the key to computing the rational univariate representation, is difficult. In order to pick out the separating element, by partitioning the parameter space we can ensure that under each branch the ideal has the same number of zeros. Subsequently with the help of the extended subresultant theorem for parametric cases, two ideas are given to conduct the further partition of parameter space for choosing and checking the separating element. Based on these, we give two algorithms for computing rational univariate representations of zero-dimensional ideals with parameters. Furthermore, the two algorithms have been implemented on the computer algebra system Singular. Experimental data show that the second algorithm has the better performance in contrast to the first one.","sentences":["Two algorithms for computing the rational univariate representation of zero-dimensional ideals with parameters are presented in the paper.","Different from the rational univariate representation of zero-dimensional ideals without parameters, the number of zeros of zero-dimensional ideals with parameters under various specializations is different, which leads to choosing and checking the separating element, the key to computing the rational univariate representation, is difficult.","In order to pick out the separating element, by partitioning the parameter space we can ensure that under each branch the ideal has the same number of zeros.","Subsequently with the help of the extended subresultant theorem for parametric cases, two ideas are given to conduct the further partition of parameter space for choosing and checking the separating element.","Based on these, we give two algorithms for computing rational univariate representations of zero-dimensional ideals with parameters.","Furthermore, the two algorithms have been implemented on the computer algebra system Singular.","Experimental data show that the second algorithm has the better performance in contrast to the first one."],"url":"http://arxiv.org/abs/2403.16519v1","category":"cs.SC"}
{"created":"2024-03-25 07:59:29","title":"Linguistically Differentiating Acts and Recalls of Racial Microaggressions on Social Media","abstract":"In this work, we examine the linguistic signature of online racial microaggressions (acts) and how it differs from that of personal narratives recalling experiences of such aggressions (recalls) by Black social media users. We manually curate and annotate a corpus of acts and recalls from in-the-wild social media discussions, and verify labels with Black workshop participants. We leverage Natural Language Processing (NLP) and qualitative analysis on this data to classify (RQ1), interpret (RQ2), and characterize (RQ3) the language underlying acts and recalls of racial microaggressions in the context of racism in the U.S. Our findings show that neural language models (LMs) can classify acts and recalls with high accuracy (RQ1) with contextual words revealing themes that associate Blacks with objects that reify negative stereotypes (RQ2). Furthermore, overlapping linguistic signatures between acts and recalls serve functionally different purposes (RQ3), providing broader implications to the current challenges in content moderation systems on social media.","sentences":["In this work, we examine the linguistic signature of online racial microaggressions (acts) and how it differs from that of personal narratives recalling experiences of such aggressions (recalls) by Black social media users.","We manually curate and annotate a corpus of acts and recalls from in-the-wild social media discussions, and verify labels with Black workshop participants.","We leverage Natural Language Processing (NLP) and qualitative analysis on this data to classify (RQ1), interpret (RQ2), and characterize (RQ3) the language underlying acts and recalls of racial microaggressions in the context of racism in the U.S.","Our findings show that neural language models (LMs) can classify acts and recalls with high accuracy (RQ1) with contextual words revealing themes that associate Blacks with objects that reify negative stereotypes (RQ2).","Furthermore, overlapping linguistic signatures between acts and recalls serve functionally different purposes (RQ3), providing broader implications to the current challenges in content moderation systems on social media."],"url":"http://arxiv.org/abs/2403.16514v1","category":"cs.HC"}
{"created":"2024-03-25 07:38:40","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification","abstract":"Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune. Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods.","sentences":["Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks.","However, these studies focused on monolingual, single-turn classification tasks.","In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions.","Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts.","LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs.","This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context.","Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune.","Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods."],"url":"http://arxiv.org/abs/2403.16504v1","category":"cs.CL"}
{"created":"2024-03-25 07:37:42","title":"Event-Horizon-Like Singularities and Quantum Phase Transitions","abstract":"A recent study shows that an emergent evolution dimension, in addition to time, can be induced if the quantum system depends on a continuous parameter. The evolution in the emergent evolution dimension is described by a parallel transport, the closest fiber bundle analog to a geodesic in (pseudo-)Riemannian geometry. Nevertheless, the evolution in the emergent dimension often exhibits some singular behaviors at critical points. In this work, we demonstrate that these singularities can be indications of quantum phase transitions. We then show that these singularities, like those at the black hole event horizon, can be removed locally.","sentences":["A recent study shows that an emergent evolution dimension, in addition to time, can be induced if the quantum system depends on a continuous parameter.","The evolution in the emergent evolution dimension is described by a parallel transport, the closest fiber bundle analog to a geodesic in (pseudo-)Riemannian geometry.","Nevertheless, the evolution in the emergent dimension often exhibits some singular behaviors at critical points.","In this work, we demonstrate that these singularities can be indications of quantum phase transitions.","We then show that these singularities, like those at the black hole event horizon, can be removed locally."],"url":"http://arxiv.org/abs/2403.16503v1","category":"quant-ph"}
{"created":"2024-03-25 07:27:07","title":"Evidence for a finite-momentum Cooper pair in tricolor $d$-wave superconducting superlattices","abstract":"Fermionic superfluidity with a nontrivial Cooper-pairing, beyond the conventional Bardeen-Cooper-Schrieffer state, is a captivating field of study in quantum many-body systems. In particular, the search for superconducting states with finite-momentum pairs has long been a challenge, but establishing its existence has long suffered from the lack of an appropriate probe to reveal its momentum. Recently, it has been proposed that the nonreciprocal {\\cred electron} transport is the most {\\cred powerful} probe for the finite-momentum pairs, {\\cred because it directly couples} to the supercurrents. Here we reveal such a pairing state by the non-reciprocal transport on tricolor superlattices with strong spin-orbit coupling combined with broken inversion-symmetry consisting of atomically thin $d$-wave superconductor CeCoIn$_5$. We find that while the second-harmonic resistance exhibits a distinct dip anomaly at the low-temperature ($T$)/high-magnetic field ($H$) corner in the $HT$-plane for ${\\bm H}$ applied to the antinodal direction of the $d$-wave gap, such an anomaly is absent for ${\\bm H}$ along the nodal direction. By meticulously isolating extrinsic effects due to vortex dynamics, we reveal the presence of a non-reciprocal response originating from intrinsic superconducting properties characterized by finite-momentum pairs. We attribute the high-field state to the helical superconducting state, wherein the phase of the order parameter is spontaneously spatially modulated.","sentences":["Fermionic superfluidity with a nontrivial Cooper-pairing, beyond the conventional Bardeen-Cooper-Schrieffer state, is a captivating field of study in quantum many-body systems.","In particular, the search for superconducting states with finite-momentum pairs has long been a challenge, but establishing its existence has long suffered from the lack of an appropriate probe to reveal its momentum.","Recently, it has been proposed that the nonreciprocal {\\cred electron} transport is the most {\\cred powerful} probe for the finite-momentum pairs, {\\cred because it directly couples} to the supercurrents.","Here we reveal such a pairing state by the non-reciprocal transport on tricolor superlattices with strong spin-orbit coupling combined with broken inversion-symmetry consisting of atomically thin $d$-wave superconductor CeCoIn$_5$.","We find that while the second-harmonic resistance exhibits a distinct dip anomaly at the low-temperature ($T$)/high-magnetic field ($H$) corner in the $HT$-plane for ${\\bm H}$ applied to the antinodal direction of the $d$-wave gap, such an anomaly is absent for ${\\bm H}$ along the nodal direction.","By meticulously isolating extrinsic effects due to vortex dynamics, we reveal the presence of a non-reciprocal response originating from intrinsic superconducting properties characterized by finite-momentum pairs.","We attribute the high-field state to the helical superconducting state, wherein the phase of the order parameter is spontaneously spatially modulated."],"url":"http://arxiv.org/abs/2403.16496v1","category":"cond-mat.supr-con"}
{"created":"2024-03-25 07:21:46","title":"Gap distribution of $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ and the circle method","abstract":"The distribution of the properly renormalized gaps of $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ with $n < N$ converges (when $N\\rightarrow \\infty$) to a non-standard limit distribution, as Elkies and McMullen proved in 2004 using techniques from homogeneous dynamics. In this paper we give an essentially self-contained proof based on the circle method. Our main innovation consists in showing that a new type of correlation functions of $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ converge. To define these correlation functions we restrict, smoothly, to those $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ that lie in minor arcs, i.e. away from rational numbers with small denominators.","sentences":["The distribution of the properly renormalized gaps of $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ with $n < N$ converges (when $N\\rightarrow \\infty$) to a non-standard limit distribution, as Elkies and McMullen proved in 2004 using techniques from homogeneous dynamics.","In this paper we give an essentially self-contained proof based on the circle method.","Our main innovation consists in showing that a new type of correlation functions of $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ converge.","To define these correlation functions we restrict, smoothly, to those $\\sqrt{n} \\,\\mathrm{mod}\\, 1$ that lie in minor arcs, i.e. away from rational numbers with small denominators."],"url":"http://arxiv.org/abs/2403.16493v1","category":"math.NT"}
{"created":"2024-03-25 07:18:49","title":"Characterisation of the Intel RealSense D415 Stereo Depth Camera for Motion-Corrected CT Perfusion Imaging","abstract":"Even for short protocols (<1 min), head movement can compromise accurate haemodynamic modelling of cerebral CT perfusion (CTP) imaging in acute stroke. Frame-to-frame registration is the most common form of retrospective correction but neglects the fact that motion is continuous, not discrete. By contrast, external tracking devices provide continuous motion monitoring and thereby the opportunity to fully correct the acquired data for motion. The aim of this study was to characterise the Intel D415 stereo depth camera, a compact low-cost markerless tracking device, in terms of its suitability for retrospective CTP motion correction. The results showed that jitter was stable, and thermally-induced pose drift was {\\le} 1.5 mm and {\\le} 0.5{\\deg} during the first 10-20 min, after which it also became stable. For static poses, the mean difference between the Intel D415 motion estimates and ground-truth poses for a head phantom was {\\le} 1.24 \\pm 0.01 mm and {\\le} 0.68 \\pm 0.01{\\deg} for position and orientation, respectively. For dynamic poses measured while a head phantom travelled smooth continuous trajectories with median speed 0.031 ms^(-1) (speed range 0-0.500 ms^(-1)), the root-mean-square-error (RMSE) was {\\le} 1.40 \\pm 0.12 mm and {\\le} 0.24 \\pm 0.02{\\deg}. When tracking a simulated patient head trajectory derived from a clinical CTP scan, the average RMSE was {\\le} 0.86 \\pm 0.03 mm and {\\le} 0.16 \\pm 0.03{\\deg}. Tracking the head motion of a human volunteer inside a clinical CT scanner, the average RMSE was {\\le} 2.72 \\pm 0.24 mm and {\\le} 0.55 \\pm 0.07{\\deg}. Overall, our results suggest that a single D415 tracking system can achieve promising pose estimation accuracy, though still worse than typical brain CT resolution, including CTP. The error is likely to be reduced to a practical level by combining multiple devices and this should be investigated in future work.","sentences":["Even for short protocols (<1 min), head movement can compromise accurate haemodynamic modelling of cerebral CT perfusion (CTP) imaging in acute stroke.","Frame-to-frame registration is the most common form of retrospective correction but neglects the fact that motion is continuous, not discrete.","By contrast, external tracking devices provide continuous motion monitoring and thereby the opportunity to fully correct the acquired data for motion.","The aim of this study was to characterise the Intel D415 stereo depth camera, a compact low-cost markerless tracking device, in terms of its suitability for retrospective CTP motion correction.","The results showed that jitter was stable, and thermally-induced pose drift was {\\le} 1.5 mm and {\\le} 0.5{\\deg} during the first 10-20 min, after which it also became stable.","For static poses, the mean difference between the Intel D415 motion estimates and ground-truth poses for a head phantom was {\\le} 1.24 \\pm 0.01 mm and {\\le} 0.68 \\pm 0.01{\\deg} for position and orientation, respectively.","For dynamic poses measured while a head phantom travelled smooth continuous trajectories with median speed 0.031 ms^(-1) (speed range 0-0.500 ms^(-1)), the root-mean-square-error (RMSE) was {\\le} 1.40 \\pm 0.12 mm and {\\le} 0.24 \\pm 0.02{\\deg}.","When tracking a simulated patient head trajectory derived from a clinical CTP scan, the average RMSE was {\\le} 0.86 \\pm 0.03 mm and {\\le} 0.16 \\pm 0.03{\\deg}.","Tracking the head motion of a human volunteer inside a clinical CT scanner, the average RMSE was {\\le} 2.72 \\pm 0.24 mm and {\\le} 0.55 \\pm 0.07{\\deg}.","Overall, our results suggest that a single D415 tracking system can achieve promising pose estimation accuracy, though still worse than typical brain CT resolution, including CTP.","The error is likely to be reduced to a practical level by combining multiple devices and this should be investigated in future work."],"url":"http://arxiv.org/abs/2403.16490v1","category":"physics.med-ph"}
{"created":"2024-03-25 07:17:44","title":"Spatially temporally distributed informative path planning for multi-robot systems","abstract":"This paper investigates the problem of informative path planning for a mobile robotic sensor network in spatially temporally distributed mapping. The robots are able to gather noisy measurements from an area of interest during their movements to build a Gaussian Process (GP) model of a spatio-temporal field. The model is then utilized to predict the spatio-temporal phenomenon at different points of interest. To spatially and temporally navigate the group of robots so that they can optimally acquire maximal information gains while their connectivity is preserved, we propose a novel multistep prediction informative path planning optimization strategy employing our newly defined local cost functions. By using the dual decomposition method, it is feasible and practical to effectively solve the optimization problem in a distributed manner. The proposed method was validated through synthetic experiments utilizing real-world data sets.","sentences":["This paper investigates the problem of informative path planning for a mobile robotic sensor network in spatially temporally distributed mapping.","The robots are able to gather noisy measurements from an area of interest during their movements to build a Gaussian Process (GP) model of a spatio-temporal field.","The model is then utilized to predict the spatio-temporal phenomenon at different points of interest.","To spatially and temporally navigate the group of robots so that they can optimally acquire maximal information gains while their connectivity is preserved, we propose a novel multistep prediction informative path planning optimization strategy employing our newly defined local cost functions.","By using the dual decomposition method, it is feasible and practical to effectively solve the optimization problem in a distributed manner.","The proposed method was validated through synthetic experiments utilizing real-world data sets."],"url":"http://arxiv.org/abs/2403.16489v1","category":"cs.RO"}
{"created":"2024-03-25 07:02:37","title":"A Method for Target Detection Based on Mmw Radar and Vision Fusion","abstract":"An efficient and accurate traffic monitoring system often takes advantages of multi-sensor detection to ensure the safety of urban traffic, promoting the accuracy and robustness of target detection and tracking. A method for target detection using Radar-Vision Fusion Path Aggregation Fully Convolutional One-Stage Network (RV-PAFCOS) is proposed in this paper, which is extended from Fully Convolutional One-Stage Network (FCOS) by introducing the modules of radar image processing branches, radar-vision fusion and path aggregation. The radar image processing branch mainly focuses on the image modeling based on the spatiotemporal calibration of millimeter-wave (mmw) radar and cameras, taking the conversion of radar point clouds to radar images. The fusion module extracts features of radar and optical images based on the principle of spatial attention stitching criterion. The path aggregation module enhances the reuse of feature layers, combining the positional information of shallow feature maps with deep semantic information, to obtain better detection performance for both large and small targets. Through the experimental analysis, the method proposed in this paper can effectively fuse the mmw radar and vision perceptions, showing good performance in traffic target detection.","sentences":["An efficient and accurate traffic monitoring system often takes advantages of multi-sensor detection to ensure the safety of urban traffic, promoting the accuracy and robustness of target detection and tracking.","A method for target detection using Radar-Vision Fusion Path Aggregation Fully Convolutional One-Stage Network (RV-PAFCOS) is proposed in this paper, which is extended from Fully Convolutional One-Stage Network (FCOS) by introducing the modules of radar image processing branches, radar-vision fusion and path aggregation.","The radar image processing branch mainly focuses on the image modeling based on the spatiotemporal calibration of millimeter-wave (mmw) radar and cameras, taking the conversion of radar point clouds to radar images.","The fusion module extracts features of radar and optical images based on the principle of spatial attention stitching criterion.","The path aggregation module enhances the reuse of feature layers, combining the positional information of shallow feature maps with deep semantic information, to obtain better detection performance for both large and small targets.","Through the experimental analysis, the method proposed in this paper can effectively fuse the mmw radar and vision perceptions, showing good performance in traffic target detection."],"url":"http://arxiv.org/abs/2403.16476v1","category":"eess.IV"}
{"created":"2024-03-25 06:55:21","title":"Inferring system parameters from the bursts of the accretion-powered pulsar IGR J17498-2921","abstract":"Thermonuclear (type-I) bursts exhibit properties that depend both on the local surface conditions of the neutron stars on which they ignite, as well as the physical parameters of the host binary system. However, constraining the system parameters requires a comprehensive method to compare the observed bursts to simulations. We have further developed the beansp code for this purpose and analysed the bursts observed from IGR J17498-2921, a 401-Hz accretion-powered pulsar, discovered during it's 2011 outburst. We find good agreement with a model having H-deficient fuel with X = 0.15 +/- 0.4, and CNO metallicity about a tenth of the solar value. The model has the system at a distance of 5.7^{+0.6}_{-0.5} kpc, with a massive (approx. 2 M_sun) neutron star and a likely inclination of 60 deg. We also re-analysed the data from the 2002 outburst of the accretion-powered millisecond pulsar SAX J1808.4-3658. For that system we find a substantially closer distance than previously inferred, at 2.7 +/- 0.3 kpc, likely driven by a larger degree of burst emission anisotropy. The other system parameters are largely consistent with the previous analysis. We briefly discuss the implications for the evolution of these two systems.","sentences":["Thermonuclear (type-I) bursts exhibit properties that depend both on the local surface conditions of the neutron stars on which they ignite, as well as the physical parameters of the host binary system.","However, constraining the system parameters requires a comprehensive method to compare the observed bursts to simulations.","We have further developed the beansp code for this purpose and analysed the bursts observed from IGR J17498-2921, a 401-Hz accretion-powered pulsar, discovered during it's 2011 outburst.","We find good agreement with a model having H-deficient fuel with X = 0.15 +/- 0.4, and CNO metallicity about a tenth of the solar value.","The model has the system at a distance of 5.7^{+0.6}_{-0.5} kpc, with a massive (approx.","2 M_sun) neutron star and a likely inclination of 60 deg.","We also re-analysed the data from the 2002 outburst of the accretion-powered millisecond pulsar SAX J1808.4-3658.","For that system we find a substantially closer distance than previously inferred, at 2.7 +/- 0.3 kpc, likely driven by a larger degree of burst emission anisotropy.","The other system parameters are largely consistent with the previous analysis.","We briefly discuss the implications for the evolution of these two systems."],"url":"http://arxiv.org/abs/2403.16471v1","category":"astro-ph.HE"}
{"created":"2024-03-25 06:52:26","title":"Data-Driven Extrusion Force Control Tuning for 3D Printing","abstract":"The quality of 3D prints often varies due to different conditions inherent to each print, such as filament type, print speed, and nozzle size. Closed-loop process control methods improve the accuracy and repeatability of 3D prints. However, optimal tuning of controllers for given process parameters and design geometry is often a challenge with manually tuned controllers resulting in inconsistent and suboptimal results. This work employs Bayesian optimization to identify the optimal controller parameters. Additionally, we explore transfer learning in the context of 3D printing by leveraging prior information from past trials. By integrating optimized extrusion force control and transfer learning, we provide a novel framework for closed-loop 3D printing and propose an automated calibration routine that produces high-quality prints for a desired combination of print settings, material, and shape.","sentences":["The quality of 3D prints often varies due to different conditions inherent to each print, such as filament type, print speed, and nozzle size.","Closed-loop process control methods improve the accuracy and repeatability of 3D prints.","However, optimal tuning of controllers for given process parameters and design geometry is often a challenge with manually tuned controllers resulting in inconsistent and suboptimal results.","This work employs Bayesian optimization to identify the optimal controller parameters.","Additionally, we explore transfer learning in the context of 3D printing by leveraging prior information from past trials.","By integrating optimized extrusion force control and transfer learning, we provide a novel framework for closed-loop 3D printing and propose an automated calibration routine that produces high-quality prints for a desired combination of print settings, material, and shape."],"url":"http://arxiv.org/abs/2403.16470v1","category":"math.OC"}
{"created":"2024-03-25 06:48:56","title":"Unified Integrated Sensing and Communication Signal Design: A Sphere Packing Perspective","abstract":"The design of communication signal sets is fundamentally a sphere packing problem. It aims to identify a set of M points in an N -dimensional space, with the objective of maximizing the separability of points that represent different bits.In contrast, signals used for sensing targets should ideally be asdeterministic as possible. This paper explores the inherent conflict and trade-off between communication and sensing when these functions are combined within the same signal set. We present a unified approach to signal design in the time, frequency, and space domains for integrated sensing and communication (ISAC), framing it as a modified sphere packing problem. Through adept formula manipulation, this problem is transformed into a large-scale quadratic constrained quadratic programming (QCQP) challenge. We propose an augmented Lagrangian and dual ascent (ALDA) algorithm for iterative problem-solving. The computational complexity of this approach is analyzed and found to be daunting for large, high-dimensional signal set designs. To address this, we introduce a bit-dimension-power splitting (BDPS) method. This method decomposes the large-scale QCQP into a series of smaller-scale problems that can be solved more efficiently and in parallel, significantly reducing the overall computational load. Extensive simulations have been conducted to validate the effectiveness of our proposed signal design methods in the context of ISAC.","sentences":["The design of communication signal sets is fundamentally a sphere packing problem.","It aims to identify a set of M points in an N -dimensional space, with the objective of maximizing the separability of points that represent different bits.","In contrast, signals used for sensing targets should ideally be asdeterministic as possible.","This paper explores the inherent conflict and trade-off between communication and sensing when these functions are combined within the same signal set.","We present a unified approach to signal design in the time, frequency, and space domains for integrated sensing and communication (ISAC), framing it as a modified sphere packing problem.","Through adept formula manipulation, this problem is transformed into a large-scale quadratic constrained quadratic programming (QCQP) challenge.","We propose an augmented Lagrangian and dual ascent (ALDA) algorithm for iterative problem-solving.","The computational complexity of this approach is analyzed and found to be daunting for large, high-dimensional signal set designs.","To address this, we introduce a bit-dimension-power splitting (BDPS) method.","This method decomposes the large-scale QCQP into a series of smaller-scale problems that can be solved more efficiently and in parallel, significantly reducing the overall computational load.","Extensive simulations have been conducted to validate the effectiveness of our proposed signal design methods in the context of ISAC."],"url":"http://arxiv.org/abs/2403.16468v1","category":"eess.SP"}
{"created":"2024-03-25 06:44:15","title":"Unbiased Extremum Seeking for PDEs","abstract":"There have been recent efforts that combine seemingly disparate methods, extremum seeking (ES) optimization and partial differential equation (PDE) backstepping, to address the problem of model-free optimization with PDE actuator dynamics. In contrast to prior PDE-compensating ES designs, which only guarantee local stability around the extremum, we introduce unbiased ES that compensates for delay and diffusion PDE dynamics while ensuring exponential and unbiased convergence to the optimum. Our method leverages exponentially decaying/growing signals within the modulation/demodulation stages and carefully selected design parameters. The stability analysis of our designs relies on a state transformation, infinite-dimensional averaging, local exponential stability of the averaged system, local stability of the transformed system, and local exponential stability of the original system. Numerical simulations are presented to demonstrate the efficacy of the developed designs.","sentences":["There have been recent efforts that combine seemingly disparate methods, extremum seeking (ES) optimization and partial differential equation (PDE) backstepping, to address the problem of model-free optimization with PDE actuator dynamics.","In contrast to prior PDE-compensating ES designs, which only guarantee local stability around the extremum, we introduce unbiased ES that compensates for delay and diffusion PDE dynamics while ensuring exponential and unbiased convergence to the optimum.","Our method leverages exponentially decaying/growing signals within the modulation/demodulation stages and carefully selected design parameters.","The stability analysis of our designs relies on a state transformation, infinite-dimensional averaging, local exponential stability of the averaged system, local stability of the transformed system, and local exponential stability of the original system.","Numerical simulations are presented to demonstrate the efficacy of the developed designs."],"url":"http://arxiv.org/abs/2403.16462v1","category":"math.OC"}
{"created":"2024-03-25 06:43:45","title":"Recent Advances on Transition-Metal-Based Layered Double Hydroxides Nanosheets for Electrocatalytic Energy Conversion","abstract":"Transition-metal-based layered double hydroxides (TM-LDHs) nanosheets are promising electrocatalysts in the renewable electrochemical energy conversion system, which are regarded as alternatives to noble metal-based materials. In this review, recent advances on effective and facile strategies to rationally design TM-LDHs nanosheets as electrocatalysts, such as increasing the number of active sties, improving the utilization of active sites (atomic-scale catalysts), modulating the electron configurations, and controlling the lattice facets, are summarized and compared. Then, the utilization of these fabricated TM-LDHs nanosheets for oxygen evolution reaction, hydrogen evolution reaction, urea oxidation reaction, nitrogen reduction reaction, small molecule oxidations, and biomass derivatives upgrading is articulated through systematically discussing the corresponding fundamental design principles and reaction mechanism. Finally, the existing challenges in increasing the density of catalytically active sites and future prospects of TM-LDHs nanosheets-based electrocatalysts in each application are also commented.","sentences":["Transition-metal-based layered double hydroxides (TM-LDHs) nanosheets are promising electrocatalysts in the renewable electrochemical energy conversion system, which are regarded as alternatives to noble metal-based materials.","In this review, recent advances on effective and facile strategies to rationally design TM-LDHs nanosheets as electrocatalysts, such as increasing the number of active sties, improving the utilization of active sites (atomic-scale catalysts), modulating the electron configurations, and controlling the lattice facets, are summarized and compared.","Then, the utilization of these fabricated TM-LDHs nanosheets for oxygen evolution reaction, hydrogen evolution reaction, urea oxidation reaction, nitrogen reduction reaction, small molecule oxidations, and biomass derivatives upgrading is articulated through systematically discussing the corresponding fundamental design principles and reaction mechanism.","Finally, the existing challenges in increasing the density of catalytically active sites and future prospects of TM-LDHs nanosheets-based electrocatalysts in each application are also commented."],"url":"http://arxiv.org/abs/2403.16461v1","category":"physics.app-ph"}
{"created":"2024-03-25 06:31:17","title":"Single-Carrier Delay-Doppler Domain Equalization","abstract":"For doubly-selective channels, delay-Doppler (DD) modulation, mostly known as orthogonal time frequency space (OTFS) modulation, enables simultaneous compensation of delay and Doppler shifts. However, OTFS modulated signal has high peak-to-average power ratio (PAPR) because of its precoding operation performed over the DD domain. In order to deal with this problem, we propose a single-carrier transmission with delay-Doppler domain equalization (SC-DDE). In this system, the discretized time-domain SC signal is converted to the DD domain by discrete Zak transform (DZT) at the receiver side, followed by delay-Doppler domain equalization (DDE). Since equalization is performed in the DD domain, the SC-DDE receiver should acquire the channel delay-Doppler response. To this end, we introduce an embedded pilot-aided channel estimation scheme designed for SC-DDE, which does not affect the peak power property of transmitted signals. Through computer simulation, distribution of PAPR and bit error rate (BER) performance of the proposed system are compared with those of the conventional OTFS and SC with frequency-domain equalization (SC-FDE). As a result, our proposed SC-DDE significantly outperforms SC-FDE in terms of BER at the expense of additional computational complexity at the receiver. Furthermore, SC-DDE shows much lower PAPR than OTFS even though they achieve comparable coded BER performance.","sentences":["For doubly-selective channels, delay-Doppler (DD) modulation, mostly known as orthogonal time frequency space (OTFS) modulation, enables simultaneous compensation of delay and Doppler shifts.","However, OTFS modulated signal has high peak-to-average power ratio (PAPR) because of its precoding operation performed over the DD domain.","In order to deal with this problem, we propose a single-carrier transmission with delay-Doppler domain equalization (SC-DDE).","In this system, the discretized time-domain SC signal is converted to the DD domain by discrete Zak transform (DZT) at the receiver side, followed by delay-Doppler domain equalization (DDE).","Since equalization is performed in the DD domain, the SC-DDE receiver should acquire the channel delay-Doppler response.","To this end, we introduce an embedded pilot-aided channel estimation scheme designed for SC-DDE, which does not affect the peak power property of transmitted signals.","Through computer simulation, distribution of PAPR and bit error rate (BER) performance of the proposed system are compared with those of the conventional OTFS and SC with frequency-domain equalization (SC-FDE).","As a result, our proposed SC-DDE significantly outperforms SC-FDE in terms of BER at the expense of additional computational complexity at the receiver.","Furthermore, SC-DDE shows much lower PAPR than OTFS even though they achieve comparable coded BER performance."],"url":"http://arxiv.org/abs/2403.16453v1","category":"cs.IT"}
{"created":"2024-03-25 05:29:51","title":"Highly dispersed Ru nanoparticles anchored on NiAl layered double oxides catalyst for selective hydrodeoxygenation of vanillin","abstract":"The hydrodeoxygenation (HDO) of lignin-derived feedstocks into value-added chemicals with high efficiency and selectivity is desirable for the utilization of biomass resource. The complex oxygen-containing groups of lignin-derived substance result in the challenge of the low selectivity toward the required product. In this work, highly dispersed Ru nanoparticles anchored on Ni3Al1 layered double oxides (LDOs) catalyst derived from NiAl layered double hydroxides (LDHs) with flower-shaped morphology was constructed by a simple deposition-reduction method. The introduction of LDHs-derived support can significantly impact the catalytic activity for the HDO of lignin-derived vanillin (VL) into 2-methoxy-4-methylphenol (MMP). The Ru/Ni3Al1-400 catalyst obtained complete conversion of VL and 94.2% yield of MMP at 130 {\\deg}C in methanol solvent, much better than the catalysts without LDHs-derived support. The methanol solvent is beneficial for the conversion of reaction intermediate of vanillin alcohol (VA). Detailed characterization reveals that the existence of the enhanced metal-support interaction over Ru/Ni3Al1-400 and the easily accessible acid sites facilitate the production of MMP.","sentences":["The hydrodeoxygenation (HDO) of lignin-derived feedstocks into value-added chemicals with high efficiency and selectivity is desirable for the utilization of biomass resource.","The complex oxygen-containing groups of lignin-derived substance result in the challenge of the low selectivity toward the required product.","In this work, highly dispersed Ru nanoparticles anchored on Ni3Al1 layered double oxides (LDOs) catalyst derived from NiAl layered double hydroxides (LDHs) with flower-shaped morphology was constructed by a simple deposition-reduction method.","The introduction of LDHs-derived support can significantly impact the catalytic activity for the HDO of lignin-derived vanillin (VL) into 2-methoxy-4-methylphenol (MMP).","The Ru/Ni3Al1-400 catalyst obtained complete conversion of VL and 94.2% yield of MMP at 130 {\\deg}C in methanol solvent, much better than the catalysts without LDHs-derived support.","The methanol solvent is beneficial for the conversion of reaction intermediate of vanillin alcohol (VA).","Detailed characterization reveals that the existence of the enhanced metal-support interaction over Ru/Ni3Al1-400 and the easily accessible acid sites facilitate the production of MMP."],"url":"http://arxiv.org/abs/2403.16433v1","category":"physics.chem-ph"}
{"created":"2024-03-25 05:21:19","title":"AeroBridge: Autonomous Drone Handoff System for Emergency Battery Service","abstract":"This paper proposes an Emergency Battery Service (EBS) for drones in which an EBS drone flies to a drone in the field with a depleted battery and transfers a fresh battery to the exhausted drone. The authors present a unique battery transfer mechanism and drone localization that uses the Cross Marker Position (CMP) method. The main challenges include a stable and balanced transfer that precisely localizes the receiver drone. The proposed EBS drone mitigates the effects of downwash due to the vertical proximity between the drones by implementing diagonal alignment with the receiver, reducing the distance to 0.5 m between the two drones. CFD analysis shows that diagonal instead of perpendicular alignment minimizes turbulence, and the authors verify the actual system for change in output airflow and thrust measurements. The CMP marker-based localization method enables position lock for the EBS drone with up to 0.9 cm accuracy. The performance of the transfer mechanism is validated experimentally by successful mid-air transfer in 5 seconds, where the EBS drone is within 0.5 m vertical distance from the receiver drone, wherein 4m/s turbulence does not affect the transfer process.","sentences":["This paper proposes an Emergency Battery Service (EBS) for drones in which an EBS drone flies to a drone in the field with a depleted battery and transfers a fresh battery to the exhausted drone.","The authors present a unique battery transfer mechanism and drone localization that uses the Cross Marker Position (CMP) method.","The main challenges include a stable and balanced transfer that precisely localizes the receiver drone.","The proposed EBS drone mitigates the effects of downwash due to the vertical proximity between the drones by implementing diagonal alignment with the receiver, reducing the distance to 0.5 m between the two drones.","CFD analysis shows that diagonal instead of perpendicular alignment minimizes turbulence, and the authors verify the actual system for change in output airflow and thrust measurements.","The CMP marker-based localization method enables position lock for the EBS drone with up to 0.9 cm accuracy.","The performance of the transfer mechanism is validated experimentally by successful mid-air transfer in 5 seconds, where the EBS drone is within 0.5 m vertical distance from the receiver drone, wherein 4m/s turbulence does not affect the transfer process."],"url":"http://arxiv.org/abs/2403.16430v1","category":"cs.RO"}
{"created":"2024-03-25 05:11:04","title":"Nonlinear Quantum Dynamics in Superconducting NISQ Processors","abstract":"A recently proposed variational quantum algorithm has expanded the horizon of variational quantum computing to nonlinear physics and fluid dynamics. In this work, we employ this algorithm to find the ground state of the nonlinear Schr\\\"{o}dinger equation with a quadratic potential and implement it on the cloud superconducting quantum processors. We analyze the expressivity of real-amplitude ansatz to capture the ground state of the nonlinear system across various interaction regimes characterized by varying strengths of nonlinearity. Our investigation reveals that although quantum hardware noise impairs the evaluation of the energy cost function, small instances of the problem consistently converge to the ground state. We implement a variety of problem instances on IBM Q devices and report analogous discrepancies in the energy cost function evaluation attributable to quantum hardware noise. The latter are absent in the state fidelity estimation. Our comprehensive analysis offers valuable insights into the practical implementation and advancement of the variational algorithms for nonlinear quantum dynamics.","sentences":["A recently proposed variational quantum algorithm has expanded the horizon of variational quantum computing to nonlinear physics and fluid dynamics.","In this work, we employ this algorithm to find the ground state of the nonlinear Schr\\\"{o}dinger equation with a quadratic potential and implement it on the cloud superconducting quantum processors.","We analyze the expressivity of real-amplitude ansatz to capture the ground state of the nonlinear system across various interaction regimes characterized by varying strengths of nonlinearity.","Our investigation reveals that although quantum hardware noise impairs the evaluation of the energy cost function, small instances of the problem consistently converge to the ground state.","We implement a variety of problem instances on IBM Q devices and report analogous discrepancies in the energy cost function evaluation attributable to quantum hardware noise.","The latter are absent in the state fidelity estimation.","Our comprehensive analysis offers valuable insights into the practical implementation and advancement of the variational algorithms for nonlinear quantum dynamics."],"url":"http://arxiv.org/abs/2403.16426v1","category":"quant-ph"}
{"created":"2024-03-25 04:11:52","title":"A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups","abstract":"Stochastic inference on Lie groups plays a key role in state estimation problems, such as inertial navigation, visual inertial odometry, pose estimation in virtual reality, etc. A key problem is fusing independent concentrated Gaussian distributions defined at different reference points on the group. In this paper we approximate distributions at different points in the group in a single set of exponential coordinates and then use classical Gaussian fusion to obtain the fused posteriori in those coordinates. We consider several approximations including the exact Jacobian of the change of coordinate map, first and second order Taylor's expansions of the Jacobian, and parallel transport with and without curvature correction associated with the underlying geometry of the Lie group. Preliminary results on SO(3) demonstrate that a novel approximation using parallel transport with curvature correction achieves similar accuracy to the state-of-the-art optimisation based algorithms at a fraction of the computational cost.","sentences":["Stochastic inference on Lie groups plays a key role in state estimation problems, such as inertial navigation, visual inertial odometry, pose estimation in virtual reality, etc.","A key problem is fusing independent concentrated Gaussian distributions defined at different reference points on the group.","In this paper we approximate distributions at different points in the group in a single set of exponential coordinates and then use classical Gaussian fusion to obtain the fused posteriori in those coordinates.","We consider several approximations including the exact Jacobian of the change of coordinate map, first and second order Taylor's expansions of the Jacobian, and parallel transport with and without curvature correction associated with the underlying geometry of the Lie group.","Preliminary results on SO(3) demonstrate that a novel approximation using parallel transport with curvature correction achieves similar accuracy to the state-of-the-art optimisation based algorithms at a fraction of the computational cost."],"url":"http://arxiv.org/abs/2403.16411v1","category":"eess.SY"}
{"created":"2024-03-25 03:38:23","title":"A Distributionally Robust Model Predictive Control for Static and Dynamic Uncertainties in Smart Grids","abstract":"The integration of various power sources, including renewables and electric vehicles, into smart grids is expanding, introducing uncertainties that can result in issues like voltage imbalances, load fluctuations, and power losses. These challenges negatively impact the reliability and stability of online scheduling in smart grids. Existing research often addresses uncertainties affecting current states but overlooks those that impact future states, such as the unpredictable charging patterns of electric vehicles. To distinguish between these, we term them static uncertainties and dynamic uncertainties, respectively. This paper introduces WDR-MPC, a novel approach that stands for two-stage Wasserstein-based Distributionally Robust (WDR) optimization within a Model Predictive Control (MPC) framework, aimed at effectively managing both types of uncertainties in smart grids. The dynamic uncertainties are first reformulated into ambiguity tubes and then the distributionally robust bounds of both dynamic and static uncertainties can be established using WDR optimization. By employing ambiguity tubes and WDR optimization, the stochastic MPC system is converted into a nominal one. Moreover, we develop a convex reformulation method to speed up WDR computation during the two-stage optimization. The distinctive contribution of this paper lies in its holistic approach to both static and dynamic uncertainties in smart grids. Comprehensive experiment results on IEEE 38-bus and 94-bus systems reveal the method's superior performance and the potential to enhance grid stability and reliability.","sentences":["The integration of various power sources, including renewables and electric vehicles, into smart grids is expanding, introducing uncertainties that can result in issues like voltage imbalances, load fluctuations, and power losses.","These challenges negatively impact the reliability and stability of online scheduling in smart grids.","Existing research often addresses uncertainties affecting current states but overlooks those that impact future states, such as the unpredictable charging patterns of electric vehicles.","To distinguish between these, we term them static uncertainties and dynamic uncertainties, respectively.","This paper introduces WDR-MPC, a novel approach that stands for two-stage Wasserstein-based Distributionally Robust (WDR) optimization within a Model Predictive Control (MPC) framework, aimed at effectively managing both types of uncertainties in smart grids.","The dynamic uncertainties are first reformulated into ambiguity tubes and then the distributionally robust bounds of both dynamic and static uncertainties can be established using WDR optimization.","By employing ambiguity tubes and WDR optimization, the stochastic MPC system is converted into a nominal one.","Moreover, we develop a convex reformulation method to speed up WDR computation during the two-stage optimization.","The distinctive contribution of this paper lies in its holistic approach to both static and dynamic uncertainties in smart grids.","Comprehensive experiment results on IEEE 38-bus and 94-bus systems reveal the method's superior performance and the potential to enhance grid stability and reliability."],"url":"http://arxiv.org/abs/2403.16402v1","category":"eess.SY"}
{"created":"2024-03-25 03:28:19","title":"Transient Waiting Time Distributions in Small Call Centres with Skills-Based Routing","abstract":"Many call centres are subject to service level agreements that stipulate that they must achieve targets in terms of the proportion of calls that are answered within a specified time. In order to manage a centre so that targets like these are met, we need to have a method of calculating the waiting time distributions experienced by customers. In this paper, we provide such a method for small call centres that employ skills-based routing. We first build the methodology for the single-skill case and then extend it to a multi-skill case. We model the call centre system as a continuous-time Markov chain and then make use of the Laplace transform to calculate the relevant quantities. We later demonstrate the use of this method to find the optimal routing policy in a certain class of policies.","sentences":["Many call centres are subject to service level agreements that stipulate that they must achieve targets in terms of the proportion of calls that are answered within a specified time.","In order to manage a centre so that targets like these are met, we need to have a method of calculating the waiting time distributions experienced by customers.","In this paper, we provide such a method for small call centres that employ skills-based routing.","We first build the methodology for the single-skill case and then extend it to a multi-skill case.","We model the call centre system as a continuous-time Markov chain and then make use of the Laplace transform to calculate the relevant quantities.","We later demonstrate the use of this method to find the optimal routing policy in a certain class of policies."],"url":"http://arxiv.org/abs/2403.16399v1","category":"math.PR"}
{"created":"2024-03-25 03:14:39","title":"Follow-up LOFAR observations of the $\u03c4$ Bo\u00f6tis exoplanetary system","abstract":"Context. Observing the radio emission from exoplanets is among the most promising methods to detect their magnetic fields and a measurement of an exoplanetary magnetic field will help constrain the planet's interior structure, star-planet interactions, atmospheric escape and dynamics, and habitability. Recently, circularly polarized bursty and slow emission from the $\\tau$ Bo\\\"{o}tis ($\\tau$ Boo) exoplanetary system was tentatively detected using LOFAR (LOW-Frequency ARray) beamformed observations. If confirmed, this detection will be a major contribution to exoplanet science. However, follow-up observations are required to confirm this detection.   Aims. Here, we present such follow-up observations of the $\\tau$ Boo system using LOFAR. These observations cover 70$\\%$ of the orbital period of $\\tau$ Boo b including the orbital phases of the previous tentative detections.   Methods. We used the BOREALIS pipeline to mitigate radio frequency interference and to search for bursty and slowing varying radio signals. BOREALIS was previously used to find the tentative radio signals from $\\tau$ Boo.   Results. Our new observations do not show any signs of bursty or slow emission from the $\\tau$ Bo\\\"{o}tis exoplanetary system.   Conclusions. The cause for our non-detection is currently degenerate. It is possible that the tentative radio signals were an unknown instrumental systematic or that we are observing variability in the planetary radio emission due to changes in its host star. More radio data (preferably multi-site) and ancillary observations (e.g. magnetic maps) are required to further investigate the potential radio emission from the $\\tau$ Bo\\\"{o}tis exoplanetary system.","sentences":["Context.","Observing the radio emission from exoplanets is among the most promising methods to detect their magnetic fields and a measurement of an exoplanetary magnetic field will help constrain the planet's interior structure, star-planet interactions, atmospheric escape and dynamics, and habitability.","Recently, circularly polarized bursty and slow emission from the $\\tau$ Bo\\\"{o}tis ($\\tau$ Boo) exoplanetary system was tentatively detected using LOFAR (LOW-Frequency ARray) beamformed observations.","If confirmed, this detection will be a major contribution to exoplanet science.","However, follow-up observations are required to confirm this detection.   ","Aims.","Here, we present such follow-up observations of the $\\tau$ Boo system using LOFAR.","These observations cover 70$\\%$ of the orbital period of $\\tau$ Boo b including the orbital phases of the previous tentative detections.   Methods.","We used the BOREALIS pipeline to mitigate radio frequency interference and to search for bursty and slowing varying radio signals.","BOREALIS was previously used to find the tentative radio signals from $\\tau$ Boo.   Results.","Our new observations do not show any signs of bursty or slow emission from the $\\tau$ Bo\\\"{o}tis exoplanetary system.   Conclusions.","The cause for our non-detection is currently degenerate.","It is possible that the tentative radio signals were an unknown instrumental systematic or that we are observing variability in the planetary radio emission due to changes in its host star.","More radio data (preferably multi-site) and ancillary observations (e.g. magnetic maps) are required to further investigate the potential radio emission from the $\\tau$ Bo\\\"{o}tis exoplanetary system."],"url":"http://arxiv.org/abs/2403.16392v1","category":"astro-ph.EP"}
{"created":"2024-03-25 03:13:56","title":"Physics-informed RL for Maximal Safety Probability Estimation","abstract":"Accurate risk quantification and reachability analysis are crucial for safe control and learning, but sampling from rare events, risky states, or long-term trajectories can be prohibitively costly. Motivated by this, we study how to estimate the long-term safety probability of maximally safe actions without sufficient coverage of samples from risky states and long-term trajectories. The use of maximal safety probability in control and learning is expected to avoid conservative behaviors due to over-approximation of risk. Here, we first show that long-term safety probability, which is multiplicative in time, can be converted into additive costs and be solved using standard reinforcement learning methods. We then derive this probability as solutions of partial differential equations (PDEs) and propose Physics-Informed Reinforcement Learning (PIRL) algorithm. The proposed method can learn using sparse rewards because the physics constraints help propagate risk information through neighbors. This suggests that, for the purpose of extracting more information for efficient learning, physics constraints can serve as an alternative to reward shaping. The proposed method can also estimate long-term risk using short-term samples and deduce the risk of unsampled states. This feature is in stark contrast with the unconstrained deep RL that demands sufficient data coverage. These merits of the proposed method are demonstrated in numerical simulation.","sentences":["Accurate risk quantification and reachability analysis are crucial for safe control and learning, but sampling from rare events, risky states, or long-term trajectories can be prohibitively costly.","Motivated by this, we study how to estimate the long-term safety probability of maximally safe actions without sufficient coverage of samples from risky states and long-term trajectories.","The use of maximal safety probability in control and learning is expected to avoid conservative behaviors due to over-approximation of risk.","Here, we first show that long-term safety probability, which is multiplicative in time, can be converted into additive costs and be solved using standard reinforcement learning methods.","We then derive this probability as solutions of partial differential equations (PDEs) and propose Physics-Informed Reinforcement Learning (PIRL) algorithm.","The proposed method can learn using sparse rewards because the physics constraints help propagate risk information through neighbors.","This suggests that, for the purpose of extracting more information for efficient learning, physics constraints can serve as an alternative to reward shaping.","The proposed method can also estimate long-term risk using short-term samples and deduce the risk of unsampled states.","This feature is in stark contrast with the unconstrained deep RL that demands sufficient data coverage.","These merits of the proposed method are demonstrated in numerical simulation."],"url":"http://arxiv.org/abs/2403.16391v1","category":"eess.SY"}
{"created":"2024-03-25 03:13:30","title":"Measurement of Out-of-Plane first-order Displacement Derivatives in Orthogonal shear directions Using Dichroic Mirrors","abstract":"This paper proposed a novel and temporal phase-shift digital shearography system for simultaneous measurement of first order displacement derivative in orthogonal shear directions. Dual lasers with wavelengths of 532nm and 637nm, three splitter prism structure, two dichroic mirrors with different response wavelength, and the color CMOS are used in the system. Two dichroic mirrors can be used as shear mirrors to realize shear in orthogonal directions at the same time. The system realizes the measurement of the first-order displacement derivative information in the orthogonal direction of the round metal aluminum plate of diameter 250mm. The experimental results show that the overall displacement integral PV error in the x and y directions is 2.7%-14.8%, which verified the reliability of the system.","sentences":["This paper proposed a novel and temporal phase-shift digital shearography system for simultaneous measurement of first order displacement derivative in orthogonal shear directions.","Dual lasers with wavelengths of 532nm and 637nm, three splitter prism structure, two dichroic mirrors with different response wavelength, and the color CMOS are used in the system.","Two dichroic mirrors can be used as shear mirrors to realize shear in orthogonal directions at the same time.","The system realizes the measurement of the first-order displacement derivative information in the orthogonal direction of the round metal aluminum plate of diameter 250mm.","The experimental results show that the overall displacement integral PV error in the x and y directions is 2.7%-14.8%, which verified the reliability of the system."],"url":"http://arxiv.org/abs/2403.16390v1","category":"physics.optics"}
{"created":"2024-03-25 03:00:33","title":"An image-computable model of speeded decision-making","abstract":"Evidence accumulation models (EAMs) are the dominant framework for modeling response time (RT) data from speeded decision-making tasks. While providing a good quantitative description of RT data in terms of abstract perceptual representations, EAMs do not explain how the visual system extracts these representations in the first place. To address this limitation, we introduce the visual accumulator model (VAM), in which convolutional neural network models of visual processing and traditional EAMs are jointly fitted to trial-level RTs and raw (pixel-space) visual stimuli from individual subjects. Models fitted to large-scale cognitive training data from a stylized flanker task captured individual differences in congruency effects, RTs, and accuracy. We find evidence that the selection of task-relevant information occurs through the orthogonalization of relevant and irrelevant representations, demonstrating how our framework can be used to relate visual representations to behavioral outputs. Together, our work provides a probabilistic framework for both constraining neural network models of vision with behavioral data and studying how the visual system extracts representations that guide decisions.","sentences":["Evidence accumulation models (EAMs) are the dominant framework for modeling response time (RT) data from speeded decision-making tasks.","While providing a good quantitative description of RT data in terms of abstract perceptual representations, EAMs do not explain how the visual system extracts these representations in the first place.","To address this limitation, we introduce the visual accumulator model (VAM), in which convolutional neural network models of visual processing and traditional EAMs are jointly fitted to trial-level RTs and raw (pixel-space) visual stimuli from individual subjects.","Models fitted to large-scale cognitive training data from a stylized flanker task captured individual differences in congruency effects, RTs, and accuracy.","We find evidence that the selection of task-relevant information occurs through the orthogonalization of relevant and irrelevant representations, demonstrating how our framework can be used to relate visual representations to behavioral outputs.","Together, our work provides a probabilistic framework for both constraining neural network models of vision with behavioral data and studying how the visual system extracts representations that guide decisions."],"url":"http://arxiv.org/abs/2403.16382v1","category":"q-bio.NC"}
{"created":"2024-03-25 02:56:31","title":"Augmented Lagrangian method for coupled-cluster","abstract":"We propose to improve the convergence properties of the single-reference coupled cluster (CC) method through an augmented Lagrangian formalism. The conventional CC method changes a linear high-dimensional eigenvalue problem with exponential size into a problem of determining the roots of a nonlinear system of equations that has a manageable size. However, current numerical procedures for solving this system of equations to get the lowest eigenvalue suffer from two practical issues: First, solving the CC equations may not converge, and second, when converging, they may converge to other -- potentially unphysical -- states, which are stationary points of the CC energy expression. We show that both issues can be dealt with when a suitably defined energy is minimized in addition to solving the original CC equations. We further propose an augmented Lagrangian method for coupled cluster (alm-CC) to solve the resulting constrained optimization problem. We numerically investigate the proposed augmented Lagrangian formulation showing that the convergence towards the ground state is significantly more stable and that the optimization procedure is less susceptible to local minima. Furthermore, the computational cost of alm-CC is comparable to the conventional CC method.","sentences":["We propose to improve the convergence properties of the single-reference coupled cluster (CC) method through an augmented Lagrangian formalism.","The conventional CC method changes a linear high-dimensional eigenvalue problem with exponential size into a problem of determining the roots of a nonlinear system of equations that has a manageable size.","However, current numerical procedures for solving this system of equations to get the lowest eigenvalue suffer from two practical issues: First, solving the CC equations may not converge, and second, when converging, they may converge to other -- potentially unphysical -- states, which are stationary points of the CC energy expression.","We show that both issues can be dealt with when a suitably defined energy is minimized in addition to solving the original CC equations.","We further propose an augmented Lagrangian method for coupled cluster (alm-CC) to solve the resulting constrained optimization problem.","We numerically investigate the proposed augmented Lagrangian formulation showing that the convergence towards the ground state is significantly more stable and that the optimization procedure is less susceptible to local minima.","Furthermore, the computational cost of alm-CC is comparable to the conventional CC method."],"url":"http://arxiv.org/abs/2403.16381v1","category":"physics.comp-ph"}
{"created":"2024-03-25 02:47:29","title":"Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes","abstract":"Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for prediction. Once trained, the model can encode an arbitrary number of observations without requiring retraining, enabling on-the-spot real-time predictions along with quantified uncertainty and can be readily updated as more online data is gathered. Furthermore, our model is designed to incorporate qualitative information (i.e., labels) from individual units. This integration not only enhances individualized predictions for each unit but also enables joint inference for both signals and their associated labels. Numerical studies on both synthetic and real-world data in reliability engineering highlight the advantageous features of our model in real-time adaptation, enhanced signal prediction with uncertainty quantification, and joint prediction for labels and signals.","sentences":["Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units.","Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings.","For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates.","However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals.","On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task.","In this paper, we propose a neural process-based approach that addresses this trade-off.","It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for prediction.","Once trained, the model can encode an arbitrary number of observations without requiring retraining, enabling on-the-spot real-time predictions along with quantified uncertainty and can be readily updated as more online data is gathered.","Furthermore, our model is designed to incorporate qualitative information (i.e., labels) from individual units.","This integration not only enhances individualized predictions for each unit but also enables joint inference for both signals and their associated labels.","Numerical studies on both synthetic and real-world data in reliability engineering highlight the advantageous features of our model in real-time adaptation, enhanced signal prediction with uncertainty quantification, and joint prediction for labels and signals."],"url":"http://arxiv.org/abs/2403.16377v1","category":"cs.LG"}
{"created":"2024-03-25 02:46:57","title":"Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion","abstract":"360 depth estimation has recently received great attention for 3D reconstruction owing to its omnidirectional field of view (FoV). Recent approaches are predominantly focused on cross-projection fusion with geometry-based re-projection: they fuse 360 images with equirectangular projection (ERP) and another projection type, e.g., cubemap projection to estimate depth with the ERP format. However, these methods suffer from 1) limited local receptive fields, making it hardly possible to capture large FoV scenes, and 2) prohibitive computational cost, caused by the complex cross-projection fusion module design. In this paper, we propose Elite360D, a novel framework that inputs the ERP image and icosahedron projection (ICOSAP) point set, which is undistorted and spatially continuous. Elite360D is superior in its capacity in learning a representation from a local-with-global perspective. With a flexible ERP image encoder, it includes an ICOSAP point encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M parameters). Specifically, the ERP image encoder can take various perspective image-trained backbones (e.g., ResNet, Transformer) to extract local features. The point encoder extracts the global features from the ICOSAP. Then, the B2F module captures the semantic- and distance-aware dependencies between each pixel of the ERP feature and the entire ICOSAP feature set. Without specific backbone design and obvious computational cost increase, Elite360D outperforms the prior arts on several benchmark datasets.","sentences":["360 depth estimation has recently received great attention for 3D reconstruction owing to its omnidirectional field of view (FoV).","Recent approaches are predominantly focused on cross-projection fusion with geometry-based re-projection: they fuse 360 images with equirectangular projection (ERP) and another projection type, e.g., cubemap projection to estimate depth with the ERP format.","However, these methods suffer from 1) limited local receptive fields, making it hardly possible to capture large FoV scenes, and 2) prohibitive computational cost, caused by the complex cross-projection fusion module design.","In this paper, we propose Elite360D, a novel framework that inputs the ERP image and icosahedron projection (ICOSAP) point set, which is undistorted and spatially continuous.","Elite360D is superior in its capacity in learning a representation from a local-with-global perspective.","With a flexible ERP image encoder, it includes an ICOSAP point encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M parameters).","Specifically, the ERP image encoder can take various perspective image-trained backbones (e.g., ResNet, Transformer) to extract local features.","The point encoder extracts the global features from the ICOSAP.","Then, the B2F module captures the semantic- and distance-aware dependencies between each pixel of the ERP feature and the entire ICOSAP feature set.","Without specific backbone design and obvious computational cost increase, Elite360D outperforms the prior arts on several benchmark datasets."],"url":"http://arxiv.org/abs/2403.16376v1","category":"cs.CV"}
{"created":"2024-03-25 02:38:34","title":"ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving","abstract":"Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation. In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth. Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component. Encouraging results were obtained in the challenging benchmarks.","sentences":["Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving.","Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation.","However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction.","In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints.","The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation.","In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth.","Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component.","Encouraging results were obtained in the challenging benchmarks."],"url":"http://arxiv.org/abs/2403.16374v1","category":"cs.LG"}
{"created":"2024-03-25 02:04:06","title":"SE(3) Linear Parameter Varying Dynamical Systems for Globally Asymptotically Stable End-Effector Control","abstract":"Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into an autonomous first-order DS that enables reactive responses to perturbations, while ensuring globally asymptotic stability at the target. However, the current LPV-DS framework is established on Euclidean data only and has not been applicable to broader robotic applications requiring pose control. In this paper we present an extension to the current LPV-DS framework, named Quaternion-DS, which efficiently learns a DS-based motion policy for orientation. Leveraging techniques from differential geometry and Riemannian statistics, our approach properly handles the non-Euclidean orientation data in quaternion space, enabling the integration with positional control, namely SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is preserved. Through simulation and real robot experiments, we validate our method, demonstrating its ability to efficiently and accurately reproduce the original SE(3) trajectory while exhibiting strong robustness to perturbations in task space.","sentences":["Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into an autonomous first-order DS that enables reactive responses to perturbations, while ensuring globally asymptotic stability at the target.","However, the current LPV-DS framework is established on Euclidean data only and has not been applicable to broader robotic applications requiring pose control.","In this paper we present an extension to the current LPV-DS framework, named Quaternion-DS, which efficiently learns a DS-based motion policy for orientation.","Leveraging techniques from differential geometry and Riemannian statistics, our approach properly handles the non-Euclidean orientation data in quaternion space, enabling the integration with positional control, namely SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is preserved.","Through simulation and real robot experiments, we validate our method, demonstrating its ability to efficiently and accurately reproduce the original SE(3) trajectory while exhibiting strong robustness to perturbations in task space."],"url":"http://arxiv.org/abs/2403.16366v1","category":"cs.RO"}
{"created":"2024-03-25 01:58:47","title":"On maximal subgroups of ample groups","abstract":"The paper is concerned with maximal subgroups of the ample (better known as topological full) groups of homeomorphisms of totally disconnected compact metrizable topological spaces. We describe all maximal subgroups that are stabilizers of finite sets. Under certain assumptions on the ample group (including minimality), we describe all maximal subgroups that are stabilizers of closed sets or stabilizers of partitions into clopen sets. In particular, our results apply to the ample groups associated with Cantor minimal systems.","sentences":["The paper is concerned with maximal subgroups of the ample (better known as topological full) groups of homeomorphisms of totally disconnected compact metrizable topological spaces.","We describe all maximal subgroups that are stabilizers of finite sets.","Under certain assumptions on the ample group (including minimality), we describe all maximal subgroups that are stabilizers of closed sets or stabilizers of partitions into clopen sets.","In particular, our results apply to the ample groups associated with Cantor minimal systems."],"url":"http://arxiv.org/abs/2403.16364v1","category":"math.GR"}
{"created":"2024-03-25 01:58:19","title":"AgentFL: Scaling LLM-based Fault Localization to Project-Level Context","abstract":"Fault Localization (FL) is an essential step during the debugging process. With the strong capabilities of code comprehension, the recent Large Language Models (LLMs) have demonstrated promising performance in diagnosing bugs in the code. Nevertheless, due to LLMs' limited performance in handling long contexts, existing LLM-based fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a large code scope (i.e., an entire software system). To address the limitation, this paper presents AgentFL, a multi-agent system based on ChatGPT for automated fault localization. By simulating the behavior of a human developer, AgentFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation. Within each step, AgentFL hires agents with diversified expertise, each of which utilizes different tools to handle specific tasks. Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step. The evaluation on the widely used Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395 bugs within Top-1, which outperforms the other LLM-based approaches and exhibits complementarity to the state-of-the-art learning-based techniques. Additionally, we confirm the indispensability of the components in AgentFL with the ablation study and demonstrate the usability of AgentFL through a user study. Finally, the cost analysis shows that AgentFL spends an average of only 0.074 dollars and 97 seconds for a single bug.","sentences":["Fault Localization (FL) is an essential step during the debugging process.","With the strong capabilities of code comprehension, the recent Large Language Models (LLMs) have demonstrated promising performance in diagnosing bugs in the code.","Nevertheless, due to LLMs' limited performance in handling long contexts, existing LLM-based fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a large code scope (i.e., an entire software system).","To address the limitation, this paper presents AgentFL, a multi-agent system based on ChatGPT for automated fault localization.","By simulating the behavior of a human developer, AgentFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation.","Within each step, AgentFL hires agents with diversified expertise, each of which utilizes different tools to handle specific tasks.","Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step.","The evaluation on the widely used Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395 bugs within Top-1, which outperforms the other LLM-based approaches and exhibits complementarity to the state-of-the-art learning-based techniques.","Additionally, we confirm the indispensability of the components in AgentFL with the ablation study and demonstrate the usability of AgentFL through a user study.","Finally, the cost analysis shows that AgentFL spends an average of only 0.074 dollars and 97 seconds for a single bug."],"url":"http://arxiv.org/abs/2403.16362v1","category":"cs.SE"}
{"created":"2024-03-25 01:53:01","title":"Le Conte de la Mesure sur les Complexes Cubiques CAT(0)","abstract":"We revisit the topic of probability measures on CAT(0) cube complexes and prove that an amenable group acting on a CAT(0) cube complex, regardless of dimension, necessarily preserves an interval in the Roller compactification. In the finite dimensional case, we prove that there must be an orbit of cardinality $2^N$, where $N$ is bounded by the dimension. This is a slight extension of the author's previous Tits' Alternative.","sentences":["We revisit the topic of probability measures on CAT(0) cube complexes and prove that an amenable group acting on a CAT(0) cube complex, regardless of dimension, necessarily preserves an interval in the Roller compactification.","In the finite dimensional case, we prove that there must be an orbit of cardinality $2^N$, where $N$ is bounded by the dimension.","This is a slight extension of the author's previous Tits' Alternative."],"url":"http://arxiv.org/abs/2403.16360v1","category":"math.GR"}
{"created":"2024-03-25 01:11:43","title":"Energy-Efficient Hybrid Beamforming with Dynamic On-off Control for Integrated Sensing, Communications, and Powering","abstract":"This paper investigates the energy-efficient hybrid beamforming design for a multi-functional integrated sensing, communications, and powering (ISCAP) system. In this system, a base station (BS) with a hybrid analog-digital (HAD) architecture sends unified wireless signals to communicate with multiple information receivers (IRs), sense multiple point targets, and wirelessly charge multiple energy receivers (ERs) at the same time. To facilitate the energy-efficient design, we present a novel HAD architecture for the BS transmitter, which allows dynamic on-off control of its radio frequency (RF) chains and analog phase shifters (PSs) through a switch network. We also consider a practical and comprehensive power consumption model for the BS, by taking into account the power-dependent non-linear power amplifier (PA) efficiency, and the on-off non-transmission power consumption model of RF chains and PSs. We jointly design the hybrid beamforming and dynamic on-off control at the BS, aiming to minimize its total power consumption, while guaranteeing the performance requirements on communication rates, sensing Cram\\'er-Rao bound (CRB), and harvested power levels. The formulation also takes into consideration the per-antenna transmit power constraint and the constant modulus constraints for the analog beamformer at the BS. The resulting optimization problem for ISCAP is highly non-convex. Please refer to the paper for a complete abstract.","sentences":["This paper investigates the energy-efficient hybrid beamforming design for a multi-functional integrated sensing, communications, and powering (ISCAP) system.","In this system, a base station (BS) with a hybrid analog-digital (HAD) architecture sends unified wireless signals to communicate with multiple information receivers (IRs), sense multiple point targets, and wirelessly charge multiple energy receivers (ERs) at the same time.","To facilitate the energy-efficient design, we present a novel HAD architecture for the BS transmitter, which allows dynamic on-off control of its radio frequency (RF) chains and analog phase shifters (PSs) through a switch network.","We also consider a practical and comprehensive power consumption model for the BS, by taking into account the power-dependent non-linear power amplifier (PA) efficiency, and the on-off non-transmission power consumption model of RF chains and PSs.","We jointly design the hybrid beamforming and dynamic on-off control at the BS, aiming to minimize its total power consumption, while guaranteeing the performance requirements on communication rates, sensing Cram\\'er-Rao bound (CRB), and harvested power levels.","The formulation also takes into consideration the per-antenna transmit power constraint and the constant modulus constraints for the analog beamformer at the BS.","The resulting optimization problem for ISCAP is highly non-convex.","Please refer to the paper for a complete abstract."],"url":"http://arxiv.org/abs/2403.16353v1","category":"cs.IT"}
{"created":"2024-03-25 01:02:55","title":"A note on the convergence of multigrid methods for the Riesz-space equation and an application to image deblurring","abstract":"In the past decades, a remarkable amount of research has been carried out regarding fast solvers for large linear systems resulting from various discretizations of fractional differential equations (FDEs). In the current work, we focus on multigrid methods for a Riesz-space FDE whose theoretical convergence analysis of such multigrids is currently limited to the two-grid method. Here we provide a detailed theoretical convergence study in the case of V-cycle and W-cycle. Moreover, we discuss its use combined with a band approximation and we compare the result with both $\\tau$ and circulant preconditionings. The numerical tests include 2D problems as well as the extension to the case of a Riesz-FDE with variable coefficients. Finally, we apply the best-performing method to an image deblurring problem with Tikhonov regularization.","sentences":["In the past decades, a remarkable amount of research has been carried out regarding fast solvers for large linear systems resulting from various discretizations of fractional differential equations (FDEs).","In the current work, we focus on multigrid methods for a Riesz-space FDE whose theoretical convergence analysis of such multigrids is currently limited to the two-grid method.","Here we provide a detailed theoretical convergence study in the case of V-cycle and W-cycle.","Moreover, we discuss its use combined with a band approximation and we compare the result with both $\\tau$ and circulant preconditionings.","The numerical tests include 2D problems as well as the extension to the case of a Riesz-FDE with variable coefficients.","Finally, we apply the best-performing method to an image deblurring problem with Tikhonov regularization."],"url":"http://arxiv.org/abs/2403.16352v1","category":"math.NA"}
{"created":"2024-03-25 00:41:46","title":"Percentile Optimization in Wireless Networks- Part II: Beamforming for Cell-Edge Throughput Maximization","abstract":"Part I of this two-part paper focused on the formulation of percentile problems, complexity analysis, and development of power control algorithms via the quadratic fractional transform (QFT) and logarithmic fractional transform (LFT) for sum-least-qth-percentile (SLqP) rate maximization problems. In this second part, we first tackle the significantly more challenging problems of optimizing SLqP rate via beamforming in a multiuser, multiple-input multiple-output (MU- MIMO) network to maximize cell-edge throughput. To this end, we first propose an adaptation of the QFT algorithm presented in Part I that enables optimization of the complex-valued multidimensional beamforming weights for the SLqP rate utility function. We also introduce a new class of problems which we term as sum-greatest-qth-percentile weighted mean squared error (SGqP-WMSE) minimization. We show that this class subsumes the well-known sum-weighted mean squared error (WMMSE) minimization and max-WMSE minimization problems. We demonstrate an equivalence between this class of problems and the SLqP rate maximization problems, and show that this correspondence can be exploited to obtain stationary-point solutions for the aforementioned beamforming problem. Next, we develop extensions for the QFT and LFT algorithms from Part I to optimize ergodic long-term average or ergodic SLqP utility. Finally, we also consider related problems which can be solved using the proposed techniques, including hybrid utility functions targeting optimization at specific subsets of users within cellular networks.","sentences":["Part I of this two-part paper focused on the formulation of percentile problems, complexity analysis, and development of power control algorithms via the quadratic fractional transform (QFT) and logarithmic fractional transform (LFT) for sum-least-qth-percentile (SLqP) rate maximization problems.","In this second part, we first tackle the significantly more challenging problems of optimizing SLqP rate via beamforming in a multiuser, multiple-input multiple-output (MU- MIMO) network to maximize cell-edge throughput.","To this end, we first propose an adaptation of the QFT algorithm presented in Part I that enables optimization of the complex-valued multidimensional beamforming weights for the SLqP rate utility function.","We also introduce a new class of problems which we term as sum-greatest-qth-percentile weighted mean squared error (SGqP-WMSE) minimization.","We show that this class subsumes the well-known sum-weighted mean squared error (WMMSE) minimization and max-WMSE minimization problems.","We demonstrate an equivalence between this class of problems and the SLqP rate maximization problems, and show that this correspondence can be exploited to obtain stationary-point solutions for the aforementioned beamforming problem.","Next, we develop extensions for the QFT and LFT algorithms from Part I to optimize ergodic long-term average or ergodic SLqP utility.","Finally, we also consider related problems which can be solved using the proposed techniques, including hybrid utility functions targeting optimization at specific subsets of users within cellular networks."],"url":"http://arxiv.org/abs/2403.16343v1","category":"cs.IT"}
{"created":"2024-03-25 00:31:21","title":"NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia","abstract":"Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.","sentences":["Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge.","This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language.","NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving.","Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications.","The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike."],"url":"http://arxiv.org/abs/2403.16341v1","category":"math.NA"}
{"created":"2024-03-24 23:36:42","title":"Algorithms of constrained uniform approximation","abstract":"We address the problem of the best uniform approximation of a continuous function on a convex domain. The approximation is by linear combinations of a finite system of functions (not necessarily Chebyshev) under arbitrary linear constraints. By modifying the concept of alternance and of the Remez iterative procedure we present a method, which demonstrates its efficiency in numerical problems. The linear rate of convergence is proved under some favourable assumptions. A special attention is paid to systems of complex exponents, Gaussian functions, lacunar algebraic and trigonometric polynomials. Applications to signal processing, linear ODE, switching dynamical systems, and to Markov-Bernstein type inequalities are considered.","sentences":["We address the problem of the best uniform approximation of a continuous function on a convex domain.","The approximation is by linear combinations of a finite system of functions (not necessarily Chebyshev) under arbitrary linear constraints.","By modifying the concept of alternance and of the Remez iterative procedure we present a method, which demonstrates its efficiency in numerical problems.","The linear rate of convergence is proved under some favourable assumptions.","A special attention is paid to systems of complex exponents, Gaussian functions, lacunar algebraic and trigonometric polynomials.","Applications to signal processing, linear ODE, switching dynamical systems, and to Markov-Bernstein type inequalities are considered."],"url":"http://arxiv.org/abs/2403.16330v1","category":"math.NA"}
{"created":"2024-03-24 23:34:40","title":"Social Deliberation vs. Social Contracts in Self-Governing Voluntary Organisations","abstract":"Self-organising multi-agent systems regulate their components' behaviour voluntarily, according to a set of socially-constructed, mutually-agreed, and mutable social arrangements. In some systems, these arrangements may be applied with a frequency, at a scale and within implicit cost constraints such that performance becomes a pressing issue. This paper introduces the \\textit{Megabike Scenario}, which consists of a negotiated agreement on a relatively 'large' set of conventional rules, 'frequent' 'democratic' decision-making according to those rules, and a resource-bounded imperative to reach 'correct' decisions. A formalism is defined for effective rule representation and processing in the scenario, and is evaluated against five interleaved socio-functional requirements. System performance is also evaluated empirically through simulation. We conclude that to self-organise their social arrangements, agents need some awareness of their own limitations and the value of compromise.","sentences":["Self-organising multi-agent systems regulate their components' behaviour voluntarily, according to a set of socially-constructed, mutually-agreed, and mutable social arrangements.","In some systems, these arrangements may be applied with a frequency, at a scale and within implicit cost constraints such that performance becomes a pressing issue.","This paper introduces the \\textit{Megabike Scenario}, which consists of a negotiated agreement on a relatively 'large' set of conventional rules, 'frequent' 'democratic' decision-making according to those rules, and a resource-bounded imperative to reach 'correct' decisions.","A formalism is defined for effective rule representation and processing in the scenario, and is evaluated against five interleaved socio-functional requirements.","System performance is also evaluated empirically through simulation.","We conclude that to self-organise their social arrangements, agents need some awareness of their own limitations and the value of compromise."],"url":"http://arxiv.org/abs/2403.16329v1","category":"cs.MA"}
{"created":"2024-03-24 22:12:40","title":"ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process","abstract":"This paper addresses the particularities in optimal control of the uranium extraction-scrubbing operation in the PUREX process. The control problem requires optimally stabilizing the system at a desired solvent saturation level, guaranteeing constraints, disturbance rejection, and adapting to set point variations. A qualified simulator named PAREX was developed by the French Alternative Energies and Atomic Energy Commission (CEA) to simulate liquid-liquid extraction operations in the PUREX process. However, since the mathematical model is complex and is described by a system of nonlinear, stiff, high-dimensional differential-algebraic equations (DAE), applying optimal control methods will lead to a large-scale nonlinear programming problem with a huge computational burden. The solution we propose in this work is to train a neural network to predict the process outputs using the measurement history. This neural network architecture, which employs the long short-term memory (LSTM), linear regression and logistic regression networks, allows reducing the number of state variables, thus reducing the complexity of the optimization problems in the control scheme. Furthermore, nonlinear model predictive control (NMPC) and moving horizon estimation (MHE) problems are developed and solved using the PSO (Particle Swarm Optimization) algorithm. Simulation results show that the proposed adaptive optimal control scheme satisfies the requirements of the control problem and provides promise for experimental testing.","sentences":["This paper addresses the particularities in optimal control of the uranium extraction-scrubbing operation in the PUREX process.","The control problem requires optimally stabilizing the system at a desired solvent saturation level, guaranteeing constraints, disturbance rejection, and adapting to set point variations.","A qualified simulator named PAREX was developed by the French Alternative Energies and Atomic Energy Commission (CEA) to simulate liquid-liquid extraction operations in the PUREX process.","However, since the mathematical model is complex and is described by a system of nonlinear, stiff, high-dimensional differential-algebraic equations (DAE), applying optimal control methods will lead to a large-scale nonlinear programming problem with a huge computational burden.","The solution we propose in this work is to train a neural network to predict the process outputs using the measurement history.","This neural network architecture, which employs the long short-term memory (LSTM), linear regression and logistic regression networks, allows reducing the number of state variables, thus reducing the complexity of the optimization problems in the control scheme.","Furthermore, nonlinear model predictive control (NMPC) and moving horizon estimation (MHE) problems are developed and solved using the PSO (Particle Swarm Optimization) algorithm.","Simulation results show that the proposed adaptive optimal control scheme satisfies the requirements of the control problem and provides promise for experimental testing."],"url":"http://arxiv.org/abs/2403.16307v1","category":"eess.SY"}
{"created":"2024-03-24 21:55:46","title":"Control-Coherent Koopman Modeling: A Physical Modeling Approach","abstract":"The modeling of nonlinear dynamics based on Koopman operator theory, which is originally applicable only to autonomous systems with no control, is extended to non-autonomous control system without approximation to input matrix B. Prevailing methods using a least square estimate of the B matrix may result in an erroneous input matrix, misinforming the controller about the structure of the input matrix in a lifted space. Here, a new method for constructing a Koopman model that comprises the exact input matrix B is presented. A set of state variables are introduced so that the control inputs are linearly involved in the dynamics of actuators. With these variables, a lifted linear model with the exact control matrix, called a Control-Coherent Koopman Model, is constructed by superposing control input terms, which are linear in local actuator dynamics, to the Koopman operator of the associated autonomous nonlinear system. The proposed method is applied to multi degree-of-freedom robotic arms and multi-cable manipulation systems. Model Predictive Control is applied to the former. It is demonstrated that the prevailing Dynamic Mode Decomposition with Control (DMDc) using an approximate control matrix B does not provide a satisfactory result, while the Control-Coherent Koopman Model performs well with the correct B matrix.","sentences":["The modeling of nonlinear dynamics based on Koopman operator theory, which is originally applicable only to autonomous systems with no control, is extended to non-autonomous control system without approximation to input matrix B. Prevailing methods using a least square estimate of the B matrix may result in an erroneous input matrix, misinforming the controller about the structure of the input matrix in a lifted space.","Here, a new method for constructing a Koopman model that comprises the exact input matrix B is presented.","A set of state variables are introduced so that the control inputs are linearly involved in the dynamics of actuators.","With these variables, a lifted linear model with the exact control matrix, called a Control-Coherent Koopman Model, is constructed by superposing control input terms, which are linear in local actuator dynamics, to the Koopman operator of the associated autonomous nonlinear system.","The proposed method is applied to multi degree-of-freedom robotic arms and multi-cable manipulation systems.","Model Predictive Control is applied to the former.","It is demonstrated that the prevailing Dynamic Mode Decomposition with Control (DMDc) using an approximate control matrix B does not provide a satisfactory result, while the Control-Coherent Koopman Model performs well with the correct B matrix."],"url":"http://arxiv.org/abs/2403.16306v1","category":"eess.SY"}
{"created":"2024-03-24 21:29:05","title":"Chance-Constrained Gaussian Mixture Steering to a Terminal Gaussian Distribution","abstract":"We address the problem of finite-horizon control of a discrete-time linear system, where the initial state distribution follows a Gaussian mixture model, the terminal state must follow a specified Gaussian distribution, and the state and control inputs must obey chance constraints. We show that, throughout the time horizon, the state and control distributions are fully characterized by Gaussian mixtures. We then formulate the cost, distributional terminal constraint, and affine/2-norm chance constraints on the state and control, as convex functions of the decision variables. This is leveraged to formulate the chance-constrained path planning problem as a single semidefinite programming problem. A numerical example demonstrates the effectiveness of the proposed method.","sentences":["We address the problem of finite-horizon control of a discrete-time linear system, where the initial state distribution follows a Gaussian mixture model, the terminal state must follow a specified Gaussian distribution, and the state and control inputs must obey chance constraints.","We show that, throughout the time horizon, the state and control distributions are fully characterized by Gaussian mixtures.","We then formulate the cost, distributional terminal constraint, and affine/2-norm chance constraints on the state and control, as convex functions of the decision variables.","This is leveraged to formulate the chance-constrained path planning problem as a single semidefinite programming problem.","A numerical example demonstrates the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.16302v1","category":"math.OC"}
{"created":"2024-03-24 21:22:52","title":"Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network","abstract":"on adaptive routing to balance network traffic for optimum performance. Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion. In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy. Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away. This inaccuracy could lead to interconnect congestion. In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology. The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers. Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing. We implement the proposed Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms. Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction.","sentences":["on adaptive routing to balance network traffic for optimum performance.","Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion.","In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy.","Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away.","This inaccuracy could lead to interconnect congestion.","In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems.","Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology.","The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers.","Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing.","We implement the proposed Q-adaptive routing in SST/Merlin simulator.","Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms.","Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction."],"url":"http://arxiv.org/abs/2403.16301v1","category":"cs.NI"}
{"created":"2024-03-24 21:08:31","title":"Rigorous ESR spectroscopy of $Fe^{3+}$ impurity ion with oxygen vacancy in ferroelectric $SrTiO_3$ crystal at 20 mK","abstract":"Impurity $Fe^{3+}$ ion electron spin resonance (ESR) spectroscopy using multiple dielectric modes in a $SrTiO_3$ dielectric resonator has been performed with a tuneable DC magnetic field of up to $1.6~T$. The $Ti^{4+}(d^0)$ ion is substituted by $Fe^{3+}$ ion forming $FeO_6$ octahedral complex with an iron-oxygen-vacancy $(Fe^{3+}-V_O)$. In such a metal-ligand complex, a giant g-factor of $g_{\\scriptscriptstyle\\parallel F} = 5.51$ was observed in the ferroelectric phase at $20~mK$. The the change of $Fe^{3+}$ ion center-symmetry in the $FeO_6$ complex as a soft-mode characteristics of ferroelectric phase transition and the influences of iron-oxygen-vacancy $(Fe^{3+}-V_O)$, interactively sensitive to asymmetry in the octahedral rotational parameter $\\Phi$ in $SrTi0_3$.","sentences":["Impurity $Fe^{3+}$ ion electron spin resonance (ESR) spectroscopy using multiple dielectric modes in a $SrTiO_3$ dielectric resonator has been performed with a tuneable DC magnetic field of up to $1.6~T$. The $Ti^{4+}(d^0)$ ion is substituted by $Fe^{3+}$ ion forming $FeO_6$ octahedral complex with an iron-oxygen-vacancy $(Fe^{3+}-V_O)$. In such a metal-ligand complex, a giant g-factor of $g_{\\scriptscriptstyle\\parallel F} = 5.51$ was observed in the ferroelectric phase at $20~mK$. The the change of $Fe^{3+}$ ion center-symmetry in the $FeO_6$ complex as a soft-mode characteristics of ferroelectric phase transition and the influences of iron-oxygen-vacancy $(Fe^{3+}-V_O)$, interactively sensitive to asymmetry in the octahedral rotational parameter $\\Phi$ in $SrTi0_3$."],"url":"http://arxiv.org/abs/2403.16299v1","category":"quant-ph"}
{"created":"2024-03-24 21:00:23","title":"Unbiased Extremum Seeking Based on Lie Bracket Averaging","abstract":"Extremum seeking is an online, model-free optimization algorithm traditionally known for its practical stability. This paper introduces an extremum seeking algorithm designed for unbiased convergence to the extremum asymptotically, allowing users to define the convergence rate. Unlike conventional extremum seeking approaches utilizing constant gains, our algorithms employ time-varying parameters. These parameters reduce perturbation amplitudes towards zero in an asymptotic manner, while incorporating asymptotically growing controller gains. The stability analysis is based on state transformation, achieved through the multiplication of the input state by asymptotic growth function, and Lie bracket averaging applied to the transformed system. The averaging ensures the practical stability of the transformed system, which, in turn, leads to the asymptotic stability of the original system. Moreover, for strongly convex maps, we achieve exponentially fast convergence. The numerical simulations validate the feasibility of the introduced designs.","sentences":["Extremum seeking is an online, model-free optimization algorithm traditionally known for its practical stability.","This paper introduces an extremum seeking algorithm designed for unbiased convergence to the extremum asymptotically, allowing users to define the convergence rate.","Unlike conventional extremum seeking approaches utilizing constant gains, our algorithms employ time-varying parameters.","These parameters reduce perturbation amplitudes towards zero in an asymptotic manner, while incorporating asymptotically growing controller gains.","The stability analysis is based on state transformation, achieved through the multiplication of the input state by asymptotic growth function, and Lie bracket averaging applied to the transformed system.","The averaging ensures the practical stability of the transformed system, which, in turn, leads to the asymptotic stability of the original system.","Moreover, for strongly convex maps, we achieve exponentially fast convergence.","The numerical simulations validate the feasibility of the introduced designs."],"url":"http://arxiv.org/abs/2403.16294v1","category":"math.OC"}
{"created":"2024-03-24 20:56:16","title":"Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling","abstract":"In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of critical state to prune the derived decision tree. Through trace-based experiments, we demonstrate that IRL is capable of converting a black-box DNN policy into an interpretable rulebased decision tree while maintaining comparable scheduling performance. Additionally, IRL can contribute to the setting of rewards in DRL scheduling.","sentences":["In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes.","However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers.","This lack of model interpretability hinders the practical deployment of DRL scheduling.","In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling.","The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning.","Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans.","To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of critical state to prune the derived decision tree.","Through trace-based experiments, we demonstrate that IRL is capable of converting a black-box DNN policy into an interpretable rulebased decision tree while maintaining comparable scheduling performance.","Additionally, IRL can contribute to the setting of rewards in DRL scheduling."],"url":"http://arxiv.org/abs/2403.16293v1","category":"cs.LG"}
{"created":"2024-03-24 20:32:12","title":"Coupled Requirements-driven Testing of CPS: From Simulation To Reality","abstract":"Failures in safety-critical Cyber-Physical Systems (CPS), both software and hardware-related, can lead to severe incidents impacting physical infrastructure or even harming humans. As a result, extensive simulations and field tests need to be conducted, as part of the verification and validation of system requirements, to ensure system safety. However, current simulation and field testing practices, particularly in the domain of small Unmanned Aerial Systems (sUAS), are ad-hoc and lack a thorough, structured testing process. Furthermore, there is a dearth of standard processes and methodologies to inform the design of comprehensive simulation and field tests. This gap in the testing process leads to the deployment of sUAS applications that are: (a) tested in simulation environments which do not adequately capture the real-world complexity, such as environmental factors, due to a lack of tool support; (b) not subjected to a comprehensive range of scenarios during simulation testing to validate the system requirements, due to the absence of a process defining the relationship between requirements and simulation tests; and (c) not analyzed through standard safety analysis processes, because of missing traceability between simulation testing artifacts and safety analysis artifacts. To address these issues, we have developed an initial framework for validating CPS, specifically focusing on sUAS and robotic applications. We demonstrate the suitability of our framework by applying it to an example from the sUAS domain. Our preliminary results confirm the applicability of our framework. We conclude with a research roadmap to outline our next research goals along with our current proposal.","sentences":["Failures in safety-critical Cyber-Physical Systems (CPS), both software and hardware-related, can lead to severe incidents impacting physical infrastructure or even harming humans.","As a result, extensive simulations and field tests need to be conducted, as part of the verification and validation of system requirements, to ensure system safety.","However, current simulation and field testing practices, particularly in the domain of small Unmanned Aerial Systems (sUAS), are ad-hoc and lack a thorough, structured testing process.","Furthermore, there is a dearth of standard processes and methodologies to inform the design of comprehensive simulation and field tests.","This gap in the testing process leads to the deployment of sUAS applications that are: (a) tested in simulation environments which do not adequately capture the real-world complexity, such as environmental factors, due to a lack of tool support; (b) not subjected to a comprehensive range of scenarios during simulation testing to validate the system requirements, due to the absence of a process defining the relationship between requirements and simulation tests; and (c) not analyzed through standard safety analysis processes, because of missing traceability between simulation testing artifacts and safety analysis artifacts.","To address these issues, we have developed an initial framework for validating CPS, specifically focusing on sUAS and robotic applications.","We demonstrate the suitability of our framework by applying it to an example from the sUAS domain.","Our preliminary results confirm the applicability of our framework.","We conclude with a research roadmap to outline our next research goals along with our current proposal."],"url":"http://arxiv.org/abs/2403.16287v1","category":"cs.SE"}
{"created":"2024-03-24 20:17:38","title":"SKdV, SmKdV flows and their supersymmetric gauge-Miura transformations","abstract":"The construction of Integrable Hierarchies in terms of zero curvature representation provides a systematic construction for a series of integrable non-linear evolution equations (flows) which shares a common affine Lie algebraic structure. The integrable hierarchies are then classified in terms of a decomposition of the underlying affine Lie algebra $\\hat \\lie $ into graded subspaces defined by a grading operator $Q$. In this paper we shall discuss explicitly the simplest case of the affine $\\hat {sl}(2)$ Kac-Moody algebra within the principal gradation given rise to the KdV and mKdV hierarchies and extend to supersymmetric models.   It is known that the positive mKdV sub-hierachy is associated to some positive odd graded abelian subalgebra with elements denoted by $E^{(2n+1)}$. Each of these elements in turn, defines a time evolution equation according to time $t=t_{2n+1}$. An interesting observation is that for negative grades, the zero curvature representation allows both, even or odd sub-hierarchies. In both cases, the flows are non-local leading to integro-differential equations. Whilst positive and negative odd sub-hierarchies admit zero vacuum solutions, the negative even admits strictly non-zero vacuum solutions. Soliton solutions can be constructed by gauge transforming the zero curvature from the vacuum into a non trivial configuration (dressing method).   Inspired by the dressing transformation method, we have constructed a gauge-Miura transformation   mapping mKdV into KdV flows. Interesting new results concerns the negative grade sector of the mKdV hierarchy in which a double degeneracy of flows (odd and its consecutive even) of mKdV are mapped into a single odd KdV flow. These results are extended to supersymmetric hierarchies based upon the affine $\\hat {sl}(2,1)$ super-algebra.","sentences":["The construction of Integrable Hierarchies in terms of zero curvature representation provides a systematic construction for a series of integrable non-linear evolution equations (flows) which shares a common affine Lie algebraic structure.","The integrable hierarchies are then classified in terms of a decomposition of the underlying affine Lie algebra $\\hat \\lie $ into graded subspaces defined by a grading operator $Q$. In this paper we shall discuss explicitly the simplest case of the affine $\\hat {sl}(2)$ Kac-Moody algebra within the principal gradation given rise to the KdV and mKdV hierarchies and extend to supersymmetric models.   ","It is known that the positive mKdV sub-hierachy is associated to some positive odd graded abelian subalgebra with elements denoted by $E^{(2n+1)}$. Each of these elements in turn, defines a time evolution equation according to time $t=t_{2n+1}$. An interesting observation is that for negative grades, the zero curvature representation allows both, even or odd sub-hierarchies.","In both cases, the flows are non-local leading to integro-differential equations.","Whilst positive and negative odd sub-hierarchies admit zero vacuum solutions, the negative even admits strictly non-zero vacuum solutions.","Soliton solutions can be constructed by gauge transforming the zero curvature from the vacuum into a non trivial configuration (dressing method).   ","Inspired by the dressing transformation method, we have constructed a gauge-Miura transformation   mapping mKdV into KdV flows.","Interesting new results concerns the negative grade sector of the mKdV hierarchy in which a double degeneracy of flows (odd and its consecutive even) of mKdV are mapped into a single odd KdV flow.","These results are extended to supersymmetric hierarchies based upon the affine $\\hat {sl}(2,1)$ super-algebra."],"url":"http://arxiv.org/abs/2403.16285v1","category":"nlin.SI"}
{"created":"2024-03-24 20:07:07","title":"Semi-Automatic Line-System Provisioning with Integrated Physical-Parameter-Aware Methodology: Field Verification and Operational Feasibility","abstract":"We propose methods and an architecture to conduct measurements and optimize newly installed optical fiber line systems semi-automatically using integrated physics-aware technologies in a data center interconnection (DCI) transmission scenario. We demonstrate, for the first time, digital longitudinal monitoring (DLM) and optical line system (OLS) physical parameter calibration working together in real-time to extract physical link parameters for transmission performance optimization. Our methodology has the following advantages over traditional design: a minimized footprint at user sites, accurate estimation of the necessary optical network characteristics via complementary telemetry technologies, and the capability to conduct all operation work remotely. The last feature is crucial, as it enables remote operation to implement network design settings for immediate response to quality of transmission (QoT) degradation and reversion in the case of unforeseen problems. We successfully performed semi-automatic line system provisioning over field fiber networks facilities at Duke University, Durham, NC. The tasks of parameter retrieval, equipment setting optimization, and system setup/provisioning were completed within 1 hour. The field operation was supervised by on-duty personnel who could access the system remotely from different time zones. By comparing Q-factor estimates calculated from the extracted link parameters with measured results from 400G transceivers, we confirmed that our methodology has a reduction in the QoT prediction errors (+-0.3 dB) over existing design (+-10.6 dB).","sentences":["We propose methods and an architecture to conduct measurements and optimize newly installed optical fiber line systems semi-automatically using integrated physics-aware technologies in a data center interconnection (DCI) transmission scenario.","We demonstrate, for the first time, digital longitudinal monitoring (DLM) and optical line system (OLS) physical parameter calibration working together in real-time to extract physical link parameters for transmission performance optimization.","Our methodology has the following advantages over traditional design: a minimized footprint at user sites, accurate estimation of the necessary optical network characteristics via complementary telemetry technologies, and the capability to conduct all operation work remotely.","The last feature is crucial, as it enables remote operation to implement network design settings for immediate response to quality of transmission (QoT) degradation and reversion in the case of unforeseen problems.","We successfully performed semi-automatic line system provisioning over field fiber networks facilities at Duke University, Durham, NC.","The tasks of parameter retrieval, equipment setting optimization, and system setup/provisioning were completed within 1 hour.","The field operation was supervised by on-duty personnel who could access the system remotely from different time zones.","By comparing Q-factor estimates calculated from the extracted link parameters with measured results from 400G transceivers, we confirmed that our methodology has a reduction in the QoT prediction errors (+-0.3 dB) over existing design (+-10.6 dB)."],"url":"http://arxiv.org/abs/2403.16281v1","category":"eess.SY"}
{"created":"2024-03-24 19:16:37","title":"Applied Category Theory in the Wolfram Language using Categorica I: Diagrams, Functors and Fibrations","abstract":"This article serves as a preliminary introduction to the design of a new, open-source applied and computational category theory framework, named Categorica, built on top of the Wolfram Language. Categorica allows one to configure and manipulate abstract quivers, categories, groupoids, diagrams, functors and natural transformations, and to perform a vast array of automated abstract algebraic computations using (arbitrary combinations of) the above structures; to manipulate and abstractly reason about arbitrary universal properties, including products, coproducts, pullbacks, pushouts, limits and colimits; and to manipulate, visualize and compute with strict (symmetric) monoidal categories, including full support for automated string diagram rewriting and diagrammatic theorem-proving. In so doing, Categorica combines the capabilities of an abstract computer algebra framework (thus allowing one to compute directly with epimorphisms, monomorphisms, retractions, sections, spans, cospans, fibrations, etc.) with those of a powerful automated theorem-proving system (thus allowing one to convert universal properties and other abstract constructions into (higher-order) equational logic statements that can be reasoned about and proved using standard automated theorem-proving methods, as well as to prove category-theoretic statements directly using purely diagrammatic methods). In this first of two articles introducing the design of the framework, we shall focus principally upon its handling of quivers, categories, diagrams, groupoids, functors and natural transformations, including demonstrations of both its algebraic manipulation and theorem-proving capabilities in each case.","sentences":["This article serves as a preliminary introduction to the design of a new, open-source applied and computational category theory framework, named Categorica, built on top of the Wolfram Language.","Categorica allows one to configure and manipulate abstract quivers, categories, groupoids, diagrams, functors and natural transformations, and to perform a vast array of automated abstract algebraic computations using (arbitrary combinations of) the above structures; to manipulate and abstractly reason about arbitrary universal properties, including products, coproducts, pullbacks, pushouts, limits and colimits; and to manipulate, visualize and compute with strict (symmetric) monoidal categories, including full support for automated string diagram rewriting and diagrammatic theorem-proving.","In so doing, Categorica combines the capabilities of an abstract computer algebra framework (thus allowing one to compute directly with epimorphisms, monomorphisms, retractions, sections, spans, cospans, fibrations, etc.) with those of a powerful automated theorem-proving system (thus allowing one to convert universal properties and other abstract constructions into (higher-order) equational logic statements that can be reasoned about and proved using standard automated theorem-proving methods, as well as to prove category-theoretic statements directly using purely diagrammatic methods).","In this first of two articles introducing the design of the framework, we shall focus principally upon its handling of quivers, categories, diagrams, groupoids, functors and natural transformations, including demonstrations of both its algebraic manipulation and theorem-proving capabilities in each case."],"url":"http://arxiv.org/abs/2403.16269v1","category":"math.CT"}
{"created":"2024-03-24 18:44:17","title":"Symmetry breaker governs synchrony patterns in neuronal inspired networks","abstract":"Experiments in the human brain reveal switching between different activity patterns and functional network organization over time. Recently, multilayer modeling has been employed across multiple neurobiological levels (from spiking networks to brain regions) to unveil novel insights into the emergence and time evolution of synchrony patterns. We consider two layers with the top layer directly coupled to the bottom layer. When isolated, the bottom layer would remain in a specific stable pattern. However, in the presence of the top layer, the network exhibits spatiotemporal switching. The top layer in combination with the inter-layer coupling acts as a symmetry breaker, governing the bottom layer and restricting the number of allowed symmetry-induced patterns. This structure allows us to demonstrate the existence and stability of pattern states on the bottom layer, but most remarkably, it enables a simple mechanism for switching between patterns based on the unique symmetry-breaking role of the governing layer. We demonstrate that the symmetry breaker prevents complete synchronization in the bottom layer, a situation that would not be desirable in a normal functioning brain. We illustrate our findings using two layers of Hindmarsh-Rose (HR) oscillators, employing the Master Stability function approach in small networks to investigate the switching between patterns.","sentences":["Experiments in the human brain reveal switching between different activity patterns and functional network organization over time.","Recently, multilayer modeling has been employed across multiple neurobiological levels (from spiking networks to brain regions) to unveil novel insights into the emergence and time evolution of synchrony patterns.","We consider two layers with the top layer directly coupled to the bottom layer.","When isolated, the bottom layer would remain in a specific stable pattern.","However, in the presence of the top layer, the network exhibits spatiotemporal switching.","The top layer in combination with the inter-layer coupling acts as a symmetry breaker, governing the bottom layer and restricting the number of allowed symmetry-induced patterns.","This structure allows us to demonstrate the existence and stability of pattern states on the bottom layer, but most remarkably, it enables a simple mechanism for switching between patterns based on the unique symmetry-breaking role of the governing layer.","We demonstrate that the symmetry breaker prevents complete synchronization in the bottom layer, a situation that would not be desirable in a normal functioning brain.","We illustrate our findings using two layers of Hindmarsh-Rose (HR) oscillators, employing the Master Stability function approach in small networks to investigate the switching between patterns."],"url":"http://arxiv.org/abs/2403.16261v1","category":"math.DS"}
{"created":"2024-03-24 18:33:15","title":"Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning","abstract":"Multimodal contrastive learning has emerged as a powerful paradigm for building high-quality features using the complementary strengths of various data modalities. However, the open nature of such systems inadvertently increases the possibility of backdoor attacks. These attacks subtly embed malicious behaviors within the model during training, which can be activated by specific triggers in the inference phase, posing significant security risks. Despite existing countermeasures through fine-tuning that reduce the adverse impacts of such attacks, these defenses often degrade the clean accuracy and necessitate the construction of extensive clean training pairs. In this paper, we explore the possibility of a less-cost defense from the perspective of model unlearning, that is, whether the model can be made to quickly \\textbf{u}nlearn \\textbf{b}ackdoor \\textbf{t}hreats (UBT) by constructing a small set of poisoned samples. Specifically, we strengthen the backdoor shortcuts to discover suspicious samples through overfitting training prioritized by weak similarity samples. Building on the initial identification of suspicious samples, we introduce an innovative token-based localized forgetting training regime. This technique specifically targets the poisoned aspects of the model, applying a focused effort to unlearn the backdoor associations and trying not to damage the integrity of the overall model. Experimental results show that our method not only ensures a minimal success rate for attacks, but also preserves the model's high clean accuracy.","sentences":["Multimodal contrastive learning has emerged as a powerful paradigm for building high-quality features using the complementary strengths of various data modalities.","However, the open nature of such systems inadvertently increases the possibility of backdoor attacks.","These attacks subtly embed malicious behaviors within the model during training, which can be activated by specific triggers in the inference phase, posing significant security risks.","Despite existing countermeasures through fine-tuning that reduce the adverse impacts of such attacks, these defenses often degrade the clean accuracy and necessitate the construction of extensive clean training pairs.","In this paper, we explore the possibility of a less-cost defense from the perspective of model unlearning, that is, whether the model can be made to quickly \\textbf{u}nlearn \\textbf{b}ackdoor \\textbf{t}hreats (UBT) by constructing a small set of poisoned samples.","Specifically, we strengthen the backdoor shortcuts to discover suspicious samples through overfitting training prioritized by weak similarity samples.","Building on the initial identification of suspicious samples, we introduce an innovative token-based localized forgetting training regime.","This technique specifically targets the poisoned aspects of the model, applying a focused effort to unlearn the backdoor associations and trying not to damage the integrity of the overall model.","Experimental results show that our method not only ensures a minimal success rate for attacks, but also preserves the model's high clean accuracy."],"url":"http://arxiv.org/abs/2403.16257v1","category":"cs.CV"}
{"created":"2024-03-24 18:30:03","title":"Phase retrieval on circles and lines","abstract":"Let $f$ and $g$ be analytic functions on the open unit disc $\\mathbb D$ such that $|f|=|g|$ on various sets $A$. We first prove that there exists $c$ in the unit circle $\\mathbb T$ such that $f=cg$ when $A$ is the union of two lines in $\\mathbb D$ intersecting at an angle that is an irrational multiple of $\\pi$. The same conclusion is valid when $f$ and $g$ are in the Nevanlinna class and $A$ is the union of the unit circle and an interior circle, tangential or not. We also provide sequential versions of the previous results and analyse the case $A=r\\mathbb T$. Finally we examine the situation when there is equality on two distinct circles in the disc, proving a result or counterexample for each possible configuration.","sentences":["Let $f$ and $g$ be analytic functions on the open unit disc $\\mathbb D$ such that $|f|=|g|$ on various sets $A$.","We first prove that there exists $c$ in the unit circle $\\mathbb T$ such that $f=cg$ when $A$ is the union of two lines in $\\mathbb D$ intersecting at an angle that is an irrational multiple of $\\pi$. The same conclusion is valid when $f$ and $g$ are in the Nevanlinna class and $A$ is the union of the unit circle and an interior circle, tangential or not.","We also provide sequential versions of the previous results and analyse the case $A=r\\mathbb","T$. Finally we examine the situation when there is equality on two distinct circles in the disc, proving a result or counterexample for each possible configuration."],"url":"http://arxiv.org/abs/2403.16255v1","category":"math.CV"}
{"created":"2024-03-24 18:13:48","title":"Unveiling Lens Light Complexity with A Novel Multi-Gaussian Expansion Approach for Strong Gravitational Lensing","abstract":"In a strong gravitational lensing system, the distorted light from a source is analysed to infer the properties of the lens. However, light emitted by the lens itself can contaminate the image of the source, introducing systematic errors in the analysis. We present a simple and efficient lens light model based on the well-tested multi-Gaussian expansion (MGE) method for representing galaxy surface brightness profiles, which we combine with a semi-linear inversion scheme for pixelized source modelling. Testing it against realistic mock lensing images, we show that our scheme can fit the lensed images to the noise level, with relative differences between the true input and best-fit lens light model remaining below 5%. We apply the MGE lens light model to 38 lenses from the HST SLACS sample. We find that the new scheme provides a good fit for the majority of the sample with only 3 exceptions -- these show clear asymmetric residuals in the lens light. We examine the radial dependence of the ellipticity and position angles and confirm that it is common for a typical lens galaxy to exhibit twisting, non-elliptical isophotes and boxy / disky isophotes. Our MGE lens light model will be a valuable tool for understanding the hidden complexity of the lens mass distribution.","sentences":["In a strong gravitational lensing system, the distorted light from a source is analysed to infer the properties of the lens.","However, light emitted by the lens itself can contaminate the image of the source, introducing systematic errors in the analysis.","We present a simple and efficient lens light model based on the well-tested multi-Gaussian expansion (MGE) method for representing galaxy surface brightness profiles, which we combine with a semi-linear inversion scheme for pixelized source modelling.","Testing it against realistic mock lensing images, we show that our scheme can fit the lensed images to the noise level, with relative differences between the true input and best-fit lens light model remaining below 5%.","We apply the MGE lens light model to 38 lenses from the HST SLACS sample.","We find that the new scheme provides a good fit for the majority of the sample with only 3 exceptions -- these show clear asymmetric residuals in the lens light.","We examine the radial dependence of the ellipticity and position angles and confirm that it is common for a typical lens galaxy to exhibit twisting, non-elliptical isophotes and boxy / disky isophotes.","Our MGE lens light model will be a valuable tool for understanding the hidden complexity of the lens mass distribution."],"url":"http://arxiv.org/abs/2403.16253v1","category":"astro-ph.GA"}
{"created":"2024-03-24 18:10:30","title":"Legged Robot State Estimation within Non-inertial Environments","abstract":"This paper investigates the robot state estimation problem within a non-inertial environment. The proposed state estimation approach relaxes the common assumption of static ground in the system modeling. The process and measurement models explicitly treat the movement of the non-inertial environments without requiring knowledge of its motion in the inertial frame or relying on GPS or sensing environmental landmarks. Further, the proposed state estimator is formulated as an invariant extended Kalman filter (InEKF) with the deterministic part of its process model obeying the group-affine property, leading to log-linear error dynamics. The observability analysis of the filter confirms that the robot's pose (i.e., position and orientation) and velocity relative to the non-inertial environment are observable. Hardware experiments on a humanoid robot moving on a rotating and translating treadmill demonstrate the high convergence rate and accuracy of the proposed InEKF even under significant treadmill pitch sway, as well as large estimation errors.","sentences":["This paper investigates the robot state estimation problem within a non-inertial environment.","The proposed state estimation approach relaxes the common assumption of static ground in the system modeling.","The process and measurement models explicitly treat the movement of the non-inertial environments without requiring knowledge of its motion in the inertial frame or relying on GPS or sensing environmental landmarks.","Further, the proposed state estimator is formulated as an invariant extended Kalman filter (InEKF) with the deterministic part of its process model obeying the group-affine property, leading to log-linear error dynamics.","The observability analysis of the filter confirms that the robot's pose (i.e., position and orientation) and velocity relative to the non-inertial environment are observable.","Hardware experiments on a humanoid robot moving on a rotating and translating treadmill demonstrate the high convergence rate and accuracy of the proposed InEKF even under significant treadmill pitch sway, as well as large estimation errors."],"url":"http://arxiv.org/abs/2403.16252v1","category":"cs.RO"}
{"created":"2024-03-24 18:04:04","title":"Spectroscopic approaches for studies of site-specific DNA base and backbone \"breathing\" using exciton-coupled dimer-labeled DNA","abstract":"DNA regulation and repair processes require direct interactions between proteins and DNA at specific sites. Local fluctuations of the sugar-phosphate backbones and bases of DNA (a form of DNA \"breathing\") play a central role in such processes. Here we review the development and application of novel spectroscopic methods and analyses - both at the ensemble and single-molecule levels - to study structural and dynamic properties of exciton-coupled cyanine and fluorescent nucleobase analogue dimer-labeled DNA constructs at key positions involved in protein-DNA complex assembly and function. The exciton-coupled dimer probes act as \"sensors\" of the local conformations adopted by the sugar-phosphate backbones and bases immediately surrounding the dimer probes. These methods can be used to study the mechanisms of protein binding and function at these sites.","sentences":["DNA regulation and repair processes require direct interactions between proteins and DNA at specific sites.","Local fluctuations of the sugar-phosphate backbones and bases of DNA (a form of DNA \"breathing\") play a central role in such processes.","Here we review the development and application of novel spectroscopic methods and analyses - both at the ensemble and single-molecule levels - to study structural and dynamic properties of exciton-coupled cyanine and fluorescent nucleobase analogue dimer-labeled DNA constructs at key positions involved in protein-DNA complex assembly and function.","The exciton-coupled dimer probes act as \"sensors\" of the local conformations adopted by the sugar-phosphate backbones and bases immediately surrounding the dimer probes.","These methods can be used to study the mechanisms of protein binding and function at these sites."],"url":"http://arxiv.org/abs/2403.16251v1","category":"physics.bio-ph"}
{"created":"2024-03-24 17:12:13","title":"Low Rank Groupwise Deformations for Motion Tracking in Cardiac Cine MRI","abstract":"Diffeomorphic image registration is a commonly used method to deform one image to resemble another. While warping a single image to another is useful, it can be advantageous to warp multiple images simultaneously, such as in tracking the motion of the heart across a sequence of images. In this paper, our objective is to propose a novel method capable of registering a group or sequence of images to a target image, resulting in registered images that appear identical and therefore have a low rank. Moreover, we aim for these registered images to closely resemble the target image. Through experimental evidence, we will demonstrate our method's superior efficacy in producing low-rank groupwise deformations compared to other state-of-the-art approaches.","sentences":["Diffeomorphic image registration is a commonly used method to deform one image to resemble another.","While warping a single image to another is useful, it can be advantageous to warp multiple images simultaneously, such as in tracking the motion of the heart across a sequence of images.","In this paper, our objective is to propose a novel method capable of registering a group or sequence of images to a target image, resulting in registered images that appear identical and therefore have a low rank.","Moreover, we aim for these registered images to closely resemble the target image.","Through experimental evidence, we will demonstrate our method's superior efficacy in producing low-rank groupwise deformations compared to other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.16240v1","category":"cs.CV"}
{"created":"2024-03-24 16:53:07","title":"The Next Generation Virgo Cluster Survey (NGVS). III. A Catalog of Surface Brightness Fluctuation Distances and the Three-Dimensional Distribution of Galaxies in the Virgo Cluster","abstract":"The surface brightness fluctuation (SBF) method is a robust and efficient way of measuring distances to galaxies containing evolved stellar populations. Although many recent applications of the method have used space-based imaging, SBF remains a powerful technique for ground-based telescopes. Deep, wide-field imaging surveys with subarsecond seeing enable SBF measurements for numerous nearby galaxies. Using a preliminary calibration, Cantiello et al. (2018) presented SBF distances for 89 bright, mainly early-type galaxies observed in the Next Generation Virgo Cluster Survey (NGVS). Here, we present a refined calibration and SBF distances for 278 galaxies extending several magnitudes fainter than in previous work. The derived distances have uncertainties of 5-12\\% depending on the properties of the individual galaxies, and our sample is more than three times larger than any previous SBF study of this region. Virgo has a famously complex structure with numerous subclusters, clouds and groups; we associate individual galaxies with the various substructures and map their three-dimensional spatial distribution. Curiously, subcluster A, centered around M87, appears to have two peaks in distance: the main peak at $\\sim$16.5 Mpc and a smaller one at $\\sim$19.4 Mpc. Subclusters B and C have distances of $\\sim$15.8 Mpc. The W and W' groups form a filament-like structure, extending more than 15~Mpc behind the cluster with a commensurate velocity increase of $\\sim$1000 \\kms\\ along its length. These measurements are a valuable resource for future studies of the relationship between galaxy properties and local environment within a dynamic and evolving region.","sentences":["The surface brightness fluctuation (SBF) method is a robust and efficient way of measuring distances to galaxies containing evolved stellar populations.","Although many recent applications of the method have used space-based imaging, SBF remains a powerful technique for ground-based telescopes.","Deep, wide-field imaging surveys with subarsecond seeing enable SBF measurements for numerous nearby galaxies.","Using a preliminary calibration, Cantiello et al.","(2018) presented SBF distances for 89 bright, mainly early-type galaxies observed in the Next Generation Virgo Cluster Survey (NGVS).","Here, we present a refined calibration and SBF distances for 278 galaxies extending several magnitudes fainter than in previous work.","The derived distances have uncertainties of 5-12\\% depending on the properties of the individual galaxies, and our sample is more than three times larger than any previous SBF study of this region.","Virgo has a famously complex structure with numerous subclusters, clouds and groups; we associate individual galaxies with the various substructures and map their three-dimensional spatial distribution.","Curiously, subcluster A, centered around M87, appears to have two peaks in distance: the main peak at $\\sim$16.5 Mpc and a smaller one at $\\sim$19.4 Mpc.","Subclusters B and C have distances of $\\sim$15.8 Mpc.","The W and W' groups form a filament-like structure, extending more than 15~Mpc behind the cluster with a commensurate velocity increase of $\\sim$1000 \\kms\\ along its length.","These measurements are a valuable resource for future studies of the relationship between galaxy properties and local environment within a dynamic and evolving region."],"url":"http://arxiv.org/abs/2403.16235v1","category":"astro-ph.GA"}
{"created":"2024-03-24 16:49:55","title":"An early warning indicator trained on stochastic disease-spreading models with different noises","abstract":"The timely detection of disease outbreaks through reliable early warning signals (EWSs) is indispensable for effective public health mitigation strategies. Nevertheless, the intricate dynamics of real-world disease spread, often influenced by diverse sources of noise and limited data in the early stages of outbreaks, pose a significant challenge in developing reliable EWSs, as the performance of existing indicators varies with extrinsic and intrinsic noises. Here, we address the challenge of modeling disease when the measurements are corrupted by additive white noise, multiplicative environmental noise, and demographic noise into a standard epidemic mathematical model. To navigate the complexities introduced by these noise sources, we employ a deep learning algorithm that provides EWS in infectious disease outbreak by training on noise-induced disease-spreading models. The indicator's effectiveness is demonstrated through its application to real-world COVID-19 cases in Edmonton and simulated time series derived from diverse disease spread models affected by noise. Notably, the indicator captures an impending transition in a time series of disease outbreaks and outperforms existing indicators. This study contributes to advancing early warning capabilities by addressing the intricate dynamics inherent in real-world disease spread, presenting a promising avenue for enhancing public health preparedness and response efforts.","sentences":["The timely detection of disease outbreaks through reliable early warning signals (EWSs) is indispensable for effective public health mitigation strategies.","Nevertheless, the intricate dynamics of real-world disease spread, often influenced by diverse sources of noise and limited data in the early stages of outbreaks, pose a significant challenge in developing reliable EWSs, as the performance of existing indicators varies with extrinsic and intrinsic noises.","Here, we address the challenge of modeling disease when the measurements are corrupted by additive white noise, multiplicative environmental noise, and demographic noise into a standard epidemic mathematical model.","To navigate the complexities introduced by these noise sources, we employ a deep learning algorithm that provides EWS in infectious disease outbreak by training on noise-induced disease-spreading models.","The indicator's effectiveness is demonstrated through its application to real-world COVID-19 cases in Edmonton and simulated time series derived from diverse disease spread models affected by noise.","Notably, the indicator captures an impending transition in a time series of disease outbreaks and outperforms existing indicators.","This study contributes to advancing early warning capabilities by addressing the intricate dynamics inherent in real-world disease spread, presenting a promising avenue for enhancing public health preparedness and response efforts."],"url":"http://arxiv.org/abs/2403.16233v1","category":"cs.LG"}
{"created":"2024-03-24 16:42:35","title":"Rank-Dependent Predictable Forward Performance Processes","abstract":"Predictable forward performance processes (PFPPs) are stochastic optimal control frameworks for an agent who controls a randomly evolving system but can only prescribe the system dynamics for a short period ahead. This is a common scenario in which a controlling agent frequently re-calibrates her model. We introduce a new class of PFPPs based on rank-dependent utility, generalizing existing models that are based on expected utility theory (EUT). We establish existence of rank-dependent PFPPs under a conditionally complete market and exogenous probability distortion functions which are updated periodically. We show that their construction reduces to solving an integral equation that generalizes the integral equation obtained under EUT in previous studies. We then propose a new approach for solving the integral equation via theory of Volterra equations. We illustrate our result in the special case of conditionally complete Black-Scholes model.","sentences":["Predictable forward performance processes (PFPPs) are stochastic optimal control frameworks for an agent who controls a randomly evolving system but can only prescribe the system dynamics for a short period ahead.","This is a common scenario in which a controlling agent frequently re-calibrates her model.","We introduce a new class of PFPPs based on rank-dependent utility, generalizing existing models that are based on expected utility theory (EUT).","We establish existence of rank-dependent PFPPs under a conditionally complete market and exogenous probability distortion functions which are updated periodically.","We show that their construction reduces to solving an integral equation that generalizes the integral equation obtained under EUT in previous studies.","We then propose a new approach for solving the integral equation via theory of Volterra equations.","We illustrate our result in the special case of conditionally complete Black-Scholes model."],"url":"http://arxiv.org/abs/2403.16228v1","category":"q-fin.MF"}
{"created":"2024-03-24 16:34:47","title":"Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields","abstract":"Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: https://whyy.site/paper/nep","sentences":["Inverse rendering aims at recovering both geometry and materials of objects.","It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs).","On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only.","Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation.","We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields.","Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy.","Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects.","Project webpage: https://whyy.site/paper/nep"],"url":"http://arxiv.org/abs/2403.16224v1","category":"cs.CV"}
{"created":"2024-03-24 16:28:57","title":"A poroelastic plate model obtained by simultaneous homogenization and dimension reduction","abstract":"In this paper, the starting point of our analysis is coupled system of elasticity and Stokes equation. We consider two small parameters: the thickness $h$ of the thin plate and the pore scale $\\varepsilon(h)$ which depend on $h$. We will focus specifically on the case when the pore size is small relative to the thickness of the plate. The main goal here is derive a model for a poroelastic plate from the $3D$ problem as $h$ goes to zero using simultaneous homogenization and dimension reduction techniques. The obtained model generalizes the poroelastic plate model derived by A. Mikeli\\'c et. al. in 2015 by dimension reduction techniques from $3D$ Biot's equations in the sense that it also covers the case of contact of poroelastic and elastic plate as well as the evolution equation with inertia term.","sentences":["In this paper, the starting point of our analysis is coupled system of elasticity and Stokes equation.","We consider two small parameters: the thickness $h$ of the thin plate and the pore scale $\\varepsilon(h)$ which depend on $h$. We will focus specifically on the case when the pore size is small relative to the thickness of the plate.","The main goal here is derive a model for a poroelastic plate from the $3D$ problem as $h$ goes to zero using simultaneous homogenization and dimension reduction techniques.","The obtained model generalizes the poroelastic plate model derived by A. Mikeli\\'c et.","al.","in 2015 by dimension reduction techniques from $3D$ Biot's equations in the sense that it also covers the case of contact of poroelastic and elastic plate as well as the evolution equation with inertia term."],"url":"http://arxiv.org/abs/2403.16220v1","category":"math.AP"}
{"created":"2024-03-24 16:16:41","title":"Systematic construction of continuous-time neural networks for linear dynamical systems","abstract":"Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space. In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems. We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties. We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing conventional neural architectures with vertical hidden layers may not be favorable. We also provide an upper bound on the numerical errors of our neural networks. Finally, we demonstrate the high accuracy of our constructed networks on three numerical examples.","sentences":["Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space.","In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems.","We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE).","Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties.","We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing conventional neural architectures with vertical hidden layers may not be favorable.","We also provide an upper bound on the numerical errors of our neural networks.","Finally, we demonstrate the high accuracy of our constructed networks on three numerical examples."],"url":"http://arxiv.org/abs/2403.16215v1","category":"cs.LG"}
{"created":"2024-03-24 16:13:27","title":"Efficient Reachable Sets on Lie Groups Using Lie Algebra Monotonicity and Tangent Intervals","abstract":"In this paper, we efficiently compute overapproximated reachable sets for control systems evolving on Lie groups, building off results from monotone systems theory and geometric integration theory. We propose to consider intervals living in the Lie algebra, which through the exponential map, describe real sets on the Lie group. A local equivalence between the original system and a system evolving on the Lie algebra allows existing interval reachability techniques to apply in the tangent space. Using interval bounds of the Baker-Campbell-Hausdorff formula, a Runge-Kutta-Munthe-Kaas reachability algorithm is proposed, providing reachable set estimates for arbitrary time horizons at little computational cost. The algorithm is demonstrated on through consensus on a torus and attitude control on $SO(3)$.","sentences":["In this paper, we efficiently compute overapproximated reachable sets for control systems evolving on Lie groups, building off results from monotone systems theory and geometric integration theory.","We propose to consider intervals living in the Lie algebra, which through the exponential map, describe real sets on the Lie group.","A local equivalence between the original system and a system evolving on the Lie algebra allows existing interval reachability techniques to apply in the tangent space.","Using interval bounds of the Baker-Campbell-Hausdorff formula, a Runge-Kutta-Munthe-Kaas reachability algorithm is proposed, providing reachable set estimates for arbitrary time horizons at little computational cost.","The algorithm is demonstrated on through consensus on a torus and attitude control on $SO(3)$."],"url":"http://arxiv.org/abs/2403.16214v1","category":"eess.SY"}
{"created":"2024-03-24 16:13:14","title":"Caracterisation of Frechet differentiability norm for dual complex Banach spaces","abstract":"Let X be a complex Banach space, in this work we characterize the property of Frechet differentiability for the dual space of X. In the following, we show that if the dual space of X is Gateaux differentiable, then the dual space of Lp(X) has the same property for all p betwwen one and infiniy","sentences":["Let X be a complex Banach space, in this work we characterize the property of Frechet differentiability for the dual space of X. In the following, we show that if the dual space of X is Gateaux differentiable, then the dual space of Lp(X) has the same property for all p betwwen one and infiniy"],"url":"http://arxiv.org/abs/2403.16213v1","category":"math.FA"}
{"created":"2024-03-24 16:11:27","title":"Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis","abstract":"Exploring the application of deep learning technologies in the field of medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique perspective for observing and diagnosing complex neurodegenerative diseases such as Alzheimer Disease (AD). With advancements in deep learning, particularly in Convolutional Neural Networks (CNNs) and the Xception network architecture, we are now able to analyze and classify vast amounts of MRI data with unprecedented accuracy. The progress of this technology not only enhances our understanding of brain structural changes but also opens up new avenues for monitoring disease progression through non-invasive means and potentially allows for precise diagnosis in the early stages of the disease.   This study aims to classify MRI images using deep learning models to identify different stages of Alzheimer Disease through a series of innovative data processing and model construction steps. Our experimental results show that the deep learning framework based on the Xception model achieved a 99.6% accuracy rate in the multi-class MRI image classification task, demonstrating its potential application value in assistive diagnosis. Future research will focus on expanding the dataset, improving model interpretability, and clinical validation to further promote the application of deep learning technology in the medical field, with the hope of bringing earlier diagnosis and more personalized treatment plans to Alzheimer Disease patients.","sentences":["Exploring the application of deep learning technologies in the field of medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique perspective for observing and diagnosing complex neurodegenerative diseases such as Alzheimer Disease (AD).","With advancements in deep learning, particularly in Convolutional Neural Networks (CNNs) and the Xception network architecture, we are now able to analyze and classify vast amounts of MRI data with unprecedented accuracy.","The progress of this technology not only enhances our understanding of brain structural changes but also opens up new avenues for monitoring disease progression through non-invasive means and potentially allows for precise diagnosis in the early stages of the disease.   ","This study aims to classify MRI images using deep learning models to identify different stages of Alzheimer Disease through a series of innovative data processing and model construction steps.","Our experimental results show that the deep learning framework based on the Xception model achieved a 99.6% accuracy rate in the multi-class MRI image classification task, demonstrating its potential application value in assistive diagnosis.","Future research will focus on expanding the dataset, improving model interpretability, and clinical validation to further promote the application of deep learning technology in the medical field, with the hope of bringing earlier diagnosis and more personalized treatment plans to Alzheimer Disease patients."],"url":"http://arxiv.org/abs/2403.16212v1","category":"eess.IV"}
{"created":"2024-03-24 16:10:02","title":"Chandra and HST studies of the X-ray sources in the Globular Cluster NGC 362","abstract":"We analyse a Chandra observation of the rich globular cluster NGC 362, finding 33 X-ray sources within 1' (1.2 half-mass radii) of the cluster center. Spectral analysis of the brightest source (X1) shows blackbody-like emission, indicating it is likely a quiescent low-mass X-ray binary; we find a possible counterpart that falls in the sub-subgiant region. We use HST UV Globular Cluster Survey (HUGS) photometry to identify 15 potential optical/UV counterparts to these X-ray sources, including two background AGN. We identify no likely CVs, probably due to crowding in optical filters in the core, though we predict of order 8 CVs among the detected X-ray sources. We identify three other sub-subgiants and two red straggler counterparts, which are likely powered by coronal activity, along with five other potential coronally active binary counterparts to three X-ray sources. Finally, we note two unusual counterpart candidates that lie to the red of the red giant branch in V_606 - I_814, and shift well to the blue of the red giant branch in ultraviolet colour-magnitude diagrams. These systems seem to contain a red giant with a distorted evolutionary history, plus a bright blue light source, either a blue straggler star (an Algol-like system) or an accreting white dwarf (a long-period CV, or a symbiotic star).","sentences":["We analyse a Chandra observation of the rich globular cluster NGC 362, finding 33 X-ray sources within 1' (1.2 half-mass radii) of the cluster center.","Spectral analysis of the brightest source (X1) shows blackbody-like emission, indicating it is likely a quiescent low-mass X-ray binary; we find a possible counterpart that falls in the sub-subgiant region.","We use HST UV Globular Cluster Survey (HUGS) photometry to identify 15 potential optical/UV counterparts to these X-ray sources, including two background AGN.","We identify no likely CVs, probably due to crowding in optical filters in the core, though we predict of order 8 CVs among the detected X-ray sources.","We identify three other sub-subgiants and two red straggler counterparts, which are likely powered by coronal activity, along with five other potential coronally active binary counterparts to three X-ray sources.","Finally, we note two unusual counterpart candidates that lie to the red of the red giant branch in V_606 - I_814, and shift well to the blue of the red giant branch in ultraviolet colour-magnitude diagrams.","These systems seem to contain a red giant with a distorted evolutionary history, plus a bright blue light source, either a blue straggler star (an Algol-like system) or an accreting white dwarf (a long-period CV, or a symbiotic star)."],"url":"http://arxiv.org/abs/2403.16211v1","category":"astro-ph.HE"}
{"created":"2024-03-24 15:45:12","title":"Acceleration of Fe3+/Fe2+ cycle in garland-like MIL-101(Fe)/MoS2 nanosheets to promote peroxymonosulfate activation for sulfamethoxazole degradation","abstract":"Iron-based molybdenum disulfide (Fe-MoS2) has emerged as a Fenton-like catalyst for the highly efficient degradation of antibiotics, but the structure-activity relationship remains elusive. Herein, garland-like MIL-101(Fe)/MoS2 nanosheets (MMS) with dual metal active sites (Fe and Mo) and rich sulfur vacancies were fabricated to directly activate peroxymonosulfate (PMS) for fast degradation of different organic pollutants (phenols, dyes and drugs), even in real water bodies. The MMS exhibited extremely fast catalytic rate constant of 0.289 min-1 in the degradation of sulfamethoxazole (SMX), which was about 36 and 29 times that of single MoS2 (0.008 min-1) and MIL-101(Fe) (0.01 min-1). Moreover, MMS with good stability and reusability could reach 92% degradation of SMX after 5 cycles. Quenching experiments and electron spin resonance (ESR) tests revealed that hydroxyl radicals (.OH) and singlet oxygen (1O2) were the dominant reactive oxygen species (ROS) for SMX degradation. The integration of experimental works, characterization techniques and density functional theory (DFT) calculations unraveled that the formation of sulfur vacancies in MMS catalyst could expose more Mo sites, improve the charge density and boost the electron transfer, which was conducive to accelerating the Fe3+/Fe2+ cycle for enhancing the activation of PMS. Finally, the C-N, N-O, S-N, C-O and C-S bonds of SMX were easily attacked by ROS to generate the nontoxic intermediates in the MMS/PMS/SMX system. This study offers a new approach to designing high-performance Fe-MoS2 catalysts for the removal of organic pollutants.","sentences":["Iron-based molybdenum disulfide (Fe-MoS2) has emerged as a Fenton-like catalyst for the highly efficient degradation of antibiotics, but the structure-activity relationship remains elusive.","Herein, garland-like MIL-101(Fe)/MoS2 nanosheets (MMS) with dual metal active sites (Fe and Mo) and rich sulfur vacancies were fabricated to directly activate peroxymonosulfate (PMS) for fast degradation of different organic pollutants (phenols, dyes and drugs), even in real water bodies.","The MMS exhibited extremely fast catalytic rate constant of 0.289 min-1 in the degradation of sulfamethoxazole (SMX), which was about 36 and 29 times that of single MoS2 (0.008 min-1) and MIL-101(Fe) (0.01 min-1).","Moreover, MMS with good stability and reusability could reach 92% degradation of SMX after 5 cycles.","Quenching experiments and electron spin resonance (ESR) tests revealed that hydroxyl radicals (.OH) and singlet oxygen (1O2) were the dominant reactive oxygen species (ROS) for SMX degradation.","The integration of experimental works, characterization techniques and density functional theory (DFT) calculations unraveled that the formation of sulfur vacancies in MMS catalyst could expose more Mo sites, improve the charge density and boost the electron transfer, which was conducive to accelerating the Fe3+/Fe2+ cycle for enhancing the activation of PMS.","Finally, the C-N, N-O, S-N, C-O and C-S bonds of SMX were easily attacked by ROS to generate the nontoxic intermediates in the MMS/PMS/SMX system.","This study offers a new approach to designing high-performance Fe-MoS2 catalysts for the removal of organic pollutants."],"url":"http://arxiv.org/abs/2403.16200v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-24 15:41:36","title":"Linear Shafarevich Conjecture in positive characteristic, Hyperbolicity and Applications","abstract":"Given a complex quasi-projective normal variety $X$ and a linear representation $\\varrho:\\pi_1(X)\\to {\\rm GL}_{N}(K)$ with $K$ any field of positive characteristic, we mainly establish the following results:   1. the construction of the Shafarevich morphism ${\\rm sh}_\\varrho:X\\to {\\rm Sh}_\\varrho(X)$ associated with $\\varrho$.   2. In cases where $X$ is projective, $\\varrho$ is faithful and the $\\Gamma$-dimension of $X$ is at most two (e.g. $\\dim X=2$), we prove that the Shafarevich conjecture holds for $X$.   3. In cases where $\\varrho$ is big, we prove that the Green-Griffiths-Lang conjecture holds for $X$.   4. When $\\varrho$ is big and the Zariski closure of $\\varrho(\\pi_1(X))$ is a semisimple algebraic group, we prove that $X$ is pseudo Picard hyperbolic, and strongly of log general type.   5. If $X$ is special or $h$-special, then $\\varrho(\\pi_1(X))$ is virtually abelian.   We also prove Claudon-H\\\"oring-Koll\\'ar's conjecture for complex projective manifolds with linear fundamental groups of any characteristic.","sentences":["Given a complex quasi-projective normal variety $X$ and a linear representation $\\varrho:\\pi_1(X)\\to {\\rm GL}_{N}(K)$ with $K$ any field of positive characteristic, we mainly establish the following results:   1.","the construction of the Shafarevich morphism ${\\rm sh}_\\varrho:X\\to {\\rm Sh}_\\varrho(X)$ associated with $\\varrho$.   2.","In cases where $X$ is projective, $\\varrho$ is faithful and the $\\Gamma$-dimension of $X$ is at most two (e.g. $\\dim X=2$), we prove that the Shafarevich conjecture holds for $X$.   3.","In cases where $\\varrho$ is big, we prove that the Green-Griffiths-Lang conjecture holds for $X$.   4.","When $\\varrho$ is big and the Zariski closure of $\\varrho(\\pi_1(X))$ is a semisimple algebraic group, we prove that $X$ is pseudo Picard hyperbolic, and strongly of log general type.   ","5.","If $X$ is special or $h$-special, then $\\varrho(\\pi_1(X))$ is virtually abelian.   ","We also prove Claudon-H\\\"oring-Koll\\'ar's conjecture for complex projective manifolds with linear fundamental groups of any characteristic."],"url":"http://arxiv.org/abs/2403.16199v1","category":"math.AG"}
{"created":"2024-03-25 12:54:10","title":"Phase Transformation in Lithium Niobate-Lithium Tantalate Solid Solutions (LiNb$_{1-x}$Ta$_x$O$_3$)","abstract":"The investigation of the structural phase transition in the vicinity of the Curie temperature $T_c$ of LiNb$_{1-x}$Ta$_x$O$_3$ crystals is motivated by the expected combination of advantageous high-temperature properties of LiNbO$_3$ and LiTaO$_3$, including high piezoelectric modules and remarkable high-temperature stability, respectively. $T_c$ marks the ultimate limit for exploiting the piezoelectric properties, however transition related structural modifications might impact this and other properties even below $T_c$. Remarkably, the phase transition from the ferroelectric to the paraelectric phase, whose temperature strongly depends on the composition $x$, shows a significant drop in the activation energy of the electrical conductivity. The magnitude, temperature dependence and underlying mechanisms of this drop are discussed from a microscopic perspective. Molecular dynamics calculations in the framework of the density functional theory show that substantial displacements of the cations occur below $T_c$ for both the end compounds LiNbO$_3$ and LiTaO$_3$, and might thus affect the electrical conductivity. Above $T_c$, the migration of lithium ions is presumably facilitated by a shortened diffusion path for the most favorable jump of the lithium ions. Electronic contributions to the conductivity, which become important above 900 K, are explained within the polaronic picture by the formation and migration of free small polarons.","sentences":["The investigation of the structural phase transition in the vicinity of the Curie temperature $T_c$ of LiNb$_{1-x}$Ta$_x$O$_3$ crystals is motivated by the expected combination of advantageous high-temperature properties of LiNbO$_3$ and LiTaO$_3$, including high piezoelectric modules and remarkable high-temperature stability, respectively.","$T_c$ marks the ultimate limit for exploiting the piezoelectric properties, however transition related structural modifications might impact this and other properties even below $T_c$. Remarkably, the phase transition from the ferroelectric to the paraelectric phase, whose temperature strongly depends on the composition $x$, shows a significant drop in the activation energy of the electrical conductivity.","The magnitude, temperature dependence and underlying mechanisms of this drop are discussed from a microscopic perspective.","Molecular dynamics calculations in the framework of the density functional theory show that substantial displacements of the cations occur below $T_c$ for both the end compounds LiNbO$_3$ and LiTaO$_3$, and might thus affect the electrical conductivity.","Above $T_c$, the migration of lithium ions is presumably facilitated by a shortened diffusion path for the most favorable jump of the lithium ions.","Electronic contributions to the conductivity, which become important above 900 K, are explained within the polaronic picture by the formation and migration of free small polarons."],"url":"http://arxiv.org/abs/2403.16717v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-25 12:43:52","title":"An alternative measure for quantifying the heterogeneity in meta-analysis","abstract":"Quantifying the heterogeneity is an important issue in meta-analysis, and among the existing measures, the $I^2$ statistic is most commonly used. In this paper, we first illustrate with a simple example that the $I^2$ statistic is heavily dependent on the study sample sizes, mainly because it is used to quantify the heterogeneity between the observed effect sizes. To reduce the influence of sample sizes, we introduce an alternative measure that aims to directly measure the heterogeneity between the study populations involved in the meta-analysis. We further propose a new estimator, namely the $I_A^2$ statistic, to estimate the newly defined measure of heterogeneity. For practical implementation, the exact formulas of the $I_A^2$ statistic are also derived under two common scenarios with the effect size as the mean difference (MD) or the standardized mean difference (SMD). Simulations and real data analysis demonstrate that the $I_A^2$ statistic provides an asymptotically unbiased estimator for the absolute heterogeneity between the study populations, and it is also independent of the study sample sizes as expected. To conclude, our newly defined $I_A^2$ statistic can be used as a supplemental measure of heterogeneity to monitor the situations where the study effect sizes are indeed similar with little biological difference. In such scenario, the fixed-effect model can be appropriate; nevertheless, when the sample sizes are sufficiently large, the $I^2$ statistic may still increase to 1 and subsequently suggest the random-effects model for meta-analysis.","sentences":["Quantifying the heterogeneity is an important issue in meta-analysis, and among the existing measures, the $I^2$ statistic is most commonly used.","In this paper, we first illustrate with a simple example that the $I^2$ statistic is heavily dependent on the study sample sizes, mainly because it is used to quantify the heterogeneity between the observed effect sizes.","To reduce the influence of sample sizes, we introduce an alternative measure that aims to directly measure the heterogeneity between the study populations involved in the meta-analysis.","We further propose a new estimator, namely the $I_A^2$ statistic, to estimate the newly defined measure of heterogeneity.","For practical implementation, the exact formulas of the $I_A^2$ statistic are also derived under two common scenarios with the effect size as the mean difference (MD) or the standardized mean difference (SMD).","Simulations and real data analysis demonstrate that the $I_A^2$ statistic provides an asymptotically unbiased estimator for the absolute heterogeneity between the study populations, and it is also independent of the study sample sizes as expected.","To conclude, our newly defined $I_A^2$ statistic can be used as a supplemental measure of heterogeneity to monitor the situations where the study effect sizes are indeed similar with little biological difference.","In such scenario, the fixed-effect model can be appropriate; nevertheless, when the sample sizes are sufficiently large, the $I^2$ statistic may still increase to 1 and subsequently suggest the random-effects model for meta-analysis."],"url":"http://arxiv.org/abs/2403.16706v1","category":"stat.ME"}
{"created":"2024-03-25 12:26:11","title":"Interior Schauder estimates for fractional elliptic equations in nondivergence form","abstract":"We obtain sharp interior Schauder estimates for solutions to nonlocal Poisson problems driven by fractional powers of nondivergence form elliptic operators $(-a^{ij}(x) \\partial_{ij})^s$, for $0<s<1$, in bounded domains under minimal regularity assumptions on the coefficients $a^{ij}(x)$. Solutions to the fractional problem are characterized by a local degenerate/singular extension problem. We introduce a novel notion of viscosity solutions for the extension problem and implement Caffarelli's perturbation methodology in the corresponding degenerate/singular Monge--Amp\\`ere geometry to prove Schauder estimates in the extension. This in turn implies interior Schauder estimates for solutions to the fractional nonlocal equation. Furthermore, we prove a new Hopf lemma, the interior Harnack inequality and H\\\"older regularity in the Monge--Amp\\`ere geometry for viscosity solutions to the extension problem.","sentences":["We obtain sharp interior Schauder estimates for solutions to nonlocal Poisson problems driven by fractional powers of nondivergence form elliptic operators $(-a^{ij}(x) \\partial_{ij})^s$, for $0<s<1$, in bounded domains under minimal regularity assumptions on the coefficients $a^{ij}(x)$. Solutions to the fractional problem are characterized by a local degenerate/singular extension problem.","We introduce a novel notion of viscosity solutions for the extension problem and implement Caffarelli's perturbation methodology in the corresponding degenerate/singular Monge--Amp\\`ere geometry to prove Schauder estimates in the extension.","This in turn implies interior Schauder estimates for solutions to the fractional nonlocal equation.","Furthermore, we prove a new Hopf lemma, the interior Harnack inequality and H\\\"older regularity in the Monge--Amp\\`ere geometry for viscosity solutions to the extension problem."],"url":"http://arxiv.org/abs/2403.16693v1","category":"math.AP"}
{"created":"2024-03-25 12:22:24","title":"JWST MIRI Flight Performance: Imaging","abstract":"The Mid-Infrared Instrument (MIRI) aboard the James Webb Space Telescope (JWST) provides the observatory with a huge advance in mid-infrared imaging and spectroscopy covering the wavelength range of 5 to 28 microns. This paper describes the performance and characteristics of the MIRI imager as understood during observatory commissioning activities, and through its first year of science operations. We discuss the measurements and results of the imager's point spread function, flux calibration, background, distortion and flat fields as well as results pertaining to best observing practices for MIRI imaging, and discuss known imaging artefacts that may be seen during or after data processing. Overall, we show that the MIRI imager has met or exceeded all its pre-flight requirements, and we expect it to make a significant contribution to mid-infrared science for the astronomy community for years to come.","sentences":["The Mid-Infrared Instrument (MIRI) aboard the James Webb Space Telescope (JWST) provides the observatory with a huge advance in mid-infrared imaging and spectroscopy covering the wavelength range of 5 to 28 microns.","This paper describes the performance and characteristics of the MIRI imager as understood during observatory commissioning activities, and through its first year of science operations.","We discuss the measurements and results of the imager's point spread function, flux calibration, background, distortion and flat fields as well as results pertaining to best observing practices for MIRI imaging, and discuss known imaging artefacts that may be seen during or after data processing.","Overall, we show that the MIRI imager has met or exceeded all its pre-flight requirements, and we expect it to make a significant contribution to mid-infrared science for the astronomy community for years to come."],"url":"http://arxiv.org/abs/2403.16686v1","category":"astro-ph.IM"}
{"created":"2024-03-25 12:13:26","title":"Self-similar solutions in cylindrical magneto-hydrodynamic blast waves with energy injection at the centre","abstract":"The evolution of shocks induced by massive stars does not depend only on the ambient magnetic field strength, but also on its orientation. In the present work, the dynamics of a magnetized blast wave is investigated under the influence of both azimuthal and axial ambient magnetic fields. The blast wave is driven by a central source and forms a shell that results from the accumulation of interstellar matter behind the shock front. A similarity form of the ambient magnetic field and a cylindrical geometry of the blast wave are assumed to obtain self-similar solutions. The model is studied separately for both azimuthal and axial magnetic field and applied to stellar wind bubbles and supernova remnants respectively, using 1D numerical simulations. We found that the magnetized blast wave differs from the self-similar case without an ambient magnetic field. The forward shock front goes slower in the azimuthal case and faster in the axial one. For both tangential orientations, the thickness of the shell increases with the magnetic strength. In the azimuthal case, the thermal energy can be converted to magnetic energy near the inner boundary of the shell. Thus, the temperature drops and the magnetic field increases at the tangential discontinuity of the stellar wind bubble. In the axial case of a supernova remnant, the numerical solution al w ays follows a special curve in the parameter space given by the self-similar model.","sentences":["The evolution of shocks induced by massive stars does not depend only on the ambient magnetic field strength, but also on its orientation.","In the present work, the dynamics of a magnetized blast wave is investigated under the influence of both azimuthal and axial ambient magnetic fields.","The blast wave is driven by a central source and forms a shell that results from the accumulation of interstellar matter behind the shock front.","A similarity form of the ambient magnetic field and a cylindrical geometry of the blast wave are assumed to obtain self-similar solutions.","The model is studied separately for both azimuthal and axial magnetic field and applied to stellar wind bubbles and supernova remnants respectively, using 1D numerical simulations.","We found that the magnetized blast wave differs from the self-similar case without an ambient magnetic field.","The forward shock front goes slower in the azimuthal case and faster in the axial one.","For both tangential orientations, the thickness of the shell increases with the magnetic strength.","In the azimuthal case, the thermal energy can be converted to magnetic energy near the inner boundary of the shell.","Thus, the temperature drops and the magnetic field increases at the tangential discontinuity of the stellar wind bubble.","In the axial case of a supernova remnant, the numerical solution al w ays follows a special curve in the parameter space given by the self-similar model."],"url":"http://arxiv.org/abs/2403.16675v1","category":"astro-ph.HE"}
{"created":"2024-03-25 11:32:05","title":"Clustering Propagation for Universal Medical Image Segmentation","abstract":"Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$ necessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both training time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$ issues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$ universal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$ Slice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive segmentation within a single model and one training session. Inspired by clustering-based segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks. Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs. S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions. It can also handle multi-class interactions with each of them serving to initialize different centroids. Experiments on three benchmarks demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups.","sentences":["Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$ necessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both training time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$ issues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$ universal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$ Slice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive segmentation within a single model and one training session.","Inspired by clustering-based segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$","This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks.","Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs.","S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions.","It can also handle multi-class interactions with each of them serving to initialize different centroids.","Experiments on three benchmarks demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups."],"url":"http://arxiv.org/abs/2403.16646v1","category":"cs.CV"}
{"created":"2024-03-25 11:28:57","title":"Linearly stable self-similar solutions of semilinear heat equations","abstract":"We classify the smooth linearly stable self-similar solutions of the semilinear heat equation $u_t=\\Delta u+|u|^{p-1}u$ in $\\mathbb{R}^n\\times (0,T)$ under an integral condition for all $p>1$. As a corollary, we prove that finite time blowing up solutions of this equation on a bounded convex domain with $u(\\cdot,0)\\geq 0$ and $u_t(\\cdot,0)\\geq 0$ converges to a constant after rescaling at the blow-up point for all $p>1$.","sentences":["We classify the smooth linearly stable self-similar solutions of the semilinear heat equation $u_t=\\Delta u+|u|^{p-1}u$ in $\\mathbb{R}^n\\times (0,T)$ under an integral condition for all $p>1$. As a corollary, we prove that finite time blowing up solutions of this equation on a bounded convex domain with $u(\\cdot,0)\\geq 0$ and $u_t(\\cdot,0)\\geq 0$ converges to a constant after rescaling at the blow-up point for all $p>1$."],"url":"http://arxiv.org/abs/2403.16641v1","category":"math.AP"}
{"created":"2024-03-25 11:20:23","title":"A comparative analysis of embedding models for patent similarity","abstract":"This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity. Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed.","sentences":["This paper makes two contributions to the field of text-based patent similarity.","First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation.","Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task.","To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners.","Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models.","Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity.","Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed."],"url":"http://arxiv.org/abs/2403.16630v1","category":"cs.CL"}
{"created":"2024-03-25 11:09:02","title":"Improving the Optimization in Model Predictive Controllers: Scheduling Large Groups of Electric Vehicles","abstract":"In parking lots with large groups of electric vehicles (EVs), charging has to happen in a coordinated manner, among others, due to the high load per vehicle and the limited capacity of the electricity grid. To achieve such coordination, model predictive control can be applied, thereby repeatedly solving an optimization problem. Due to its repetitive nature and its dependency on the time granularity, optimization has to be (computationally) efficient. The work presented here focuses on that optimization subroutine, its computational efficiency and how to speed up the optimization for large groups of EVs. In particular, we adapt FOCS, an algorithm that can solve the underlying optimization problem, to better suit the repetitive set-up of model predictive control by adding a pre-mature stop feature. Based on real-world data, we empirically show that the added feature speeds up the median computation time for 1-minute granularity by up to 44%. Furthermore, since FOCS is an algorithm that uses maximum flow methods as a subroutine, the impact of choosing various maximum flow methods on the runtime is investigated. Finally, we compare FOCS to a commercially available solver, concluding that FOCS outperforms the state-of-the-art when making a full-day schedule for large groups of EVs.","sentences":["In parking lots with large groups of electric vehicles (EVs), charging has to happen in a coordinated manner, among others, due to the high load per vehicle and the limited capacity of the electricity grid.","To achieve such coordination, model predictive control can be applied, thereby repeatedly solving an optimization problem.","Due to its repetitive nature and its dependency on the time granularity, optimization has to be (computationally) efficient.","The work presented here focuses on that optimization subroutine, its computational efficiency and how to speed up the optimization for large groups of EVs.","In particular, we adapt FOCS, an algorithm that can solve the underlying optimization problem, to better suit the repetitive set-up of model predictive control by adding a pre-mature stop feature.","Based on real-world data, we empirically show that the added feature speeds up the median computation time for 1-minute granularity by up to 44%.","Furthermore, since FOCS is an algorithm that uses maximum flow methods as a subroutine, the impact of choosing various maximum flow methods on the runtime is investigated.","Finally, we compare FOCS to a commercially available solver, concluding that FOCS outperforms the state-of-the-art when making a full-day schedule for large groups of EVs."],"url":"http://arxiv.org/abs/2403.16622v1","category":"math.OC"}
{"created":"2024-03-25 10:44:38","title":"Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts","abstract":"Tasks such as semantic search and clustering on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions. Pre-trained language models have advanced performance in crisis informatics, but their contextual embeddings lack semantic meaningfulness. Although the CrisisTransformers family includes a sentence encoder to address the semanticity issue, it remains monolingual, processing only English texts. Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts. Therefore, we propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity. Results in sentence encoding and sentence matching tasks are promising, suggesting these models could serve as robust baselines when embedding multi-lingual crisis-related social media texts. The models are publicly available at: https://huggingface.co/crisistransformers.","sentences":["Tasks such as semantic search and clustering on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions.","Pre-trained language models have advanced performance in crisis informatics, but their contextual embeddings lack semantic meaningfulness.","Although the CrisisTransformers family includes a sentence encoder to address the semanticity issue, it remains monolingual, processing only English texts.","Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts.","Therefore, we propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity.","Results in sentence encoding and sentence matching tasks are promising, suggesting these models could serve as robust baselines when embedding multi-lingual crisis-related social media texts.","The models are publicly available at: https://huggingface.co/crisistransformers."],"url":"http://arxiv.org/abs/2403.16614v1","category":"cs.CL"}
{"created":"2024-03-25 10:42:48","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","abstract":"Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change. Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world. Calibration of the neural networks provides a way to ensure our confidence in the predictions. However, calibrating regression models is an under-researched topic, especially in forecasters. We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies. We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts. We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters.","sentences":["Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change.","Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world.","Calibration of the neural networks provides a way to ensure our confidence in the predictions.","However, calibrating regression models is an under-researched topic, especially in forecasters.","We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies.","We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts.","We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters."],"url":"http://arxiv.org/abs/2403.16612v1","category":"cs.LG"}
{"created":"2024-03-25 10:13:52","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for Medical Image Segmentation","abstract":"Deploying deep learning (DL) models in medical applications relies on predictive performance and other critical factors, such as conveying trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide potential solutions for evaluating prediction reliability and improving the model confidence calibration. Despite increasing interest in UE, challenges persist, such as the need for explicit methods to capture aleatoric uncertainty and align uncertainty estimates with real-life disagreements among domain experts. This paper proposes an Expert Disagreement-Guided Uncertainty Estimation (EDUE) for medical image segmentation. By leveraging variability in ground-truth annotations from multiple raters, we guide the model during training and incorporate random sampling-based strategies to enhance calibration confidence. Our method achieves 55% and 23% improvement in correlation on average with expert disagreements at the image and pixel levels, respectively, better calibration, and competitive segmentation performance compared to the state-of-the-art deep ensembles, requiring only a single forward pass.","sentences":["Deploying deep learning (DL) models in medical applications relies on predictive performance and other critical factors, such as conveying trustworthy predictive uncertainty.","Uncertainty estimation (UE) methods provide potential solutions for evaluating prediction reliability and improving the model confidence calibration.","Despite increasing interest in UE, challenges persist, such as the need for explicit methods to capture aleatoric uncertainty and align uncertainty estimates with real-life disagreements among domain experts.","This paper proposes an Expert Disagreement-Guided Uncertainty Estimation (EDUE) for medical image segmentation.","By leveraging variability in ground-truth annotations from multiple raters, we guide the model during training and incorporate random sampling-based strategies to enhance calibration confidence.","Our method achieves 55% and 23% improvement in correlation on average with expert disagreements at the image and pixel levels, respectively, better calibration, and competitive segmentation performance compared to the state-of-the-art deep ensembles, requiring only a single forward pass."],"url":"http://arxiv.org/abs/2403.16594v1","category":"eess.IV"}
{"created":"2024-03-25 09:58:59","title":"Linearised Calder\u00f3n problem: Reconstruction of unbounded perturbations in 3D","abstract":"Recently an algorithm was given in [Garde & Hyv\\\"onen, SIAM J. Math. Anal., 2024] for exact direct reconstruction of any $L^2$ perturbation from linearised data in the two-dimensional linearised Calder\\'on problem. It was a simple forward substitution method based on a 2D Zernike basis. We now consider the three-dimensional linearised Calder\\'on problem in a ball, and use a 3D Zernike basis to obtain a method for exact direct reconstruction of any $L^3$ perturbation from linearised data. The method is likewise a forward substitution, hence making it very efficient to numerically implement. Moreover, the 3D method only makes use of a relatively small subset of boundary measurements for exact reconstruction, compared to a full $L^2$ basis of current densities.","sentences":["Recently an algorithm was given in [Garde & Hyv\\\"onen, SIAM J. Math.","Anal., 2024] for exact direct reconstruction of any $L^2$ perturbation from linearised data in the two-dimensional linearised Calder\\'on problem.","It was a simple forward substitution method based on a 2D Zernike basis.","We now consider the three-dimensional linearised Calder\\'on problem in a ball, and use a 3D Zernike basis to obtain a method for exact direct reconstruction of any $L^3$ perturbation from linearised data.","The method is likewise a forward substitution, hence making it very efficient to numerically implement.","Moreover, the 3D method only makes use of a relatively small subset of boundary measurements for exact reconstruction, compared to a full $L^2$ basis of current densities."],"url":"http://arxiv.org/abs/2403.16588v1","category":"math.AP"}
{"created":"2024-03-25 08:35:19","title":"Differentially Private Online Federated Learning with Correlated Noise","abstract":"We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.","sentences":["We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models.","To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility.","Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition.","Subject to an $(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments.","Numerical experiments validate the efficacy of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.16542v1","category":"cs.LG"}
{"created":"2024-03-25 08:34:49","title":"Effects of tensor spin polarization on the chiral restoration and deconfinement phase transitions","abstract":"Effects of tensor spin polarization (TSP) on the chiral restoration and deconfinement phase transitions are studied in Polyakov loop extended Nambu-Jona-Lasinio (PNJL) model. For chiral phase transition, the higher the polarized degree of quark-antiquark pairs under the strong magnetic field, the higher the phase transition temperature. The TSP corrects the position of the critical end point (CEP). The small impact of TSP on the phase transition temperature is found for the deconfinement phase transition. On the other hand, we divide the phase space into three ranges based on the phase diagram obtained from the PNJL model: the confinement phase with chiral symmetry broken, the deconfinement phase with restored chiral symmetry, and the confinement phase with restored chiral symmetry (quarkonic phase). It is found that TSP has only a very small effect on the anisotropic pressure in the deconfined phase with chiral symmetry restored and the quarkyonic phase, but it has a very strong effect on the anisotropic pressure in the confined phase with chiral symmetry broken. This is because TSP is closely related to chiral symmetry. The restoration of chiral symmetry means the dissociation of spin polarization condensate.","sentences":["Effects of tensor spin polarization (TSP) on the chiral restoration and deconfinement phase transitions are studied in Polyakov loop extended Nambu-Jona-Lasinio (PNJL) model.","For chiral phase transition, the higher the polarized degree of quark-antiquark pairs under the strong magnetic field, the higher the phase transition temperature.","The TSP corrects the position of the critical end point (CEP).","The small impact of TSP on the phase transition temperature is found for the deconfinement phase transition.","On the other hand, we divide the phase space into three ranges based on the phase diagram obtained from the PNJL model: the confinement phase with chiral symmetry broken, the deconfinement phase with restored chiral symmetry, and the confinement phase with restored chiral symmetry (quarkonic phase).","It is found that TSP has only a very small effect on the anisotropic pressure in the deconfined phase with chiral symmetry restored and the quarkyonic phase, but it has a very strong effect on the anisotropic pressure in the confined phase with chiral symmetry broken.","This is because TSP is closely related to chiral symmetry.","The restoration of chiral symmetry means the dissociation of spin polarization condensate."],"url":"http://arxiv.org/abs/2403.16541v1","category":"hep-ph"}
{"created":"2024-03-25 08:31:14","title":"DOrA: 3D Visual Grounding with Order-Aware Referring","abstract":"3D visual grounding aims to identify the target object within a 3D point cloud scene referred to by a natural language description. While previous works attempt to exploit the verbo-visual relation with proposed cross-modal transformers, unstructured natural utterances and scattered objects might lead to undesirable performances. In this paper, we introduce DOrA, a novel 3D visual grounding framework with Order-Aware referring. DOrA is designed to leverage Large Language Models (LLMs) to parse language description, suggesting a referential order of anchor objects. Such ordered anchor objects allow DOrA to update visual features and locate the target object during the grounding process. Experimental results on the NR3D and ScanRefer datasets demonstrate our superiority in both low-resource and full-data scenarios. In particular, DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding accuracy under 1% data and 10% data settings, respectively.","sentences":["3D visual grounding aims to identify the target object within a 3D point cloud scene referred to by a natural language description.","While previous works attempt to exploit the verbo-visual relation with proposed cross-modal transformers, unstructured natural utterances and scattered objects might lead to undesirable performances.","In this paper, we introduce DOrA, a novel 3D visual grounding framework with Order-Aware referring.","DOrA is designed to leverage Large Language Models (LLMs) to parse language description, suggesting a referential order of anchor objects.","Such ordered anchor objects allow DOrA to update visual features and locate the target object during the grounding process.","Experimental results on the NR3D and ScanRefer datasets demonstrate our superiority in both low-resource and full-data scenarios.","In particular, DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding accuracy under 1% data and 10% data settings, respectively."],"url":"http://arxiv.org/abs/2403.16539v1","category":"cs.CV"}
{"created":"2024-03-25 08:26:20","title":"Arm-Constrained Curriculum Learning for Loco-Manipulation of the Wheel-Legged Robot","abstract":"Incorporating a robotic manipulator into a wheel-legged robot enhances its agility and expands its potential for practical applications. However, the presence of potential instability and uncertainties presents additional challenges for control objectives. In this paper, we introduce an arm-constrained curriculum learning architecture to tackle the issues introduced by adding the manipulator. Firstly, we develop an arm-constrained reinforcement learning algorithm to ensure safety and stability in control performance. Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware curriculum learning method. The policy is first trained in Isaac gym and transferred to the physical robot to do dynamic grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task. The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master dynamic grasping skills, allowing it to chase and catch a moving object while in motion. The code can be found at https://github.com/aCodeDog/legged-robots-manipulation. To view the supplemental video, please visit https://youtu.be/sNXT-rwPNMM.","sentences":["Incorporating a robotic manipulator into a wheel-legged robot enhances its agility and expands its potential for practical applications.","However, the presence of potential instability and uncertainties presents additional challenges for control objectives.","In this paper, we introduce an arm-constrained curriculum learning architecture to tackle the issues introduced by adding the manipulator.","Firstly, we develop an arm-constrained reinforcement learning algorithm to ensure safety and stability in control performance.","Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware curriculum learning method.","The policy is first trained in Isaac gym and transferred to the physical robot to do dynamic grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task.","The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master dynamic grasping skills, allowing it to chase and catch a moving object while in motion.","The code can be found at https://github.com/aCodeDog/legged-robots-manipulation.","To view the supplemental video, please visit https://youtu.be/sNXT-rwPNMM."],"url":"http://arxiv.org/abs/2403.16535v1","category":"cs.RO"}
{"created":"2024-03-25 08:14:22","title":"Open-Set Recognition in the Age of Vision-Language Models","abstract":"Are vision-language models (VLMs) open-set models because they are trained on internet-scale datasets? We answer this question with a clear no - VLMs introduce closed-set assumptions via their finite query set, making them vulnerable to open-set conditions. We systematically evaluate VLMs for open-set recognition and find they frequently misclassify objects not contained in their query set, leading to alarmingly low precision when tuned for high recall and vice versa. We show that naively increasing the size of the query set to contain more and more classes does not mitigate this problem, but instead causes diminishing task performance and open-set performance. We establish a revised definition of the open-set problem for the age of VLMs, define a new benchmark and evaluation protocol to facilitate standardised evaluation and research in this important area, and evaluate promising baseline approaches based on predictive uncertainty and dedicated negative embeddings on a range of VLM classifiers and object detectors.","sentences":["Are vision-language models (VLMs) open-set models because they are trained on internet-scale datasets?","We answer this question with a clear no - VLMs introduce closed-set assumptions via their finite query set, making them vulnerable to open-set conditions.","We systematically evaluate VLMs for open-set recognition and find they frequently misclassify objects not contained in their query set, leading to alarmingly low precision when tuned for high recall and vice versa.","We show that naively increasing the size of the query set to contain more and more classes does not mitigate this problem, but instead causes diminishing task performance and open-set performance.","We establish a revised definition of the open-set problem for the age of VLMs, define a new benchmark and evaluation protocol to facilitate standardised evaluation and research in this important area, and evaluate promising baseline approaches based on predictive uncertainty and dedicated negative embeddings on a range of VLM classifiers and object detectors."],"url":"http://arxiv.org/abs/2403.16528v1","category":"cs.CV"}
{"created":"2024-03-25 07:40:29","title":"In situ growth of hydrophilic nickel-cobalt layered double hydroxides nanosheets on biomass waste-derived porous carbon for high-performance hybrid supercapacitors","abstract":"Rational design and cost-effective fabrication of layered double hydroxides (LDHs) nanosheets with extraordinary electrochemical performance is a key challenge for hybrid supercapacitors (HSCs). Herein, we report a facile in situ growth methodology to eco-friendly synthesize hydrophilic NiCo-LDHs nanosheets on biomass waste-derived porous carbon (BC) for robust high-performance HSC cathode. The in situ growth process under ultrasonication realizes the rational arrangement of NiCo-LDHs nanosheets on the surface of BC, which effectively increases the specific surface area, promotes the electronic conductivity and enhances the wettability of NiCo-LDHs nanosheets without affecting their thickness values. With the beneficial effects of ultrathin thickness of LDHs nanosheets (6.20 nm), large specific surface area (2324.1 m2 g-1), low charge transfer resistance (1.65 ohm), and high wettability with electrolyte (34-35 degree), the obtained Ni2Co1-LDHs/BC50 electrode possesses an ultra-high specific capacitance of 2390 F g-1 (956 C g-1) at 1 A g-1, which is superior to most reported values. Furthermore, an assembled Ni2Co1-LDHs/BC50//YP-80F HSC delivers a maximum specific energy of 52.47 Wh kg-1 at 375 W kg-1, and maintains a high capacitance retention of 75.9% even after 4000 cycles. This work provides a facile approach to fabricate LDHs nanosheets based cathode materials for high-performance HSCs.","sentences":["Rational design and cost-effective fabrication of layered double hydroxides (LDHs) nanosheets with extraordinary electrochemical performance is a key challenge for hybrid supercapacitors (HSCs).","Herein, we report a facile in situ growth methodology to eco-friendly synthesize hydrophilic NiCo-LDHs nanosheets on biomass waste-derived porous carbon (BC) for robust high-performance HSC cathode.","The in situ growth process under ultrasonication realizes the rational arrangement of NiCo-LDHs nanosheets on the surface of BC, which effectively increases the specific surface area, promotes the electronic conductivity and enhances the wettability of NiCo-LDHs nanosheets without affecting their thickness values.","With the beneficial effects of ultrathin thickness of LDHs nanosheets (6.20 nm), large specific surface area (2324.1 m2 g-1), low charge transfer resistance (1.65 ohm), and high wettability with electrolyte (34-35 degree), the obtained Ni2Co1-LDHs/BC50 electrode possesses an ultra-high specific capacitance of 2390 F g-1 (956 C g-1) at 1 A g-1, which is superior to most reported values.","Furthermore, an assembled Ni2Co1-LDHs/BC50//YP-80F HSC delivers a maximum specific energy of 52.47 Wh kg-1 at 375 W kg-1, and maintains a high capacitance retention of 75.9% even after 4000 cycles.","This work provides a facile approach to fabricate LDHs nanosheets based cathode materials for high-performance HSCs."],"url":"http://arxiv.org/abs/2403.16506v1","category":"physics.app-ph"}
{"created":"2024-03-25 07:39:14","title":"Ergodic theorem for branching Markov chains indexed by trees with arbitrary shape","abstract":"We prove an ergodic theorem for Markov chains indexed by the Ulam-Harris-Neveu tree over large subsets with arbitrary shape under two assumptions: with high probability, two vertices in the large subset are far from each other and have their common ancestor close to the root. The assumption on the common ancestor can be replaced by some regularity assumption on the Markov transition kernel. We verify that those assumptions are satisfied for some usual trees. Finally, with Markov-Chain Monte-Carlo considerations in mind, we prove when the underlying Markov chain is stationary and reversible that the Markov chain, that is the line graph, yields minimal variance for the empirical average estimator among trees with a given number of nodes.","sentences":["We prove an ergodic theorem for Markov chains indexed by the Ulam-Harris-Neveu tree over large subsets with arbitrary shape under two assumptions: with high probability, two vertices in the large subset are far from each other and have their common ancestor close to the root.","The assumption on the common ancestor can be replaced by some regularity assumption on the Markov transition kernel.","We verify that those assumptions are satisfied for some usual trees.","Finally, with Markov-Chain Monte-Carlo considerations in mind, we prove when the underlying Markov chain is stationary and reversible that the Markov chain, that is the line graph, yields minimal variance for the empirical average estimator among trees with a given number of nodes."],"url":"http://arxiv.org/abs/2403.16505v1","category":"math.PR"}
{"created":"2024-03-25 07:35:28","title":"Medical Image Registration and Its Application in Retinal Images: A Review","abstract":"Medical image registration is vital for disease diagnosis and treatment with its ability to merge diverse information of images, which may be captured under different times, angles, or modalities. Although several surveys have reviewed the development of medical image registration, these surveys have not systematically summarized methodologies of existing medical image registration methods. To this end, we provide a comprehensive review of these methods from traditional and deep learning-based directions, aiming to help audiences understand the development of medical image registration quickly. In particular, we review recent advances in retinal image registration at the end of each section, which has not attracted much attention. Additionally, we also discuss the current challenges of retinal image registration and provide insights and prospects for future research.","sentences":["Medical image registration is vital for disease diagnosis and treatment with its ability to merge diverse information of images, which may be captured under different times, angles, or modalities.","Although several surveys have reviewed the development of medical image registration, these surveys have not systematically summarized methodologies of existing medical image registration methods.","To this end, we provide a comprehensive review of these methods from traditional and deep learning-based directions, aiming to help audiences understand the development of medical image registration quickly.","In particular, we review recent advances in retinal image registration at the end of each section, which has not attracted much attention.","Additionally, we also discuss the current challenges of retinal image registration and provide insights and prospects for future research."],"url":"http://arxiv.org/abs/2403.16502v1","category":"cs.CV"}
{"created":"2024-03-25 06:50:25","title":"Learning from Reduced Labels for Long-Tailed Data","abstract":"Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including ImageNet validate the effectiveness of our approach, surpassing the performance of state-of-the-art weakly supervised methods.","sentences":["Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming.","Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes.","To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label.","The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data.","Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels.","Extensive experiments conducted on benchmark datasets including ImageNet validate the effectiveness of our approach, surpassing the performance of state-of-the-art weakly supervised methods."],"url":"http://arxiv.org/abs/2403.16469v1","category":"cs.LG"}
{"created":"2024-03-25 06:48:50","title":"One-Shot Non-Catalytic Distributed Purity Distillation","abstract":"Pure states are an important resource in many quantum information processing protocols. However, even making a fixed pure state, say $|0\\rangle$, in the laboratory requires a considerable amount of effort. Often one ends up with a mixed state $\\rho$ whose classical description is nevertheless known. Hence it is important to develop protocols that extract a fixed pure state from a known mixed state. In this work, we study the problem of extracting a fixed pure state $|0\\rangle^{A'} |0\\rangle^{B'}$ from a known pure state $\\rho^{AB}$ distributed between two parties $A$ and $B$. Here, $A'$, $B'$ are subspaces of $A$, $B$ and the total amount of purity extracted is $\\log |A'| + \\log |B'|$. The parties can borrow local pure ancilla, apply local unitary operations and send a message from $A$ to $B$ through a dephasing channel. If local pure ancilla is borrowed, it must be subtracted in order to properly account for the purity extracted. We obtain the most efficient achievable bounds on one shot distributed purity extraction, in terms of the rate of local ancilla borrowed by the protocol, while distilling pure qubits at the best known rate. Our protocols borrow little to no local pure ancilla. Our bounds improve upon the existing bounds for this problem in both one shot as well as asymptotic iid settings. In particular they subsume all the asymptotic iid results of Devetak and Krovi-Devetak. In addition, we derive upper bounds for the rate of distillation in the one shot setting, which nearly match our achievable bounds.","sentences":["Pure states are an important resource in many quantum information processing protocols.","However, even making a fixed pure state, say $|0\\rangle$, in the laboratory requires a considerable amount of effort.","Often one ends up with a mixed state $\\rho$ whose classical description is nevertheless known.","Hence it is important to develop protocols that extract a fixed pure state from a known mixed state.","In this work, we study the problem of extracting a fixed pure state $|0\\rangle^{A'} |0\\rangle^{B'}$ from a known pure state $\\rho^{AB}$ distributed between two parties $A$ and $B$. Here, $A'$, $B'$ are subspaces of $A$, $B$ and the total amount of purity extracted is $\\log |A'| + \\log |B'|$. The parties can borrow local pure ancilla, apply local unitary operations and send a message from $A$ to $B$ through a dephasing channel.","If local pure ancilla is borrowed, it must be subtracted in order to properly account for the purity extracted.","We obtain the most efficient achievable bounds on one shot distributed purity extraction, in terms of the rate of local ancilla borrowed by the protocol, while distilling pure qubits at the best known rate.","Our protocols borrow little to no local pure ancilla.","Our bounds improve upon the existing bounds for this problem in both one shot as well as asymptotic iid settings.","In particular they subsume all the asymptotic iid results of Devetak and Krovi-Devetak.","In addition, we derive upper bounds for the rate of distillation in the one shot setting, which nearly match our achievable bounds."],"url":"http://arxiv.org/abs/2403.16466v1","category":"quant-ph"}
{"created":"2024-03-25 06:34:26","title":"Topological iron silicide with H* intermediate modulated surface for efficient electrocatalytic hydrogenation of nitrobenzene in neutral medium","abstract":"Electrocatalytic hydrogenation of nitrobenzene (Ph-NO2) reaction (EHNR) has been considered as a potential alternative to the traditional thermocatalytic process in the production of high-value aniline (Ph-NH2). However, due to the absence of robust catalyst and low surface H* coverage, the EHNR faces the challenges of undesired performance and indetermined mechanism. Herein, we construct a type of noble-metal free topological FeSi (M-FeSi) materials through a solvent-free microwave strategy for efficient EHNR in neutral medium. Impressively, benefiting from abundant active H* intermediates on the surface of M-FeSi catalyst, the topological M-FeSi catalyst exhibits 99.7% conversion of Ph-NO2 and 93.8% yield of Ph-NH2 after 200 C in neutral medium, which are superior to previous candidates and FeSi catalyst synthesized via the traditional arc-melting method under same conditions. Besides, theoretical calculations validate that high surface H* coverage over M-FeSi catalyst is conducive to switching the rate-determining step from Ph-NO2* Ph-NO* to Ph-NO* Ph-NHOH*, and thus decreasing the total energy barrier of electrocatalytic Ph-NH2 production.","sentences":["Electrocatalytic hydrogenation of nitrobenzene (Ph-NO2) reaction (EHNR) has been considered as a potential alternative to the traditional thermocatalytic process in the production of high-value aniline (Ph-NH2).","However, due to the absence of robust catalyst and low surface H* coverage, the EHNR faces the challenges of undesired performance and indetermined mechanism.","Herein, we construct a type of noble-metal free topological FeSi (M-FeSi) materials through a solvent-free microwave strategy for efficient EHNR in neutral medium.","Impressively, benefiting from abundant active H* intermediates on the surface of M-FeSi catalyst, the topological M-FeSi catalyst exhibits 99.7% conversion of Ph-NO2 and 93.8% yield of Ph-NH2 after 200 C in neutral medium, which are superior to previous candidates and FeSi catalyst synthesized via the traditional arc-melting method under same conditions.","Besides, theoretical calculations validate that high surface H* coverage over M-FeSi catalyst is conducive to switching the rate-determining step from Ph-NO2* Ph-NO* to Ph-NO* Ph-NHOH*, and thus decreasing the total energy barrier of electrocatalytic Ph-NH2 production."],"url":"http://arxiv.org/abs/2403.16455v1","category":"physics.chem-ph"}
{"created":"2024-03-25 06:15:21","title":"KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models","abstract":"Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks. Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper, We introduce \\textit{KIT-19} as an instruction dataset for the development of LLM in Korean. \\textit{KIT-19} is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness. The experimental results show that the model trained on \\textit{KIT-19} significantly outperforms existing Korean LLMs. Based on the its quality and empirical results, this paper proposes that \\textit{KIT-19} has the potential to make a substantial contribution to the future improvement of Korean LLMs' performance.","sentences":["Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks.","Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available.","In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English.","In this paper, We introduce \\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.","\\textit{KIT-19} is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks.","In this paper, we train a Korean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness.","The experimental results show that the model trained on \\textit{KIT-19} significantly outperforms existing Korean LLMs.","Based on the its quality and empirical results, this paper proposes that \\textit{KIT-19} has the potential to make a substantial contribution to the future improvement of Korean LLMs' performance."],"url":"http://arxiv.org/abs/2403.16444v1","category":"cs.CL"}
{"created":"2024-03-25 05:58:33","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","abstract":"High-definition (HD) maps have played an integral role in the development of modern autonomous vehicle (AV) stacks, albeit with high associated labeling and maintenance costs. As a result, many recent works have proposed methods for estimating HD maps online from sensor data, enabling AVs to operate outside of previously-mapped regions. However, current online map estimation approaches are developed in isolation of their downstream tasks, complicating their integration in AV stacks. In particular, they do not produce uncertainty or confidence estimates. In this work, we extend multiple state-of-the-art online map estimation methods to additionally estimate uncertainty and show how this enables more tightly integrating online mapping with trajectory forecasting. In doing so, we find that incorporating uncertainty yields up to 50% faster training convergence and up to 15% better prediction performance on the real-world nuScenes driving dataset.","sentences":["High-definition (HD) maps have played an integral role in the development of modern autonomous vehicle (AV) stacks, albeit with high associated labeling and maintenance costs.","As a result, many recent works have proposed methods for estimating HD maps online from sensor data, enabling AVs to operate outside of previously-mapped regions.","However, current online map estimation approaches are developed in isolation of their downstream tasks, complicating their integration in AV stacks.","In particular, they do not produce uncertainty or confidence estimates.","In this work, we extend multiple state-of-the-art online map estimation methods to additionally estimate uncertainty and show how this enables more tightly integrating online mapping with trajectory forecasting.","In doing so, we find that incorporating uncertainty yields up to 50% faster training convergence and up to 15% better prediction performance on the real-world nuScenes driving dataset."],"url":"http://arxiv.org/abs/2403.16439v1","category":"cs.RO"}
{"created":"2024-03-25 05:17:37","title":"Detection of spin pumping free of rectification and thermal artefacts in molecular-based ferromagnetic insulator V[TCNE]x~2","abstract":"The molecular-based ferrimagnetic insulator V(TCNE)x has gained recent interest for efficient spin-wave excitation due to its low Gilbert damping ratio a=4E-5, and narrow ferromagnetic resonance linewidth f=1Oe. Here we report a clean spin pumping signal detected on V(TCNE)x/metal bilayer structures, free from spin rectification or thermal artifacts. On-chip coupling of microwave power is achieved via a coplanar waveguide to measure the in-plane angle-dependence of the inverse spin-Hall effect under ferromagnetic resonance conditions with respect to a constant external magnetic field. A signature of pure spin current from V(TCNE)x is observed in both platinum and permalloy metal layers, demonstrating the utility of V(TCNE)x for magnon spintronics studies in molecule/solid-state heterostructures.","sentences":["The molecular-based ferrimagnetic insulator V(TCNE)x has gained recent interest for efficient spin-wave excitation due to its low Gilbert damping ratio a=4E-5, and narrow ferromagnetic resonance linewidth f=1Oe.","Here we report a clean spin pumping signal detected on V(TCNE)x/metal bilayer structures, free from spin rectification or thermal artifacts.","On-chip coupling of microwave power is achieved via a coplanar waveguide to measure the in-plane angle-dependence of the inverse spin-Hall effect under ferromagnetic resonance conditions with respect to a constant external magnetic field.","A signature of pure spin current from V(TCNE)x is observed in both platinum and permalloy metal layers, demonstrating the utility of V(TCNE)x for magnon spintronics studies in molecule/solid-state heterostructures."],"url":"http://arxiv.org/abs/2403.16429v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-25 04:19:26","title":"Wide-Spectral-Band Nuller Insensitive to Finite Stellar Angular Diameter with a One-Dimensional Diffraction-Limited Coronagraph","abstract":"Potentially habitable planets around nearby stars less massive than solar-type stars could join targets of the spectroscopy of the planetary reflected light with future space telescopes. However, the orbits of most of these planets occur near the diffraction limit for 6-m-diameter telescopes. Thus, while securing contrast-mitigation ability under a broad spectral bandwidth and a finite stellar angular diameter, we must maintain planetary throughput even at the diffraction-limited angles to be able to reduce the effect of the photon noise within a reasonable observation time. A one-dimensional diffraction-limited coronagraph (1DDLC) observes planets near the diffraction limit with undistorted point spread functions but has a finite-stellar diameter problem in wideband use. This study presents a method for wide-spectral-band nulling insensitive to stellar-angular-diameter by adding a fiber nulling with a Lyot-plane phase mask to the 1DDLC. Designing the pattern of the Lyot-plane mask function focuses on the parity of the amplitude spread function of light. Our numerical simulation shows that the planetary throughput (including the fiber-coupling efficiency) can reach about 11% for about 1.35-$\\lambda/D$ planetary separation almost independently of the spectral bandwidth. The simulation also shows the raw contrast of about $4\\times10^{-8}$ (the spectral bandwidth of 25%) and $5\\times10^{-10}$ (the spectral bandwidth of 10%) for $3\\times 10^{-2}$ $\\lambda/D$ stellar angular diameter. The planetary throughput depends on the planetary azimuthal angle, which may degrade the exploration efficiency compared to an isotropic throughput but is partially offset by the wide spectral band.","sentences":["Potentially habitable planets around nearby stars less massive than solar-type stars could join targets of the spectroscopy of the planetary reflected light with future space telescopes.","However, the orbits of most of these planets occur near the diffraction limit for 6-m-diameter telescopes.","Thus, while securing contrast-mitigation ability under a broad spectral bandwidth and a finite stellar angular diameter, we must maintain planetary throughput even at the diffraction-limited angles to be able to reduce the effect of the photon noise within a reasonable observation time.","A one-dimensional diffraction-limited coronagraph (1DDLC) observes planets near the diffraction limit with undistorted point spread functions but has a finite-stellar diameter problem in wideband use.","This study presents a method for wide-spectral-band nulling insensitive to stellar-angular-diameter by adding a fiber nulling with a Lyot-plane phase mask to the 1DDLC.","Designing the pattern of the Lyot-plane mask function focuses on the parity of the amplitude spread function of light.","Our numerical simulation shows that the planetary throughput (including the fiber-coupling efficiency) can reach about 11% for about 1.35-$\\lambda/D$ planetary separation almost independently of the spectral bandwidth.","The simulation also shows the raw contrast of about $4\\times10^{-8}$ (the spectral bandwidth of 25%) and $5\\times10^{-10}$ (the spectral bandwidth of 10%) for $3\\times 10^{-2}$ $\\lambda/D$ stellar angular diameter.","The planetary throughput depends on the planetary azimuthal angle, which may degrade the exploration efficiency compared to an isotropic throughput but is partially offset by the wide spectral band."],"url":"http://arxiv.org/abs/2403.16415v1","category":"astro-ph.EP"}
{"created":"2024-03-25 03:18:58","title":"Multi-attention Associate Prediction Network for Visual Tracking","abstract":"Classification-regression prediction networks have realized impressive success in several modern deep trackers. However, there is an inherent difference between classification and regression tasks, so they have diverse even opposite demands for feature matching. Existed models always ignore the key issue and only employ a unified matching block in two task branches, decaying the decision quality. Besides, these models also struggle with decision misalignment situation. In this paper, we propose a multi-attention associate prediction network (MAPNet) to tackle the above problems. Concretely, two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are first designed for feature comparison by integrating self, cross, channel or spatial attentions organically. They are capable of fully capturing the category-related semantics for classification and the local spatial contexts for regression, respectively. Then, we present a dual alignment module to enhance the correspondences between two branches, which is useful to find the optimal tracking solution. Finally, we describe a Siamese tracker built upon the proposed prediction network, which achieves the leading performance on five tracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and UAV123, and surpasses other state-of-the-art approaches.","sentences":["Classification-regression prediction networks have realized impressive success in several modern deep trackers.","However, there is an inherent difference between classification and regression tasks, so they have diverse even opposite demands for feature matching.","Existed models always ignore the key issue and only employ a unified matching block in two task branches, decaying the decision quality.","Besides, these models also struggle with decision misalignment situation.","In this paper, we propose a multi-attention associate prediction network (MAPNet) to tackle the above problems.","Concretely, two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are first designed for feature comparison by integrating self, cross, channel or spatial attentions organically.","They are capable of fully capturing the category-related semantics for classification and the local spatial contexts for regression, respectively.","Then, we present a dual alignment module to enhance the correspondences between two branches, which is useful to find the optimal tracking solution.","Finally, we describe a Siamese tracker built upon the proposed prediction network, which achieves the leading performance on five tracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and UAV123, and surpasses other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.16395v1","category":"cs.CV"}
{"created":"2024-03-25 02:39:12","title":"A Monte Carlo Simulation of the Broad Band X-ray Emission of the Accreting Millisecond X-ray pulsar MAXI J1816-195","abstract":"MAXI J1816-195 is an accreting millisecond X-ray pulsar (AMXP) discovered in 2022. According to the Insight-HXMT data, the pulsations of this source extend all the way to over 100 keV, and its pulse profiles change from a single peak in low-energy range to double peaks in high-energy range. In this work, we simulate its energy spectra and pulse profiles with a Compton scattering Monte Carlo program. The simulation results suggest that the low energy X-ray source on the neutron star surface should be pencil-beamed radiations from the magnetic poles, and there should be a boundary layer in a hollow cylinder shape between the accretion disc and the neutron star surface: the up-scattering of the polar radiations in the boundary layer leads to the double peak structure of the high-energy pulse profile. Under this boundary layer geometry, we suggest that the rarity of AMXPs can be caused by the smearing of the boundary layer. To estimate the mass M and radius R of accretion-powered millisecond pulsars whose surface radiations are badly polluted by the accretion disk and boundary layer, the impact of Compton scattering in the boundary layer on the radiation should be removed before employing the X-ray pulse profile modeling method.","sentences":["MAXI J1816-195 is an accreting millisecond X-ray pulsar (AMXP) discovered in 2022.","According to the Insight-HXMT data, the pulsations of this source extend all the way to over 100 keV, and its pulse profiles change from a single peak in low-energy range to double peaks in high-energy range.","In this work, we simulate its energy spectra and pulse profiles with a Compton scattering Monte Carlo program.","The simulation results suggest that the low energy X-ray source on the neutron star surface should be pencil-beamed radiations from the magnetic poles, and there should be a boundary layer in a hollow cylinder shape between the accretion disc and the neutron star surface: the up-scattering of the polar radiations in the boundary layer leads to the double peak structure of the high-energy pulse profile.","Under this boundary layer geometry, we suggest that the rarity of AMXPs can be caused by the smearing of the boundary layer.","To estimate the mass M and radius R of accretion-powered millisecond pulsars whose surface radiations are badly polluted by the accretion disk and boundary layer, the impact of Compton scattering in the boundary layer on the radiation should be removed before employing the X-ray pulse profile modeling method."],"url":"http://arxiv.org/abs/2403.16375v1","category":"astro-ph.HE"}
{"created":"2024-03-25 00:44:56","title":"Manipulating the direction of one-way steering in an optomechanical ring cavity","abstract":"Quantum steering refers to the apparent possibility of exploiting nonseparable quantum correlations to remotely influence the quantum state of an observer via local measurements. Different from entanglement and Bell nonlocality, quantum steering exhibits an inherent asymmetric property, which makes it relevant for many asymmetric quantum information processing tasks. Here, we study Gaussian quantum steering between two mechanical modes in an optomechanical ring cavity. Using experimentally feasible parameters, we show that the state of the two considered modes can exhibit two-way steering and even one-way steering. Instead of using unbalanced losses or noises, we propose a simple practical way to control the direction of one-way steering. A comparative study between the steering and entanglement of the studied modes shows that both steering and entanglement undergo a sudden death-like behavior. In particular, steering is found more fragile against thermal noise remaining constantly upper bounded by entanglement. The proposed scheme may be meaningful for one-sided device-independent quantum key distribution, where the security of such protocol depends fundamentally on the direction of steering.","sentences":["Quantum steering refers to the apparent possibility of exploiting nonseparable quantum correlations to remotely influence the quantum state of an observer via local measurements.","Different from entanglement and Bell nonlocality, quantum steering exhibits an inherent asymmetric property, which makes it relevant for many asymmetric quantum information processing tasks.","Here, we study Gaussian quantum steering between two mechanical modes in an optomechanical ring cavity.","Using experimentally feasible parameters, we show that the state of the two considered modes can exhibit two-way steering and even one-way steering.","Instead of using unbalanced losses or noises, we propose a simple practical way to control the direction of one-way steering.","A comparative study between the steering and entanglement of the studied modes shows that both steering and entanglement undergo a sudden death-like behavior.","In particular, steering is found more fragile against thermal noise remaining constantly upper bounded by entanglement.","The proposed scheme may be meaningful for one-sided device-independent quantum key distribution, where the security of such protocol depends fundamentally on the direction of steering."],"url":"http://arxiv.org/abs/2403.16346v1","category":"quant-ph"}
{"created":"2024-03-24 23:56:48","title":"Cosmic Microwave Background Signatures from Current-carrying Cosmic Strings","abstract":"We continue our studies of the evolution and cosmological consequences of current-carrying cosmic string networks, described by a charge-velocity-dependent one scale (CVOS) model. We present a detailed calculation of the effects of these networks on the cosmic microwave background (CMB), in the context of this model, and specifically discuss how such current-carrying strings may be distinguished from their uncharged (Nambu-Goto) counterparts by current or forthcoming CMB data. We find that, under the CVOS hypothesis, the constraints on current-carrying strings should not differ much from those of their structureless counterparts in that the impact on the CMB can at most be reduced by a factor of ~25%. Nevertheless, the presence of a current and charge affects the distribution of power among scalar, vector and tensor modes, and also its distribution between small and large scales. It should therefore be possible for future high-sensitivity CMB experiments to distinguish between the two types of strings.","sentences":["We continue our studies of the evolution and cosmological consequences of current-carrying cosmic string networks, described by a charge-velocity-dependent one scale (CVOS) model.","We present a detailed calculation of the effects of these networks on the cosmic microwave background (CMB), in the context of this model, and specifically discuss how such current-carrying strings may be distinguished from their uncharged (Nambu-Goto) counterparts by current or forthcoming CMB data.","We find that, under the CVOS hypothesis, the constraints on current-carrying strings should not differ much from those of their structureless counterparts in that the impact on the CMB can at most be reduced by a factor of ~25%.","Nevertheless, the presence of a current and charge affects the distribution of power among scalar, vector and tensor modes, and also its distribution between small and large scales.","It should therefore be possible for future high-sensitivity CMB experiments to distinguish between the two types of strings."],"url":"http://arxiv.org/abs/2403.16332v1","category":"astro-ph.CO"}
{"created":"2024-03-24 20:31:42","title":"HemoSet: The First Blood Segmentation Dataset for Automation of Hemostasis Management","abstract":"Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly adapt to the visual interference that results from blood rapidly filling the surgical field. Introducing automation into the crucial surgical task of hemostasis management would offload mental and physical tasks from the surgeon and surgical assistants while simultaneously increasing the efficiency and safety of the operation. The first step in automation of hemostasis management is detection of blood in the surgical field. To propel the development of blood detection algorithms in surgeries, we present HemoSet, the first blood segmentation dataset based on bleeding during a live animal robotic surgery. Our dataset features vessel hemorrhage scenarios where turbulent flow leads to abnormal pooling geometries in surgical fields. These pools are formed in conditions endemic to surgical procedures -- uneven heterogeneous tissue, under glossy lighting conditions and rapid tool movement. We benchmark several state-of-the-art segmentation models and provide insight into the difficulties specific to blood detection. We intend for HemoSet to spur development of autonomous blood suction tools by providing a platform for training and refining blood segmentation models, addressing the precision needed for such robotics.","sentences":["Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly adapt to the visual interference that results from blood rapidly filling the surgical field.","Introducing automation into the crucial surgical task of hemostasis management would offload mental and physical tasks from the surgeon and surgical assistants while simultaneously increasing the efficiency and safety of the operation.","The first step in automation of hemostasis management is detection of blood in the surgical field.","To propel the development of blood detection algorithms in surgeries, we present HemoSet, the first blood segmentation dataset based on bleeding during a live animal robotic surgery.","Our dataset features vessel hemorrhage scenarios where turbulent flow leads to abnormal pooling geometries in surgical fields.","These pools are formed in conditions endemic to surgical procedures -- uneven heterogeneous tissue, under glossy lighting conditions and rapid tool movement.","We benchmark several state-of-the-art segmentation models and provide insight into the difficulties specific to blood detection.","We intend for HemoSet to spur development of autonomous blood suction tools by providing a platform for training and refining blood segmentation models, addressing the precision needed for such robotics."],"url":"http://arxiv.org/abs/2403.16286v1","category":"eess.IV"}
{"created":"2024-03-24 20:13:43","title":"Sample Empirical Likelihood Methods for Causal Inference","abstract":"Causal inference is crucial for understanding the true impact of interventions, policies, or actions, enabling informed decision-making and providing insights into the underlying mechanisms that shape our world. In this paper, we establish a framework for the estimation and inference of average treatment effects using a two-sample empirical likelihood function. Two different approaches to incorporating propensity scores are developed. The first approach introduces propensity scores calibrated constraints in addition to the standard model-calibration constraints; the second approach uses the propensity scores to form weighted versions of the model-calibration constraints. The resulting estimators from both approaches are doubly robust. The limiting distributions of the two sample empirical likelihood ratio statistics are derived, facilitating the construction of confidence intervals and hypothesis tests for the average treatment effect. Bootstrap methods for constructing sample empirical likelihood ratio confidence intervals are also discussed for both approaches. Finite sample performances of the methods are investigated through simulation studies.","sentences":["Causal inference is crucial for understanding the true impact of interventions, policies, or actions, enabling informed decision-making and providing insights into the underlying mechanisms that shape our world.","In this paper, we establish a framework for the estimation and inference of average treatment effects using a two-sample empirical likelihood function.","Two different approaches to incorporating propensity scores are developed.","The first approach introduces propensity scores calibrated constraints in addition to the standard model-calibration constraints; the second approach uses the propensity scores to form weighted versions of the model-calibration constraints.","The resulting estimators from both approaches are doubly robust.","The limiting distributions of the two sample empirical likelihood ratio statistics are derived, facilitating the construction of confidence intervals and hypothesis tests for the average treatment effect.","Bootstrap methods for constructing sample empirical likelihood ratio confidence intervals are also discussed for both approaches.","Finite sample performances of the methods are investigated through simulation studies."],"url":"http://arxiv.org/abs/2403.16283v1","category":"stat.ME"}
{"created":"2024-03-24 19:57:52","title":"The nontrivial effects of annealing on superconducting properties of Nb single crystals","abstract":"The effect of annealing on the superconducting properties of niobium single crystals cut from the same master boule was studied by local and global magnetic measurements, as well as scanning tunneling microscopy (STM). The formation of large hydride precipitates was observed in unannealed samples. The variation in structural and magnetic properties was studied after annealing under high vacuum at 800 C, 1400 C, and near the melting point of niobium (2477 C) for a few seconds. The initial samples had a high hydrogen content. Polarized optics and magneto-optical studies show that the formation of large niobium hydride precipitates is suppressed already by 800 C annealing. However, the overall superconducting properties in the annealed samples did not improve after annealing, and in fact, worsened. The superconducting transition temperature decreased, the upper critical field increased, and the pinning strength increased. Parallel studies were conducted using STM, where the sample was annealed initially at 400 C, measured, annealed again at 1700 C, and measured again. These studies revealed a ``dirty'' superconducting gap with a significant spatial variation of tunneling conductance after annealing at 400 C. The clean gap was recovered after annealing at 1700 C. It is likely that these results are due to oxygen redistribution near the surface, which is always covered by oxide layers in as-grown crystals. Overall, the results indicate that vacuum annealing at least up to 1400 C, while expected to remove a large amount of hydrogen, introduces additional nanosized defects, perhaps hydride precipitates, that act as efficient pair-breaking and pinning centers.","sentences":["The effect of annealing on the superconducting properties of niobium single crystals cut from the same master boule was studied by local and global magnetic measurements, as well as scanning tunneling microscopy (STM).","The formation of large hydride precipitates was observed in unannealed samples.","The variation in structural and magnetic properties was studied after annealing under high vacuum at 800 C, 1400 C, and near the melting point of niobium (2477 C) for a few seconds.","The initial samples had a high hydrogen content.","Polarized optics and magneto-optical studies show that the formation of large niobium hydride precipitates is suppressed already by 800 C annealing.","However, the overall superconducting properties in the annealed samples did not improve after annealing, and in fact, worsened.","The superconducting transition temperature decreased, the upper critical field increased, and the pinning strength increased.","Parallel studies were conducted using STM, where the sample was annealed initially at 400 C, measured, annealed again at 1700 C, and measured again.","These studies revealed a ``dirty'' superconducting gap with a significant spatial variation of tunneling conductance after annealing at 400 C. The clean gap was recovered after annealing at 1700 C. It is likely that these results are due to oxygen redistribution near the surface, which is always covered by oxide layers in as-grown crystals.","Overall, the results indicate that vacuum annealing at least up to 1400 C, while expected to remove a large amount of hydrogen, introduces additional nanosized defects, perhaps hydride precipitates, that act as efficient pair-breaking and pinning centers."],"url":"http://arxiv.org/abs/2403.16279v1","category":"cond-mat.supr-con"}
{"created":"2024-03-24 18:32:41","title":"Covariate-adjusted marginal cumulative incidence curves for competing risk analysis","abstract":"Covariate imbalance between treatment groups makes it difficult to compare cumulative incidence curves in competing risk analyses. In this paper we discuss different methods to estimate adjusted cumulative incidence curves including inverse probability of treatment weighting and outcome regression modeling. For these methods to work, correct specification of the propensity score model or outcome regression model, respectively, is needed. We introduce a new doubly robust estimator, which requires correct specification of only one of the two models. We conduct a simulation study to assess the performance of these three methods, including scenarios with model misspecification of the relationship between covariates and treatment and/or outcome. We illustrate their usage in a cohort study of breast cancer patients estimating covariate-adjusted marginal cumulative incidence curves for recurrence, second primary tumour development and death after undergoing mastectomy treatment or breast-conserving therapy. Our study points out the advantages and disadvantages of each covariate adjustment method when applied in competing risk analysis.","sentences":["Covariate imbalance between treatment groups makes it difficult to compare cumulative incidence curves in competing risk analyses.","In this paper we discuss different methods to estimate adjusted cumulative incidence curves including inverse probability of treatment weighting and outcome regression modeling.","For these methods to work, correct specification of the propensity score model or outcome regression model, respectively, is needed.","We introduce a new doubly robust estimator, which requires correct specification of only one of the two models.","We conduct a simulation study to assess the performance of these three methods, including scenarios with model misspecification of the relationship between covariates and treatment and/or outcome.","We illustrate their usage in a cohort study of breast cancer patients estimating covariate-adjusted marginal cumulative incidence curves for recurrence, second primary tumour development and death after undergoing mastectomy treatment or breast-conserving therapy.","Our study points out the advantages and disadvantages of each covariate adjustment method when applied in competing risk analysis."],"url":"http://arxiv.org/abs/2403.16256v1","category":"stat.ME"}
{"created":"2024-03-24 18:00:01","title":"The APO-K2 Catalog. II. Accurate Stellar Ages for Red Giant Branch Stars Across the Milky Way","abstract":"We present stellar age determinations for 4,661 red giant branch (RGB) stars in the APO-K2 Catalog, derived using mass estimates from K2 asteroseismology from the K2 Galactic Archaeology Program and elemental abundances from the Apache Point Galactic Evolution Experiment (APOGEE) survey. Our sample includes 17 of the 19 fields observed by K2, making it one of the most comprehensive catalogs of accurate stellar ages across the Galaxy in terms of the wide range of populations spanned by its stars, enabling rigorous tests of Galactic chemical evolution models. Taking into account the selection functions of the K2 sample, the data appear to support the age-chemistry morphology of stellar populations predicted by both inside-out and late-burst scenarios. We also investigate trends in age versus stellar chemistry and Galactic position, which are consistent with previous findings. Comparisons against APOKASC-3 asteroseismic ages show agreement to within ~3%. We also discuss offsets between our ages and spectroscopic ages. Finally, we note that ignoring the effects of $\\alpha$-enhancement on stellar opacity (either directly or with the Salaris metallicity correction) results in an ~10% offset in age estimates for the most $\\alpha$-enhanced stars, which is an important consideration for continued tests of Galactic models with this and other asteroseismic age samples.","sentences":["We present stellar age determinations for 4,661 red giant branch (RGB) stars in the APO-K2 Catalog, derived using mass estimates from K2 asteroseismology from the K2 Galactic Archaeology Program and elemental abundances from the Apache Point Galactic Evolution Experiment (APOGEE) survey.","Our sample includes 17 of the 19 fields observed by K2, making it one of the most comprehensive catalogs of accurate stellar ages across the Galaxy in terms of the wide range of populations spanned by its stars, enabling rigorous tests of Galactic chemical evolution models.","Taking into account the selection functions of the K2 sample, the data appear to support the age-chemistry morphology of stellar populations predicted by both inside-out and late-burst scenarios.","We also investigate trends in age versus stellar chemistry and Galactic position, which are consistent with previous findings.","Comparisons against APOKASC-3 asteroseismic ages show agreement to within ~3%.","We also discuss offsets between our ages and spectroscopic ages.","Finally, we note that ignoring the effects of $\\alpha$-enhancement on stellar opacity (either directly or with the Salaris metallicity correction) results in an ~10% offset in age estimates for the most $\\alpha$-enhanced stars, which is an important consideration for continued tests of Galactic models with this and other asteroseismic age samples."],"url":"http://arxiv.org/abs/2403.16250v1","category":"astro-ph.GA"}
{"created":"2024-03-24 17:39:36","title":"Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches","abstract":"As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before. Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline. When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract. Humans have a unique ability to create abstractions. However, automatic summarization is a complicated problem to solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive text summarization has been ascending as far as prevalence. Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle different issues like saliency, familiarity, and human lucidness and create excellent synopses. In this article, we aimed toward enhancing the present architectures and models for abstractive text summarization. The modifications have been aimed at fine-tuning hyper-parameters, attempting specific encoder-decoder combinations. We examined many experiments on an extensively used CNN/DailyMail dataset to check the effectiveness of various models.","sentences":["As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before.","Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline.","When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract.","Humans have a unique ability to create abstractions.","However, automatic summarization is a complicated problem to solve.","The use of sequence-to-sequence (seq2seq) models for neural abstractive text summarization has been ascending as far as prevalence.","Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle different issues like saliency, familiarity, and human lucidness and create excellent synopses.","In this article, we aimed toward enhancing the present architectures and models for abstractive text summarization.","The modifications have been aimed at fine-tuning hyper-parameters, attempting specific encoder-decoder combinations.","We examined many experiments on an extensively used CNN/DailyMail dataset to check the effectiveness of various models."],"url":"http://arxiv.org/abs/2403.16247v1","category":"cs.CL"}
{"created":"2024-03-24 17:15:51","title":"$qt$RSK${}^*$: A probabilistic dual RSK correspondence for Macdonald polynomials","abstract":"We introduce a probabilistic generalization of the dual Robinson--Schensted--Knuth correspondence, called $qt$RSK${}^*$, depending on two parameters $q$ and $t$. This correspondence extends the $q$RS$t$ correspondence, recently introduced by the authors, and allows the first tableaux-theoretic proof of the dual Cauchy identity for Macdonald polynomials. By specializing $q$ and $t$, one recovers the row and column insertion version of the classical dual RSK correspondence as well as of $q$- and $t$-deformations thereof which are connected to $q$-Whittaker and Hall--Littlewood polynomials. When restricting to Jack polynomials and $\\{0,1\\}$-matrices corresponding to words, we prove that the insertion tableaux obtained by $qt$RSK${}^*$ are invariant under swapping letters in the input word. Our approach is based on Fomin's growth diagrams and the notion of probabilistic bijections.","sentences":["We introduce a probabilistic generalization of the dual Robinson--Schensted--Knuth correspondence, called $qt$RSK${}^*$, depending on two parameters $q$ and $t$. This correspondence extends the $q$RS$t$ correspondence, recently introduced by the authors, and allows the first tableaux-theoretic proof of the dual Cauchy identity for Macdonald polynomials.","By specializing $q$ and $t$, one recovers the row and column insertion version of the classical dual RSK correspondence as well as of $q$- and $t$-deformations thereof which are connected to $q$-Whittaker and Hall--Littlewood polynomials.","When restricting to Jack polynomials and $\\{0,1\\}$-matrices corresponding to words, we prove that the insertion tableaux obtained by $qt$RSK${}^*$ are invariant under swapping letters in the input word.","Our approach is based on Fomin's growth diagrams and the notion of probabilistic bijections."],"url":"http://arxiv.org/abs/2403.16243v1","category":"math.CO"}
{"created":"2024-03-24 17:06:45","title":"Thermal Analysis for NVIDIA GTX480 Fermi GPU Architecture","abstract":"In this project, we design a four-layer (Silicon|TIM|Silicon|TIM), 3D floor plan for NVIDIA GTX480 Fermi GPU architecture and compare heat dissipation and power trends for matrix multiplication and Needleman-Wunsch kernels. First, cuda kernels for the two algorithms are written. These kernels are compiled and executed with the GPGPU Simulator to extract power logs for varying tensor sizes. These power logs are converted to ptrace files with an automation script written in Python. The 3D floor plan, along with the generated ptrace files are given to HotSpot, which generates thermal heat maps to show heat dissipation for various components of the Fermi architecture. These heat dissipation trends for both the kernels are observed for multiple tensor sizes to draw qualitative conclusions. The behavioral and execution patterns of both kernels are also observed with these varying heat dissipation trends. With this project, we observe that an increase in tensor size results in an increase of heat dissipation in components of the Fermi Architecture. However, the temperature of the chip remains saturated after a particular tensor size and remains constant thereafter. Heat dissipation is non-uniform with smaller tensor sizes, and becomes more uniform after a certain tensor size. This means, that after a particular tensor size, more cores of the architecture get activated in the computations, thereby resulting in an almost constant temperature. We also observe that Needleman Wunsch uses more data movement between DRAM and caches, thereby showing higher heat dissipation patterns in DRAMs when compared to Matrix multiplication for the same tensor size. Our observations are in accordance with the theoretical concepts behind the working of the two algorithms, thereby making our results consistent.","sentences":["In this project, we design a four-layer (Silicon|TIM|Silicon|TIM), 3D floor plan for NVIDIA GTX480 Fermi GPU architecture and compare heat dissipation and power trends for matrix multiplication and Needleman-Wunsch kernels.","First, cuda kernels for the two algorithms are written.","These kernels are compiled and executed with the GPGPU Simulator to extract power logs for varying tensor sizes.","These power logs are converted to ptrace files with an automation script written in Python.","The 3D floor plan, along with the generated ptrace files are given to HotSpot, which generates thermal heat maps to show heat dissipation for various components of the Fermi architecture.","These heat dissipation trends for both the kernels are observed for multiple tensor sizes to draw qualitative conclusions.","The behavioral and execution patterns of both kernels are also observed with these varying heat dissipation trends.","With this project, we observe that an increase in tensor size results in an increase of heat dissipation in components of the Fermi Architecture.","However, the temperature of the chip remains saturated after a particular tensor size and remains constant thereafter.","Heat dissipation is non-uniform with smaller tensor sizes, and becomes more uniform after a certain tensor size.","This means, that after a particular tensor size, more cores of the architecture get activated in the computations, thereby resulting in an almost constant temperature.","We also observe that Needleman Wunsch uses more data movement between DRAM and caches, thereby showing higher heat dissipation patterns in DRAMs when compared to Matrix multiplication for the same tensor size.","Our observations are in accordance with the theoretical concepts behind the working of the two algorithms, thereby making our results consistent."],"url":"http://arxiv.org/abs/2403.16239v1","category":"cs.AR"}
{"created":"2024-03-24 16:49:01","title":"Mean Field Game of Mutual Holding with common noise","abstract":"We consider the mean field game of cross--holding introduced in \\citeauthor*{DjeteTouzi} \\cite{DjeteTouzi} in the context where the equity value dynamics are affected by a common noise. In contrast with \\cite{DjeteTouzi}, the problem exhibits the standard paradigm of mean--variance trade off. Our crucial observation is to search for equilibrium solutions of our mean field game among those models which satisfy an appropriate notion of no--arbitrage. Under this condition, it follows that the representative agent optimization step is reduced to a standard portfolio optimization problem with random endowment.","sentences":["We consider the mean field game of cross--holding introduced in \\citeauthor*{DjeteTouzi} \\cite{DjeteTouzi} in the context where the equity value dynamics are affected by a common noise.","In contrast with \\cite{DjeteTouzi}, the problem exhibits the standard paradigm of mean--variance trade off.","Our crucial observation is to search for equilibrium solutions of our mean field game among those models which satisfy an appropriate notion of no--arbitrage.","Under this condition, it follows that the representative agent optimization step is reduced to a standard portfolio optimization problem with random endowment."],"url":"http://arxiv.org/abs/2403.16232v1","category":"math.PR"}
{"created":"2024-03-24 15:24:04","title":"Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery","abstract":"Unsupervised landmarks discovery (ULD) for an object category is a challenging computer vision problem. In pursuit of developing a robust ULD framework, we explore the potential of a recent paradigm of self-supervised learning algorithms, known as diffusion models. Some recent works have shown that these models implicitly contain important correspondence cues. Towards harnessing the potential of diffusion models for the ULD task, we make the following core contributions. First, we propose a ZeroShot ULD baseline based on simple clustering of random pixel locations with nearest neighbour matching. It delivers better results than existing ULD methods. Second, motivated by the ZeroShot performance, we develop a ULD algorithm based on diffusion features using self-training and clustering which also outperforms prior methods by notable margins. Third, we introduce a new proxy task based on generating latent pose codes and also propose a two-stage clustering mechanism to facilitate effective pseudo-labeling, resulting in a significant performance improvement. Overall, our approach consistently outperforms state-of-the-art methods on four challenging benchmarks AFLW, MAFL, CatHeads and LS3D by significant margins.","sentences":["Unsupervised landmarks discovery (ULD) for an object category is a challenging computer vision problem.","In pursuit of developing a robust ULD framework, we explore the potential of a recent paradigm of self-supervised learning algorithms, known as diffusion models.","Some recent works have shown that these models implicitly contain important correspondence cues.","Towards harnessing the potential of diffusion models for the ULD task, we make the following core contributions.","First, we propose a ZeroShot ULD baseline based on simple clustering of random pixel locations with nearest neighbour matching.","It delivers better results than existing ULD methods.","Second, motivated by the ZeroShot performance, we develop a ULD algorithm based on diffusion features using self-training and clustering which also outperforms prior methods by notable margins.","Third, we introduce a new proxy task based on generating latent pose codes and also propose a two-stage clustering mechanism to facilitate effective pseudo-labeling, resulting in a significant performance improvement.","Overall, our approach consistently outperforms state-of-the-art methods on four challenging benchmarks AFLW, MAFL, CatHeads and LS3D by significant margins."],"url":"http://arxiv.org/abs/2403.16194v1","category":"cs.CV"}
{"created":"2024-03-24 15:00:41","title":"Back-and-forth equivalent group von Neumann algebras","abstract":"We prove that if $G$ and $H$ are $\\alpha$-back-and-forth equivalent groups (in the sense of computable structure theory) for some ordinal $\\alpha \\geq \\omega$, then their group von Neumann algebras $L(G)$ and $L(H)$ are also $\\alpha$-back-and-forth equivalent. In particular, if $G$ and $H$ are $\\omega$-back-and-forth-equivalent groups, then $L(G)$ and $L(H)$ are elementarily equivalent; this is known to fail under the weaker hypothesis that $G$ and $H$ are merely elementarily equivalent. We extend this result to crossed product von Neumann algebras associated to Bernoulli actions of back-and-forth equivalent groups.","sentences":["We prove that if $G$ and $H$ are $\\alpha$-back-and-forth equivalent groups (in the sense of computable structure theory) for some ordinal $\\alpha \\geq \\omega$, then their group von Neumann algebras $L(G)$ and $L(H)$ are also $\\alpha$-back-and-forth equivalent.","In particular, if $G$ and $H$ are $\\omega$-back-and-forth-equivalent groups, then $L(G)$ and $L(H)$ are elementarily equivalent; this is known to fail under the weaker hypothesis that $G$ and $H$ are merely elementarily equivalent.","We extend this result to crossed product von Neumann algebras associated to Bernoulli actions of back-and-forth equivalent groups."],"url":"http://arxiv.org/abs/2403.16181v1","category":"math.LO"}
{"created":"2024-03-24 14:47:08","title":"The Road to Near-Capacity CV-QKD Reconciliation: An FEC-Agnostic Design","abstract":"New near-capacity continuous-variable quantum key distribution (CV-QKD) reconciliation schemes are proposed, where both the authenticated classical channel (ClC) and the quantum channel (QuC) for QKD are protected by separate forward error correction (FEC) coding schemes. More explicitly, a new codeword-based - rather than syndrome-based - QKD reconciliation scheme is proposed, where Alice sends an FEC-protected codeword to Bob through a ClC, while Bob sends a separate FEC protected codeword to Alice through a QuC. Upon decoding the codeword received from the other side, the final key is obtained by applying a simple modulo-2 operation to the local codeword and the decoded remote codeword. As a result, first of all, the proposed codeword-based QKD reconciliation system ensures protection of both the QuC and of the ClC. Secondly, the proposed system has a similar complexity at both sides, where both Alice and Bob have an FEC encoder and an FEC decoder. Thirdly, the proposed system makes QKD reconciliation compatible with a wide range of FEC schemes, including polar codes, CCs and irregular convolutional codes (IRCCs), where a near-capacity performance can be achieved for both the QuC and for the ClC.Our simulation results demonstrate that thanks to the proposed regime, the performance improvements of the QuC and of the ClC benefit each other, hence leading to an improved secret key rate (SKR) that inches closer to both the Pirandola-Laurenza-Ottaviani-Banchi (PLOB) bound and to the maximum achieveable rate bound.","sentences":["New near-capacity continuous-variable quantum key distribution (CV-QKD) reconciliation schemes are proposed, where both the authenticated classical channel (ClC) and the quantum channel (QuC) for QKD are protected by separate forward error correction (FEC) coding schemes.","More explicitly, a new codeword-based - rather than syndrome-based - QKD reconciliation scheme is proposed, where Alice sends an FEC-protected codeword to Bob through a ClC, while Bob sends a separate FEC protected codeword to Alice through a QuC. Upon decoding the codeword received from the other side, the final key is obtained by applying a simple modulo-2 operation to the local codeword and the decoded remote codeword.","As a result, first of all, the proposed codeword-based QKD reconciliation system ensures protection of both the QuC and of the ClC. Secondly, the proposed system has a similar complexity at both sides, where both Alice and Bob have an FEC encoder and an FEC decoder.","Thirdly, the proposed system makes QKD reconciliation compatible with a wide range of FEC schemes, including polar codes, CCs and irregular convolutional codes (IRCCs), where a near-capacity performance can be achieved for both the QuC and for the ClC.Our simulation results demonstrate that thanks to the proposed regime, the performance improvements of the QuC and of the ClC benefit each other, hence leading to an improved secret key rate (SKR) that inches closer to both the Pirandola-Laurenza-Ottaviani-Banchi (PLOB) bound and to the maximum achieveable rate bound."],"url":"http://arxiv.org/abs/2403.16180v1","category":"quant-ph"}
{"created":"2024-03-24 14:37:36","title":"The Informativeness of Combined Experimental and Observational Data under Dynamic Selection","abstract":"This paper addresses the challenge of estimating the Average Treatment Effect on the Treated Survivors (ATETS; Vikstrom et al., 2018) in the absence of long-term experimental data, utilizing available long-term observational data instead. We establish two theoretical results. First, it is impossible to obtain informative bounds for the ATETS with no model restriction and no auxiliary data. Second, to overturn this negative result, we explore as a promising avenue the recent econometric developments in combining experimental and observational data (e.g., Athey et al., 2020, 2019); we indeed find that exploiting short-term experimental data can be informative without imposing classical model restrictions. Furthermore, building on Chesher and Rosen (2017), we explore how to systematically derive sharp identification bounds, exploiting both the novel data-combination principles and classical model restrictions. Applying the proposed method, we explore what can be learned about the long-run effects of job training programs on employment without long-term experimental data.","sentences":["This paper addresses the challenge of estimating the Average Treatment Effect on the Treated Survivors (ATETS; Vikstrom et al., 2018) in the absence of long-term experimental data, utilizing available long-term observational data instead.","We establish two theoretical results.","First, it is impossible to obtain informative bounds for the ATETS with no model restriction and no auxiliary data.","Second, to overturn this negative result, we explore as a promising avenue the recent econometric developments in combining experimental and observational data (e.g., Athey et al., 2020, 2019); we indeed find that exploiting short-term experimental data can be informative without imposing classical model restrictions.","Furthermore, building on Chesher and Rosen (2017), we explore how to systematically derive sharp identification bounds, exploiting both the novel data-combination principles and classical model restrictions.","Applying the proposed method, we explore what can be learned about the long-run effects of job training programs on employment without long-term experimental data."],"url":"http://arxiv.org/abs/2403.16177v1","category":"econ.EM"}
{"created":"2024-03-24 14:35:44","title":"Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals","abstract":"Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is inevitable in subspace learning, we propose an independence criterion to disentangle clean signals from perturbations. Experimental results show that the proposed strategy enables the model to inherently suppress adversaries, which not only boosts model robustness but also motivates new directions of effective adversarial defense.","sentences":["Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs.","To better understand such attacks, a characterization of the features carried by adversarial examples is needed.","In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis.","We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals.","This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples.","To prevent the residual perturbations that is inevitable in subspace learning, we propose an independence criterion to disentangle clean signals from perturbations.","Experimental results show that the proposed strategy enables the model to inherently suppress adversaries, which not only boosts model robustness but also motivates new directions of effective adversarial defense."],"url":"http://arxiv.org/abs/2403.16176v1","category":"cs.LG"}
{"created":"2024-03-24 14:30:13","title":"A robust optimization approach model for a multi-vaccine multi-echelon supply chain","abstract":"This research investigates a multi-product, multi-echelon, and multi-period vaccine supply chain network model under uncertainty and quality inspection errors. The objective function seeks optimizing the total cost of the supply chain. Moreover, the proposed model is formulated as a mixed integer linear programming problem under multiple sources of uncertain parameters including demand, inspection errors, vaccine waste generated in healthcare centers, and defective treatment rate of vaccine waste. To provide meaningful solutions that are robust against future fluctuation of parameters, the robust optimization approach is utilized to incorporate the decision maker risk attitude under different type of uncertainty sets. Namely, box, polyhedral and combination of interval polyhedral. The performance of the proposed model is demonstrated through an illustrative example. The results show the effect of different types of uncertainties on the overall objective function. Managerial insights and research implications in terms of vaccine supply chain is advised and future research directions are proposed.","sentences":["This research investigates a multi-product, multi-echelon, and multi-period vaccine supply chain network model under uncertainty and quality inspection errors.","The objective function seeks optimizing the total cost of the supply chain.","Moreover, the proposed model is formulated as a mixed integer linear programming problem under multiple sources of uncertain parameters including demand, inspection errors, vaccine waste generated in healthcare centers, and defective treatment rate of vaccine waste.","To provide meaningful solutions that are robust against future fluctuation of parameters, the robust optimization approach is utilized to incorporate the decision maker risk attitude under different type of uncertainty sets.","Namely, box, polyhedral and combination of interval polyhedral.","The performance of the proposed model is demonstrated through an illustrative example.","The results show the effect of different types of uncertainties on the overall objective function.","Managerial insights and research implications in terms of vaccine supply chain is advised and future research directions are proposed."],"url":"http://arxiv.org/abs/2403.16173v1","category":"math.OC"}
{"created":"2024-03-24 14:10:12","title":"Input-to-State Stability of Newton Methods for Generalized Equations in Nonlinear Optimization","abstract":"We show that Newton methods for generalized equations are input-to-state stable with respect to disturbances such as due to inexact computations. We then use this result to obtain convergence and robustness of a multistep Newton-type method for multivariate generalized equations. We demonstrate the usefulness of the results with other applications to nonlinear optimization. In particular, we provide a new proof for (robust) local convergence of the augmented Lagrangian method.","sentences":["We show that Newton methods for generalized equations are input-to-state stable with respect to disturbances such as due to inexact computations.","We then use this result to obtain convergence and robustness of a multistep Newton-type method for multivariate generalized equations.","We demonstrate the usefulness of the results with other applications to nonlinear optimization.","In particular, we provide a new proof for (robust) local convergence of the augmented Lagrangian method."],"url":"http://arxiv.org/abs/2403.16165v1","category":"math.OC"}
{"created":"2024-03-24 14:02:25","title":"Towards Online Real-Time Memory-based Video Inpainting Transformers","abstract":"Video inpainting tasks have seen significant improvements in recent years with the rise of deep neural networks and, in particular, vision transformers. Although these models show promising reconstruction quality and temporal consistency, they are still unsuitable for live videos, one of the last steps to make them completely convincing and usable. The main limitations are that these state-of-the-art models inpaint using the whole video (offline processing) and show an insufficient frame rate. In our approach, we propose a framework to adapt existing inpainting transformers to these constraints by memorizing and refining redundant computations while maintaining a decent inpainting quality. Using this framework with some of the most recent inpainting models, we show great online results with a consistent throughput above 20 frames per second. The code and pretrained models will be made available upon acceptance.","sentences":["Video inpainting tasks have seen significant improvements in recent years with the rise of deep neural networks and, in particular, vision transformers.","Although these models show promising reconstruction quality and temporal consistency, they are still unsuitable for live videos, one of the last steps to make them completely convincing and usable.","The main limitations are that these state-of-the-art models inpaint using the whole video (offline processing) and show an insufficient frame rate.","In our approach, we propose a framework to adapt existing inpainting transformers to these constraints by memorizing and refining redundant computations while maintaining a decent inpainting quality.","Using this framework with some of the most recent inpainting models, we show great online results with a consistent throughput above 20 frames per second.","The code and pretrained models will be made available upon acceptance."],"url":"http://arxiv.org/abs/2403.16161v1","category":"cs.CV"}
{"created":"2024-03-24 13:50:03","title":"pyKCN: A Python Tool for Bridging Scientific Knowledge","abstract":"The study of research trends is pivotal for understanding scientific development on specific topics. Traditionally, this involves keyword analysis within scholarly literature, yet comprehensive tools for such analysis are scarce, especially those capable of parsing large datasets with precision. pyKCN, a Python toolkit, addresses this gap by automating keyword cleaning, extraction and trend analysis from extensive academic corpora. It is equipped with modules for text processing, deduplication, extraction, and advanced keyword co-occurrence and analysis, providing a granular view of research trends. This toolkit stands out by enabling researchers to visualize keyword relationships, thereby identifying seminal works and emerging trends. Its application spans diverse domains, enhancing scholars' capacity to understand developments within their fields. The implications of using pyKCN are significant. It offers an empirical basis for predicting research trends, which can inform funding directions, policy-making, and academic curricula. The code source and details can be found on: https://github.com/zhenyuanlu/pyKCN","sentences":["The study of research trends is pivotal for understanding scientific development on specific topics.","Traditionally, this involves keyword analysis within scholarly literature, yet comprehensive tools for such analysis are scarce, especially those capable of parsing large datasets with precision.","pyKCN, a Python toolkit, addresses this gap by automating keyword cleaning, extraction and trend analysis from extensive academic corpora.","It is equipped with modules for text processing, deduplication, extraction, and advanced keyword co-occurrence and analysis, providing a granular view of research trends.","This toolkit stands out by enabling researchers to visualize keyword relationships, thereby identifying seminal works and emerging trends.","Its application spans diverse domains, enhancing scholars' capacity to understand developments within their fields.","The implications of using pyKCN are significant.","It offers an empirical basis for predicting research trends, which can inform funding directions, policy-making, and academic curricula.","The code source and details can be found on: https://github.com/zhenyuanlu/pyKCN"],"url":"http://arxiv.org/abs/2403.16157v1","category":"cs.DL"}
{"created":"2024-03-24 13:48:47","title":"An instability result of Hamiltonian systems related to optimal swing-up control of a pendulum","abstract":"This paper presents an instability result of Hamiltonian systems associated with optimal swing-up control for a pendulum. The systems possess weak (higher-order) instability at the initial point of the swing-up control, the analysis for which requires techniques from celestial mechanics. The obtained result may have relationships with the previously obtained numerical studies for the existence of multiple locally optimal solutions and the non-existence conjecture of the optimal control.","sentences":["This paper presents an instability result of Hamiltonian systems associated with optimal swing-up control for a pendulum.","The systems possess weak (higher-order) instability at the initial point of the swing-up control, the analysis for which requires techniques from celestial mechanics.","The obtained result may have relationships with the previously obtained numerical studies for the existence of multiple locally optimal solutions and the non-existence conjecture of the optimal control."],"url":"http://arxiv.org/abs/2403.16156v1","category":"math.OC"}
{"created":"2024-03-24 13:44:32","title":"Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection","abstract":"The online community has increasingly been inundated by a toxic wave of harmful comments. In response to this growing challenge, we introduce a two-stage ultra-low-cost multimodal harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates. We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images. Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or logistic regression, enabling the system to be trained rapidly and to perform inference at an ultra-low cost. By converting tweets into rich multimodal embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual information with near-perfect performance, achieving precision and recall rates above 99\\% but also demonstrates the ability to zero-shot harmful images without additional training, thanks to its multimodal embedding input. This capability empowers our system to identify unseen harmful images without requiring extensive and costly image datasets. Additionally, our system quickly adapts to new harmful content; if a new harmful content pattern is identified, we can fine-tune the classifier with the corresponding tweets' embeddings to promptly update the system. This makes it well suited to addressing the ever-evolving nature of online harmfulness, providing online communities with a robust, generalizable, and cost-effective tool to safeguard their communities.","sentences":["The online community has increasingly been inundated by a toxic wave of harmful comments.","In response to this growing challenge, we introduce a two-stage ultra-low-cost multimodal harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates.","We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images.","Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or logistic regression, enabling the system to be trained rapidly and to perform inference at an ultra-low cost.","By converting tweets into rich multimodal embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual information with near-perfect performance, achieving precision and recall rates above 99\\% but also demonstrates the ability to zero-shot harmful images without additional training, thanks to its multimodal embedding input.","This capability empowers our system to identify unseen harmful images without requiring extensive and costly image datasets.","Additionally, our system quickly adapts to new harmful content; if a new harmful content pattern is identified, we can fine-tune the classifier with the corresponding tweets' embeddings to promptly update the system.","This makes it well suited to addressing the ever-evolving nature of online harmfulness, providing online communities with a robust, generalizable, and cost-effective tool to safeguard their communities."],"url":"http://arxiv.org/abs/2403.16151v1","category":"cs.MA"}
{"created":"2024-03-24 13:36:23","title":"Realtime Robust Shape Estimation of Deformable Linear Object","abstract":"Realtime shape estimation of continuum objects and manipulators is essential for developing accurate planning and control paradigms. The existing methods that create dense point clouds from camera images, and/or use distinguishable markers on a deformable body have limitations in realtime tracking of large continuum objects/manipulators. The physical occlusion of markers can often compromise accurate shape estimation. We propose a robust method to estimate the shape of linear deformable objects in realtime using scattered and unordered key points. By utilizing a robust probability-based labeling algorithm, our approach identifies the true order of the detected key points and then reconstructs the shape using piecewise spline interpolation. The approach only relies on knowing the number of the key points and the interval between two neighboring points. We demonstrate the robustness of the method when key points are partially occluded. The proposed method is also integrated into a simulation in Unity for tracking the shape of a cable with a length of 1m and a radius of 5mm. The simulation results show that our proposed approach achieves an average length error of 1.07% over the continuum's centerline and an average cross-section error of 2.11mm. The real-world experiments of tracking and estimating a heavy-load cable prove that the proposed approach is robust under occlusion and complex entanglement scenarios.","sentences":["Realtime shape estimation of continuum objects and manipulators is essential for developing accurate planning and control paradigms.","The existing methods that create dense point clouds from camera images, and/or use distinguishable markers on a deformable body have limitations in realtime tracking of large continuum objects/manipulators.","The physical occlusion of markers can often compromise accurate shape estimation.","We propose a robust method to estimate the shape of linear deformable objects in realtime using scattered and unordered key points.","By utilizing a robust probability-based labeling algorithm, our approach identifies the true order of the detected key points and then reconstructs the shape using piecewise spline interpolation.","The approach only relies on knowing the number of the key points and the interval between two neighboring points.","We demonstrate the robustness of the method when key points are partially occluded.","The proposed method is also integrated into a simulation in Unity for tracking the shape of a cable with a length of 1m and a radius of 5mm.","The simulation results show that our proposed approach achieves an average length error of 1.07% over the continuum's centerline and an average cross-section error of 2.11mm.","The real-world experiments of tracking and estimating a heavy-load cable prove that the proposed approach is robust under occlusion and complex entanglement scenarios."],"url":"http://arxiv.org/abs/2403.16146v1","category":"cs.RO"}
{"created":"2024-03-24 13:32:42","title":"Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach","abstract":"Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a recurrent neural network (RNN) architecture fed with time series data derived from geometrical parameters, as for example droplet diameter variation, our study shows the accuracy of our approach in predicting energy budgets, as for instance the kinetic, dissipation, and surface energy trends, across a range of Reynolds and Weber numbers in droplet dynamic problems. Finally, a two-phase sequential neural network using only geometric data, which is readily available in experimental settings, is employed to predict the energies and then use them to estimate static parameters, such as the Reynolds and Weber numbers. While our methodology has been primarily validated with simulation data, its adaptability to experimental datasets is a promising avenue for future exploration. We hope that our strategy can be useful for diverse applications, spanning from inkjet printing to combustion engines, where the prediction of energy budgets or dissipation energies is crucial.","sentences":["Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows.","The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs.","This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects.","Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision.","Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget.","The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics.","Using a recurrent neural network (RNN) architecture fed with time series data derived from geometrical parameters, as for example droplet diameter variation, our study shows the accuracy of our approach in predicting energy budgets, as for instance the kinetic, dissipation, and surface energy trends, across a range of Reynolds and Weber numbers in droplet dynamic problems.","Finally, a two-phase sequential neural network using only geometric data, which is readily available in experimental settings, is employed to predict the energies and then use them to estimate static parameters, such as the Reynolds and Weber numbers.","While our methodology has been primarily validated with simulation data, its adaptability to experimental datasets is a promising avenue for future exploration.","We hope that our strategy can be useful for diverse applications, spanning from inkjet printing to combustion engines, where the prediction of energy budgets or dissipation energies is crucial."],"url":"http://arxiv.org/abs/2403.16144v1","category":"physics.flu-dyn"}
{"created":"2024-03-24 13:24:01","title":"Rearranged Stochastic Heat Equation: Ergodicity and Related Gradient Descent on the Space of Probability Measures","abstract":"This article provides a case study for a recently introduced diffusion in the space of probability measures over the reals, namely rearranged stochastic heat, which solves a stochastic partial differential equation valued in the set of symmetrised quantile functions over the unit circle. Probability measure-valued flows perturbed by this noise are studied, with a special focus on gradient flows. This is done by introducing a drift to the rearranged stochastic heat equation by means of a vector field from the set of random variables over the unit circle into itself. When the flow is a gradient flow, the vector field may coincide with the Wasserstein derivative of a mean-field potential function, as defined in Lions' approach to the differential calculus on the space of probability measures. The resulting equation reads as a sort of McKean-Vlasov stochastic differential equation with an infinite dimensional common noise. Conditions on the drift are provided, under which solutions exist uniquely for any time horizon and converge exponentially fast towards a unique equilibrium. When the drift derives from a potential on the space of probability measures, some metastability properties are obtained as the intensity of the noise is tuned to zero: it is shown that under a particular scaling regime, the gradient descent lingers near local minimizers for expected times of the same order as in the finite dimensional setting. Interestingly, in order to accommodate a wider class of potentials, such as the square of the second Wasserstein distance, a weaker notion of derivative is defined over the subspace of symmetrised quantile functions.","sentences":["This article provides a case study for a recently introduced diffusion in the space of probability measures over the reals, namely rearranged stochastic heat, which solves a stochastic partial differential equation valued in the set of symmetrised quantile functions over the unit circle.","Probability measure-valued flows perturbed by this noise are studied, with a special focus on gradient flows.","This is done by introducing a drift to the rearranged stochastic heat equation by means of a vector field from the set of random variables over the unit circle into itself.","When the flow is a gradient flow, the vector field may coincide with the Wasserstein derivative of a mean-field potential function, as defined in Lions' approach to the differential calculus on the space of probability measures.","The resulting equation reads as a sort of McKean-Vlasov stochastic differential equation with an infinite dimensional common noise.","Conditions on the drift are provided, under which solutions exist uniquely for any time horizon and converge exponentially fast towards a unique equilibrium.","When the drift derives from a potential on the space of probability measures, some metastability properties are obtained as the intensity of the noise is tuned to zero: it is shown that under a particular scaling regime, the gradient descent lingers near local minimizers for expected times of the same order as in the finite dimensional setting.","Interestingly, in order to accommodate a wider class of potentials, such as the square of the second Wasserstein distance, a weaker notion of derivative is defined over the subspace of symmetrised quantile functions."],"url":"http://arxiv.org/abs/2403.16140v1","category":"math.PR"}
{"created":"2024-03-24 12:41:58","title":"Enhancing Visual Continual Learning with Language-Guided Supervision","abstract":"Continual learning (CL) aims to empower models to learn new tasks without forgetting previously acquired knowledge. Most prior works concentrate on the techniques of architectures, replay data, regularization, \\etc. However, the category name of each class is largely neglected. Existing methods commonly utilize the one-hot labels and randomly initialize the classifier head. We argue that the scarce semantic information conveyed by the one-hot labels hampers the effective knowledge transfer across tasks. In this paper, we revisit the role of the classifier head within the CL paradigm and replace the classifier with semantic knowledge from pretrained language models (PLMs). Specifically, we use PLMs to generate semantic targets for each class, which are frozen and serve as supervision signals during training. Such targets fully consider the semantic correlation between all classes across tasks. Empirical studies show that our approach mitigates forgetting by alleviating representation drifting and facilitating knowledge transfer across tasks. The proposed method is simple to implement and can seamlessly be plugged into existing methods with negligible adjustments. Extensive experiments based on eleven mainstream baselines demonstrate the effectiveness and generalizability of our approach to various protocols. For example, under the class-incremental learning setting on ImageNet-100, our method significantly improves the Top-1 accuracy by 3.2\\% to 6.1\\% while reducing the forgetting rate by 2.6\\% to 13.1\\%.","sentences":["Continual learning (CL) aims to empower models to learn new tasks without forgetting previously acquired knowledge.","Most prior works concentrate on the techniques of architectures, replay data, regularization, \\etc.","However, the category name of each class is largely neglected.","Existing methods commonly utilize the one-hot labels and randomly initialize the classifier head.","We argue that the scarce semantic information conveyed by the one-hot labels hampers the effective knowledge transfer across tasks.","In this paper, we revisit the role of the classifier head within the CL paradigm and replace the classifier with semantic knowledge from pretrained language models (PLMs).","Specifically, we use PLMs to generate semantic targets for each class, which are frozen and serve as supervision signals during training.","Such targets fully consider the semantic correlation between all classes across tasks.","Empirical studies show that our approach mitigates forgetting by alleviating representation drifting and facilitating knowledge transfer across tasks.","The proposed method is simple to implement and can seamlessly be plugged into existing methods with negligible adjustments.","Extensive experiments based on eleven mainstream baselines demonstrate the effectiveness and generalizability of our approach to various protocols.","For example, under the class-incremental learning setting on ImageNet-100, our method significantly improves the Top-1 accuracy by 3.2\\% to 6.1\\% while reducing the forgetting rate by 2.6\\% to 13.1\\%."],"url":"http://arxiv.org/abs/2403.16124v1","category":"cs.CV"}
{"created":"2024-03-24 12:28:08","title":"Log-rank test with coarsened exact matching","abstract":"It is of special importance in the clinical trial to compare survival times between the treatment group and the control group. Propensity score methods with a logistic regression model are often used to reduce the effects of confounders. However, the modeling of complex structures between the covariates, the treatment assignment and the survival time is difficult. In this paper, we consider coarsened exact matching (CEM), which does not need any parametric models, and we propose the weighted log-rank statistic based on CEM. We derive asymptotic properties of the weighted log-rank statistic, such as the weak convergence to a Gaussian process in Skorokhod space, in particular the asymptotic normality, under the null hypothesis and the consistency of the log-rank test. Simulation studies show that the log-rank statistic based on CEM is more robust than the log-rank statistic based on the propensity score.","sentences":["It is of special importance in the clinical trial to compare survival times between the treatment group and the control group.","Propensity score methods with a logistic regression model are often used to reduce the effects of confounders.","However, the modeling of complex structures between the covariates, the treatment assignment and the survival time is difficult.","In this paper, we consider coarsened exact matching (CEM), which does not need any parametric models, and we propose the weighted log-rank statistic based on CEM.","We derive asymptotic properties of the weighted log-rank statistic, such as the weak convergence to a Gaussian process in Skorokhod space, in particular the asymptotic normality, under the null hypothesis and the consistency of the log-rank test.","Simulation studies show that the log-rank statistic based on CEM is more robust than the log-rank statistic based on the propensity score."],"url":"http://arxiv.org/abs/2403.16121v1","category":"math.ST"}
{"created":"2024-03-24 12:24:48","title":"Bulk universality for deformed GinUEs","abstract":"For the deformed complex Ginibre ensemble with a mean normal matrix, under certain assumptions on the mean matrix we prove that the same bulk statistics holds as in the complex Ginibre matrix bulk. This is the continuation of the previous joint papers ``Critical edge statistics for deformed GinUEs Preprint arXiv: 2311.13227v1'' and ``Repeated erfc statistics for deformed GinUEs Preprint arXiv: 2402.14362'', which deal with local eigenvalue statistics at the edge.","sentences":["For the deformed complex Ginibre ensemble with a mean normal matrix, under certain assumptions on the mean matrix we prove that the same bulk statistics holds as in the complex Ginibre matrix bulk.","This is the continuation of the previous joint papers ``Critical edge statistics for deformed GinUEs Preprint arXiv: 2311.13227v1'' and ``Repeated erfc statistics for deformed GinUEs Preprint arXiv: 2402.14362'', which deal with local eigenvalue statistics at the edge."],"url":"http://arxiv.org/abs/2403.16120v1","category":"math.PR"}
{"created":"2024-03-24 12:24:02","title":"Pressure-tuning topological phase transitions in Kagome superconductor CsTi$_3$Bi$_5$","abstract":"Recently, the Kagome metal CsTi$_3$Bi$_5$ has exhibited several novel quantum properties similar to CsV$_3$Sb$_5$, such as nontrivial topology, double-dome superconductivity, and flat band features. However, CsTi$_3$Bi$_5$ lacks the charge-density wave (CDW) present in CsV$_3$Sb$_5$, making the study of its emergence of double-dome superconductivity a focus of research. In this work, we have identified an order parameter, the three-band Z$_2$ topological index, that can describe the superconducting phase diagram of CsTi$_3$Bi$_5$ under pressure. Its evolution with pressure follows the expected behavior for superconductivity. Furthermore, the results of the Fermi surface under pressure reveal the potential presence of a Lifshitz transition in the vicinity of the vanishing point of the superconducting temperature change with pressure in CsTi$_3$Bi$_5$. These results indicate that the superconducting behavior of CsTi$_3$Bi$_5$ under pressure is caused by changes in the electronic structure leading to alterations in the topological properties, provide new insights and approaches for understanding the superconducting phenomenon in Kagome metals.","sentences":["Recently, the Kagome metal CsTi$_3$Bi$_5$ has exhibited several novel quantum properties similar to CsV$_3$Sb$_5$, such as nontrivial topology, double-dome superconductivity, and flat band features.","However, CsTi$_3$Bi$_5$ lacks the charge-density wave (CDW) present in CsV$_3$Sb$_5$, making the study of its emergence of double-dome superconductivity a focus of research.","In this work, we have identified an order parameter, the three-band Z$_2$ topological index, that can describe the superconducting phase diagram of CsTi$_3$Bi$_5$ under pressure.","Its evolution with pressure follows the expected behavior for superconductivity.","Furthermore, the results of the Fermi surface under pressure reveal the potential presence of a Lifshitz transition in the vicinity of the vanishing point of the superconducting temperature change with pressure in CsTi$_3$Bi$_5$. These results indicate that the superconducting behavior of CsTi$_3$Bi$_5$ under pressure is caused by changes in the electronic structure leading to alterations in the topological properties, provide new insights and approaches for understanding the superconducting phenomenon in Kagome metals."],"url":"http://arxiv.org/abs/2403.16119v1","category":"cond-mat.supr-con"}
{"created":"2024-03-24 12:22:18","title":"Dynamical system analysis of LRS-BI Universe with $\\boldsymbol{f(Q)}$ gravity theory","abstract":"Considering the Universe as a dynamic system, the understanding of its evolution is an interesting aspect of study in cosmology. Here, we investigate the anisotropic locally rotationally symmetric (LRS) Bianchi type-I (LRS-BI) spacetime under the $f(Q)$ gravity of symmetric teleparallel theory equivalent to the GR (STEGR) as a dynamical system and try to understand the role of anisotropy in the evolution of its various phases. For this work, we consider two models, viz., $f(Q) = -\\,(Q+ 2\\Lambda)$ and $f(Q) = -\\, \\beta Q^{n}$ to study the critical points and stability of the LRS-BI Universe. In both the models, it is found that the various phases like radiation-dominated, matter-dominated, and dark energy-dominated phases are heteroclinically connected, and there is some role of anisotropy in the evolution of these phases of the Universe. However, for both the models, there are some unphysical solutions too depending upon the values of model parameters $\\beta$ and $n$ along with the anisotropic parameter $\\alpha$.","sentences":["Considering the Universe as a dynamic system, the understanding of its evolution is an interesting aspect of study in cosmology.","Here, we investigate the anisotropic locally rotationally symmetric (LRS) Bianchi type-I (LRS-BI) spacetime under the $f(Q)$ gravity of symmetric teleparallel theory equivalent to the GR (STEGR) as a dynamical system and try to understand the role of anisotropy in the evolution of its various phases.","For this work, we consider two models, viz., $f(Q)","= -\\,(Q+ 2\\Lambda)$ and $f(Q)","= -\\, \\beta Q^{n}$ to study the critical points and stability of the LRS-BI Universe.","In both the models, it is found that the various phases like radiation-dominated, matter-dominated, and dark energy-dominated phases are heteroclinically connected, and there is some role of anisotropy in the evolution of these phases of the Universe.","However, for both the models, there are some unphysical solutions too depending upon the values of model parameters $\\beta$ and $n$ along with the anisotropic parameter $\\alpha$."],"url":"http://arxiv.org/abs/2403.16118v1","category":"gr-qc"}
{"created":"2024-03-24 12:01:27","title":"ByteCard: Enhancing Data Warehousing with Learned Cardinality Estimation","abstract":"Cardinality estimation is a critical component and a longstanding challenge in modern data warehouses. ByteHouse, ByteDance's cloud-native engine for big data analysis in exabyte-scale environments, serves numerous internal decision-making business scenarios. With the increasing demand of ByteHouse, cardinality estimation becomes the bottleneck for efficiently processing queries. Specifically, the existing query optimizer of ByteHouse uses the traditional Selinger-like cardinality estimator, which can produce huge estimation errors, resulting in sub-optimal query plans. To improve cardinality estimation accuracy while maintaining a practical inference overhead, we develop ByteCard framework that enables efficient training/updating and integration of cardinality estimators. Furthermore, ByteCard adapts recent advances in cardinality estimation to build models that can balance accuracy and practicality (e.g., inference latency, model size, training/updating overhead). We observe significant query processing speed-up in ByteHouse after replacing the system's existing cardinality estimation with ByteCard's estimations for several optimization strategies. Evaluations on real-world datasets show the integration of ByteCard leads to an improvement of up to 30% in the 99th quantile of latency. At last, we share our valuable experience in engineering advanced cardinality estimators. We believe this experience can help other data warehouses integrate more accurate and sophisticated solutions on the critical path of query execution.","sentences":["Cardinality estimation is a critical component and a longstanding challenge in modern data warehouses.","ByteHouse, ByteDance's cloud-native engine for big data analysis in exabyte-scale environments, serves numerous internal decision-making business scenarios.","With the increasing demand of ByteHouse, cardinality estimation becomes the bottleneck for efficiently processing queries.","Specifically, the existing query optimizer of ByteHouse uses the traditional Selinger-like cardinality estimator, which can produce huge estimation errors, resulting in sub-optimal query plans.","To improve cardinality estimation accuracy while maintaining a practical inference overhead, we develop ByteCard framework that enables efficient training/updating and integration of cardinality estimators.","Furthermore, ByteCard adapts recent advances in cardinality estimation to build models that can balance accuracy and practicality (e.g., inference latency, model size, training/updating overhead).","We observe significant query processing speed-up in ByteHouse after replacing the system's existing cardinality estimation with ByteCard's estimations for several optimization strategies.","Evaluations on real-world datasets show the integration of ByteCard leads to an improvement of up to 30% in the 99th quantile of latency.","At last, we share our valuable experience in engineering advanced cardinality estimators.","We believe this experience can help other data warehouses integrate more accurate and sophisticated solutions on the critical path of query execution."],"url":"http://arxiv.org/abs/2403.16110v1","category":"cs.DB"}
{"created":"2024-03-24 11:19:59","title":"CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field","abstract":"Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam.","sentences":["Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM).","Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline.","This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability.","Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping.","Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy.","Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz.","We will make our source code publicly available.","Project page: https://zju3dv.github.io/cg-slam."],"url":"http://arxiv.org/abs/2403.16095v1","category":"cs.CV"}
{"created":"2024-03-24 11:09:00","title":"Uncertainties in Measurements of Bubbly Flows Using Phase-Detection Probes","abstract":"The analysis of bubbly two-phase flows is challenging due to their turbulent nature and the need for intrusive phase-detection probes. However, accurately characterizing these flows is crucial for safely designing critical infrastructure such as dams and their appurtenant structures. The combination of dual-tip intrusive phase-detection probes with advanced signal processing algorithms enables the assessment of pseudo-instantaneous 1-D velocity time series; for which the limitations are not fully fathomed. In this investigation, we theoretically define four major sources of error, which we quantify using synthetically generated turbulent time series, coupled with the simulated response of a phase detection probe. Our findings show that typical high-velocity flows in hydraulic structures hold up to 15% error in the mean velocity estimations and up to 35% error in the turbulence intensity estimations for the most critical conditions, typically occurring in the proximity of the wall. Based on thousands of simulations, our study provides a novel data-driven tool for the estimation of these baseline errors (bias and uncertainties) in real-word phase-detection probe measurements.","sentences":["The analysis of bubbly two-phase flows is challenging due to their turbulent nature and the need for intrusive phase-detection probes.","However, accurately characterizing these flows is crucial for safely designing critical infrastructure such as dams and their appurtenant structures.","The combination of dual-tip intrusive phase-detection probes with advanced signal processing algorithms enables the assessment of pseudo-instantaneous 1-D velocity time series; for which the limitations are not fully fathomed.","In this investigation, we theoretically define four major sources of error, which we quantify using synthetically generated turbulent time series, coupled with the simulated response of a phase detection probe.","Our findings show that typical high-velocity flows in hydraulic structures hold up to 15% error in the mean velocity estimations and up to 35% error in the turbulence intensity estimations for the most critical conditions, typically occurring in the proximity of the wall.","Based on thousands of simulations, our study provides a novel data-driven tool for the estimation of these baseline errors (bias and uncertainties) in real-word phase-detection probe measurements."],"url":"http://arxiv.org/abs/2403.16091v1","category":"physics.flu-dyn"}
{"created":"2024-03-24 10:45:55","title":"RankingSHAP -- Listwise Feature Attribution Explanations for Ranking Models","abstract":"Feature attributions are a commonly used explanation type, when we want to posthoc explain the prediction of a trained model. Yet, they are not very well explored in IR. Importantly, feature attribution has rarely been rigorously defined, beyond attributing the most important feature the highest value. What it means for a feature to be more important than others is often left vague. Consequently, most approaches focus on just selecting the most important features and under utilize or even ignore the relative importance within features. In this work, we rigorously define the notion of feature attribution for ranking models, and list essential properties that a valid attribution should have. We then propose RankingSHAP as a concrete instantiation of a list-wise ranking attribution method. Contrary to current explanation evaluation schemes that focus on selections, we propose two novel evaluation paradigms for evaluating attributions over learning-to-rank models. We evaluate RankingSHAP for commonly used learning-to-rank datasets to showcase the more nuanced use of an attribution method while highlighting the limitations of selection-based explanations. In a simulated experiment we design an interpretable model to demonstrate how list-wise ranking attributes can be used to investigate model decisions and evaluate the explanations qualitatively. Because of the contrastive nature of the ranking task, our understanding of ranking model decisions can substantially benefit from feature attribution explanations like RankingSHAP.","sentences":["Feature attributions are a commonly used explanation type, when we want to posthoc explain the prediction of a trained model.","Yet, they are not very well explored in IR.","Importantly, feature attribution has rarely been rigorously defined, beyond attributing the most important feature the highest value.","What it means for a feature to be more important than others is often left vague.","Consequently, most approaches focus on just selecting the most important features and under utilize or even ignore the relative importance within features.","In this work, we rigorously define the notion of feature attribution for ranking models, and list essential properties that a valid attribution should have.","We then propose RankingSHAP as a concrete instantiation of a list-wise ranking attribution method.","Contrary to current explanation evaluation schemes that focus on selections, we propose two novel evaluation paradigms for evaluating attributions over learning-to-rank models.","We evaluate RankingSHAP for commonly used learning-to-rank datasets to showcase the more nuanced use of an attribution method while highlighting the limitations of selection-based explanations.","In a simulated experiment we design an interpretable model to demonstrate how list-wise ranking attributes can be used to investigate model decisions and evaluate the explanations qualitatively.","Because of the contrastive nature of the ranking task, our understanding of ranking model decisions can substantially benefit from feature attribution explanations like RankingSHAP."],"url":"http://arxiv.org/abs/2403.16085v1","category":"cs.IR"}
{"created":"2024-03-24 10:43:21","title":"Argument Quality Assessment in the Age of Instruction-Following Large Language Models","abstract":"The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument's quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby.","sentences":["The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like.","A critical task in any such application is the assessment of an argument's quality - but it is also particularly challenging.","In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment.","We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment.","Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems.","We discuss the real-world opportunities and ethical issues emerging thereby."],"url":"http://arxiv.org/abs/2403.16084v1","category":"cs.CL"}
{"created":"2024-03-24 09:33:45","title":"IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History","abstract":"Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming problem by utilizing the behavioral evolution history of the batched contextual bandit with inaccessible rewards. We demonstrate that IBCB is a unified framework for both deterministic and randomized bandit policies. The experimental results indicate that IBCB outperforms several existing imitation learning algorithms on synthetic and real-world data and significantly reduces running time. Additionally, empirical analyses reveal that IBCB exhibits better out-of-distribution generalization and is highly effective in learning the bandit policy from the interaction history of novice experts.","sentences":["Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert.","However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert.","This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts.","To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history.","Specifically, IBCB formulates the inverse problem into a simple quadratic programming problem by utilizing the behavioral evolution history of the batched contextual bandit with inaccessible rewards.","We demonstrate that IBCB is a unified framework for both deterministic and randomized bandit policies.","The experimental results indicate that IBCB outperforms several existing imitation learning algorithms on synthetic and real-world data and significantly reduces running time.","Additionally, empirical analyses reveal that IBCB exhibits better out-of-distribution generalization and is highly effective in learning the bandit policy from the interaction history of novice experts."],"url":"http://arxiv.org/abs/2403.16075v1","category":"cs.LG"}
{"created":"2024-03-24 09:22:52","title":"On the Secrecy Enhancement of an Integrated Ground-Aerial Network with a Hybrid FSO/THz Feeder Link","abstract":"High altitude platforms (HAPs)-aided terrestrial-aerial communication technology based on free-space optical (FSO) and Terahertz (THz) feeder links has been attracting notable interest recently due to its great potential in reaching a higher data rate and connectivity. Nonetheless, the presence of harsh vertical propagation environments and potential aerial eavesdroppers are two of the main challenges limiting the reliability and security of such a technology. In this work, a secrecy-enhancing scheme for HAP-aided ground-aerial communication is proposed. The considered network consists of HAP-assisted communication between a ground station and a legitimate user under the threat of an aerial and ground eavesdropper. Thus, the proposed scheme leverages (i) HAP diversity by exploiting the presence of multiple flying HAPs and (ii) the use of a hybrid FSO/THz transmission scheme to offer better resilience against eavesdropping attacks. An analytical secrecy outage probability (SOP) expression is derived for the scheme in consideration. Results manifest the notable gain in security of the proposed scheme with respect to both (i) the single-HAP and (ii) THz feeder-based benchmark ones, where the proposed scheme's SOP is decreased by four orders of magnitude using $4$ HAPs with respect to the first benchmark scheme, while a $5$-dB secrecy gain is manifested with respect to the second benchmark one.","sentences":["High altitude platforms (HAPs)-aided terrestrial-aerial communication technology based on free-space optical (FSO) and Terahertz (THz) feeder links has been attracting notable interest recently due to its great potential in reaching a higher data rate and connectivity.","Nonetheless, the presence of harsh vertical propagation environments and potential aerial eavesdroppers are two of the main challenges limiting the reliability and security of such a technology.","In this work, a secrecy-enhancing scheme for HAP-aided ground-aerial communication is proposed.","The considered network consists of HAP-assisted communication between a ground station and a legitimate user under the threat of an aerial and ground eavesdropper.","Thus, the proposed scheme leverages (i) HAP diversity by exploiting the presence of multiple flying HAPs and (ii) the use of a hybrid FSO/THz transmission scheme to offer better resilience against eavesdropping attacks.","An analytical secrecy outage probability (SOP) expression is derived for the scheme in consideration.","Results manifest the notable gain in security of the proposed scheme with respect to both (i) the single-HAP and (ii) THz feeder-based benchmark ones, where the proposed scheme's SOP is decreased by four orders of magnitude using $4$ HAPs with respect to the first benchmark scheme, while a $5$-dB secrecy gain is manifested with respect to the second benchmark one."],"url":"http://arxiv.org/abs/2403.16072v1","category":"cs.IT"}
{"created":"2024-03-24 08:26:54","title":"Markovian dynamics for a quantum/classical system and quantum trajectories","abstract":"Quantum trajectory techniques have been used in the theory of open systems as a starting point for numerical computations and to describe the monitoring of a quantum system in continuous time. Here we extend this technique and use it to develop a general approach to the dynamics of quantum/classical hybrid systems. By using two coupled stochastic differential equations, we can describe a classical component and a quantum one which have their own intrinsic dynamics and which interact with each other. A mathematically rigorous construction is given, under the restriction of having a Markovian joint dynamics and of involving only bounded operators on the Hilbert space of the quantum component. An important feature is that, if the interaction allows for a flow of information from the quantum component to the classical one, necessarily the dynamics is dissipative. We show also how this theory is connected to a suitable hybrid dynamical semigroup, which reduces to a quantum dynamical semigroup in the purely quantum case and includes Liouville and Kolmogorov-Fokker-Plank equations in the purely classical case. Moreover, this semigroup allows to compare the proposed stochastic dynamics with various other proposals based on hybrid master equations. Some simple example are constructed in order to show the variety of physical behaviours which can be described; in particular, a model presenting hidden entanglement is introduced.","sentences":["Quantum trajectory techniques have been used in the theory of open systems as a starting point for numerical computations and to describe the monitoring of a quantum system in continuous time.","Here we extend this technique and use it to develop a general approach to the dynamics of quantum/classical hybrid systems.","By using two coupled stochastic differential equations, we can describe a classical component and a quantum one which have their own intrinsic dynamics and which interact with each other.","A mathematically rigorous construction is given, under the restriction of having a Markovian joint dynamics and of involving only bounded operators on the Hilbert space of the quantum component.","An important feature is that, if the interaction allows for a flow of information from the quantum component to the classical one, necessarily the dynamics is dissipative.","We show also how this theory is connected to a suitable hybrid dynamical semigroup, which reduces to a quantum dynamical semigroup in the purely quantum case and includes Liouville and Kolmogorov-Fokker-Plank equations in the purely classical case.","Moreover, this semigroup allows to compare the proposed stochastic dynamics with various other proposals based on hybrid master equations.","Some simple example are constructed in order to show the variety of physical behaviours which can be described; in particular, a model presenting hidden entanglement is introduced."],"url":"http://arxiv.org/abs/2403.16065v1","category":"quant-ph"}
{"created":"2024-03-24 08:10:38","title":"Port Forwarding Services Are Forwarding Security Risks","abstract":"We conduct the first comprehensive security study on representative port forwarding services (PFS), which emerge in recent years and make the web services deployed in internal networks available on the Internet along with better usability but less complexity compared to traditional techniques (e.g., NAT traversal techniques). Our study is made possible through a set of novel methodologies, which are designed to uncover the technical mechanisms of PFS, experiment attack scenarios for PFS protocols, automatically discover and snapshot port-forwarded websites (PFWs) at scale, and classify PFWs into well-observed categories. Leveraging these methodologies, we have observed the widespread adoption of PFS with millions of PFWs distributed across tens of thousands of ISPs worldwide. Furthermore, 32.31% PFWs have been classified into website categories that serve access to critical data or infrastructure, such as, web consoles for industrial control systems, IoT controllers, code repositories, and office automation systems. And 18.57% PFWs didn't enforce any access control for external visitors. Also identified are two types of attacks inherent in the protocols of Oray (one well-adopted PFS provider), and the notable abuse of PFSes by malicious actors in activities such as malware distribution, botnet operation and phishing.","sentences":["We conduct the first comprehensive security study on representative port forwarding services (PFS), which emerge in recent years and make the web services deployed in internal networks available on the Internet along with better usability but less complexity compared to traditional techniques (e.g., NAT traversal techniques).","Our study is made possible through a set of novel methodologies, which are designed to uncover the technical mechanisms of PFS, experiment attack scenarios for PFS protocols, automatically discover and snapshot port-forwarded websites (PFWs) at scale, and classify PFWs into well-observed categories.","Leveraging these methodologies, we have observed the widespread adoption of PFS with millions of PFWs distributed across tens of thousands of ISPs worldwide.","Furthermore, 32.31% PFWs have been classified into website categories that serve access to critical data or infrastructure, such as, web consoles for industrial control systems, IoT controllers, code repositories, and office automation systems.","And 18.57% PFWs didn't enforce any access control for external visitors.","Also identified are two types of attacks inherent in the protocols of Oray (one well-adopted PFS provider), and the notable abuse of PFSes by malicious actors in activities such as malware distribution, botnet operation and phishing."],"url":"http://arxiv.org/abs/2403.16060v1","category":"cs.CR"}
{"created":"2024-03-24 07:46:26","title":"Many-hypercube codes: High-rate quantum error-correcting codes for high-performance fault-tolerant quantum computation","abstract":"Conventional approaches to quantum error correction for fault-tolerant quantum computation are based on encoding a single logical qubit into many physical qubits, resulting in asymptotically zero encoding rates and therefore huge resource overheads. To overcome this issue, high-rate quantum codes, such as quantum low-density parity-check codes, have been studied over the past decade. However, such codes have complex structure, making it difficult to perform logical gate operations in parallel without sacrificing their advantage. Observing the simple structure and high rates of quantum error-detecting codes, here we propose concatenated high-rate quantum error-detecting codes as a new family of high-rate quantum codes. Their simple structure allows for a geometrical interpretation using hypercubes, each of which corresponds to a logical qubit. We thus call them many-hypercube codes. The encoding rate is remarkably high, e.g., 30% (64 logical qubits are encoded into 216 physical qubits). Developing a dedicated high-performance decoder, we achieve high error thresholds even in a circuit-level noise model. Logical gate operations are also parallelizable. Thus, the many-hypercube codes will pave the way to high-performance fault-tolerant quantum computation.","sentences":["Conventional approaches to quantum error correction for fault-tolerant quantum computation are based on encoding a single logical qubit into many physical qubits, resulting in asymptotically zero encoding rates and therefore huge resource overheads.","To overcome this issue, high-rate quantum codes, such as quantum low-density parity-check codes, have been studied over the past decade.","However, such codes have complex structure, making it difficult to perform logical gate operations in parallel without sacrificing their advantage.","Observing the simple structure and high rates of quantum error-detecting codes, here we propose concatenated high-rate quantum error-detecting codes as a new family of high-rate quantum codes.","Their simple structure allows for a geometrical interpretation using hypercubes, each of which corresponds to a logical qubit.","We thus call them many-hypercube codes.","The encoding rate is remarkably high, e.g., 30% (64 logical qubits are encoded into 216 physical qubits).","Developing a dedicated high-performance decoder, we achieve high error thresholds even in a circuit-level noise model.","Logical gate operations are also parallelizable.","Thus, the many-hypercube codes will pave the way to high-performance fault-tolerant quantum computation."],"url":"http://arxiv.org/abs/2403.16054v1","category":"quant-ph"}
{"created":"2024-03-24 07:24:22","title":"Elemental Patterns from the Erd\u0151s Straus Conjecture","abstract":"This paper makes the following conjecture: For every prime $p$ there exists a positive integer $x$ with $\\left\\lceil \\frac{p}{4} \\right\\rceil \\leq x \\leq \\left\\lceil \\frac{p}{2} \\right\\rceil$ and a positive divisor $d|x^2$ so that either: (1) $ d \\bmod \\left( 4x - p \\right) \\equiv -px$; or (2) $d \\leq x$ and $ d \\bmod \\left( 4x - p \\right) \\equiv -x$. Furthermore this paper proves that the solutions to these modular equations are in one-to-one correspondence with the solutions of the diophantine equation used in the Erd\\H{o}s Straus conjecture.","sentences":["This paper makes the following conjecture: For every prime $p$ there exists a positive integer $x$ with $\\left\\lceil \\frac{p}{4} \\right\\rceil \\leq x \\leq \\left\\lceil \\frac{p}{2} \\right\\rceil$ and a positive divisor $d|x^2$ so that either: (1) $ d \\bmod \\left( 4x - p \\right) \\equiv -px$; or (2) $d \\leq","x$","and $ d \\bmod \\left( 4x - p \\right) \\equiv -x$.","Furthermore this paper proves that the solutions to these modular equations are in one-to-one correspondence with the solutions of the diophantine equation used in the Erd\\H{o}s Straus conjecture."],"url":"http://arxiv.org/abs/2403.16047v1","category":"math.NT"}
{"created":"2024-03-24 07:04:07","title":"Force Controlled Printing for Material Extrusion Additive Manufacturing","abstract":"In material extrusion additive manufacturing, the extrusion process is commonly controlled in a feed-forward fashion. The amount of material to be extruded at each printing location is pre-computed by a planning software. This approach is inherently unable to adapt the extrusion to external and unexpected disturbances, and the quality of the results strongly depends on a number of modeling and tuning parameters. To overcome these limitations, we propose the first framework for Force Controlled Printing for material extrusion additive manufacturing. We utilize a custom-built extruder to measure the extrusion force in real time, and use this quantity as feedback to continuously control the material flow in closed-loop. We demonstrate the existence of a strong correlation between extrusion force and line width, which we exploit to deposit lines of desired width in a width range of 33 % up to 233 % of the nozzle diameter. We also show how Force Controlled Printing outperforms conventional feed-forward extrusion in print quality and disturbance rejection, while requiring little tuning and automatically adapting to changes in the hardware settings. With no adaptation, Force Controlled Printing can deposit lines of desired width under severe disturbances in bed leveling, such as at layer heights ranging between 20 % and 200 % of the nominal height.","sentences":["In material extrusion additive manufacturing, the extrusion process is commonly controlled in a feed-forward fashion.","The amount of material to be extruded at each printing location is pre-computed by a planning software.","This approach is inherently unable to adapt the extrusion to external and unexpected disturbances, and the quality of the results strongly depends on a number of modeling and tuning parameters.","To overcome these limitations, we propose the first framework for Force Controlled Printing for material extrusion additive manufacturing.","We utilize a custom-built extruder to measure the extrusion force in real time, and use this quantity as feedback to continuously control the material flow in closed-loop.","We demonstrate the existence of a strong correlation between extrusion force and line width, which we exploit to deposit lines of desired width in a width range of 33 % up to 233 % of the nozzle diameter.","We also show how Force Controlled Printing outperforms conventional feed-forward extrusion in print quality and disturbance rejection, while requiring little tuning and automatically adapting to changes in the hardware settings.","With no adaptation, Force Controlled Printing can deposit lines of desired width under severe disturbances in bed leveling, such as at layer heights ranging between 20 % and 200 % of the nominal height."],"url":"http://arxiv.org/abs/2403.16042v1","category":"eess.SY"}
{"created":"2024-03-24 06:54:20","title":"Robust estimations from distribution structures: III. Invariant Moments","abstract":"Descriptive statistics for parametric models are currently highly sensative to departures, gross errors, and/or random errors. Here, leveraging the structures of parametric distributions and their central moment kernel distributions, a class of estimators, consistent simultanously for both a semiparametric distribution and a distinct parametric distribution, is proposed. These efficient estimators are robust to both gross errors and departures from parametric assumptions, making them ideal for estimating the mean and central moments of common unimodal distributions. This article opens up the possibility of utilizing the common nature of probability models to construct near-optimal estimators that are suitable for various scenarios.","sentences":["Descriptive statistics for parametric models are currently highly sensative to departures, gross errors, and/or random errors.","Here, leveraging the structures of parametric distributions and their central moment kernel distributions, a class of estimators, consistent simultanously for both a semiparametric distribution and a distinct parametric distribution, is proposed.","These efficient estimators are robust to both gross errors and departures from parametric assumptions, making them ideal for estimating the mean and central moments of common unimodal distributions.","This article opens up the possibility of utilizing the common nature of probability models to construct near-optimal estimators that are suitable for various scenarios."],"url":"http://arxiv.org/abs/2403.16039v1","category":"math.ST"}
{"created":"2024-03-24 06:41:33","title":"Knowledge-aware Dual-side Attribute-enhanced Recommendation","abstract":"\\textit{Knowledge-aware} recommendation methods (KGR) based on \\textit{graph neural networks} (GNNs) and \\textit{contrastive learning} (CL) have achieved promising performance. However, they fall short in modeling fine-grained user preferences and further fail to leverage the \\textit{preference-attribute connection} to make predictions, leading to sub-optimal performance. To address the issue, we propose a method named \\textit{\\textbf{K}nowledge-aware \\textbf{D}ual-side \\textbf{A}ttribute-enhanced \\textbf{R}ecommendation} (KDAR). Specifically, we build \\textit{user preference representations} and \\textit{attribute fusion representations} upon the attribute information in knowledge graphs, which are utilized to enhance \\textit{collaborative filtering} (CF) based user and item representations, respectively. To discriminate the contribution of each attribute in these two types of attribute-based representations, a \\textit{multi-level collaborative alignment contrasting} mechanism is proposed to align the importance of attributes with CF signals. Experimental results on four benchmark datasets demonstrate the superiority of KDAR over several state-of-the-art baselines. Further analyses verify the effectiveness of our method. The code of KDAR is released at: \\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.","sentences":["\\textit{Knowledge-aware} recommendation methods (KGR) based on \\textit{graph neural networks} (GNNs) and \\textit{contrastive learning} (CL) have achieved promising performance.","However, they fall short in modeling fine-grained user preferences and further fail to leverage the \\textit{preference-attribute connection} to make predictions, leading to sub-optimal performance.","To address the issue, we propose a method named \\textit{\\textbf{K}nowledge-aware \\textbf{D}ual-side \\textbf{A}ttribute-enhanced \\textbf{R}ecommendation} (KDAR).","Specifically, we build \\textit{user preference representations} and \\textit{attribute fusion representations} upon the attribute information in knowledge graphs, which are utilized to enhance \\textit{collaborative filtering} (CF) based user and item representations, respectively.","To discriminate the contribution of each attribute in these two types of attribute-based representations, a \\textit{multi-level collaborative alignment contrasting} mechanism is proposed to align the importance of attributes with CF signals.","Experimental results on four benchmark datasets demonstrate the superiority of KDAR over several state-of-the-art baselines.","Further analyses verify the effectiveness of our method.","The code of KDAR is released at: \\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}."],"url":"http://arxiv.org/abs/2403.16037v1","category":"cs.IR"}
{"created":"2024-03-24 06:28:54","title":"Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks","abstract":"Graph data, also known as complex network data, is omnipresent across various domains and applications. Prior graph neural network models primarily focused on extracting task-specific structural features through supervised learning objectives, but they fell short in capturing the inherent semantic and structural features of the entire graph. In this paper, we introduce the semantic-structural attention-enhanced graph convolutional network (SSA-GCN), which not only models the graph structure but also extracts generalized unsupervised features to enhance vertex classification performance. The SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic information through unsupervised feature extraction from a knowledge graph perspective; secondly, it obtains structural information through unsupervised feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attention mechanism. By leveraging these features, we augment the graph convolutional network, thereby enhancing the model's generalization capabilities. Our experiments on the Cora and CiteSeer datasets demonstrate the performance improvements achieved by our proposed method. Furthermore, our approach also exhibits excellent accuracy under privacy settings, making it a robust and effective solution for graph data analysis.","sentences":["Graph data, also known as complex network data, is omnipresent across various domains and applications.","Prior graph neural network models primarily focused on extracting task-specific structural features through supervised learning objectives, but they fell short in capturing the inherent semantic and structural features of the entire graph.","In this paper, we introduce the semantic-structural attention-enhanced graph convolutional network (SSA-GCN), which not only models the graph structure but also extracts generalized unsupervised features to enhance vertex classification performance.","The SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic information through unsupervised feature extraction from a knowledge graph perspective; secondly, it obtains structural information through unsupervised feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attention mechanism.","By leveraging these features, we augment the graph convolutional network, thereby enhancing the model's generalization capabilities.","Our experiments on the Cora and CiteSeer datasets demonstrate the performance improvements achieved by our proposed method.","Furthermore, our approach also exhibits excellent accuracy under privacy settings, making it a robust and effective solution for graph data analysis."],"url":"http://arxiv.org/abs/2403.16033v1","category":"cs.LG"}
{"created":"2024-03-24 06:10:56","title":"VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections","abstract":"Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from complex graph topological information and makes heavy feature engineering offline and independent, such that mini-batch training of graph transformers is possible by loading each node's token list in batches. We further prove this PPR tokenization is viable as a graph convolution network with a fixed polynomial filter and jumping knowledge. However, only using personalized PageRank may limit information carried by a token list, which could not support different graph inductive biases for model training. To this end, (2) we rewire graphs by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR tokenization to encode local and global contexts, long-range interaction, and heterophilous information into each node's token list, and then formalize our Virtual Connection Ranking based Graph Transformer (VCR-Graphormer).","sentences":["Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs.","Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data.","Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations.","Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations.","This PPR tokenization method decouples model training from complex graph topological information and makes heavy feature engineering offline and independent, such that mini-batch training of graph transformers is possible by loading each node's token list in batches.","We further prove this PPR tokenization is viable as a graph convolution network with a fixed polynomial filter and jumping knowledge.","However, only using personalized PageRank may limit information carried by a token list, which could not support different graph inductive biases for model training.","To this end, (2) we rewire graphs by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR tokenization to encode local and global contexts, long-range interaction, and heterophilous information into each node's token list, and then formalize our Virtual Connection Ranking based Graph Transformer (VCR-Graphormer)."],"url":"http://arxiv.org/abs/2403.16030v1","category":"cs.LG"}
{"created":"2024-03-24 06:10:22","title":"Exploring the Impact of Dataset Bias on Dataset Distillation","abstract":"Dataset Distillation (DD) is a promising technique to synthesize a smaller dataset that preserves essential information from the original dataset. This synthetic dataset can serve as a substitute for the original large-scale one, and help alleviate the training workload. However, current DD methods typically operate under the assumption that the dataset is unbiased, overlooking potential bias issues within the dataset itself. To fill in this blank, we systematically investigate the influence of dataset bias on DD. To the best of our knowledge, this is the first exploration in the DD domain. Given that there are no suitable biased datasets for DD, we first construct two biased datasets, CMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis. Then we utilize existing DD methods to generate synthetic datasets on CMNIST-DD and CCIFAR10-DD, and evaluate their performance following the standard process. Experiments demonstrate that biases present in the original dataset significantly impact the performance of the synthetic dataset in most cases, which highlights the necessity of identifying and mitigating biases in the original datasets during DD. Finally, we reformulate DD within the context of a biased dataset. Our code along with biased datasets are available at https://github.com/yaolu-zjut/Biased-DD.","sentences":["Dataset Distillation (DD) is a promising technique to synthesize a smaller dataset that preserves essential information from the original dataset.","This synthetic dataset can serve as a substitute for the original large-scale one, and help alleviate the training workload.","However, current DD methods typically operate under the assumption that the dataset is unbiased, overlooking potential bias issues within the dataset itself.","To fill in this blank, we systematically investigate the influence of dataset bias on DD.","To the best of our knowledge, this is the first exploration in the DD domain.","Given that there are no suitable biased datasets for DD, we first construct two biased datasets, CMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis.","Then we utilize existing DD methods to generate synthetic datasets on CMNIST-DD and CCIFAR10-DD, and evaluate their performance following the standard process.","Experiments demonstrate that biases present in the original dataset significantly impact the performance of the synthetic dataset in most cases, which highlights the necessity of identifying and mitigating biases in the original datasets during DD.","Finally, we reformulate DD within the context of a biased dataset.","Our code along with biased datasets are available at https://github.com/yaolu-zjut/Biased-DD."],"url":"http://arxiv.org/abs/2403.16028v1","category":"cs.CV"}
{"created":"2024-03-24 06:06:55","title":"Many-one reducibility with realizability","abstract":"In this article, we propose a new classification of $\\Sigma^0_2$ formulas under the realizability interpretation of many-one reducibility (i.e., Levin reducibility). For example, ${\\sf Fin}$, the decision of being eventually zero for sequences, is many-one/Levin complete among $\\Sigma^0_2$ formulas of the form $\\exists n\\forall m\\geq n.\\varphi(m,x)$, where $\\varphi$ is decidable. The decision of boundedness for sequences ${\\sf BddSeq}$ and posets ${\\sf PO}_{\\sf top}$ are many-one/Levin complete among $\\Sigma^0_2$ formulas of the form $\\exists n\\forall m\\geq n\\forall k.\\varphi(m,k,x)$, where $\\varphi$ is decidable. However, unlike the classical many-one reducibility, none of the above is $\\Sigma^0_2$-complete. The decision of non-density of linear order ${\\sf NonDense}$ is truly $\\Sigma^0_2$-complete.","sentences":["In this article, we propose a new classification of $\\Sigma^0_2$ formulas under the realizability interpretation of many-one reducibility (i.e., Levin reducibility).","For example, ${\\sf Fin}$, the decision of being eventually zero for sequences, is many-one/Levin complete among $\\Sigma^0_2$ formulas of the form $\\exists n\\forall m\\geq n.\\varphi(m,x)$, where $\\varphi$ is decidable.","The decision of boundedness for sequences ${\\sf BddSeq}$ and posets ${\\sf PO}_{\\sf top}$ are many-one/Levin complete among $\\Sigma^0_2$ formulas of the form $\\exists n\\forall m\\geq n\\forall k.\\varphi(m,k,x)$, where $\\varphi$ is decidable.","However, unlike the classical many-one reducibility, none of the above is $\\Sigma^0_2$-complete.","The decision of non-density of linear order ${\\sf NonDense}$ is truly $\\Sigma^0_2$-complete."],"url":"http://arxiv.org/abs/2403.16027v1","category":"math.LO"}
{"created":"2024-03-24 04:07:30","title":"Fine-Grained Assertion-Based Test Selection","abstract":"For large software applications, running the whole test suite after each code change is time- and resource-intensive. Regression test selection techniques aim at reducing test execution time by selecting only the tests that are affected by code changes. However, existing techniques select test entities at coarse granularity levels such as test class, which causes imprecise test selection and executing unaffected tests. We propose a novel approach that increases the selection precision by analyzing test code at statement level and treating test assertions as the unit for selection. We implement our fine-grained test selection approach in a tool called SELERTION and evaluate it by comparing against two state-of-the-art test selection techniques using 11 open-source subjects. Our results show that SELERTION increases selection precision for all the subjects. Our test selection reduces, on average, 63% of the overall test time, making regression testing up to 23% faster than the other techniques. Our results also indicate that subjects with longer test execution time benefit more by our fine-grained selection technique.","sentences":["For large software applications, running the whole test suite after each code change is time- and resource-intensive.","Regression test selection techniques aim at reducing test execution time by selecting only the tests that are affected by code changes.","However, existing techniques select test entities at coarse granularity levels such as test class, which causes imprecise test selection and executing unaffected tests.","We propose a novel approach that increases the selection precision by analyzing test code at statement level and treating test assertions as the unit for selection.","We implement our fine-grained test selection approach in a tool called SELERTION and evaluate it by comparing against two state-of-the-art test selection techniques using 11 open-source subjects.","Our results show that SELERTION increases selection precision for all the subjects.","Our test selection reduces, on average, 63% of the overall test time, making regression testing up to 23% faster than the other techniques.","Our results also indicate that subjects with longer test execution time benefit more by our fine-grained selection technique."],"url":"http://arxiv.org/abs/2403.16001v1","category":"cs.SE"}
{"created":"2024-03-24 03:57:21","title":"Near-Optimal differentially private low-rank trace regression with guaranteed private initialization","abstract":"We study differentially private (DP) estimation of a rank-$r$ matrix $M \\in \\mathbb{R}^{d_1\\times d_2}$ under the trace regression model with Gaussian measurement matrices. Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the differential-privacy-constrained minimax lower bound for estimating $M$ under the Schatten-$q$ norm is established. Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a sample size of $n \\geq \\widetilde O (r^2 (d_1\\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and sample size of $n \\geq \\widetilde O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap between the minimax lower bound and the upper bound of low-rank matrix estimation under the trace regression model. It is shown that the estimator given by DP-RGrad attains the optimal convergence rate in a weaker notion of differential privacy. Our powerful technique for analyzing the sensitivity of initialization requires no eigengap condition between $r$ non-zero singular values.","sentences":["We study differentially private (DP) estimation of a rank-$r$ matrix $M \\in \\mathbb{R}^{d_1\\times d_2}$ under the trace regression model with Gaussian measurement matrices.","Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the differential-privacy-constrained minimax lower bound for estimating $M$ under the Schatten-$q$ norm is established.","Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a sample size of $n \\geq \\widetilde O (r^2 (d_1\\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and sample size of $n \\geq \\widetilde O(r (d_1 +","d_2))$. Finally, the paper discusses the non-trivial gap between the minimax lower bound and the upper bound of low-rank matrix estimation under the trace regression model.","It is shown that the estimator given by DP-RGrad attains the optimal convergence rate in a weaker notion of differential privacy.","Our powerful technique for analyzing the sensitivity of initialization requires no eigengap condition between $r$ non-zero singular values."],"url":"http://arxiv.org/abs/2403.15999v1","category":"stat.ML"}
{"created":"2024-03-24 03:46:29","title":"Output Feedback Control of Suspended Sediment Load Entrainment in Water Canals and Reservoirs","abstract":"This paper addresses the management of water flow in a rectangular open channel, considering the dynamic nature of both the channel's bathymetry and the suspended sediment particles caused by entrainment and deposition effects. The control-oriented model under study is a set of coupled nonlinear partial differential equations (PDEs) describing conservation of mass and momentum while accounting for constitutive relations that govern sediment erosion and deposition phenomena. The proposed boundary control problem presents a fresh perspective in water canal management and expands Saint-Venant Exner (SVE) control frameworks by integrating dynamics related to the transport of fine particles. After linearization, PDE backstepping design is employed to stabilize both the bathymetry, the water dynamics together with the concentration of suspended sediment particles. Two underflow sluice gates are used for flow control at the upstream and downstream boundaries with only the downstream component being actuated. An observer-based backstepping control design is carried out for the downstream gate using state measurement at the upstream gate to globally exponentially stabilize the linearized system to a desired equilibrium point in $\\mathscr{L}^2$ sense. The stability analysis is performed on the linearized model which is a system of four coupled PDEs, three of which are rightward convecting and one leftward. The proposed control design has the potential to facilitate efficient reservoir flushing operations. Consistent simulation results are presented to illustrate the feasibility of the designed control law.","sentences":["This paper addresses the management of water flow in a rectangular open channel, considering the dynamic nature of both the channel's bathymetry and the suspended sediment particles caused by entrainment and deposition effects.","The control-oriented model under study is a set of coupled nonlinear partial differential equations (PDEs) describing conservation of mass and momentum while accounting for constitutive relations that govern sediment erosion and deposition phenomena.","The proposed boundary control problem presents a fresh perspective in water canal management and expands Saint-Venant Exner (SVE) control frameworks by integrating dynamics related to the transport of fine particles.","After linearization, PDE backstepping design is employed to stabilize both the bathymetry, the water dynamics together with the concentration of suspended sediment particles.","Two underflow sluice gates are used for flow control at the upstream and downstream boundaries with only the downstream component being actuated.","An observer-based backstepping control design is carried out for the downstream gate using state measurement at the upstream gate to globally exponentially stabilize the linearized system to a desired equilibrium point in $\\mathscr{L}^2$ sense.","The stability analysis is performed on the linearized model which is a system of four coupled PDEs, three of which are rightward convecting and one leftward.","The proposed control design has the potential to facilitate efficient reservoir flushing operations.","Consistent simulation results are presented to illustrate the feasibility of the designed control law."],"url":"http://arxiv.org/abs/2403.15998v1","category":"math.OC"}
{"created":"2024-03-25 12:07:24","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network","abstract":"Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively.","sentences":["Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks.","The existing methods for MAV detection assume that the training set and testing set have the same distribution.","As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy.","In this paper, we study the problem of cross-domain MAV detection.","The contributions of this paper are threefold.","1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images.","Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles.","A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset.","2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure.","To reduce the challenging pseudo-label noises, two novel modules are designed in this network.","The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties.","The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises.","3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones.","In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively."],"url":"http://arxiv.org/abs/2403.16669v1","category":"cs.RO"}
{"created":"2024-03-25 12:00:57","title":"Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $\u03b1$","abstract":"The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields. However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis. In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts. Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy.","sentences":["The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields.","However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis.","In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts.","Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy."],"url":"http://arxiv.org/abs/2403.16665v1","category":"cs.DS"}
{"created":"2024-03-25 11:40:14","title":"A short proof of the Dvoretzky--Kiefer--Wolfowitz--Massart inequality","abstract":"The Dvoretzky--Kiefer--Wolfowitz--Massart inequality gives a sub-Gaussian tail bound on the supremum norm distance between the empirical distribution function of a random sample and its population counterpart. We provide a short proof of a result that improves the existing bound in two respects. First, our one-sided bound holds without any restrictions on the failure probability, thereby verifying a conjecture of Birnbaum and McCarty (1958). Second, it is local in the sense that it holds uniformly over sub-intervals of the real line with an error rate that adapts to the behaviour of the population distribution function on the interval.","sentences":["The Dvoretzky--Kiefer--Wolfowitz--Massart inequality gives a sub-Gaussian tail bound on the supremum norm distance between the empirical distribution function of a random sample and its population counterpart.","We provide a short proof of a result that improves the existing bound in two respects.","First, our one-sided bound holds without any restrictions on the failure probability, thereby verifying a conjecture of Birnbaum and McCarty (1958).","Second, it is local in the sense that it holds uniformly over sub-intervals of the real line with an error rate that adapts to the behaviour of the population distribution function on the interval."],"url":"http://arxiv.org/abs/2403.16651v1","category":"math.PR"}
{"created":"2024-03-25 11:24:02","title":"V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster","abstract":"The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle's perception capability through message communication among neighboring traffic agents. Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units. However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication. To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information. The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling. Building upon this representation, we propose a novel framework V2X-PC for collaborative perception. This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers. As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object. To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without finetuning. Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps.","sentences":["The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle's perception capability through message communication among neighboring traffic agents.","Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units.","However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication.","To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information.","The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling.","Building upon this representation, we propose a novel framework V2X-PC for collaborative perception.","This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers.","As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object.","To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without finetuning.","Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps."],"url":"http://arxiv.org/abs/2403.16635v1","category":"cs.CV"}
{"created":"2024-03-25 10:20:50","title":"Research Challenges for Adaptive Architecture: Empowering Occupants of Multi-Occupancy Buildings","abstract":"This positional paper outlines our vision of 'adaptive architecture', which involves the integration of robotic technology to physically change an architectural space in supporting the changing needs of its occupants, in response to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data & Technology\" call on \"How do new technologies enable and empower the inhabitants of multi-occupancy buildings?\". Specifically, while adaptive architecture holds promise for enhancing occupant satisfaction, comfort, and overall health and well-being, there remains a range of research challenges of (1) how it can effectively support individual occupants, while (2) mediating the conflicting needs of collocated others, and (3) integrating meaningfully into the sociocultural characteristics of their building community.","sentences":["This positional paper outlines our vision of 'adaptive architecture', which involves the integration of robotic technology to physically change an architectural space in supporting the changing needs of its occupants, in response to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data & Technology\" call on \"How do new technologies enable and empower the inhabitants of multi-occupancy buildings?\".","Specifically, while adaptive architecture holds promise for enhancing occupant satisfaction, comfort, and overall health and well-being, there remains a range of research challenges of (1) how it can effectively support individual occupants, while (2) mediating the conflicting needs of collocated others, and (3) integrating meaningfully into the sociocultural characteristics of their building community."],"url":"http://arxiv.org/abs/2403.16600v1","category":"cs.RO"}
{"created":"2024-03-25 07:29:18","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","abstract":"As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality prompt tuning approaches. Significantly, PathoTune facilitates the direct adaptation of natural visual foundation models to pathological tasks, drastically outperforming pathological foundation models with simple linear probing. The code will be available upon acceptance.","sentences":["As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving.","Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored.","For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap.","To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning.","The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features.","Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality prompt tuning approaches.","Significantly, PathoTune facilitates the direct adaptation of natural visual foundation models to pathological tasks, drastically outperforming pathological foundation models with simple linear probing.","The code will be available upon acceptance."],"url":"http://arxiv.org/abs/2403.16497v1","category":"cs.CV"}
{"created":"2024-03-25 07:20:59","title":"Sutured Heegaard Floer and embedded contact homologies are isomorphic","abstract":"We prove the equivalence of the sutured versions of Heegaard Floer homology, monopole Floer homology, and embedded contact homology. As applications we show that the knot versions of Heegaard Floer homology and embedded contact homology are equivalent and that product sutured 3-manifolds are characterized by the fact that they carry an adapted Reeb vector field without periodic orbits.","sentences":["We prove the equivalence of the sutured versions of Heegaard Floer homology, monopole Floer homology, and embedded contact homology.","As applications we show that the knot versions of Heegaard Floer homology and embedded contact homology are equivalent and that product sutured 3-manifolds are characterized by the fact that they carry an adapted Reeb vector field without periodic orbits."],"url":"http://arxiv.org/abs/2403.16492v1","category":"math.SG"}
{"created":"2024-03-25 03:43:36","title":"Implementing and Evaluating E2LSH on Storage","abstract":"Locality sensitive hashing (LSH) is one of the widely-used approaches to approximate nearest neighbor search (ANNS) in high-dimensional spaces. The first work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be solved efficiently at a sublinear query time in the database size with theoretically-guaranteed accuracy, although it required a large hash index size. Since then, several LSH variants having much smaller index sizes have been proposed. Their query time is linear or superlinear, but they have been shown to run effectively faster because they require fewer I/Os when the index is stored on hard disk drives and because they also permit in-memory execution with modern DRAM capacity.   In this paper, we show that E2LSH is regaining the advantage in query speed with the advent of modern flash storage devices such as solid-state drives (SSDs). We evaluate E2LSH on a modern single-node computing environment and analyze its computational cost and I/O cost, from which we derive storage performance requirements for its external memory execution. Our analysis indicates that E2LSH on a single consumer-grade SSD can run faster than the state-of-the-art small-index methods executed in-memory. It also indicates that E2LSH with emerging high-performance storage devices and interfaces can approach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to external memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical large datasets of up to one billion objects using different combinations of modern storage devices and interfaces. We demonstrate that our E2LSHoS implementation runs much faster than small-index methods and can approach in-memory E2LSH speeds, and also that its query time scales sublinearly with the database size beyond the index size limit of in-memory E2LSH.","sentences":["Locality sensitive hashing (LSH) is one of the widely-used approaches to approximate nearest neighbor search (ANNS) in high-dimensional spaces.","The first work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be solved efficiently at a sublinear query time in the database size with theoretically-guaranteed accuracy, although it required a large hash index size.","Since then, several LSH variants having much smaller index sizes have been proposed.","Their query time is linear or superlinear, but they have been shown to run effectively faster because they require fewer I/","Os when the index is stored on hard disk drives and because they also permit in-memory execution with modern DRAM capacity.   ","In this paper, we show that E2LSH is regaining the advantage in query speed with the advent of modern flash storage devices such as solid-state drives (SSDs).","We evaluate E2LSH on a modern single-node computing environment and analyze its computational cost and I/O cost, from which we derive storage performance requirements for its external memory execution.","Our analysis indicates that E2LSH on a single consumer-grade SSD can run faster than the state-of-the-art small-index methods executed in-memory.","It also indicates that E2LSH with emerging high-performance storage devices and interfaces can approach in-memory E2LSH speeds.","We implement a simple adaptation of E2LSH to external memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical large datasets of up to one billion objects using different combinations of modern storage devices and interfaces.","We demonstrate that our E2LSHoS implementation runs much faster than small-index methods and can approach in-memory E2LSH speeds, and also that its query time scales sublinearly with the database size beyond the index size limit of in-memory E2LSH."],"url":"http://arxiv.org/abs/2403.16404v1","category":"cs.PF"}
{"created":"2024-03-24 17:13:46","title":"Adversarially Masked Video Consistency for Unsupervised Domain Adaptation","abstract":"We study the problem of unsupervised domain adaptation for egocentric videos. We propose a transformer-based model to learn class-discriminative and domain-invariant feature representations. It consists of two novel designs. The first module is called Generative Adversarial Domain Alignment Network with the aim of learning domain-invariant representations. It simultaneously learns a mask generator and a domain-invariant encoder in an adversarial way. The domain-invariant encoder is trained to minimize the distance between the source and target domain. The masking generator, conversely, aims at producing challenging masks by maximizing the domain distance. The second is a Masked Consistency Learning module to learn class-discriminative representations. It enforces the prediction consistency between the masked target videos and their full forms. To better evaluate the effectiveness of domain adaptation methods, we construct a more challenging benchmark for egocentric videos, U-Ego4D. Our method achieves state-of-the-art performance on the Epic-Kitchen and the proposed U-Ego4D benchmark.","sentences":["We study the problem of unsupervised domain adaptation for egocentric videos.","We propose a transformer-based model to learn class-discriminative and domain-invariant feature representations.","It consists of two novel designs.","The first module is called Generative Adversarial Domain Alignment Network with the aim of learning domain-invariant representations.","It simultaneously learns a mask generator and a domain-invariant encoder in an adversarial way.","The domain-invariant encoder is trained to minimize the distance between the source and target domain.","The masking generator, conversely, aims at producing challenging masks by maximizing the domain distance.","The second is a Masked Consistency Learning module to learn class-discriminative representations.","It enforces the prediction consistency between the masked target videos and their full forms.","To better evaluate the effectiveness of domain adaptation methods, we construct a more challenging benchmark for egocentric videos, U-Ego4D.","Our method achieves state-of-the-art performance on the Epic-Kitchen and the proposed U-Ego4D benchmark."],"url":"http://arxiv.org/abs/2403.16242v1","category":"cs.CV"}
{"created":"2024-03-24 16:03:27","title":"Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing","abstract":"Deducing the 3D face from a skull is an essential but challenging task in forensic science and archaeology. Existing methods for automated facial reconstruction yield inaccurate results, suffering from the non-determinative nature of the problem that a skull with a sparse set of tissue depth cannot fully determine the skinned face. Additionally, their texture-less results require further post-processing stages to achieve a photo-realistic appearance. This paper proposes an end-to-end 3D face reconstruction and exploration tool, providing textured 3D faces for reference. With the help of state-of-the-art text-to-image diffusion models and image-based facial reconstruction techniques, we generate an initial reference 3D face, whose biological profile aligns with the given skull. We then adapt these initial faces to meet the statistical expectations of extruded anatomical landmarks on the skull through an optimization process. The joint statistical distribution of tissue depths is learned on a small set of anatomical landmarks on the skull. To support further adjustment, we propose an efficient face adaptation tool to assist users in tuning tissue depths, either globally or at local regions, while observing plausible visual feedback. Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability.","sentences":["Deducing the 3D face from a skull is an essential but challenging task in forensic science and archaeology.","Existing methods for automated facial reconstruction yield inaccurate results, suffering from the non-determinative nature of the problem that a skull with a sparse set of tissue depth cannot fully determine the skinned face.","Additionally, their texture-less results require further post-processing stages to achieve a photo-realistic appearance.","This paper proposes an end-to-end 3D face reconstruction and exploration tool, providing textured 3D faces for reference.","With the help of state-of-the-art text-to-image diffusion models and image-based facial reconstruction techniques, we generate an initial reference 3D face, whose biological profile aligns with the given skull.","We then adapt these initial faces to meet the statistical expectations of extruded anatomical landmarks on the skull through an optimization process.","The joint statistical distribution of tissue depths is learned on a small set of anatomical landmarks on the skull.","To support further adjustment, we propose an efficient face adaptation tool to assist users in tuning tissue depths, either globally or at local regions, while observing plausible visual feedback.","Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability."],"url":"http://arxiv.org/abs/2403.16207v1","category":"cs.CV"}
{"created":"2024-03-24 15:10:22","title":"Cross-domain Multi-modal Few-shot Object Detection via Rich Text","abstract":"Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features. However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift. Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD. Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model's language understanding capability. We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods.","sentences":["Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features.","However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient.","We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift.","Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD.","Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model's language understanding capability.","We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods."],"url":"http://arxiv.org/abs/2403.16188v1","category":"cs.CV"}
{"created":"2024-03-24 15:09:55","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models","abstract":"Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.","sentences":["Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models.","Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method.","However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks.","Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process.","First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank.","Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks.","We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters."],"url":"http://arxiv.org/abs/2403.16187v1","category":"cs.CL"}
{"created":"2024-03-24 13:10:09","title":"A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective","abstract":"Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/Pretext.","sentences":["Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models.","There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge.","In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc).","It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies.","Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/Pretext."],"url":"http://arxiv.org/abs/2403.16137v1","category":"cs.LG"}
{"created":"2024-03-24 13:09:46","title":"Data-Driven Sliding Mode Control for Partially Unknown Nonlinear Systems","abstract":"This paper introduces a new design method for data-driven control of nonlinear systems with partially unknown dynamics and unknown bounded disturbance. Since it is not possible to achieve exact nonlinearity cancellation in the presence of unknown disturbance, this paper adapts the idea of sliding mode control (SMC) to ensure system stability and robustness without assuming that the nonlinearity goes to zero faster than the state as in the existing methods. The SMC consists of a data-dependent robust controller ensuring the system state trajectory reach and remain on the sliding surface and a nominal controller solved from a data-dependent semidefinite program (SDP) ensuring robust stability of the state trajectory on the sliding surface. Numerical simulation results demonstrate effectiveness of the proposed data-driven SMC and its superior in terms of robust stability over the existing data-driven control that also uses approximate nonlinearity cancellation.","sentences":["This paper introduces a new design method for data-driven control of nonlinear systems with partially unknown dynamics and unknown bounded disturbance.","Since it is not possible to achieve exact nonlinearity cancellation in the presence of unknown disturbance, this paper adapts the idea of sliding mode control (SMC) to ensure system stability and robustness without assuming that the nonlinearity goes to zero faster than the state as in the existing methods.","The SMC consists of a data-dependent robust controller ensuring the system state trajectory reach and remain on the sliding surface and a nominal controller solved from a data-dependent semidefinite program (SDP) ensuring robust stability of the state trajectory on the sliding surface.","Numerical simulation results demonstrate effectiveness of the proposed data-driven SMC and its superior in terms of robust stability over the existing data-driven control that also uses approximate nonlinearity cancellation."],"url":"http://arxiv.org/abs/2403.16136v1","category":"eess.SY"}
{"created":"2024-03-24 13:03:27","title":"Runtime Monitoring and Fault Detection for Neural Network-Controlled Systems","abstract":"There is an emerging trend in applying deep learning methods to control complex nonlinear systems. This paper considers enhancing the runtime safety of nonlinear systems controlled by neural networks in the presence of disturbance and measurement noise. A robustly stable interval observer is designed to generate sound and precise lower and upper bounds for the neural network, nonlinear function, and system state. The obtained interval is utilised to monitor the real-time system safety and detect faults in the system outputs or actuators. An adaptive cruise control vehicular system is simulated to demonstrate effectiveness of the proposed design.","sentences":["There is an emerging trend in applying deep learning methods to control complex nonlinear systems.","This paper considers enhancing the runtime safety of nonlinear systems controlled by neural networks in the presence of disturbance and measurement noise.","A robustly stable interval observer is designed to generate sound and precise lower and upper bounds for the neural network, nonlinear function, and system state.","The obtained interval is utilised to monitor the real-time system safety and detect faults in the system outputs or actuators.","An adaptive cruise control vehicular system is simulated to demonstrate effectiveness of the proposed design."],"url":"http://arxiv.org/abs/2403.16132v1","category":"eess.SY"}
{"created":"2024-03-24 12:43:04","title":"A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters","abstract":"Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters. However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space. The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism). The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling. This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster. Crius proposes a novel scheduling granularity called Cell. It represents a job with deterministic resources and pipeline stages. The exploration space of Cell is shrunk to the product of only data and tensor parallelism, thus exposing the potential for accurate and low-overhead performance estimation. Crius then accurately estimates Cells and efficiently schedules training jobs. When a Cell is selected as a scheduling choice, its represented job runs with the optimal parallelism plan explored. Experimental results show that Crius reduces job completion time by up to 48.9% and schedules large models with up to 1.49x cluster throughput improvement.","sentences":["Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters.","However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space.","The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism).","The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling.","This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster.","Crius proposes a novel scheduling granularity called Cell.","It represents a job with deterministic resources and pipeline stages.","The exploration space of Cell is shrunk to the product of only data and tensor parallelism, thus exposing the potential for accurate and low-overhead performance estimation.","Crius then accurately estimates Cells and efficiently schedules training jobs.","When a Cell is selected as a scheduling choice, its represented job runs with the optimal parallelism plan explored.","Experimental results show that Crius reduces job completion time by up to 48.9% and schedules large models with up to 1.49x cluster throughput improvement."],"url":"http://arxiv.org/abs/2403.16125v1","category":"cs.DC"}
{"created":"2024-03-24 07:47:00","title":"Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM","abstract":"Financial prediction from Monetary Policy Conference (MPC) calls is a new yet challenging task, which targets at predicting the price movement and volatility for specific financial assets by analyzing multimodal information including text, video, and audio. Although the existing work has achieved great success using cross-modal transformer blocks, it overlooks the potential external financial knowledge, the varying contributions of different modalities to financial prediction, as well as the innate relations among different financial assets. To tackle these limitations, we propose a novel Modal-Adaptive kNowledge-enhAnced Graph-basEd financial pRediction scheme, named MANAGER. Specifically, MANAGER resorts to FinDKG to obtain the external related knowledge for the input text. Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit BERT (HuBERT) to extract the video and audio features, respectively. Thereafter, MANAGER introduces a novel knowledge-enhanced cross-modal graph that fully characterizes the semantic relations among text, external knowledge, video and audio, to adaptively utilize the information in different modalities, with ChatGLM2 as the backbone. Extensive experiments on a publicly available dataset Monopoly verify the superiority of our model over cutting-edge methods.","sentences":["Financial prediction from Monetary Policy Conference (MPC) calls is a new yet challenging task, which targets at predicting the price movement and volatility for specific financial assets by analyzing multimodal information including text, video, and audio.","Although the existing work has achieved great success using cross-modal transformer blocks, it overlooks the potential external financial knowledge, the varying contributions of different modalities to financial prediction, as well as the innate relations among different financial assets.","To tackle these limitations, we propose a novel Modal-Adaptive kNowledge-enhAnced Graph-basEd financial pRediction scheme, named MANAGER.","Specifically, MANAGER resorts to FinDKG to obtain the external related knowledge for the input text.","Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit BERT (HuBERT) to extract the video and audio features, respectively.","Thereafter, MANAGER introduces a novel knowledge-enhanced cross-modal graph that fully characterizes the semantic relations among text, external knowledge, video and audio, to adaptively utilize the information in different modalities, with ChatGLM2 as the backbone.","Extensive experiments on a publicly available dataset Monopoly verify the superiority of our model over cutting-edge methods."],"url":"http://arxiv.org/abs/2403.16055v1","category":"cs.CE"}
{"created":"2024-03-24 07:36:38","title":"Segment Anything Model for Road Network Graph Extraction","abstract":"We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) for extracting large-scale, vectorized road network graphs from satellite imagery. To predict graph geometry, we formulate it as a dense semantic segmentation task, leveraging the inherent strengths of SAM. The image encoder of SAM is fine-tuned to produce probability masks for roads and intersections, from which the graph vertices are extracted via simple non-maximum suppression. To predict graph topology, we designed a lightweight transformer-based graph neural network, which leverages the SAM image embeddings to estimate the edge existence probabilities between vertices. Our approach directly predicts the graph vertices and edges for large regions without expensive and complex post-processing heuristics, and is capable of building complete road network graphs spanning multiple square kilometers in a matter of seconds. With its simple, straightforward, and minimalist design, SAM-Road achieves comparable accuracy with the state-of-the-art method RNGDet++, while being 40 times faster on the City-scale dataset. We thus demonstrate the power of a foundational vision model when applied to a graph learning task. The code is available at https://github.com/htcr/sam_road.","sentences":["We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) for extracting large-scale, vectorized road network graphs from satellite imagery.","To predict graph geometry, we formulate it as a dense semantic segmentation task, leveraging the inherent strengths of SAM.","The image encoder of SAM is fine-tuned to produce probability masks for roads and intersections, from which the graph vertices are extracted via simple non-maximum suppression.","To predict graph topology, we designed a lightweight transformer-based graph neural network, which leverages the SAM image embeddings to estimate the edge existence probabilities between vertices.","Our approach directly predicts the graph vertices and edges for large regions without expensive and complex post-processing heuristics, and is capable of building complete road network graphs spanning multiple square kilometers in a matter of seconds.","With its simple, straightforward, and minimalist design, SAM-Road achieves comparable accuracy with the state-of-the-art method RNGDet++, while being 40 times faster on the City-scale dataset.","We thus demonstrate the power of a foundational vision model when applied to a graph learning task.","The code is available at https://github.com/htcr/sam_road."],"url":"http://arxiv.org/abs/2403.16051v1","category":"cs.CV"}
{"created":"2024-03-24 04:15:50","title":"SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking","abstract":"Multimodal Visual Object Tracking (VOT) has recently gained significant attention due to its robustness. Early research focused on fully fine-tuning RGB-based trackers, which was inefficient and lacked generalized representation due to the scarcity of multimodal data. Therefore, recent studies have utilized prompt tuning to transfer pre-trained RGB-based trackers to multimodal data. However, the modality gap limits pre-trained knowledge recall, and the dominance of the RGB modality persists, preventing the full utilization of information from other modalities. To address these issues, we propose a novel symmetric multimodal tracking framework called SDSTrack. We introduce lightweight adaptation for efficient fine-tuning, which directly transfers the feature extraction ability from RGB to other domains with a small number of trainable parameters and integrates multimodal features in a balanced, symmetric manner. Furthermore, we design a complementary masked patch distillation strategy to enhance the robustness of trackers in complex environments, such as extreme weather, poor imaging, and sensor failure. Extensive experiments demonstrate that SDSTrack outperforms state-of-the-art methods in various multimodal tracking scenarios, including RGB+Depth, RGB+Thermal, and RGB+Event tracking, and exhibits impressive results in extreme conditions. Our source code is available at https://github.com/hoqolo/SDSTrack.","sentences":["Multimodal Visual Object Tracking (VOT) has recently gained significant attention due to its robustness.","Early research focused on fully fine-tuning RGB-based trackers, which was inefficient and lacked generalized representation due to the scarcity of multimodal data.","Therefore, recent studies have utilized prompt tuning to transfer pre-trained RGB-based trackers to multimodal data.","However, the modality gap limits pre-trained knowledge recall, and the dominance of the RGB modality persists, preventing the full utilization of information from other modalities.","To address these issues, we propose a novel symmetric multimodal tracking framework called SDSTrack.","We introduce lightweight adaptation for efficient fine-tuning, which directly transfers the feature extraction ability from RGB to other domains with a small number of trainable parameters and integrates multimodal features in a balanced, symmetric manner.","Furthermore, we design a complementary masked patch distillation strategy to enhance the robustness of trackers in complex environments, such as extreme weather, poor imaging, and sensor failure.","Extensive experiments demonstrate that SDSTrack outperforms state-of-the-art methods in various multimodal tracking scenarios, including RGB+Depth, RGB+Thermal, and RGB+Event tracking, and exhibits impressive results in extreme conditions.","Our source code is available at https://github.com/hoqolo/SDSTrack."],"url":"http://arxiv.org/abs/2403.16002v1","category":"cs.CV"}
{"created":"2024-03-24 01:14:35","title":"Prioritized Multi-Tenant Traffic Engineering for Dynamic QoS Provisioning in Autonomous SDN-OpenFlow Edge Networks","abstract":"This letter indicates the critical need for prioritized multi-tenant quality-of-service (QoS) management by emerging mobile edge systems, particularly for high-throughput beyond fifth-generation networks. Existing traffic engineering tools utilize complex functions baked into closed, proprietary infrastructures, largely limiting design flexibility, scalability, and adaptiveness. Hence, this study introduces a software-defined networking (SDN)-based dynamic QoS provisioning scheme that prioritizes multi-tenant network traffic while focusing on the base station-edge cloud scenario. The designed scheme first separates control and data planes and enables traffic management automation using SDN programmability. It then implements dynamic QoS management via the SDN-OpenFlow protocol, which ensures ample bandwidth for multiple priority flows and efficiently manages the remaining bandwidth for non-priority traffic. Empirical experiments are conducted with a Mininet network emulator and an OpenDayLight controller. Performance evaluation validates the proposed scheme's effectiveness in meeting multi-tenant QoS criteria, offering a robust solution for traffic prioritization in SDN-based edge networks.","sentences":["This letter indicates the critical need for prioritized multi-tenant quality-of-service (QoS) management by emerging mobile edge systems, particularly for high-throughput beyond fifth-generation networks.","Existing traffic engineering tools utilize complex functions baked into closed, proprietary infrastructures, largely limiting design flexibility, scalability, and adaptiveness.","Hence, this study introduces a software-defined networking (SDN)-based dynamic QoS provisioning scheme that prioritizes multi-tenant network traffic while focusing on the base station-edge cloud scenario.","The designed scheme first separates control and data planes and enables traffic management automation using SDN programmability.","It then implements dynamic QoS management via the SDN-OpenFlow protocol, which ensures ample bandwidth for multiple priority flows and efficiently manages the remaining bandwidth for non-priority traffic.","Empirical experiments are conducted with a Mininet network emulator and an OpenDayLight controller.","Performance evaluation validates the proposed scheme's effectiveness in meeting multi-tenant QoS criteria, offering a robust solution for traffic prioritization in SDN-based edge networks."],"url":"http://arxiv.org/abs/2403.15975v1","category":"cs.NI"}
{"created":"2024-03-23 23:36:26","title":"Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction","abstract":"Tasks where robots must cooperate with humans, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human's intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP's ability to predict and adapt to a diverse set of dynamic human intents.","sentences":["Tasks where robots must cooperate with humans, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes.","Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human.","Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect.","In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human's intent cannot be controlled.","RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings.","Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy.","Experiments across a variety of simulated and real-world environments demonstrate RCIP's ability to predict and adapt to a diverse set of dynamic human intents."],"url":"http://arxiv.org/abs/2403.15959v1","category":"cs.RO"}
{"created":"2024-03-23 22:32:06","title":"Deep Domain Adaptation: A Sim2Real Neural Approach for Improving Eye-Tracking Systems","abstract":"Eye image segmentation is a critical step in eye tracking that has great influence over the final gaze estimate. Segmentation models trained using supervised machine learning can excel at this task, their effectiveness is determined by the degree of overlap between the narrow distributions of image properties defined by the target dataset and highly specific training datasets, of which there are few. Attempts to broaden the distribution of existing eye image datasets through the inclusion of synthetic eye images have found that a model trained on synthetic images will often fail to generalize back to real-world eye images. In remedy, we use dimensionality-reduction techniques to measure the overlap between the target eye images and synthetic training data, and to prune the training dataset in a manner that maximizes distribution overlap. We demonstrate that our methods result in robust, improved performance when tackling the discrepancy between simulation and real-world data samples.","sentences":["Eye image segmentation is a critical step in eye tracking that has great influence over the final gaze estimate.","Segmentation models trained using supervised machine learning can excel at this task, their effectiveness is determined by the degree of overlap between the narrow distributions of image properties defined by the target dataset and highly specific training datasets, of which there are few.","Attempts to broaden the distribution of existing eye image datasets through the inclusion of synthetic eye images have found that a model trained on synthetic images will often fail to generalize back to real-world eye images.","In remedy, we use dimensionality-reduction techniques to measure the overlap between the target eye images and synthetic training data, and to prune the training dataset in a manner that maximizes distribution overlap.","We demonstrate that our methods result in robust, improved performance when tackling the discrepancy between simulation and real-world data samples."],"url":"http://arxiv.org/abs/2403.15947v1","category":"cs.CV"}
{"created":"2024-03-23 21:44:21","title":"Delay-Optimal Forwarding and Computation Offloading for Service Chain Tasks","abstract":"Emerging edge computing paradigms enable heterogeneous devices to collaborate on complex computation applications. However, for congestible links and computing units, delay-optimal forwarding and offloading for service chain tasks (e.g., DNN with vertical split) in edge computing networks remains an open problem. In this paper, we formulate the service chain forwarding and offloading problem with arbitrary topology and heterogeneous transmission/computation capability, and aim to minimize the aggregated network cost. We consider congestion-aware nonlinear cost functions that cover various performance metrics and constraints, such as average queueing delay with limited processor capacity. We solve the non-convex optimization problem globally by analyzing the KKT condition and proposing a sufficient condition for optimality. We then propose a distributed algorithm that converges to the global optimum. The algorithm adapts to changes in input rates and network topology, and can be implemented as an online algorithm. Numerical evaluation shows that our method significantly outperforms baselines in multiple network instances, especially in congested scenarios.","sentences":["Emerging edge computing paradigms enable heterogeneous devices to collaborate on complex computation applications.","However, for congestible links and computing units, delay-optimal forwarding and offloading for service chain tasks (e.g., DNN with vertical split) in edge computing networks remains an open problem.","In this paper, we formulate the service chain forwarding and offloading problem with arbitrary topology and heterogeneous transmission/computation capability, and aim to minimize the aggregated network cost.","We consider congestion-aware nonlinear cost functions that cover various performance metrics and constraints, such as average queueing delay with limited processor capacity.","We solve the non-convex optimization problem globally by analyzing the KKT condition and proposing a sufficient condition for optimality.","We then propose a distributed algorithm that converges to the global optimum.","The algorithm adapts to changes in input rates and network topology, and can be implemented as an online algorithm.","Numerical evaluation shows that our method significantly outperforms baselines in multiple network instances, especially in congested scenarios."],"url":"http://arxiv.org/abs/2403.15936v1","category":"cs.NI"}
{"created":"2024-03-23 20:21:46","title":"LOAM: Low-latency Communication, Caching, and Computation Placement in Data-Intensive Computing Networks","abstract":"Deploying data- and computation-intensive applications such as large-scale AI into heterogeneous dispersed computing networks can significantly enhance application performance by mitigating bottlenecks caused by limited network resources, including bandwidth, storage, and computing power. However, current resource allocation methods in dispersed computing do not provide a comprehensive solution that considers arbitrary topology, elastic resource amount, reuse of computation results, and nonlinear congestion-dependent optimization objectives. In this paper, we propose LOAM, a low-latency joint communication, caching, and computation placement framework with a rigorous analytical foundation that incorporates the above aspects. We tackle the NP-hard aggregated cost minimization problem with two methods: an offline method with a 1/2 approximation and an online adaptive method with a bounded gap from the optimum. Through extensive simulation, the proposed framework outperforms multiple baselines in both synthesis and real-world network scenarios.","sentences":["Deploying data- and computation-intensive applications such as large-scale AI into heterogeneous dispersed computing networks can significantly enhance application performance by mitigating bottlenecks caused by limited network resources, including bandwidth, storage, and computing power.","However, current resource allocation methods in dispersed computing do not provide a comprehensive solution that considers arbitrary topology, elastic resource amount, reuse of computation results, and nonlinear congestion-dependent optimization objectives.","In this paper, we propose LOAM, a low-latency joint communication, caching, and computation placement framework with a rigorous analytical foundation that incorporates the above aspects.","We tackle the NP-hard aggregated cost minimization problem with two methods: an offline method with a 1/2 approximation and an online adaptive method with a bounded gap from the optimum.","Through extensive simulation, the proposed framework outperforms multiple baselines in both synthesis and real-world network scenarios."],"url":"http://arxiv.org/abs/2403.15927v1","category":"cs.NI"}
{"created":"2024-03-23 19:31:13","title":"Self-consistent autocorrelation of a disordered Kuramoto model in the asynchronous state","abstract":"The Kuramoto model has provided deep insights into synchronization phenomena and remains an important paradigm to study the dynamics of coupled oscillators. Yet, despite its success, the asynchronous regime in the Kuramoto model has received limited attention. Here, we adapt and enhance the mean-field approach originally proposed by Stiller and Radons [Phys. Rev. E 58 (1998)] to study the asynchronous state in the Kuramoto model with a finite number of oscillators and with disordered connectivity. By employing an iterative stochastic mean field (IMF) approximation, the complex N-oscillator system can effectively be reduced to a one-dimensional dynamics, both for homogeneous and heterogeneous networks. This method allows us to investigate the power spectra of individual oscillators as well as of the multiplicative \"network noise\" in the Kuramoto model in the asynchronous regime. By taking into account the finite system size and disorder in the connectivity, our findings become relevant for the dynamics of coupled oscillators that appear in the context of biological or technical systems.","sentences":["The Kuramoto model has provided deep insights into synchronization phenomena and remains an important paradigm to study the dynamics of coupled oscillators.","Yet, despite its success, the asynchronous regime in the Kuramoto model has received limited attention.","Here, we adapt and enhance the mean-field approach originally proposed by Stiller and Radons [Phys.","Rev. E 58 (1998)] to study the asynchronous state in the Kuramoto model with a finite number of oscillators and with disordered connectivity.","By employing an iterative stochastic mean field (IMF) approximation, the complex N-oscillator system can effectively be reduced to a one-dimensional dynamics, both for homogeneous and heterogeneous networks.","This method allows us to investigate the power spectra of individual oscillators as well as of the multiplicative \"network noise\" in the Kuramoto model in the asynchronous regime.","By taking into account the finite system size and disorder in the connectivity, our findings become relevant for the dynamics of coupled oscillators that appear in the context of biological or technical systems."],"url":"http://arxiv.org/abs/2403.15922v1","category":"math-ph"}
{"created":"2024-03-23 18:19:02","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices","abstract":"The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet model, three datasets, three different training sizes, and a Raspberry Pi. Compared with the $Block Avg$, where each block is fine-tuned individually and their performance improvements are averaged, TBFT exhibits an improvement in model accuracy by an average of 15.30% whilst saving 41.57% energy consumption on average compared with full fine-tuning.","sentences":["The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications.","Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs.","However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios.","We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices.","We categorize data drift and personalization into three types: input-level, feature-level, and output-level.","For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs.","Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of the model.","We evaluate TBFT on a ResNet model, three datasets, three different training sizes, and a Raspberry Pi.","Compared with the $Block Avg$, where each block is fine-tuned individually and their performance improvements are averaged, TBFT exhibits an improvement in model accuracy by an average of 15.30% whilst saving 41.57% energy consumption on average compared with full fine-tuning."],"url":"http://arxiv.org/abs/2403.15905v1","category":"cs.LG"}
{"created":"2024-03-23 17:17:08","title":"Human Motion Prediction under Unexpected Perturbation","abstract":"We investigate a new task in human motion prediction, which is predicting motions under unexpected physical perturbation potentially involving multiple people. Compared with existing research, this task involves predicting less controlled, unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people. It brings new challenges such as data scarcity and predicting complex interactions. To this end, we propose a new method capitalizing differential physics and deep neural networks, leading to an explicit Latent Differential Physics (LDP) model. Through experiments, we demonstrate that LDP has high data efficiency, outstanding prediction accuracy, strong generalizability and good explainability. Since there is no similar research, a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted, showing LDP outperforming existing research both quantitatively and qualitatively, improving prediction accuracy by as much as 70%, and demonstrating significantly stronger generalization.","sentences":["We investigate a new task in human motion prediction, which is predicting motions under unexpected physical perturbation potentially involving multiple people.","Compared with existing research, this task involves predicting less controlled, unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people.","It brings new challenges such as data scarcity and predicting complex interactions.","To this end, we propose a new method capitalizing differential physics and deep neural networks, leading to an explicit Latent Differential Physics (LDP) model.","Through experiments, we demonstrate that LDP has high data efficiency, outstanding prediction accuracy, strong generalizability and good explainability.","Since there is no similar research, a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted, showing LDP outperforming existing research both quantitatively and qualitatively, improving prediction accuracy by as much as 70%, and demonstrating significantly stronger generalization."],"url":"http://arxiv.org/abs/2403.15891v1","category":"cs.CV"}
{"created":"2024-03-23 16:26:49","title":"VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding","abstract":"The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding. To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark. Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language. For the purpose of future research, CafeBERT is made publicly available for research purposes.","sentences":["The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks.","To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark.","The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding.","To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark.","Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark.","Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic knowledge.","CafeBERT is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language.","For the purpose of future research, CafeBERT is made publicly available for research purposes."],"url":"http://arxiv.org/abs/2403.15882v1","category":"cs.CL"}
{"created":"2024-03-23 15:35:05","title":"Evolution beats random chance: Performance-dependent network evolution for enhanced computational capacity","abstract":"The quest to understand structure-function relationships in networks across scientific disciplines has intensified. However, the optimal network architecture remains elusive, particularly for complex information processing. Therefore, we investigate how optimal and specific network structures form to efficiently solve distinct tasks using a novel framework of performance-dependent network evolution, leveraging reservoir computing principles. Our study demonstrates that task-specific minimal network structures obtained through this framework consistently outperform networks generated by alternative growth strategies and Erd\\H{o}s-R\\'enyi random networks. Evolved networks exhibit unexpected sparsity and adhere to scaling laws in node-density space while showcasing a distinctive asymmetry in input and information readout nodes distribution. Consequently, we propose a heuristic for quantifying task complexity from performance-dependently evolved networks, offering valuable insights into the evolutionary dynamics of network structure-function relationships. Our findings not only advance the fundamental understanding of process-specific network evolution but also shed light on the design and optimization of complex information processing mechanisms, notably in machine learning.","sentences":["The quest to understand structure-function relationships in networks across scientific disciplines has intensified.","However, the optimal network architecture remains elusive, particularly for complex information processing.","Therefore, we investigate how optimal and specific network structures form to efficiently solve distinct tasks using a novel framework of performance-dependent network evolution, leveraging reservoir computing principles.","Our study demonstrates that task-specific minimal network structures obtained through this framework consistently outperform networks generated by alternative growth strategies and Erd\\H{o}s-R\\'enyi random networks.","Evolved networks exhibit unexpected sparsity and adhere to scaling laws in node-density space while showcasing a distinctive asymmetry in input and information readout nodes distribution.","Consequently, we propose a heuristic for quantifying task complexity from performance-dependently evolved networks, offering valuable insights into the evolutionary dynamics of network structure-function relationships.","Our findings not only advance the fundamental understanding of process-specific network evolution but also shed light on the design and optimization of complex information processing mechanisms, notably in machine learning."],"url":"http://arxiv.org/abs/2403.15869v1","category":"nlin.AO"}
{"created":"2024-03-23 15:29:52","title":"Existence and multiplicity of solutions for the logarithmic Schr\u00f6dinger equation with a potential on lattice graphs","abstract":"In this paper, we consider the existence and multiplicity of solutions for the logarithmic Schr\\\"{o}dinger equation on lattice graphs $\\mathbb{Z}^N$ $$ -\\Delta u+V(x) u=u \\log u^2, \\quad x \\in \\mathbb{Z}^N, $$ When the potential $V$ is coercive, we obtain infinitely many solutions by adapting some arguments of the Fountain theorem. In the cases of periodic potential, asymptotically periodic potential and bounded potential, we first investigate the existence of ground state solutions via the variation methods, and then we generalize these results from $\\mathbb{Z}^N$ to quasi-transitive graphs. Finally, we extend the main results of the paper to the $p$-Laplacian equation with the logarithmic nonlinearity.","sentences":["In this paper, we consider the existence and multiplicity of solutions for the logarithmic Schr\\\"{o}dinger equation on lattice graphs $\\mathbb{Z}^N$ $$ -\\Delta u+V(x) u=u \\log u^2, \\quad x \\in \\mathbb{Z}^N, $$ When the potential $V$ is coercive, we obtain infinitely many solutions by adapting some arguments of the Fountain theorem.","In the cases of periodic potential, asymptotically periodic potential and bounded potential, we first investigate the existence of ground state solutions via the variation methods, and then we generalize these results from $\\mathbb{Z}^N$ to quasi-transitive graphs.","Finally, we extend the main results of the paper to the $p$-Laplacian equation with the logarithmic nonlinearity."],"url":"http://arxiv.org/abs/2403.15866v1","category":"math.AP"}
{"created":"2024-03-23 15:00:04","title":"Non-monotone dependence modeling with copulas: an application to the volume-return relationship","abstract":"This paper introduces an innovative method for constructing copula models capable of describing arbitrary non-monotone dependence structures. The proposed method enables the creation of such copulas in parametric form, thus allowing the resulting models to adapt to diverse and intricate real-world data patterns. We apply this novel methodology to analyze the relationship between returns and trading volumes in financial markets, a domain where the existence of non-monotone dependencies is well-documented in the existing literature. Our approach exhibits superior adaptability compared to other models which have previously been proposed in the literature, enabling a deeper understanding of the dependence structure among the considered variables.","sentences":["This paper introduces an innovative method for constructing copula models capable of describing arbitrary non-monotone dependence structures.","The proposed method enables the creation of such copulas in parametric form, thus allowing the resulting models to adapt to diverse and intricate real-world data patterns.","We apply this novel methodology to analyze the relationship between returns and trading volumes in financial markets, a domain where the existence of non-monotone dependencies is well-documented in the existing literature.","Our approach exhibits superior adaptability compared to other models which have previously been proposed in the literature, enabling a deeper understanding of the dependence structure among the considered variables."],"url":"http://arxiv.org/abs/2403.15862v1","category":"stat.ME"}
{"created":"2024-03-23 13:50:08","title":"The First High-Contrast Images of Near High-Mass X-Ray Binaries with Keck/NIRC2","abstract":"Although the study of X-ray binaries has led to major breakthroughs in high-energy astrophysics, their circumbinary environment at scales of $\\sim$100--10,000 astronomical units has not been thoroughly investigated. In this paper, we undertake a novel and exploratory study by employing direct and high-contrast imaging techniques on a sample of X-ray binaries, using adaptive optics and the vortex coronagraph on Keck/NIRC2. High-contrast imaging opens up the possibility to search for exoplanets, brown dwarfs, circumbinary companion stars, and protoplanetary disks in these extreme systems. Here, we present the first near-infrared high-contrast images of 13 high-mass X-ray binaries located within $\\sim$2--3 kpc. The key results of this campaign involve the discovery of several candidate circumbinary companions ranging from sub-stellar (brown dwarf) to stellar masses. By conducting an analysis based on galactic population models, we discriminate sources that are likely background/foreground stars and isolate those that have a high probability ($\\gtrsim 60 - 99\\%$) of being gravitationally bound to the X-ray binary. This publication seeks to establish a preliminary catalog for future analyses of proper motion and subsequent observations. With our preliminary results, we calculate the first estimate of the companion frequency and the multiplicity frequency for X-ray binaries: $\\approx$0.6 and 1.8 $\\pm$ 0.9 respectively, considering only the sources that are most likely bound to the X-ray binary. In addition to extending our comprehension of how brown dwarfs and stars can form and survive in such extreme systems, our study opens a new window to our understanding of the formation of X-ray binaries.","sentences":["Although the study of X-ray binaries has led to major breakthroughs in high-energy astrophysics, their circumbinary environment at scales of $\\sim$100--10,000 astronomical units has not been thoroughly investigated.","In this paper, we undertake a novel and exploratory study by employing direct and high-contrast imaging techniques on a sample of X-ray binaries, using adaptive optics and the vortex coronagraph on Keck/NIRC2.","High-contrast imaging opens up the possibility to search for exoplanets, brown dwarfs, circumbinary companion stars, and protoplanetary disks in these extreme systems.","Here, we present the first near-infrared high-contrast images of 13 high-mass X-ray binaries located within $\\sim$2--3 kpc.","The key results of this campaign involve the discovery of several candidate circumbinary companions ranging from sub-stellar (brown dwarf) to stellar masses.","By conducting an analysis based on galactic population models, we discriminate sources that are likely background/foreground stars and isolate those that have a high probability ($\\gtrsim 60 - 99\\%$) of being gravitationally bound to the X-ray binary.","This publication seeks to establish a preliminary catalog for future analyses of proper motion and subsequent observations.","With our preliminary results, we calculate the first estimate of the companion frequency and the multiplicity frequency for X-ray binaries: $\\approx$0.6 and 1.8 $\\pm$ 0.9 respectively, considering only the sources that are most likely bound to the X-ray binary.","In addition to extending our comprehension of how brown dwarfs and stars can form and survive in such extreme systems, our study opens a new window to our understanding of the formation of X-ray binaries."],"url":"http://arxiv.org/abs/2403.15845v1","category":"astro-ph.SR"}
{"created":"2024-03-23 13:22:36","title":"Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression","abstract":"Recent Vision Transformer Compression (VTC) works mainly follow a two-stage scheme, where the importance score of each model unit is first evaluated or preset in each submodule, followed by the sparsity score evaluation according to the target sparsity constraint. Such a separate evaluation process induces the gap between importance and sparsity score distributions, thus causing high search costs for VTC. In this work, for the first time, we investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner. Specifically, we present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for VTC. First, a bi-mask scheme is developed by entangling the importance score and the differentiable sparsity score to jointly determine the pruning potential (prunability) of each unit. Such a bi-mask search strategy is further used together with a proposed adaptive one-hot loss to realize the progressive-and-efficient search for the most important subnet. Finally, Progressive Masked Image Modeling (PMIM) is proposed to regularize the feature space to be more representative during the search process, which may be degraded by the dimension reduction. Extensive experiments demonstrate that OFB can achieve superior compression performance over state-of-the-art searching-based and pruning-based methods under various Vision Transformer architectures, meanwhile promoting search efficiency significantly, e.g., costing one GPU search day for the compression of DeiT-S on ImageNet-1K.","sentences":["Recent Vision Transformer Compression (VTC) works mainly follow a two-stage scheme, where the importance score of each model unit is first evaluated or preset in each submodule, followed by the sparsity score evaluation according to the target sparsity constraint.","Such a separate evaluation process induces the gap between importance and sparsity score distributions, thus causing high search costs for VTC.","In this work, for the first time, we investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner.","Specifically, we present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for VTC.","First, a bi-mask scheme is developed by entangling the importance score and the differentiable sparsity score to jointly determine the pruning potential (prunability) of each unit.","Such a bi-mask search strategy is further used together with a proposed adaptive one-hot loss to realize the progressive-and-efficient search for the most important subnet.","Finally, Progressive Masked Image Modeling (PMIM) is proposed to regularize the feature space to be more representative during the search process, which may be degraded by the dimension reduction.","Extensive experiments demonstrate that OFB can achieve superior compression performance over state-of-the-art searching-based and pruning-based methods under various Vision Transformer architectures, meanwhile promoting search efficiency significantly, e.g., costing one GPU search day for the compression of DeiT-S on ImageNet-1K."],"url":"http://arxiv.org/abs/2403.15835v1","category":"cs.CV"}
{"created":"2024-03-23 12:09:16","title":"Resource-efficient Parallel Split Learning in Heterogeneous Edge Computing","abstract":"Edge AI has been recently proposed to facilitate the training and deployment of Deep Neural Network (DNN) models in proximity to the sources of data. To enable the training of large models on resource-constraint edge devices and protect data privacy, parallel split learning is becoming a practical and popular approach. However, current parallel split learning neglects the resource heterogeneity of edge devices, which may lead to the straggler issue. In this paper, we propose EdgeSplit, a novel parallel split learning framework to better accelerate distributed model training on heterogeneous and resource-constraint edge devices. EdgeSplit enhances the efficiency of model training on less powerful edge devices by adaptively segmenting the model into varying depths. Our approach focuses on reducing total training time by formulating and solving a task scheduling problem, which determines the most efficient model partition points and bandwidth allocation for each device. We employ a straightforward yet effective alternating algorithm for this purpose. Comprehensive tests conducted with a range of DNN models and datasets demonstrate that EdgeSplit not only facilitates the training of large models on resource-restricted edge devices but also surpasses existing baselines in performance.","sentences":["Edge AI has been recently proposed to facilitate the training and deployment of Deep Neural Network (DNN) models in proximity to the sources of data.","To enable the training of large models on resource-constraint edge devices and protect data privacy, parallel split learning is becoming a practical and popular approach.","However, current parallel split learning neglects the resource heterogeneity of edge devices, which may lead to the straggler issue.","In this paper, we propose EdgeSplit, a novel parallel split learning framework to better accelerate distributed model training on heterogeneous and resource-constraint edge devices.","EdgeSplit enhances the efficiency of model training on less powerful edge devices by adaptively segmenting the model into varying depths.","Our approach focuses on reducing total training time by formulating and solving a task scheduling problem, which determines the most efficient model partition points and bandwidth allocation for each device.","We employ a straightforward yet effective alternating algorithm for this purpose.","Comprehensive tests conducted with a range of DNN models and datasets demonstrate that EdgeSplit not only facilitates the training of large models on resource-restricted edge devices but also surpasses existing baselines in performance."],"url":"http://arxiv.org/abs/2403.15815v1","category":"cs.DC"}
{"created":"2024-03-23 11:14:02","title":"MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training","abstract":"In medical information extraction, medical Named Entity Recognition (NER) is indispensable, playing a crucial role in developing medical knowledge graphs, enhancing medical question-answering systems, and analyzing electronic medical records. The challenge in medical NER arises from the complex nested structures and sophisticated medical terminologies, distinguishing it from its counterparts in traditional domains. In response to these complexities, we propose a medical NER model based on Machine Reading Comprehension (MRC), which uses a task-adaptive pre-training strategy to improve the model's capability in the medical field. Meanwhile, our model introduces multiple word-pair embeddings and multi-granularity dilated convolution to enhance the model's representation ability and uses a combined predictor of Biaffine and MLP to improve the model's recognition performance. Experimental evaluations conducted on the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our proposed model outperforms the compared state-of-the-art (SOTA) models.","sentences":["In medical information extraction, medical Named Entity Recognition (NER) is indispensable, playing a crucial role in developing medical knowledge graphs, enhancing medical question-answering systems, and analyzing electronic medical records.","The challenge in medical NER arises from the complex nested structures and sophisticated medical terminologies, distinguishing it from its counterparts in traditional domains.","In response to these complexities, we propose a medical NER model based on Machine Reading Comprehension (MRC), which uses a task-adaptive pre-training strategy to improve the model's capability in the medical field.","Meanwhile, our model introduces multiple word-pair embeddings and multi-granularity dilated convolution to enhance the model's representation ability and uses a combined predictor of Biaffine and MLP to improve the model's recognition performance.","Experimental evaluations conducted on the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our proposed model outperforms the compared state-of-the-art (SOTA) models."],"url":"http://arxiv.org/abs/2403.15800v1","category":"cs.CL"}
{"created":"2024-03-23 08:26:04","title":"Phase estimation via coherent and photon-catalyzed squeezed vacuum states","abstract":"The research focused on enhancing the measurement accuracy through the use of non-Gaussian states has garnered increasing attention. In this study, we propose a scheme to input the coherent state mixed with photon-catalyzed squeezed vacuum state into the Mach-Zender interferometer to enhance phase measurement accuracy. The findings demonstrate that photon catalysis, particularly multi-photon catalysis, can effectively improve the phase sensitivity of parity detection and the quantum Fisher information. Moreover, the situation of photon losses in practical measurement was studied. The results indicate that external dissipation has a greater influence on phase sensitivity than the internal dissipation. Compared to input coherent state mixed with squeezed vacuum state, the utilization of coherent state mixed photon-catalyzed squeezed vacuum state, particularly the mixed multi-photon catalyzed squeezed vacuum state as input, can enhance the phase sensitivity and quantum Fisher information. Furthermore, the phase measurement accuracy can exceed the standard quantum limit, and even surpass the Heisenberg limit. This research is expected to significantly contribute to quantum precision measurement.","sentences":["The research focused on enhancing the measurement accuracy through the use of non-Gaussian states has garnered increasing attention.","In this study, we propose a scheme to input the coherent state mixed with photon-catalyzed squeezed vacuum state into the Mach-Zender interferometer to enhance phase measurement accuracy.","The findings demonstrate that photon catalysis, particularly multi-photon catalysis, can effectively improve the phase sensitivity of parity detection and the quantum Fisher information.","Moreover, the situation of photon losses in practical measurement was studied.","The results indicate that external dissipation has a greater influence on phase sensitivity than the internal dissipation.","Compared to input coherent state mixed with squeezed vacuum state, the utilization of coherent state mixed photon-catalyzed squeezed vacuum state, particularly the mixed multi-photon catalyzed squeezed vacuum state as input, can enhance the phase sensitivity and quantum Fisher information.","Furthermore, the phase measurement accuracy can exceed the standard quantum limit, and even surpass the Heisenberg limit.","This research is expected to significantly contribute to quantum precision measurement."],"url":"http://arxiv.org/abs/2403.15761v1","category":"quant-ph"}
{"created":"2024-03-25 11:20:54","title":"Nonequilibrium relaxation inequality on short timescales","abstract":"An integral relation is derived from the Fokker-Planck equation which connects the steady-state probability currents with the dynamics of relaxation on short timescales in the limit of small perturbation fields. As a consequence of this integral relation, a lower bound on the steady-state entropy production is obtained. For the particular case of an ensemble of random perturbation fields of weak spatial gradient, a simpler bound is derived from the integral relation which provides a feasible method to estimate entropy production from relaxation experiments.","sentences":["An integral relation is derived from the Fokker-Planck equation which connects the steady-state probability currents with the dynamics of relaxation on short timescales in the limit of small perturbation fields.","As a consequence of this integral relation, a lower bound on the steady-state entropy production is obtained.","For the particular case of an ensemble of random perturbation fields of weak spatial gradient, a simpler bound is derived from the integral relation which provides a feasible method to estimate entropy production from relaxation experiments."],"url":"http://arxiv.org/abs/2403.16631v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-25 10:41:32","title":"Investigating the impact of the modeling of Earth structure on the neutrino propagation at ultra-high-energies","abstract":"The flux of atmospheric and astrophysical neutrinos measured in UHE neutrino detectors is strongly dependent on the description of the propagation and absorption of the neutrinos during the passage through Earth to the detector. In particular, the attenuation of the incident neutrino flux depends on the details of the matter structure between the source and the detector. In this paper, we will investigate the impact of different descriptions for the density profile of Earth on the transmission coefficient, defined as the ratio between the flux measured in the detector and the incoming neutrino flux. We will consider five different models for the Earth's density profile and estimate how these different models modify the target column density and transmission coefficients for different flavors. The results are derived by solving the cascade equations taking into account the NC interactions and tau regeneration. A comparison with approximated solutions is also presented. Our results indicated that the predictions are sensitive to the model considered for the density profile, with the simplified three layer model providing a satisfactory description when compared with the PREM results.","sentences":["The flux of atmospheric and astrophysical neutrinos measured in UHE neutrino detectors is strongly dependent on the description of the propagation and absorption of the neutrinos during the passage through Earth to the detector.","In particular, the attenuation of the incident neutrino flux depends on the details of the matter structure between the source and the detector.","In this paper, we will investigate the impact of different descriptions for the density profile of Earth on the transmission coefficient, defined as the ratio between the flux measured in the detector and the incoming neutrino flux.","We will consider five different models for the Earth's density profile and estimate how these different models modify the target column density and transmission coefficients for different flavors.","The results are derived by solving the cascade equations taking into account the NC interactions and tau regeneration.","A comparison with approximated solutions is also presented.","Our results indicated that the predictions are sensitive to the model considered for the density profile, with the simplified three layer model providing a satisfactory description when compared with the PREM results."],"url":"http://arxiv.org/abs/2403.16611v1","category":"hep-ph"}
{"created":"2024-03-25 10:21:28","title":"Continuous primitives for higher degree differential forms in Euclidean spaces, Heisenberg groups and applications","abstract":"It is shown that higher degree exact differential forms on compact Riemannian $n$-manifolds possess continuous primitives whose uniform norm is controlled by their $L^n$ norm. A contact sub-Riemannian analogue is proven, with differential forms replaced with Rumin differential forms.","sentences":["It is shown that higher degree exact differential forms on compact Riemannian $n$-manifolds possess continuous primitives whose uniform norm is controlled by their $L^n$ norm.","A contact sub-Riemannian analogue is proven, with differential forms replaced with Rumin differential forms."],"url":"http://arxiv.org/abs/2403.16602v1","category":"math.AP"}
{"created":"2024-03-25 08:00:10","title":"Blow Up of Compact Mean Curvature Flow Solutions with Bounded Mean Curvature","abstract":"In 1994, Vel\\'{a}zquez constructed a countable family of complete hypersurfaces flowing in $\\mathbb{R}^{2N}$ $(N\\geq 4)$ by mean curvature, each of which develops a type II singularity at the origin in finite time. Later Guo and Sesum showed that for a non-empty subset of Vel\\'{a}zquez's solutions, the mean curvature blows up near the origin, at a rate smaller than that of the second fundamental form; recently Stolarski proved another subset of these solutions has bounded mean curvature up to the singular time. In this paper, we follow their arguments to construct compact mean curvature flow solutions in $\\mathbb{R}^n$ $(n\\geq 8)$ with bounded mean curvature.","sentences":["In 1994, Vel\\'{a}zquez constructed a countable family of complete hypersurfaces flowing in $\\mathbb{R}^{2N}$ $(N\\geq 4)$ by mean curvature, each of which develops a type II singularity at the origin in finite time.","Later Guo and Sesum showed that for a non-empty subset of Vel\\'{a}zquez's solutions, the mean curvature blows up near the origin, at a rate smaller than that of the second fundamental form; recently Stolarski proved another subset of these solutions has bounded mean curvature up to the singular time.","In this paper, we follow their arguments to construct compact mean curvature flow solutions in $\\mathbb{R}^n$ $(n\\geq 8)$ with bounded mean curvature."],"url":"http://arxiv.org/abs/2403.16515v1","category":"math.DG"}
{"created":"2024-03-25 07:07:50","title":"REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices","abstract":"This work tackles the challenging task of achieving real-time novel view synthesis on various scenes, including highly reflective objects and unbounded outdoor scenes. Existing real-time rendering methods, especially those based on meshes, often have subpar performance in modeling surfaces with rich view-dependent appearances. Our key idea lies in leveraging meshes for rendering acceleration while incorporating a novel approach to parameterize view-dependent information. We decompose the color into diffuse and specular, and model the specular color in the reflected direction based on a neural environment map. Our experiments demonstrate that our method achieves comparable reconstruction quality for highly reflective surfaces compared to state-of-the-art offline methods, while also efficiently enabling real-time rendering on edge devices such as smartphones.","sentences":["This work tackles the challenging task of achieving real-time novel view synthesis on various scenes, including highly reflective objects and unbounded outdoor scenes.","Existing real-time rendering methods, especially those based on meshes, often have subpar performance in modeling surfaces with rich view-dependent appearances.","Our key idea lies in leveraging meshes for rendering acceleration while incorporating a novel approach to parameterize view-dependent information.","We decompose the color into diffuse and specular, and model the specular color in the reflected direction based on a neural environment map.","Our experiments demonstrate that our method achieves comparable reconstruction quality for highly reflective surfaces compared to state-of-the-art offline methods, while also efficiently enabling real-time rendering on edge devices such as smartphones."],"url":"http://arxiv.org/abs/2403.16481v1","category":"cs.CV"}
{"created":"2024-03-25 06:42:02","title":"On the rates of convergence for learning with convolutional neural networks","abstract":"We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.","sentences":["We study the approximation and learning capacities of convolutional neural networks (CNNs).","Our first result proves a new approximation bound for CNNs with certain constraint on the weights.","Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases.","The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations.","Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems.","In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting.","For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss.","It is also shown that the obtained rates are minimax optimal in several settings."],"url":"http://arxiv.org/abs/2403.16459v1","category":"cs.LG"}
{"created":"2024-03-25 05:30:59","title":"The classification of complete improper affine spheres with singularities of low total curvature and new examples","abstract":"We classify complete improper affine spheres with singularities (say improper affine fronts) in unimodular affine three-space $\\boldsymbol{R}^3$ whose total curvature is greater than or equal to $-8\\pi$. We also study the asymptotic behavior of complete embedded ends of improper affine fronts. Moreover, we give new examples for this class of surfaces, including one which satisfies the equality condition of an Osserman-type inequality and is of positive genus.","sentences":["We classify complete improper affine spheres with singularities (say improper affine fronts) in unimodular affine three-space $\\boldsymbol{R}^3$ whose total curvature is greater than or equal to $-8\\pi$. We also study the asymptotic behavior of complete embedded ends of improper affine fronts.","Moreover, we give new examples for this class of surfaces, including one which satisfies the equality condition of an Osserman-type inequality and is of positive genus."],"url":"http://arxiv.org/abs/2403.16434v1","category":"math.DG"}
{"created":"2024-03-25 03:44:36","title":"Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models","abstract":"The integration of an ensemble of deep learning models has been extensively explored to enhance defense against adversarial attacks. The diversity among sub-models increases the attack cost required to deceive the majority of the ensemble, thereby improving the adversarial robustness. While existing approaches mainly center on increasing diversity in feature representations or dispersion of first-order gradients with respect to input, the limited correlation between these diversity metrics and adversarial robustness constrains the performance of ensemble adversarial defense. In this work, we aim to enhance ensemble diversity by reducing attack transferability. We identify second-order gradients, which depict the loss curvature, as a key factor in adversarial robustness. Computing the Hessian matrix involved in second-order gradients is computationally expensive. To address this, we approximate the Hessian-vector product using differential approximation. Given that low curvature provides better robustness, our ensemble model was designed to consider the influence of curvature among different sub-models. We introduce a novel regularizer to train multiple more-diverse low-curvature network models. Extensive experiments across various datasets demonstrate that our ensemble model exhibits superior robustness against a range of attacks, underscoring the effectiveness of our approach.","sentences":["The integration of an ensemble of deep learning models has been extensively explored to enhance defense against adversarial attacks.","The diversity among sub-models increases the attack cost required to deceive the majority of the ensemble, thereby improving the adversarial robustness.","While existing approaches mainly center on increasing diversity in feature representations or dispersion of first-order gradients with respect to input, the limited correlation between these diversity metrics and adversarial robustness constrains the performance of ensemble adversarial defense.","In this work, we aim to enhance ensemble diversity by reducing attack transferability.","We identify second-order gradients, which depict the loss curvature, as a key factor in adversarial robustness.","Computing the Hessian matrix involved in second-order gradients is computationally expensive.","To address this, we approximate the Hessian-vector product using differential approximation.","Given that low curvature provides better robustness, our ensemble model was designed to consider the influence of curvature among different sub-models.","We introduce a novel regularizer to train multiple more-diverse low-curvature network models.","Extensive experiments across various datasets demonstrate that our ensemble model exhibits superior robustness against a range of attacks, underscoring the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.16405v1","category":"cs.LG"}
{"created":"2024-03-25 01:54:57","title":"RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable and Circular Convolutions","abstract":"Four-dimensional cone-beam computed tomography (4D CBCT) provides respiration-resolved images and can be used for image-guided radiation therapy. However, the ability to reveal respiratory motion comes at the cost of image artifacts. As raw projection data are sorted into multiple respiratory phases, there is a limited number of cone-beam projections available for image reconstruction. Consequently, the 4D CBCT images are covered by severe streak artifacts. Although several deep learning-based methods have been proposed to address this issue, most algorithms employ ordinary network models, neglecting the intrinsic structural prior within 4D CBCT images. In this paper, we first explore the origin and appearance of streak artifacts in 4D CBCT images.Specifically, we find that streak artifacts exhibit a periodic rotational motion along with the patient's respiration. This unique motion pattern inspires us to distinguish the artifacts from the desired anatomical structures in the spatiotemporal domain. Thereafter, we propose a spatiotemporal neural network named RSTAR-Net with separable and circular convolutions for Rotational Streak Artifact Reduction. The specially designed model effectively encodes dynamic image features, facilitating the recovery of 4D CBCT images. Moreover, RSTAR-Net is also lightweight and computationally efficient. Extensive experiments substantiate the effectiveness of our proposed method, and RSTAR-Net shows superior performance to comparison methods.","sentences":["Four-dimensional cone-beam computed tomography (4D CBCT) provides respiration-resolved images and can be used for image-guided radiation therapy.","However, the ability to reveal respiratory motion comes at the cost of image artifacts.","As raw projection data are sorted into multiple respiratory phases, there is a limited number of cone-beam projections available for image reconstruction.","Consequently, the 4D CBCT images are covered by severe streak artifacts.","Although several deep learning-based methods have been proposed to address this issue, most algorithms employ ordinary network models, neglecting the intrinsic structural prior within 4D CBCT images.","In this paper, we first explore the origin and appearance of streak artifacts in 4D CBCT images.","Specifically, we find that streak artifacts exhibit a periodic rotational motion along with the patient's respiration.","This unique motion pattern inspires us to distinguish the artifacts from the desired anatomical structures in the spatiotemporal domain.","Thereafter, we propose a spatiotemporal neural network named RSTAR-Net with separable and circular convolutions for Rotational Streak Artifact Reduction.","The specially designed model effectively encodes dynamic image features, facilitating the recovery of 4D CBCT images.","Moreover, RSTAR-Net is also lightweight and computationally efficient.","Extensive experiments substantiate the effectiveness of our proposed method, and RSTAR-Net shows superior performance to comparison methods."],"url":"http://arxiv.org/abs/2403.16361v1","category":"eess.IV"}
{"created":"2024-03-24 16:48:44","title":"Accretion tori around rotating neutron stars -- Paper II: Oscillations and precessions","abstract":"The four characteristic oscillation frequencies of accretion flows are, in addition to the Keplerian orbital frequency, often discussed in the context of the time variability of the black hole and neutron star (NS) low-mass X-ray binaries (LMXBs). These are namely the frequencies of the axisymmetric radial and vertical epicyclic oscillations, and the frequencies of non-axisymmetric oscillations corresponding to the periastron (radial) and Lense-Thirring (vertical) precessions. In this context, we investigate the effect of the quadrupole moment of a slowly rotating NS and provide complete formulae for calculating these oscillation and precession frequencies, as well as their convenient approximations. Simple formulae corresponding to the geodesic limit of a slender torus (and test particle motion) and the limit of a marginally overflowing torus (torus exhibiting a critical cusp) are presented, and furthermore, more general approximate formulae are included to allow calculations for arbitrarily thick tori. We provide the Wolfram Mathematica code used for our calculations together with C++ and PYTHON codes for calculations of the frequencies. Our formulae can be used for various calculations describing the astrophysical signatures of the NSs' superdense matter equation of state. For instance, we demonstrate that, even for a given fixed number of free parameters, a model accounting for fluid flow precession better matches the frequencies of twin-peak quasiperiodic oscillations observed in NS LMXBs than a model using geodesic precession.","sentences":["The four characteristic oscillation frequencies of accretion flows are, in addition to the Keplerian orbital frequency, often discussed in the context of the time variability of the black hole and neutron star (NS) low-mass X-ray binaries (LMXBs).","These are namely the frequencies of the axisymmetric radial and vertical epicyclic oscillations, and the frequencies of non-axisymmetric oscillations corresponding to the periastron (radial) and Lense-Thirring (vertical) precessions.","In this context, we investigate the effect of the quadrupole moment of a slowly rotating NS and provide complete formulae for calculating these oscillation and precession frequencies, as well as their convenient approximations.","Simple formulae corresponding to the geodesic limit of a slender torus (and test particle motion) and the limit of a marginally overflowing torus (torus exhibiting a critical cusp) are presented, and furthermore, more general approximate formulae are included to allow calculations for arbitrarily thick tori.","We provide the Wolfram Mathematica code used for our calculations together with C++ and PYTHON codes for calculations of the frequencies.","Our formulae can be used for various calculations describing the astrophysical signatures of the NSs' superdense matter equation of state.","For instance, we demonstrate that, even for a given fixed number of free parameters, a model accounting for fluid flow precession better matches the frequencies of twin-peak quasiperiodic oscillations observed in NS LMXBs than a model using geodesic precession."],"url":"http://arxiv.org/abs/2403.16231v1","category":"astro-ph.HE"}
{"created":"2024-03-24 16:47:54","title":"Non-Inertial Rigid Frames and the Principle of Equivalence","abstract":"One version of the principle of equivalence, as originally formulated by Einstein, states that ``gravity\" can be mimicked locally by going to an ``accelerated frame of reference\". As highlighted by Synge, the physical content of this principle remains obscure in so far as it does not refer to the Riemann tensor $R_{abcd}$, which encodes the true effects of gravity. We here give the acceleration profile of a $Born$ rigid, Rindler$esque$, frame that can mimic a gravitational field corresponding to a given $R_{abcd}$. The generalised deviation equation that yields this result also has Centrifugal and Coriolis terms appearing in a purely relational context, yielding a similar connection between angular velocity of rotating, rigid inertial frames and the Riemann tensor. We comment briefly on implications for Mach principle.","sentences":["One version of the principle of equivalence, as originally formulated by Einstein, states that ``gravity\" can be mimicked locally by going to an ``accelerated frame of reference\".","As highlighted by Synge, the physical content of this principle remains obscure in so far as it does not refer to the Riemann tensor $R_{abcd}$, which encodes the true effects of gravity.","We here give the acceleration profile of a $Born$ rigid, Rindler$esque$, frame that can mimic a gravitational field corresponding to a given $R_{abcd}$. The generalised deviation equation that yields this result also has Centrifugal and Coriolis terms appearing in a purely relational context, yielding a similar connection between angular velocity of rotating, rigid inertial frames and the Riemann tensor.","We comment briefly on implications for Mach principle."],"url":"http://arxiv.org/abs/2403.16229v1","category":"gr-qc"}
{"created":"2024-03-24 16:37:12","title":"Accretion tori around rotating neutron stars -- Paper I: Structure, shape and size","abstract":"We present a full general relativistic analytic solution for a radiation-pressure supported equilibrium fluid torus orbiting a rotating neutron star (NS). Previously developed analytical methods are thoroughly applied in the Hartle-Thorne geometry, including the effects of both the NS's angular momentum and quadrupole moment. The structure, size and shape of the torus are explored, focusing especially on the critically thick solution - the cusp tori. For the astrophysically relevant range of NS parameters, we examine how our findings differ from those obtained for the Schwarzschild spacetime. The solutions for rotating stars display signatures of the interplay between relativistic and Newtonian effects where the impact of NS angular momentum and quadrupole moment are almost counterbalanced at the given radius. Nevertheless, the spacetime parameters still strongly influence the size of tori, which can be shown in a coordinate-independent way. Finally, we discuss the importance of the size of the central neutron star, determining whether or not the surrounding torus may exist. We provide a set of tools in a Wolfram Mathematica code, which poses a basis allowing for a further investigation of the impact of the NSs' superdense matter equation of state on the spectral and temporal behaviour of accretion tori.","sentences":["We present a full general relativistic analytic solution for a radiation-pressure supported equilibrium fluid torus orbiting a rotating neutron star (NS).","Previously developed analytical methods are thoroughly applied in the Hartle-Thorne geometry, including the effects of both the NS's angular momentum and quadrupole moment.","The structure, size and shape of the torus are explored, focusing especially on the critically thick solution - the cusp tori.","For the astrophysically relevant range of NS parameters, we examine how our findings differ from those obtained for the Schwarzschild spacetime.","The solutions for rotating stars display signatures of the interplay between relativistic and Newtonian effects where the impact of NS angular momentum and quadrupole moment are almost counterbalanced at the given radius.","Nevertheless, the spacetime parameters still strongly influence the size of tori, which can be shown in a coordinate-independent way.","Finally, we discuss the importance of the size of the central neutron star, determining whether or not the surrounding torus may exist.","We provide a set of tools in a Wolfram Mathematica code, which poses a basis allowing for a further investigation of the impact of the NSs' superdense matter equation of state on the spectral and temporal behaviour of accretion tori."],"url":"http://arxiv.org/abs/2403.16226v1","category":"astro-ph.HE"}
{"created":"2024-03-24 16:05:57","title":"Convergence analysis of OT-Flow for sample generation","abstract":"Deep generative models aim to learn the underlying distribution of data and generate new ones. Despite the diversity of generative models and their high-quality generation performance in practice, most of them lack rigorous theoretical convergence proofs. In this work, we aim to establish some convergence results for OT-Flow, one of the deep generative models. First, by reformulating the framework of OT-Flow model, we establish the $\\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal transport (OT) problem as the regularization term parameter $\\alpha$ goes to infinity. Second, since the loss function will be approximated by Monte Carlo method in training, we established the convergence between the discrete loss function and the continuous one when the sample number $N$ goes to infinity as well. Meanwhile, the approximation capability of the neural network provides an upper bound for the discrete loss function of the minimizers. The proofs in both aspects provide convincing assurances for OT-Flow.","sentences":["Deep generative models aim to learn the underlying distribution of data and generate new ones.","Despite the diversity of generative models and their high-quality generation performance in practice, most of them lack rigorous theoretical convergence proofs.","In this work, we aim to establish some convergence results for OT-Flow, one of the deep generative models.","First, by reformulating the framework of OT-Flow model, we establish the $\\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal transport (OT) problem as the regularization term parameter $\\alpha$ goes to infinity.","Second, since the loss function will be approximated by Monte Carlo method in training, we established the convergence between the discrete loss function and the continuous one when the sample number $N$ goes to infinity as well.","Meanwhile, the approximation capability of the neural network provides an upper bound for the discrete loss function of the minimizers.","The proofs in both aspects provide convincing assurances for OT-Flow."],"url":"http://arxiv.org/abs/2403.16208v1","category":"math.NA"}
{"created":"2024-03-24 15:48:29","title":"From Discrete to Continuous: Deep Fair Clustering With Transferable Representations","abstract":"We consider the problem of deep fair clustering, which partitions data into clusters via the representations extracted by deep neural networks while hiding sensitive data attributes. To achieve fairness, existing methods present a variety of fairness-related objective functions based on the group fairness criterion. However, these works typically assume that the sensitive attributes are discrete and do not work for continuous sensitive variables, such as the proportion of the female population in an area. Besides, the potential of the representations learned from clustering tasks to improve performance on other tasks is ignored by existing works. In light of these limitations, we propose a flexible deep fair clustering method that can handle discrete and continuous sensitive attributes simultaneously. Specifically, we design an information bottleneck style objective function to learn fair and clustering-friendly representations. Furthermore, we explore for the first time the transferability of the extracted representations to other downstream tasks. Unlike existing works, we impose fairness at the representation level, which could guarantee fairness for the transferred task regardless of clustering results. To verify the effectiveness of the proposed method, we perform extensive experiments on datasets with discrete and continuous sensitive attributes, demonstrating the advantage of our method in comparison with state-of-the-art methods.","sentences":["We consider the problem of deep fair clustering, which partitions data into clusters via the representations extracted by deep neural networks while hiding sensitive data attributes.","To achieve fairness, existing methods present a variety of fairness-related objective functions based on the group fairness criterion.","However, these works typically assume that the sensitive attributes are discrete and do not work for continuous sensitive variables, such as the proportion of the female population in an area.","Besides, the potential of the representations learned from clustering tasks to improve performance on other tasks is ignored by existing works.","In light of these limitations, we propose a flexible deep fair clustering method that can handle discrete and continuous sensitive attributes simultaneously.","Specifically, we design an information bottleneck style objective function to learn fair and clustering-friendly representations.","Furthermore, we explore for the first time the transferability of the extracted representations to other downstream tasks.","Unlike existing works, we impose fairness at the representation level, which could guarantee fairness for the transferred task regardless of clustering results.","To verify the effectiveness of the proposed method, we perform extensive experiments on datasets with discrete and continuous sensitive attributes, demonstrating the advantage of our method in comparison with state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16201v1","category":"cs.LG"}
{"created":"2024-03-24 14:35:06","title":"Enhancing MRI-Based Classification of Alzheimer's Disease with Explainable 3D Hybrid Compact Convolutional Transformers","abstract":"Alzheimer's disease (AD), characterized by progressive cognitive decline and memory loss, presents a formidable global health challenge, underscoring the critical importance of early and precise diagnosis for timely interventions and enhanced patient outcomes. While MRI scans provide valuable insights into brain structures, traditional analysis methods often struggle to discern intricate 3D patterns crucial for AD identification. Addressing this challenge, we introduce an alternative end-to-end deep learning model, the 3D Hybrid Compact Convolutional Transformers 3D (HCCT). By synergistically combining convolutional neural networks (CNNs) and vision transformers (ViTs), the 3D HCCT adeptly captures both local features and long-range relationships within 3D MRI scans. Extensive evaluations on prominent AD benchmark dataset, ADNI, demonstrate the 3D HCCT's superior performance, surpassing state of the art CNN and transformer-based methods in classification accuracy. Its robust generalization capability and interpretability marks a significant stride in AD classification from 3D MRI scans, promising more accurate and reliable diagnoses for improved patient care and superior clinical outcomes.","sentences":["Alzheimer's disease (AD), characterized by progressive cognitive decline and memory loss, presents a formidable global health challenge, underscoring the critical importance of early and precise diagnosis for timely interventions and enhanced patient outcomes.","While MRI scans provide valuable insights into brain structures, traditional analysis methods often struggle to discern intricate 3D patterns crucial for AD identification.","Addressing this challenge, we introduce an alternative end-to-end deep learning model, the 3D Hybrid Compact Convolutional Transformers 3D (HCCT).","By synergistically combining convolutional neural networks (CNNs) and vision transformers (ViTs), the 3D HCCT adeptly captures both local features and long-range relationships within 3D MRI scans.","Extensive evaluations on prominent AD benchmark dataset, ADNI, demonstrate the 3D HCCT's superior performance, surpassing state of the art CNN and transformer-based methods in classification accuracy.","Its robust generalization capability and interpretability marks a significant stride in AD classification from 3D MRI scans, promising more accurate and reliable diagnoses for improved patient care and superior clinical outcomes."],"url":"http://arxiv.org/abs/2403.16175v1","category":"eess.IV"}
{"created":"2024-03-24 14:33:02","title":"On properties of a semi-explicit in time fourth-order vector compact scheme for the multidimensional acoustic wave equation","abstract":"We deal with an initial-boundary value problem for the multidimensional acoustic wave equation, with the variable speed of sound. For a three-level semi-explicit in time higher-order vector compact scheme, we prove stability and derive 4th order error bound in the enlarged energy norm. This scheme is three-point in each spatial direction, and it exploits additional sought functions which approximate 2nd order non-mixed spatial derivatives of the solution to the equation. At the first time level, a similar two-level in time scheme is applied, with no derivatives of the data. No iterations are required to implement the scheme. We also present results of various 3D numerical experiments that demonstrate a very high accuracy of the scheme for smooth data, its advantages in the error behavior over the classical explicit 2nd order scheme for nonsmooth data as well and an example of the wave in a layered medium initiated by the Ricker-type wavelet source function.","sentences":["We deal with an initial-boundary value problem for the multidimensional acoustic wave equation, with the variable speed of sound.","For a three-level semi-explicit in time higher-order vector compact scheme, we prove stability and derive 4th order error bound in the enlarged energy norm.","This scheme is three-point in each spatial direction, and it exploits additional sought functions which approximate 2nd order non-mixed spatial derivatives of the solution to the equation.","At the first time level, a similar two-level in time scheme is applied, with no derivatives of the data.","No iterations are required to implement the scheme.","We also present results of various 3D numerical experiments that demonstrate a very high accuracy of the scheme for smooth data, its advantages in the error behavior over the classical explicit 2nd order scheme for nonsmooth data as well and an example of the wave in a layered medium initiated by the Ricker-type wavelet source function."],"url":"http://arxiv.org/abs/2403.16174v1","category":"math.NA"}
{"created":"2024-03-24 14:29:41","title":"Fusion of Minutia Cylinder Codes and Minutia Patch Embeddings for Latent Fingerprint Recognition","abstract":"Latent fingerprints are one of the most widely used forensic evidence by law enforcement agencies. However, latent recognition performance is far from the exemplary performance of sensor fingerprint recognition due to deformations and artifacts within these images. In this study, we propose a fusion based local matching approach towards latent fingerprint recognition. Recent latent recognition studies typically relied on local descriptor generation methods, in which either handcrafted minutiae features or deep neural network features are extracted around a minutia of interest, in the latent recognition process. Proposed approach would integrate these handcrafted features with a recently proposed deep neural network embedding features in a multi-stage fusion approach to significantly improve latent recognition results. Effectiveness of the proposed approach has been shown on several public and private data sets. As demonstrated in our experimental results, proposed method improves rank-1 identification accuracy by considerably for real-world datasets when compared to either the single usage of these features or existing state-of-the-art methods in the literature.","sentences":["Latent fingerprints are one of the most widely used forensic evidence by law enforcement agencies.","However, latent recognition performance is far from the exemplary performance of sensor fingerprint recognition due to deformations and artifacts within these images.","In this study, we propose a fusion based local matching approach towards latent fingerprint recognition.","Recent latent recognition studies typically relied on local descriptor generation methods, in which either handcrafted minutiae features or deep neural network features are extracted around a minutia of interest, in the latent recognition process.","Proposed approach would integrate these handcrafted features with a recently proposed deep neural network embedding features in a multi-stage fusion approach to significantly improve latent recognition results.","Effectiveness of the proposed approach has been shown on several public and private data sets.","As demonstrated in our experimental results, proposed method improves rank-1 identification accuracy by considerably for real-world datasets when compared to either the single usage of these features or existing state-of-the-art methods in the literature."],"url":"http://arxiv.org/abs/2403.16172v1","category":"cs.CV"}
{"created":"2024-03-24 14:25:12","title":"q-Deformed glueballs spectrum in AdS/QCD correspondence","abstract":"This work presents the application of the q-algebra in the glueballs spectrum. This algebra will be implemented through Jackson's derivatives in a Schr\\\"odinger-like equation, where such an equation comes from the gravity fluctuations around the braneworld scenario in five dimensions. In our prescription, we find the shape of the modified spectrum in the AdS/QCD scenario. The boundary conditions for the spectrum constrain the value of q.","sentences":["This work presents the application of the q-algebra in the glueballs spectrum.","This algebra will be implemented through Jackson's derivatives in a Schr\\\"odinger-like equation, where such an equation comes from the gravity fluctuations around the braneworld scenario in five dimensions.","In our prescription, we find the shape of the modified spectrum in the AdS/QCD scenario.","The boundary conditions for the spectrum constrain the value of q."],"url":"http://arxiv.org/abs/2403.16171v1","category":"hep-th"}
{"created":"2024-03-24 13:46:10","title":"Heaven and Earth: Nuclear Astrophysics after GW170817","abstract":"The historical detection of gravitational waves from the binary neutron star merger GW170817 is providing fundamental new insights into the astrophysical site for the creation of the heaviest elements in the cosmos and on the equation of state of neutron-rich matter. Shortly after this historical detection, electromagnetic observations of neutron stars together with measurements of the properties of neutron-rich nuclei at terrestrial facilities have placed additional constraints on the dynamics of neutron-rich matter. It is this unique synergy between heaven and earth that is the focus of this article.","sentences":["The historical detection of gravitational waves from the binary neutron star merger GW170817 is providing fundamental new insights into the astrophysical site for the creation of the heaviest elements in the cosmos and on the equation of state of neutron-rich matter.","Shortly after this historical detection, electromagnetic observations of neutron stars together with measurements of the properties of neutron-rich nuclei at terrestrial facilities have placed additional constraints on the dynamics of neutron-rich matter.","It is this unique synergy between heaven and earth that is the focus of this article."],"url":"http://arxiv.org/abs/2403.16154v1","category":"nucl-th"}
{"created":"2024-03-24 13:27:49","title":"Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes","abstract":"Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively.","sentences":["Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics.","However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present.","In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds.","Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies.","This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation.","To assess our methodology, we created an urban scene dataset masked with moving objects.","Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2403.16141v1","category":"cs.CV"}
{"created":"2024-03-24 12:37:23","title":"Lattice Boltzmann model for containerless freezing","abstract":"In this paper, a lattice Boltzmann model is proposed to simulate solid-liquid phase change phenomena in multiphase systems. The model couples the thermal properties of the solidification front with the dynamics of the liquid droplet interface, which enables the description of the complex interfacial changes during solid-liquid phase change process. The model treats the interfaces of gas, liquid, and solid phases using the phase field order parameter and the solid fraction. The volume expansion or contraction caused by the change of properties such as density during phase change is represented by adding a mass source term to the continuum equation. The proposed model is first validated by the three-phase Stefan problem and the droplet solidification on a cold surface, and the numerical results are in good agreement with the analytical and experimental results. Then it is used to model the solidification problem with bubbles. The results show that the model is able to accurately capture the effect of bubbles on the solidification process, which is in good agreement with previous work. In addition, a parametric study is carried out to examine the dependence of the sessile droplet solidification on different physical and numerical parameters. The results show that the droplet solidification time increases with increasing droplet volume and contact angle.","sentences":["In this paper, a lattice Boltzmann model is proposed to simulate solid-liquid phase change phenomena in multiphase systems.","The model couples the thermal properties of the solidification front with the dynamics of the liquid droplet interface, which enables the description of the complex interfacial changes during solid-liquid phase change process.","The model treats the interfaces of gas, liquid, and solid phases using the phase field order parameter and the solid fraction.","The volume expansion or contraction caused by the change of properties such as density during phase change is represented by adding a mass source term to the continuum equation.","The proposed model is first validated by the three-phase Stefan problem and the droplet solidification on a cold surface, and the numerical results are in good agreement with the analytical and experimental results.","Then it is used to model the solidification problem with bubbles.","The results show that the model is able to accurately capture the effect of bubbles on the solidification process, which is in good agreement with previous work.","In addition, a parametric study is carried out to examine the dependence of the sessile droplet solidification on different physical and numerical parameters.","The results show that the droplet solidification time increases with increasing droplet volume and contact angle."],"url":"http://arxiv.org/abs/2403.16122v1","category":"physics.flu-dyn"}
{"created":"2024-03-24 11:45:33","title":"Quantum Field Theory of Electrons and Nuclei","abstract":"We develop a non-relativistic quantum field theory of electrons and nuclei based on the Coulomb Hamiltonian. We derive the exact equations of motion and write these equations in the form of Hedin's equations for all species of identical particles involved. Theory derived allows the computation of exact observables and provides a rigorous starting point to derive approximations in a systematic way.","sentences":["We develop a non-relativistic quantum field theory of electrons and nuclei based on the Coulomb Hamiltonian.","We derive the exact equations of motion and write these equations in the form of Hedin's equations for all species of identical particles involved.","Theory derived allows the computation of exact observables and provides a rigorous starting point to derive approximations in a systematic way."],"url":"http://arxiv.org/abs/2403.16103v1","category":"quant-ph"}
{"created":"2024-03-24 11:09:41","title":"Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap","abstract":"Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators.","sentences":["Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities.","However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way.","Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully.","To this end, we propose a novel perspective for addressing the real-to-simulated data gap.","Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data.","Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique.","Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies.","Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases.","Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators."],"url":"http://arxiv.org/abs/2403.16092v1","category":"cs.CV"}
{"created":"2024-03-24 10:06:40","title":"PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling","abstract":"High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: https://pku-dymvhumans.github.io.","sentences":["High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics.","Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses.","In part, this is due to the challenges of acquiring high-quality human datasets.","To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos.","It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios.","These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion.","Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset.","It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene.","Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data.","The dataset is available at: https://pku-dymvhumans.github.io."],"url":"http://arxiv.org/abs/2403.16080v1","category":"cs.CV"}
{"created":"2024-03-24 09:11:55","title":"Towards a MATLAB Toolbox to compute backstepping kernels using the power series method","abstract":"In this paper, we extend our previous work on the power series method for computing backstepping kernels. Our first contribution is the development of initial steps towards a MATLAB toolbox dedicated to backstepping kernel computation. This toolbox would exploit MATLAB's linear algebra and sparse matrix manipulation features for enhanced efficiency; our initial findings show considerable improvements in computational speed with respect to the use of symbolical software without loss of precision at high orders. Additionally, we tackle limitations observed in our earlier work, such as slow convergence (due to oscillatory behaviors) and non-converging series (due to loss of analiticity at some singular points). To overcome these challenges, we introduce a technique that mitigates this behaviour by computing the expansion at different points, denoted as localized power series. This approach effectively navigates around singularities, and can also accelerates convergence by using more local approximations. Basic examples are provided to demonstrate these enhancements. Although this research is still ongoing, the significant potential and simplicity of the method already establish the power series approach as a viable and versatile solution for solving backstepping kernel equations, benefiting both novel and experienced practitioners in the field. We anticipate that these developments will be particularly beneficial in training the recently introduced neural operators that approximate backstepping kernels and gains.","sentences":["In this paper, we extend our previous work on the power series method for computing backstepping kernels.","Our first contribution is the development of initial steps towards a MATLAB toolbox dedicated to backstepping kernel computation.","This toolbox would exploit MATLAB's linear algebra and sparse matrix manipulation features for enhanced efficiency; our initial findings show considerable improvements in computational speed with respect to the use of symbolical software without loss of precision at high orders.","Additionally, we tackle limitations observed in our earlier work, such as slow convergence (due to oscillatory behaviors) and non-converging series (due to loss of analiticity at some singular points).","To overcome these challenges, we introduce a technique that mitigates this behaviour by computing the expansion at different points, denoted as localized power series.","This approach effectively navigates around singularities, and can also accelerates convergence by using more local approximations.","Basic examples are provided to demonstrate these enhancements.","Although this research is still ongoing, the significant potential and simplicity of the method already establish the power series approach as a viable and versatile solution for solving backstepping kernel equations, benefiting both novel and experienced practitioners in the field.","We anticipate that these developments will be particularly beneficial in training the recently introduced neural operators that approximate backstepping kernels and gains."],"url":"http://arxiv.org/abs/2403.16070v1","category":"eess.SY"}
{"created":"2024-03-24 08:12:32","title":"Nonstandard interactions of neutrinos with dense matter","abstract":"Nonstandard interaction is expected to be a crucial hint to explain the experimental data of neutrino scattering off electrons. In this context, the nonstandard interaction vector and axial-vector couplings are needed to be extracted from recent experiments and a few of them are now available in the literature. With these coupling bounds, in this paper, I explore their impacts on the neutrinos interacting with the free electron gas in dense matter. To this end, I compute and predict the neutrino differential cross section and mean free path in dense matter for all those experimental bounds. Interesting behavior in the neutrino cross sections and mean free path is found for the different nonstandard interaction couplings from different experiment constraints. I also found that the neutrino cross-section and mean free path in the dense matter are very sensitive to values and signs of the nonstandard interaction couplings, leading to different prediction results to the Standard Model cross-section and mean free path.","sentences":["Nonstandard interaction is expected to be a crucial hint to explain the experimental data of neutrino scattering off electrons.","In this context, the nonstandard interaction vector and axial-vector couplings are needed to be extracted from recent experiments and a few of them are now available in the literature.","With these coupling bounds, in this paper, I explore their impacts on the neutrinos interacting with the free electron gas in dense matter.","To this end, I compute and predict the neutrino differential cross section and mean free path in dense matter for all those experimental bounds.","Interesting behavior in the neutrino cross sections and mean free path is found for the different nonstandard interaction couplings from different experiment constraints.","I also found that the neutrino cross-section and mean free path in the dense matter are very sensitive to values and signs of the nonstandard interaction couplings, leading to different prediction results to the Standard Model cross-section and mean free path."],"url":"http://arxiv.org/abs/2403.16061v1","category":"hep-ph"}
{"created":"2024-03-24 07:33:08","title":"A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data","abstract":"Federated Split Learning (FSL) is a promising distributed learning paradigm in practice, which gathers the strengths of both Federated Learning (FL) and Split Learning (SL) paradigms, to ensure model privacy while diminishing the resource overhead of each client, especially on large transformer models in a resource-constrained environment, e.g., Internet of Things (IoT). However, almost all works merely investigate the performance with simple neural network models in FSL. Despite the minor efforts focusing on incorporating Vision Transformers (ViT) as model architectures, they train ViT from scratch, thereby leading to enormous training overhead in each device with limited resources. Therefore, in this paper, we harness Pre-trained Image Transformers (PITs) as the initial model, coined FES-PIT, to accelerate the training process and improve model robustness. Furthermore, we propose FES-PTZO to hinder the gradient inversion attack, especially having the capability compatible with black-box scenarios, where the gradient information is unavailable. Concretely, FES-PTZO approximates the server gradient by utilizing a zeroth-order (ZO) optimization, which replaces the backward propagation with just one forward process. Empirically, we are the first to provide a systematic evaluation of FSL methods with PITs in real-world datasets, different partial device participations, and heterogeneous data splits. Our experiments verify the effectiveness of our algorithms.","sentences":["Federated Split Learning (FSL) is a promising distributed learning paradigm in practice, which gathers the strengths of both Federated Learning (FL) and Split Learning (SL) paradigms, to ensure model privacy while diminishing the resource overhead of each client, especially on large transformer models in a resource-constrained environment, e.g., Internet of Things (IoT).","However, almost all works merely investigate the performance with simple neural network models in FSL.","Despite the minor efforts focusing on incorporating Vision Transformers (ViT) as model architectures, they train ViT from scratch, thereby leading to enormous training overhead in each device with limited resources.","Therefore, in this paper, we harness Pre-trained Image Transformers (PITs) as the initial model, coined FES-PIT, to accelerate the training process and improve model robustness.","Furthermore, we propose FES-PTZO to hinder the gradient inversion attack, especially having the capability compatible with black-box scenarios, where the gradient information is unavailable.","Concretely, FES-PTZO approximates the server gradient by utilizing a zeroth-order (ZO) optimization, which replaces the backward propagation with just one forward process.","Empirically, we are the first to provide a systematic evaluation of FSL methods with PITs in real-world datasets, different partial device participations, and heterogeneous data splits.","Our experiments verify the effectiveness of our algorithms."],"url":"http://arxiv.org/abs/2403.16050v1","category":"cs.CV"}
{"created":"2024-03-24 06:59:53","title":"General One-loop Generating Function by IBP relations","abstract":"In this paper we have studied the most general generating function of reduction for one loop integrals with arbitrary tensor structure in numerator and arbitrary power distribution of propagators in denominator. Using IBP relations, we have established the partial differential equations for these generating functions and solved them analytically. These results provide useful guidance for applying generating function method to reductions of higher loop integrals.","sentences":["In this paper we have studied the most general generating function of reduction for one loop integrals with arbitrary tensor structure in numerator and arbitrary power distribution of propagators in denominator.","Using IBP relations, we have established the partial differential equations for these generating functions and solved them analytically.","These results provide useful guidance for applying generating function method to reductions of higher loop integrals."],"url":"http://arxiv.org/abs/2403.16040v1","category":"hep-ph"}
{"created":"2024-03-24 06:10:54","title":"Planning Charging Stations and Service Operations of Dockless Electric Micromobility Systems","abstract":"Dockless electric micro-mobility services (e.g., shared e-scooters and e-bikes) have been increasingly popular in the recent decade, and a variety of charging technologies have emerged for these services. The use of charging stations, to/from which service vehicles are transported by the riders for charging, poses as a promising approach because it reduces the need for dedicated staff or contractors. However, unique challenges also arise, such as how to incentivize riders to drop off vehicles at stations and how to efficiently utilize the vehicles being charged at the stations. This paper focuses on dockless e-scooters as an example and develops a new spatial queuing network model to capture the steady-state scooter service cycles, battery consumption and charging processes, and the associated pricing and management mechanisms. Building upon this model, a system of closed-form equations is formulated and incorporated into a constrained nonlinear program to optimize the deployment of the service fleet, the design of charging stations (i.e., number, location, and capacity), user-based charging price promotions and priorities, and repositioning truck operations (i.e., headway and truck load). The proposed queuing network model is found to match very well with agent-based simulations. It is applied to a series of numerical experiments to draw insights into the optimal designs and the system performance. The numerical results reveal strong advantages of using charging stations for shared dockless electric micro-mobility services as compared to state-of-the-art alternatives. The proposed model can also be used to analyze other micromobility services and other charging approaches.","sentences":["Dockless electric micro-mobility services (e.g., shared e-scooters and e-bikes) have been increasingly popular in the recent decade, and a variety of charging technologies have emerged for these services.","The use of charging stations, to/from which service vehicles are transported by the riders for charging, poses as a promising approach because it reduces the need for dedicated staff or contractors.","However, unique challenges also arise, such as how to incentivize riders to drop off vehicles at stations and how to efficiently utilize the vehicles being charged at the stations.","This paper focuses on dockless e-scooters as an example and develops a new spatial queuing network model to capture the steady-state scooter service cycles, battery consumption and charging processes, and the associated pricing and management mechanisms.","Building upon this model, a system of closed-form equations is formulated and incorporated into a constrained nonlinear program to optimize the deployment of the service fleet, the design of charging stations (i.e., number, location, and capacity), user-based charging price promotions and priorities, and repositioning truck operations (i.e., headway and truck load).","The proposed queuing network model is found to match very well with agent-based simulations.","It is applied to a series of numerical experiments to draw insights into the optimal designs and the system performance.","The numerical results reveal strong advantages of using charging stations for shared dockless electric micro-mobility services as compared to state-of-the-art alternatives.","The proposed model can also be used to analyze other micromobility services and other charging approaches."],"url":"http://arxiv.org/abs/2403.16029v1","category":"math.OC"}
{"created":"2024-03-24 06:03:53","title":"A transformer-based neural operator for large-eddy simulation of turbulence","abstract":"Predicting the large-scale dynamics of three-dimensional (3D) turbulence is challenging for machine learning approaches. This paper introduces a transformer-based neural operator (TNO) to achieve precise and efficient predictions in the large-eddy simulation (LES) of 3D turbulence. The performance of the proposed TNO model is systematically tested and compared with classical sub-grid scale (SGS) models, including the dynamic Smagorinsky model (DSM) and the dynamic mixed model(DMM), as well as the original Fourier neural operator (FNO) model, in homogeneous isotropic turbulence (HIT) and free-shear turbulent mixing layer. The numerical simulations comprehensively evaluate the performance of these models on a variety of flow statistics, including the velocity spectrum, the probability density functions (PDFs) of vorticity, the PDFs of velocity increments, the evolution of turbulent kinetic energy, and the iso-surface of the Q-criterion. The results demonstrate that the TNO model exhibits better accuracy than the DSM, DMM, and FNO models in both HIT and the turbulent mixing layer. Moreover, the TNO model has fewer parameters than the FNO model and enables long-term stable predictions, which the FNO model cannot achieve. Besides, the proposed TNO model is much faster than traditional LES with DSM and DMM models, showing great potential in tackling 3D nonlinear engineering problems.","sentences":["Predicting the large-scale dynamics of three-dimensional (3D) turbulence is challenging for machine learning approaches.","This paper introduces a transformer-based neural operator (TNO) to achieve precise and efficient predictions in the large-eddy simulation (LES) of 3D turbulence.","The performance of the proposed TNO model is systematically tested and compared with classical sub-grid scale (SGS) models, including the dynamic Smagorinsky model (DSM) and the dynamic mixed model(DMM), as well as the original Fourier neural operator (FNO) model, in homogeneous isotropic turbulence (HIT) and free-shear turbulent mixing layer.","The numerical simulations comprehensively evaluate the performance of these models on a variety of flow statistics, including the velocity spectrum, the probability density functions (PDFs) of vorticity, the PDFs of velocity increments, the evolution of turbulent kinetic energy, and the iso-surface of the Q-criterion.","The results demonstrate that the TNO model exhibits better accuracy than the DSM, DMM, and FNO models in both HIT and the turbulent mixing layer.","Moreover, the TNO model has fewer parameters than the FNO model and enables long-term stable predictions, which the FNO model cannot achieve.","Besides, the proposed TNO model is much faster than traditional LES with DSM and DMM models, showing great potential in tackling 3D nonlinear engineering problems."],"url":"http://arxiv.org/abs/2403.16026v1","category":"physics.flu-dyn"}
{"created":"2024-03-24 05:57:00","title":"A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA","abstract":"This paper presents a comprehensive study on the unified module for accelerating stable-diffusion processes, specifically focusing on the lcm-lora module. Stable-diffusion processes play a crucial role in various scientific and engineering domains, and their acceleration is of paramount importance for efficient computational performance. The standard iterative procedures for solving fixed-source discrete ordinates problems often exhibit slow convergence, particularly in optically thick scenarios. To address this challenge, unconditionally stable diffusion-acceleration methods have been developed, aiming to enhance the computational efficiency of transport equations and discrete ordinates problems. This study delves into the theoretical foundations and numerical results of unconditionally stable diffusion synthetic acceleration methods, providing insights into their stability and performance for model discrete ordinates problems. Furthermore, the paper explores recent advancements in diffusion model acceleration, including on device acceleration of large diffusion models via gpu aware optimizations, highlighting the potential for significantly improved inference latency. The results and analyses in this study provide important insights into stable diffusion processes and have important ramifications for the creation and application of acceleration methods specifically, the lcm-lora module in a variety of computing environments.","sentences":["This paper presents a comprehensive study on the unified module for accelerating stable-diffusion processes, specifically focusing on the lcm-lora module.","Stable-diffusion processes play a crucial role in various scientific and engineering domains, and their acceleration is of paramount importance for efficient computational performance.","The standard iterative procedures for solving fixed-source discrete ordinates problems often exhibit slow convergence, particularly in optically thick scenarios.","To address this challenge, unconditionally stable diffusion-acceleration methods have been developed, aiming to enhance the computational efficiency of transport equations and discrete ordinates problems.","This study delves into the theoretical foundations and numerical results of unconditionally stable diffusion synthetic acceleration methods, providing insights into their stability and performance for model discrete ordinates problems.","Furthermore, the paper explores recent advancements in diffusion model acceleration, including on device acceleration of large diffusion models via gpu aware optimizations, highlighting the potential for significantly improved inference latency.","The results and analyses in this study provide important insights into stable diffusion processes and have important ramifications for the creation and application of acceleration methods specifically, the lcm-lora module in a variety of computing environments."],"url":"http://arxiv.org/abs/2403.16024v1","category":"cs.LG"}
{"created":"2024-03-24 05:52:14","title":"Roles of boundary and equation-of-motion terms in cosmological correlation functions","abstract":"We revisit the properties of total time-derivative terms as well as terms proportional to the free equation of motion (EOM) in a Schwinger-Keldysh formalism. They are relevant to the correct calculation of correlation functions of curvature perturbations in the context of inflationary Universe. We show that these two contributions to the action play different roles in the operator or the path-integral formalism, but they give the same correlation functions as each other. As a concrete example, we confirm that the Maldacena's consistency relations for the three-point correlation function in the slow-roll inflationary scenario driven by a minimally coupled canonical scalar field hold in both the operator and path-integral formalisms. We also give some comments on loop calculations.","sentences":["We revisit the properties of total time-derivative terms as well as terms proportional to the free equation of motion (EOM) in a Schwinger-Keldysh formalism.","They are relevant to the correct calculation of correlation functions of curvature perturbations in the context of inflationary Universe.","We show that these two contributions to the action play different roles in the operator or the path-integral formalism, but they give the same correlation functions as each other.","As a concrete example, we confirm that the Maldacena's consistency relations for the three-point correlation function in the slow-roll inflationary scenario driven by a minimally coupled canonical scalar field hold in both the operator and path-integral formalisms.","We also give some comments on loop calculations."],"url":"http://arxiv.org/abs/2403.16022v1","category":"hep-th"}
{"created":"2024-03-24 05:50:00","title":"PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference","abstract":"As deep neural networks evolve from convolutional neural networks (ConvNets) to advanced vision transformers (ViTs), there is an increased need to eliminate redundant data for faster processing without compromising accuracy. Previous methods are often architecture-specific or necessitate re-training, restricting their applicability with frequent model updates. To solve this, we first introduce a novel property of lightweight ConvNets: their ability to identify key discriminative patch regions in images, irrespective of model's final accuracy or size. We demonstrate that fully-connected layers are the primary bottleneck for ConvNets performance, and their suppression with simple weight recalibration markedly enhances discriminative patch localization performance. Using this insight, we introduce PaPr, a method for substantially pruning redundant patches with minimal accuracy loss using lightweight ConvNets across a variety of deep learning architectures, including ViTs, ConvNets, and hybrid transformers, without any re-training. Moreover, the simple early-stage one-step patch pruning with PaPr enhances existing patch reduction methods. Through extensive testing on diverse architectures, PaPr achieves significantly higher accuracy over state-of-the-art patch reduction methods with similar FLOP count reduction. More specifically, PaPr reduces about 70% of redundant patches in videos with less than 0.8% drop in accuracy, and up to 3.7x FLOPs reduction, which is a 15% more reduction with 2.5% higher accuracy.","sentences":["As deep neural networks evolve from convolutional neural networks (ConvNets) to advanced vision transformers (ViTs), there is an increased need to eliminate redundant data for faster processing without compromising accuracy.","Previous methods are often architecture-specific or necessitate re-training, restricting their applicability with frequent model updates.","To solve this, we first introduce a novel property of lightweight ConvNets: their ability to identify key discriminative patch regions in images, irrespective of model's final accuracy or size.","We demonstrate that fully-connected layers are the primary bottleneck for ConvNets performance, and their suppression with simple weight recalibration markedly enhances discriminative patch localization performance.","Using this insight, we introduce PaPr, a method for substantially pruning redundant patches with minimal accuracy loss using lightweight ConvNets across a variety of deep learning architectures, including ViTs, ConvNets, and hybrid transformers, without any re-training.","Moreover, the simple early-stage one-step patch pruning with PaPr enhances existing patch reduction methods.","Through extensive testing on diverse architectures, PaPr achieves significantly higher accuracy over state-of-the-art patch reduction methods with similar FLOP count reduction.","More specifically, PaPr reduces about 70% of redundant patches in videos with less than 0.8% drop in accuracy, and up to 3.7x FLOPs reduction, which is a 15% more reduction with 2.5% higher accuracy."],"url":"http://arxiv.org/abs/2403.16020v1","category":"cs.CV"}
{"created":"2024-03-24 04:58:49","title":"Performance evaluation of accelerated complex multiple-precision LU decomposition","abstract":"The direct method is one of the most important algorithms for solving linear systems of equations, with LU decomposition comprising a significant portion of its computation time. This study explores strategies to accelerate complex LU decomposition using multiple-precision floating-point arithmetic of the multiple-component type. Specifically, we explore the potential efficiency gains using a combination of SIMDization and the 3M method for complex matrix multiplication. Our benchmark tests compare this approach with the direct method implementation in MPLAPACK, focusing on computation time and numerical errors.","sentences":["The direct method is one of the most important algorithms for solving linear systems of equations, with LU decomposition comprising a significant portion of its computation time.","This study explores strategies to accelerate complex LU decomposition using multiple-precision floating-point arithmetic of the multiple-component type.","Specifically, we explore the potential efficiency gains using a combination of SIMDization and the 3M method for complex matrix multiplication.","Our benchmark tests compare this approach with the direct method implementation in MPLAPACK, focusing on computation time and numerical errors."],"url":"http://arxiv.org/abs/2403.16013v1","category":"math.NA"}
{"created":"2024-03-24 03:20:11","title":"On the Navier-Stokes equations and the Hamilton-Jacobi-Bellman equation on the group of volume preserving diffeomorphisms","abstract":"In this paper, we give a new derivation of the incompressible Navier-Stokes equations on a compact Riemannian manifold $M$ via the Bellman dynamic programming principle on the infinite dimensional group of volume preserving diffeomorphisms $G={\\rm SDiff}(M)$. In particular, when the viscosity vanishes, we give a new derivation of the incompressible Euler equation on a compact Riemannian manifold. The main result of this paper indicates an interesting relationship between the incompressible Navier-Stokes equations on $M$ and the Hamilton-Jacobi-Bellman equation on $G={\\rm SDiff}(M)$.","sentences":["In this paper, we give a new derivation of the incompressible Navier-Stokes equations on a compact Riemannian manifold $M$ via the Bellman dynamic programming principle on the infinite dimensional group of volume preserving diffeomorphisms $G={\\rm SDiff}(M)$.","In particular, when the viscosity vanishes, we give a new derivation of the incompressible Euler equation on a compact Riemannian manifold.","The main result of this paper indicates an interesting relationship between the incompressible Navier-Stokes equations on $M$ and the Hamilton-Jacobi-Bellman equation on $G={\\rm SDiff}(M)$."],"url":"http://arxiv.org/abs/2403.15997v1","category":"math.PR"}
{"created":"2024-03-24 03:12:41","title":"A unified kinematic wave theory for melt infiltration into firn","abstract":"Motivated by the refreezing of melt water in firn we revisit the one-dimensional percolation of liquid water and non-reactive gas in porous ice. We analyze the dynamics of infiltration in the absence of capillary forces and heat conduction to understand the coupling between advective heat and mass transport in firn. In this limit, we formulate a kinematic wave theory that results in a 2X2-system of hyperbolic partial differential equations (PDEs) corresponding to the conservation of composition and enthalpy. For simple initial conditions (Riemann problems) this system admits self-similar solutions that illuminate the structure of melting/refreezing fronts and analytical solutions are provided for 12 basic cases of physical relevance encountered in the literature. Further we develop an extended kinematic theory that encompasses the cases when the firn saturates completely to form a perched water table governed by elliptic PDE so that the model is no longer fully hyperbolic (local). These solutions provide benchmarks for numerical models of melt infiltration into firn. They also provide insight into important physical processes such as the formation of frozen fringes, the perching of meltwater on pre-existing low porosity layers and the conditions required for impermeable ice lens formation. Lastly, these analytic solutions can be utilized to improve and compare the performance of the firn hydrology, ice-sheet and Earth system models. Our analysis provides a theoretical framework to understand these important processes in firn which affect the partitioning between meltwater infiltration and surface runoff and therefore determine the surface mass loss from ice sheets and its contribution to sea level rise.","sentences":["Motivated by the refreezing of melt water in firn we revisit the one-dimensional percolation of liquid water and non-reactive gas in porous ice.","We analyze the dynamics of infiltration in the absence of capillary forces and heat conduction to understand the coupling between advective heat and mass transport in firn.","In this limit, we formulate a kinematic wave theory that results in a 2X2-system of hyperbolic partial differential equations (PDEs) corresponding to the conservation of composition and enthalpy.","For simple initial conditions (Riemann problems) this system admits self-similar solutions that illuminate the structure of melting/refreezing fronts and analytical solutions are provided for 12 basic cases of physical relevance encountered in the literature.","Further we develop an extended kinematic theory that encompasses the cases when the firn saturates completely to form a perched water table governed by elliptic PDE so that the model is no longer fully hyperbolic (local).","These solutions provide benchmarks for numerical models of melt infiltration into firn.","They also provide insight into important physical processes such as the formation of frozen fringes, the perching of meltwater on pre-existing low porosity layers and the conditions required for impermeable ice lens formation.","Lastly, these analytic solutions can be utilized to improve and compare the performance of the firn hydrology, ice-sheet and Earth system models.","Our analysis provides a theoretical framework to understand these important processes in firn which affect the partitioning between meltwater infiltration and surface runoff and therefore determine the surface mass loss from ice sheets and its contribution to sea level rise."],"url":"http://arxiv.org/abs/2403.15996v1","category":"physics.flu-dyn"}
{"created":"2024-03-24 02:55:45","title":"Mars Spectrometry 2: Gas Chromatography -- Second place solution","abstract":"The Mars Spectrometry 2: Gas Chromatography challenge was sponsored by NASA and run on the DrivenData competition platform in 2022. This report describes the solution which achieved the second-best score on the competition's test dataset. The solution utilized two-dimensional, image-like representations of the competition's chromatography data samples. A number of different Convolutional Neural Network models were trained and ensembled for the final submission.","sentences":["The Mars Spectrometry 2: Gas Chromatography challenge was sponsored by NASA and run on the DrivenData competition platform in 2022.","This report describes the solution which achieved the second-best score on the competition's test dataset.","The solution utilized two-dimensional, image-like representations of the competition's chromatography data samples.","A number of different Convolutional Neural Network models were trained and ensembled for the final submission."],"url":"http://arxiv.org/abs/2403.15990v1","category":"cs.CV"}
{"created":"2024-03-24 02:53:04","title":"Infinite dimensional open-loop linear quadratic stochastic optimal control problems and related games","abstract":"We investigate the linear quadratic stochastic optimal control problems in infinite dimension without Markovian restriction for coefficients. The necessary and sufficient conditions for open-loop optimal controls are presented. We prove the Fr\\'echet differentiable of the cost functional with respect to the control variable, and the Fr\\'echet derivatives are characterized in detail by operators derived from dual analysis, which are proven to be the stationary conditions. Transposition methods are adopted to deal with the adjoint equations. As applications, we employ the results to study open-loop Nash equilibria for two-person stochastic differential games.","sentences":["We investigate the linear quadratic stochastic optimal control problems in infinite dimension without Markovian restriction for coefficients.","The necessary and sufficient conditions for open-loop optimal controls are presented.","We prove the Fr\\'echet differentiable of the cost functional with respect to the control variable, and the Fr\\'echet derivatives are characterized in detail by operators derived from dual analysis, which are proven to be the stationary conditions.","Transposition methods are adopted to deal with the adjoint equations.","As applications, we employ the results to study open-loop Nash equilibria for two-person stochastic differential games."],"url":"http://arxiv.org/abs/2403.15988v1","category":"math.OC"}
{"created":"2024-03-24 02:28:21","title":"Initial data sets with vanishing mass are contained in pp-wave spacetimes","abstract":"In 1981, Schoen-Yau and Witten showed that in General Relativity both the total energy $E$ and the total mass $m$ of an initial data set modelling an isolated gravitational system are non-negative. Moreover, if $E=0$, the initial data set must be contained in Minkowski space. In this paper, we show that if $m=0$, i.e. if $E$ equals the total momentum $|P|$, the initial data set must be contained in a pp-wave spacetime. Our proof combines spinorial methods with spacetime harmonic functions and works in all dimensions. Additionally, we find the decay rate threshold where the embedding has to be within Minkowski space and construct non-vacuum initial data sets with $m=0$ in the borderline case. As a consequence, this completely settles the rigidity of the spacetime positive mass theorem for spin manifolds.","sentences":["In 1981, Schoen-Yau and Witten showed that in General Relativity both the total energy $E$ and the total mass $m$ of an initial data set modelling an isolated gravitational system are non-negative.","Moreover, if $E=0$, the initial data set must be contained in Minkowski space.","In this paper, we show that if $m=0$, i.e. if $E$ equals the total momentum $|P|$, the initial data set must be contained in a pp-wave spacetime.","Our proof combines spinorial methods with spacetime harmonic functions and works in all dimensions.","Additionally, we find the decay rate threshold where the embedding has to be within Minkowski space and construct non-vacuum initial data sets with $m=0$ in the borderline case.","As a consequence, this completely settles the rigidity of the spacetime positive mass theorem for spin manifolds."],"url":"http://arxiv.org/abs/2403.15984v1","category":"gr-qc"}
{"created":"2024-03-24 02:20:03","title":"Generally covariant geometric momentum and geometric potential for a Dirac fermion on a two-dimensional hypersurface","abstract":"Geometric momentum is the proper momentum for a moving particle constrained on a curved surface, which depends on the outer curvature and has observable effects. In the context of multi-component quantum states, geometric momentum should be rewritten as generally covariant geometric momentum. For a Dirac fermion constrained on a two-dimensional hypersurface, we give the generally covariant geometric momentum, and show that on the pseudosphere and the helical surface there exist no curvature-induced geometric potentials. These results verify that the dynamical quantization conditions are effective in dealing with constrained systems on hypersurfaces, and one could obtain the generally convariant geometric momentum and the geometric potential of a spin particle constrained on surfaces with definite parametric equations.","sentences":["Geometric momentum is the proper momentum for a moving particle constrained on a curved surface, which depends on the outer curvature and has observable effects.","In the context of multi-component quantum states, geometric momentum should be rewritten as generally covariant geometric momentum.","For a Dirac fermion constrained on a two-dimensional hypersurface, we give the generally covariant geometric momentum, and show that on the pseudosphere and the helical surface there exist no curvature-induced geometric potentials.","These results verify that the dynamical quantization conditions are effective in dealing with constrained systems on hypersurfaces, and one could obtain the generally convariant geometric momentum and the geometric potential of a spin particle constrained on surfaces with definite parametric equations."],"url":"http://arxiv.org/abs/2403.15982v1","category":"quant-ph"}
{"created":"2024-03-24 02:10:55","title":"Markovian projections for It\u00f4 semimartingales with jumps","abstract":"Given a general It\\^o semimartingale, its Markovian projection is an It\\^o process, with Markovian differential characteristics, that matches the one-dimensional marginal laws of the original process. We construct Markovian projections for It\\^o semimartingales with jumps, whose flows of one-dimensional marginal laws are solutions to non-local Fokker--Planck--Kolmogorov equations (FPKEs). As an application, we show how Markovian projections appear in building calibrated diffusion/jump models with both local and stochastic features.","sentences":["Given a general It\\^o semimartingale, its Markovian projection is an It\\^o process, with Markovian differential characteristics, that matches the one-dimensional marginal laws of the original process.","We construct Markovian projections for It\\^o semimartingales with jumps, whose flows of one-dimensional marginal laws are solutions to non-local Fokker--Planck--Kolmogorov equations (FPKEs).","As an application, we show how Markovian projections appear in building calibrated diffusion/jump models with both local and stochastic features."],"url":"http://arxiv.org/abs/2403.15980v1","category":"math.PR"}
{"created":"2024-03-24 01:30:29","title":"Geometric signals","abstract":"In signal processing, a signal is the graph of a function. We define a signal as a submanifold of a Riemannian manifold (with corners). We obtain inequalities that relate the energy of the signal and the energy of its Fourier transform. We quantify noise and filtering.","sentences":["In signal processing, a signal is the graph of a function.","We define a signal as a submanifold of a Riemannian manifold (with corners).","We obtain inequalities that relate the energy of the signal and the energy of its Fourier transform.","We quantify noise and filtering."],"url":"http://arxiv.org/abs/2403.15978v1","category":"math.DG"}
{"created":"2024-03-24 00:44:15","title":"Isoperimetric profile function comparisons with Integral Ricci curvature bounds","abstract":"We prove comparison results for the Isoperimetric profile function in the setting of manifolds with integral bounds on the Ricci curvature. We extend previous work of Ni and Wang and Bayle and Rosales under the usual pointwise bounds for the Ricci curvature.","sentences":["We prove comparison results for the Isoperimetric profile function in the setting of manifolds with integral bounds on the Ricci curvature.","We extend previous work of Ni and Wang and Bayle and Rosales under the usual pointwise bounds for the Ricci curvature."],"url":"http://arxiv.org/abs/2403.15973v1","category":"math.DG"}
{"created":"2024-03-24 00:43:35","title":"Positive mass and isoperimetry for continuous metrics with nonnegative scalar curvature","abstract":"This paper deals with the positive mass theorem and the existence of isoperimetric sets on $3$-manifolds endowed with continuous complete metrics having nonnegative scalar curvature in a suitable weak sense.   We prove that if the manifold has an end that is $C^0$-locally asymptotically flat, and the metric is the local uniform limit of smooth metrics with vanishing lower bounds on the scalar curvature outside a compact set, then Huisken's isoperimetric mass is nonnegative. This addresses a version of a recent conjecture of Huisken about positive isoperimetric mass theorems for continuous metrics satisfying $R_g\\geq 0$ in a weak sense. As a consequence, any fill-in of a truncation of a Schwarzschild space with negative ADM mass has nonnegative isoperimetric mass.   Moreover, in case the whole manifold is $C^0$-locally asymptotically flat and the metric is the local uniform limit of smooth metrics with vanishing lower bounds on the scalar curvature outside a compact set, we prove that, as a large scale effect, isoperimetric sets with arbitrarily large volume exist.","sentences":["This paper deals with the positive mass theorem and the existence of isoperimetric sets on $3$-manifolds endowed with continuous complete metrics having nonnegative scalar curvature in a suitable weak sense.   ","We prove that if the manifold has an end that is $C^0$-locally asymptotically flat, and the metric is the local uniform limit of smooth metrics with vanishing lower bounds on the scalar curvature outside a compact set, then Huisken's isoperimetric mass is nonnegative.","This addresses a version of a recent conjecture of Huisken about positive isoperimetric mass theorems for continuous metrics satisfying $R_g\\geq 0$ in a weak sense.","As a consequence, any fill-in of a truncation of a Schwarzschild space with negative ADM mass has nonnegative isoperimetric mass.   ","Moreover, in case the whole manifold is $C^0$-locally asymptotically flat and the metric is the local uniform limit of smooth metrics with vanishing lower bounds on the scalar curvature outside a compact set, we prove that, as a large scale effect, isoperimetric sets with arbitrarily large volume exist."],"url":"http://arxiv.org/abs/2403.15972v1","category":"math.DG"}
{"created":"2024-03-24 00:08:32","title":"Symplectic differential reduction algebras and skew-affine generalized Weyl algebras","abstract":"For a map $\\varphi\\!:U(\\mathfrak{g})\\rightarrow A$ of associative algebras, $U(\\mathfrak{g})$ the universal enveloping algebra of a (complex) finite-dimensional reductive Lie algebra, the representation theory of $A$ is intimately tied to the representation theory of the $A$-subquotient known as the reduction algebra for $(A,\\mathfrak{g}, \\varphi)$. Herlemont and Ogievetsky studied differential reduction algebras for the general linear Lie algebra $\\mathfrak{gl}(n)$ as the algebra of $h$-deformed differential operators formed from realizations of $\\mathfrak{gl}(n)$ in the $N$-fold tensor product of the $n $th Weyl algebra. In this paper, we further the study of differential reduction algebras by presenting the symplectic differential reduction algebra $D\\big(\\mathfrak{sp}(4)\\big)$, by generators and relations, and showing its connections to Bavula's generalized Weyl algebras (GWAs). In doing so, we determine a new class of GWAs we call $\\textit{skew-affine}$ GWAs, of which $D\\big(\\mathfrak{gl}(2)\\big)$ and $D\\big(\\mathfrak{sp}(4)\\big)$ are examples. We conjecture that the differential reduction algebra of the orthosymplectic Lie superalgebra $\\mathfrak{osp}(1|2n)$ is a twisted generalized Weyl algebra (TGWA) and that the relations for $D\\big(\\mathfrak{sp}(2n)\\big)$ yield solutions to the dynamical Yang-Baxter equation (DYBE).","sentences":["For a map $\\varphi\\!:U(\\mathfrak{g})\\rightarrow A$ of associative algebras, $U(\\mathfrak{g})$ the universal enveloping algebra of a (complex) finite-dimensional reductive Lie algebra, the representation theory of $A$ is intimately tied to the representation theory of the $A$-subquotient known as the reduction algebra for $(A,\\mathfrak{g}, \\varphi)$. Herlemont and Ogievetsky studied differential reduction algebras for the general linear Lie algebra $\\mathfrak{gl}(n)$ as the algebra of $h$-deformed differential operators formed from realizations of $\\mathfrak{gl}(n)$ in the $N$-fold tensor product of the $n $th Weyl algebra.","In this paper, we further the study of differential reduction algebras by presenting the symplectic differential reduction algebra $D\\big(\\mathfrak{sp}(4)\\big)$, by generators and relations, and showing its connections to Bavula's generalized Weyl algebras (GWAs).","In doing so, we determine a new class of GWAs we call $\\textit{skew-affine}$ GWAs, of which $D\\big(\\mathfrak{gl}(2)\\big)$ and $D\\big(\\mathfrak{sp}(4)\\big)$ are examples.","We conjecture that the differential reduction algebra of the orthosymplectic Lie superalgebra $\\mathfrak{osp}(1|2n)$ is a twisted generalized Weyl algebra (TGWA) and that the relations for $D\\big(\\mathfrak{sp}(2n)\\big)$ yield solutions to the dynamical Yang-Baxter equation (DYBE)."],"url":"http://arxiv.org/abs/2403.15968v1","category":"math.RT"}
{"created":"2024-03-23 23:56:17","title":"Optimal lower estimate for the first eigenvalue of the p-Laplacian","abstract":"An integral inequality is derived for compact submanifolds (with or without boundary) in the unit sphere. This result leads to a characterization of spheres.","sentences":["An integral inequality is derived for compact submanifolds (with or without boundary) in the unit sphere.","This result leads to a characterization of spheres."],"url":"http://arxiv.org/abs/2403.15964v1","category":"math.DG"}
{"created":"2024-03-23 23:51:55","title":"Homogenization of nonconvex viscous Hamilton-Jacobi equations in stationary ergodic media in one dimension","abstract":"We establish homogenization for nondegenerate viscous Hamilton-Jacobi equations in one space dimension when the diffusion coefficient $a(x,\\omega) > 0$ and the Hamiltonian $H(p,x,\\omega)$ are general stationary ergodic processes in $x$. Our result is valid under mild regularity assumptions on $a$ and $H$ plus standard coercivity and growth assumptions (in $p$) on the latter. In particular, we impose neither any additional condition on the law of the media nor any shape restriction on the graph of $p\\mapsto H(p,x,\\omega)$. Our approach consists of two main steps: (i) constructing a suitable candidate $\\overline{H}$ for the effective Hamiltonian; (ii) proving homogenization. In the first step, we work with the set $E$ of all points at which $\\overline{H}$ is naturally determined by correctors with stationary derivatives. We prove that $E$ is a closed subset of $\\mathbb{R}$ that is unbounded from above and below, and, if $E\\neq\\mathbb{R}$, then $\\overline{H}$ can be extended continuously to $\\mathbb{R}$ by setting it to be constant on each connected component of $E^c$. In the second step, we use a key bridging lemma, comparison arguments and several general results to verify that homogenization holds with this $\\overline{H}$ as the effective Hamiltonian.","sentences":["We establish homogenization for nondegenerate viscous Hamilton-Jacobi equations in one space dimension when the diffusion coefficient $a(x,\\omega) > 0$ and the Hamiltonian $H(p,x,\\omega)$ are general stationary ergodic processes in $x$.","Our result is valid under mild regularity assumptions on $a$ and $H$ plus standard coercivity and growth assumptions (in $p$) on the latter.","In particular, we impose neither any additional condition on the law of the media nor any shape restriction on the graph of $p\\mapsto H(p,x,\\omega)$. Our approach consists of two main steps: (i) constructing a suitable candidate $\\overline{H}$ for the effective Hamiltonian; (ii) proving homogenization.","In the first step, we work with the set $E$ of all points at which $\\overline{H}$ is naturally determined by correctors with stationary derivatives.","We prove that $E$ is a closed subset of $\\mathbb{R}$ that is unbounded from above and below, and, if $E\\neq\\mathbb{R}$, then $\\overline{H}$ can be extended continuously to $\\mathbb{R}$ by setting it to be constant on each connected component of $E^c$. In the second step, we use a key bridging lemma, comparison arguments and several general results to verify that homogenization holds with this $\\overline{H}$ as the effective Hamiltonian."],"url":"http://arxiv.org/abs/2403.15963v1","category":"math.AP"}
{"created":"2024-03-23 23:27:37","title":"Convection-Enabled Boundary Control of a 2D Channel Flow","abstract":"We consider the incompressible Navier-Stokes equations in a two-dimensional channel. The tangential and normal velocities are assumed to be periodic in the streamwise (horizontal) direction. Moreover, we consider no-slip boundary conditions on the tangential velocity at the top and bottom walls of the channel, and normal velocity actuation at the top and bottom walls. For an arbitrarily large Reynolds number, we design the boundary control inputs to achieve global exponential stabilization, in the L2 sense, of a chosen parabolic Poiseuille profile. Moreover, we design the control inputs such that they have zero mean, but non-zero cubic mean. The zero-mean property is to ensure that the conservation of mass constraint is verified. The non-zero cubic mean property is the key to exploiting the stabilizing effect of nonlinear convection and achieving global stabilization independently of the size of the Reynolds number. This paper is not only the first work where a closed-form feedback law is proposed for global stabilization of parabolic Poiseuille profiles for arbitrary Reynolds number but is also the first generalization of the Cardano-Lyapunov formula, designed initially to stabilize scalar-valued convective PDEs, to a vector-valued convective PDE with a divergence-free constraint on the state.","sentences":["We consider the incompressible Navier-Stokes equations in a two-dimensional channel.","The tangential and normal velocities are assumed to be periodic in the streamwise (horizontal) direction.","Moreover, we consider no-slip boundary conditions on the tangential velocity at the top and bottom walls of the channel, and normal velocity actuation at the top and bottom walls.","For an arbitrarily large Reynolds number, we design the boundary control inputs to achieve global exponential stabilization, in the L2 sense, of a chosen parabolic Poiseuille profile.","Moreover, we design the control inputs such that they have zero mean, but non-zero cubic mean.","The zero-mean property is to ensure that the conservation of mass constraint is verified.","The non-zero cubic mean property is the key to exploiting the stabilizing effect of nonlinear convection and achieving global stabilization independently of the size of the Reynolds number.","This paper is not only the first work where a closed-form feedback law is proposed for global stabilization of parabolic Poiseuille profiles for arbitrary Reynolds number but is also the first generalization of the Cardano-Lyapunov formula, designed initially to stabilize scalar-valued convective PDEs, to a vector-valued convective PDE with a divergence-free constraint on the state."],"url":"http://arxiv.org/abs/2403.15958v1","category":"eess.SY"}
{"created":"2024-03-23 23:06:32","title":"IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models","abstract":"The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially degrade the performance of GeminiPro on the localization task. Tangentially, we discover a potential weakness in the ICL capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a few-shot example.","sentences":["The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language.","Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning.","This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable?","To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization.","GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought).","Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization.","We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially degrade the performance of GeminiPro on the localization task.","Tangentially, we discover a potential weakness in the ICL capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a few-shot example."],"url":"http://arxiv.org/abs/2403.15952v1","category":"cs.CV"}
{"created":"2024-03-23 22:53:21","title":"Deep Probabilistic Direction Prediction in 3D with Applications to Directional Dark Matter Detectors","abstract":"We present the first method to probabilistically predict 3D direction in a deep neural network model. The probabilistic predictions are modeled as a heteroscedastic von Mises-Fisher distribution on the sphere $\\mathbb{S}^2$, giving a simple way to quantify aleatoric uncertainty. This approach generalizes the cosine distance loss which is a special case of our loss function when the uncertainty is assumed to be uniform across samples. We develop approximations required to make the likelihood function and gradient calculations stable. The method is applied to the task of predicting the 3D directions of electrons, the most complex signal in a class of experimental particle physics detectors designed to demonstrate the particle nature of dark matter and study solar neutrinos. Using simulated Monte Carlo data, the initial direction of recoiling electrons is inferred from their tortuous trajectories, as captured by the 3D detectors. For $40\\,$keV electrons in a $70\\%$ $\\textrm{He}$ $30 \\%$ $\\textrm{CO}_2$ gas mixture at STP, the new approach achieves a mean cosine distance of $0.104$ ($26^\\circ$) compared to $0.556$ ($64^\\circ$) achieved by a non-machine learning algorithm. We show that the model is well-calibrated and accuracy can be increased further by removing samples with high predicted uncertainty. This advancement in probabilistic 3D directional learning could increase the sensitivity of directional dark matter detectors.","sentences":["We present the first method to probabilistically predict 3D direction in a deep neural network model.","The probabilistic predictions are modeled as a heteroscedastic von Mises-Fisher distribution on the sphere $\\mathbb{S}^2$, giving a simple way to quantify aleatoric uncertainty.","This approach generalizes the cosine distance loss which is a special case of our loss function when the uncertainty is assumed to be uniform across samples.","We develop approximations required to make the likelihood function and gradient calculations stable.","The method is applied to the task of predicting the 3D directions of electrons, the most complex signal in a class of experimental particle physics detectors designed to demonstrate the particle nature of dark matter and study solar neutrinos.","Using simulated Monte Carlo data, the initial direction of recoiling electrons is inferred from their tortuous trajectories, as captured by the 3D detectors.","For $40\\,$keV electrons in a $70\\%$ $\\textrm{He}$ $30 \\%$ $\\textrm{CO}_2$ gas mixture at STP, the new approach achieves a mean cosine distance of $0.104$ ($26^\\circ$) compared to $0.556$ ($64^\\circ$) achieved by a non-machine learning algorithm.","We show that the model is well-calibrated and accuracy can be increased further by removing samples with high predicted uncertainty.","This advancement in probabilistic 3D directional learning could increase the sensitivity of directional dark matter detectors."],"url":"http://arxiv.org/abs/2403.15949v1","category":"hep-ex"}
{"created":"2024-03-23 21:28:42","title":"Debiased Machine Learning when Nuisance Parameters Appear in Indicator Functions","abstract":"This paper studies debiased machine learning when nuisance parameters appear in indicator functions. An important example is maximized average welfare under optimal treatment assignment rules. For asymptotically valid inference for a parameter of interest, the current literature on debiased machine learning relies on Gateaux differentiability of the functions inside moment conditions, which does not hold when nuisance parameters appear in indicator functions. In this paper, we propose smoothing the indicator functions, and develop an asymptotic distribution theory for this class of models. The asymptotic behavior of the proposed estimator exhibits a trade-off between bias and variance due to smoothing. We study how a parameter which controls the degree of smoothing can be chosen optimally to minimize an upper bound of the asymptotic mean squared error. A Monte Carlo simulation supports the asymptotic distribution theory, and an empirical example illustrates the implementation of the method.","sentences":["This paper studies debiased machine learning when nuisance parameters appear in indicator functions.","An important example is maximized average welfare under optimal treatment assignment rules.","For asymptotically valid inference for a parameter of interest, the current literature on debiased machine learning relies on Gateaux differentiability of the functions inside moment conditions, which does not hold when nuisance parameters appear in indicator functions.","In this paper, we propose smoothing the indicator functions, and develop an asymptotic distribution theory for this class of models.","The asymptotic behavior of the proposed estimator exhibits a trade-off between bias and variance due to smoothing.","We study how a parameter which controls the degree of smoothing can be chosen optimally to minimize an upper bound of the asymptotic mean squared error.","A Monte Carlo simulation supports the asymptotic distribution theory, and an empirical example illustrates the implementation of the method."],"url":"http://arxiv.org/abs/2403.15934v1","category":"econ.EM"}
{"created":"2024-03-23 20:17:06","title":"Altered patterning of neural activity in a tauopathy mouse model","abstract":"Alzheimer's disease (AD) is a complex neurodegenerative condition that manifests at multiple levels and involves a spectrum of abnormalities ranging from the cellular to cognitive. Here, we investigate the impact of AD-related tau-pathology on hippocampal circuits in mice engaged in spatial navigation, and study changes of neuronal firing and dynamics of extracellular fields. While most studies are based on analyzing instantaneous or time-averaged characteristics of neuronal activity, we focus on intermediate timescales -- spike trains and waveforms of oscillatory potentials, which we consider as single entities. We find that, in healthy mice, spike arrangements and wave patterns (series of crests or troughs) are coupled to the animal's location, speed, and acceleration. In contrast, in tau-mice, neural activity is structurally disarrayed: brainwave cadence is detached from locomotion, spatial selectivity is lost, the spike flow is scrambled. Importantly, these alterations start early and accumulate with age, which exposes progressive disinvolvement the hippocampus circuit in spatial navigation. These features highlight qualitatively different neurodynamics than the ones provided by conventional analyses, and are more salient, thus revealing a new level of the hippocampal circuit disruptions.","sentences":["Alzheimer's disease (AD) is a complex neurodegenerative condition that manifests at multiple levels and involves a spectrum of abnormalities ranging from the cellular to cognitive.","Here, we investigate the impact of AD-related tau-pathology on hippocampal circuits in mice engaged in spatial navigation, and study changes of neuronal firing and dynamics of extracellular fields.","While most studies are based on analyzing instantaneous or time-averaged characteristics of neuronal activity, we focus on intermediate timescales -- spike trains and waveforms of oscillatory potentials, which we consider as single entities.","We find that, in healthy mice, spike arrangements and wave patterns (series of crests or troughs) are coupled to the animal's location, speed, and acceleration.","In contrast, in tau-mice, neural activity is structurally disarrayed: brainwave cadence is detached from locomotion, spatial selectivity is lost, the spike flow is scrambled.","Importantly, these alterations start early and accumulate with age, which exposes progressive disinvolvement the hippocampus circuit in spatial navigation.","These features highlight qualitatively different neurodynamics than the ones provided by conventional analyses, and are more salient, thus revealing a new level of the hippocampal circuit disruptions."],"url":"http://arxiv.org/abs/2403.15926v1","category":"q-bio.NC"}
{"created":"2024-03-23 19:36:21","title":"On Merton's Optimal Portfolio Problem under Sporadic Bankruptcy","abstract":"Consider a stock market following a geometric Brownian motion and a riskless asset continuously compounded at a constant rate. Assuming the stock can go bankrupt, i.e., lose all of its value, at some exogenous random time (independent of the stock price) modeled as the first arrival time of a homogeneous Poisson process, we study the Merton's optimal portfolio problem consisting of maximizing the expected logarithmic utility of the total wealth at a preselected finite maturity time. First, we present a heuristic derivation based on a new type of Hamilton-Jacobi-Bellman equation. Then, we formally reduce the problem to a classical controlled Markovian diffusion with a new type of terminal and running costs. A new version of Merton's ratio is rigorously derived using Bellman's dynamic programming principle and validated with a suitable type of verification theorem. A real-world example comparing the latter ratio to the classical Merton's ratio is given.","sentences":["Consider a stock market following a geometric Brownian motion and a riskless asset continuously compounded at a constant rate.","Assuming the stock can go bankrupt, i.e., lose all of its value, at some exogenous random time (independent of the stock price) modeled as the first arrival time of a homogeneous Poisson process, we study the Merton's optimal portfolio problem consisting of maximizing the expected logarithmic utility of the total wealth at a preselected finite maturity time.","First, we present a heuristic derivation based on a new type of Hamilton-Jacobi-Bellman equation.","Then, we formally reduce the problem to a classical controlled Markovian diffusion with a new type of terminal and running costs.","A new version of Merton's ratio is rigorously derived using Bellman's dynamic programming principle and validated with a suitable type of verification theorem.","A real-world example comparing the latter ratio to the classical Merton's ratio is given."],"url":"http://arxiv.org/abs/2403.15923v1","category":"q-fin.MF"}
{"created":"2024-03-23 19:01:55","title":"The automorphisms of differential extensions of characteristic $p$","abstract":"Nonassociative differential extensions are generalizations of associative differential extensions, either of a purely inseparable field extension $K$ of exponent one of a field $F$, $F$ of characteristic $p$, or of a central division algebra over a purely inseparable field extension of $F$. Associative differential extensions are well known central simple algebras first defined by Amitsur and Jacobson. We explicitly compute the automorphisms of nonassociative differential extensions. These are canonically obtained by restricting automorphisms of the differential polynomial ring used in the construction of the algebra. In particular, we obtain descriptions for the automorphisms of associative differential extensions of $D$ and $K$, which are known to be inner.","sentences":["Nonassociative differential extensions are generalizations of associative differential extensions, either of a purely inseparable field extension $K$ of exponent one of a field $F$, $F$ of characteristic $p$, or of a central division algebra over a purely inseparable field extension of $F$. Associative differential extensions are well known central simple algebras first defined by Amitsur and Jacobson.","We explicitly compute the automorphisms of nonassociative differential extensions.","These are canonically obtained by restricting automorphisms of the differential polynomial ring used in the construction of the algebra.","In particular, we obtain descriptions for the automorphisms of associative differential extensions of $D$ and $K$, which are known to be inner."],"url":"http://arxiv.org/abs/2403.15914v1","category":"math.RA"}
{"created":"2024-03-23 18:59:55","title":"GPU-accelerated nonlinear model predictive control with ExaModels and MadNLP","abstract":"We investigate the potential of Graphics Processing Units (GPUs) to solve large-scale nonlinear model predictive control (NMPC) problems. We accelerate the solution of the constrained nonlinear programs in the NMPC algorithm using the GPU-accelerated automatic differentiation tool ExaModels with the interior-point solver MadNLP. The sparse linear systems formulated in the interior-point method is solved on the GPU using a hybrid solver combining an iterative method with a sparse Cholesky factorization, which harness the newly released NVIDIA cuDSS solver. Our results on the classical distillation column instance show that despite a significant pre-processing time, the hybrid solver allows to reduce the time per iteration by a factor of 25 for the largest instance.","sentences":["We investigate the potential of Graphics Processing Units (GPUs) to solve large-scale nonlinear model predictive control (NMPC) problems.","We accelerate the solution of the constrained nonlinear programs in the NMPC algorithm using the GPU-accelerated automatic differentiation tool ExaModels with the interior-point solver MadNLP.","The sparse linear systems formulated in the interior-point method is solved on the GPU using a hybrid solver combining an iterative method with a sparse Cholesky factorization, which harness the newly released NVIDIA cuDSS solver.","Our results on the classical distillation column instance show that despite a significant pre-processing time, the hybrid solver allows to reduce the time per iteration by a factor of 25 for the largest instance."],"url":"http://arxiv.org/abs/2403.15913v1","category":"math.OC"}
{"created":"2024-03-23 18:42:22","title":"Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search","abstract":"Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and probabilistic models. During our tests, we place particular emphasis on the robustness of the learned policies with respect to noisy initial states.","sentences":["Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples.","Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions.","We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting.","We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs.","We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and probabilistic models.","During our tests, we place particular emphasis on the robustness of the learned policies with respect to noisy initial states."],"url":"http://arxiv.org/abs/2403.15908v1","category":"cs.LG"}
{"created":"2024-03-23 18:21:01","title":"The breadth of Berikashvili's functor D","abstract":"We discuss variants of Berikashvili's functor that arise in differential homological algebra, from simplicial bundles, from ordinary topological bundles, and in more general categorical settings. We prove that, under suitable circumstances, the value of Berikasvili's functor parametrizes isomorphism classes of bundles in various contexts.","sentences":["We discuss variants of Berikashvili's functor that arise in differential homological algebra, from simplicial bundles, from ordinary topological bundles, and in more general categorical settings.","We prove that, under suitable circumstances, the value of Berikasvili's functor parametrizes isomorphism classes of bundles in various contexts."],"url":"http://arxiv.org/abs/2403.15906v1","category":"math.QA"}
{"created":"2024-03-23 17:38:51","title":"A Deep Learning Architectures for Kidney Disease Classification","abstract":"Deep learning has become an extremely powerful tool for complex tasks such as image classification and segmentation. The medical industry often lacks high-quality, balanced datasets, which can be a challenge for deep learning algorithms that need sufficiently large amounts of data to train and increase their performance. This is especially important in the context of kidney issues such as for stones, cysts and tumors. We used deep learning models for this study to classify or detect several types of kidney diseases. We use different classification models, such as VGG-19, (CNNs) Convolutional Neural Networks, ResNet-101, VGG-16, ResNet-50, and DenseNet-169, which can be enhanced through techniques such as classification, segmentation, and transfer learning. These algorithms can help improve model accuracy by allowing them to learn from multiple datasets. This technique has the potential to revolutionize the diagnosis and treatment of kidney problems as it enables more accurate and effective classification of CT-scan images. This may ultimately lead to better patient outcomes and improved overall health outcomes.","sentences":["Deep learning has become an extremely powerful tool for complex tasks such as image classification and segmentation.","The medical industry often lacks high-quality, balanced datasets, which can be a challenge for deep learning algorithms that need sufficiently large amounts of data to train and increase their performance.","This is especially important in the context of kidney issues such as for stones, cysts and tumors.","We used deep learning models for this study to classify or detect several types of kidney diseases.","We use different classification models, such as VGG-19, (CNNs) Convolutional Neural Networks, ResNet-101, VGG-16, ResNet-50, and DenseNet-169, which can be enhanced through techniques such as classification, segmentation, and transfer learning.","These algorithms can help improve model accuracy by allowing them to learn from multiple datasets.","This technique has the potential to revolutionize the diagnosis and treatment of kidney problems as it enables more accurate and effective classification of CT-scan images.","This may ultimately lead to better patient outcomes and improved overall health outcomes."],"url":"http://arxiv.org/abs/2403.15895v1","category":"eess.IV"}
{"created":"2024-03-23 17:07:05","title":"Higher codimension area-minimizing currents mod$(q)$: structure of singularities near $(m-1)$-invariant cones","abstract":"We study finer properties related to the interior regularity of $m$-dimensional area minimizing currents mod$(q)$ in arbitrary codimension. We show that the set of points where at least one tangent cone is translation invariant along $m-1$ directions is locally a connected $C^{1,\\beta}$ submanifold, and moreover such points have unique tangent cones. We establish these results as consequences of a fine excess decay theorem.","sentences":["We study finer properties related to the interior regularity of $m$-dimensional area minimizing currents mod$(q)$ in arbitrary codimension.","We show that the set of points where at least one tangent cone is translation invariant along $m-1$ directions is locally a connected $C^{1,\\beta}$ submanifold, and moreover such points have unique tangent cones.","We establish these results as consequences of a fine excess decay theorem."],"url":"http://arxiv.org/abs/2403.15889v1","category":"math.AP"}
{"created":"2024-03-23 16:59:24","title":"The $L^p$-spectrum of the Laplacian on forms over warped products and Kleinian groups","abstract":"In this article, we generalize the set of manifolds over which the $L^p$-spectrum of the Laplacian on $k$-forms depends on $p$. We will consider the case of manifolds that are warped products at infinity and certain quotients of Hyperbolic space. In the case of warped products at infinity we prove that the $L^p$-spectrum of the Laplacian on $k$-forms contains a parabolic region which depends on $k$, $p$ and the limiting curvature $a_0$ at infinity. For $M=\\mathbb{H}^{N+1}/\\Gamma $ with $\\Gamma$ a geometrically finite group such that $M$ has infinite volume and no cusps, we prove that the $L^p$-spectrum of the Laplacian on $k$-forms is a exactly a parabolic region together with a set of isolated eigenvalues on the real line.","sentences":["In this article, we generalize the set of manifolds over which the $L^p$-spectrum of the Laplacian on $k$-forms depends on $p$. We will consider the case of manifolds that are warped products at infinity and certain quotients of Hyperbolic space.","In the case of warped products at infinity we prove that the $L^p$-spectrum of the Laplacian on $k$-forms contains a parabolic region which depends on $k$, $p$ and the limiting curvature $a_0$ at infinity.","For $M=\\mathbb{H}^{N+1}/\\Gamma $ with $\\Gamma$ a geometrically finite group such that $M$ has infinite volume and no cusps, we prove that the $L^p$-spectrum of the Laplacian on $k$-forms is a exactly a parabolic region together with a set of isolated eigenvalues on the real line."],"url":"http://arxiv.org/abs/2403.15888v1","category":"math.DG"}
{"created":"2024-03-23 16:19:21","title":"Semiclassical Limit of the Bogoliubov-de Gennes Equation","abstract":"In this paper, we rewrite the time-dependent Bogoliubov$\\unicode{x2013}$de Gennes equation in an appropriate semiclassical form and establish its semiclassical limit to a two-particle kinetic transport equation with an effective mean-field background potential satisfying the one-particle Vlasov equation. Moreover, for some semiclassical regimes, we obtain a higher-order correction to the two-particle kinetic transport equation, capturing a nontrivial two-body interaction effect. The convergence is proven for $C^2$ interaction potentials in terms of a semiclassical optimal transport pseudo-metric. Furthermore, combining our current results with the results of Marcantoni et al. [arXiv:2310.15280], we establish a joint semiclassical and mean-field approximation of the dynamics of a system of spin-$\\frac{1}{2}$ Fermions by the Vlasov equation in some negative order Sobolev topology.","sentences":["In this paper, we rewrite the time-dependent Bogoliubov$\\unicode{x2013}$de Gennes equation in an appropriate semiclassical form and establish its semiclassical limit to a two-particle kinetic transport equation with an effective mean-field background potential satisfying the one-particle Vlasov equation.","Moreover, for some semiclassical regimes, we obtain a higher-order correction to the two-particle kinetic transport equation, capturing a nontrivial two-body interaction effect.","The convergence is proven for $C^2$ interaction potentials in terms of a semiclassical optimal transport pseudo-metric.","Furthermore, combining our current results with the results of Marcantoni et al.","[arXiv:2310.15280], we establish a joint semiclassical and mean-field approximation of the dynamics of a system of spin-$\\frac{1}{2}$ Fermions by the Vlasov equation in some negative order Sobolev topology."],"url":"http://arxiv.org/abs/2403.15880v1","category":"math-ph"}
{"created":"2024-03-23 15:35:54","title":"iA$^*$: Imperative Learning-based A$^*$ Search for Pathfinding","abstract":"The pathfinding problem, which aims to identify a collision-free path between two points, is crucial for many applications, such as robot navigation and autonomous driving. Classic methods, such as A$^*$ search, perform well on small-scale maps but face difficulties scaling up. Conversely, data-driven approaches can improve pathfinding efficiency but require extensive data labeling and lack theoretical guarantees, making it challenging for practical applications. To combine the strengths of the two methods, we utilize the imperative learning (IL) strategy and propose a novel self-supervised pathfinding framework, termed imperative learning-based A$^*$ (iA$^*$). Specifically, iA$^*$ is a bilevel optimization process where the lower-level optimization is dedicated to finding the optimal path by a differentiable A$^*$ search module, and the upper-level optimization narrows down the search space to improve efficiency via setting suitable initial values from a data-driven model. Besides, the model within the upper-level optimization is a fully convolutional network, trained by the calculated loss in the lower-level optimization. Thus, the framework avoids extensive data labeling and can be applied in diverse environments. Our comprehensive experiments demonstrate that iA$^*$ surpasses both classical and data-driven methods in pathfinding efficiency and shows superior robustness among different tasks, validated with public datasets and simulation environments.","sentences":["The pathfinding problem, which aims to identify a collision-free path between two points, is crucial for many applications, such as robot navigation and autonomous driving.","Classic methods, such as A$^*$ search, perform well on small-scale maps but face difficulties scaling up.","Conversely, data-driven approaches can improve pathfinding efficiency but require extensive data labeling and lack theoretical guarantees, making it challenging for practical applications.","To combine the strengths of the two methods, we utilize the imperative learning (IL) strategy and propose a novel self-supervised pathfinding framework, termed imperative learning-based A$^*$ (iA$^*$).","Specifically, iA$^*$ is a bilevel optimization process where the lower-level optimization is dedicated to finding the optimal path by a differentiable A$^*$ search module, and the upper-level optimization narrows down the search space to improve efficiency via setting suitable initial values from a data-driven model.","Besides, the model within the upper-level optimization is a fully convolutional network, trained by the calculated loss in the lower-level optimization.","Thus, the framework avoids extensive data labeling and can be applied in diverse environments.","Our comprehensive experiments demonstrate that iA$^*$ surpasses both classical and data-driven methods in pathfinding efficiency and shows superior robustness among different tasks, validated with public datasets and simulation environments."],"url":"http://arxiv.org/abs/2403.15870v1","category":"cs.RO"}
{"created":"2024-03-23 15:31:12","title":"Change of the sign of the Hubble parameter and its stability in higher-order torsion gravity","abstract":"We explore the change in the sign of the Hubble parameter in the early universe and its stability in higher-order torsion gravity. Our study explores the various scenarios involving the sub-relativistic universe, radiation universe, ultra-relativistic universe, dust universe, and stiff fluid universe. Different analytical methods, including power-law, exponential scalar factor, and hybrid scale factor methods, are employed to examine the behavior of the universe about the equation of state (EoS) parameters. This study is based on the previous ones by presenting a more comprehensive analysis of the bouncing scenarios within the higher-order torsion gravity framework. It makes significant progress in reconstructing gravitational Lagrangians, which are tailored to specific parameter values, allowing for a thorough examination of the energy conditions necessary for successful bouncing models. These derived Lagrangians provide analytical solutions for a range of bouncing models, including symmetric bounce, super-bounce, oscillatory bounce, matter bounce, and exponential bouncing settings. The presence of exotic matter is responsible for the accelerated expansion of the universe, as it exhibits substantial negative pressure. A thorough analysis of torsion-based gravity theories and the specific investigations into bouncing scenarios set it apart from previous works exploring alternative gravity theories and their cosmological consequences. These contributions lie in the comprehensive exploration of bouncing models and the detailed examination of the energy conditions in higher-order torsion gravity.","sentences":["We explore the change in the sign of the Hubble parameter in the early universe and its stability in higher-order torsion gravity.","Our study explores the various scenarios involving the sub-relativistic universe, radiation universe, ultra-relativistic universe, dust universe, and stiff fluid universe.","Different analytical methods, including power-law, exponential scalar factor, and hybrid scale factor methods, are employed to examine the behavior of the universe about the equation of state (EoS) parameters.","This study is based on the previous ones by presenting a more comprehensive analysis of the bouncing scenarios within the higher-order torsion gravity framework.","It makes significant progress in reconstructing gravitational Lagrangians, which are tailored to specific parameter values, allowing for a thorough examination of the energy conditions necessary for successful bouncing models.","These derived Lagrangians provide analytical solutions for a range of bouncing models, including symmetric bounce, super-bounce, oscillatory bounce, matter bounce, and exponential bouncing settings.","The presence of exotic matter is responsible for the accelerated expansion of the universe, as it exhibits substantial negative pressure.","A thorough analysis of torsion-based gravity theories and the specific investigations into bouncing scenarios set it apart from previous works exploring alternative gravity theories and their cosmological consequences.","These contributions lie in the comprehensive exploration of bouncing models and the detailed examination of the energy conditions in higher-order torsion gravity."],"url":"http://arxiv.org/abs/2403.15867v1","category":"gr-qc"}
{"created":"2024-03-23 15:23:45","title":"Localized structures in three-field models: geometrically constrained configurations and the first-order framework","abstract":"This work deals with models described by three real scalar fields in one spatial dimension. We study the case where two of the three fields engender kinematical modifications, which respond as geometrical constrictions, that can be used to change the center and/or the tails of the kinklike configurations. An important advantage of our procedure is the construction of a method for the obtention of first-order differential equations that solve the equations of motion and give rise to stable localized structures. We illustrate the general procedure investigating several distinct examples, and suggesting some possibilities of applications of practical use, in particular, to the case of domain walls and skyrmions in magnetic material, collisions of kinks and to deal with braneworld scenarios having a warped five-dimensional anti de Sitter geometry, with a single extra dimension of infinite extent.","sentences":["This work deals with models described by three real scalar fields in one spatial dimension.","We study the case where two of the three fields engender kinematical modifications, which respond as geometrical constrictions, that can be used to change the center and/or the tails of the kinklike configurations.","An important advantage of our procedure is the construction of a method for the obtention of first-order differential equations that solve the equations of motion and give rise to stable localized structures.","We illustrate the general procedure investigating several distinct examples, and suggesting some possibilities of applications of practical use, in particular, to the case of domain walls and skyrmions in magnetic material, collisions of kinks and to deal with braneworld scenarios having a warped five-dimensional anti de Sitter geometry, with a single extra dimension of infinite extent."],"url":"http://arxiv.org/abs/2403.15865v1","category":"hep-th"}
{"created":"2024-03-23 14:16:26","title":"An edge detection-based deep learning approach for tear meniscus height measurement","abstract":"Automatic measurements of tear meniscus height (TMH) have been achieved by using deep learning techniques; however, annotation is significantly influenced by subjective factors and is both time-consuming and labor-intensive. In this paper, we introduce an automatic TMH measurement technique based on edge detection-assisted annotation within a deep learning framework. This method generates mask labels less affected by subjective factors with enhanced efficiency compared to previous annotation approaches. For improved segmentation of the pupil and tear meniscus areas, the convolutional neural network Inceptionv3 was first implemented as an image quality assessment model, effectively identifying higher-quality images with an accuracy of 98.224%. Subsequently, by using the generated labels, various algorithms, including Unet, ResUnet, Deeplabv3+FcnResnet101, Deeplabv3+FcnResnet50, FcnResnet50, and FcnResnet101 were trained, with Unet demonstrating the best performance. Finally, Unet was used for automatic pupil and tear meniscus segmentation to locate the center of the pupil and calculate TMH,respectively. An evaluation of the mask quality predicted by Unet indicated a Mean Intersection over Union of 0.9362, a recall of 0.9261, a precision of 0.9423, and an F1-Score of 0.9326. Additionally, the TMH predicted by the model was assessed, with the fitting curve represented as y= 0.982x-0.862, an overall correlation coefficient of r^2=0.961 , and an accuracy of 94.80% (237/250). In summary, the algorithm can automatically screen images based on their quality,segment the pupil and tear meniscus areas, and automatically measure TMH. Measurement results using the AI algorithm demonstrate a high level of consistency with manual measurements, offering significant support to clinical doctors in diagnosing dry eye disease.","sentences":["Automatic measurements of tear meniscus height (TMH) have been achieved by using deep learning techniques; however, annotation is significantly influenced by subjective factors and is both time-consuming and labor-intensive.","In this paper, we introduce an automatic TMH measurement technique based on edge detection-assisted annotation within a deep learning framework.","This method generates mask labels less affected by subjective factors with enhanced efficiency compared to previous annotation approaches.","For improved segmentation of the pupil and tear meniscus areas, the convolutional neural network Inceptionv3 was first implemented as an image quality assessment model, effectively identifying higher-quality images with an accuracy of 98.224%.","Subsequently, by using the generated labels, various algorithms, including Unet, ResUnet, Deeplabv3+FcnResnet101, Deeplabv3+FcnResnet50, FcnResnet50, and FcnResnet101 were trained, with Unet demonstrating the best performance.","Finally, Unet was used for automatic pupil and tear meniscus segmentation to locate the center of the pupil and calculate TMH,respectively.","An evaluation of the mask quality predicted by Unet indicated a Mean Intersection over Union of 0.9362, a recall of 0.9261, a precision of 0.9423, and an F1-Score of 0.9326.","Additionally, the TMH predicted by the model was assessed, with the fitting curve represented as y= 0.982x-0.862, an overall correlation coefficient of r^2=0.961 , and an accuracy of 94.80% (237/250).","In summary, the algorithm can automatically screen images based on their quality,segment the pupil and tear meniscus areas, and automatically measure TMH.","Measurement results using the AI algorithm demonstrate a high level of consistency with manual measurements, offering significant support to clinical doctors in diagnosing dry eye disease."],"url":"http://arxiv.org/abs/2403.15853v1","category":"eess.IV"}
{"created":"2024-03-23 13:52:16","title":"Inpainting-Driven Mask Optimization for Object Removal","abstract":"This paper proposes a mask optimization method for improving the quality of object removal using image inpainting. While many inpainting methods are trained with a set of random masks, a target for inpainting may be an object, such as a person, in many realistic scenarios. This domain gap between masks in training and inference images increases the difficulty of the inpainting task. In our method, this domain gap is resolved by training the inpainting network with object masks extracted by segmentation, and such object masks are also used in the inference step. Furthermore, to optimize the object masks for inpainting, the segmentation network is connected to the inpainting network and end-to-end trained to improve the inpainting performance. The effect of this end-to-end training is further enhanced by our mask expansion loss for achieving the trade-off between large and small masks. Experimental results demonstrate the effectiveness of our method for better object removal using image inpainting.","sentences":["This paper proposes a mask optimization method for improving the quality of object removal using image inpainting.","While many inpainting methods are trained with a set of random masks, a target for inpainting may be an object, such as a person, in many realistic scenarios.","This domain gap between masks in training and inference images increases the difficulty of the inpainting task.","In our method, this domain gap is resolved by training the inpainting network with object masks extracted by segmentation, and such object masks are also used in the inference step.","Furthermore, to optimize the object masks for inpainting, the segmentation network is connected to the inpainting network and end-to-end trained to improve the inpainting performance.","The effect of this end-to-end training is further enhanced by our mask expansion loss for achieving the trade-off between large and small masks.","Experimental results demonstrate the effectiveness of our method for better object removal using image inpainting."],"url":"http://arxiv.org/abs/2403.15849v1","category":"cs.CV"}
{"created":"2024-03-23 13:51:25","title":"$f(R, T)$ Gravity Bouncing Universe with Cosmological Parameters","abstract":"The basic aim of this manuscript is to investigate the cosmological solutions in the context of the modified $f(R, T)$ theory of gravity, where $R$ is the Ricci scalar and $T$ is the trace of the energy-momentum tensor. For our current work, we consider the Friedmann-Robertson-Walker space-time for finding the solutions of field equations. We investigate the nature of universe by considering acceleration expansion of universe, ultra relativistic universe, sub-relativistic universe, dust universe, radiation universe, stiff universe. Moreover, we apply the power law technique by taking two different $f(R, T)$ gravity models to observe the expanding nature of the universe. The bouncing scenario is also discussed by choosing some particular values of the model parameters and observed the energy conditions, which are satisfied for a successful bouncing model. It is also concluded that some solutions in $f(R, T)$ theory of gravity supports the concept of exotic matter and accelerated expansion of the universe due to a large amount of negative pressure.","sentences":["The basic aim of this manuscript is to investigate the cosmological solutions in the context of the modified $f(R, T)$ theory of gravity, where $R$ is the Ricci scalar and $T$ is the trace of the energy-momentum tensor.","For our current work, we consider the Friedmann-Robertson-Walker space-time for finding the solutions of field equations.","We investigate the nature of universe by considering acceleration expansion of universe, ultra relativistic universe, sub-relativistic universe, dust universe, radiation universe, stiff universe.","Moreover, we apply the power law technique by taking two different $f(R, T)$ gravity models to observe the expanding nature of the universe.","The bouncing scenario is also discussed by choosing some particular values of the model parameters and observed the energy conditions, which are satisfied for a successful bouncing model.","It is also concluded that some solutions in $f(R, T)$ theory of gravity supports the concept of exotic matter and accelerated expansion of the universe due to a large amount of negative pressure."],"url":"http://arxiv.org/abs/2403.15847v1","category":"gr-qc"}
{"created":"2024-03-23 13:28:37","title":"TablePuppet: A Generic Framework for Relational Federated Learning","abstract":"Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables?   In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table. TablePuppet incorporates computation/communication optimizations to deal with the duplicate tuples introduced by joins, as well as differential privacy (DP) to protect against both feature and label leakages. We demonstrate the efficiency of TablePuppet in combination with two widely-used ML training algorithms, stochastic gradient descent (SGD) and alternating direction method of multipliers (ADMM), and compare their computation/communication complexity. We evaluate the SGD/ADMM algorithms developed atop TablePuppet by training diverse ML models. Our experimental results show that TablePuppet achieves model accuracy comparable to the centralized baselines running directly atop the SQL results. Moreover, ADMM takes less communication time than SGD to converge to similar model accuracy.","sentences":["Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns).","However, these approaches are inadequate for handling distributed relational tables across databases.","This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns.","This raises the question: can we directly run FL on distributed relational tables?   ","In this paper, we formalize this problem as relational federated learning (RFL).","We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU).","In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table.","TablePuppet incorporates computation/communication optimizations to deal with the duplicate tuples introduced by joins, as well as differential privacy (DP) to protect against both feature and label leakages.","We demonstrate the efficiency of TablePuppet in combination with two widely-used ML training algorithms, stochastic gradient descent (SGD) and alternating direction method of multipliers (ADMM), and compare their computation/communication complexity.","We evaluate the SGD/ADMM algorithms developed atop TablePuppet by training diverse ML models.","Our experimental results show that TablePuppet achieves model accuracy comparable to the centralized baselines running directly atop the SQL results.","Moreover, ADMM takes less communication time than SGD to converge to similar model accuracy."],"url":"http://arxiv.org/abs/2403.15839v1","category":"cs.LG"}
{"created":"2024-03-23 13:16:07","title":"Time-series Initialization and Conditioning for Video-agnostic Stabilization of Video Super-Resolution using Recurrent Networks","abstract":"A Recurrent Neural Network (RNN) for Video Super Resolution (VSR) is generally trained with randomly clipped and cropped short videos extracted from original training videos due to various challenges in learning RNNs. However, since this RNN is optimized to super-resolve short videos, VSR of long videos is degraded due to the domain gap. Our preliminary experiments reveal that such degradation changes depending on the video properties, such as the video length and dynamics. To avoid this degradation, this paper proposes the training strategy of RNN for VSR that can work efficiently and stably independently of the video length and dynamics. The proposed training strategy stabilizes VSR by training a VSR network with various RNN hidden states changed depending on the video properties. Since computing such a variety of hidden states is time-consuming, this computational cost is reduced by reusing the hidden states for efficient training. In addition, training stability is further improved with frame-number conditioning. Our experimental results demonstrate that the proposed method performed better than base methods in videos with various lengths and dynamics.","sentences":["A Recurrent Neural Network (RNN) for Video Super Resolution (VSR) is generally trained with randomly clipped and cropped short videos extracted from original training videos due to various challenges in learning RNNs.","However, since this RNN is optimized to super-resolve short videos, VSR of long videos is degraded due to the domain gap.","Our preliminary experiments reveal that such degradation changes depending on the video properties, such as the video length and dynamics.","To avoid this degradation, this paper proposes the training strategy of RNN for VSR that can work efficiently and stably independently of the video length and dynamics.","The proposed training strategy stabilizes VSR by training a VSR network with various RNN hidden states changed depending on the video properties.","Since computing such a variety of hidden states is time-consuming, this computational cost is reduced by reusing the hidden states for efficient training.","In addition, training stability is further improved with frame-number conditioning.","Our experimental results demonstrate that the proposed method performed better than base methods in videos with various lengths and dynamics."],"url":"http://arxiv.org/abs/2403.15832v1","category":"cs.CV"}
{"created":"2024-03-23 12:33:13","title":"Relativistic stochastic hydrodynamics from quantum nonlinear projection operator","abstract":"We systematically derive relativistic stochastic hydrodynamics based on the method of quantum nonlinear projection operator. Morozov's nonlinear projection operator is a generalization of the well-known linear Mori-Zwanzig projection operator method, from which one can account for the nonlinear interaction between macroscopic modes. The quantum generalized Fokker-Planck and Langevin equations are also obtained using this formalism, which are fundamentally important in non-equilibrium statistical physics. As an application, the relativistic stochastic hydrodynamic equations with Gaussian noises are derived, which are applicable in studying anomalous transport phenomena near critical points. The possible extension to include multiplicative noises is also discussed.","sentences":["We systematically derive relativistic stochastic hydrodynamics based on the method of quantum nonlinear projection operator.","Morozov's nonlinear projection operator is a generalization of the well-known linear Mori-Zwanzig projection operator method, from which one can account for the nonlinear interaction between macroscopic modes.","The quantum generalized Fokker-Planck and Langevin equations are also obtained using this formalism, which are fundamentally important in non-equilibrium statistical physics.","As an application, the relativistic stochastic hydrodynamic equations with Gaussian noises are derived, which are applicable in studying anomalous transport phenomena near critical points.","The possible extension to include multiplicative noises is also discussed."],"url":"http://arxiv.org/abs/2403.15825v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-23 12:16:04","title":"Probing marginal stability in the spherical $p=2$ model","abstract":"In this paper we investigate the marginally stable nature of the low-temperature trivial spin glass phase in the spherical $p=2$ spin glass, by perturbing the system with three different kinds of non-linear interactions. In particular, we compare the effect of three additional dense four-body interactions: ferromagnetic couplings, purely disordered couplings and couplings with competing disordered and ferromagnetic interactions. Our study, characterized by the effort to present in a clear and pedagogical way the derivation of all the results, shows that the marginal stability property of the spherical spin glass depends in fact on which kind of perturbation is applied to the system: in general, a certain degree of frustration is needed also in the additional terms in order to induce a transition from a trivial to a non-trivial spin-glass phase. On the contrary, the addition of generic non-frustrated interactions does not destabilize the trivial spin-glass phase.","sentences":["In this paper we investigate the marginally stable nature of the low-temperature trivial spin glass phase in the spherical $p=2$ spin glass, by perturbing the system with three different kinds of non-linear interactions.","In particular, we compare the effect of three additional dense four-body interactions: ferromagnetic couplings, purely disordered couplings and couplings with competing disordered and ferromagnetic interactions.","Our study, characterized by the effort to present in a clear and pedagogical way the derivation of all the results, shows that the marginal stability property of the spherical spin glass depends in fact on which kind of perturbation is applied to the system: in general, a certain degree of frustration is needed also in the additional terms in order to induce a transition from a trivial to a non-trivial spin-glass phase.","On the contrary, the addition of generic non-frustrated interactions does not destabilize the trivial spin-glass phase."],"url":"http://arxiv.org/abs/2403.15819v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-23 11:33:15","title":"On Dynamical Systems in the \u00c9tale Topology","abstract":"We discuss $\\mathcal{D}$-modules and dynamical systems in the \\'etale topology. We introduce the differential scheme associated to a morphism $f: X\\to S$ of schemes of the same dimension. We introduce differential inertia group $I_{diff}^i$ which act trivially on the special fibers of these schemes. As an application we discuss the problem of counting points on elliptic curves which we show is connected to vanishing cycles of singularities and to closed orbits of dynamical systems.","sentences":["We discuss $\\mathcal{D}$-modules and dynamical systems in the \\'etale topology.","We introduce the differential scheme associated to a morphism $f: X\\to S$ of schemes of the same dimension.","We introduce differential inertia group $I_{diff}^i$ which act trivially on the special fibers of these schemes.","As an application we discuss the problem of counting points on elliptic curves which we show is connected to vanishing cycles of singularities and to closed orbits of dynamical systems."],"url":"http://arxiv.org/abs/2403.15806v1","category":"math.AG"}
{"created":"2024-03-23 11:14:35","title":"Comparison Principles for Stochastic Volterra equations","abstract":"In this work, we establish a comparison principle for stochastic Volterra equations with respect to the initial condition and the drift $b$ applicable to a wide class of Volterra kernels and driving forces $g$ that may be singular in zero. For completely monotone Volterra kernels such a result holds without any further restrictions, while for not completely monotone kernels it is shown that such a principle fails unless the drift is additionally monotone. As a side-product of our results, we also complement the literature on the weak existence of continuous nonnegative solutions. This covers the rough Cox-Ingersoll-Ross process with singular initial conditions.","sentences":["In this work, we establish a comparison principle for stochastic Volterra equations with respect to the initial condition and the drift $b$ applicable to a wide class of Volterra kernels and driving forces $g$ that may be singular in zero.","For completely monotone Volterra kernels such a result holds without any further restrictions, while for not completely monotone kernels it is shown that such a principle fails unless the drift is additionally monotone.","As a side-product of our results, we also complement the literature on the weak existence of continuous nonnegative solutions.","This covers the rough Cox-Ingersoll-Ross process with singular initial conditions."],"url":"http://arxiv.org/abs/2403.15801v1","category":"math.PR"}
{"created":"2024-03-23 11:09:06","title":"Droplet shape representation using Fourier series and autoencoders","abstract":"The shape of liquid droplets in air plays an important role in aerodynamic behavior and combustion dynamics of miniaturized propulsion systems such as microsatellites and small drones. Their precise manipulation can yield optimal efficiency in such systems. It is desired to have a minimal representation of droplet shapes using as few parameters to automate shape manipulation using self-learning algorithms, such as reinforcement learning. In this paper, we use a neural compression algorithm to represent, with only two parameters, elliptical and bullet-shaped droplets initially represented with 200 points (400 real numbers) at the droplet boundary. The mapping of many to two points is achieved in two stages. Initially, a Fourier series is formulated to approximate the contour of the droplet. Subsequently, the coefficients of this Fourier series are condensed to lower dimensions utilizing a neural network with a bottleneck architecture. Finally, 5000 synthetically generated droplet shapes were used to train the neural network. With a two real numbers representation, the recovered droplet shapes had excellent overlap with the original ones, with a mean square error 10^-3. Hence, this method compresses the droplet contour to merely two numerical parameters via a fully reversible process, a crucial feature for rendering learning algorithms computationally tractable.","sentences":["The shape of liquid droplets in air plays an important role in aerodynamic behavior and combustion dynamics of miniaturized propulsion systems such as microsatellites and small drones.","Their precise manipulation can yield optimal efficiency in such systems.","It is desired to have a minimal representation of droplet shapes using as few parameters to automate shape manipulation using self-learning algorithms, such as reinforcement learning.","In this paper, we use a neural compression algorithm to represent, with only two parameters, elliptical and bullet-shaped droplets initially represented with 200 points (400 real numbers) at the droplet boundary.","The mapping of many to two points is achieved in two stages.","Initially, a Fourier series is formulated to approximate the contour of the droplet.","Subsequently, the coefficients of this Fourier series are condensed to lower dimensions utilizing a neural network with a bottleneck architecture.","Finally, 5000 synthetically generated droplet shapes were used to train the neural network.","With a two real numbers representation, the recovered droplet shapes had excellent overlap with the original ones, with a mean square error 10^-3.","Hence, this method compresses the droplet contour to merely two numerical parameters via a fully reversible process, a crucial feature for rendering learning algorithms computationally tractable."],"url":"http://arxiv.org/abs/2403.15797v1","category":"physics.flu-dyn"}
{"created":"2024-03-23 10:42:26","title":"Screened hydrogen model of excitons in semiconducting nanoribbons","abstract":"The optical response of quasi-one-dimensional systems is often dominated by tightly bound excitons, that significantly influence their basic electronic properties. Despite their importance for device performance, accurately predicting their excitonic effects typically requires computationally demanding many-body approaches. Here, we present a simplified model to describe the static macroscopic dielectric function, which depends only on the width of the quasi-one-dimensional system and its polarizability per unit length. We show that at certain interaction distances, the screened Coulomb potential is greater than its bare counterpart, which results from the enhanced repulsive electron-electron interactions. As a test case, we study fourteen different nanoribbons, twelve of them armchair graphene nanoribbons of different families. Initially, we devised a simplified equation to estimate the exciton binding energy and extension that provides results comparable to those from the full Bethe-Salpeter equation, albeit for a specific nanoribbon family. Then, we used our proposed screening potential to solve the 1D Wannier-Mott equation, which turn out to be broad approach, that is able to predict binding energies that match quite well the ones obtained with the Bethe-Salpeter equation, irrespective of the nanoribbon family.","sentences":["The optical response of quasi-one-dimensional systems is often dominated by tightly bound excitons, that significantly influence their basic electronic properties.","Despite their importance for device performance, accurately predicting their excitonic effects typically requires computationally demanding many-body approaches.","Here, we present a simplified model to describe the static macroscopic dielectric function, which depends only on the width of the quasi-one-dimensional system and its polarizability per unit length.","We show that at certain interaction distances, the screened Coulomb potential is greater than its bare counterpart, which results from the enhanced repulsive electron-electron interactions.","As a test case, we study fourteen different nanoribbons, twelve of them armchair graphene nanoribbons of different families.","Initially, we devised a simplified equation to estimate the exciton binding energy and extension that provides results comparable to those from the full Bethe-Salpeter equation, albeit for a specific nanoribbon family.","Then, we used our proposed screening potential to solve the 1D Wannier-Mott equation, which turn out to be broad approach, that is able to predict binding energies that match quite well the ones obtained with the Bethe-Salpeter equation, irrespective of the nanoribbon family."],"url":"http://arxiv.org/abs/2403.15793v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-23 10:38:59","title":"DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving Environment for Real-World Performance Validation","abstract":"In this study, we introduce the DriveEnv-NeRF framework, which leverages Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting of the efficacy of autonomous driving agents in a targeted real-world scene. Standard simulator-based rendering often fails to accurately reflect real-world performance due to the sim-to-real gap, which represents the disparity between virtual simulations and real-world conditions. To mitigate this gap, we propose a workflow for building a high-fidelity simulation environment of the targeted real-world scene using NeRF. This approach is capable of rendering realistic images from novel viewpoints and constructing 3D meshes for emulating collisions. The validation of these capabilities through the comparison of success rates in both simulated and real environments demonstrates the benefits of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the DriveEnv-NeRF framework can serve as a training environment for autonomous driving agents under various lighting conditions. This approach enhances the robustness of the agents and reduces performance degradation when deployed to the target real scene, compared to agents fully trained using the standard simulator rendering pipeline.","sentences":["In this study, we introduce the DriveEnv-NeRF framework, which leverages Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting of the efficacy of autonomous driving agents in a targeted real-world scene.","Standard simulator-based rendering often fails to accurately reflect real-world performance due to the sim-to-real gap, which represents the disparity between virtual simulations and real-world conditions.","To mitigate this gap, we propose a workflow for building a high-fidelity simulation environment of the targeted real-world scene using NeRF.","This approach is capable of rendering realistic images from novel viewpoints and constructing 3D meshes for emulating collisions.","The validation of these capabilities through the comparison of success rates in both simulated and real environments demonstrates the benefits of using DriveEnv-NeRF as a real-world performance indicator.","Furthermore, the DriveEnv-NeRF framework can serve as a training environment for autonomous driving agents under various lighting conditions.","This approach enhances the robustness of the agents and reduces performance degradation when deployed to the target real scene, compared to agents fully trained using the standard simulator rendering pipeline."],"url":"http://arxiv.org/abs/2403.15791v1","category":"cs.RO"}
{"created":"2024-03-23 10:16:36","title":"Depth Estimation fusing Image and Radar Measurements with Uncertain Directions","abstract":"This paper proposes a depth estimation method using radar-image fusion by addressing the uncertain vertical directions of sparse radar measurements. In prior radar-image fusion work, image features are merged with the uncertain sparse depths measured by radar through convolutional layers. This approach is disturbed by the features computed with the uncertain radar depths. Furthermore, since the features are computed with a fully convolutional network, the uncertainty of each depth corresponding to a pixel is spread out over its surrounding pixels. Our method avoids this problem by computing features only with an image and conditioning the features pixelwise with the radar depth. Furthermore, the set of possibly correct radar directions is identified with reliable LiDAR measurements, which are available only in the training stage. Our method improves training data by learning only these possibly correct radar directions, while the previous method trains raw radar measurements, including erroneous measurements. Experimental results demonstrate that our method can improve the quantitative and qualitative results compared with its base method using radar-image fusion.","sentences":["This paper proposes a depth estimation method using radar-image fusion by addressing the uncertain vertical directions of sparse radar measurements.","In prior radar-image fusion work, image features are merged with the uncertain sparse depths measured by radar through convolutional layers.","This approach is disturbed by the features computed with the uncertain radar depths.","Furthermore, since the features are computed with a fully convolutional network, the uncertainty of each depth corresponding to a pixel is spread out over its surrounding pixels.","Our method avoids this problem by computing features only with an image and conditioning the features pixelwise with the radar depth.","Furthermore, the set of possibly correct radar directions is identified with reliable LiDAR measurements, which are available only in the training stage.","Our method improves training data by learning only these possibly correct radar directions, while the previous method trains raw radar measurements, including erroneous measurements.","Experimental results demonstrate that our method can improve the quantitative and qualitative results compared with its base method using radar-image fusion."],"url":"http://arxiv.org/abs/2403.15787v1","category":"cs.CV"}
{"created":"2024-03-23 10:10:38","title":"Optimal Control of Spin Qudits Subject to Decoherence Using Amplitude-and-Frequency-Constrained Pulses","abstract":"Quantum optimal control theory (QOCT) can be used to design the shape of electromagnetic pulses that implement operations on quantum devices. By using non-trivially shaped waveforms, gates can be made significantly faster than those built by concatenating monochromatic pulses. Recently, we applied this technique to the control of molecular spin qudits modelled with Schr\\\"odinger's equation and showed it can speed up operations, helping mitigate the effects of decoherence [Phys. Rev. Appl. {\\bf 17}, 064028 (2022)]. However, short gate times result in large optimal pulse amplitudes, which may not be experimentally accessible. Introducing bounds to the amplitudes then unavoidably leads to longer operation times, for which decoherence can no longer be neglected. Here, we study how to improve this procedure by applying QOCT on top of Lindblad's equation, to design control pulses accounting for decoherence already in the optimization process. In addition, we introduce a formulation that allows us to bound the maximum amplitude and frequency of the signals, which are the typical limitations of waveform generators. The pulses we obtain consistently enhance operation fidelities compared to those achieved with Schr\\\"odinger's equation across various target gates and durations, demonstrating the flexibility and robustness of our method. The improvement is larger the shorter the spin coherence time $T_{2}$.","sentences":["Quantum optimal control theory (QOCT) can be used to design the shape of electromagnetic pulses that implement operations on quantum devices.","By using non-trivially shaped waveforms, gates can be made significantly faster than those built by concatenating monochromatic pulses.","Recently, we applied this technique to the control of molecular spin qudits modelled with Schr\\\"odinger's equation and showed it can speed up operations, helping mitigate the effects of decoherence","[Phys. Rev. Appl.","{\\bf 17}, 064028 (2022)].","However, short gate times result in large optimal pulse amplitudes, which may not be experimentally accessible.","Introducing bounds to the amplitudes then unavoidably leads to longer operation times, for which decoherence can no longer be neglected.","Here, we study how to improve this procedure by applying QOCT on top of Lindblad's equation, to design control pulses accounting for decoherence already in the optimization process.","In addition, we introduce a formulation that allows us to bound the maximum amplitude and frequency of the signals, which are the typical limitations of waveform generators.","The pulses we obtain consistently enhance operation fidelities compared to those achieved with Schr\\\"odinger's equation across various target gates and durations, demonstrating the flexibility and robustness of our method.","The improvement is larger the shorter the spin coherence time $T_{2}$."],"url":"http://arxiv.org/abs/2403.15785v1","category":"quant-ph"}
{"created":"2024-03-23 10:04:06","title":"Divergence conforming DG method for the optimal control of the Oseen equation with variable viscosity","abstract":"This study introduces the divergence-conforming discontinuous Galerkin finite element method (DGFEM) for numerically approximating optimal control problems with distributed constraints, specifically those governed by stationary generalized Oseen equations. We provide optimal a priori error estimates in energy norms for such problems using the divergence-conforming DGFEM approach. Moreover, we thoroughly analyze $L^2$ error estimates for scenarios dominated by diffusion and convection. Additionally, we establish the new reliable and efficient a posteriori error estimators for the optimal control of the Oseen equation with variable viscosity. Theoretical findings are validated through numerical experiments conducted in both two and three dimensions.","sentences":["This study introduces the divergence-conforming discontinuous Galerkin finite element method (DGFEM) for numerically approximating optimal control problems with distributed constraints, specifically those governed by stationary generalized Oseen equations.","We provide optimal a priori error estimates in energy norms for such problems using the divergence-conforming DGFEM approach.","Moreover, we thoroughly analyze $L^2$ error estimates for scenarios dominated by diffusion and convection.","Additionally, we establish the new reliable and efficient a posteriori error estimators for the optimal control of the Oseen equation with variable viscosity.","Theoretical findings are validated through numerical experiments conducted in both two and three dimensions."],"url":"http://arxiv.org/abs/2403.15783v1","category":"math.NA"}
{"created":"2024-03-23 08:57:46","title":"Graph Image Prior for Unsupervised Dynamic MRI Reconstruction","abstract":"The inductive bias of the convolutional neural network (CNN) can act as a strong prior for image restoration, which is known as the Deep Image Prior (DIP). In recent years, DIP has been utilized in unsupervised dynamic MRI reconstruction, which adopts a generative model from the latent space to the image space. However, existing methods usually utilize a single pyramid-shaped CNN architecture to parameterize the generator, which cannot effectively exploit the spatio-temporal correlations within the dynamic data. In this work, we propose a novel scheme to exploit the DIP prior for dynamic MRI reconstruction, named ``Graph Image Prior'' (GIP). The generative model is decomposed into two stages: image recovery and manifold discovery, which is bridged by a graph convolutional network to exploit the spatio-temporal correlations. In addition, we devise an ADMM algorithm to alternately optimize the images and the network parameters to further improve the reconstruction performance. Experimental results demonstrate that GIP outperforms compressed sensing methods and unsupervised methods over different sampling trajectories, and significantly reduces the performance gap with the state-of-art supervised deep-learning methods. Moreover, GIP displays superior generalization ability when transferred to a different reconstruction setting, without the need for any additional data.","sentences":["The inductive bias of the convolutional neural network (CNN) can act as a strong prior for image restoration, which is known as the Deep Image Prior (DIP).","In recent years, DIP has been utilized in unsupervised dynamic MRI reconstruction, which adopts a generative model from the latent space to the image space.","However, existing methods usually utilize a single pyramid-shaped CNN architecture to parameterize the generator, which cannot effectively exploit the spatio-temporal correlations within the dynamic data.","In this work, we propose a novel scheme to exploit the DIP prior for dynamic MRI reconstruction, named ``Graph Image Prior'' (GIP).","The generative model is decomposed into two stages: image recovery and manifold discovery, which is bridged by a graph convolutional network to exploit the spatio-temporal correlations.","In addition, we devise an ADMM algorithm to alternately optimize the images and the network parameters to further improve the reconstruction performance.","Experimental results demonstrate that GIP outperforms compressed sensing methods and unsupervised methods over different sampling trajectories, and significantly reduces the performance gap with the state-of-art supervised deep-learning methods.","Moreover, GIP displays superior generalization ability when transferred to a different reconstruction setting, without the need for any additional data."],"url":"http://arxiv.org/abs/2403.15770v1","category":"eess.IV"}
{"created":"2024-03-23 08:44:10","title":"The sharp $C^0$-fragmentation property for Hamiltonian diffeomorphisms and homeomorphisms on surfaces","abstract":"In this paper, we present a $C^0$-fragmentation property for Hamiltonian diffeomorphisms. More precisely, it is known that for a given open covering $\\mathcal{U}$ of a compact symplectic surface we can write each $C^0$-small enough Hamiltonian diffeomorphism as the composition of Hamiltonian diffeomorphisms compactly supported inside the open sets of the covering $\\mathcal{U}$. We show that such a decomposition can be done with a Lipschitz estimate on the $C^0$-norm of the fragments. We also show the same property for the kernel of $\\theta$, the mass-flow homomorphism for homeomorphisms. This answers a question from Buhovsky and Seyfaddini.","sentences":["In this paper, we present a $C^0$-fragmentation property for Hamiltonian diffeomorphisms.","More precisely, it is known that for a given open covering $\\mathcal{U}$ of a compact symplectic surface we can write each $C^0$-small enough Hamiltonian diffeomorphism as the composition of Hamiltonian diffeomorphisms compactly supported inside the open sets of the covering $\\mathcal{U}$. We show that such a decomposition can be done with a Lipschitz estimate on the $C^0$-norm of the fragments.","We also show the same property for the kernel of $\\theta$, the mass-flow homomorphism for homeomorphisms.","This answers a question from Buhovsky and Seyfaddini."],"url":"http://arxiv.org/abs/2403.15767v1","category":"math.SG"}
{"created":"2024-03-25 12:23:39","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","abstract":"This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., \"good parking spot\", \"convenient drop-off location\") from visual input. Despite its similarity to learning factual concepts (e.g., \"red cube\"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related concepts in mobile robotics and autonomous driving. Our evaluation demonstrates that Synapse significantly outperforms existing baselines as well as its own ablations. The code and other details can be found on the project website https://amrl.cs.utexas.edu/synapse .","sentences":["This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., \"good parking spot\", \"convenient drop-off location\") from visual input.","Despite its similarity to learning factual concepts (e.g., \"red cube\"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data.","We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations.","Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences.","We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related concepts in mobile robotics and autonomous driving.","Our evaluation demonstrates that Synapse significantly outperforms existing baselines as well as its own ablations.","The code and other details can be found on the project website https://amrl.cs.utexas.edu/synapse ."],"url":"http://arxiv.org/abs/2403.16689v1","category":"cs.RO"}
{"created":"2024-03-25 12:23:19","title":"Optimal convex $M$-estimation via score matching","abstract":"In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator of the regression coefficients that uses knowledge of this error distribution; in this sense, we obtain robustness without sacrificing much efficiency. Numerical experiments confirm the practical merits of our proposal.","sentences":["In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients.","Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution.","At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence.","The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators.","As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator of the regression coefficients that uses knowledge of this error distribution; in this sense, we obtain robustness without sacrificing much efficiency.","Numerical experiments confirm the practical merits of our proposal."],"url":"http://arxiv.org/abs/2403.16688v1","category":"math.ST"}
{"created":"2024-03-25 10:38:17","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus","abstract":"Addressing the challenge of data scarcity in industrial domains, transfer learning emerges as a pivotal paradigm. This work introduces Style Filter, a tailored methodology for industrial contexts. By selectively filtering source domain data before knowledge transfer, Style Filter reduces the quantity of data while maintaining or even enhancing the performance of transfer learning strategy. Offering label-free operation, minimal reliance on prior knowledge, independence from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional transfer strategies in the deep learning domain. The results underscore the effectiveness of Style Filter in real-world industrial applications.","sentences":["Addressing the challenge of data scarcity in industrial domains, transfer learning emerges as a pivotal paradigm.","This work introduces Style Filter, a tailored methodology for industrial contexts.","By selectively filtering source domain data before knowledge transfer, Style Filter reduces the quantity of data while maintaining or even enhancing the performance of transfer learning strategy.","Offering label-free operation, minimal reliance on prior knowledge, independence from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional transfer strategies in the deep learning domain.","The results underscore the effectiveness of Style Filter in real-world industrial applications."],"url":"http://arxiv.org/abs/2403.16607v1","category":"cs.LG"}
{"created":"2024-03-25 09:51:54","title":"Can Large Language Models (or Humans) Distill Text?","abstract":"We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable. We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of distillation methods that achieve statistical independence in representation space if this is difficult for human coders operating on raw text to attain.","sentences":["We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable.","We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals.","Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data.","In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation.","Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content.","This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of distillation methods that achieve statistical independence in representation space if this is difficult for human coders operating on raw text to attain."],"url":"http://arxiv.org/abs/2403.16584v1","category":"cs.CL"}
{"created":"2024-03-25 08:09:22","title":"ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise Optimization in Medical Image Registration","abstract":"Deformable image registration plays a crucial role in medical imaging, aiding in disease diagnosis and image-guided interventions. Traditional iterative methods are slow, while deep learning (DL) accelerates solutions but faces usability and precision challenges. This study introduces a pyramid network with the enhanced motion decomposition Transformer (ModeTv2) operator, showcasing superior pairwise optimization (PO) akin to traditional methods. We re-implement ModeT operator with CUDA extensions to enhance its computational efficiency. We further propose RegHead module which refines deformation fields, improves the realism of deformation and reduces parameters. By adopting the PO, the proposed network balances accuracy, efficiency, and generalizability. Extensive experiments on two public brain MRI datasets and one abdominal CT dataset demonstrate the network's suitability for PO, providing a DL model with enhanced usability and interpretability. The code is publicly available.","sentences":["Deformable image registration plays a crucial role in medical imaging, aiding in disease diagnosis and image-guided interventions.","Traditional iterative methods are slow, while deep learning (DL) accelerates solutions but faces usability and precision challenges.","This study introduces a pyramid network with the enhanced motion decomposition Transformer (ModeTv2) operator, showcasing superior pairwise optimization (PO) akin to traditional methods.","We re-implement ModeT operator with CUDA extensions to enhance its computational efficiency.","We further propose RegHead module which refines deformation fields, improves the realism of deformation and reduces parameters.","By adopting the PO, the proposed network balances accuracy, efficiency, and generalizability.","Extensive experiments on two public brain MRI datasets and one abdominal CT dataset demonstrate the network's suitability for PO, providing a DL model with enhanced usability and interpretability.","The code is publicly available."],"url":"http://arxiv.org/abs/2403.16526v1","category":"cs.CV"}
{"created":"2024-03-25 06:56:38","title":"Plaintext-Free Deep Learning for Privacy-Preserving Medical Image Analysis via Frequency Information Embedding","abstract":"In the fast-evolving field of medical image analysis, Deep Learning (DL)-based methods have achieved tremendous success. However, these methods require plaintext data for training and inference stages, raising privacy concerns, especially in the sensitive area of medical data. To tackle these concerns, this paper proposes a novel framework that uses surrogate images for analysis, eliminating the need for plaintext images. This approach is called Frequency-domain Exchange Style Fusion (FESF). The framework includes two main components: Image Hidden Module (IHM) and Image Quality Enhancement Module~(IQEM). The~IHM performs in the frequency domain, blending the features of plaintext medical images into host medical images, and then combines this with IQEM to improve and create surrogate images effectively. During the diagnostic model training process, only surrogate images are used, enabling anonymous analysis without any plaintext data during both training and inference stages. Extensive evaluations demonstrate that our framework effectively preserves the privacy of medical images and maintains diagnostic accuracy of DL models at a relatively high level, proving its effectiveness across various datasets and DL-based models.","sentences":["In the fast-evolving field of medical image analysis, Deep Learning (DL)-based methods have achieved tremendous success.","However, these methods require plaintext data for training and inference stages, raising privacy concerns, especially in the sensitive area of medical data.","To tackle these concerns, this paper proposes a novel framework that uses surrogate images for analysis, eliminating the need for plaintext images.","This approach is called Frequency-domain Exchange Style Fusion (FESF).","The framework includes two main components: Image Hidden Module (IHM) and Image Quality Enhancement Module~(IQEM).","The~IHM performs in the frequency domain, blending the features of plaintext medical images into host medical images, and then combines this with IQEM to improve and create surrogate images effectively.","During the diagnostic model training process, only surrogate images are used, enabling anonymous analysis without any plaintext data during both training and inference stages.","Extensive evaluations demonstrate that our framework effectively preserves the privacy of medical images and maintains diagnostic accuracy of DL models at a relatively high level, proving its effectiveness across various datasets and DL-based models."],"url":"http://arxiv.org/abs/2403.16473v1","category":"cs.CR"}
{"created":"2024-03-25 05:35:40","title":"A Mixed Method Study of DevOps Challenges","abstract":"Context: DevOps practices combine software development and IT operations. There is a growing number of DevOps related posts in popular online developer forum Stack Overflow (SO). While previous research analyzed SO posts related to build/release engineering, we are aware of no research that specifically focused on DevOps related discussions. Objective: To learn the challenges developers face while using the currently available DevOps tools and techniques along with the organizational challenges in DevOps practices. Method: We conduct an empirical study by applying topic modeling on 174K SO posts that contain DevOps discussions. We then validate and extend the empirical study findings with a survey of 21 professional DevOps practitioners. Results: We find that: (1) There are 23 DevOps topics grouped into four categories: Cloud & CI/CD Tools, Infrastructure as Code, Container & Orchestration, and Quality Assurance. (2) The topic category Cloud & CI/CD Tools contains the highest number of topics (10) which cover 48.6% of all questions in our dataset, followed by the category Infrastructure as Code (28.9%). (3) File management is the most popular topic followed by Jenkins Pipeline, while infrastructural Exception Handling and Jenkins Distributed Architecture are the most difficult topics (with least accepted answers). (4) In the survey, developers mention that it requires hands-on experience before current DevOps tools can be considered easy. They raised the needs for better documentation and learning resources to learn the rapidly changing DevOps tools and techniques. Practitioners also emphasized on the formal training approach by the organizations for DevOps skill development. Conclusion: Architects and managers can use the findings of this research to adopt appropriate DevOps technologies, and organizations can design tool or process specific DevOps training programs.","sentences":["Context: DevOps practices combine software development and IT operations.","There is a growing number of DevOps related posts in popular online developer forum Stack Overflow (SO).","While previous research analyzed SO posts related to build/release engineering, we are aware of no research that specifically focused on DevOps related discussions.","Objective: To learn the challenges developers face while using the currently available DevOps tools and techniques along with the organizational challenges in DevOps practices.","Method: We conduct an empirical study by applying topic modeling on 174K SO posts that contain DevOps discussions.","We then validate and extend the empirical study findings with a survey of 21 professional DevOps practitioners.","Results: We find that: (1) There are 23 DevOps topics grouped into four categories: Cloud & CI/CD Tools, Infrastructure as Code, Container & Orchestration, and Quality Assurance.","(2) The topic category Cloud & CI/CD Tools contains the highest number of topics (10) which cover 48.6% of all questions in our dataset, followed by the category Infrastructure as Code (28.9%).","(3) File management is the most popular topic followed by Jenkins Pipeline, while infrastructural Exception Handling and Jenkins Distributed Architecture are the most difficult topics (with least accepted answers).","(4) In the survey, developers mention that it requires hands-on experience before current DevOps tools can be considered easy.","They raised the needs for better documentation and learning resources to learn the rapidly changing DevOps tools and techniques.","Practitioners also emphasized on the formal training approach by the organizations for DevOps skill development.","Conclusion: Architects and managers can use the findings of this research to adopt appropriate DevOps technologies, and organizations can design tool or process specific DevOps training programs."],"url":"http://arxiv.org/abs/2403.16436v1","category":"cs.SE"}
{"created":"2024-03-25 04:45:16","title":"Terrain-Attentive Learning for Efficient 6-DoF Kinodynamic Modeling on Vertically Challenging Terrain","abstract":"Wheeled robots have recently demonstrated superior mechanical capability to traverse vertically challenging terrain (e.g., extremely rugged boulders comparable in size to the vehicles themselves). Negotiating such terrain introduces significant variations of vehicle pose in all six Degrees-of-Freedom (DoFs), leading to imbalanced contact forces, varying momentum, and chassis deformation due to non-rigid tires and suspensions. To autonomously navigate on vertically challenging terrain, all these factors need to be efficiently reasoned within limited onboard computation and strict real-time constraints. In this paper, we propose a 6-DoF kinodynamics learning approach that is attentive only to the specific underlying terrain critical to the current vehicle-terrain interaction, so that it can be efficiently queried in real-time motion planners onboard small robots. Physical experiment results show our Terrain-Attentive Learning demonstrates on average 51.1% reduction in model prediction error among all 6 DoFs compared to a state-of-the-art model for vertically challenging terrain.","sentences":["Wheeled robots have recently demonstrated superior mechanical capability to traverse vertically challenging terrain (e.g., extremely rugged boulders comparable in size to the vehicles themselves).","Negotiating such terrain introduces significant variations of vehicle pose in all six Degrees-of-Freedom (DoFs), leading to imbalanced contact forces, varying momentum, and chassis deformation due to non-rigid tires and suspensions.","To autonomously navigate on vertically challenging terrain, all these factors need to be efficiently reasoned within limited onboard computation and strict real-time constraints.","In this paper, we propose a 6-DoF kinodynamics learning approach that is attentive only to the specific underlying terrain critical to the current vehicle-terrain interaction, so that it can be efficiently queried in real-time motion planners onboard small robots.","Physical experiment results show our Terrain-Attentive Learning demonstrates on average 51.1% reduction in model prediction error among all 6 DoFs compared to a state-of-the-art model for vertically challenging terrain."],"url":"http://arxiv.org/abs/2403.16419v1","category":"cs.RO"}
{"created":"2024-03-25 03:53:50","title":"Accuracy-Aware Cooperative Sensing and Computing for Connected Autonomous Vehicles","abstract":"To maintain high perception performance among connected and autonomous vehicles (CAVs), in this paper, we propose an accuracy-aware and resource-efficient raw-level cooperative sensing and computing scheme among CAVs and road-side infrastructure. The scheme enables fined-grained partial raw sensing data selection, transmission, fusion, and processing in per-object granularity, by exploiting the parallelism among object classification subtasks associated with each object. A supervised learning model is trained to capture the relationship between the object classification accuracy and the data quality of selected object sensing data, facilitating accuracy-aware sensing data selection. We formulate an optimization problem for joint sensing data selection, subtask placement and resource allocation among multiple object classification subtasks, to minimize the total resource cost while satisfying the delay and accuracy requirements. A genetic algorithm based iterative solution is proposed for the optimization problem. Simulation results demonstrate the accuracy awareness and resource efficiency achieved by the proposed cooperative sensing and computing scheme, in comparison with benchmark solutions.","sentences":["To maintain high perception performance among connected and autonomous vehicles (CAVs), in this paper, we propose an accuracy-aware and resource-efficient raw-level cooperative sensing and computing scheme among CAVs and road-side infrastructure.","The scheme enables fined-grained partial raw sensing data selection, transmission, fusion, and processing in per-object granularity, by exploiting the parallelism among object classification subtasks associated with each object.","A supervised learning model is trained to capture the relationship between the object classification accuracy and the data quality of selected object sensing data, facilitating accuracy-aware sensing data selection.","We formulate an optimization problem for joint sensing data selection, subtask placement and resource allocation among multiple object classification subtasks, to minimize the total resource cost while satisfying the delay and accuracy requirements.","A genetic algorithm based iterative solution is proposed for the optimization problem.","Simulation results demonstrate the accuracy awareness and resource efficiency achieved by the proposed cooperative sensing and computing scheme, in comparison with benchmark solutions."],"url":"http://arxiv.org/abs/2403.16408v1","category":"cs.NI"}
{"created":"2024-03-25 03:30:37","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation","abstract":"In medical and industrial domains, providing guidance for assembly processes is critical to ensure efficiency and safety. Errors in assembly can lead to significant consequences such as extended surgery times, and prolonged manufacturing or maintenance times in industry. Assembly scenarios can benefit from in-situ AR visualization to provide guidance, reduce assembly times and minimize errors. To enable in-situ visualization 6D pose estimation can be leveraged. Existing 6D pose estimation techniques primarily focus on individual objects and static captures. However, assembly scenarios have various dynamics including occlusion during assembly and dynamics in the assembly objects appearance. Existing work, combining object detection/6D pose estimation and assembly state detection focuses either on pure deep learning-based approaches, or limit the assembly state detection to building blocks. To address the challenges of 6D pose estimation in combination with assembly state detection, our approach ASDF builds upon the strengths of YOLOv8, a real-time capable object detection framework. We extend this framework, refine the object pose and fuse pose knowledge with network-detected pose information. Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection. By combining both pose and state information, our Pose2State module predicts the final assembly state with precision. Our evaluation on our ASDF dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation. Moreover, on the GBOT dataset, we outperform the pure deep learning-based network, and even outperform the hybrid and pure tracking-based approaches.","sentences":["In medical and industrial domains, providing guidance for assembly processes is critical to ensure efficiency and safety.","Errors in assembly can lead to significant consequences such as extended surgery times, and prolonged manufacturing or maintenance times in industry.","Assembly scenarios can benefit from in-situ AR visualization to provide guidance, reduce assembly times and minimize errors.","To enable in-situ visualization 6D pose estimation can be leveraged.","Existing 6D pose estimation techniques primarily focus on individual objects and static captures.","However, assembly scenarios have various dynamics including occlusion during assembly and dynamics in the assembly objects appearance.","Existing work, combining object detection/6D pose estimation and assembly state detection focuses either on pure deep learning-based approaches, or limit the assembly state detection to building blocks.","To address the challenges of 6D pose estimation in combination with assembly state detection, our approach ASDF builds upon the strengths of YOLOv8, a real-time capable object detection framework.","We extend this framework, refine the object pose and fuse pose knowledge with network-detected pose information.","Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection.","By combining both pose and state information, our Pose2State module predicts the final assembly state with precision.","Our evaluation on our ASDF dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation.","Moreover, on the GBOT dataset, we outperform the pure deep learning-based network, and even outperform the hybrid and pure tracking-based approaches."],"url":"http://arxiv.org/abs/2403.16400v1","category":"cs.CV"}
{"created":"2024-03-25 02:32:43","title":"SignSGD with Federated Voting","abstract":"Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \\textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently, these weights are employed to decode the signs of the aggregated local gradients in such a way to minimize the sign decoding error probability. We provide a unified convergence rate analysis framework applicable to scenarios where the estimated weights are known to the parameter server either perfectly or imperfectly. We demonstrate that the proposed signSGD-FV algorithm has a theoretical convergence guarantee even when edge devices use heterogeneous mini-batch sizes. Experimental results show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence rate, especially in heterogeneous mini-batch sizes.","sentences":["Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices.","However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server.","SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization.","However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers.","To overcome this, we propose a novel signSGD optimizer with \\textit{federated voting} (signSGD-FV).","The idea of federated voting is to exploit learnable weights to perform weighted majority voting.","The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities.","Subsequently, these weights are employed to decode the signs of the aggregated local gradients in such a way to minimize the sign decoding error probability.","We provide a unified convergence rate analysis framework applicable to scenarios where the estimated weights are known to the parameter server either perfectly or imperfectly.","We demonstrate that the proposed signSGD-FV algorithm has a theoretical convergence guarantee even when edge devices use heterogeneous mini-batch sizes.","Experimental results show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence rate, especially in heterogeneous mini-batch sizes."],"url":"http://arxiv.org/abs/2403.16372v1","category":"cs.LG"}
{"created":"2024-03-24 23:50:15","title":"Modeling Analog Dynamic Range Compressors using Deep Learning and State-space Models","abstract":"We describe a novel approach for developing realistic digital models of dynamic range compressors for digital audio production by analyzing their analog prototypes. While realistic digital dynamic compressors are potentially useful for many applications, the design process is challenging because the compressors operate nonlinearly over long time scales. Our approach is based on the structured state space sequence model (S4), as implementing the state-space model (SSM) has proven to be efficient at learning long-range dependencies and is promising for modeling dynamic range compressors. We present in this paper a deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic range compressor. The model is causal, executes efficiently in real time, and achieves roughly the same quality as previous deep-learning models but with fewer parameters.","sentences":["We describe a novel approach for developing realistic digital models of dynamic range compressors for digital audio production by analyzing their analog prototypes.","While realistic digital dynamic compressors are potentially useful for many applications, the design process is challenging because the compressors operate nonlinearly over long time scales.","Our approach is based on the structured state space sequence model (S4), as implementing the state-space model (SSM) has proven to be efficient at learning long-range dependencies and is promising for modeling dynamic range compressors.","We present in this paper a deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic range compressor.","The model is causal, executes efficiently in real time, and achieves roughly the same quality as previous deep-learning models but with fewer parameters."],"url":"http://arxiv.org/abs/2403.16331v1","category":"cs.SD"}
{"created":"2024-03-24 19:32:39","title":"Object Detectors in the Open Environment:Challenges, Solutions, and Outlook","abstract":"With the emergence of foundation models, deep learning-based object detectors have shown practical usability in closed set scenarios. However, for real-world tasks, object detectors often operate in open environments, where crucial factors (\\eg, data distribution, objective) that influence model learning are often changing. The dynamic and intricate nature of the open environment poses novel and formidable challenges to object detectors. Unfortunately, current research on object detectors in open environments lacks a comprehensive analysis of their distinctive characteristics, challenges, and corresponding solutions, which hinders their secure deployment in critical real-world scenarios. This paper aims to bridge this gap by conducting a comprehensive review and analysis of object detectors in open environments. We initially identified limitations of key structural components within the existing detection pipeline and propose the open environment object detector challenge framework that includes four quadrants (\\ie, out-of-domain, out-of-category, robust learning, and incremental learning) based on the dimensions of the data / target changes. For each quadrant of challenges in the proposed framework, we present a detailed description and systematic analysis of the overarching goals and core difficulties, systematically review the corresponding solutions, and benchmark their performance over multiple widely adopted datasets. In addition, we engage in a discussion of open problems and potential avenues for future research. This paper aims to provide a fresh, comprehensive, and systematic understanding of the challenges and solutions associated with open-environment object detectors, thus catalyzing the development of more solid applications in real-world scenarios.","sentences":["With the emergence of foundation models, deep learning-based object detectors have shown practical usability in closed set scenarios.","However, for real-world tasks, object detectors often operate in open environments, where crucial factors (\\eg, data distribution, objective) that influence model learning are often changing.","The dynamic and intricate nature of the open environment poses novel and formidable challenges to object detectors.","Unfortunately, current research on object detectors in open environments lacks a comprehensive analysis of their distinctive characteristics, challenges, and corresponding solutions, which hinders their secure deployment in critical real-world scenarios.","This paper aims to bridge this gap by conducting a comprehensive review and analysis of object detectors in open environments.","We initially identified limitations of key structural components within the existing detection pipeline and propose the open environment object detector challenge framework that includes four quadrants (\\ie, out-of-domain, out-of-category, robust learning, and incremental learning) based on the dimensions of the data / target changes.","For each quadrant of challenges in the proposed framework, we present a detailed description and systematic analysis of the overarching goals and core difficulties, systematically review the corresponding solutions, and benchmark their performance over multiple widely adopted datasets.","In addition, we engage in a discussion of open problems and potential avenues for future research.","This paper aims to provide a fresh, comprehensive, and systematic understanding of the challenges and solutions associated with open-environment object detectors, thus catalyzing the development of more solid applications in real-world scenarios."],"url":"http://arxiv.org/abs/2403.16271v1","category":"cs.CV"}
{"created":"2024-03-24 18:59:38","title":"Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs","abstract":"We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases. As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized contextual information do not perform satisfactorily in inferring patent phrase similarity. To address this, we introduce a graph-augmented approach to amplify the global contextual information of the patent phrases. For each patent phrase, we construct a phrase graph that links to its focal patents and a list of patents that are either cited by or cite these focal patents. The augmented phrase embedding is then derived from combining its localized contextual embedding with its global embedding within the phrase graph. We further propose a self-supervised learning objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the graph parameters in an end-to-end manner. Experimental results from a unique patent phrase similarity dataset demonstrate that our approach significantly enhances the representation of patent phrases, resulting in marked improvements in similarity inference in a self-supervised fashion. Substantial improvements are also observed in the supervised setting, underscoring the potential benefits of leveraging retrieved phrase graph augmentation.","sentences":["We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases.","As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized contextual information do not perform satisfactorily in inferring patent phrase similarity.","To address this, we introduce a graph-augmented approach to amplify the global contextual information of the patent phrases.","For each patent phrase, we construct a phrase graph that links to its focal patents and a list of patents that are either cited by or cite these focal patents.","The augmented phrase embedding is then derived from combining its localized contextual embedding with its global embedding within the phrase graph.","We further propose a self-supervised learning objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the graph parameters in an end-to-end manner.","Experimental results from a unique patent phrase similarity dataset demonstrate that our approach significantly enhances the representation of patent phrases, resulting in marked improvements in similarity inference in a self-supervised fashion.","Substantial improvements are also observed in the supervised setting, underscoring the potential benefits of leveraging retrieved phrase graph augmentation."],"url":"http://arxiv.org/abs/2403.16265v1","category":"cs.CL"}
{"created":"2024-03-24 17:33:22","title":"Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective","abstract":"In order to adhere to regulatory standards governing individual data privacy and safety, machine learning models must systematically eliminate information derived from specific subsets of a user's training data that can no longer be utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch. The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network. This intentional removal is crafted to degrade the model's performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model's performance in other classes. To achieve this goal, we frame the class unlearning problem from a Bayesian perspective, which yields a loss function that minimizes the log-likelihood associated with the unlearned data with a stability regularization in parameter space. This stability regularization incorporates Mohalanobis distance with respect to the Fisher Information matrix and $l_2$ distance from the pre-trained model parameters. Our novel approach, termed \\textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing state-of-the-art class unlearning methods, demonstrating superior effectiveness. Notably, PBU achieves this efficacy without requiring awareness of the entire training dataset but only to the unlearned data points, marking a distinctive feature of its performance.","sentences":["In order to adhere to regulatory standards governing individual data privacy and safety, machine learning models must systematically eliminate information derived from specific subsets of a user's training data that can no longer be utilized.","The emerging discipline of Machine Unlearning has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch.","The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network.","This intentional removal is crafted to degrade the model's performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model's performance in other classes.","To achieve this goal, we frame the class unlearning problem from a Bayesian perspective, which yields a loss function that minimizes the log-likelihood associated with the unlearned data with a stability regularization in parameter space.","This stability regularization incorporates Mohalanobis distance with respect to the Fisher Information matrix and $l_2$ distance from the pre-trained model parameters.","Our novel approach, termed \\textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing state-of-the-art class unlearning methods, demonstrating superior effectiveness.","Notably, PBU achieves this efficacy without requiring awareness of the entire training dataset but only to the unlearned data points, marking a distinctive feature of its performance."],"url":"http://arxiv.org/abs/2403.16246v1","category":"cs.LG"}
{"created":"2024-03-24 16:29:50","title":"Exemplar-Free Class Incremental Learning via Incremental Representation","abstract":"Exemplar-Free Class Incremental Learning (efCIL) aims to continuously incorporate the knowledge from new classes while retaining previously learned information, without storing any old-class exemplars (i.e., samples). For this purpose, various efCIL methods have been proposed over the past few years, generally with elaborately constructed old pseudo-features, increasing the difficulty of model development and interpretation. In contrast, we propose a \\textbf{simple Incremental Representation (IR) framework} for efCIL without constructing old pseudo-features. IR utilizes dataset augmentation to cover a suitable feature space and prevents the model from forgetting by using a single L2 space maintenance loss. We discard the transient classifier trained on each one of the sequence tasks and instead replace it with a 1-near-neighbor classifier for inference, ensuring the representation is incrementally updated during CIL. Extensive experiments demonstrate that our proposed IR achieves comparable performance while significantly preventing the model from forgetting on CIFAR100, TinyImageNet, and ImageNetSubset datasets.","sentences":["Exemplar-Free Class Incremental Learning (efCIL) aims to continuously incorporate the knowledge from new classes while retaining previously learned information, without storing any old-class exemplars (i.e., samples).","For this purpose, various efCIL methods have been proposed over the past few years, generally with elaborately constructed old pseudo-features, increasing the difficulty of model development and interpretation.","In contrast, we propose a \\textbf{simple Incremental Representation (IR) framework} for efCIL without constructing old pseudo-features.","IR utilizes dataset augmentation to cover a suitable feature space and prevents the model from forgetting by using a single L2 space maintenance loss.","We discard the transient classifier trained on each one of the sequence tasks and instead replace it with a 1-near-neighbor classifier for inference, ensuring the representation is incrementally updated during CIL.","Extensive experiments demonstrate that our proposed IR achieves comparable performance while significantly preventing the model from forgetting on CIFAR100, TinyImageNet, and ImageNetSubset datasets."],"url":"http://arxiv.org/abs/2403.16221v1","category":"cs.CV"}
{"created":"2024-03-24 16:18:07","title":"Predicting Feynman periods in $\u03c6^4$-theory","abstract":"We present efficient data-driven approaches to predict Feynman periods in $\\phi^4$-theory from properties of the underlying Feynman graphs. We find that the numbers of cuts and cycles determines the period to approximately 2% accuracy. Hepp bound and Martin invariant allow to predict the period with accuracy much better than 1%. In most cases, the period is a multi-linear function of the parameters in question. Besides classical correlation analysis, we also investigate the usefulness of machine-learning algorithms to predict the period. When sufficiently many properties of the graph are used, the period can be predicted with better than 0.05% relative accuracy.   We use one of the constructed prediction models for weighted Monte-Carlo sampling of Feynman graphs, and compute the primitive contribution to the beta function of $\\phi^4$-theory at $L\\in \\left \\lbrace 13, 14, 15, 16 \\right \\rbrace $ loops. Our results confirm the previously known numerical estimates of the primitive beta function and improve their accuracy. Compared to uniform random sampling of graphs, our new algorithm reaches 35-fold higher accuracy in fixed runtime, or requires 1000-fold less runtime to reach a given accuracy.   The data set of all periods computed for this work, combined with a previous data set, is made publicly available. Besides the physical application, it could serve as a benchmark for graph-based machine learning algorithms.","sentences":["We present efficient data-driven approaches to predict Feynman periods in $\\phi^4$-theory from properties of the underlying Feynman graphs.","We find that the numbers of cuts and cycles determines the period to approximately 2% accuracy.","Hepp bound and Martin invariant allow to predict the period with accuracy much better than 1%.","In most cases, the period is a multi-linear function of the parameters in question.","Besides classical correlation analysis, we also investigate the usefulness of machine-learning algorithms to predict the period.","When sufficiently many properties of the graph are used, the period can be predicted with better than 0.05% relative accuracy.   ","We use one of the constructed prediction models for weighted Monte-Carlo sampling of Feynman graphs, and compute the primitive contribution to the beta function of $\\phi^4$-theory at $L\\in \\left \\lbrace 13, 14, 15, 16 \\right \\rbrace $ loops.","Our results confirm the previously known numerical estimates of the primitive beta function and improve their accuracy.","Compared to uniform random sampling of graphs, our new algorithm reaches 35-fold higher accuracy in fixed runtime, or requires 1000-fold less runtime to reach a given accuracy.   ","The data set of all periods computed for this work, combined with a previous data set, is made publicly available.","Besides the physical application, it could serve as a benchmark for graph-based machine learning algorithms."],"url":"http://arxiv.org/abs/2403.16217v1","category":"hep-th"}
{"created":"2024-03-24 15:58:48","title":"Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains","abstract":"This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device. This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring. The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device. Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details. The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively. Our code and data are available at https://zero1778.github.io/blur2blur/","sentences":["This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device.","This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring.","The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device.","Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details.","The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively.","Our code and data are available at https://zero1778.github.io/blur2blur/"],"url":"http://arxiv.org/abs/2403.16205v1","category":"cs.CV"}
{"created":"2024-03-24 15:57:24","title":"SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder","abstract":"Detecting structural similarity between queries is essential for selecting examples in in-context learning models. However, assessing structural similarity based solely on the natural language expressions of queries, without considering SQL queries, presents a significant challenge. This paper explores the significance of this similarity metric and proposes a model for accurately estimating it. To achieve this, we leverage a dataset comprising 170k question pairs, meticulously curated to train a similarity prediction model. Our comprehensive evaluation demonstrates that the proposed model adeptly captures the structural similarity between questions, as evidenced by improvements in Kendall-Tau distance and precision@k metrics. Notably, our model outperforms strong competitive embedding models from OpenAI and Cohere. Furthermore, compared to these competitive models, our proposed encoder enhances the downstream performance of NL2SQL models in 1-shot in-context learning scenarios by 1-2\\% for GPT-3.5-turbo, 4-8\\% for CodeLlama-7B, and 2-3\\% for CodeLlama-13B.","sentences":["Detecting structural similarity between queries is essential for selecting examples in in-context learning models.","However, assessing structural similarity based solely on the natural language expressions of queries, without considering SQL queries, presents a significant challenge.","This paper explores the significance of this similarity metric and proposes a model for accurately estimating it.","To achieve this, we leverage a dataset comprising 170k question pairs, meticulously curated to train a similarity prediction model.","Our comprehensive evaluation demonstrates that the proposed model adeptly captures the structural similarity between questions, as evidenced by improvements in Kendall-Tau distance and precision@k metrics.","Notably, our model outperforms strong competitive embedding models from OpenAI and Cohere.","Furthermore, compared to these competitive models, our proposed encoder enhances the downstream performance of NL2SQL models in 1-shot in-context learning scenarios by 1-2\\% for GPT-3.5-turbo, 4-8\\% for CodeLlama-7B, and 2-3\\% for CodeLlama-13B."],"url":"http://arxiv.org/abs/2403.16204v1","category":"cs.CL"}
{"created":"2024-03-24 15:51:17","title":"FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial Temporal Network","abstract":"Biometric authentication, which utilizes contactless features, such as forehead patterns, has become increasingly important for identity verification and access management. The proposed method is based on learning a 3D spatio-spatial temporal convolution to create detailed pictures of forehead patterns. We introduce a new CNN model called the Forehead Spatio-Spatial Temporal Network (FH-SSTNet), which utilizes a 3D CNN architecture with triplet loss to capture distinguishing features. We enhance the model's discrimination capability using Arcloss in the network's head. Experimentation on the Forehead Creases version 1 (FH-V1) dataset, containing 247 unique subjects, demonstrates the superior performance of FH-SSTNet compared to existing methods and pre-trained CNNs like ResNet50, especially for forehead-based user verification. The results demonstrate the superior performance of FH-SSTNet for forehead-based user verification, confirming its effectiveness in identity authentication.","sentences":["Biometric authentication, which utilizes contactless features, such as forehead patterns, has become increasingly important for identity verification and access management.","The proposed method is based on learning a 3D spatio-spatial temporal convolution to create detailed pictures of forehead patterns.","We introduce a new CNN model called the Forehead Spatio-Spatial Temporal Network (FH-SSTNet), which utilizes a 3D CNN architecture with triplet loss to capture distinguishing features.","We enhance the model's discrimination capability using Arcloss in the network's head.","Experimentation on the Forehead Creases version 1 (FH-V1) dataset, containing 247 unique subjects, demonstrates the superior performance of FH-SSTNet compared to existing methods and pre-trained CNNs like ResNet50, especially for forehead-based user verification.","The results demonstrate the superior performance of FH-SSTNet for forehead-based user verification, confirming its effectiveness in identity authentication."],"url":"http://arxiv.org/abs/2403.16202v1","category":"cs.CV"}
{"created":"2024-03-24 15:09:03","title":"Site-Specific Beam Alignment in 6G via Deep Learning","abstract":"Beam alignment (BA) in modern millimeter wave standards such as 5G NR and WiGig (802.11ay) is based on exhaustive and/or hierarchical beam searches over pre-defined codebooks of wide and narrow beams. This approach is slow and bandwidth/power-intensive, and is a considerable hindrance to the wide deployment of millimeter wave bands. A new approach is needed as we move towards 6G. BA is a promising use case for deep learning (DL) in the 6G air interface, offering the possibility of automated custom tuning of the BA procedure for each cell based on its unique propagation environment and user equipment (UE) location patterns. We overview and advocate for such an approach in this paper, which we term site-specific beam alignment (SSBA). SSBA largely eliminates wasteful searches and allows UEs to be found much more quickly and reliably, without many of the drawbacks of other machine learning-aided approaches. We first overview and demonstrate new results on SSBA, then identify the key open challenges facing SSBA.","sentences":["Beam alignment (BA) in modern millimeter wave standards such as 5G NR and WiGig (802.11ay) is based on exhaustive and/or hierarchical beam searches over pre-defined codebooks of wide and narrow beams.","This approach is slow and bandwidth/power-intensive, and is a considerable hindrance to the wide deployment of millimeter wave bands.","A new approach is needed as we move towards 6G. BA is a promising use case for deep learning (DL) in the 6G air interface, offering the possibility of automated custom tuning of the BA procedure for each cell based on its unique propagation environment and user equipment (UE) location patterns.","We overview and advocate for such an approach in this paper, which we term site-specific beam alignment (SSBA).","SSBA largely eliminates wasteful searches and allows UEs to be found much more quickly and reliably, without many of the drawbacks of other machine learning-aided approaches.","We first overview and demonstrate new results on SSBA, then identify the key open challenges facing SSBA."],"url":"http://arxiv.org/abs/2403.16186v1","category":"cs.IT"}
{"created":"2024-03-24 15:00:44","title":"EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World","abstract":"Being able to map the activities of others into one's own point of view is one fundamental human skill even from a very early age. Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos. Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories. Along with the videos we record high-quality gaze data and provide detailed multimodal annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints. To this end, we present benchmarks such as cross-view association, cross-view action planning, and cross-view referenced skill assessment, along with detailed analysis. We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world. Code and data can be found at: https://github.com/OpenGVLab/EgoExoLearn","sentences":["Being able to map the activities of others into one's own point of view is one fundamental human skill even from a very early age.","Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos.","Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories.","Along with the videos we record high-quality gaze data and provide detailed multimodal annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints.","To this end, we present benchmarks such as cross-view association, cross-view action planning, and cross-view referenced skill assessment, along with detailed analysis.","We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world.","Code and data can be found at: https://github.com/OpenGVLab/EgoExoLearn"],"url":"http://arxiv.org/abs/2403.16182v1","category":"cs.CV"}
{"created":"2024-03-24 14:21:06","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models","abstract":"Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, where it selectively penalizes hallucinated tokens according to their token-level hallucination scores. Our framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2 by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved solely through signals derived from the image itself, without the need for any image-text pairs.","sentences":["Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions.","Current methods fall short of accurately identifying and mitigating these hallucinations.","To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens.","Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image.","This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption.","Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination.","Finally, ESREAL employs a proximal policy optimization algorithm, where it selectively penalizes hallucinated tokens according to their token-level hallucination scores.","Our framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2 by 32.81%, 27.08%, and 7.46% on the CHAIR metric.","This improvement is achieved solely through signals derived from the image itself, without the need for any image-text pairs."],"url":"http://arxiv.org/abs/2403.16167v1","category":"cs.CV"}
{"created":"2024-03-24 13:54:05","title":"Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning","abstract":"Project-based learning (PBL) is an instructional method that is very helpful in nurturing students' creativity, but it requires significant time and energy from both students and teachers. Large language models (LLMs) have been proven to assist in creative tasks, yet much controversy exists regarding their role in fostering creativity. This paper explores the potential of LLMs in PBL settings, with a special focus on fostering creativity. We began with an exploratory study involving 12 middle school students and identified five design considerations for LLM applications in PBL. Building on this, we developed an LLM-empowered, 48-hour PBL program and conducted an instructional experiment with 31 middle school students. Our results indicated that LLMs can enhance every stage of PBL. Additionally, we also discovered ambivalent perspectives among students and mentors toward LLM usage. Furthermore, we explored the challenge and design implications of integrating LLMs into PBL and reflected on the program. By bridging AI advancements into educational practice, our work aims to inspire further discourse and investigation into harnessing AI's potential in child-centric educational settings.","sentences":["Project-based learning (PBL) is an instructional method that is very helpful in nurturing students' creativity, but it requires significant time and energy from both students and teachers.","Large language models (LLMs) have been proven to assist in creative tasks, yet much controversy exists regarding their role in fostering creativity.","This paper explores the potential of LLMs in PBL settings, with a special focus on fostering creativity.","We began with an exploratory study involving 12 middle school students and identified five design considerations for LLM applications in PBL.","Building on this, we developed an LLM-empowered, 48-hour PBL program and conducted an instructional experiment with 31 middle school students.","Our results indicated that LLMs can enhance every stage of PBL.","Additionally, we also discovered ambivalent perspectives among students and mentors toward LLM usage.","Furthermore, we explored the challenge and design implications of integrating LLMs into PBL and reflected on the program.","By bridging AI advancements into educational practice, our work aims to inspire further discourse and investigation into harnessing AI's potential in child-centric educational settings."],"url":"http://arxiv.org/abs/2403.16159v1","category":"cs.HC"}
{"created":"2024-03-24 13:31:31","title":"CFAT: Unleashing TriangularWindows for Image Super-resolution","abstract":"Transformer-based models have revolutionized the field of image super-resolution (SR) by harnessing their inherent ability to capture complex contextual features. The overlapping rectangular shifted window technique used in transformer architecture nowadays is a common practice in super-resolution models to improve the quality and robustness of image upscaling. However, it suffers from distortion at the boundaries and has limited unique shifting modes. To overcome these weaknesses, we propose a non-overlapping triangular window technique that synchronously works with the rectangular one to mitigate boundary-level distortion and allows the model to access more unique sifting modes. In this paper, we propose a Composite Fusion Attention Transformer (CFAT) that incorporates triangular-rectangular window-based local attention with a channel-based global attention technique in image super-resolution. As a result, CFAT enables attention mechanisms to be activated on more image pixels and captures long-range, multi-scale features to improve SR performance. The extensive experimental results and ablation study demonstrate the effectiveness of CFAT in the SR domain. Our proposed model shows a significant 0.7 dB performance improvement over other state-of-the-art SR architectures.","sentences":["Transformer-based models have revolutionized the field of image super-resolution (SR) by harnessing their inherent ability to capture complex contextual features.","The overlapping rectangular shifted window technique used in transformer architecture nowadays is a common practice in super-resolution models to improve the quality and robustness of image upscaling.","However, it suffers from distortion at the boundaries and has limited unique shifting modes.","To overcome these weaknesses, we propose a non-overlapping triangular window technique that synchronously works with the rectangular one to mitigate boundary-level distortion and allows the model to access more unique sifting modes.","In this paper, we propose a Composite Fusion Attention Transformer (CFAT) that incorporates triangular-rectangular window-based local attention with a channel-based global attention technique in image super-resolution.","As a result, CFAT enables attention mechanisms to be activated on more image pixels and captures long-range, multi-scale features to improve SR performance.","The extensive experimental results and ablation study demonstrate the effectiveness of CFAT in the SR domain.","Our proposed model shows a significant 0.7 dB performance improvement over other state-of-the-art SR architectures."],"url":"http://arxiv.org/abs/2403.16143v1","category":"eess.IV"}
{"created":"2024-03-24 13:21:58","title":"A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish","abstract":"Large Language Models (LLMs) are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data. Additionally, we propose a self-detection approach that uses few-shot learning in which LLMs detect whether instances are present or absent in their training data, in contrast to previous methods that do not employ explicit learning. To explore the ease of generating leaked information, we create a dataset of prompts designed to elicit personal information, copyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs produce leaked information in most cases despite less such data in their training set. This indicates even small amounts of leaked data can greatly affect outputs. Our self-detection method showed superior performance compared to existing detection methods.","sentences":["Large Language Models (LLMs) are trained on massive web-crawled corpora.","This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets.","Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance.","We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data.","Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate.","In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data.","Additionally, we propose a self-detection approach that uses few-shot learning in which LLMs detect whether instances are present or absent in their training data, in contrast to previous methods that do not employ explicit learning.","To explore the ease of generating leaked information, we create a dataset of prompts designed to elicit personal information, copyrighted text, and benchmarks from LLMs.","Our experiments reveal that LLMs produce leaked information in most cases despite less such data in their training set.","This indicates even small amounts of leaked data can greatly affect outputs.","Our self-detection method showed superior performance compared to existing detection methods."],"url":"http://arxiv.org/abs/2403.16139v1","category":"cs.CL"}
{"created":"2024-03-24 12:58:48","title":"A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation","abstract":"This paper explores techniques that focus on understanding and resolving ambiguity in language within the field of natural language processing (NLP), highlighting the complexity of linguistic phenomena such as polysemy and homonymy and their implications for computational models. Focusing extensively on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from deep learning techniques to leveraging lexical resources and knowledge graphs like WordNet. The paper introduces cutting-edge methodologies like word sense extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy by predicting new word senses. It examines specific applications in biomedical disambiguation and language specific optimisation and discusses the significance of cognitive metaphors in discourse analysis. The research identifies persistent challenges in the field, such as the scarcity of sense annotated corpora and the complexity of informal clinical texts. It concludes by suggesting future directions, including using large language models, visual WSD, and multilingual WSD systems, emphasising the ongoing evolution in addressing lexical complexities in NLP. This thinking perspective highlights the advancement in this field to enable computers to understand language more accurately.","sentences":["This paper explores techniques that focus on understanding and resolving ambiguity in language within the field of natural language processing (NLP), highlighting the complexity of linguistic phenomena such as polysemy and homonymy and their implications for computational models.","Focusing extensively on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from deep learning techniques to leveraging lexical resources and knowledge graphs like WordNet.","The paper introduces cutting-edge methodologies like word sense extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy by predicting new word senses.","It examines specific applications in biomedical disambiguation and language specific optimisation and discusses the significance of cognitive metaphors in discourse analysis.","The research identifies persistent challenges in the field, such as the scarcity of sense annotated corpora and the complexity of informal clinical texts.","It concludes by suggesting future directions, including using large language models, visual WSD, and multilingual WSD systems, emphasising the ongoing evolution in addressing lexical complexities in NLP.","This thinking perspective highlights the advancement in this field to enable computers to understand language more accurately."],"url":"http://arxiv.org/abs/2403.16129v1","category":"cs.CL"}
{"created":"2024-03-24 10:57:08","title":"LLMs as Compiler for Arabic Programming Language","abstract":"In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code. Designing a full pipeline from the structure of the APL text then a prompt (using prompt engineering) then running the prodcued python code using PyRunner. This project has a three parts first python library, a playground with simple interface and this research paper.","sentences":["In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code.","Designing a full pipeline from the structure of the APL text then a prompt (using prompt engineering) then running the prodcued python code using PyRunner.","This project has a three parts first python library, a playground with simple interface and this research paper."],"url":"http://arxiv.org/abs/2403.16087v1","category":"cs.SE"}
{"created":"2024-03-24 09:42:05","title":"Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy","abstract":"Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications. One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process. AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE. In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system. We have good reasons to expect an improved performance. To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for self-supervised learning. The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction. Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective.","sentences":["Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications.","One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process.","AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE.","In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system.","We have good reasons to expect an improved performance.","To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for self-supervised learning.","The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction.","Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective."],"url":"http://arxiv.org/abs/2403.16078v1","category":"cs.SD"}
{"created":"2024-03-24 08:06:34","title":"Manifold Regularization Classification Model Based On Improved Diffusion Map","abstract":"Manifold regularization model is a semi-supervised learning model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers. However, the original manifold norm limits the performance of models to local regions. To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model. We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold. Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps. Subsequently, we extend the label propagation function to the entire data manifold. We prove that the extended label propagation function converges to a stable distribution after a sufficiently long time and can be considered as a classifier. Building upon this concept, we propose a viable improvement to the manifold regularization model and validate its superiority through experiments.","sentences":["Manifold regularization model is a semi-supervised learning model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers.","However, the original manifold norm limits the performance of models to local regions.","To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model.","We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold.","Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps.","Subsequently, we extend the label propagation function to the entire data manifold.","We prove that the extended label propagation function converges to a stable distribution after a sufficiently long time and can be considered as a classifier.","Building upon this concept, we propose a viable improvement to the manifold regularization model and validate its superiority through experiments."],"url":"http://arxiv.org/abs/2403.16059v1","category":"stat.ML"}
{"created":"2024-03-24 07:29:12","title":"Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning","abstract":"Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply this method to public bicycle rental-and-return data in Seoul, South Korea, employing a spatial-temporal convolutional graph attention network. Our improved architecture incorporates batch attention and modified node feature updates for better prediction accuracy across different time scales. We demonstrate the effectiveness of our framework in predicting temporal patterns and its potential applications.","sentences":["Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories.","To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning.","Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns.","Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations.","In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches.","The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before.","We apply this method to public bicycle rental-and-return data in Seoul, South Korea, employing a spatial-temporal convolutional graph attention network.","Our improved architecture incorporates batch attention and modified node feature updates for better prediction accuracy across different time scales.","We demonstrate the effectiveness of our framework in predicting temporal patterns and its potential applications."],"url":"http://arxiv.org/abs/2403.16049v1","category":"cs.LG"}
{"created":"2024-03-24 07:29:04","title":"Edit3K: Universal Representation Learning for Video Editing Components","abstract":"This paper focuses on understanding the predominant video creation pipeline, i.e., compositional video editing with six main types of editing components, including video effects, animation, transition, filter, sticker, and text. In contrast to existing visual representation learning of visual materials (i.e., images/videos), we aim to learn visual representations of editing actions/components that are generally applied on raw materials. We start by proposing the first large-scale dataset for editing components of video creation, which covers about $3,094$ editing components with $618,800$ videos. Each video in our dataset is rendered by various image/video materials with a single editing component, which supports atomic visual understanding of different editing components. It can also benefit several downstream tasks, e.g., editing component recommendation, editing component recognition/retrieval, etc. Existing visual representation methods perform poorly because it is difficult to disentangle the visual appearance of editing components from raw materials. To that end, we benchmark popular alternative solutions and propose a novel method that learns to attend to the appearance of editing components regardless of raw materials. Our method achieves favorable results on editing component retrieval/recognition compared to the alternative solutions. A user study is also conducted to show that our representations cluster visually similar editing components better than other alternatives. Furthermore, our learned representations used to transition recommendation tasks achieve state-of-the-art results on the AutoTransition dataset. The code and dataset will be released for academic use.","sentences":["This paper focuses on understanding the predominant video creation pipeline, i.e., compositional video editing with six main types of editing components, including video effects, animation, transition, filter, sticker, and text.","In contrast to existing visual representation learning of visual materials (i.e., images/videos), we aim to learn visual representations of editing actions/components that are generally applied on raw materials.","We start by proposing the first large-scale dataset for editing components of video creation, which covers about $3,094$ editing components with $618,800$ videos.","Each video in our dataset is rendered by various image/video materials with a single editing component, which supports atomic visual understanding of different editing components.","It can also benefit several downstream tasks, e.g., editing component recommendation, editing component recognition/retrieval, etc.","Existing visual representation methods perform poorly because it is difficult to disentangle the visual appearance of editing components from raw materials.","To that end, we benchmark popular alternative solutions and propose a novel method that learns to attend to the appearance of editing components regardless of raw materials.","Our method achieves favorable results on editing component retrieval/recognition compared to the alternative solutions.","A user study is also conducted to show that our representations cluster visually similar editing components better than other alternatives.","Furthermore, our learned representations used to transition recommendation tasks achieve state-of-the-art results on the AutoTransition dataset.","The code and dataset will be released for academic use."],"url":"http://arxiv.org/abs/2403.16048v1","category":"cs.CV"}
{"created":"2024-03-24 06:14:50","title":"Learning Directed Acyclic Graphs from Partial Orderings","abstract":"Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.","sentences":["Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables.","In general, learning the DAG structure is both computationally and statistically challenging.","Moreover, without additional information, the direction of edges may not be estimable from observational data.","In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions.","In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available.","We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems.","The advantages of the proposed framework are illustrated via numerical studies."],"url":"http://arxiv.org/abs/2403.16031v1","category":"stat.ML"}
{"created":"2024-03-24 05:13:37","title":"MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment","abstract":"The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms. Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/ .","sentences":["The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots.","However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration.","To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios.","MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications.","We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms.","Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions.","MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning.","For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/ ."],"url":"http://arxiv.org/abs/2403.16015v1","category":"cs.RO"}
{"created":"2024-03-25 12:26:04","title":"Selective laser etching of displays: Closing the gap between optical simulations and fabrication","abstract":"Simulations and measurements on selective laser etching of display glasses are reported. By means of a holographic 3D beam splitter, ultrashort laser pulses are focused inside the volume of a glass sample creating type III modifications along a specific trajectory like pearls on a string. Superimposed by a feed of the glass sample a full 3D area of modifications is achieved building the cornerstone for subsequent etch processes. Based on KOH the modifications are selectively etched at a much higher rate compared to unmodified regions resulting in a separation of the glass along the trajectory of modifications. For gaining further insight into the etch process, we perform simulations on this wet chemical process and compare it to our experimental results.","sentences":["Simulations and measurements on selective laser etching of display glasses are reported.","By means of a holographic 3D beam splitter, ultrashort laser pulses are focused inside the volume of a glass sample creating type III modifications along a specific trajectory like pearls on a string.","Superimposed by a feed of the glass sample a full 3D area of modifications is achieved building the cornerstone for subsequent etch processes.","Based on KOH the modifications are selectively etched at a much higher rate compared to unmodified regions resulting in a separation of the glass along the trajectory of modifications.","For gaining further insight into the etch process, we perform simulations on this wet chemical process and compare it to our experimental results."],"url":"http://arxiv.org/abs/2403.16692v1","category":"physics.optics"}
{"created":"2024-03-25 10:44:57","title":"Pulse-shaping in the interaction of an elliptically-polarized laser and magnetized-plasma","abstract":"Pulse shaping provides a significant level of control and precision when optimizing laser-plasma interactions. Pulse shaping enables precise control and manipulation, resulting in enhanced energy deposition, optimized particle acceleration, controlled polarization, and exploitation of resonant effects. The present study investigates the interaction of structured light with magnetized plasma, considering various spatial profiles and polarization states. This phenomenon involves modification of the temporal and spatial characteristics of the laser pulse due to the presence of the magnetized plasma. The discussion reveals how the electric field and electron velocity evolve within the plasma both spatially and temporally. Factors such as absorption, dispersion, collisions, and scattering are taken into account to understand how they influence the evolution of the pulse. The effects of electron density, external magnetic fields, relativistic velocities, and polarization states on pulse compression are examined. The spatial laser profile impact on pulse-shaping and plasma channel formation is also discussed. This exploration sheds light on the intricate interplays and potential pulse-shaping applications in laser-plasma interactions.","sentences":["Pulse shaping provides a significant level of control and precision when optimizing laser-plasma interactions.","Pulse shaping enables precise control and manipulation, resulting in enhanced energy deposition, optimized particle acceleration, controlled polarization, and exploitation of resonant effects.","The present study investigates the interaction of structured light with magnetized plasma, considering various spatial profiles and polarization states.","This phenomenon involves modification of the temporal and spatial characteristics of the laser pulse due to the presence of the magnetized plasma.","The discussion reveals how the electric field and electron velocity evolve within the plasma both spatially and temporally.","Factors such as absorption, dispersion, collisions, and scattering are taken into account to understand how they influence the evolution of the pulse.","The effects of electron density, external magnetic fields, relativistic velocities, and polarization states on pulse compression are examined.","The spatial laser profile impact on pulse-shaping and plasma channel formation is also discussed.","This exploration sheds light on the intricate interplays and potential pulse-shaping applications in laser-plasma interactions."],"url":"http://arxiv.org/abs/2403.16615v1","category":"physics.plasm-ph"}
{"created":"2024-03-25 09:58:17","title":"A nonlocal approach to graded surface modeling in topology optimization","abstract":"Additively manufactured structures often exhibit a correlation between their mechanical properties, such as stiffness, strength, and porosity, and their wall thickness. This correlation stems from the interplay between the manufacturing process and the properties of the filler material. In this study, we investigate the thickness-dependent effect on structural stiffness and propose a nonlocal integral model that introduces surface grading of Young's modulus to capture this phenomenon. We incorporate this model into topology optimization for designing structures with optimized compliance subject to a volume constraint. Notably, elastically degraded surfaces penalize excessively thin features, effectively eliminating them from the optimized design. We showcase the efficacy of our proposed framework by optimizing the design of a two-dimensional cantilever beam and a bridge.","sentences":["Additively manufactured structures often exhibit a correlation between their mechanical properties, such as stiffness, strength, and porosity, and their wall thickness.","This correlation stems from the interplay between the manufacturing process and the properties of the filler material.","In this study, we investigate the thickness-dependent effect on structural stiffness and propose a nonlocal integral model that introduces surface grading of Young's modulus to capture this phenomenon.","We incorporate this model into topology optimization for designing structures with optimized compliance subject to a volume constraint.","Notably, elastically degraded surfaces penalize excessively thin features, effectively eliminating them from the optimized design.","We showcase the efficacy of our proposed framework by optimizing the design of a two-dimensional cantilever beam and a bridge."],"url":"http://arxiv.org/abs/2403.16587v1","category":"math.OC"}
{"created":"2024-03-25 07:44:17","title":"An experimental evaluation of choices of SSA forecasting parameters","abstract":"Six time series related to atmospheric phenomena are used as inputs for experiments offorecasting with singular spectrum analysis (SSA). Existing methods for SSA parametersselection are compared throughout their forecasting accuracy relatively to an optimal aposteriori selection and to a naive forecasting methods. The comparison shows that awidespread practice of selecting longer windows leads often to poorer predictions. It alsoconfirms that the choices of the window length and of the grouping are essential. Withthe mean error of rainfall forecasting below 1.5%, SSA appears as a viable alternative forhorizons beyond two weeks.","sentences":["Six time series related to atmospheric phenomena are used as inputs for experiments offorecasting with singular spectrum analysis (SSA).","Existing methods for SSA parametersselection are compared throughout their forecasting accuracy relatively to an optimal aposteriori selection and to a naive forecasting methods.","The comparison shows that awidespread practice of selecting longer windows leads often to poorer predictions.","It alsoconfirms that the choices of the window length and of the grouping are essential.","Withthe mean error of rainfall forecasting below 1.5%, SSA appears as a viable alternative forhorizons beyond two weeks."],"url":"http://arxiv.org/abs/2403.16507v1","category":"cs.CE"}
{"created":"2024-03-25 07:07:01","title":"Low-rank quaternion tensor completion for color video inpainting via a novel factorization strategy","abstract":"Recently, a quaternion tensor product named Qt-product was proposed, and then the singular value decomposition and the rank of a third-order quaternion tensor were given. From a more applicable perspective, we extend the Qt-product and propose a novel multiplication principle for third-order quaternion tensor named gQt-product. With the gQt-product, we introduce a brand-new singular value decomposition for third-order quaternion tensors named gQt-SVD and then define gQt-rank and multi-gQt-rank. We prove that the optimal low-rank approximation of a third-order quaternion tensor exists and some numerical experiments demonstrate the low-rankness of color videos. So, we apply the low-rank quaternion tensor completion to color video inpainting problems and present alternating least-square algorithms to solve the proposed low gQt-rank and multi-gQt-rank quaternion tensor completion models. The convergence analyses of the proposed algorithms are established and some numerical experiments on various color video datasets show the high recovery accuracy and computational efficiency of our methods.","sentences":["Recently, a quaternion tensor product named Qt-product was proposed, and then the singular value decomposition and the rank of a third-order quaternion tensor were given.","From a more applicable perspective, we extend the Qt-product and propose a novel multiplication principle for third-order quaternion tensor named gQt-product.","With the gQt-product, we introduce a brand-new singular value decomposition for third-order quaternion tensors named gQt-SVD and then define gQt-rank and multi-gQt-rank.","We prove that the optimal low-rank approximation of a third-order quaternion tensor exists and some numerical experiments demonstrate the low-rankness of color videos.","So, we apply the low-rank quaternion tensor completion to color video inpainting problems and present alternating least-square algorithms to solve the proposed low gQt-rank and multi-gQt-rank quaternion tensor completion models.","The convergence analyses of the proposed algorithms are established and some numerical experiments on various color video datasets show the high recovery accuracy and computational efficiency of our methods."],"url":"http://arxiv.org/abs/2403.16480v1","category":"math.OC"}
{"created":"2024-03-25 06:16:45","title":"The $L^p$ restriction bounds for Neumann data on surface","abstract":"Let $\\{u_\\lambda\\}$ be a sequence of $L^2$-normalized Laplacian eigenfunctions on a compact two-dimensional smooth Riemanniann manifold $(M,g)$. We seek to get an $L^p$ restriction bounds of the Neumann data $ \\lambda^{-1} \\partial_\\nu u_{\\lambda}\\,\\vline_\\gamma$ along a unit geodesic $\\gamma$. Using the $T$-$T^*$ argument one can transfer the problem to an estimate of the norm of a Fourier integral operator and show that such bound is $O(\\lambda^{-\\frac{1}p+\\frac{3}2})$. The Van De Corput theorem (Lemma 2.1) plays the crucial role in our proof. Moreover, this upper bound is shown to be optimal.","sentences":["Let $\\{u_\\lambda\\}$ be a sequence of $L^2$-normalized Laplacian eigenfunctions on a compact two-dimensional smooth Riemanniann manifold $(M,g)$.","We seek to get an $L^p$ restriction bounds of the Neumann data $ \\lambda^{-1} \\partial_\\nu u_{\\lambda}\\,\\vline_\\gamma$ along a unit geodesic $\\gamma$. Using the $T$-$T^*$ argument one can transfer the problem to an estimate of the norm of a Fourier integral operator and show that such bound is $O(\\lambda^{-\\frac{1}p+\\frac{3}2})$. The Van De Corput theorem (Lemma 2.1) plays the crucial role in our proof.","Moreover, this upper bound is shown to be optimal."],"url":"http://arxiv.org/abs/2403.16445v1","category":"math.AP"}
{"created":"2024-03-25 04:14:46","title":"Optimal testing in a class of nonregular models","abstract":"This paper studies optimal hypothesis testing for nonregular statistical models with parameter-dependent support. We consider both one-sided and two-sided hypothesis testing and develop asymptotically uniformly most powerful tests based on the likelihood ratio process. The proposed one-sided test involves randomization to achieve asymptotic size control, some tuning constant to avoid discontinuities in the limiting likelihood ratio process, and a user-specified alternative hypothetical value to achieve the asymptotic optimality. Our two-sided test becomes asymptotically uniformly most powerful without imposing further restrictions such as unbiasedness. Simulation results illustrate desirable power properties of the proposed tests.","sentences":["This paper studies optimal hypothesis testing for nonregular statistical models with parameter-dependent support.","We consider both one-sided and two-sided hypothesis testing and develop asymptotically uniformly most powerful tests based on the likelihood ratio process.","The proposed one-sided test involves randomization to achieve asymptotic size control, some tuning constant to avoid discontinuities in the limiting likelihood ratio process, and a user-specified alternative hypothetical value to achieve the asymptotic optimality.","Our two-sided test becomes asymptotically uniformly most powerful without imposing further restrictions such as unbiasedness.","Simulation results illustrate desirable power properties of the proposed tests."],"url":"http://arxiv.org/abs/2403.16413v1","category":"math.ST"}
{"created":"2024-03-25 02:16:25","title":"Quantum Networks Enhanced by Distributed Quantum Memories","abstract":"Building large-scale quantum communication networks has its unique challenges. Here, we demonstrate that a network-wide synergistic usage of quantum memories distributed in a quantum communication network offers a fundamental advantage. We first map the problem of quantum communication with local usage of memories into a classical continuum percolation model. Then, we show that this mapping can be improved through a cooperation of entanglement distillation and relay protocols via remote access to distributed memories. This improved mapping, which we term $\\alpha$-percolation, can be formulated in terms of graph-merging rules, analogous to the decimation rules of the renormalization group treatment of disordered quantum magnets. These rules can be performed in any order, yielding the same optimal result, which is characterized by the emergence of a ``positive feedback'' mechanism and the formation of spatially disconnected ``hopping'' communication components -- both marking significant improvements in quantum network connectivity.","sentences":["Building large-scale quantum communication networks has its unique challenges.","Here, we demonstrate that a network-wide synergistic usage of quantum memories distributed in a quantum communication network offers a fundamental advantage.","We first map the problem of quantum communication with local usage of memories into a classical continuum percolation model.","Then, we show that this mapping can be improved through a cooperation of entanglement distillation and relay protocols via remote access to distributed memories.","This improved mapping, which we term $\\alpha$-percolation, can be formulated in terms of graph-merging rules, analogous to the decimation rules of the renormalization group treatment of disordered quantum magnets.","These rules can be performed in any order, yielding the same optimal result, which is characterized by the emergence of a ``positive feedback'' mechanism and the formation of spatially disconnected ``hopping'' communication components -- both marking significant improvements in quantum network connectivity."],"url":"http://arxiv.org/abs/2403.16367v1","category":"quant-ph"}
{"created":"2024-03-25 00:28:46","title":"ethraid: A simple method for characterizing long-period companions using Doppler, astrometric, and imaging constraints","abstract":"We present \\texttt{ethraid}, an open source Python package designed to measure the mass ($m_c$) and separation ($a$) of a bound companion from measurements covering a fraction of the orbital period. \\texttt{ethraid} constrains $m_c$ and $a$ by jointly modeling radial velocity (RV), astrometric, and/or direct imaging data in a Bayesian framework. Partial orbit data sets, especially those with highly limited phase coverage, are well-represented by a few method-specific summary statistics. By modeling these statistics rather than the original data, \\texttt{ethraid} optimizes computational efficiency with minimal reduction in accuracy. \\texttt{ethraid} uses importance sampling to efficiently explore the often broad posteriors that arise from partial orbits. The core computations of \\texttt{ethraid} are implemented in Cython for speed. We validate \\texttt{ethraid}'s performance by using it to constrain the masses and separations of the planetary companions to HD 117207 and TOI-1694. We designed \\texttt{ethraid} to be both fast and simple, and to give broad, \"quick look\" constraints on companion parameters using minimal data. \\texttt{ethraid} is pip installable and available on Github.","sentences":["We present \\texttt{ethraid}, an open source Python package designed to measure the mass ($m_c$) and separation ($a$) of a bound companion from measurements covering a fraction of the orbital period.","\\texttt{ethraid} constrains $m_c$ and $a$ by jointly modeling radial velocity (RV), astrometric, and/or direct imaging data in a Bayesian framework.","Partial orbit data sets, especially those with highly limited phase coverage, are well-represented by a few method-specific summary statistics.","By modeling these statistics rather than the original data, \\texttt{ethraid} optimizes computational efficiency with minimal reduction in accuracy.","\\texttt{ethraid} uses importance sampling to efficiently explore the often broad posteriors that arise from partial orbits.","The core computations of \\texttt{ethraid} are implemented in Cython for speed.","We validate \\texttt{ethraid}'s performance by using it to constrain the masses and separations of the planetary companions to HD 117207 and TOI-1694.","We designed \\texttt{ethraid} to be both fast and simple, and to give broad, \"quick look\" constraints on companion parameters using minimal data.","\\texttt{ethraid} is pip installable and available on Github."],"url":"http://arxiv.org/abs/2403.16340v1","category":"astro-ph.EP"}
{"created":"2024-03-24 21:51:02","title":"Electron Thermometry","abstract":"The performance and accuracy of quantum electronics is substantially degraded when the temperature of the electrons in the devices is too high. The electron temperature can be reduced with appropriate thermal anchoring and by filtering both the low frequency and radio frequency noise. Ultimately, for high performance filters the electron temperature can approach the phonon temperature (as measured by resistive thermometers) in a dilution refrigerator. In this application note, the method for measuring the electron temperature in a typical quantum electronics device using Coulomb blockade thermometry is described. This technique is applied to find the readily achievable electron temperature in the device when using the QFilter provided by QDevil. With our thermometry measurements, using a single GaAs/AlGaAs quantum dot in an optimized experimental setup, we determined an electron temperature of 28 $\\pm$ 2 mK for a dilution refrigerator base temperature of 18 mK.","sentences":["The performance and accuracy of quantum electronics is substantially degraded when the temperature of the electrons in the devices is too high.","The electron temperature can be reduced with appropriate thermal anchoring and by filtering both the low frequency and radio frequency noise.","Ultimately, for high performance filters the electron temperature can approach the phonon temperature (as measured by resistive thermometers) in a dilution refrigerator.","In this application note, the method for measuring the electron temperature in a typical quantum electronics device using Coulomb blockade thermometry is described.","This technique is applied to find the readily achievable electron temperature in the device when using the QFilter provided by QDevil.","With our thermometry measurements, using a single GaAs/AlGaAs quantum dot in an optimized experimental setup, we determined an electron temperature of 28 $\\pm$ 2 mK for a dilution refrigerator base temperature of 18 mK."],"url":"http://arxiv.org/abs/2403.16305v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-24 19:11:30","title":"Road layout in the KPZ class","abstract":"We propose a road layout and traffic model, based on last passage percolation (LPP). An easy naive argument shows that coalescence of traffic trajectories is essential to be considered when observing traffic networks around us. This is a fundamental feature in first passage percolation (FPP) models where nearby geodesics naturally coalesce in search of the easiest passage through the landscape. Road designers seek the same in pursuing cost savings, hence FPP geodesics are straightforward candidates to model road layouts. Unfortunately no detailed knowledge is rigorously available on FPP geodesics. To address this, we use exponential LPP instead to build a stochastic model of road traffic and prove certain characteristics thereof. Cars start from every point of the lattice and follow half-infinite geodesics in random directions. Exponential LPP is known to be in the KPZ universality class and it is widely expected that FPP shares very similar properties, hence our findings should equally apply to FPP-based modelling. We address several traffic-related quantities of this model and compare our theorems to real life road networks.","sentences":["We propose a road layout and traffic model, based on last passage percolation (LPP).","An easy naive argument shows that coalescence of traffic trajectories is essential to be considered when observing traffic networks around us.","This is a fundamental feature in first passage percolation (FPP) models where nearby geodesics naturally coalesce in search of the easiest passage through the landscape.","Road designers seek the same in pursuing cost savings, hence FPP geodesics are straightforward candidates to model road layouts.","Unfortunately no detailed knowledge is rigorously available on FPP geodesics.","To address this, we use exponential LPP instead to build a stochastic model of road traffic and prove certain characteristics thereof.","Cars start from every point of the lattice and follow half-infinite geodesics in random directions.","Exponential LPP is known to be in the KPZ universality class and it is widely expected that FPP shares very similar properties, hence our findings should equally apply to FPP-based modelling.","We address several traffic-related quantities of this model and compare our theorems to real life road networks."],"url":"http://arxiv.org/abs/2403.16268v1","category":"math.PR"}
{"created":"2024-03-24 16:56:07","title":"Evaluation of Greedy and CBF for ETSI non-area GeoNetworking: The impact of DCC","abstract":"This paper evaluates the performance of the two ETSI non-area forwarding algorithms in the GeoNetworking specification: Greedy Forwarding and Non-Area Contention-Based Forwarding (CBF). Non-area forwarding occurs when a packet is sent to a geographical Destination Area from a node located outside of this area, e.g., when a vehicle wants to alert of hazardous events to other vehicles located in a distant geographical area. The evaluation has been carried out both in urban and highway scenarios and takes into account the complete ETSI Architecture, including the interaction with the Decentralized Congestion Control (DCC) mechanism. We have also compared ETSI-defined mechanisms with optimizations found in the literature. Our main findings are that Greedy Forwarding, when combined with DCC, is extremely ineffective even with optimizations, and Non-Area CBFs (both ETSI CBF and an optimized version called S-FoT+) outperform Greedy Forwarding both in highway and urban scenarios.","sentences":["This paper evaluates the performance of the two ETSI non-area forwarding algorithms in the GeoNetworking specification: Greedy Forwarding and Non-Area Contention-Based Forwarding (CBF).","Non-area forwarding occurs when a packet is sent to a geographical Destination Area from a node located outside of this area, e.g., when a vehicle wants to alert of hazardous events to other vehicles located in a distant geographical area.","The evaluation has been carried out both in urban and highway scenarios and takes into account the complete ETSI Architecture, including the interaction with the Decentralized Congestion Control (DCC) mechanism.","We have also compared ETSI-defined mechanisms with optimizations found in the literature.","Our main findings are that Greedy Forwarding, when combined with DCC, is extremely ineffective even with optimizations, and Non-Area CBFs (both ETSI CBF and an optimized version called S-FoT+) outperform Greedy Forwarding both in highway and urban scenarios."],"url":"http://arxiv.org/abs/2403.16237v1","category":"cs.NI"}
{"created":"2024-03-24 16:51:34","title":"Towards multi-messenger observations of core-collapse supernovae harboring choked jets","abstract":"Over the last decade, choked jets have attracted particular attention as potential sources of high-energy cosmic neutrinos. Testing this hypothesis is challenging because of the missing gamma-ray counterpart, hence the identification of other electromagnetic signatures is crucial. A choked-jet source is expected harboring in core-collapse supernovae with extended hydrogen envelopes, leading to the release of ultraviolet and optical emission for a few days. The ultraviolet band will be visible with an unprecedentedly large field of view by the future mission satellite ULTRASAT, for which we investigate the detection prospects in relation to the chocked source visibility in the optical band with the currently operating telescope ZTF. As these sources can produce neutrinos via hadronic and photohadronic interactions in choked jets, we also investigate how neutrino observations by existing Cherenkov high-energy neutrino telescopes (as IceCube and KM3NeT) can be used in association with electromagnetic signals coming from shock breakout events. By considering fiducial parameters of the source population and instruments performances, we estimate the maximum redshift up to which ULTRASAT and ZTF are able to detect ultraviolet and optical signals from these explosions, respectively. Furthermore, we discuss coordinated multi-messenger observations among those instruments and high-energy neutrino telescopes. ULTRASAT will be able to double the volume of sky currently visible by ZTF for the same emitting sources enlarging the sample of observed Type II supernovae by around 60%. For optimized multi-messenger detections, the delay between neutrino produced at the shock breakout occurrence (during the jet propagation inside the stellar envelope) and ULTRASAT observations should be of around 4(5) days, with a subsequent follow-up by instruments like ZTF about one week after.","sentences":["Over the last decade, choked jets have attracted particular attention as potential sources of high-energy cosmic neutrinos.","Testing this hypothesis is challenging because of the missing gamma-ray counterpart, hence the identification of other electromagnetic signatures is crucial.","A choked-jet source is expected harboring in core-collapse supernovae with extended hydrogen envelopes, leading to the release of ultraviolet and optical emission for a few days.","The ultraviolet band will be visible with an unprecedentedly large field of view by the future mission satellite ULTRASAT, for which we investigate the detection prospects in relation to the chocked source visibility in the optical band with the currently operating telescope ZTF.","As these sources can produce neutrinos via hadronic and photohadronic interactions in choked jets, we also investigate how neutrino observations by existing Cherenkov high-energy neutrino telescopes (as IceCube and KM3NeT) can be used in association with electromagnetic signals coming from shock breakout events.","By considering fiducial parameters of the source population and instruments performances, we estimate the maximum redshift up to which ULTRASAT and ZTF are able to detect ultraviolet and optical signals from these explosions, respectively.","Furthermore, we discuss coordinated multi-messenger observations among those instruments and high-energy neutrino telescopes.","ULTRASAT will be able to double the volume of sky currently visible by ZTF for the same emitting sources enlarging the sample of observed Type II supernovae by around 60%.","For optimized multi-messenger detections, the delay between neutrino produced at the shock breakout occurrence (during the jet propagation inside the stellar envelope) and ULTRASAT observations should be of around 4(5) days, with a subsequent follow-up by instruments like ZTF about one week after."],"url":"http://arxiv.org/abs/2403.16234v1","category":"astro-ph.HE"}
{"created":"2024-03-24 16:33:01","title":"A Coupled Optimization Framework for Correlated Equilibria in Normal-Form Game","abstract":"In competitive multi-player interactions, simultaneous optimality is a key requirement for establishing strategic equilibria. This property is explicit when the game-theoretic equilibrium is the simultaneously optimal solution of coupled optimization problems. However, no such optimization problems exist for the correlated equilibrium, a strategic equilibrium where the players can correlate their actions. We address the lack of a coupled optimization framework for the correlated equilibrium by introducing an {unnormalized game} -- an extension of normal-form games in which the player strategies are lifted to unnormalized measures over the joint actions. We show that the set of fully mixed generalized Nash equilibria of this unnormalized game is a subset of the correlated equilibrium of the normal-form game. Furthermore, we introduce an entropy regularization to the unnormalized game and prove that the entropy-regularized generalized Nash equilibrium is a sub-optimal correlated equilibrium of the normal form game where the degree of sub-optimality depends on the magnitude of regularization. We prove that the entropy-regularized unnormalized game has a closed-form solution, and empirically verify its computational efficacy at approximating the correlated equilibrium of normal-form games.","sentences":["In competitive multi-player interactions, simultaneous optimality is a key requirement for establishing strategic equilibria.","This property is explicit when the game-theoretic equilibrium is the simultaneously optimal solution of coupled optimization problems.","However, no such optimization problems exist for the correlated equilibrium, a strategic equilibrium where the players can correlate their actions.","We address the lack of a coupled optimization framework for the correlated equilibrium by introducing an {unnormalized game} -- an extension of normal-form games in which the player strategies are lifted to unnormalized measures over the joint actions.","We show that the set of fully mixed generalized Nash equilibria of this unnormalized game is a subset of the correlated equilibrium of the normal-form game.","Furthermore, we introduce an entropy regularization to the unnormalized game and prove that the entropy-regularized generalized Nash equilibrium is a sub-optimal correlated equilibrium of the normal form game where the degree of sub-optimality depends on the magnitude of regularization.","We prove that the entropy-regularized unnormalized game has a closed-form solution, and empirically verify its computational efficacy at approximating the correlated equilibrium of normal-form games."],"url":"http://arxiv.org/abs/2403.16223v1","category":"cs.GT"}
{"created":"2024-03-24 15:06:24","title":"Passive Screen-to-Camera Communication","abstract":"A recent technology known as transparent screens is transforming windows into displays. These smart windows are present in buses, airports and offices. They can remain transparent, as a normal window, or display relevant information that overlays their panoramic views. In this paper, we propose transforming these windows not only into screens but also into wireless transmitters. To achieve this goal, we build upon the research area of screen-to-camera communication. In this area, videos are modified in a way that smartphone cameras can decode data out of them, while this data remains invisible to the viewers. A person sees a normal video, but the camera sees the video plus additional information. In this communication method, one of the biggest disadvantages is the traditional screens' power consumption, more than 80% of which is used to generate light. To solve this, we employ novel transparent screens relying on ambient light to display pictures, hence eliminating the power source. However, this comes at the cost of a lower image quality, since they use variable and out-of-control environment light, instead of generating a constant and strong light by LED panels. Our work, dubbed PassiveCam, overcomes the challenge of creating the first screen-to-camera communication link using passive displays. This paper presents two main contributions. First, we analyze and modify existing screens and encoding methods to embed information reliably in ambient light. Second, we develop an Android App that optimizes the decoding process, obtaining a real-time performance. Our evaluation, which considers a musical application, shows a Packet Success Rate (PSR) of close to 90%. In addition, our real-time application achieves response times of 530 ms and 1071 ms when the camera is static and when it is hand-held, respectively.","sentences":["A recent technology known as transparent screens is transforming windows into displays.","These smart windows are present in buses, airports and offices.","They can remain transparent, as a normal window, or display relevant information that overlays their panoramic views.","In this paper, we propose transforming these windows not only into screens but also into wireless transmitters.","To achieve this goal, we build upon the research area of screen-to-camera communication.","In this area, videos are modified in a way that smartphone cameras can decode data out of them, while this data remains invisible to the viewers.","A person sees a normal video, but the camera sees the video plus additional information.","In this communication method, one of the biggest disadvantages is the traditional screens' power consumption, more than 80% of which is used to generate light.","To solve this, we employ novel transparent screens relying on ambient light to display pictures, hence eliminating the power source.","However, this comes at the cost of a lower image quality, since they use variable and out-of-control environment light, instead of generating a constant and strong light by LED panels.","Our work, dubbed PassiveCam, overcomes the challenge of creating the first screen-to-camera communication link using passive displays.","This paper presents two main contributions.","First, we analyze and modify existing screens and encoding methods to embed information reliably in ambient light.","Second, we develop an Android App that optimizes the decoding process, obtaining a real-time performance.","Our evaluation, which considers a musical application, shows a Packet Success Rate (PSR) of close to 90%.","In addition, our real-time application achieves response times of 530 ms and 1071 ms when the camera is static and when it is hand-held, respectively."],"url":"http://arxiv.org/abs/2403.16185v1","category":"eess.IV"}
{"created":"2024-03-24 14:24:13","title":"Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method","abstract":"Gaze plays a crucial role in revealing human attention and intention, shedding light on the cognitive processes behind human actions. The integration of gaze guidance with the dynamics of hand-object interactions boosts the accuracy of human motion prediction. However, the lack of datasets that capture the intricate relationship and consistency among gaze, hand, and object movements remains a substantial hurdle. In this paper, we introduce the first Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task for synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI, features simultaneous 3D modeling of gaze, hand, and object interactions, comprising 479 sequences with an average duration of 19.1 seconds, 812 sub-sequences, and 33 objects of various sizes. We propose a hierarchical framework centered on a gaze-guided hand-object interaction diffusion model, named GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions into spatial-temporal features and goal pose conditions at different levels of information granularity. During the diffusion phase, two gaze-conditioned diffusion models are stacked to simplify the complex synthesis of hand-object motions. Here, the object motion diffusion model generates sequences of object motions based on gaze conditions, while the hand motion diffusion model produces hand motions based on the generated object motion. To improve fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint to guide the denoising step. In the subsequent post-diffusion phase, we optimize the generated hand motions using contact consistency. Our extensive experiments highlight the uniqueness of our dataset and the effectiveness of our approach.","sentences":["Gaze plays a crucial role in revealing human attention and intention, shedding light on the cognitive processes behind human actions.","The integration of gaze guidance with the dynamics of hand-object interactions boosts the accuracy of human motion prediction.","However, the lack of datasets that capture the intricate relationship and consistency among gaze, hand, and object movements remains a substantial hurdle.","In this paper, we introduce the first Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task for synthesizing gaze-guided hand-object interactions.","Our dataset, GazeHOI, features simultaneous 3D modeling of gaze, hand, and object interactions, comprising 479 sequences with an average duration of 19.1 seconds, 812 sub-sequences, and 33 objects of various sizes.","We propose a hierarchical framework centered on a gaze-guided hand-object interaction diffusion model, named GHO-Diffusion.","In the pre-diffusion phase, we separate gaze conditions into spatial-temporal features and goal pose conditions at different levels of information granularity.","During the diffusion phase, two gaze-conditioned diffusion models are stacked to simplify the complex synthesis of hand-object motions.","Here, the object motion diffusion model generates sequences of object motions based on gaze conditions, while the hand motion diffusion model produces hand motions based on the generated object motion.","To improve fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint to guide the denoising step.","In the subsequent post-diffusion phase, we optimize the generated hand motions using contact consistency.","Our extensive experiments highlight the uniqueness of our dataset and the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.16169v1","category":"cs.CV"}
{"created":"2024-03-24 13:44:34","title":"Cost of excursions until first return for random walks and L\u00e9vy flights: an exact general formula","abstract":"We consider a discrete-time random walk where a cost is incurred at each jump. We obtain an exact analytical formula for the distribution of the total cost of a trajectory until the process returns to the origin for the first time. The formula is valid for arbitrary jump distribution and cost function (heavy- and light-tailed alike), provided they are symmetric and continuous. The tail behavior of the distribution is universal and independent of the details of the model. Applications are given to the motion of an active particle in one dimension and extensions to multiple cost variables are considered. The analytical results are in perfect agreement with numerical simulations.","sentences":["We consider a discrete-time random walk where a cost is incurred at each jump.","We obtain an exact analytical formula for the distribution of the total cost of a trajectory until the process returns to the origin for the first time.","The formula is valid for arbitrary jump distribution and cost function (heavy- and light-tailed alike), provided they are symmetric and continuous.","The tail behavior of the distribution is universal and independent of the details of the model.","Applications are given to the motion of an active particle in one dimension and extensions to multiple cost variables are considered.","The analytical results are in perfect agreement with numerical simulations."],"url":"http://arxiv.org/abs/2403.16152v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-24 12:13:24","title":"From Clicks to Conversions: Analysis of Traffic Sources in E-Commerce","abstract":"Over the past years, e-commerce platforms have expanded substantially, providing customers with convenient shopping experiences. To enhance e-commerce websites, it's essential to grasp user engagement and factors affecting conversion rates. This optimization is achieved by aligning platforms with user expectations, thereby fostering successful online shopping experiences, and contributing to sustained growth in the dynamic digital marketplace. In this paper, we conduct a comprehensive analysis focusing on user interactions, conversion metrics, and the entire user journey within an e-commerce platform. Our exploration spans exit rates and sessions across different devices and browsers, conversion rates for various traffic sources, and the user journey from product details to checkout. Findings suggest a need for targeted improvements in mobile optimization and browser compatibility, indicated by higher exit rates on mobile devices and varying rates on different browsers. The conversion rate analysis emphasizes the varying effectiveness of traffic sources, highlighting the potential of advertised mediums, while identifying areas for improvement in referral and affiliate traffic. Examining the user journey showed potential bottlenecks in the conversion process, the identified gap was between user interest and completed transactions. Our study suggests improving the checkout process strategically to streamline user transactions. These insights provide actionable guidance for businesses seeking to refine their platforms and optimize performance in the ever-evolving landscape of e-commerce.","sentences":["Over the past years, e-commerce platforms have expanded substantially, providing customers with convenient shopping experiences.","To enhance e-commerce websites, it's essential to grasp user engagement and factors affecting conversion rates.","This optimization is achieved by aligning platforms with user expectations, thereby fostering successful online shopping experiences, and contributing to sustained growth in the dynamic digital marketplace.","In this paper, we conduct a comprehensive analysis focusing on user interactions, conversion metrics, and the entire user journey within an e-commerce platform.","Our exploration spans exit rates and sessions across different devices and browsers, conversion rates for various traffic sources, and the user journey from product details to checkout.","Findings suggest a need for targeted improvements in mobile optimization and browser compatibility, indicated by higher exit rates on mobile devices and varying rates on different browsers.","The conversion rate analysis emphasizes the varying effectiveness of traffic sources, highlighting the potential of advertised mediums, while identifying areas for improvement in referral and affiliate traffic.","Examining the user journey showed potential bottlenecks in the conversion process, the identified gap was between user interest and completed transactions.","Our study suggests improving the checkout process strategically to streamline user transactions.","These insights provide actionable guidance for businesses seeking to refine their platforms and optimize performance in the ever-evolving landscape of e-commerce."],"url":"http://arxiv.org/abs/2403.16115v1","category":"cs.SI"}
{"created":"2024-03-24 11:08:40","title":"beSnake: A routing algorithm for scalable spin-qubit architectures","abstract":"As quantum computing devices become larger with respect to the number of qubits, the realization of two-qubit interactions becomes more challenging, necessitating innovative and scalable qubit routing solutions. In this work, we introduce beSnake, a novel algorithm specifically designed to address the intricate qubit routing challenges in scalable spin-qubit architectures. The algorithm utilizes shuttle operations that physically move a qubit to an adjacent, unoccupied quantum dot. Unlike traditional methods in superconducting architectures that solely rely on SWAP operations, beSnake incorporates those shuttle operations to optimize the execution time of the circuits and achieve higher operational fidelity and quicker computation times of the routing task itself. By employing a breadth-first search approach, beSnake effectively manages the restrictions created by diverse topologies and positions of qubits. It also dynamically tackles parallelized routing tasks with multiple qubits that act as obstacles with the option to adjust the level of optimization. Our simulations demonstrate beSnake's advantage over an existing routing solution for random circuits and real quantum algorithms with up to $1,000$ qubits. It shows on average an up to $80\\%$ and $54\\%$ improvement in gate overhead and depth overhead, respectively, and up to $8.33$ times faster routing time.","sentences":["As quantum computing devices become larger with respect to the number of qubits, the realization of two-qubit interactions becomes more challenging, necessitating innovative and scalable qubit routing solutions.","In this work, we introduce beSnake, a novel algorithm specifically designed to address the intricate qubit routing challenges in scalable spin-qubit architectures.","The algorithm utilizes shuttle operations that physically move a qubit to an adjacent, unoccupied quantum dot.","Unlike traditional methods in superconducting architectures that solely rely on SWAP operations, beSnake incorporates those shuttle operations to optimize the execution time of the circuits and achieve higher operational fidelity and quicker computation times of the routing task itself.","By employing a breadth-first search approach, beSnake effectively manages the restrictions created by diverse topologies and positions of qubits.","It also dynamically tackles parallelized routing tasks with multiple qubits that act as obstacles with the option to adjust the level of optimization.","Our simulations demonstrate beSnake's advantage over an existing routing solution for random circuits and real quantum algorithms with up to $1,000$ qubits.","It shows on average an up to $80\\%$ and $54\\%$ improvement in gate overhead and depth overhead, respectively, and up to $8.33$ times faster routing time."],"url":"http://arxiv.org/abs/2403.16090v1","category":"quant-ph"}
{"created":"2024-03-24 11:05:05","title":"Parallax Effect in Microlensing Events due to Free-Floating Planets","abstract":"One of most important applications of microlensing observations is detecting free-floating planets(FFPs). The time scale of microlensing due to FFPs ($t_{\\rm E}$) is short (a few days). Discerning the annual parallax effect in observations from these short-duration events by one observer is barely possible, though their parallax amplitude is larger than that in common events. In microlensing events due to FFPs, the lens-source relative trajectory alters because of the observer's motion by $\\boldsymbol{\\delta u}$. This deviation is a straight line if $t_{\\rm E} \\ll P_{\\oplus}$, and its size is $\\delta u\\propto \\pi_{\\rm{rel}}$ ($P_{\\oplus}$ is the observer's orbital period). So, most of observed microlensing events due to close FFPs have simple Paczy\\'nsky lightcurves with indiscernible and valuable parallax. To evaluate destructive effects of invisible parallax in such events, we simulate $\\sim9650$ microlensing events due to FFPs with $t_{\\rm E}<10$ days that are observed only by The Nancy Grace Roman Space Telescope(\\wfirst). We conclude that in half of these microlensing events the missing parallax alters the real lightcurves, changing their shape and derived properties(by $\\Delta \\chi^{2}\\gtrsim100$). By fitting Paczy\\'nski lightcurves to these affected events we evaluate the relative and dimensionless deviations in the lensing parameters from their real values ($\\delta t_{\\rm E}, \\delta \\rho_{\\star}, ...$). We conclude that around $46$ FFPs which are discovered by \\wfirst\\ have lightcurves highly affected by invisible parallax with $\\delta t_{\\rm E}>0.1~\\rm{and}~\\delta \\rho_{\\star}>0.1$. Our study reveals the importance of simultaneous and dense observations of microlensing events viewed by \\wfirst\\ by other observers rotating the Sun in different orbits.","sentences":["One of most important applications of microlensing observations is detecting free-floating planets(FFPs).","The time scale of microlensing due to FFPs ($t_{\\rm E}$) is short (a few days).","Discerning the annual parallax effect in observations from these short-duration events by one observer is barely possible, though their parallax amplitude is larger than that in common events.","In microlensing events due to FFPs, the lens-source relative trajectory alters because of the observer's motion by $\\boldsymbol{\\delta u}$. This deviation is a straight line if $t_{\\rm E} \\ll P_{\\oplus}$, and its size is $\\delta u\\propto \\pi_{\\rm{rel}}$ ($P_{\\oplus}$ is the observer's orbital period).","So, most of observed microlensing events due to close FFPs have simple Paczy\\'nsky lightcurves with indiscernible and valuable parallax.","To evaluate destructive effects of invisible parallax in such events, we simulate $\\sim9650$ microlensing events due to FFPs with $t_{\\rm E}<10$ days that are observed only by The Nancy Grace Roman Space Telescope(\\wfirst).","We conclude that in half of these microlensing events the missing parallax alters the real lightcurves, changing their shape and derived properties(by $\\Delta \\chi^{2}\\gtrsim100$).","By fitting Paczy\\'nski lightcurves to these affected events we evaluate the relative and dimensionless deviations in the lensing parameters from their real values ($\\delta t_{\\rm E}, \\delta \\rho_{\\star}, ...$).","We conclude that around $46$ FFPs which are discovered by \\wfirst\\ have lightcurves highly affected by invisible parallax with $\\delta t_{\\rm E}>0.1~\\rm{and}~\\delta \\rho_{\\star}>0.1$.","Our study reveals the importance of simultaneous and dense observations of microlensing events viewed by \\wfirst\\ by other observers rotating the Sun in different orbits."],"url":"http://arxiv.org/abs/2403.16089v1","category":"astro-ph.EP"}
{"created":"2024-03-24 09:38:01","title":"On the Bailout Dividend Problem with Periodic Dividend Payments and Fixed Transaction Costs","abstract":"We study the optimal bailout dividend problem with transaction costs for an insurance company, where shareholder payouts align with the arrival times of an independent Poisson process. In this scenario, the underlying risk model follows a spectrally negative L\\'evy process. Our analysis confirms the optimality of a periodic $(b_{1},b_{2})$-barrier policy with classical reflection at zero. This strategy involves reducing the surplus to $b_1$ when it exceeds $b_{2}$ at the Poisson arrival times and pushes the surplus to 0 whenever it goes below zero.","sentences":["We study the optimal bailout dividend problem with transaction costs for an insurance company, where shareholder payouts align with the arrival times of an independent Poisson process.","In this scenario, the underlying risk model follows a spectrally negative L\\'evy process.","Our analysis confirms the optimality of a periodic $(b_{1},b_{2})$-barrier policy with classical reflection at zero.","This strategy involves reducing the surplus to $b_1$ when it exceeds $b_{2}$ at the Poisson arrival times and pushes the surplus to 0 whenever it goes below zero."],"url":"http://arxiv.org/abs/2403.16077v1","category":"math.OC"}
{"created":"2024-03-24 08:25:42","title":"Explainable Port Mapping Inference with Sparse Performance Counters for AMD's Zen Architectures","abstract":"Performance models are instrumental for optimizing performance-sensitive code. When modeling the use of functional units of out-of-order x86-64 CPUs, data availability varies by the manufacturer: Instruction-to-port mappings for Intel's processors are available, whereas information for AMD's designs are lacking. The reason for this disparity is that standard techniques to infer exact port mappings require hardware performance counters that AMD does not provide.   In this work, we modify the port mapping inference algorithm of the widely used uops.info project to not rely on Intel's performance counters. The modifications are based on a formal port mapping model with a counter-example-guided algorithm powered by an SMT solver. We investigate in how far AMD's processors comply with this model and where unexpected performance characteristics prevent an accurate port mapping. Our results provide valuable insights for creators of CPU performance models as well as for software developers who want to achieve peak performance on recent AMD CPUs.","sentences":["Performance models are instrumental for optimizing performance-sensitive code.","When modeling the use of functional units of out-of-order x86-64 CPUs, data availability varies by the manufacturer: Instruction-to-port mappings for Intel's processors are available, whereas information for AMD's designs are lacking.","The reason for this disparity is that standard techniques to infer exact port mappings require hardware performance counters that AMD does not provide.   ","In this work, we modify the port mapping inference algorithm of the widely used uops.info project to not rely on Intel's performance counters.","The modifications are based on a formal port mapping model with a counter-example-guided algorithm powered by an SMT solver.","We investigate in how far AMD's processors comply with this model and where unexpected performance characteristics prevent an accurate port mapping.","Our results provide valuable insights for creators of CPU performance models as well as for software developers who want to achieve peak performance on recent AMD CPUs."],"url":"http://arxiv.org/abs/2403.16063v1","category":"cs.PF"}
{"created":"2024-03-24 08:01:52","title":"Exponential mixing of constrained random dynamical systems via controllability conditions","abstract":"We provide deterministic controllability conditions that imply exponential mixing properties for randomly forced constrained dynamical systems with possibly unbounded state space. As an application, new ergodicity results are obtained for non-smooth models in elasto-plasticity driven by various types of noise, including white noise. It is thereby illustrated how tools from control theory can be utilized to tackle regularity issues that commonly arise in the qualitative study of constrained systems.","sentences":["We provide deterministic controllability conditions that imply exponential mixing properties for randomly forced constrained dynamical systems with possibly unbounded state space.","As an application, new ergodicity results are obtained for non-smooth models in elasto-plasticity driven by various types of noise, including white noise.","It is thereby illustrated how tools from control theory can be utilized to tackle regularity issues that commonly arise in the qualitative study of constrained systems."],"url":"http://arxiv.org/abs/2403.16058v1","category":"math.OC"}
{"created":"2024-03-24 07:13:33","title":"Digital control of negative imaginary systems: a discrete-time hybrid integrator-gain system approach","abstract":"A hybrid integrator-gain system (HIGS) is a control element that switches between an integrator and a gain, which overcomes some inherent limitations of linear controllers. In this paper, we consider using discrete-time HIGS controllers for the digital control of negative imaginary (NI) systems. We show that the discrete-time HIGS themselves are step-advanced negative imaginary systems. For a minimal linear NI system, there always exists a HIGS controller that can asymptotically stablize it. An illustrative example is provided, where we use the proposed HIGS control method to stabilize a discrete-time mass-spring system.","sentences":["A hybrid integrator-gain system (HIGS) is a control element that switches between an integrator and a gain, which overcomes some inherent limitations of linear controllers.","In this paper, we consider using discrete-time HIGS controllers for the digital control of negative imaginary (NI) systems.","We show that the discrete-time HIGS themselves are step-advanced negative imaginary systems.","For a minimal linear NI system, there always exists a HIGS controller that can asymptotically stablize it.","An illustrative example is provided, where we use the proposed HIGS control method to stabilize a discrete-time mass-spring system."],"url":"http://arxiv.org/abs/2403.16046v1","category":"eess.SY"}
{"created":"2024-03-24 07:10:59","title":"MIMO with Analogue 1-bit Phase Shifters: A Quantum Annealing Perspective","abstract":"In this letter, we study the analogue pre/post-coding vector design for a point-to-point multiple-input multiple-output (MIMO) system with 1-bit phase shifters. Specifically, we focus on the signal-to-noise ratio (SNR) maximization problem which corresponds to a combinatorial NP-hard due to the binary phase resolution. Two classical computation heuristics are proposed i.e., i) an 1-bit real-valued approximation of the optimal digital designs, and ii) an alternating optimization where a Rayleigh quotient problem is solved at each iteration. An iterative quantum annealing (QA)-based heuristic is also investigated, which outperforms classical counterparts and achieves near-optimal performance while ensuring polynomial time complexity. Experimental results in a real-world D-WAVE QA device validate the efficiency of the proposed QA approach.","sentences":["In this letter, we study the analogue pre/post-coding vector design for a point-to-point multiple-input multiple-output (MIMO) system with 1-bit phase shifters.","Specifically, we focus on the signal-to-noise ratio (SNR) maximization problem which corresponds to a combinatorial NP-hard due to the binary phase resolution.","Two classical computation heuristics are proposed i.e., i) an 1-bit real-valued approximation of the optimal digital designs, and ii) an alternating optimization where a Rayleigh quotient problem is solved at each iteration.","An iterative quantum annealing (QA)-based heuristic is also investigated, which outperforms classical counterparts and achieves near-optimal performance while ensuring polynomial time complexity.","Experimental results in a real-world D-WAVE QA device validate the efficiency of the proposed QA approach."],"url":"http://arxiv.org/abs/2403.16045v1","category":"cs.IT"}
{"created":"2024-03-24 04:04:05","title":"Stochastic maximum principle for weighted mean-field system with jump","abstract":"In this article, we consider a weighted mean-field control problem with jump-diffusion as its state process. The main difficulty is from the non-Lipschitz property of the coefficients. We overcome this difficulty by an $L_{p,q}$-estimate of the solution processes with a suitably chosen $p$ and $q$. Convex pertubation method combining with the aforementioned $L_{p,q}$-estimation method is utilized to derive the stochastic maximum principle for this control problem. A sufficient condition for the optimality is also given.","sentences":["In this article, we consider a weighted mean-field control problem with jump-diffusion as its state process.","The main difficulty is from the non-Lipschitz property of the coefficients.","We overcome this difficulty by an $L_{p,q}$-estimate of the solution processes with a suitably chosen $p$ and $q$. Convex pertubation method combining with the aforementioned $L_{p,q}$-estimation method is utilized to derive the stochastic maximum principle for this control problem.","A sufficient condition for the optimality is also given."],"url":"http://arxiv.org/abs/2403.16000v1","category":"math.OC"}
{"created":"2024-03-24 03:10:18","title":"Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control","abstract":"This study introduces a robust planning framework that utilizes a model predictive control (MPC) approach, enhanced by incorporating signal temporal logic (STL) specifications. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion, specifically designed to handle both translational and orientational perturbations. Existing recovery strategies often struggle with reasoning complex task logic and evaluating locomotion robustness systematically, making them susceptible to failures caused by inappropriate recovery strategies or lack of robustness. To address these issues, we design an analytical robustness metric for bipedal locomotion and quantify this metric using STL specifications, which guide the generation of recovery trajectories to achieve maximum locomotion robustness. To enable safe and computational-efficient crossed-leg maneuver, we design data-driven self-leg-collision constraints that are $1000$ times faster than the traditional inverse-kinematics-based approach. Our framework outperforms a state-of-the-art locomotion controller, a standard MPC without STL, and a linear-temporal-logic-based planner in a high-fidelity dynamic simulation, especially in scenarios involving crossed-leg maneuvers. Additionally, the Cassie bipedal robot achieves robust performance under horizontal and orientational perturbations such as those observed in ship motions. These environments are validated in simulations and deployed on hardware. Furthermore, our proposed method demonstrates versatility on stepping stones and terrain-agnostic features on inclined terrains.","sentences":["This study introduces a robust planning framework that utilizes a model predictive control (MPC) approach, enhanced by incorporating signal temporal logic (STL) specifications.","This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion, specifically designed to handle both translational and orientational perturbations.","Existing recovery strategies often struggle with reasoning complex task logic and evaluating locomotion robustness systematically, making them susceptible to failures caused by inappropriate recovery strategies or lack of robustness.","To address these issues, we design an analytical robustness metric for bipedal locomotion and quantify this metric using STL specifications, which guide the generation of recovery trajectories to achieve maximum locomotion robustness.","To enable safe and computational-efficient crossed-leg maneuver, we design data-driven self-leg-collision constraints that are $1000$ times faster than the traditional inverse-kinematics-based approach.","Our framework outperforms a state-of-the-art locomotion controller, a standard MPC without STL, and a linear-temporal-logic-based planner in a high-fidelity dynamic simulation, especially in scenarios involving crossed-leg maneuvers.","Additionally, the Cassie bipedal robot achieves robust performance under horizontal and orientational perturbations such as those observed in ship motions.","These environments are validated in simulations and deployed on hardware.","Furthermore, our proposed method demonstrates versatility on stepping stones and terrain-agnostic features on inclined terrains."],"url":"http://arxiv.org/abs/2403.15993v1","category":"cs.RO"}
{"created":"2024-03-23 23:26:19","title":"Putting all eggs in one basket: some insights from a correlation inequality","abstract":"We give examples of situations -- stochastic production, military tactics, corporate merger -- where it is beneficial to concentrate risk rather than to diversify it, that is, to put all eggs in one basket. Our examples admit a dual interpretation: as optimal strategies of a single player (the `principal') or, alternatively, as dominant strategies in a non-cooperative game with multiple players (the `agents').   The key mathematical result can be formulated in terms of a convolution structure on the set of increasing functions on a Boolean lattice (the lattice of subsets of a finite set). This generalizes the well-known Harris inequality from statistical physics and discrete mathematics; we give a simple self-contained proof of this result, and prove a further generalization based on the game-theoretic approach.","sentences":["We give examples of situations -- stochastic production, military tactics, corporate merger -- where it is beneficial to concentrate risk rather than to diversify it, that is, to put all eggs in one basket.","Our examples admit a dual interpretation: as optimal strategies of a single player (the `principal') or, alternatively, as dominant strategies in a non-cooperative game with multiple players (the `agents').   ","The key mathematical result can be formulated in terms of a convolution structure on the set of increasing functions on a Boolean lattice (the lattice of subsets of a finite set).","This generalizes the well-known Harris inequality from statistical physics and discrete mathematics; we give a simple self-contained proof of this result, and prove a further generalization based on the game-theoretic approach."],"url":"http://arxiv.org/abs/2403.15957v1","category":"math.PR"}
{"created":"2024-03-23 21:39:56","title":"Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update","abstract":"In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\\epsilon$-stationary point. To lower communication complexity in MARL-PE, a \"natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential \"agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesting open question: Can the local TD-update approach entail low sample and communication complexities? In this paper, we make the first attempt to answer this fundamental question. We focus on the setting of MARL-PE with average reward, which is motivated by many multi-agent network optimization problems. Our theoretical and experimental results confirm that allowing multiple local TD-update steps is indeed an effective approach in lowering the sample and communication complexities of MARL-PE compared to consensus-based MARL-PE algorithms. Specifically, the local TD-update steps between two consecutive communication rounds can be as large as $\\mathcal{O}(1/\\epsilon^{1/2}\\log{(1/\\epsilon)})$ in order to converge to an $\\epsilon$-stationary point of MARL-PE. Moreover, we show theoretically that in order to reach the optimal sample complexity, the communication complexity of local TD-update approach is $\\mathcal{O}(1/\\epsilon^{1/2}\\log{(1/\\epsilon)})$.","sentences":["In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors.","In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\\epsilon$-stationary point.","To lower communication complexity in MARL-PE, a \"natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency.","However, the validity of the local TD-update approach remains unclear due to the potential \"agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general.","This leads to an interesting open question: Can the local TD-update approach entail low sample and communication complexities?","In this paper, we make the first attempt to answer this fundamental question.","We focus on the setting of MARL-PE with average reward, which is motivated by many multi-agent network optimization problems.","Our theoretical and experimental results confirm that allowing multiple local TD-update steps is indeed an effective approach in lowering the sample and communication complexities of MARL-PE compared to consensus-based MARL-PE algorithms.","Specifically, the local TD-update steps between two consecutive communication rounds can be as large as $\\mathcal{O}(1/\\epsilon^{1/2}\\log{(1/\\epsilon)})$ in order to converge to an $\\epsilon$-stationary point of MARL-PE.","Moreover, we show theoretically that in order to reach the optimal sample complexity, the communication complexity of local TD-update approach is $\\mathcal{O}(1/\\epsilon^{1/2}\\log{(1/\\epsilon)})$."],"url":"http://arxiv.org/abs/2403.15935v1","category":"cs.LG"}
{"created":"2024-03-23 21:04:48","title":"Towards optimal spatiotemporal wavefront shaping for the cocktail party problem with inverse design of an acoustic reconfigurable metasurface in disordered media","abstract":"Multiple-user multiple-input multiple-output applications have recently gained a lot of attention. Here, we show an efficient optimization formulation for the design of all the temporal and spatial degrees of freedom of an acoustic reconfigurable metasurface for the cocktail party problem. In the frequency domain, the closed-form least square solution matches the optimal time reversal solution for multiple emitter-receiver pairs, optimizing for each frequency independently. This is more efficient than solving in the time domain where the time convolution mixes all the degrees of freedom into a resource-intensive optimization. We illustrate this methodology by optimizing the frequency response of a design for two pairs of emitters-receivers using the Green's functions of disordered media that are measured experimentally. We report strong performance that will be put in perspective in future work, where we will analyze the robustness of the design to noise in the data and design the convolutional filters that match the optimal frequency response for experiment validation of the design.","sentences":["Multiple-user multiple-input multiple-output applications have recently gained a lot of attention.","Here, we show an efficient optimization formulation for the design of all the temporal and spatial degrees of freedom of an acoustic reconfigurable metasurface for the cocktail party problem.","In the frequency domain, the closed-form least square solution matches the optimal time reversal solution for multiple emitter-receiver pairs, optimizing for each frequency independently.","This is more efficient than solving in the time domain where the time convolution mixes all the degrees of freedom into a resource-intensive optimization.","We illustrate this methodology by optimizing the frequency response of a design for two pairs of emitters-receivers using the Green's functions of disordered media that are measured experimentally.","We report strong performance that will be put in perspective in future work, where we will analyze the robustness of the design to noise in the data and design the convolutional filters that match the optimal frequency response for experiment validation of the design."],"url":"http://arxiv.org/abs/2403.15932v1","category":"physics.app-ph"}
{"created":"2024-03-23 20:22:30","title":"Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time","abstract":"In this paper, we present an online reinforcement learning algorithm for constrained Markov decision processes with a safety constraint. Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. To this end, we propose an algorithm based on linear programming that does not require a process model. We show that the learned policy is safe with high confidence. We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. Finally, we provide simulation results to show the efficacy of the proposed algorithm. Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.","sentences":["In this paper, we present an online reinforcement learning algorithm for constrained Markov decision processes with a safety constraint.","Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed.","To this end, we propose an algorithm based on linear programming that does not require a process model.","We show that the learned policy is safe with high confidence.","We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints.","Finally, we provide simulation results to show the efficacy of the proposed algorithm.","Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set."],"url":"http://arxiv.org/abs/2403.15928v1","category":"cs.LG"}
{"created":"2024-03-23 18:54:31","title":"Intersubband polaritonic metasurfaces for high-contrast ultra-fast power limiting and optical switching","abstract":"Nonlinear intersubband polaritonic metasurfaces support one of the strongest known ultrafast nonlinear responses in the mid-infrared frequency range across all condensed matter systems. Beyond harmonic generation and frequency mixing, these nonlinearities can be leveraged for ultrafast optical switching and power limiting, based on tailored transitions from strong to weak polaritonic coupling. Here, we demonstrate synergistic optimization of materials and photonic nanostructures to achieve large reflection contrast in ultrafast polaritonic metasurface limiters. The devices are based on optimized semiconductor heterostructure materials that minimize the intersubband transition linewidth and reduce absorption in optically saturated nanoresonators, achieving a record-high reflection contrast of 54% experimentally. We also discuss opportunities to further boost the metrics of performance of this class of ultrafast limiters, showing that reflection contrast as high as 94% may be realistically achieved using all-dielectric intersubband polaritonic metasurfaces.","sentences":["Nonlinear intersubband polaritonic metasurfaces support one of the strongest known ultrafast nonlinear responses in the mid-infrared frequency range across all condensed matter systems.","Beyond harmonic generation and frequency mixing, these nonlinearities can be leveraged for ultrafast optical switching and power limiting, based on tailored transitions from strong to weak polaritonic coupling.","Here, we demonstrate synergistic optimization of materials and photonic nanostructures to achieve large reflection contrast in ultrafast polaritonic metasurface limiters.","The devices are based on optimized semiconductor heterostructure materials that minimize the intersubband transition linewidth and reduce absorption in optically saturated nanoresonators, achieving a record-high reflection contrast of 54% experimentally.","We also discuss opportunities to further boost the metrics of performance of this class of ultrafast limiters, showing that reflection contrast as high as 94% may be realistically achieved using all-dielectric intersubband polaritonic metasurfaces."],"url":"http://arxiv.org/abs/2403.15911v1","category":"physics.optics"}
{"created":"2024-03-23 18:47:49","title":"Quantum state transfer performance of Heisenberg spin chains with site-dependent interactions designed using a generic genetic algorithm","abstract":"Designing a good transfer channel for arbitrary quantum states in spin chains implies optimizing a cost function, usually the averaged fidelity of transmission. The fidelity of transmission measures how much the transferred state resembles the state prepared at the beginning of the transfer protocol. When averaged over all the possible initial states, the figure of merit quantifies the quality of the protocol. There are proposals for optimizing a given Hamiltonian to accomplish a particular task. The transfer of quantum states is one of them. In particular, we consider the design of Heisenberg spin chains using a genetic algorithm. This very efficient algorithm allows us to study different properties of Hamiltonians with good to excellent transfer ability. One apparent drawback of using a random search method is that it results in exchange coefficient strengths that change abruptly from site to site. Modifying the cost function, we obtain Hamiltonians with exchange coefficients varying smoothly along the chain length. Our results show that the smoothed Hamiltonians have the same, or less, transfer ability than the rough ones, and both kinds show similar robustness against static disorder. By studying the statistical properties of the eigenvalues of Hamiltonians with varying transfer abilities, we determine the ensemble of random matrices to which the spectra belong.","sentences":["Designing a good transfer channel for arbitrary quantum states in spin chains implies optimizing a cost function, usually the averaged fidelity of transmission.","The fidelity of transmission measures how much the transferred state resembles the state prepared at the beginning of the transfer protocol.","When averaged over all the possible initial states, the figure of merit quantifies the quality of the protocol.","There are proposals for optimizing a given Hamiltonian to accomplish a particular task.","The transfer of quantum states is one of them.","In particular, we consider the design of Heisenberg spin chains using a genetic algorithm.","This very efficient algorithm allows us to study different properties of Hamiltonians with good to excellent transfer ability.","One apparent drawback of using a random search method is that it results in exchange coefficient strengths that change abruptly from site to site.","Modifying the cost function, we obtain Hamiltonians with exchange coefficients varying smoothly along the chain length.","Our results show that the smoothed Hamiltonians have the same, or less, transfer ability than the rough ones, and both kinds show similar robustness against static disorder.","By studying the statistical properties of the eigenvalues of Hamiltonians with varying transfer abilities, we determine the ensemble of random matrices to which the spectra belong."],"url":"http://arxiv.org/abs/2403.15909v1","category":"quant-ph"}
{"created":"2024-03-23 17:23:48","title":"Rational approximation of holomorphic semigroups revisited","abstract":"Using a recently developed $\\mathcal H$-calculus we propose a unified approach to the study of rational approximations of holomorphic semigroups on Banach spaces. We provide unified and simple proofs to a number of basic results on semigroup approximations and substantially improve some of them. We show that many of our estimates are essentially optimal, thus complementing the existing literature.","sentences":["Using a recently developed $\\mathcal H$-calculus we propose a unified approach to the study of rational approximations of holomorphic semigroups on Banach spaces.","We provide unified and simple proofs to a number of basic results on semigroup approximations and substantially improve some of them.","We show that many of our estimates are essentially optimal, thus complementing the existing literature."],"url":"http://arxiv.org/abs/2403.15894v1","category":"math.FA"}
{"created":"2024-03-23 15:38:10","title":"On the complexity and approximability of Bounded access Lempel Ziv coding","abstract":"We study the complexity of constructing an optimal parsing $\\varphi$ of a string ${\\bf s} = s_1 \\dots s_n$ under the constraint that given a position $p$ in the original text, and the LZ76-like (Lempel Ziv 76) encoding of $T$ based on $\\varphi$, it is possible to identify/decompress the character $s_p$ by performing at most $c$ accesses to the LZ encoding, for a given integer $c.$ We refer to such a parsing $\\varphi$ as a $c$-bounded access LZ parsing or $c$-BLZ parsing of ${\\bf s}.$ We show that for any constant $c$ the problem of computing the optimal $c$-BLZ parsing of a string, i.e., the one with the minimum number of phrases, is NP-hard and also APX hard, i.e., no PTAS can exist under the standard complexity assumption $P \\neq NP.$ We also study the ratio between the sizes of an optimal $c$-BLZ parsing of a string ${\\bf s}$ and an optimal LZ76 parsing of ${\\bf s}$ (which can be greedily computed in polynomial time).","sentences":["We study the complexity of constructing an optimal parsing $\\varphi$ of a string ${\\bf s} = s_1 \\dots s_n$ under the constraint that given a position $p$ in the original text, and the LZ76-like (Lempel Ziv 76) encoding of $T$ based on $\\varphi$, it is possible to identify/decompress the character $s_p$ by performing at most $c$ accesses to the LZ encoding, for a given integer $c.$ We refer to such a parsing $\\varphi$ as a $c$-bounded access LZ parsing or $c$-BLZ parsing of ${\\bf s}.$","We show that for any constant $c$ the problem of computing the optimal $c$-BLZ parsing of a string, i.e., the one with the minimum number of phrases, is NP-hard and also APX hard, i.e., no PTAS can exist under the standard complexity assumption $P \\neq NP.$","We also study the ratio between the sizes of an optimal $c$-BLZ parsing of a string ${\\bf s}$ and an optimal LZ76 parsing of ${\\bf s}$ (which can be greedily computed in polynomial time)."],"url":"http://arxiv.org/abs/2403.15871v1","category":"cs.DS"}
{"created":"2024-03-23 14:00:13","title":"Engineering the electronic and magnetic properties of monolayer TiS$_2$ through systematic transition-metal doping","abstract":"Layered materials that exhibit magnetic ordering in their pristine form are very rare. Several standard approaches, such as adsorption of atoms, introduction of point defects, and edge engineering, have been developed to induce magnetism in two-dimensional materials. In this way, we investigate the electronic and magnetic properties of monolayer TiS$_2$ doped with 3$d$ transition metals (TMs) atoms in both octahedral 1T and trigonal prismatic 1H structures using first-principles calculations. In its pristine form, TiS$_2$ is a non-magnetic semiconductor. The bands near the Fermi energy primarily exhibit $d$ orbital characters, and due to the presence of ideal octahedral and trigonal arrangements, they are well separated from other bands with $p$ character. Upon substituting 3$d$-TM atoms in both structures, a variety of electronic and magnetic phases emerge, including magnetic semiconductor, magnetic half-metal, non-magnetic metal, and magnetic metal. Chromium exhibits the largest magnetic moment in both the 1T and 1H structures. The 1T structure shows a slightly higher magnetic moment of 3.419 $\\mu_B$ compared to the 1H structure 3.138 $\\mu_B$, attributed to the distorted octahedral structure of the 1T structure. Unlike pristine TiS$_2$, the deficiency in saturation of neighboring S atoms in the presence of impurities leads to the proximity of energy levels of $d$ and $p$ states, resulting in unexpectedly sizable magnetic moments. Another interesting case is Cobalt, which leads to a magnetic moment of approximately 0.805 $\\mu_B$ in the 1H structure, while the Co exhibits a non-magnetic state in the 1H structure. These materials demonstrate a high degree of tunability and can be optimized for various magnetic applications.","sentences":["Layered materials that exhibit magnetic ordering in their pristine form are very rare.","Several standard approaches, such as adsorption of atoms, introduction of point defects, and edge engineering, have been developed to induce magnetism in two-dimensional materials.","In this way, we investigate the electronic and magnetic properties of monolayer TiS$_2$ doped with 3$d$ transition metals (TMs) atoms in both octahedral 1T and trigonal prismatic 1H structures using first-principles calculations.","In its pristine form, TiS$_2$ is a non-magnetic semiconductor.","The bands near the Fermi energy primarily exhibit $d$ orbital characters, and due to the presence of ideal octahedral and trigonal arrangements, they are well separated from other bands with $p$ character.","Upon substituting 3$d$-TM atoms in both structures, a variety of electronic and magnetic phases emerge, including magnetic semiconductor, magnetic half-metal, non-magnetic metal, and magnetic metal.","Chromium exhibits the largest magnetic moment in both the 1T and 1H structures.","The 1T structure shows a slightly higher magnetic moment of 3.419 $\\mu_B$ compared to the 1H structure 3.138 $\\mu_B$, attributed to the distorted octahedral structure of the 1T structure.","Unlike pristine TiS$_2$, the deficiency in saturation of neighboring S atoms in the presence of impurities leads to the proximity of energy levels of $d$ and $p$ states, resulting in unexpectedly sizable magnetic moments.","Another interesting case is Cobalt, which leads to a magnetic moment of approximately 0.805 $\\mu_B$ in the 1H structure, while the Co exhibits a non-magnetic state in the 1H structure.","These materials demonstrate a high degree of tunability and can be optimized for various magnetic applications."],"url":"http://arxiv.org/abs/2403.15850v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-23 13:01:26","title":"TJCCT: A Two-timescale Approach for UAV-assisted Mobile Edge Computing","abstract":"Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services in close proximity to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply heterogeneity between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex and NP-hard mixed integer nonlinear programming (MINLP), we propose a two-timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach for solving the problem. In the short timescale, we propose a price-incentive model for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long timescale, we propose a convex optimization-based method for UAV trajectory control. Besides, we theoretically prove the stability, optimality, and polynomial complexity of TJCCT. Extended simulation results demonstrate that the proposed TJCCT outperforms the comparative algorithms in terms of the system utility, average processing rate, average completion delay, and average completion ratio.","sentences":["Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services in close proximity to mobile devices (MDs).","However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply heterogeneity between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network.","To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility.","Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility.","Since the problem is a non-convex and NP-hard mixed integer nonlinear programming (MINLP), we propose a two-timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach for solving the problem.","In the short timescale, we propose a price-incentive model for on-demand computing resource allocation and a matching mechanism-based method for computation offloading.","In the long timescale, we propose a convex optimization-based method for UAV trajectory control.","Besides, we theoretically prove the stability, optimality, and polynomial complexity of TJCCT.","Extended simulation results demonstrate that the proposed TJCCT outperforms the comparative algorithms in terms of the system utility, average processing rate, average completion delay, and average completion ratio."],"url":"http://arxiv.org/abs/2403.15828v1","category":"eess.SY"}
{"created":"2024-03-23 12:30:42","title":"Toward Optimum Coupling between Free Electrons and Confined Optical Modes","abstract":"Free electrons are unique tools to probe and manipulate nanoscale optical fields with emerging applications in ultrafast spectromicroscopy and quantum metrology. However, advances in this field are hindered by the small probability associated with the excitation of single optical modes by individual free electrons. Here, we theoretically investigate the scaling properties of the electron-driven excitation probability for a wide variety of optical modes including plasmons in metallic nanostructures and Mie resonances in dielectric cavities, spanning a broad spectral range that extends from the ultraviolet to the infrared. The highest probabilities for the direct generation of three-dimensionally confined modes are observed at low electron and mode energies in small structures, with order-unity ($\\sim100$\\%) coupling demanding the use of $<100$~eV electrons interacting with $<1$~eV polaritons confined down to tens of nm. Electronic transitions in artificial atoms also emerge as practical systems to realize strong coupling to few-eV free electrons. In contrast, conventional dielectric cavities reach a maximum probability in the few-percent range. In addition, we show that waveguide modes can be generated with higher-than-unity efficiency by phase-matched interaction with grazing electrons, suggesting an efficient method to create multiple excitations of a localized optical mode by an individual electron through funneling the so-generated propagating photons into a confining cavity -- an alternative approach to direct electron-cavity interaction. Our work provides a roadmap to optimize electron-photon coupling with potential applications in electron spectromicroscopy as well as nonlinear and quantum optics at the nanoscale.","sentences":["Free electrons are unique tools to probe and manipulate nanoscale optical fields with emerging applications in ultrafast spectromicroscopy and quantum metrology.","However, advances in this field are hindered by the small probability associated with the excitation of single optical modes by individual free electrons.","Here, we theoretically investigate the scaling properties of the electron-driven excitation probability for a wide variety of optical modes including plasmons in metallic nanostructures and Mie resonances in dielectric cavities, spanning a broad spectral range that extends from the ultraviolet to the infrared.","The highest probabilities for the direct generation of three-dimensionally confined modes are observed at low electron and mode energies in small structures, with order-unity ($\\sim100$\\%) coupling demanding the use of $<100$~eV electrons interacting with $<1$~eV polaritons confined down to tens of nm.","Electronic transitions in artificial atoms also emerge as practical systems to realize strong coupling to few-eV free electrons.","In contrast, conventional dielectric cavities reach a maximum probability in the few-percent range.","In addition, we show that waveguide modes can be generated with higher-than-unity efficiency by phase-matched interaction with grazing electrons, suggesting an efficient method to create multiple excitations of a localized optical mode by an individual electron through funneling the so-generated propagating photons into a confining cavity -- an alternative approach to direct electron-cavity interaction.","Our work provides a roadmap to optimize electron-photon coupling with potential applications in electron spectromicroscopy as well as nonlinear and quantum optics at the nanoscale."],"url":"http://arxiv.org/abs/2403.15823v1","category":"physics.optics"}
{"created":"2024-03-23 12:12:11","title":"Combining genetic algorithm and compressed sensing for features and operators selection in symbolic regression","abstract":"Symbolic-inference methods have recently found a broad application in materials science. In particular, the Sure-Independence Screening and Sparsifying Operator (SISSO) performs symbolic regression and classification by adopting compressed sensing for the selection of an optimized subset of features and mathematical operators out of a given set of candidates. However, SISSO becomes computationally unpractical when the set of candidate features and operators exceeds the size of few tens. In the present work, we combine SISSO with a genetic algorithm (GA) for the global search of the optimal subset of features and operators. We demonstrate that GA-SISSO efficiently finds more accurate predictive models than the original SISSO, due to the possibility to access a larger input feature and operator space. GA-SISSO was applied for the search of the model for the prediction of carbon-dioxide adsorption energies on semiconductor oxides. The obtained with GA-SISSO model has much higher accuracy compared to models previously discussed in the literature (based solely on the O 2p-band center). The analysis of features importance shows that, besides the O 2p-band center, the contribution of the electrostatic potential above adsorption sites and the surface formation energies are also important.","sentences":["Symbolic-inference methods have recently found a broad application in materials science.","In particular, the Sure-Independence Screening and Sparsifying Operator (SISSO) performs symbolic regression and classification by adopting compressed sensing for the selection of an optimized subset of features and mathematical operators out of a given set of candidates.","However, SISSO becomes computationally unpractical when the set of candidate features and operators exceeds the size of few tens.","In the present work, we combine SISSO with a genetic algorithm (GA) for the global search of the optimal subset of features and operators.","We demonstrate that GA-SISSO efficiently finds more accurate predictive models than the original SISSO, due to the possibility to access a larger input feature and operator space.","GA-SISSO was applied for the search of the model for the prediction of carbon-dioxide adsorption energies on semiconductor oxides.","The obtained with GA-SISSO model has much higher accuracy compared to models previously discussed in the literature (based solely on the O 2p-band center).","The analysis of features importance shows that, besides the O 2p-band center, the contribution of the electrostatic potential above adsorption sites and the surface formation energies are also important."],"url":"http://arxiv.org/abs/2403.15816v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-23 11:47:58","title":"Anticipatory Gains and Event-Driven Losses in Blockchain-Based Fan Tokens: Evidence from the FIFA World Cup","abstract":"National football teams increasingly issue tradeable blockchain-based fan tokens to strategically enhance fan engagement. This study investigates the impact of 2022 World Cup matches on the dynamic performance of each team's fan token. The event study uncovers fan token returns surged six months before the World Cup, driven by positive anticipation effects. However, intraday analysis reveals a reversal of fan token returns consistently declining and trading volumes rising as matches unfold. To explain findings, we uncover asymmetries whereby defeats in high-stake matches caused a plunge in fan token returns, compared to low-stake matches, intensifying in magnitude for knockout matches. Contrarily, victories enhance trading volumes, reflecting increased market activity without a corresponding positive effect on returns. We align findings with the classic market adage \"buy the rumor, sell the news,\" unveiling cognitive biases and nuances in investor sentiment, cautioning the dichotomy of pre-event optimism and subsequent performance declines.","sentences":["National football teams increasingly issue tradeable blockchain-based fan tokens to strategically enhance fan engagement.","This study investigates the impact of 2022 World Cup matches on the dynamic performance of each team's fan token.","The event study uncovers fan token returns surged six months before the World Cup, driven by positive anticipation effects.","However, intraday analysis reveals a reversal of fan token returns consistently declining and trading volumes rising as matches unfold.","To explain findings, we uncover asymmetries whereby defeats in high-stake matches caused a plunge in fan token returns, compared to low-stake matches, intensifying in magnitude for knockout matches.","Contrarily, victories enhance trading volumes, reflecting increased market activity without a corresponding positive effect on returns.","We align findings with the classic market adage \"buy the rumor, sell the news,\" unveiling cognitive biases and nuances in investor sentiment, cautioning the dichotomy of pre-event optimism and subsequent performance declines."],"url":"http://arxiv.org/abs/2403.15810v1","category":"q-fin.GN"}
{"created":"2024-03-23 11:30:09","title":"Semi-on-Demand Hybrid Transit Route Design with Shared Autonomous Mobility Services","abstract":"This study examines the route design of a semi-on-demand hybrid route directional service in the public transit network, offering on-demand flexible route service in low-density areas and fixed route service in higher-density areas with Shared Autonomous Mobility Service (SAMS). The study develops analytically tractable cost expressions that capture access, waiting, and riding costs for users, and distance-based operating and time-based vehicle costs for operators. Two formulations are presented for strategic and tactical decisions in flexible route portion, fleet size, headway, and vehicle size optimization, enabling the determination of route types between fixed, hybrid, and flexible routes based on demand, cost, and operational parameters. The practical applications and benefits of semi-on-demand feeders are demonstrated with numerical examples and a large-scale case study in the Chicago metropolitan area. Findings reveal scenarios in which flexible route portions serving passengers located further away reduce total costs, particularly user costs. Lower operating costs in lower-demand areas favor more flexible routes, whereas higher demand densities favor more traditional line-based operations. On two studied lines, a current cost forecast favors smaller vehicles with flexible routes, but operating constraints and higher operating costs would favor bigger vehicles with hybrid routes. The study provides an analytical tool to design SAMS as directional services and transit feeders, and tractable continuous approximation formulations for future research in transit network design.","sentences":["This study examines the route design of a semi-on-demand hybrid route directional service in the public transit network, offering on-demand flexible route service in low-density areas and fixed route service in higher-density areas with Shared Autonomous Mobility Service (SAMS).","The study develops analytically tractable cost expressions that capture access, waiting, and riding costs for users, and distance-based operating and time-based vehicle costs for operators.","Two formulations are presented for strategic and tactical decisions in flexible route portion, fleet size, headway, and vehicle size optimization, enabling the determination of route types between fixed, hybrid, and flexible routes based on demand, cost, and operational parameters.","The practical applications and benefits of semi-on-demand feeders are demonstrated with numerical examples and a large-scale case study in the Chicago metropolitan area.","Findings reveal scenarios in which flexible route portions serving passengers located further away reduce total costs, particularly user costs.","Lower operating costs in lower-demand areas favor more flexible routes, whereas higher demand densities favor more traditional line-based operations.","On two studied lines, a current cost forecast favors smaller vehicles with flexible routes, but operating constraints and higher operating costs would favor bigger vehicles with hybrid routes.","The study provides an analytical tool to design SAMS as directional services and transit feeders, and tractable continuous approximation formulations for future research in transit network design."],"url":"http://arxiv.org/abs/2403.15804v1","category":"eess.SY"}
{"created":"2024-03-23 10:16:05","title":"Adversarial Defense Teacher for Cross-Domain Object Detection under Poor Visibility Conditions","abstract":"Existing object detectors encounter challenges in handling domain shifts between training and real-world data, particularly under poor visibility conditions like fog and night. Cutting-edge cross-domain object detection methods use teacher-student frameworks and compel teacher and student models to produce consistent predictions under weak and strong augmentations, respectively. In this paper, we reveal that manually crafted augmentations are insufficient for optimal teaching and present a simple yet effective framework named Adversarial Defense Teacher (ADT), leveraging adversarial defense to enhance teaching quality. Specifically, we employ adversarial attacks, encouraging the model to generalize on subtly perturbed inputs that effectively deceive the model. To address small objects under poor visibility conditions, we propose a Zoom-in Zoom-out strategy, which zooms-in images for better pseudo-labels and zooms-out images and pseudo-labels to learn refined features. Our results demonstrate that ADT achieves superior performance, reaching 54.5% mAP on Foggy Cityscapes, surpassing the previous state-of-the-art by 2.6% mAP.","sentences":["Existing object detectors encounter challenges in handling domain shifts between training and real-world data, particularly under poor visibility conditions like fog and night.","Cutting-edge cross-domain object detection methods use teacher-student frameworks and compel teacher and student models to produce consistent predictions under weak and strong augmentations, respectively.","In this paper, we reveal that manually crafted augmentations are insufficient for optimal teaching and present a simple yet effective framework named Adversarial Defense Teacher (ADT), leveraging adversarial defense to enhance teaching quality.","Specifically, we employ adversarial attacks, encouraging the model to generalize on subtly perturbed inputs that effectively deceive the model.","To address small objects under poor visibility conditions, we propose a Zoom-in Zoom-out strategy, which zooms-in images for better pseudo-labels and zooms-out images and pseudo-labels to learn refined features.","Our results demonstrate that ADT achieves superior performance, reaching 54.5% mAP on Foggy Cityscapes, surpassing the previous state-of-the-art by 2.6% mAP."],"url":"http://arxiv.org/abs/2403.15786v1","category":"cs.CV"}
{"created":"2024-03-23 09:07:20","title":"A high-performance lattice Boltzmann model for multicomponent turbulent jet simulations","abstract":"In this work an optimized multicomponent lattice Boltzmann (LB) model is deployed to simulate axisymmetric turbulent jets of a fluid evolving in a quiescent, immiscible environment over a wide range of dynamic regimes. The implementation of the multicomponent lattice Boltzmann code achieves peak performances on graphic processing units with a significant reduction of the memory footprint, retains the algorithmic simplicity inherent to standard LB computing and being based on a high-order extension of the thread-safe lattice Boltzmann algorithm, it allows to perform stable simulations at vanishingly low viscosities. The proposed approach opens attractive prospects for high-performance computing simulations of realistic turbulent flows with interfaces on GPU-based architectures.","sentences":["In this work an optimized multicomponent lattice Boltzmann (LB) model is deployed to simulate axisymmetric turbulent jets of a fluid evolving in a quiescent, immiscible environment over a wide range of dynamic regimes.","The implementation of the multicomponent lattice Boltzmann code achieves peak performances on graphic processing units with a significant reduction of the memory footprint, retains the algorithmic simplicity inherent to standard LB computing and being based on a high-order extension of the thread-safe lattice Boltzmann algorithm, it allows to perform stable simulations at vanishingly low viscosities.","The proposed approach opens attractive prospects for high-performance computing simulations of realistic turbulent flows with interfaces on GPU-based architectures."],"url":"http://arxiv.org/abs/2403.15773v1","category":"physics.flu-dyn"}
{"created":"2024-03-23 08:26:20","title":"RicMonk: A Three-Link Brachiation Robot with Passive Grippers for Energy-Efficient Brachiation","abstract":"This paper presents the design, analysis, and performance evaluation of RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped grippers. Brachiation, an agile and energy-efficient mode of locomotion observed in primates, has inspired the development of RicMonk to explore versatile locomotion and maneuvers on ladder-like structures. The robot's anatomical resemblance to gibbons and the integration of a tail mechanism for energy injection contribute to its unique capabilities. The paper discusses the use of the Direct Collocation methodology for optimizing trajectories for the robot's dynamic behaviors and stabilization of these trajectories using a Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate bidirectional brachiation, and provide comparative analysis with its predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the presence of a passive tail helps improve energy efficiency. The system design, controllers, and software implementation are publicly available on GitHub and the video demonstration of the experiments can be viewed YouTube.","sentences":["This paper presents the design, analysis, and performance evaluation of RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped grippers.","Brachiation, an agile and energy-efficient mode of locomotion observed in primates, has inspired the development of RicMonk to explore versatile locomotion and maneuvers on ladder-like structures.","The robot's anatomical resemblance to gibbons and the integration of a tail mechanism for energy injection contribute to its unique capabilities.","The paper discusses the use of the Direct Collocation methodology for optimizing trajectories for the robot's dynamic behaviors and stabilization of these trajectories using a Time-varying Linear Quadratic Regulator.","With RicMonk we demonstrate bidirectional brachiation, and provide comparative analysis with its predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the presence of a passive tail helps improve energy efficiency.","The system design, controllers, and software implementation are publicly available on GitHub and the video demonstration of the experiments can be viewed YouTube."],"url":"http://arxiv.org/abs/2403.15762v1","category":"cs.RO"}
{"created":"2024-03-25 12:46:19","title":"The crossover line in the $(T, \u03bc)$-phase diagram of QCD","abstract":"An efficient way to study the QCD phase diagram at small finite density is to extrapolate thermodynamical observables from imaginary chemical potential. The phase diagram features a crossover line starting from the transition temperature already determined at zero chemical potential. In this work we focus on the Taylor expansion of this line up to $\\mu^4$ contributions. We present the continuum extrapolation of the crossover temperature based on different observables at several lattice spacings.","sentences":["An efficient way to study the QCD phase diagram at small finite density is to extrapolate thermodynamical observables from imaginary chemical potential.","The phase diagram features a crossover line starting from the transition temperature already determined at zero chemical potential.","In this work we focus on the Taylor expansion of this line up to $\\mu^4$ contributions.","We present the continuum extrapolation of the crossover temperature based on different observables at several lattice spacings."],"url":"http://arxiv.org/abs/2403.16709v1","category":"hep-lat"}
{"created":"2024-03-25 12:37:04","title":"Consistency of pion form factor and unpolarized transverse momentum dependent parton distributions beyond leading twist in the light-front quark model","abstract":"We investigate the interplay among the pion's form factor, transverse momentum dependent distributions (TMDs), and parton distribution functions (PDFs) extending our light-front quark model (LFQM) computation based on the Bakamjian-Thomas construction for the two-point function[41,42] to the three-point and four-point functions. Ensuring the four-momentum conservation at the meson-quark vertex from the Bakamjian-Thomas construction, the meson mass is taken consistently as the corresponding invariant meson mass both in the matrix element and the Lorentz factor in our LFQM computation. We achieve the current-component independence in the physical observables such as the pion form factor and delve into the derivation of unpolarized TMDs and PDFs associated with the forward matrix element. We address the challenges posed by twist-4 TMDs and exhibit the fulfillment of the sum rule. Effectively, our LFQM successfully handles the light-front zero modes and offers insights for broader three-point and four-point functions and related observables.","sentences":["We investigate the interplay among the pion's form factor, transverse momentum dependent distributions (TMDs), and parton distribution functions (PDFs) extending our light-front quark model (LFQM) computation based on the Bakamjian-Thomas construction for the two-point function[41,42] to the three-point and four-point functions.","Ensuring the four-momentum conservation at the meson-quark vertex from the Bakamjian-Thomas construction, the meson mass is taken consistently as the corresponding invariant meson mass both in the matrix element and the Lorentz factor in our LFQM computation.","We achieve the current-component independence in the physical observables such as the pion form factor and delve into the derivation of unpolarized TMDs and PDFs associated with the forward matrix element.","We address the challenges posed by twist-4 TMDs and exhibit the fulfillment of the sum rule.","Effectively, our LFQM successfully handles the light-front zero modes and offers insights for broader three-point and four-point functions and related observables."],"url":"http://arxiv.org/abs/2403.16703v1","category":"hep-ph"}
{"created":"2024-03-25 11:10:16","title":"Direct Production of Light Scalar in the Type-I Two-Higgs-Doublet Model at the Lifetime Frontier of LHC","abstract":"A light pseudoscalar $A$ in the sufficient large $\\tan\\beta$ region of type-I two-Higgs-doublet model (2HDM) can be naturally a long-lived particle (LLP). We focus on the $H^{\\pm}A$, $HA$ and $AA$ pair productions via the electroweak processes mediated by the bosons at the LHC, including $pp \\rightarrow W^\\pm/Z \\rightarrow H^{\\pm}/H A$ and $pp \\rightarrow h \\rightarrow AA$ at the 14 TeV LHC. The possibility of probing $A$ as a LLP at the FASER-2, FACET, MoEDAL-MAPP-2, MATHUSLA is discussed. We find that FASER-2 fails to probe any parameter space within 0.2 GeV $< m_A <$ 10 GeV for both processes. For 130 $< m_{H\\pm} = m_H <$ 400 GeV, FACET, MoEDAL-MAPP-2 and MATHUSLA can probes $\\tan \\beta \\lesssim 10^{4-6}$ for $m_A \\lesssim 3$ GeV, and $\\tan \\beta \\lesssim 10^{6-8}$ for 3 GeV $\\lesssim m_A <$ 10 GeV from $pp \\rightarrow W^\\pm/Z \\rightarrow H^{\\pm}/Z A$ processes. And $pp \\rightarrow h \\rightarrow AA$ covers similar parameter space. All processes can surpass the current limits.","sentences":["A light pseudoscalar $A$ in the sufficient large $\\tan\\beta$ region of type-I two-Higgs-doublet model (2HDM) can be naturally a long-lived particle (LLP).","We focus on the $H^{\\pm}A$, $HA$ and $AA$ pair productions via the electroweak processes mediated by the bosons at the LHC, including $pp \\rightarrow W^\\pm/Z \\rightarrow H^{\\pm}/H A$ and $pp \\rightarrow h \\rightarrow AA$ at the 14 TeV LHC.","The possibility of probing $A$ as a LLP at the FASER-2, FACET, MoEDAL-MAPP-2, MATHUSLA is discussed.","We find that FASER-2 fails to probe any parameter space within 0.2 GeV $< m_A <$ 10 GeV for both processes.","For 130 $< m_{H\\pm} = m_H <$ 400 GeV, FACET, MoEDAL-MAPP-2 and MATHUSLA can probes $\\tan \\beta \\lesssim 10^{4-6}$ for $m_A \\lesssim 3$ GeV, and $\\tan \\beta \\lesssim 10^{6-8}$ for 3 GeV $\\lesssim m_A <$ 10 GeV from $pp \\rightarrow W^\\pm/Z \\rightarrow H^{\\pm}/Z A$ processes.","And $pp \\rightarrow h \\rightarrow AA$ covers similar parameter space.","All processes can surpass the current limits."],"url":"http://arxiv.org/abs/2403.16623v1","category":"hep-ph"}
{"created":"2024-03-25 10:51:16","title":"Radiation damping of a Rayleigh scatterer illuminated by a plane wave","abstract":"We investigate the radiation damping experienced by a dielectric spherical particle when it is illuminated by an electromagnetic plane wave within the Rayleigh regime. We derive the equivalent electric dipole of the moving particle and subsequently calculate the electromagnetic force acting on it from two different approaches. In the first approach, we calculate the force from the integration of stress tensor and field momentum. In the second one, we calculate the force directly from the integration of the force density. Our derivations reveal that the damping coefficient is equal to $6P_{scat}/mc^2$ along the propagation direction, whereas it is $P_{scat}/mc^2$ along perpendicular directions. Here, $P_{scat}$ denotes the power scattered by the particle, and $mc^2$ represents the particle's mass energy. The radiation damping derived in this study sets upper limits on the quality factor of optically levitated objects and ensures the existence of a steady-state solution of the particle's dynamics.","sentences":["We investigate the radiation damping experienced by a dielectric spherical particle when it is illuminated by an electromagnetic plane wave within the Rayleigh regime.","We derive the equivalent electric dipole of the moving particle and subsequently calculate the electromagnetic force acting on it from two different approaches.","In the first approach, we calculate the force from the integration of stress tensor and field momentum.","In the second one, we calculate the force directly from the integration of the force density.","Our derivations reveal that the damping coefficient is equal to $6P_{scat}/mc^2$ along the propagation direction, whereas it is $P_{scat}/mc^2$ along perpendicular directions.","Here, $P_{scat}$ denotes the power scattered by the particle, and $mc^2$ represents the particle's mass energy.","The radiation damping derived in this study sets upper limits on the quality factor of optically levitated objects and ensures the existence of a steady-state solution of the particle's dynamics."],"url":"http://arxiv.org/abs/2403.16618v1","category":"physics.optics"}
{"created":"2024-03-25 10:47:45","title":"Magnetic Moments of Hidden-Bottom Pentaquark States","abstract":"We study systematically magnetic moments of hiddden-bottom pentaquark states with quantum numbers $J^P=\\frac{1}{2}^{\\pm}$, $J^P=\\frac{3}{2}^{\\pm}$, and $J^P=\\frac{5}{2}^{\\pm}$ with molecular, diquark-diquark-antiquark, and diquark-triquark models. The numerical results show that magnetic moments are different within the same model according to same quantum numbers and spin-orbit couplings. The results are also different when different models are taken into account with the same angular momentum. The magnetic moments encode valuable information about inner structures. We believe that our results may be helpful for experimental studies.","sentences":["We study systematically magnetic moments of hiddden-bottom pentaquark states with quantum numbers $J^P=\\frac{1}{2}^{\\pm}$, $J^P=\\frac{3}{2}^{\\pm}$, and $J^P=\\frac{5}{2}^{\\pm}$ with molecular, diquark-diquark-antiquark, and diquark-triquark models.","The numerical results show that magnetic moments are different within the same model according to same quantum numbers and spin-orbit couplings.","The results are also different when different models are taken into account with the same angular momentum.","The magnetic moments encode valuable information about inner structures.","We believe that our results may be helpful for experimental studies."],"url":"http://arxiv.org/abs/2403.16616v1","category":"hep-ph"}
{"created":"2024-03-25 10:21:11","title":"Singular profile of free boundary of incompressible inviscid fluid with external force","abstract":"This article is devoted to investigate the singular profile of the free boundary of two-dimensional incompressible inviscid fluid with external force near the stagnation point. More precisely, given an external force with some polynomial type decay close to the stagnation point, the singular profile of the free boundary at stagnation point possible are corner wave, flat and cusp singularity. Through excluding the cusp and flat singularity, we know the only singular profile is corner wave singularity, and the corner depends on the decay rate of the solution near the stagnation point. The analysis depends on the geometric method to a class of Bernoulli-type free boundary problem with given degenerate gradient function on free boundary. This work is motivated by the significant work [E. V$\\breve{a}$rv$\\breve{a}$ruc$\\breve{a}$ and G. Weiss, Acta Math, 206, 363-403, (2011)] on Stokes conjecture to the incompressible inviscid fluid acted on by gravity.","sentences":["This article is devoted to investigate the singular profile of the free boundary of two-dimensional incompressible inviscid fluid with external force near the stagnation point.","More precisely, given an external force with some polynomial type decay close to the stagnation point, the singular profile of the free boundary at stagnation point possible are corner wave, flat and cusp singularity.","Through excluding the cusp and flat singularity, we know the only singular profile is corner wave singularity, and the corner depends on the decay rate of the solution near the stagnation point.","The analysis depends on the geometric method to a class of Bernoulli-type free boundary problem with given degenerate gradient function on free boundary.","This work is motivated by the significant work [E. V$\\breve{a}$rv$\\breve{a}$ruc$\\breve{a}$ and G. Weiss, Acta Math, 206, 363-403, (2011)] on Stokes conjecture to the incompressible inviscid fluid acted on by gravity."],"url":"http://arxiv.org/abs/2403.16601v1","category":"math.AP"}
{"created":"2024-03-25 09:47:15","title":"Cosmic dipoles of active galactic nuclei display much larger amplitudes than the cosmic microwave background dipole","abstract":"Sky distributions of large samples of distant active galactic nuclei (AGNs) have shown dipoles significantly larger than the cosmic microwave background (CMB) dipole. However, a recent Bayesian analysis of the QUAIA sample, comprising 1.3 million quasars, has yielded a dipole that seems to be in tandem with the CMB dipole, in contravention of most previous studies of AGN dipoles. Since the question has large cosmological implications, we investigate the QUAIA quasar sample afresh, by directly computing the dipole from asymmetries observed in the source number counts. We instead find a dipole 3-4 times as large as the CMB dipole though in the same direction. Further, it has been claimed elsewhere that the difference between the CMB dipole and the radio dipole estimated from the NRAO VLA Sky Survey (NVSS), the first large catalogue that showed an AGN dipole about four times larger than the CMB dipole, can be fully accounted for by incorporating the shot-noise and clustering contributions to the total NVSS dipole. A careful reinvestigation of the NVSS dipole, however, shows that the random phenomena like shot noise or clustering cannot account for the actually observed NVSS asymmetries, which show a systematic dipole pattern over the sky.","sentences":["Sky distributions of large samples of distant active galactic nuclei (AGNs) have shown dipoles significantly larger than the cosmic microwave background (CMB) dipole.","However, a recent Bayesian analysis of the QUAIA sample, comprising 1.3 million quasars, has yielded a dipole that seems to be in tandem with the CMB dipole, in contravention of most previous studies of AGN dipoles.","Since the question has large cosmological implications, we investigate the QUAIA quasar sample afresh, by directly computing the dipole from asymmetries observed in the source number counts.","We instead find a dipole 3-4 times as large as the CMB dipole though in the same direction.","Further, it has been claimed elsewhere that the difference between the CMB dipole and the radio dipole estimated from the NRAO VLA Sky Survey (NVSS), the first large catalogue that showed an AGN dipole about four times larger than the CMB dipole, can be fully accounted for by incorporating the shot-noise and clustering contributions to the total NVSS dipole.","A careful reinvestigation of the NVSS dipole, however, shows that the random phenomena like shot noise or clustering cannot account for the actually observed NVSS asymmetries, which show a systematic dipole pattern over the sky."],"url":"http://arxiv.org/abs/2403.16581v1","category":"astro-ph.CO"}
{"created":"2024-03-25 09:33:01","title":"Non-commutative divergence and the Turaev cobracket","abstract":"The divergence map, an important ingredient in the algebraic description of the Turaev cobracket on a connected oriented compact surface with boundary, is reformulated in the context of non-commutative geometry and is generalised to give a similar algebraic description of the Turaev cobracket on a closed surface. We also look into a relation between the Satoh trace and the divergence map on a free Lie algebra, using a non-commutative analogue of a flat connection.","sentences":["The divergence map, an important ingredient in the algebraic description of the Turaev cobracket on a connected oriented compact surface with boundary, is reformulated in the context of non-commutative geometry and is generalised to give a similar algebraic description of the Turaev cobracket on a closed surface.","We also look into a relation between the Satoh trace and the divergence map on a free Lie algebra, using a non-commutative analogue of a flat connection."],"url":"http://arxiv.org/abs/2403.16566v1","category":"math.AT"}
{"created":"2024-03-25 08:28:51","title":"Broadband and fabrication-tolerant 3-dB couplers with topological valley edge modes","abstract":"3-dB couplers, which are commonly used in photonic integrated circuits for on-chip information processing, precision measurement, and quantum computing, face challenges in achieving robust performance due to their limited 3-dB bandwidths and sensitivity to fabrication errors. To address this, we introduce topological physics to nanophotonics, developing a framework for topological 3-dB couplers. These couplers exhibit broad working wavelength range and robustness against fabrication dimensional errors. By leveraging valley-Hall topology and mirror symmetry, the photonic-crystal-slab couplers achieve ideal 3-dB splitting characterized by a wavelength-insensitive scattering matrix. Tolerance analysis confirms the superiority on broad bandwidth of 48 nm and robust splitting against dimensional errors of 20 nm. We further propose a topological interferometer for on-chip distance measurement, which also exhibits robustness against dimensional errors. This extension of topological principles to the fields of interferometers, may open up new possibilities for constructing robust wavelength division multiplexing, temperature-drift-insensitive sensing, and optical coherence tomography applications.","sentences":["3-dB couplers, which are commonly used in photonic integrated circuits for on-chip information processing, precision measurement, and quantum computing, face challenges in achieving robust performance due to their limited 3-dB bandwidths and sensitivity to fabrication errors.","To address this, we introduce topological physics to nanophotonics, developing a framework for topological 3-dB couplers.","These couplers exhibit broad working wavelength range and robustness against fabrication dimensional errors.","By leveraging valley-Hall topology and mirror symmetry, the photonic-crystal-slab couplers achieve ideal 3-dB splitting characterized by a wavelength-insensitive scattering matrix.","Tolerance analysis confirms the superiority on broad bandwidth of 48 nm and robust splitting against dimensional errors of 20 nm.","We further propose a topological interferometer for on-chip distance measurement, which also exhibits robustness against dimensional errors.","This extension of topological principles to the fields of interferometers, may open up new possibilities for constructing robust wavelength division multiplexing, temperature-drift-insensitive sensing, and optical coherence tomography applications."],"url":"http://arxiv.org/abs/2403.16538v1","category":"physics.optics"}
{"created":"2024-03-25 07:15:42","title":"Green fabrication of nickel-iron layered double hydroxides nanosheets efficient for the enhanced capacitive performance","abstract":"Rational synthesis of robust layered double hydroxides (LDHs) nanosheets for high-energy supercapacitors is full of challenges. Herein, we reported an ultrasonication-assisted strategy to eco-friendly fabricate NiFe-LDHs nanosheets for the enhanced capacitive behavior. The experimental results combined with different advanced characterization tools document that the utilization of ultrasonication has a profound effect on the morphology and thickness of the as-obtained NiFe-LDHs, alternatively affecting the capacitive behavior. It shows that NiFe-LDHs nanosheets prepared with 2-h ultrasonic treatments display the exceptional capacitive performance because of the synergetic effect of ultrathin thickness, large specific surface area, and high mesoporous volume. The maximum specific capacitance of Ni3Fe1-LDHs nanosheets with the thickness of 7.39 nm and the specific surface area of 77.16 m2 g-1 reached 1923 F g-1, which is competitive with most previously reported values. In addition, the maximum specific energy of the assembled NiFe-LDHs//AC asymmetric supercapacitor achieved 49.13 Wh kg-1 at 400 W kg-1. This work provides a green technology to fabricate LDHs nanosheets, and offers deep insights for understanding the relationship between the morphology/structure and capacitive behavior of LDHs nanosheets, which is helpful for achieving high-performance LDHs-based electrode materials.","sentences":["Rational synthesis of robust layered double hydroxides (LDHs) nanosheets for high-energy supercapacitors is full of challenges.","Herein, we reported an ultrasonication-assisted strategy to eco-friendly fabricate NiFe-LDHs nanosheets for the enhanced capacitive behavior.","The experimental results combined with different advanced characterization tools document that the utilization of ultrasonication has a profound effect on the morphology and thickness of the as-obtained NiFe-LDHs, alternatively affecting the capacitive behavior.","It shows that NiFe-LDHs nanosheets prepared with 2-h ultrasonic treatments display the exceptional capacitive performance because of the synergetic effect of ultrathin thickness, large specific surface area, and high mesoporous volume.","The maximum specific capacitance of Ni3Fe1-LDHs nanosheets with the thickness of 7.39 nm and the specific surface area of 77.16 m2 g-1 reached 1923 F g-1, which is competitive with most previously reported values.","In addition, the maximum specific energy of the assembled NiFe-LDHs//AC asymmetric supercapacitor achieved 49.13 Wh kg-1 at 400 W kg-1.","This work provides a green technology to fabricate LDHs nanosheets, and offers deep insights for understanding the relationship between the morphology/structure and capacitive behavior of LDHs nanosheets, which is helpful for achieving high-performance LDHs-based electrode materials."],"url":"http://arxiv.org/abs/2403.16487v1","category":"physics.app-ph"}
{"created":"2024-03-25 06:48:50","title":"Analysing radio pulsar timing noise with a Kalman filter: a demonstration involving PSR J1359$-$6038","abstract":"In the standard two-component crust-superfluid model of a neutron star, timing noise can arise when the two components are perturbed by stochastic torques. Here it is demonstrated how to analyse fluctuations in radio pulse times of arrival with a Kalman filter to measure physical properties of the two-component model, including the crust-superfluid coupling time-scale and the variances of the crust and superfluid torques. The analysis technique, validated previously on synthetic data, is applied to observations with the Molonglo Observatory Synthesis Telescope of the representative pulsar PSR J1359$-$6038. It is shown that the two-component model is preferred to a one-component model, with log Bayes factor $6.81 \\pm 0.02$. The coupling time-scale and the torque variances on the crust and superfluid are measured with $90\\%$ confidence to be $10^{7.1^{+0.8}_{-0.5}}$ $\\rm{s}$ and $10^{-24.0^{+0.4}_{-5.6}}$ $\\rm{rad^2~s^{-3}}$ and $10^{-21.7^{+3.5}_{-0.9}}$ $\\rm{rad^2~s^{-3}}$ respectively.","sentences":["In the standard two-component crust-superfluid model of a neutron star, timing noise can arise when the two components are perturbed by stochastic torques.","Here it is demonstrated how to analyse fluctuations in radio pulse times of arrival with a Kalman filter to measure physical properties of the two-component model, including the crust-superfluid coupling time-scale and the variances of the crust and superfluid torques.","The analysis technique, validated previously on synthetic data, is applied to observations with the Molonglo Observatory Synthesis Telescope of the representative pulsar PSR J1359$-$6038.","It is shown that the two-component model is preferred to a one-component model, with log Bayes factor $6.81 \\pm 0.02$. The coupling time-scale and the torque variances on the crust and superfluid are measured with $90\\%$ confidence to be $10^{7.1^{+0.8}_{-0.5}}$ $\\rm{s}$ and $10^{-24.0^{+0.4}_{-5.6}}$ $\\rm{rad^2~s^{-3}}$ and $10^{-21.7^{+3.5}_{-0.9}}$ $\\rm{rad^2~s^{-3}}$ respectively."],"url":"http://arxiv.org/abs/2403.16467v1","category":"astro-ph.HE"}
{"created":"2024-03-25 06:21:24","title":"Next-to-leading power corrections to spherocity distribution at NLO in QCD","abstract":"This study explores the leading contributions at the next-to-leading order for the event shape variable, spherocity. Our investigation is presented through a combination of analytical derivations and graphical representations. Additionally, we delve into the intriguing behavior of the spherocity distribution, mainly as it arises from different regions of the allowed phase space.","sentences":["This study explores the leading contributions at the next-to-leading order for the event shape variable, spherocity.","Our investigation is presented through a combination of analytical derivations and graphical representations.","Additionally, we delve into the intriguing behavior of the spherocity distribution, mainly as it arises from different regions of the allowed phase space."],"url":"http://arxiv.org/abs/2403.16449v1","category":"hep-ph"}
{"created":"2024-03-25 06:02:05","title":"RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection","abstract":"Three-dimensional object detection is one of the key tasks in autonomous driving. To reduce costs in practice, low-cost multi-view cameras for 3D object detection are proposed to replace the expansive LiDAR sensors. However, relying solely on cameras is difficult to achieve highly accurate and robust 3D object detection. An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a radar-camera fusion 3D object detection method in the bird's eye view (BEV). Specifically, we first design RadarBEVNet for radar BEV feature extraction. RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based encoder and a transformer-based encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to scattering the point feature in BEV. Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the multi-modal BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers. Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D object detectors with a faster inference speed at 21~28 FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.","sentences":["Three-dimensional object detection is one of the key tasks in autonomous driving.","To reduce costs in practice, low-cost multi-view cameras for 3D object detection are proposed to replace the expansive LiDAR sensors.","However, relying solely on cameras is difficult to achieve highly accurate and robust 3D object detection.","An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable multi-modal 3D object detection.","In this paper, we introduce RCBEVDet, a radar-camera fusion 3D object detection method in the bird's eye view (BEV).","Specifically, we first design RadarBEVNet for radar BEV feature extraction.","RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder.","In the dual-stream radar backbone, a point-based encoder and a transformer-based encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders.","The RCS-aware BEV encoder takes RCS as the object size prior to scattering the point feature in BEV.","Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the multi-modal BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers.","Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D object detection benchmarks.","Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D object detectors with a faster inference speed at 21~28 FPS.","The source code will be released at https://github.com/VDIGPKU/RCBEVDet."],"url":"http://arxiv.org/abs/2403.16440v1","category":"cs.CV"}
{"created":"2024-03-25 03:42:55","title":"The polarization angle flip in GRB prompt emission","abstract":"Aims. We aim to provide an explanation for the PA rotation in GRBs and find the physical conditions that lead to the rotation by 90 degrees in the toroidal magnetic field (MF) model. Moreover, we present some observable polarization properties in the MF model that can be tested in the future. Results. We find that the PA rotation in the toroidal MF is primarily related to three critical factors: the viewing angle, the jet opening angle, and the jet Lorentz factor. Additionally, the PA can experience twice flips of 90 degrees. The conditions for the flips are $q \\gtrsim 0.5$ (except for $q\\simeq 1$) and $y_j =(\\Gamma \\theta_j)^2 \\gtrsim 4$. However, the two flips in the PA might not be concurrently observable due to the constraint of flux. Taking these conditions into account and assuming a random orientation between the jet axis and the line of sight (LOS), we obtain a theoretical upper limit (without any constraints) for the observed rate of GRBs in the X-ray or $\\gamma$-ray band displaying the flips in PA as $R_{ch} \\lesssim 80\\%$. We further constrain the observed rate as $R_{ch} \\sim 16\\%$ according to the maximal post-flip polarized flux level, where the observed rate of single and double flips each account for $\\sim 8\\%$. Moreover, when the LOS is close to the jet edge ($q\\to 1$), it is the easiest case to observe the 90-degree PA flip due to the relatively high post-flip polarized flux level.","sentences":["Aims.","We aim to provide an explanation for the PA rotation in GRBs and find the physical conditions that lead to the rotation by 90 degrees in the toroidal magnetic field (MF) model.","Moreover, we present some observable polarization properties in the MF model that can be tested in the future.","Results.","We find that the PA rotation in the toroidal MF is primarily related to three critical factors: the viewing angle, the jet opening angle, and the jet Lorentz factor.","Additionally, the PA can experience twice flips of 90 degrees.","The conditions for the flips are $q \\gtrsim 0.5$ (except for $q\\simeq 1$) and $y_j =(\\Gamma \\theta_j)^2 \\gtrsim 4$.","However, the two flips in the PA might not be concurrently observable due to the constraint of flux.","Taking these conditions into account and assuming a random orientation between the jet axis and the line of sight (LOS), we obtain a theoretical upper limit (without any constraints) for the observed rate of GRBs in the X-ray or $\\gamma$-ray band displaying the flips in PA as $R_{ch} \\lesssim 80\\%$.","We further constrain the observed rate as $R_{ch} \\sim 16\\%$ according to the maximal post-flip polarized flux level, where the observed rate of single and double flips each account for $\\sim 8\\%$.","Moreover, when the LOS is close to the jet edge ($q\\to 1$), it is the easiest case to observe the 90-degree PA flip due to the relatively high post-flip polarized flux level."],"url":"http://arxiv.org/abs/2403.16403v1","category":"astro-ph.HE"}
{"created":"2024-03-24 22:56:08","title":"Single-Motor Robotic Gripper with Multi-Surface Fingers for Variable Grasping Configurations","abstract":"This study proposes a novel robotic gripper with variable grasping configurations for grasping various objects. The fingers of the developed gripper incorporate multiple different surfaces. The gripper possesses the function of altering the finger surfaces facing a target object by rotating the fingers in its longitudinal direction. In the proposed design equipped with two fingers, the two fingers incorporate three and four surfaces, respectively, resulting in the nine available grasping configurations by the combination of these finger surfaces. The developed gripper is equipped with the functions of opening/closing its fingers for grasping and rotating its fingers to alter the grasping configuration -all achieved with a single motor. To enable the two motions using a single motor, this study introduces a self-motion switching mechanism utilizing magnets. This mechanism automatically transitions between gripper motions based on the direction of the motor rotation when the gripper is fully opened. In this state, rotating the motor towards closing initiates the finger closing action, while further opening the fingers from the fully opened state activates the finger rotation. This letter presents the gripper design, the mechanics of the self-motion switching mechanism, the control method, and the grasping configuration selection strategy. The performance of the gripper is experimentally demonstrated.","sentences":["This study proposes a novel robotic gripper with variable grasping configurations for grasping various objects.","The fingers of the developed gripper incorporate multiple different surfaces.","The gripper possesses the function of altering the finger surfaces facing a target object by rotating the fingers in its longitudinal direction.","In the proposed design equipped with two fingers, the two fingers incorporate three and four surfaces, respectively, resulting in the nine available grasping configurations by the combination of these finger surfaces.","The developed gripper is equipped with the functions of opening/closing its fingers for grasping and rotating its fingers to alter the grasping configuration -all achieved with a single motor.","To enable the two motions using a single motor, this study introduces a self-motion switching mechanism utilizing magnets.","This mechanism automatically transitions between gripper motions based on the direction of the motor rotation when the gripper is fully opened.","In this state, rotating the motor towards closing initiates the finger closing action, while further opening the fingers from the fully opened state activates the finger rotation.","This letter presents the gripper design, the mechanics of the self-motion switching mechanism, the control method, and the grasping configuration selection strategy.","The performance of the gripper is experimentally demonstrated."],"url":"http://arxiv.org/abs/2403.16320v1","category":"cs.RO"}
{"created":"2024-03-24 22:40:36","title":"Aggregate Frequency Width, Nuclear Hyperfine Coupling and Jahn-Teller Effect of $Cu^{2+}$ Impurity Ion ESR in $SrLaAlO_4$ Dielectric Resonator at $20$ Millikelvin","abstract":"The impurity paramagnetic ion, $Cu^{2+}$ substitutes $Al$ in the $SrLaAlO_4$ single crystal lattice, this results in a $CuO_6$ elongated octahedron, the resulting measured g-factors shows four-fold axes variation condition. The aggregate frequency width of the electron spin resonance with the required minimum level of impurity concentration has been evaluated in single crystal $SrLaAlO_4$ at $20$ millikelvin. Measured parallel hyperfine constants, $A_{\\scriptscriptstyle\\parallel Cu}$, were determined to be $-155.7\\times10^{-4}~cm^{-1},~ -163.0\\times10^{-4}~cm^{-1},~ -178.3\\times10^{-4}~cm^{-1} $ and$~-211.1\\times10^{-4}~cm^{-1}$ at $9.072~GHz~(WGH_{4,1,1})$ for the nuclear magnetic quantum number $M_I=+\\frac{3}{2},+\\frac{1}{2},-\\frac{1}{2}$,~and$-\\frac{3}{2}$ respectively. The anisotropy of the hyperfine structure reveals a characteristics of static Jahn-Teller effect. The second-order-anisotropy-term, $\\sim (\\frac{spin-orbit~coupling}{10D_q})^2$, is significant and can not be disregarded, with the local strain dominating over the observed Zeeman-anisotropy-energy difference. The Bohr electron magneton, $\\beta=9.23\\times 10^{-24} JT^{-1}$, (within $-0.43\\%$ so-called experimental error) has been found using the measured spin-Hamiltonian parameters. Measured nuclear dipolar hyperfine structure parameter $P_{\\scriptscriptstyle\\parallel}=12.3\\times10^{-4}~cm^{-1}$ shows that the mean inverse third power of the electron distance from the nucleus is $\\langle r^{-3}_q\\rangle\\simeq 5.23$ a.u. for $Cu^{2+}$ ion in the substituted $Al^{3+}$ ion site assuming nuclear electric quadruple moment $Q=-0.211$ barn.","sentences":["The impurity paramagnetic ion, $Cu^{2+}$ substitutes $Al$ in the $SrLaAlO_4$ single crystal lattice, this results in a $CuO_6$ elongated octahedron, the resulting measured g-factors shows four-fold axes variation condition.","The aggregate frequency width of the electron spin resonance with the required minimum level of impurity concentration has been evaluated in single crystal $SrLaAlO_4$ at $20$ millikelvin.","Measured parallel hyperfine constants, $A_{\\scriptscriptstyle\\parallel Cu}$, were determined to be $-155.7\\times10^{-4}~cm^{-1},~ -163.0\\times10^{-4}~cm^{-1},~ -178.3\\times10^{-4}~cm^{-1} $ and$~-211.1\\times10^{-4}~cm^{-1}$ at $9.072~GHz~(WGH_{4,1,1})$ for the nuclear magnetic quantum number $M_I=+\\frac{3}{2},+\\frac{1}{2},-\\frac{1}{2}$,~and$-\\frac{3}{2}$ respectively.","The anisotropy of the hyperfine structure reveals a characteristics of static Jahn-Teller effect.","The second-order-anisotropy-term, $\\sim (\\frac{spin-orbit~coupling}{10D_q})^2$, is significant and can not be disregarded, with the local strain dominating over the observed Zeeman-anisotropy-energy difference.","The Bohr electron magneton, $\\beta=9.23\\times 10^{-24} JT^{-1}$, (within $-0.43\\%$ so-called experimental error) has been found using the measured spin-Hamiltonian parameters.","Measured nuclear dipolar hyperfine structure parameter $P_{\\scriptscriptstyle\\parallel}=12.3\\times10^{-4}~cm^{-1}$ shows that the mean inverse third power of the electron distance from the nucleus is $\\langle r^{-3}_q\\rangle\\simeq 5.23$ a.u. for $Cu^{2+}$ ion in the substituted $Al^{3+}$ ion site assuming nuclear electric quadruple moment $Q=-0.211$ barn."],"url":"http://arxiv.org/abs/2403.16315v1","category":"quant-ph"}
{"created":"2024-03-24 16:54:10","title":"General Relativistic Magnetohydrodynamic Simulations of Accreting Tori: Resolution Study","abstract":"We present two-dimensional general relativistic radiative magnetohydrodynamical simulations of accretion disks around non-rotating stellar-mass black hole. We study the evolution of an equilibrium accreting torus in different grid resolutions to determine an adequate resolution to produce a stable turbulent disk driven by magneto-rotational instability. We evaluate the quality parameter, $Q_{\\theta}$, from the ratio of MRI wavelength to the grid zone size and examine the effect of resolution in various quantitative values such as the accretion rate, magnetisation, fluxes of physical quantities and disk scale-height. We also analyse how the resolution affects the formation of plasmoids produced in the magnetic reconnection events.","sentences":["We present two-dimensional general relativistic radiative magnetohydrodynamical simulations of accretion disks around non-rotating stellar-mass black hole.","We study the evolution of an equilibrium accreting torus in different grid resolutions to determine an adequate resolution to produce a stable turbulent disk driven by magneto-rotational instability.","We evaluate the quality parameter, $Q_{\\theta}$, from the ratio of MRI wavelength to the grid zone size and examine the effect of resolution in various quantitative values such as the accretion rate, magnetisation, fluxes of physical quantities and disk scale-height.","We also analyse how the resolution affects the formation of plasmoids produced in the magnetic reconnection events."],"url":"http://arxiv.org/abs/2403.16236v1","category":"astro-ph.HE"}
{"created":"2024-03-24 15:22:22","title":"Energy estimation of small-scale jets from the quiet-Sun region","abstract":"Context. Solar jets play a role in the coronal heating and the supply of solar wind. Aims. This study calculated the energies of 23 small-scale jets emerging from a quiet-Sun region to investigate their contributions for coronal heating. Conclusions. Our observations suggest that although these jets cannot provide sufficient energy for the heating of the whole quiet-Sun coronal region, they are likely to account for a significant portion of the energy demand in the local regions where the jets occur.","sentences":["Context.","Solar jets play a role in the coronal heating and the supply of solar wind.","Aims.","This study calculated the energies of 23 small-scale jets emerging from a quiet-Sun region to investigate their contributions for coronal heating.","Conclusions.","Our observations suggest that although these jets cannot provide sufficient energy for the heating of the whole quiet-Sun coronal region, they are likely to account for a significant portion of the energy demand in the local regions where the jets occur."],"url":"http://arxiv.org/abs/2403.16193v1","category":"astro-ph.SR"}
{"created":"2024-03-24 15:18:00","title":"Testing disformal non-circular deformation of Kerr black holes with LISA","abstract":"There is strong observational evidence that almost every large galaxy has a supermassive black hole at its center. It is of fundamental importance to know whether such black holes are described by the standard Kerr solution in General Relativity (GR) or by another black hole solution. An interesting alternative is the so-called disformal Kerr black holes which exist within the framework of degenerate higher-order scalar-tensor (DHOST) theories of gravity. The departure from the standard Kerr black hole spacetime is parametrized by a parameter $D$, called $\\textit{disformal parameter}$. In the present work, we discuss the capability of LISA to detect the disformal parameter. For this purpose, we study Extreme Mass Ratio Inspirals (EMRI's) around disformal Kerr black holes within the framework of the quadrupole hybrid formalism. Even when the disformal parameter is very small, its effect on the globally accumulated phase of the gravitational waveform of an EMRI can be significant due to the large number of cycles in the LISA band made by the small compact object. We show that LISA will in principle be able to detect and measure extremely small values of the disformal parameter which in turn, can be seen as an assessment of LISA's ability to detect very small deviations from the Kerr geometry.","sentences":["There is strong observational evidence that almost every large galaxy has a supermassive black hole at its center.","It is of fundamental importance to know whether such black holes are described by the standard Kerr solution in General Relativity (GR) or by another black hole solution.","An interesting alternative is the so-called disformal Kerr black holes which exist within the framework of degenerate higher-order scalar-tensor (DHOST) theories of gravity.","The departure from the standard Kerr black hole spacetime is parametrized by a parameter $D$, called $\\textit{disformal parameter}$. In the present work, we discuss the capability of LISA to detect the disformal parameter.","For this purpose, we study Extreme Mass Ratio Inspirals (EMRI's) around disformal Kerr black holes within the framework of the quadrupole hybrid formalism.","Even when the disformal parameter is very small, its effect on the globally accumulated phase of the gravitational waveform of an EMRI can be significant due to the large number of cycles in the LISA band made by the small compact object.","We show that LISA will in principle be able to detect and measure extremely small values of the disformal parameter which in turn, can be seen as an assessment of LISA's ability to detect very small deviations from the Kerr geometry."],"url":"http://arxiv.org/abs/2403.16192v1","category":"gr-qc"}
{"created":"2024-03-24 15:15:06","title":"Unveiling the underlying structure of axial-vector bottom-charm tetraquarks in the light of their magnetic moments","abstract":"The magnetic moment yields an excellent framework to explore the inner structure of particles determined by the quark-gluon dynamics of QCD, as it is the leading-order response of a bound system to a weak external magnetic field. Motivated by this, in this study, the magnetic moments of possible axial-vector $T_{bc\\bar u \\bar u}$, $T_{bc\\bar d \\bar d}$, and $T_{bc\\bar u \\bar d}$ tetraquarks are obtained with the help of light-cone QCD sum rules. For this purpose, we assume that these states are represented as a diquark-antidiquark picture with different structures and interpolating currents. The magnetic moment results derived using different diquark-antidiquark configurations differ substantially from each other. This can be translated into more than one tetraquark state with the same quantum number and quark content yet possessing different magnetic moments. From the numerical results obtained, we have concluded that the magnetic moments of the $T_{bc}$ states can project their inner structure, which can be used for their quantum numbers and quark-gluon organization. The contribution of individual quarks to the magnetic moments is also analyzed for completeness. We hope that our predictions of the magnetic moments of the $T_{bc}$ tetraquarks, together with the results of other theoretical investigations of the spectroscopic parameters and decay widths of these interesting tetraquarks, may be valuable in the search for these states in future experiments and in unraveling the internal structure of these tetraquarks.","sentences":["The magnetic moment yields an excellent framework to explore the inner structure of particles determined by the quark-gluon dynamics of QCD, as it is the leading-order response of a bound system to a weak external magnetic field.","Motivated by this, in this study, the magnetic moments of possible axial-vector $T_{bc\\bar u \\bar u}$, $T_{bc\\bar d \\bar d}$, and $T_{bc\\bar u \\bar d}$ tetraquarks are obtained with the help of light-cone QCD sum rules.","For this purpose, we assume that these states are represented as a diquark-antidiquark picture with different structures and interpolating currents.","The magnetic moment results derived using different diquark-antidiquark configurations differ substantially from each other.","This can be translated into more than one tetraquark state with the same quantum number and quark content yet possessing different magnetic moments.","From the numerical results obtained, we have concluded that the magnetic moments of the $T_{bc}$ states can project their inner structure, which can be used for their quantum numbers and quark-gluon organization.","The contribution of individual quarks to the magnetic moments is also analyzed for completeness.","We hope that our predictions of the magnetic moments of the $T_{bc}$ tetraquarks, together with the results of other theoretical investigations of the spectroscopic parameters and decay widths of these interesting tetraquarks, may be valuable in the search for these states in future experiments and in unraveling the internal structure of these tetraquarks."],"url":"http://arxiv.org/abs/2403.16191v1","category":"hep-ph"}
{"created":"2024-03-24 15:02:24","title":"Improving Scene Graph Generation with Relation Words' Debiasing in Vision-Language Models","abstract":"Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between various objects. However, this complexity and diversity in SGG also leads to underrepresentation, where part of test triplets are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose using the SGG models with pretrained vision-language models (VLMs) to enhance representation. However, due to the gap between the pretraining and SGG, directly ensembling the pretrained VLMs leads to severe biases across relation words. Thus, we introduce LM Estimation to approximate the words' distribution underlies in the pretraining language sets, and then use the distribution for debiasing. After that, we ensemble VLMs with SGG models to enhance representation. Considering that each model may represent better at different samples, we use a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our method effectively addresses the words biases, enhances SGG's representation, and achieve markable performance enhancements. It is training-free and integrates well with existing SGG models.","sentences":["Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between various objects.","However, this complexity and diversity in SGG also leads to underrepresentation, where part of test triplets are rare or even unseen during training, resulting in imprecise predictions.","To tackle this, we propose using the SGG models with pretrained vision-language models (VLMs) to enhance representation.","However, due to the gap between the pretraining and SGG, directly ensembling the pretrained VLMs leads to severe biases across relation words.","Thus, we introduce LM Estimation to approximate the words' distribution underlies in the pretraining language sets, and then use the distribution for debiasing.","After that, we ensemble VLMs with SGG models to enhance representation.","Considering that each model may represent better at different samples, we use a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights.","Our method effectively addresses the words biases, enhances SGG's representation, and achieve markable performance enhancements.","It is training-free and integrates well with existing SGG models."],"url":"http://arxiv.org/abs/2403.16184v1","category":"cs.CV"}
{"created":"2024-03-24 14:39:15","title":"Isoleucine gate blocks K+ conduction in C-type inactivation","abstract":"Many voltage-gated potassium (Kv) channels display a time-dependent phenomenon called C-type inactivation, whereby prolonged activation by voltage leads to the inhibition of ionic conduction, a process that involves a conformational change at the selectivity filter toward a non-conductive state. Recently, a high-resolution structure of a strongly inactivating triple-mutant channel kv1.2-kv2.1-3m revealed a novel conformation of the selectivity filter that is dilated at its outer end, distinct from the well-characterized conductive state. While the experimental structure was interpreted as the elusive non-conductive state, molecular dynamics simulations and electrophysiology measurements demonstrate that the dilated filter of kv1.2-kv2.1-3m, however, is conductive and, as such, cannot completely account for the inactivation of the channel observed in functional experiments. An additional conformational change implicating isoleucine residues at position 398 along the pore lining segment S6 is required to effectively block ion conduction. It is shown that the I398 residues from the four subunits act as a state-dependent hydrophobic gate located immediately beneath the selectivity filter. As a critical piece of the C-type inactivation machinery, this structural feature is the potential target of a broad class of QA blockers and negatively charged activators thus opening new research directions towards the development of drugs that specifically modulate gating-states of Kv channels.","sentences":["Many voltage-gated potassium (Kv) channels display a time-dependent phenomenon called C-type inactivation, whereby prolonged activation by voltage leads to the inhibition of ionic conduction, a process that involves a conformational change at the selectivity filter toward a non-conductive state.","Recently, a high-resolution structure of a strongly inactivating triple-mutant channel kv1.2-kv2.1-3m revealed a novel conformation of the selectivity filter that is dilated at its outer end, distinct from the well-characterized conductive state.","While the experimental structure was interpreted as the elusive non-conductive state, molecular dynamics simulations and electrophysiology measurements demonstrate that the dilated filter of kv1.2-kv2.1-3m, however, is conductive and, as such, cannot completely account for the inactivation of the channel observed in functional experiments.","An additional conformational change implicating isoleucine residues at position 398 along the pore lining segment S6 is required to effectively block ion conduction.","It is shown that the I398 residues from the four subunits act as a state-dependent hydrophobic gate located immediately beneath the selectivity filter.","As a critical piece of the C-type inactivation machinery, this structural feature is the potential target of a broad class of QA blockers and negatively charged activators thus opening new research directions towards the development of drugs that specifically modulate gating-states of Kv channels."],"url":"http://arxiv.org/abs/2403.16179v1","category":"physics.bio-ph"}
{"created":"2024-03-24 14:20:19","title":"de Sitter at all loops: the story of the Schwinger model","abstract":"We consider the two-dimensional Schwinger model of a massless charged fermion coupled to an Abelian gauge field on a fixed de Sitter background. The theory admits an exact solution, first examined by Jayewardena, and can be analyzed efficiently using Euclidean methods. We calculate fully non-perturbative, gauge-invariant correlation functions of the electric field as well as the fermion and analyze these correlators in the late-time limit. We compare these results with the perturbative picture, for example by verifying the one-loop contribution to the fermion two-point function, as predicted from the exact solution, matches the direct computation of the one-loop Feynman diagram. We demonstrate many features endemic of quantum field theory in de Sitter space, including the appearance of late-time logarithms, their resummation to de Sitter invariant expressions, and Boltzmann suppressed non-perturbative phenomena, with surprising late-time features.","sentences":["We consider the two-dimensional Schwinger model of a massless charged fermion coupled to an Abelian gauge field on a fixed de Sitter background.","The theory admits an exact solution, first examined by Jayewardena, and can be analyzed efficiently using Euclidean methods.","We calculate fully non-perturbative, gauge-invariant correlation functions of the electric field as well as the fermion and analyze these correlators in the late-time limit.","We compare these results with the perturbative picture, for example by verifying the one-loop contribution to the fermion two-point function, as predicted from the exact solution, matches the direct computation of the one-loop Feynman diagram.","We demonstrate many features endemic of quantum field theory in de Sitter space, including the appearance of late-time logarithms, their resummation to de Sitter invariant expressions, and Boltzmann suppressed non-perturbative phenomena, with surprising late-time features."],"url":"http://arxiv.org/abs/2403.16166v1","category":"hep-th"}
{"created":"2024-03-24 14:09:25","title":"Covariant Lagrangian Cubic Interaction Vertices For Irreducible Higher Spin Fields in Minkowski Backgrounds","abstract":"We review the application of BRST and BRST-BV approaches to construct the generic off-shell local Lorentz covariant cubic interaction vertices for irreducible massless and massive higher integer spin fields (as the candidates for massive particles in the Dark Matter problem) on $d$-dimensional Minkowski spaces. It is shown that equivalence among two Lagrangian dynamics for the same cubically interacting fields with given masses and spins obtained by means of the approach with complete BRST, $Q$, operator and of one with incomplete BRST, $Q_c$, operator in presence of consistent off-shell holonomic (traceless) constraints can be uplifted from the equivalence of the Lagrangians for free higher spin fields. We found that to get non-contradictory Lagrangians for irreducible interacting higher-spin fields within approach with $Q_c$ operator, together with off-shell algebraic constraints in addition to necessary condition of superconmmuting of $Q_c$ with appropriate holonomic constraints on the field and gauge parameter vectors, these constraints should form Abelian superalgebra both with BRST operator above and with operators of cubic vertices.","sentences":["We review the application of BRST and BRST-BV approaches to construct the generic off-shell local Lorentz covariant cubic interaction vertices for irreducible massless and massive higher integer spin fields (as the candidates for massive particles in the Dark Matter problem) on $d$-dimensional Minkowski spaces.","It is shown that equivalence among two Lagrangian dynamics for the same cubically interacting fields with given masses and spins obtained by means of the approach with complete BRST, $Q$, operator and of one with incomplete BRST, $Q_c$, operator in presence of consistent off-shell holonomic (traceless) constraints can be uplifted from the equivalence of the Lagrangians for free higher spin fields.","We found that to get non-contradictory Lagrangians for irreducible interacting higher-spin fields within approach with $Q_c$ operator, together with off-shell algebraic constraints in addition to necessary condition of superconmmuting of $Q_c$ with appropriate holonomic constraints on the field and gauge parameter vectors, these constraints should form Abelian superalgebra both with BRST operator above and with operators of cubic vertices."],"url":"http://arxiv.org/abs/2403.16164v1","category":"hep-th"}
{"created":"2024-03-24 13:44:29","title":"Fusion of Active and Passive Measurements for Robust and Scalable Positioning","abstract":"This paper addresses the challenge of achieving reliable and robust positioning of a mobile agent, such as a radio device carried by a person, in scenarios where direct line-of-sight (LOS) links are obstructed or unavailable. The human body is considered as an extended object that scatters, attenuates and blocks the radio signals. We propose a novel particle-based sum-product algorithm (SPA) that fuses active measurements between the agent and anchors with passive measurements from pairs of anchors reflected off the body. We first formulate radio signal models for both active and passive measurements. Then, a joint tracking algorithm that utilizes both active and passive measurements is developed for the extended object. The algorithm exploits the probabilistic data association (PDA) for multiple object-related measurements. The results demonstrate superior accuracy during and after the obstructed line-of-sight (OLOS) situation, outperforming conventional methods that solely rely on active measurements. The proposed joint estimation approach significantly enhances the localization robustness via radio sensing.","sentences":["This paper addresses the challenge of achieving reliable and robust positioning of a mobile agent, such as a radio device carried by a person, in scenarios where direct line-of-sight (LOS) links are obstructed or unavailable.","The human body is considered as an extended object that scatters, attenuates and blocks the radio signals.","We propose a novel particle-based sum-product algorithm (SPA) that fuses active measurements between the agent and anchors with passive measurements from pairs of anchors reflected off the body.","We first formulate radio signal models for both active and passive measurements.","Then, a joint tracking algorithm that utilizes both active and passive measurements is developed for the extended object.","The algorithm exploits the probabilistic data association (PDA) for multiple object-related measurements.","The results demonstrate superior accuracy during and after the obstructed line-of-sight (OLOS) situation, outperforming conventional methods that solely rely on active measurements.","The proposed joint estimation approach significantly enhances the localization robustness via radio sensing."],"url":"http://arxiv.org/abs/2403.16150v1","category":"eess.SP"}
{"created":"2024-03-24 13:17:04","title":"Glimmers in the Cosmic Dawn: A Census of the Youngest Supermassive Black Holes by Photometric Variability","abstract":"We report first results from a deep near infrared campaign with the Hubble Space Telescope to obtain late-epoch images of the Hubble Ultra-Deep Field (HUDF), 10-15 years after the first epoch data were obtained. The main objectives are to search for faint active galactic nuclei (AGN) at high redshifts by virtue of their photometric variability, and measure (or constrain) the comoving number density of supermassive black holes (SMBHs), n_SMBH, at early times. In this Letter we present a brief overview of the program and preliminary results regarding eight objects. Three variables are supernovae, two of which are apparently hostless with indeterminable redshifts, although one has previously been recorded at a z\\approx 6 galaxy. Two further objects are clear AGN candidates at z=2.0 and 3.2, based on morphology and/or spectroscopy, in particular infrared spectroscopy from JWST. Three variable targets are identified at z=6-7, which are also likely AGN candidates. These sources provide a first measure of n_SMBH in the reionization epoch by photometric variability, which places a firm lower limit of 3x10^{-4} cMpc^{-3}. After accounting for variability and luminosity incompleteness, we estimate n_SMBH \\gtrsim 8x10^{-3} cMpc^{-3}, which is the largest value so far reported at these redshifts. This SMBH abundance is also strikingly similar to estimates of n_SMBH in the local Universe. We discuss how these results test various theories for SMBH formation.","sentences":["We report first results from a deep near infrared campaign with the Hubble Space Telescope to obtain late-epoch images of the Hubble Ultra-Deep Field (HUDF), 10-15 years after the first epoch data were obtained.","The main objectives are to search for faint active galactic nuclei (AGN) at high redshifts by virtue of their photometric variability, and measure (or constrain) the comoving number density of supermassive black holes (SMBHs), n_SMBH, at early times.","In this Letter we present a brief overview of the program and preliminary results regarding eight objects.","Three variables are supernovae, two of which are apparently hostless with indeterminable redshifts, although one has previously been recorded at a z\\approx 6 galaxy.","Two further objects are clear AGN candidates at z=2.0 and 3.2, based on morphology and/or spectroscopy, in particular infrared spectroscopy from JWST.","Three variable targets are identified at z=6-7, which are also likely AGN candidates.","These sources provide a first measure of n_SMBH in the reionization epoch by photometric variability, which places a firm lower limit of 3x10^{-4} cMpc^{-3}.","After accounting for variability and luminosity incompleteness, we estimate n_SMBH \\gtrsim 8x10^{-3} cMpc^{-3}, which is the largest value so far reported at these redshifts.","This SMBH abundance is also strikingly similar to estimates of n_SMBH in the local Universe.","We discuss how these results test various theories for SMBH formation."],"url":"http://arxiv.org/abs/2403.16138v1","category":"astro-ph.GA"}
{"created":"2024-03-24 12:55:50","title":"Enhancing Video Transformers for Action Understanding with VLM-aided Training","abstract":"Owing to their ability to extract relevant spatio-temporal video embeddings, Vision Transformers (ViTs) are currently the best performing models in video action understanding. However, their generalization over domains or datasets is somewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated exceptional generalization performance, but are currently unable to process videos. Consequently, they cannot extract spatio-temporal patterns that are crucial for action understanding. In this paper, we propose the Four-tiered Prompts (FTP) framework that takes advantage of the complementary strengths of ViTs and VLMs. We retain ViTs' strong spatio-temporal representation ability but improve the visual encodings to be more comprehensive and general by aligning them with VLM outputs. The FTP framework adds four feature processors that focus on specific aspects of human action in videos: action category, action components, action description, and context information. The VLMs are only employed during training, and inference incurs a minimal computation cost. Our approach consistently yields state-of-the-art performance. For instance, we achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.","sentences":["Owing to their ability to extract relevant spatio-temporal video embeddings, Vision Transformers (ViTs) are currently the best performing models in video action understanding.","However, their generalization over domains or datasets is somewhat limited.","In contrast, Visual Language Models (VLMs) have demonstrated exceptional generalization performance, but are currently unable to process videos.","Consequently, they cannot extract spatio-temporal patterns that are crucial for action understanding.","In this paper, we propose the Four-tiered Prompts (FTP) framework that takes advantage of the complementary strengths of ViTs and VLMs.","We retain ViTs' strong spatio-temporal representation ability but improve the visual encodings to be more comprehensive and general by aligning them with VLM outputs.","The FTP framework adds four feature processors that focus on specific aspects of human action in videos: action category, action components, action description, and context information.","The VLMs are only employed during training, and inference incurs a minimal computation cost.","Our approach consistently yields state-of-the-art performance.","For instance, we achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively."],"url":"http://arxiv.org/abs/2403.16128v1","category":"cs.CV"}
{"created":"2024-03-24 12:48:52","title":"The strong coupling in the nonperturbative and near-perturbative regimes","abstract":"We use analytic continuation to extend the gauge/gravity duality nonperturbative description of the strong force coupling into the transition, near-perturbative, regime where perturbative effects become important. By excluding the unphysical region in coupling space from the flow of singularities in the complex plane, we derive a specific relation between the scales relevant at large and short distances; this relation is uniquely fixed by requiring maximal analyticity. The unified effective coupling model gives an accurate description of the data in the nonperturbative and the near-perturbative regions.","sentences":["We use analytic continuation to extend the gauge/gravity duality nonperturbative description of the strong force coupling into the transition, near-perturbative, regime where perturbative effects become important.","By excluding the unphysical region in coupling space from the flow of singularities in the complex plane, we derive a specific relation between the scales relevant at large and short distances; this relation is uniquely fixed by requiring maximal analyticity.","The unified effective coupling model gives an accurate description of the data in the nonperturbative and the near-perturbative regions."],"url":"http://arxiv.org/abs/2403.16126v1","category":"hep-ph"}
{"created":"2024-03-24 11:50:23","title":"QGSJET-III model of high energy hadronic interactions: II. Particle production and extensive air shower characteristics","abstract":"The hadronization procedure of the QGSJET-III Monte Carlo (MC) generator of high energy hadronic interactions is discussed. Selected results of the model, regarding production spectra of secondary particles, are presented in comparison to experimental data and to the corresponding predictions of the QGSJET-II-04 MC generator. The model is applied to calculations of basic characteristics of extensive air showers initiated by cosmic ray interactions in the atmosphere and the results are compared to predictions of other MC generators of cosmic ray interactions.","sentences":["The hadronization procedure of the QGSJET-III Monte Carlo (MC) generator of high energy hadronic interactions is discussed.","Selected results of the model, regarding production spectra of secondary particles, are presented in comparison to experimental data and to the corresponding predictions of the QGSJET-II-04 MC generator.","The model is applied to calculations of basic characteristics of extensive air showers initiated by cosmic ray interactions in the atmosphere and the results are compared to predictions of other MC generators of cosmic ray interactions."],"url":"http://arxiv.org/abs/2403.16106v1","category":"hep-ph"}
{"created":"2024-03-24 11:47:00","title":"Compositional statistical mechanics, entropy and variational inference","abstract":"In this document, we aim to gather various results related to a compositional/categorical approach to rigorous Statistical Mechanics. Rigorous Statistical Mechanics is centered on the mathematical study of statistical systems. Central concepts in this field have a natural expression in terms of diagrams in a category that couples measurable maps and Markov kernels. We showed that statistical systems are particular representations of partially ordered sets (posets), that we call A-specifications, and expressed their phases, i.e., Gibbs measures, as invariants of these representations. It opens the way to the use of homological algebra to compute phases of statistical systems. Two central results of rigorous Statistical Mechanics are, firstly, the characterization of extreme Gibbs measures as it relates to the zero-one law for extreme Gibbs measures, and, secondly, their variational principle which states that for translation invariant Hamiltonians, Gibbs measures are the minima of the Gibbs free energy. We showed how the characterization of extreme Gibbs measures extends to A-specifications; we proposed an Entropy functional for A-specifications and gave a message-passing algorithm, that generalized the belief propagation algorithm of graphical models, to find critical points of the associated variational free energy.","sentences":["In this document, we aim to gather various results related to a compositional/categorical approach to rigorous Statistical Mechanics.","Rigorous Statistical Mechanics is centered on the mathematical study of statistical systems.","Central concepts in this field have a natural expression in terms of diagrams in a category that couples measurable maps and Markov kernels.","We showed that statistical systems are particular representations of partially ordered sets (posets), that we call A-specifications, and expressed their phases, i.e., Gibbs measures, as invariants of these representations.","It opens the way to the use of homological algebra to compute phases of statistical systems.","Two central results of rigorous Statistical Mechanics are, firstly, the characterization of extreme Gibbs measures as it relates to the zero-one law for extreme Gibbs measures, and, secondly, their variational principle which states that for translation invariant Hamiltonians, Gibbs measures are the minima of the Gibbs free energy.","We showed how the characterization of extreme Gibbs measures extends to A-specifications; we proposed an Entropy functional for A-specifications and gave a message-passing algorithm, that generalized the belief propagation algorithm of graphical models, to find critical points of the associated variational free energy."],"url":"http://arxiv.org/abs/2403.16104v1","category":"math-ph"}
{"created":"2024-03-24 09:28:56","title":"Finding Candidate TeV Halos among Very-High Energy Sources","abstract":"We search for possible pulsar TeV halos among the very-high-energy (VHE) sources reported in different VHE surveys, among which in particular we use the results from the first Large High Altitude Air Shower Observatory (LHAASO) catalog of $\\gamma$-ray sources. Six candidates are found. They share the properties of containing a middle-aged, gamma-ray--bright pulsar in their positional error circles (the respective pulsars are J0248+6021, J0359+5414, J0622+3749, J0633+0632, J2006+3102, and J2238+5903), being in a rather clean field without any common Galactic VHE-emitting supernova remnants or (bright) pulsar wind nebulae (PWNe), and showing the absence of any gamma-ray emissions in 0.1--500\\,GeV after removing the pulsars' emissions. Combining with several (candidate) TeV halos reported, we find nearly the same relations as previously ones between their luminosities at 50\\,TeV, $L_{\\rm 50TeV}$, and the corresponding pulsars' spin-down luminosities, $\\dot{E}$, which are $L_{\\rm 50TeV}\\sim \\dot{E}^{0.9}$ and $L_{\\rm 50TeV}/\\dot{E}\\sim 6.4\\times 10^{-4}$. We probe possible connections between the extension sizes of the VHE sources and the pulsars' ages, and find a weak trend of being older and smaller. By comparing to the VHE detection results for PWNe, it is clear to see that the (candidate) TeV halos have hard emissions by having their power-law indices smaller than 2 in 1--25\\,TeV or only being detected in 25--100\\,TeV. In addition, we also consider other seven VHE sources as possible TeV halos because of different study results for them, but they do not cleanly fit in the properties listed above, indicating their potential complex nature.","sentences":["We search for possible pulsar TeV halos among the very-high-energy (VHE) sources reported in different VHE surveys, among which in particular we use the results from the first Large High Altitude Air Shower Observatory (LHAASO) catalog of $\\gamma$-ray sources.","Six candidates are found.","They share the properties of containing a middle-aged, gamma-ray--bright pulsar in their positional error circles (the respective pulsars are J0248+6021, J0359+5414, J0622+3749, J0633+0632, J2006+3102, and J2238+5903), being in a rather clean field without any common Galactic VHE-emitting supernova remnants or (bright) pulsar wind nebulae (PWNe), and showing the absence of any gamma-ray emissions in 0.1--500\\,GeV after removing the pulsars' emissions.","Combining with several (candidate) TeV halos reported, we find nearly the same relations as previously ones between their luminosities at 50\\,TeV, $L_{\\rm 50TeV}$, and the corresponding pulsars' spin-down luminosities, $\\dot{E}$, which are $L_{\\rm 50TeV}\\sim \\dot{E}^{0.9}$ and $L_{\\rm 50TeV}/\\dot{E}\\sim 6.4\\times 10^{-4}$.","We probe possible connections between the extension sizes of the VHE sources and the pulsars' ages, and find a weak trend of being older and smaller.","By comparing to the VHE detection results for PWNe, it is clear to see that the (candidate) TeV halos have hard emissions by having their power-law indices smaller than 2 in 1--25\\,TeV or only being detected in 25--100\\,TeV.","In addition, we also consider other seven VHE sources as possible TeV halos because of different study results for them, but they do not cleanly fit in the properties listed above, indicating their potential complex nature."],"url":"http://arxiv.org/abs/2403.16074v1","category":"astro-ph.HE"}
{"created":"2024-03-24 09:07:30","title":"The mass-metallicity and fundamental metallicity relations in non-AGN and AGN-host galaxies","abstract":"Galaxies' stellar masses, gas-phase oxygen abundances (metallicity), and star formation rates (SFRs) obey a series of empirical correlations, most notably the mass-metallicity relation (MZR) and fundamental metallicity relation (FZR), which relates oxygen abundance to a combination of stellar mass and SFR. However, due to the difficulty of measuring oxygen abundances and SFRs in galaxies that host powerful active galactic nuclei (AGN), to date it is unknown to what extent AGN-host galaxies also follow these correlations. In this work, we apply Bayesian methods to the MaNGA integral field spectrographic (IFS) survey that allow us to measure oxygen abundances and SFRs in AGN hosts, and use these measurements to explore how the MZR and FZR differ between galaxies that do and do not host AGN. We find similar MZRs at stellar masses above $10^{10.5} \\mathrm{M}_\\odot$, but that at lower stellar masses AGN hosts show up to $\\sim 0.2$ dex higher oxygen abundances. The offset in the FZR is significantly smaller, suggesting that the larger deviation in the MZR is a result of AGN-host galaxies having systematically lower SFRs at fixed stellar mass. However, within the AGN-host sample there is little correlation between SFR and oxygen abundance. These findings support a scenario in which an AGN can halt efficient gas accretion, which drives non-AGN host galaxies to both higher SFR and lower oxygen abundance, resulting in the galaxy evolving off the star-forming main sequence (SFMS). As a consequence, as the SFR declines for an individual system its metallicity remains mostly unchanged.","sentences":["Galaxies' stellar masses, gas-phase oxygen abundances (metallicity), and star formation rates (SFRs) obey a series of empirical correlations, most notably the mass-metallicity relation (MZR) and fundamental metallicity relation (FZR), which relates oxygen abundance to a combination of stellar mass and SFR.","However, due to the difficulty of measuring oxygen abundances and SFRs in galaxies that host powerful active galactic nuclei (AGN), to date it is unknown to what extent AGN-host galaxies also follow these correlations.","In this work, we apply Bayesian methods to the MaNGA integral field spectrographic (IFS) survey that allow us to measure oxygen abundances and SFRs in AGN hosts, and use these measurements to explore how the MZR and FZR differ between galaxies that do and do not host AGN.","We find similar MZRs at stellar masses above $10^{10.5} \\mathrm{M}_\\odot$, but that at lower stellar masses AGN hosts show up to $\\sim 0.2$ dex higher oxygen abundances.","The offset in the FZR is significantly smaller, suggesting that the larger deviation in the MZR is a result of AGN-host galaxies having systematically lower SFRs at fixed stellar mass.","However, within the AGN-host sample there is little correlation between SFR and oxygen abundance.","These findings support a scenario in which an AGN can halt efficient gas accretion, which drives non-AGN host galaxies to both higher SFR and lower oxygen abundance, resulting in the galaxy evolving off the star-forming main sequence (SFMS).","As a consequence, as the SFR declines for an individual system its metallicity remains mostly unchanged."],"url":"http://arxiv.org/abs/2403.16069v1","category":"astro-ph.GA"}
{"created":"2024-03-24 08:53:10","title":"The seeding of cosmic ray electrons by cluster radio galaxies: a review","abstract":"Radio galaxies in clusters of galaxies are prominent reservoirs of magnetic fields and of non-thermal particles, which get mixed with the intracluster medium. We review the observational and theoretical knowledge of the role of these crucial ingredients for the formation of diffuse radio emission in clusters (radio halos, relics, mini halos) and outline the open questions in this field.","sentences":["Radio galaxies in clusters of galaxies are prominent reservoirs of magnetic fields and of non-thermal particles, which get mixed with the intracluster medium.","We review the observational and theoretical knowledge of the role of these crucial ingredients for the formation of diffuse radio emission in clusters (radio halos, relics, mini halos) and outline the open questions in this field."],"url":"http://arxiv.org/abs/2403.16068v1","category":"astro-ph.GA"}
{"created":"2024-03-24 08:25:48","title":"Axion production in the $\u03b7\\to \u03c0\u03c0a$ decay within $SU(3)$ chiral perturbation theory","abstract":"We study the axion and axion-like particle production from the $\\eta\\to\\pi\\pi a$ decay within the $SU(3)$ chiral perturbation theory up to the one-loop level. The conventional $SU(3)$ chiral low energy constants are found to be able to reabsorb all the divergences from the chiral loops in the $\\eta\\to\\pi\\pi a$ decay amplitude, and hence render the amplitude independent of the renormalization scale. The unitarized $\\eta\\to\\pi\\pi a$ decay amplitudes are constructed to take into account the $\\pi\\pi$ final-state interactions and also properly reproduce the perturbative results from the chiral perturbation theory. Detailed analyses between the perturbative amplitudes and the unitarized ones are given in the phenomenological discussions. By taking the values of the chiral low energy constants in literature, we predict the Dalitz distributions, the spectra of the $\\pi\\pi$ and $a\\pi$ systems, and also the branching ratios of the $\\eta\\to\\pi\\pi a$ process by varying $m_a$ from 0 to $m_\\eta-2m_{\\pi}$.","sentences":["We study the axion and axion-like particle production from the $\\eta\\to\\pi\\pi a$ decay within the $SU(3)$ chiral perturbation theory up to the one-loop level.","The conventional $SU(3)$ chiral low energy constants are found to be able to reabsorb all the divergences from the chiral loops in the $\\eta\\to\\pi\\pi a$ decay amplitude, and hence render the amplitude independent of the renormalization scale.","The unitarized $\\eta\\to\\pi\\pi a$ decay amplitudes are constructed to take into account the $\\pi\\pi$ final-state interactions and also properly reproduce the perturbative results from the chiral perturbation theory.","Detailed analyses between the perturbative amplitudes and the unitarized ones are given in the phenomenological discussions.","By taking the values of the chiral low energy constants in literature, we predict the Dalitz distributions, the spectra of the $\\pi\\pi$ and $a\\pi$ systems, and also the branching ratios of the $\\eta\\to\\pi\\pi a$ process by varying $m_a$ from 0 to $m_\\eta-2m_{\\pi}$."],"url":"http://arxiv.org/abs/2403.16064v1","category":"hep-ph"}
{"created":"2024-03-24 07:02:59","title":"KG-particles in a cosmic string rainbow gravity spacetime in mixed magnetic fields","abstract":"We investigate and report the effects of rainbow gravity on the spectroscopic structure of KG-oscillators in a mixed magnetic field (in the sense that it has the usually described as a uniform and a non-uniform magnetic fields, each at a time) introduced by the 4-vector potential $A_\\mu=(0,0,A_\\varphi,0)$, where $A_\\varphi=B_1 r^2/2+B_2 r$, and $B_1$ and $B_2$ are magnetic field strengths. We also discuss and report the effects of such a mixed magnetic field on the spectra of KG-oscillators in cosmic string rainbow gravity. In so doing, we introduce a new and quite handy \\textit{conditionally exact solution} associated with the truncation of the biconfluent Heun functions into polynomials. Using a loop quantum gravity motivated rainbow functions, we observe interesting effects when the magnetic field strength $B_2$ grows up from zero. Such effects include energy levels crossings which, in this case, turns the spectra of the KG-oscillators upside down. Moreover, Landau-like signature on the spectra are observed and discussed. Yet, interestingly, we also observe that rainbow gravity affects the magnetic field as well, in the sense that the magnetic field becomes probe particle energy-dependent one.","sentences":["We investigate and report the effects of rainbow gravity on the spectroscopic structure of KG-oscillators in a mixed magnetic field (in the sense that it has the usually described as a uniform and a non-uniform magnetic fields, each at a time) introduced by the 4-vector potential $A_\\mu=(0,0,A_\\varphi,0)$, where $A_\\varphi=B_1 r^2/2+B_2 r$, and $B_1$ and $B_2$ are magnetic field strengths.","We also discuss and report the effects of such a mixed magnetic field on the spectra of KG-oscillators in cosmic string rainbow gravity.","In so doing, we introduce a new and quite handy \\textit{conditionally exact solution} associated with the truncation of the biconfluent Heun functions into polynomials.","Using a loop quantum gravity motivated rainbow functions, we observe interesting effects when the magnetic field strength $B_2$ grows up from zero.","Such effects include energy levels crossings which, in this case, turns the spectra of the KG-oscillators upside down.","Moreover, Landau-like signature on the spectra are observed and discussed.","Yet, interestingly, we also observe that rainbow gravity affects the magnetic field as well, in the sense that the magnetic field becomes probe particle energy-dependent one."],"url":"http://arxiv.org/abs/2403.16041v1","category":"gr-qc"}
{"created":"2024-03-24 03:12:32","title":"Dimensionally Reducing Generalized Symmetries from (3+1)-Dimensions","abstract":"Recently there has been an increasing interest in the study of generalized symmetries in dimensions higher than two. This has lead to the discovery of various manifestations of generalized symmetries, notably higher-group and non-invertible symmetries, in four dimensions. In this paper we shall examine what happens to this structure when the 4d theory is compactified to lower dimensions, specifically to 3d and 2d, where we shall be mainly interested in generalized symmetry structures whose origin can be linked to mixed flavor-gauge anomalies. We discuss several aspects of the compactification, and in particular argue that under certain conditions the discussed generalized symmetry structure may trivialize in the infrared. Nevertheless, we show that even when this happens the presence of the 4d generalized symmetry structure may still leave an imprint on the low-energy theory in terms of additional 't Hooft anomalies or by breaking part of the symmetry. We apply and illustrate this using known examples of compactifications from four dimensions, particularly, the reduction of 4d $\\mathcal{N}=1$ $U(N_c)$ SQCD on a circle to 3d and on a sphere to 2d.","sentences":["Recently there has been an increasing interest in the study of generalized symmetries in dimensions higher than two.","This has lead to the discovery of various manifestations of generalized symmetries, notably higher-group and non-invertible symmetries, in four dimensions.","In this paper we shall examine what happens to this structure when the 4d theory is compactified to lower dimensions, specifically to 3d and 2d, where we shall be mainly interested in generalized symmetry structures whose origin can be linked to mixed flavor-gauge anomalies.","We discuss several aspects of the compactification, and in particular argue that under certain conditions the discussed generalized symmetry structure may trivialize in the infrared.","Nevertheless, we show that even when this happens the presence of the 4d generalized symmetry structure may still leave an imprint on the low-energy theory in terms of additional 't Hooft anomalies or by breaking part of the symmetry.","We apply and illustrate this using known examples of compactifications from four dimensions, particularly, the reduction of 4d $\\mathcal{N}=1$ $U(N_c)$ SQCD on a circle to 3d and on a sphere to 2d."],"url":"http://arxiv.org/abs/2403.15995v1","category":"hep-th"}
{"created":"2024-03-24 00:36:21","title":"PSHop: A Lightweight Feed-Forward Method for 3D Prostate Gland Segmentation","abstract":"Automatic prostate segmentation is an important step in computer-aided diagnosis of prostate cancer and treatment planning. Existing methods of prostate segmentation are based on deep learning models which have a large size and lack of transparency which is essential for physicians. In this paper, a new data-driven 3D prostate segmentation method on MRI is proposed, named PSHop. Different from deep learning based methods, the core methodology of PSHop is a feed-forward encoder-decoder system based on successive subspace learning (SSL). It consists of two modules: 1) encoder: fine to coarse unsupervised representation learning with cascaded VoxelHop units, 2) decoder: coarse to fine segmentation prediction with voxel-wise classification and local refinement. Experiments are conducted on the publicly available ISBI-2013 dataset, as well as on a larger private one. Experimental analysis shows that our proposed PSHop is effective, robust and lightweight in the tasks of prostate gland and zonal segmentation, achieving a Dice Similarity Coefficient (DSC) of 0.873 for the gland segmentation task. PSHop achieves a competitive performance comparatively to other deep learning methods, while keeping the model size and inference complexity an order of magnitude smaller.","sentences":["Automatic prostate segmentation is an important step in computer-aided diagnosis of prostate cancer and treatment planning.","Existing methods of prostate segmentation are based on deep learning models which have a large size and lack of transparency which is essential for physicians.","In this paper, a new data-driven 3D prostate segmentation method on MRI is proposed, named PSHop.","Different from deep learning based methods, the core methodology of PSHop is a feed-forward encoder-decoder system based on successive subspace learning (SSL).","It consists of two modules: 1) encoder: fine to coarse unsupervised representation learning with cascaded VoxelHop units, 2) decoder: coarse to fine segmentation prediction with voxel-wise classification and local refinement.","Experiments are conducted on the publicly available ISBI-2013 dataset, as well as on a larger private one.","Experimental analysis shows that our proposed PSHop is effective, robust and lightweight in the tasks of prostate gland and zonal segmentation, achieving a Dice Similarity Coefficient (DSC) of 0.873 for the gland segmentation task.","PSHop achieves a competitive performance comparatively to other deep learning methods, while keeping the model size and inference complexity an order of magnitude smaller."],"url":"http://arxiv.org/abs/2403.15971v1","category":"eess.IV"}
