{"created":"2024-04-18 17:59:58","title":"On the Content Bias in Fr\u00e9chet Video Distance","abstract":"Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video generation models, is known to conflict with human perception occasionally. In this paper, we aim to explore the extent of FVD's bias toward per-frame quality over temporal realism and identify its sources. We first quantify the FVD's sensitivity to the temporal axis by decoupling the frame and motion quality and find that the FVD increases only slightly with large temporal corruption. We then analyze the generated videos and show that via careful sampling from a large set of generated videos that do not contain motions, one can drastically decrease FVD without improving the temporal quality. Both studies suggest FVD's bias towards the quality of individual frames. We further observe that the bias can be attributed to the features extracted from a supervised video classifier trained on the content-biased dataset. We show that FVD with features extracted from the recent large-scale self-supervised video models is less biased toward image quality. Finally, we revisit a few real-world examples to validate our hypothesis.","sentences":["Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video generation models, is known to conflict with human perception occasionally.","In this paper, we aim to explore the extent of FVD's bias toward per-frame quality over temporal realism and identify its sources.","We first quantify the FVD's sensitivity to the temporal axis by decoupling the frame and motion quality and find that the FVD increases only slightly with large temporal corruption.","We then analyze the generated videos and show that via careful sampling from a large set of generated videos that do not contain motions, one can drastically decrease FVD without improving the temporal quality.","Both studies suggest FVD's bias towards the quality of individual frames.","We further observe that the bias can be attributed to the features extracted from a supervised video classifier trained on the content-biased dataset.","We show that FVD with features extracted from the recent large-scale self-supervised video models is less biased toward image quality.","Finally, we revisit a few real-world examples to validate our hypothesis."],"url":"http://arxiv.org/abs/2404.12391v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:54","title":"BLINK: Multimodal Large Language Models Can See but Not Perceive","abstract":"We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans \"within a blink\" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.","sentences":["We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations.","Most of the Blink tasks can be solved by humans \"within a blink\" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning).","However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language.","Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting.","While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not \"emerged\" yet in recent multimodal LLMs.","Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements.","We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception."],"url":"http://arxiv.org/abs/2404.12390v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:53","title":"VideoGigaGAN: Towards Detail-rich Video Super-Resolution","abstract":"Video super-resolution (VSR) approaches have shown impressive temporal consistency in upsampled videos. However, these approaches tend to generate blurrier results than their image counterparts as they are limited in their generative capability. This raises a fundamental question: can we extend the success of a generative image upsampler to the VSR task while preserving the temporal consistency? We introduce VideoGigaGAN, a new generative VSR model that can produce videos with high-frequency details and temporal consistency. VideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply inflating GigaGAN to a video model by adding temporal modules produces severe temporal flickering. We identify several key issues and propose techniques that significantly improve the temporal consistency of upsampled videos. Our experiments show that, unlike previous VSR methods, VideoGigaGAN generates temporally consistent videos with more fine-grained appearance details. We validate the effectiveness of VideoGigaGAN by comparing it with state-of-the-art VSR models on public datasets and showcasing video results with $8\\times$ super-resolution.","sentences":["Video super-resolution (VSR) approaches have shown impressive temporal consistency in upsampled videos.","However, these approaches tend to generate blurrier results than their image counterparts as they are limited in their generative capability.","This raises a fundamental question: can we extend the success of a generative image upsampler to the VSR task while preserving the temporal consistency?","We introduce VideoGigaGAN, a new generative VSR model that can produce videos with high-frequency details and temporal consistency.","VideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN.","Simply inflating GigaGAN to a video model by adding temporal modules produces severe temporal flickering.","We identify several key issues and propose techniques that significantly improve the temporal consistency of upsampled videos.","Our experiments show that, unlike previous VSR methods, VideoGigaGAN generates temporally consistent videos with more fine-grained appearance details.","We validate the effectiveness of VideoGigaGAN by comparing it with state-of-the-art VSR models on public datasets and showcasing video results with $8\\times$ super-resolution."],"url":"http://arxiv.org/abs/2404.12388v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:46","title":"SOHES: Self-supervised Open-world Hierarchical Entity Segmentation","abstract":"Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that eliminates the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pre-trained self-supervised representation, we produce abundant high-quality pseudo-labels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks. Project page: https://SOHES.github.io.","sentences":["Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts.","Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators.","This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that eliminates the need for human annotations.","SOHES operates in three phases: self-exploration, self-instruction, and self-correction.","Given a pre-trained self-supervised representation, we produce abundant high-quality pseudo-labels through visual feature clustering.","Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure.","Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities.","Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks.","Project page: https://SOHES.github.io."],"url":"http://arxiv.org/abs/2404.12386v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:41","title":"MeshLRM: Large Reconstruction Model for High-Quality Mesh","abstract":"We propose MeshLRM, a novel LRM-based approach that can reconstruct a high-quality mesh from merely four input images in less than one second. Different from previous large reconstruction models (LRMs) that focus on NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction and rendering within the LRM framework. This allows for end-to-end mesh reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering. Moreover, we improve the LRM architecture by simplifying several complex designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained with low- and high-resolution images; this new LRM training strategy enables significantly faster convergence and thereby leads to better quality with less compute. Our approach achieves state-of-the-art mesh reconstruction from sparse-view inputs and also allows for many downstream applications, including text-to-3D and single-image-to-3D generation. Project page: https://sarahweiii.github.io/meshlrm/","sentences":["We propose MeshLRM, a novel LRM-based approach that can reconstruct a high-quality mesh from merely four input images in less than one second.","Different from previous large reconstruction models (LRMs) that focus on NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction and rendering within the LRM framework.","This allows for end-to-end mesh reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.","Moreover, we improve the LRM architecture by simplifying several complex designs in previous LRMs.","MeshLRM's NeRF initialization is sequentially trained with low- and high-resolution images; this new LRM training strategy enables significantly faster convergence and thereby leads to better quality with less compute.","Our approach achieves state-of-the-art mesh reconstruction from sparse-view inputs and also allows for many downstream applications, including text-to-3D and single-image-to-3D generation.","Project page: https://sarahweiii.github.io/meshlrm/"],"url":"http://arxiv.org/abs/2404.12385v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:28","title":"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis","abstract":"We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 categories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines.   Project website: https://judyye.github.io/ghop-www","sentences":["We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category.","To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object.","We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis.","We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 categories, represents a first approach that allows jointly generating both hand and object.","Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines.   ","Project website: https://judyye.github.io/ghop-www"],"url":"http://arxiv.org/abs/2404.12383v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:27","title":"Lazy Diffusion Transformer for Interactive Image Editing","abstract":"We introduce a novel diffusion transformer, LazyDiffusion, that generates partial image updates efficiently. Our approach targets interactive image editing applications in which, starting from a blank canvas or an image, a user specifies a sequence of localized image modifications using binary masks and text prompts. Our generator operates in two phases. First, a context encoder processes the current canvas and user mask to produce a compact global context tailored to the region to generate. Second, conditioned on this context, a diffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\" fashion, i.e., it only generates the masked region. This contrasts with previous works that either regenerate the full canvas, wasting time and computation, or confine processing to a tight rectangular crop around the mask, ignoring the global image context altogether. Our decoder's runtime scales with the mask size, which is typically small, while our encoder introduces negligible overhead. We demonstrate that our approach is competitive with state-of-the-art inpainting methods in terms of quality and fidelity while providing a 10x speedup for typical user interactions, where the editing mask represents 10% of the image.","sentences":["We introduce a novel diffusion transformer, LazyDiffusion, that generates partial image updates efficiently.","Our approach targets interactive image editing applications in which, starting from a blank canvas or an image, a user specifies a sequence of localized image modifications using binary masks and text prompts.","Our generator operates in two phases.","First, a context encoder processes the current canvas and user mask to produce a compact global context tailored to the region to generate.","Second, conditioned on this context, a diffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\" fashion, i.e., it only generates the masked region.","This contrasts with previous works that either regenerate the full canvas, wasting time and computation, or confine processing to a tight rectangular crop around the mask, ignoring the global image context altogether.","Our decoder's runtime scales with the mask size, which is typically small, while our encoder introduces negligible overhead.","We demonstrate that our approach is competitive with state-of-the-art inpainting methods in terms of quality and fidelity while providing a 10x speedup for typical user interactions, where the editing mask represents 10% of the image."],"url":"http://arxiv.org/abs/2404.12382v1","category":"cs.CV"}
{"created":"2024-04-18 17:59:11","title":"Wavelength-accurate and wafer-scale process for nonlinear frequency mixers in thin-film lithium niobate","abstract":"Recent advancements in thin-film lithium niobate (TFLN) photonics have led to a new generation of high-performance electro-optic devices, including modulators, frequency combs, and microwave-to-optical transducers. However, the broader adoption of TFLN-based devices that rely on all-optical nonlinearities have been limited by the sensitivity of quasi-phase matching (QPM), realized via ferroelectric poling, to fabrication tolerances. Here, we propose a scalable fabrication process aimed at improving the wavelength-accuracy of optical frequency mixers in TFLN. In contrast to the conventional pole-before-etch approach, we first define the waveguide in TFLN and then perform ferroelectric poling. This sequence allows for precise metrology before and after waveguide definition to fully capture the geometry imperfections. Systematic errors can also be calibrated by measuring a subset of devices to fine-tune the QPM design for remaining devices on the wafer. Using this method, we fabricated a large number of second harmonic generation devices aimed at generating 737 nm light, with 73% operating within 5 nm of the target wavelength. Furthermore, we also demonstrate thermo-optic tuning and trimming of the devices via cladding deposition, with the former bringing ~96% of tested devices to the target wavelength. Our technique enables the rapid growth of integrated quantum frequency converters, photon pair sources, and optical parametric amplifiers, thus facilitating the integration of TFLN-based nonlinear frequency mixers into more complex and functional photonic systems.","sentences":["Recent advancements in thin-film lithium niobate (TFLN) photonics have led to a new generation of high-performance electro-optic devices, including modulators, frequency combs, and microwave-to-optical transducers.","However, the broader adoption of TFLN-based devices that rely on all-optical nonlinearities have been limited by the sensitivity of quasi-phase matching (QPM), realized via ferroelectric poling, to fabrication tolerances.","Here, we propose a scalable fabrication process aimed at improving the wavelength-accuracy of optical frequency mixers in TFLN.","In contrast to the conventional pole-before-etch approach, we first define the waveguide in TFLN and then perform ferroelectric poling.","This sequence allows for precise metrology before and after waveguide definition to fully capture the geometry imperfections.","Systematic errors can also be calibrated by measuring a subset of devices to fine-tune the QPM design for remaining devices on the wafer.","Using this method, we fabricated a large number of second harmonic generation devices aimed at generating 737 nm light, with 73% operating within 5 nm of the target wavelength.","Furthermore, we also demonstrate thermo-optic tuning and trimming of the devices via cladding deposition, with the former bringing ~96% of tested devices to the target wavelength.","Our technique enables the rapid growth of integrated quantum frequency converters, photon pair sources, and optical parametric amplifiers, thus facilitating the integration of TFLN-based nonlinear frequency mixers into more complex and functional photonic systems."],"url":"http://arxiv.org/abs/2404.12381v1","category":"physics.app-ph"}
{"created":"2024-04-18 17:58:45","title":"Internal sequential commutation and single generation","abstract":"We extract a precise internal description of the sequential commutation equivalence relation introduced in [KEP23] for tracial von Neumann algebras. As an application we prove that if a tracial von Neumann algebra $N$ is generated by unitaries $\\{u_i\\}_{i\\in \\mathbb{N}}$ such that $u_i\\sim u_j$ (i.e, there exists a finite set of Haar unitaries $\\{w_i\\}_{i=1}^{n}$ in $N^\\mathcal{U}$ such that $[u_i, w_1]= [w_k, w_{k+1}]=[w_n,u_j]=0$ for all $1\\leq k< n$) then $N$ is singly generated. This generalizes and recovers several known single generation phenomena for II$_1$ factors in the literature with a unified proof.","sentences":["We extract a precise internal description of the sequential commutation equivalence relation introduced in [KEP23] for tracial von Neumann algebras.","As an application we prove that if a tracial von Neumann algebra $N$ is generated by unitaries $\\{u_i\\}_{i\\in \\mathbb{N}}$ such that $u_i\\sim u_j$ (i.e, there exists a finite set of Haar unitaries $\\{w_i\\}_{i=1}^{n}$ in $N^\\mathcal{U}$ such that $[u_i, w_1]= [w_k, w_{k+1}]=[w_n,u_j]=0$ for all $1\\leq k< n$) then $N$ is singly generated.","This generalizes and recovers several known single generation phenomena for II$_1$ factors in the literature with a unified proof."],"url":"http://arxiv.org/abs/2404.12380v1","category":"math.OA"}
{"created":"2024-04-18 17:58:16","title":"6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction","abstract":"Current 3D reconstruction techniques struggle to infer unbounded scenes from a few images faithfully. Specifically, existing methods have high computational demands, require detailed pose information, and cannot reconstruct occluded regions reliably. We introduce 6Img-to-3D, an efficient, scalable transformer-based encoder-renderer method for single-shot image to 3D reconstruction. Our method outputs a 3D-consistent parameterized triplane from only six outward-facing input images for large-scale, unbounded outdoor driving scenarios. We take a step towards resolving existing shortcomings by combining contracted custom cross- and self-attention mechanisms for triplane parameterization, differentiable volume rendering, scene contraction, and image feature projection. We showcase that six surround-view vehicle images from a single timestamp without global pose information are enough to reconstruct 360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows, for example, rendering third-person images and birds-eye views. Our code is available at https://github.com/continental/6Img-to-3D, and more examples can be found at our website here https://6Img-to-3D.GitHub.io/.","sentences":["Current 3D reconstruction techniques struggle to infer unbounded scenes from a few images faithfully.","Specifically, existing methods have high computational demands, require detailed pose information, and cannot reconstruct occluded regions reliably.","We introduce 6Img-to-3D, an efficient, scalable transformer-based encoder-renderer method for single-shot image to 3D reconstruction.","Our method outputs a 3D-consistent parameterized triplane from only six outward-facing input images for large-scale, unbounded outdoor driving scenarios.","We take a step towards resolving existing shortcomings by combining contracted custom cross- and self-attention mechanisms for triplane parameterization, differentiable volume rendering, scene contraction, and image feature projection.","We showcase that six surround-view vehicle images from a single timestamp without global pose information are enough to reconstruct 360$^{\\circ}$ scenes during inference time, taking 395 ms.","Our method allows, for example, rendering third-person images and birds-eye views.","Our code is available at https://github.com/continental/6Img-to-3D, and more examples can be found at our website here https://6Img-to-3D.GitHub.io/."],"url":"http://arxiv.org/abs/2404.12378v1","category":"cs.CV"}
{"created":"2024-04-18 17:58:03","title":"RoboDreamer: Learning Compositional World Models for Robot Imagination","abstract":"Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation.","sentences":["Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation.","However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time.","This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments.","To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation.","We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos.","We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components.","We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image.","Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation."],"url":"http://arxiv.org/abs/2404.12377v1","category":"cs.RO"}
{"created":"2024-04-18 17:54:16","title":"Tunable Kondo physics in a van der Waals kagome antiferromagnet","abstract":"The Kondo lattice physics, describing the hybridization of localized spin matrix with dispersive conduction electrons, breeds numerous discoveries in the realm of strongly correlated quantum matter. Generally observed in lanthanide and actinide compounds, increasing attention has been directed towards alternative pathways for achieving flat band structures, such as Morie superlattices and Kagome metals. However, fine control of Kondo interaction outside of heterostructures remains elusive. Here we report the discovery of a van der Waals (vdW) kagome antiferromagnet CsCr6Sb6. Angle-resolved photoemission spectra and theoretical analysis show clear flat bands, consisting of half-filled 3dxz and 3dyz orbitals of Cr, situated 50 meV below the Fermi level. Importantly, we observe the emergence of anomalous Hall effect with remarkable tunability by simple reduction the sample thickness. The effective control of kondo interaction in CsCr6Sb6 render it an ideal platform for exploring unpresented phenomena using the vast toolkit of vdW structures.","sentences":["The Kondo lattice physics, describing the hybridization of localized spin matrix with dispersive conduction electrons, breeds numerous discoveries in the realm of strongly correlated quantum matter.","Generally observed in lanthanide and actinide compounds, increasing attention has been directed towards alternative pathways for achieving flat band structures, such as Morie superlattices and Kagome metals.","However, fine control of Kondo interaction outside of heterostructures remains elusive.","Here we report the discovery of a van der Waals (vdW) kagome antiferromagnet CsCr6Sb6.","Angle-resolved photoemission spectra and theoretical analysis show clear flat bands, consisting of half-filled 3dxz and 3dyz orbitals of Cr, situated 50 meV below the Fermi level.","Importantly, we observe the emergence of anomalous Hall effect with remarkable tunability by simple reduction the sample thickness.","The effective control of kondo interaction in CsCr6Sb6 render it an ideal platform for exploring unpresented phenomena using the vast toolkit of vdW structures."],"url":"http://arxiv.org/abs/2404.12374v1","category":"cond-mat.str-el"}
{"created":"2024-04-18 17:54:10","title":"Sound speed as a source of the gravitational field in modified gravity","abstract":"In the context of $f(R,T)$ gravity and other modified theories of gravity, the knowledge of the first order variation of the trace $T$ of the energy-momentum tensor with respect to the metric is essential for an accurate characterization of the gravitational field. In this paper, by considering a paradigmatic example of a perfect fluid whose dynamics is described by a pure k-essence matter Lagrangian in $f(R,T)=R+\\mathcal F(T)$ gravity, we show that the first order variation of the trace of the energy-momentum tensor cannot in general be determined from the proper density, proper pressure and 4-velocity of the fluid alone, and that the sound speed of the fluid can directly influence the dynamics of gravity. We also confirm that the second variation of the matter Lagrangian with respect to the metric should not in general be neglected. These results can be particularly relevant for cosmological studies of $f(R,T)$ gravity in which some of the material content of the Universe is modeled as a perfect fluid.","sentences":["In the context of $f(R,T)$ gravity and other modified theories of gravity, the knowledge of the first order variation of the trace $T$ of the energy-momentum tensor with respect to the metric is essential for an accurate characterization of the gravitational field.","In this paper, by considering a paradigmatic example of a perfect fluid whose dynamics is described by a pure k-essence matter Lagrangian in $f(R,T)=R+\\mathcal F(T)$ gravity, we show that the first order variation of the trace of the energy-momentum tensor cannot in general be determined from the proper density, proper pressure and 4-velocity of the fluid alone, and that the sound speed of the fluid can directly influence the dynamics of gravity.","We also confirm that the second variation of the matter Lagrangian with respect to the metric should not in general be neglected.","These results can be particularly relevant for cosmological studies of $f(R,T)$ gravity in which some of the material content of the Universe is modeled as a perfect fluid."],"url":"http://arxiv.org/abs/2404.12373v1","category":"gr-qc"}
{"created":"2024-04-18 17:53:19","title":"MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale","abstract":"Medical Visual Question Answering (MedVQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing MedVQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamlining data preparation and build new benchmark MedVQA datasets R-RAD and R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales into the training process. The framework includes three distinct strategies to generate decision outcomes and corresponding rationales, thereby clearly showcasing the medical decision-making process during reasoning. Extensive experiments demonstrate that our method can achieve an accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming existing state-of-the-art baselines. Dataset and code will be released.","sentences":["Medical Visual Question Answering (MedVQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare.","It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses.","However, the model interpretability and transparency of existing MedVQA solutions are often limited, posing challenges in understanding their decision-making processes.","To address this issue, we devise a semi-automated annotation process to streamlining data preparation and build new benchmark MedVQA datasets R-RAD and R-SLAKE.","The R-RAD and R-SLAKE datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD and SLAKE.","Moreover, we design a novel framework which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales into the training process.","The framework includes three distinct strategies to generate decision outcomes and corresponding rationales, thereby clearly showcasing the medical decision-making process during reasoning.","Extensive experiments demonstrate that our method can achieve an accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming existing state-of-the-art baselines.","Dataset and code will be released."],"url":"http://arxiv.org/abs/2404.12372v1","category":"cs.CV"}
{"created":"2024-04-18 17:50:15","title":"Information theory unifies atomistic machine learning, uncertainty quantification, and materials thermodynamics","abstract":"An accurate description of information is relevant for a range of problems in atomistic modeling, such as sampling methods, detecting rare events, analyzing datasets, or performing uncertainty quantification (UQ) in machine learning (ML)-driven simulations. Although individual methods have been proposed for each of these tasks, they lack a common theoretical background integrating their solutions. Here, we introduce an information theoretical framework that unifies predictions of phase transformations, kinetic events, dataset optimality, and model-free UQ from atomistic simulations, thus bridging materials modeling, ML, and statistical mechanics. We first demonstrate that, for a proposed representation, the information entropy of a distribution of atom-centered environments is a surrogate value for thermodynamic entropy. Using molecular dynamics (MD) simulations, we show that information entropy differences from trajectories can be used to build phase diagrams, identify rare events, and recover classical theories of nucleation. Building on these results, we use this general concept of entropy to quantify information in datasets for ML interatomic potentials (IPs), informing compression, explaining trends in testing errors, and evaluating the efficiency of active learning strategies. Finally, we propose a model-free UQ method for MLIPs using information entropy, showing it reliably detects extrapolation regimes, scales to millions of atoms, and goes beyond model errors. This method is made available as the package QUESTS: Quick Uncertainty and Entropy via STructural Similarity, providing a new unifying theory for data-driven atomistic modeling and combining efforts in ML, first-principles thermodynamics, and simulations.","sentences":["An accurate description of information is relevant for a range of problems in atomistic modeling, such as sampling methods, detecting rare events, analyzing datasets, or performing uncertainty quantification (UQ) in machine learning (ML)-driven simulations.","Although individual methods have been proposed for each of these tasks, they lack a common theoretical background integrating their solutions.","Here, we introduce an information theoretical framework that unifies predictions of phase transformations, kinetic events, dataset optimality, and model-free UQ from atomistic simulations, thus bridging materials modeling, ML, and statistical mechanics.","We first demonstrate that, for a proposed representation, the information entropy of a distribution of atom-centered environments is a surrogate value for thermodynamic entropy.","Using molecular dynamics (MD) simulations, we show that information entropy differences from trajectories can be used to build phase diagrams, identify rare events, and recover classical theories of nucleation.","Building on these results, we use this general concept of entropy to quantify information in datasets for ML interatomic potentials (IPs), informing compression, explaining trends in testing errors, and evaluating the efficiency of active learning strategies.","Finally, we propose a model-free UQ method for MLIPs using information entropy, showing it reliably detects extrapolation regimes, scales to millions of atoms, and goes beyond model errors.","This method is made available as the package QUESTS:","Quick Uncertainty and Entropy via STructural Similarity, providing a new unifying theory for data-driven atomistic modeling and combining efforts in ML, first-principles thermodynamics, and simulations."],"url":"http://arxiv.org/abs/2404.12367v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 17:48:05","title":"When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes","abstract":"We present FastFit, a method, and a Python package design to provide fast and accurate few-shot classification, especially for scenarios with many semantically similar classes. FastFit utilizes a novel approach integrating batch contrastive learning and token-level similarity score. Compared to existing few-shot learning packages, such as SetFit, Transformers, or few-shot prompting of large language models via API calls, FastFit significantly improves multiclass classification performance in speed and accuracy across FewMany, our newly curated English benchmark, and Multilingual datasets. FastFit demonstrates a 3-20x improvement in training speed, completing training in just a few seconds. The FastFit package is now available on GitHub and PyPi, presenting a user-friendly solution for NLP practitioners.","sentences":["We present FastFit, a method, and a Python package design to provide fast and accurate few-shot classification, especially for scenarios with many semantically similar classes.","FastFit utilizes a novel approach integrating batch contrastive learning and token-level similarity score.","Compared to existing few-shot learning packages, such as SetFit, Transformers, or few-shot prompting of large language models via API calls, FastFit significantly improves multiclass classification performance in speed and accuracy across FewMany, our newly curated English benchmark, and Multilingual datasets.","FastFit demonstrates a 3-20x improvement in training speed, completing training in just a few seconds.","The FastFit package is now available on GitHub and PyPi, presenting a user-friendly solution for NLP practitioners."],"url":"http://arxiv.org/abs/2404.12365v1","category":"cs.CL"}
{"created":"2024-04-18 17:45:49","title":"CRIRES$^+$ transmission spectroscopy of WASP-127b. Detection of the resolved signatures of a supersonic equatorial jet and cool poles in a hot planet","abstract":"General circulation models of gas giant exoplanets predict equatorial jets that drive inhomogeneities across the planetary atmosphere. We studied the transmission spectrum of the hot Jupiter WASP-127b during one transit in the K band with CRIRES+. Telluric and stellar signals were removed from the data using SYSREM and the planetary signal was investigated using the cross-correlation (CCF) technique. After detecting a spectral signal indicative of atmospheric inhomogeneities, we employed a Bayesian retrieval framework with a 2D modelling approach tailored to address this scenario. We detected strong signals of H$_2$O and CO, which exhibited not one but two distinct CCF peaks. The double peaked signal can be explained by a supersonic equatorial jet and muted signals at the poles, with the two peaks representing the signals from the planet's morning and evening terminators, respectively. We calculated an equatorial jet velocity of $7.7\\pm0.2$km/s from our retrieved overall equatorial velocity and the planet's tidally locked rotation, and derive distinct atmospheric properties for the two terminators as well as the polar region. The evening terminator is found to be hotter than the morning terminator by $175^{+116}_{-133}$K and the muted signals from the poles can be explained by significantly lower temperatures or a high cloud deck. Our retrieval yields a solar C/O ratio and metallicity and challenges previous studies of WASP-127b's atmosphere. The presence of a double peaked signal highlights the importance of accounting for planetary 3D structure during interpretation of atmospheric signals. The measured supersonic jet velocity and the lack of signal from the polar regions, representing a detection of latitudinal inhomogeneity in a spatially unresolved target, showcases the power of high-resolution transmission spectroscopy for the characterization of global circulation in exoplanets.","sentences":["General circulation models of gas giant exoplanets predict equatorial jets that drive inhomogeneities across the planetary atmosphere.","We studied the transmission spectrum of the hot Jupiter WASP-127b during one transit in the K band with CRIRES+.","Telluric and stellar signals were removed from the data using SYSREM and the planetary signal was investigated using the cross-correlation (CCF) technique.","After detecting a spectral signal indicative of atmospheric inhomogeneities, we employed a Bayesian retrieval framework with a 2D modelling approach tailored to address this scenario.","We detected strong signals of H$_2$O and CO, which exhibited not one but two distinct CCF peaks.","The double peaked signal can be explained by a supersonic equatorial jet and muted signals at the poles, with the two peaks representing the signals from the planet's morning and evening terminators, respectively.","We calculated an equatorial jet velocity of $7.7\\pm0.2$km/s from our retrieved overall equatorial velocity and the planet's tidally locked rotation, and derive distinct atmospheric properties for the two terminators as well as the polar region.","The evening terminator is found to be hotter than the morning terminator by $175^{+116}_{-133}$K and the muted signals from the poles can be explained by significantly lower temperatures or a high cloud deck.","Our retrieval yields a solar C/O ratio and metallicity and challenges previous studies of WASP-127b's atmosphere.","The presence of a double peaked signal highlights the importance of accounting for planetary 3D structure during interpretation of atmospheric signals.","The measured supersonic jet velocity and the lack of signal from the polar regions, representing a detection of latitudinal inhomogeneity in a spatially unresolved target, showcases the power of high-resolution transmission spectroscopy for the characterization of global circulation in exoplanets."],"url":"http://arxiv.org/abs/2404.12363v1","category":"astro-ph.EP"}
{"created":"2024-04-18 17:40:23","title":"Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI using Diffusion Models","abstract":"Deep learning methods for accelerated MRI achieve state-of-the-art results but largely ignore additional speedups possible with noncartesian sampling trajectories. To address this gap, we created a generative diffusion model-based reconstruction algorithm for multi-coil highly undersampled spiral MRI. This model uses conditioning during training as well as frequency-based guidance to ensure consistency between images and measurements. Evaluated on retrospective data, we show high quality (structural similarity > 0.87) in reconstructed images with ultrafast scan times (0.02 seconds for a 2D image). We use this algorithm to identify a set of optimal variable-density spiral trajectories and show large improvements in image quality compared to conventional reconstruction using the non-uniform fast Fourier transform. By combining efficient spiral sampling trajectories, multicoil imaging, and deep learning reconstruction, these methods could enable the extremely high acceleration factors needed for real-time 3D imaging.","sentences":["Deep learning methods for accelerated MRI achieve state-of-the-art results but largely ignore additional speedups possible with noncartesian sampling trajectories.","To address this gap, we created a generative diffusion model-based reconstruction algorithm for multi-coil highly undersampled spiral MRI.","This model uses conditioning during training as well as frequency-based guidance to ensure consistency between images and measurements.","Evaluated on retrospective data, we show high quality (structural similarity > 0.87) in reconstructed images with ultrafast scan times (0.02 seconds for a 2D image).","We use this algorithm to identify a set of optimal variable-density spiral trajectories and show large improvements in image quality compared to conventional reconstruction using the non-uniform fast Fourier transform.","By combining efficient spiral sampling trajectories, multicoil imaging, and deep learning reconstruction, these methods could enable the extremely high acceleration factors needed for real-time 3D imaging."],"url":"http://arxiv.org/abs/2404.12361v1","category":"cs.AI"}
{"created":"2024-04-18 17:39:47","title":"False vacuum decay and nucleation dynamics in neutral atom systems","abstract":"False vacuum decay and nucleation offer the opportunity to study non-equilibrium dynamical phenomena in quantum many-body systems with confinement. Recent work has examined false vacuum decay in 1D ferromagnetic Ising spins and superfluids. In this paper, we study false vacuum nucleation dynamics in 1D antiferromagnetic neutral atom chains with Rydberg interactions, using both numerical simulations and analytic modeling. We apply a staggered local detuning field to generate the false and true vacuum states. Our efforts focus on two dynamical regimes: decay and annealing. In the first, we corroborate the phenomenological decay rate scaling and determine the associated parameter range for the decay process; in the second, we uncover and elucidate a procedure to anneal the false vacuum from the initial to the final system, with intermediate nucleation events. We further propose experimental protocols to prepare the required states and perform quenches on near-term neutral atom quantum simulators, examining the experimental feasibility of our proposed setup and parameter regime.","sentences":["False vacuum decay and nucleation offer the opportunity to study non-equilibrium dynamical phenomena in quantum many-body systems with confinement.","Recent work has examined false vacuum decay in 1D ferromagnetic Ising spins and superfluids.","In this paper, we study false vacuum nucleation dynamics in 1D antiferromagnetic neutral atom chains with Rydberg interactions, using both numerical simulations and analytic modeling.","We apply a staggered local detuning field to generate the false and true vacuum states.","Our efforts focus on two dynamical regimes: decay and annealing.","In the first, we corroborate the phenomenological decay rate scaling and determine the associated parameter range for the decay process; in the second, we uncover and elucidate a procedure to anneal the false vacuum from the initial to the final system, with intermediate nucleation events.","We further propose experimental protocols to prepare the required states and perform quenches on near-term neutral atom quantum simulators, examining the experimental feasibility of our proposed setup and parameter regime."],"url":"http://arxiv.org/abs/2404.12360v1","category":"quant-ph"}
{"created":"2024-04-18 17:37:53","title":"Inverse Neural Rendering for Explainable Multi-Object Tracking","abstract":"Today, most methods for image understanding tasks rely on feed-forward neural networks. While this approach has allowed for empirical accuracy, efficiency, and task adaptation via fine-tuning, it also comes with fundamental disadvantages. Existing networks often struggle to generalize across different datasets, even on the same task. By design, these networks ultimately reason about high-dimensional scene features, which are challenging to analyze. This is true especially when attempting to predict 3D information based on 2D images. We propose to recast 3D multi-object tracking from RGB cameras as an \\emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations and retrieve the latents that best represent object instances in a given input image. To this end, we optimize an image loss over generative latent spaces that inherently disentangle shape and appearance properties. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure situations, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method by learning the generative prior exclusively from synthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo datasets. Both these datasets are completely unseen to our method and do not require fine-tuning. Videos and code are available at https://light.princeton.edu/inverse-rendering-tracking/.","sentences":["Today, most methods for image understanding tasks rely on feed-forward neural networks.","While this approach has allowed for empirical accuracy, efficiency, and task adaptation via fine-tuning, it also comes with fundamental disadvantages.","Existing networks often struggle to generalize across different datasets, even on the same task.","By design, these networks ultimately reason about high-dimensional scene features, which are challenging to analyze.","This is true especially when attempting to predict 3D information based on 2D images.","We propose to recast 3D multi-object tracking from RGB cameras as an \\emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations and retrieve the latents that best represent object instances in a given input image.","To this end, we optimize an image loss over generative latent spaces that inherently disentangle shape and appearance properties.","We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure situations, and resolving ambiguous cases.","We validate the generalization and scaling capabilities of our method by learning the generative prior exclusively from synthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo datasets.","Both these datasets are completely unseen to our method and do not require fine-tuning.","Videos and code are available at https://light.princeton.edu/inverse-rendering-tracking/."],"url":"http://arxiv.org/abs/2404.12359v1","category":"cs.CV"}
{"created":"2024-04-18 17:37:02","title":"From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function","abstract":"Reinforcement Learning From Human Feedback (RLHF) has been a critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. In this work we rectify this difference, first we theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using our theoretical results, we provide three concrete empirical insights. First, we show that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, we prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy. Empirically we show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, we show how the choice of reference policy causes implicit rewards to decline during training. We conclude by discussing applications of our work, including information elicitation in multi-tun dialogue, reasoning, agentic applications and end-to-end training of multi-model systems.","sentences":["Reinforcement Learning From Human Feedback (RLHF) has been a critical to the success of the latest generation of generative AI models.","In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach.","Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches.","Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm.","In this work we rectify this difference, first we theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation.","Using our theoretical results, we provide three concrete empirical insights.","First, we show that because of its token level interpretation, DPO is able to perform some type of credit assignment.","Next, we prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy.","Empirically we show that a simple beam search yields meaningful improvement over the base DPO policy.","Finally, we show how the choice of reference policy causes implicit rewards to decline during training.","We conclude by discussing applications of our work, including information elicitation in multi-tun dialogue, reasoning, agentic applications and end-to-end training of multi-model systems."],"url":"http://arxiv.org/abs/2404.12358v1","category":"cs.LG"}
{"created":"2024-04-18 17:34:20","title":"Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation","abstract":"Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities.","sentences":["Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks.","In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE.","Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system.","Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology.","More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training.","Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities."],"url":"http://arxiv.org/abs/2404.12355v1","category":"cs.LG"}
{"created":"2024-04-18 17:32:46","title":"V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning","abstract":"Video summarization aims to create short, accurate, and cohesive summaries of longer videos. Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective fine-tuning of advanced large vision-language models (VLMs). Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization. Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT). However, the textual summaries in previous multimodal datasets are inadequate. To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39\\%. Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries. In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions. Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks. Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks.","sentences":["Video summarization aims to create short, accurate, and cohesive summaries of longer videos.","Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective fine-tuning of advanced large vision-language models (VLMs).","Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization.","Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT).","However, the textual summaries in previous multimodal datasets are inadequate.","To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39\\%.","Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries.","In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions.","Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks.","Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks."],"url":"http://arxiv.org/abs/2404.12353v1","category":"cs.CV"}
{"created":"2024-04-18 17:32:32","title":"Point-In-Context: Understanding Point Cloud via In-Context Learning","abstract":"With the emergence of large-scale models trained on diverse datasets, in-context learning has emerged as a promising paradigm for multitasking, notably in natural language processing and image processing. However, its application in 3D point cloud tasks remains largely unexplored. In this work, we introduce Point-In-Context (PIC), a novel framework for 3D point cloud understanding via in-context learning. We address the technical challenge of effectively extending masked point modeling to 3D point clouds by introducing a Joint Sampling module and proposing a vanilla version of PIC called Point-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist model for various 3D point cloud tasks, with inputs and outputs modeled as coordinates. In this paradigm, the challenging segmentation task is achieved by assigning label points with XYZ coordinates for each category; the final prediction is then chosen based on the label point closest to the predictions. To break the limitation by the fixed label-coordinate assignment, which has poor generalization upon novel classes, we propose two novel training strategies, In-Context Labeling and In-Context Enhancing, forming an extended version of PIC named Point-In-Context-Segmenter (PIC-S), targeting improving dynamic context labeling and model training. By utilizing dynamic in-context labels and extra in-context pairs, PIC-S achieves enhanced performance and generalization capability in and across part segmentation datasets. PIC is a general framework so that other tasks or datasets can be seamlessly introduced into our PIC through a unified data format. We conduct extensive experiments to validate the versatility and adaptability of our proposed methods in handling a wide range of tasks and segmenting multi-datasets. Our PIC-S is capable of generalizing unseen datasets and performing novel part segmentation by customizing prompts.","sentences":["With the emergence of large-scale models trained on diverse datasets, in-context learning has emerged as a promising paradigm for multitasking, notably in natural language processing and image processing.","However, its application in 3D point cloud tasks remains largely unexplored.","In this work, we introduce Point-In-Context (PIC), a novel framework for 3D point cloud understanding via in-context learning.","We address the technical challenge of effectively extending masked point modeling to 3D point clouds by introducing a Joint Sampling module and proposing a vanilla version of PIC called Point-In-Context-Generalist (PIC-G).","PIC-G is designed as a generalist model for various 3D point cloud tasks, with inputs and outputs modeled as coordinates.","In this paradigm, the challenging segmentation task is achieved by assigning label points with XYZ coordinates for each category; the final prediction is then chosen based on the label point closest to the predictions.","To break the limitation by the fixed label-coordinate assignment, which has poor generalization upon novel classes, we propose two novel training strategies, In-Context Labeling and In-Context Enhancing, forming an extended version of PIC named Point-In-Context-Segmenter (PIC-S), targeting improving dynamic context labeling and model training.","By utilizing dynamic in-context labels and extra in-context pairs, PIC-S achieves enhanced performance and generalization capability in and across part segmentation datasets.","PIC is a general framework so that other tasks or datasets can be seamlessly introduced into our PIC through a unified data format.","We conduct extensive experiments to validate the versatility and adaptability of our proposed methods in handling a wide range of tasks and segmenting multi-datasets.","Our PIC-S is capable of generalizing unseen datasets and performing novel part segmentation by customizing prompts."],"url":"http://arxiv.org/abs/2404.12352v1","category":"cs.CV"}
{"created":"2024-04-18 17:30:00","title":"Gravitational wave luminosity distance-weighted anisotropies","abstract":"Measurements of the luminosity distance of propagating gravitational waves can provide invaluable information on the geometry and content of our Universe. Due to the clustering of cosmic structures, in realistic situations we need to average the luminosity distance of events coming from patches inside a volume. In this work we evaluate, in a gauge-invariant and fully-relativistic treatment, the impact of cosmological perturbations on such averaging process. We find that clustering, lensing and peculiar velocity effects impact estimates for future detectors such as Einstein Telescope, Cosmic Explorer, the Big Bang Observer and DECIGO. The signal-to-noise ratio of the angular power spectrum of the average luminosity distance over all the redshift bins is 17 in the case of binary black holes detected by Einstein Telescope and Cosmic Explorer. We also provide fitting formulas for the corrections to the average luminosity distance due to general relativistic effects.","sentences":["Measurements of the luminosity distance of propagating gravitational waves can provide invaluable information on the geometry and content of our Universe.","Due to the clustering of cosmic structures, in realistic situations we need to average the luminosity distance of events coming from patches inside a volume.","In this work we evaluate, in a gauge-invariant and fully-relativistic treatment, the impact of cosmological perturbations on such averaging process.","We find that clustering, lensing and peculiar velocity effects impact estimates for future detectors such as Einstein Telescope, Cosmic Explorer, the Big Bang Observer and DECIGO.","The signal-to-noise ratio of the angular power spectrum of the average luminosity distance over all the redshift bins is 17 in the case of binary black holes detected by Einstein Telescope and Cosmic Explorer.","We also provide fitting formulas for the corrections to the average luminosity distance due to general relativistic effects."],"url":"http://arxiv.org/abs/2404.12351v1","category":"astro-ph.CO"}
{"created":"2024-04-18 17:26:01","title":"Evaluating AI for Law: Bridging the Gap with Open-Source Solutions","abstract":"This study evaluates the performance of general-purpose AI, like ChatGPT, in legal question-answering tasks, highlighting significant risks to legal professionals and clients. It suggests leveraging foundational models enhanced by domain-specific knowledge to overcome these issues. The paper advocates for creating open-source legal AI systems to improve accuracy, transparency, and narrative diversity, addressing general AI's shortcomings in legal contexts.","sentences":["This study evaluates the performance of general-purpose AI, like ChatGPT, in legal question-answering tasks, highlighting significant risks to legal professionals and clients.","It suggests leveraging foundational models enhanced by domain-specific knowledge to overcome these issues.","The paper advocates for creating open-source legal AI systems to improve accuracy, transparency, and narrative diversity, addressing general AI's shortcomings in legal contexts."],"url":"http://arxiv.org/abs/2404.12349v1","category":"cs.AI"}
{"created":"2024-04-18 17:24:56","title":"Radio Observations as an Extrasolar Planet Discovery and Characterization: Interior Structure and Habitability","abstract":"Detection of radio emission from Jupiter was identified quickly as being due to its planetary-scale magnetic field. Subsequent spacecraft investigations have revealed that many of the planets, and even some moons, either have or have had large-scale magnetic fields. In the case of the Earth, Jupiter, Saturn, Uranus, and Neptune, the their magnetic fields are generated by dynamo processes within these planets, and an interaction between the solar wind and their magnetic fields generates intense radio emission via the electron cyclotron maser instability. In the case of Jupiter, its magnetic field interacts with the moon Io to result in radio emission as well.   Extrasolar planets reasonably may be expected to generate large-scale magnetic fields and to sustain an electron cyclotron maser instability. Not only may these radio emissions be a means for discovering extrasolar planets, because magnetic fields are tied to the properties of planetary interiors, radio emissions may be a remote sensing means of constraining extrasolar planetary properties that will be otherwise difficult to access. In the case of terrestrial planets, the presence or absence of a magnetic field may be an indicator for habitability. Since the first edition of the Handbook, there have been a number of advances, albeit there remain no unambigous detection of radio emission from extrasolar planets. New ground-based telescopes and new possibilities for space-based telescopes provide promise for the future.","sentences":["Detection of radio emission from Jupiter was identified quickly as being due to its planetary-scale magnetic field.","Subsequent spacecraft investigations have revealed that many of the planets, and even some moons, either have or have had large-scale magnetic fields.","In the case of the Earth, Jupiter, Saturn, Uranus, and Neptune, the their magnetic fields are generated by dynamo processes within these planets, and an interaction between the solar wind and their magnetic fields generates intense radio emission via the electron cyclotron maser instability.","In the case of Jupiter, its magnetic field interacts with the moon Io to result in radio emission as well.   ","Extrasolar planets reasonably may be expected to generate large-scale magnetic fields and to sustain an electron cyclotron maser instability.","Not only may these radio emissions be a means for discovering extrasolar planets, because magnetic fields are tied to the properties of planetary interiors, radio emissions may be a remote sensing means of constraining extrasolar planetary properties that will be otherwise difficult to access.","In the case of terrestrial planets, the presence or absence of a magnetic field may be an indicator for habitability.","Since the first edition of the Handbook, there have been a number of advances, albeit there remain no unambigous detection of radio emission from extrasolar planets.","New ground-based telescopes and new possibilities for space-based telescopes provide promise for the future."],"url":"http://arxiv.org/abs/2404.12348v1","category":"astro-ph.EP"}
{"created":"2024-04-18 17:24:28","title":"AniClipart: Clipart Animation with Text-to-Video Priors","abstract":"Clipart, a pre-made graphic art form, offers a convenient and efficient way of illustrating visual content. Traditional workflows to convert static clipart images into motion sequences are laborious and time-consuming, involving numerous intricate steps like rigging, key animation and in-betweening. Recent advancements in text-to-video generation hold great potential in resolving this problem. Nevertheless, direct application of text-to-video generation models often struggles to retain the visual identity of clipart images or generate cartoon-style motions, resulting in unsatisfactory animation outcomes. In this paper, we introduce AniClipart, a system that transforms static clipart images into high-quality motion sequences guided by text-to-video priors. To generate cartoon-style and smooth motion, we first define B\\'{e}zier curves over keypoints of the clipart image as a form of motion regularization. We then align the motion trajectories of the keypoints with the provided text prompt by optimizing the Video Score Distillation Sampling (VSDS) loss, which encodes adequate knowledge of natural motion within a pretrained text-to-video diffusion model. With a differentiable As-Rigid-As-Possible shape deformation algorithm, our method can be end-to-end optimized while maintaining deformation rigidity. Experimental results show that the proposed AniClipart consistently outperforms existing image-to-video generation models, in terms of text-video alignment, visual identity preservation, and motion consistency. Furthermore, we showcase the versatility of AniClipart by adapting it to generate a broader array of animation formats, such as layered animation, which allows topological changes.","sentences":["Clipart, a pre-made graphic art form, offers a convenient and efficient way of illustrating visual content.","Traditional workflows to convert static clipart images into motion sequences are laborious and time-consuming, involving numerous intricate steps like rigging, key animation and in-betweening.","Recent advancements in text-to-video generation hold great potential in resolving this problem.","Nevertheless, direct application of text-to-video generation models often struggles to retain the visual identity of clipart images or generate cartoon-style motions, resulting in unsatisfactory animation outcomes.","In this paper, we introduce AniClipart, a system that transforms static clipart images into high-quality motion sequences guided by text-to-video priors.","To generate cartoon-style and smooth motion, we first define B\\'{e}zier curves over keypoints of the clipart image as a form of motion regularization.","We then align the motion trajectories of the keypoints with the provided text prompt by optimizing the Video Score Distillation Sampling (VSDS) loss, which encodes adequate knowledge of natural motion within a pretrained text-to-video diffusion model.","With a differentiable As-Rigid-As-Possible shape deformation algorithm, our method can be end-to-end optimized while maintaining deformation rigidity.","Experimental results show that the proposed AniClipart consistently outperforms existing image-to-video generation models, in terms of text-video alignment, visual identity preservation, and motion consistency.","Furthermore, we showcase the versatility of AniClipart by adapting it to generate a broader array of animation formats, such as layered animation, which allows topological changes."],"url":"http://arxiv.org/abs/2404.12347v1","category":"cs.CV"}
{"created":"2024-04-18 17:20:05","title":"Modeling nonlinear scales with COLA: preparing for LSST-Y1","abstract":"Year 1 results of the Legacy Survey of Space and Time (LSST) will provide tighter constraints on small-scale cosmology, beyond the validity of linear perturbation theory. This heightens the demand for a computationally affordable prescription that can accurately capture nonlinearities in beyond-$\\Lambda$CDM models. The COmoving Lagrangian Acceleration (COLA) method, a cost-effective \\textit{N}-body technique, has been proposed as a viable alternative to high-resolution \\textit{N}-body simulations for training emulators of the nonlinear matter power spectrum. In this study, we evaluate this approach by employing COLA emulators to conduct a cosmic shear analysis with LSST-Y1 simulated data across three different nonlinear scale cuts. We use the $w$CDM model, for which the \\textsc{EuclidEmulator2} (\\textsc{ee2}) exists as a benchmark, having been trained with high-resolution \\textit{N}-body simulations. We primarily utilize COLA simulations with mass resolution $M_{\\rm part}\\approx 8 \\times 10^{10} ~h^{-1} M_{\\odot}$ and force resolution $\\ell_{\\rm force}=0.5 ~h^{-1}$Mpc, though we also test refined settings with $M_{\\rm part}\\approx 1 \\times 10^{10} ~h^{-1}M_{\\odot}$ and force resolution $\\ell_{\\rm force}=0.17 ~h^{-1}$Mpc. We find the performance of the COLA emulators is sensitive to the placement of high-resolution \\textit{N}-body reference samples inside the prior, which only ensure agreement in their local vicinity. However, the COLA emulators pass stringent criteria in goodness-of-fit and parameter bias throughout the prior, when $\\Lambda$CDM predictions of \\textsc{ee2} are computed alongside every COLA emulator prediction, suggesting a promising approach for extended models.","sentences":["Year 1 results of the Legacy Survey of Space and Time (LSST) will provide tighter constraints on small-scale cosmology, beyond the validity of linear perturbation theory.","This heightens the demand for a computationally affordable prescription that can accurately capture nonlinearities in beyond-$\\Lambda$CDM models.","The COmoving Lagrangian Acceleration (COLA) method, a cost-effective \\textit{N}-body technique, has been proposed as a viable alternative to high-resolution \\textit{N}-body simulations for training emulators of the nonlinear matter power spectrum.","In this study, we evaluate this approach by employing COLA emulators to conduct a cosmic shear analysis with LSST-Y1 simulated data across three different nonlinear scale cuts.","We use the $w$CDM model, for which the \\textsc{EuclidEmulator2} (\\textsc{ee2}) exists as a benchmark, having been trained with high-resolution \\textit{N}-body simulations.","We primarily utilize COLA simulations with mass resolution $M_{\\rm part}\\approx 8 \\times 10^{10} ~h^{-1} M_{\\odot}$ and force resolution $\\ell_{\\rm force}=0.5 ~h^{-1}$Mpc, though we also test refined settings with $M_{\\rm part}\\approx 1","\\times 10^{10} ~h^{-1}M_{\\odot}$ and force resolution $\\ell_{\\rm force}=0.17 ~h^{-1}$Mpc.","We find the performance of the COLA emulators is sensitive to the placement of high-resolution \\textit{N}-body reference samples inside the prior, which only ensure agreement in their local vicinity.","However, the COLA emulators pass stringent criteria in goodness-of-fit and parameter bias throughout the prior, when $\\Lambda$CDM predictions of \\textsc{ee2} are computed alongside every COLA emulator prediction, suggesting a promising approach for extended models."],"url":"http://arxiv.org/abs/2404.12344v1","category":"astro-ph.CO"}
{"created":"2024-04-18 17:16:52","title":"Gemini Near Infrared Spectrograph -- Distant Quasar Survey: Rest-Frame Ultraviolet-Optical Spectral Properties of Broad Absorption Line Quasars","abstract":"We present the rest-frame ultraviolet-optical spectral properties of 65 broad absorption line (BAL) quasars from the Gemini Near Infrared Spectrograph-Distant Quasar Survey (GNIRS-DQS). These properties are compared with those of 195 non-BAL quasars from GNIRS-DQS in order to identify the drivers for the appearance of BALs in quasar spectra. In particular, we compare equivalent widths and velocity widths, as well as velocity offsets from systemic redshifts, of principal emission lines. In spite of the differences between their rest-frame ultraviolet spectra, we find that luminous BAL quasars are generally indistinguishable from their non-BAL counterparts in the rest-frame optical band at redshifts $1.55 \\lesssim z \\lesssim 3.50$. We do not find any correlation between BAL trough properties and the H$\\beta$-based supermassive black hole masses and normalized accretion rates in our sample. Considering the Sloan Digital Sky Survey quasar sample, which includes the GNIRS-DQS sample, we find that a monochromatic luminosity at rest-frame 2500 A of $\\gtrsim 10^{45}$ erg s$^{-1}$ is a necessary condition for launching BAL outflows in quasars. We compare our findings with other BAL quasar samples and discuss the roles that accretion rate and orientation play in the appearance of BAL troughs in quasar spectra.","sentences":["We present the rest-frame ultraviolet-optical spectral properties of 65 broad absorption line (BAL) quasars from the Gemini Near Infrared Spectrograph-Distant Quasar Survey (GNIRS-DQS).","These properties are compared with those of 195 non-BAL quasars from GNIRS-DQS in order to identify the drivers for the appearance of BALs in quasar spectra.","In particular, we compare equivalent widths and velocity widths, as well as velocity offsets from systemic redshifts, of principal emission lines.","In spite of the differences between their rest-frame ultraviolet spectra, we find that luminous BAL quasars are generally indistinguishable from their non-BAL counterparts in the rest-frame optical band at redshifts $1.55 \\lesssim z \\lesssim 3.50$.","We do not find any correlation between BAL trough properties and the H$\\beta$-based supermassive black hole masses and normalized accretion rates in our sample.","Considering the Sloan Digital Sky Survey quasar sample, which includes the GNIRS-DQS sample, we find that a monochromatic luminosity at rest-frame","2500 A of $\\gtrsim 10^{45}$ erg s$^{-1}$ is a necessary condition for launching BAL outflows in quasars.","We compare our findings with other BAL quasar samples and discuss the roles that accretion rate and orientation play in the appearance of BAL troughs in quasar spectra."],"url":"http://arxiv.org/abs/2404.12343v1","category":"astro-ph.GA"}
{"created":"2024-04-18 17:16:16","title":"Large Language Models in Targeted Sentiment Analysis","abstract":"In this paper we investigate the use of decoder-based generative transformers for extracting sentiment towards the named entities in Russian news articles. We study sentiment analysis capabilities of instruction-tuned large language models (LLMs). We consider the dataset of RuSentNE-2023 in our study. The first group of experiments was aimed at the evaluation of zero-shot capabilities of LLMs with closed and open transparencies. The second covers the fine-tuning of Flan-T5 using the \"chain-of-thought\" (CoT) three-hop reasoning framework (THoR). We found that the results of the zero-shot approaches are similar to the results achieved by baseline fine-tuned encoder-based transformers (BERT-base). Reasoning capabilities of the fine-tuned Flan-T5 models with THoR achieve at least 5% increment with the base-size model compared to the results of the zero-shot experiment. The best results of sentiment analysis on RuSentNE-2023 were achieved by fine-tuned Flan-T5-xl, which surpassed the results of previous state-of-the-art transformer-based classifiers. Our CoT application framework is publicly available: https://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework","sentences":["In this paper we investigate the use of decoder-based generative transformers for extracting sentiment towards the named entities in Russian news articles.","We study sentiment analysis capabilities of instruction-tuned large language models (LLMs).","We consider the dataset of RuSentNE-2023 in our study.","The first group of experiments was aimed at the evaluation of zero-shot capabilities of LLMs with closed and open transparencies.","The second covers the fine-tuning of Flan-T5 using the \"chain-of-thought\" (CoT) three-hop reasoning framework (THoR).","We found that the results of the zero-shot approaches are similar to the results achieved by baseline fine-tuned encoder-based transformers (BERT-base).","Reasoning capabilities of the fine-tuned Flan-T5 models with THoR achieve at least 5% increment with the base-size model compared to the results of the zero-shot experiment.","The best results of sentiment analysis on RuSentNE-2023 were achieved by fine-tuned Flan-T5-xl, which surpassed the results of previous state-of-the-art transformer-based classifiers.","Our CoT application framework is publicly available: https://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework"],"url":"http://arxiv.org/abs/2404.12342v1","category":"cs.CL"}
{"created":"2024-04-18 17:10:18","title":"Measuring Feature Dependency of Neural Networks by Collapsing Feature Dimensions in the Data Manifold","abstract":"This paper introduces a new technique to measure the feature dependency of neural network models. The motivation is to better understand a model by querying whether it is using information from human-understandable features, e.g., anatomical shape, volume, or image texture. Our method is based on the principle that if a model is dependent on a feature, then removal of that feature should significantly harm its performance. A targeted feature is \"removed\" by collapsing the dimension in the data distribution that corresponds to that feature. We perform this by moving data points along the feature dimension to a baseline feature value while staying on the data manifold, as estimated by a deep generative model. Then we observe how the model's performance changes on the modified test data set, with the target feature dimension removed. We test our method on deep neural network models trained on synthetic image data with known ground truth, an Alzheimer's disease prediction task using MRI and hippocampus segmentations from the OASIS-3 dataset, and a cell nuclei classification task using the Lizard dataset.","sentences":["This paper introduces a new technique to measure the feature dependency of neural network models.","The motivation is to better understand a model by querying whether it is using information from human-understandable features, e.g., anatomical shape, volume, or image texture.","Our method is based on the principle that if a model is dependent on a feature, then removal of that feature should significantly harm its performance.","A targeted feature is \"removed\" by collapsing the dimension in the data distribution that corresponds to that feature.","We perform this by moving data points along the feature dimension to a baseline feature value while staying on the data manifold, as estimated by a deep generative model.","Then we observe how the model's performance changes on the modified test data set, with the target feature dimension removed.","We test our method on deep neural network models trained on synthetic image data with known ground truth, an Alzheimer's disease prediction task using MRI and hippocampus segmentations from the OASIS-3 dataset, and a cell nuclei classification task using the Lizard dataset."],"url":"http://arxiv.org/abs/2404.12341v1","category":"cs.LG"}
{"created":"2024-04-18 17:07:41","title":"Hot Carrier Organic Solar Cells","abstract":"Hot-carrier solar cells use the photon excess energy, that is, the energy exceeding the absorber bandgap, to do additional work. These devices have the potential to beat the upper limit for the photovoltaic power conversion efficiency set by near-equilibrium thermodynamics. However, since their conceptual inception in 1982, making this concept work under practical conditions has proven a tremendous hurdle, mostly due to the fast thermalization of photo-generated charges in typical semiconductor materials like silicon. Here, we use noise spectroscopy in combination with numerical modelling to show that common bulk heterojunction organic solar cells actually work as hot-carrier devices. Due to static energetic disorder, thermalization of photo-generated electrons and holes in the global density of states is slow compared to the charge carrier lifetime, leading to thermal populations of localized charge carriers that have an electronic temperature exceeding the lattice temperature. Since charge extraction takes place in a high-lying, narrow energy window around the transport energy, the latter takes the role of an energy filter. For common disorder values, this leads to substantial enhancements in open circuit voltage. We expect these results to inspire new strategies to more efficiently convert solar energy into electricity.","sentences":["Hot-carrier solar cells use the photon excess energy, that is, the energy exceeding the absorber bandgap, to do additional work.","These devices have the potential to beat the upper limit for the photovoltaic power conversion efficiency set by near-equilibrium thermodynamics.","However, since their conceptual inception in 1982, making this concept work under practical conditions has proven a tremendous hurdle, mostly due to the fast thermalization of photo-generated charges in typical semiconductor materials like silicon.","Here, we use noise spectroscopy in combination with numerical modelling to show that common bulk heterojunction organic solar cells actually work as hot-carrier devices.","Due to static energetic disorder, thermalization of photo-generated electrons and holes in the global density of states is slow compared to the charge carrier lifetime, leading to thermal populations of localized charge carriers that have an electronic temperature exceeding the lattice temperature.","Since charge extraction takes place in a high-lying, narrow energy window around the transport energy, the latter takes the role of an energy filter.","For common disorder values, this leads to substantial enhancements in open circuit voltage.","We expect these results to inspire new strategies to more efficiently convert solar energy into electricity."],"url":"http://arxiv.org/abs/2404.12338v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 17:05:08","title":"Adiabatic Transformations in Dissipative and Non-Hermitian Phase Transitions","abstract":"The quantum geometric tensor has established itself as a general framework for the analysis and detection of equilibrium phase transitions in isolated quantum systems. We propose a novel generalization of the quantum geometric tensor, which offers a universal approach to studying phase transitions in non-Hermitian quantum systems. Our generalization is based on the concept of the generator of adiabatic transformations and can be applied to systems described by either a Liouvillian superoperator or by an effective non-Hermitian Hamiltonian. We illustrate the proposed method by analyzing the non-Hermitian Su-Schrieffer-Heeger model and a generic quasi-free dissipative fermionic system with a quadratic Liouvillian. Our findings reveal that this method effectively identifies phase transitions across all examined models, providing a universal tool for investigating general non-Hermitian systems.","sentences":["The quantum geometric tensor has established itself as a general framework for the analysis and detection of equilibrium phase transitions in isolated quantum systems.","We propose a novel generalization of the quantum geometric tensor, which offers a universal approach to studying phase transitions in non-Hermitian quantum systems.","Our generalization is based on the concept of the generator of adiabatic transformations and can be applied to systems described by either a Liouvillian superoperator or by an effective non-Hermitian Hamiltonian.","We illustrate the proposed method by analyzing the non-Hermitian Su-Schrieffer-Heeger model and a generic quasi-free dissipative fermionic system with a quadratic Liouvillian.","Our findings reveal that this method effectively identifies phase transitions across all examined models, providing a universal tool for investigating general non-Hermitian systems."],"url":"http://arxiv.org/abs/2404.12337v1","category":"quant-ph"}
{"created":"2024-04-18 17:04:20","title":"Combining Power and Arithmetic Optimization via Datapath Rewriting","abstract":"Industrial datapath designers consider dynamic power consumption to be a key metric. Arithmetic circuits contribute a major component of total chip power consumption and are therefore a common target for power optimization. While arithmetic circuit area and dynamic power consumption are often correlated, there is also a tradeoff to consider, as additional gates can be added to explicitly reduce arithmetic circuit activity and hence reduce power consumption. In this work, we consider two forms of power optimization and their interaction: circuit area reduction via arithmetic optimization, and the elimination of redundant computations using both data and clock gating. By encoding both these classes of optimization as local rewrites of expressions, our tool flow can simultaneously explore them, uncovering new opportunities for power saving through arithmetic rewrites using the e-graph data structure. Since power consumption is highly dependent upon the workload performed by the circuit, our tool flow facilitates a data dependent design paradigm, where an implementation is automatically tailored to particular contexts of data activity. We develop an automated RTL to RTL optimization framework, ROVER, that takes circuit input stimuli and generates power-efficient architectures. We evaluate the effectiveness on both open-source arithmetic benchmarks and benchmarks derived from Intel production examples. The tool is able to reduce the total power consumption by up to 33.9%.","sentences":["Industrial datapath designers consider dynamic power consumption to be a key metric.","Arithmetic circuits contribute a major component of total chip power consumption and are therefore a common target for power optimization.","While arithmetic circuit area and dynamic power consumption are often correlated, there is also a tradeoff to consider, as additional gates can be added to explicitly reduce arithmetic circuit activity and hence reduce power consumption.","In this work, we consider two forms of power optimization and their interaction: circuit area reduction via arithmetic optimization, and the elimination of redundant computations using both data and clock gating.","By encoding both these classes of optimization as local rewrites of expressions, our tool flow can simultaneously explore them, uncovering new opportunities for power saving through arithmetic rewrites using the e-graph data structure.","Since power consumption is highly dependent upon the workload performed by the circuit, our tool flow facilitates a data dependent design paradigm, where an implementation is automatically tailored to particular contexts of data activity.","We develop an automated RTL to RTL optimization framework, ROVER, that takes circuit input stimuli and generates power-efficient architectures.","We evaluate the effectiveness on both open-source arithmetic benchmarks and benchmarks derived from Intel production examples.","The tool is able to reduce the total power consumption by up to 33.9%."],"url":"http://arxiv.org/abs/2404.12336v1","category":"cs.AR"}
{"created":"2024-04-18 16:59:51","title":"Customizing Text-to-Image Diffusion with Camera Viewpoint Control","abstract":"Model customization introduces new concepts to existing text-to-image models, enabling the generation of the new concept in novel contexts. However, such methods lack accurate camera view control w.r.t the object, and users must resort to prompt engineering (e.g., adding \"top-view\") to achieve coarse view control. In this work, we introduce a new task -- enabling explicit control of camera viewpoint for model customization. This allows us to modify object properties amongst various background scenes via text prompts, all while incorporating the target camera pose as additional control. This new task presents significant challenges in merging a 3D representation from the multi-view images of the new concept with a general, 2D text-to-image model. To bridge this gap, we propose to condition the 2D diffusion process on rendered, view-dependent features of the new object. During training, we jointly adapt the 2D diffusion modules and 3D feature predictions to reconstruct the object's appearance and geometry while reducing overfitting to the input multi-view images. Our method outperforms existing image editing and model personalization baselines in preserving the custom object's identity while following the input text prompt and the object's camera pose.","sentences":["Model customization introduces new concepts to existing text-to-image models, enabling the generation of the new concept in novel contexts.","However, such methods lack accurate camera view control w.r.t the object, and users must resort to prompt engineering (e.g., adding \"top-view\") to achieve coarse view control.","In this work, we introduce a new task -- enabling explicit control of camera viewpoint for model customization.","This allows us to modify object properties amongst various background scenes via text prompts, all while incorporating the target camera pose as additional control.","This new task presents significant challenges in merging a 3D representation from the multi-view images of the new concept with a general, 2D text-to-image model.","To bridge this gap, we propose to condition the 2D diffusion process on rendered, view-dependent features of the new object.","During training, we jointly adapt the 2D diffusion modules and 3D feature predictions to reconstruct the object's appearance and geometry while reducing overfitting to the input multi-view images.","Our method outperforms existing image editing and model personalization baselines in preserving the custom object's identity while following the input text prompt and the object's camera pose."],"url":"http://arxiv.org/abs/2404.12333v1","category":"cs.CV"}
{"created":"2024-04-18 16:58:57","title":"Decision making in stochastic extensive form I: Stochastic decision forests","abstract":"A general theory of stochastic decision forests reconciling two concepts of information flow -- decision trees and refined partitions on the one hand, filtrations from probability theory on the other -- is constructed. The traditional \"nature\" agent is replaced with a one-shot lottery draw that determines a tree of a given decision forest, while each \"personal\" agent is equipped with an oracle providing updates on the draw's result and makes partition refining choices adapted to this information. This theory overcomes the incapacity of existing approaches to extensive form theory to capture continuous time stochastic processes like Brownian motion as outcomes of \"nature\" decision making in particular. Moreover, a class of stochastic decision forests based on paths of action indexed by time is constructed, covering a large fraction of models from the literature and constituting a first step towards an approximation theory for stochastic differential games in extensive form.","sentences":["A general theory of stochastic decision forests reconciling two concepts of information flow -- decision trees and refined partitions on the one hand, filtrations from probability theory on the other -- is constructed.","The traditional \"nature\" agent is replaced with a one-shot lottery draw that determines a tree of a given decision forest, while each \"personal\" agent is equipped with an oracle providing updates on the draw's result and makes partition refining choices adapted to this information.","This theory overcomes the incapacity of existing approaches to extensive form theory to capture continuous time stochastic processes like Brownian motion as outcomes of \"nature\" decision making in particular.","Moreover, a class of stochastic decision forests based on paths of action indexed by time is constructed, covering a large fraction of models from the literature and constituting a first step towards an approximation theory for stochastic differential games in extensive form."],"url":"http://arxiv.org/abs/2404.12332v1","category":"econ.TH"}
{"created":"2024-04-18 16:55:12","title":"Gapless non-hydrodynamic modes in relativistic kinetic theory","abstract":"We rigorously prove, for the first time, that the non-hydrodynamic sector is gapless in any relativistic kinetic theory whose scattering cross-section decays to zero at large energies. In fact, if particles with very high energy (compared to the temperature) are free streaming, we can use them to build hot non-hydrodynamic waves, which live longer than any hydrodynamic wave. Since many standard cross-sections in quantum field theory vanish at high energies, the existence of these non-thermal long-lived waves is a rather general feature of relativistic systems.","sentences":["We rigorously prove, for the first time, that the non-hydrodynamic sector is gapless in any relativistic kinetic theory whose scattering cross-section decays to zero at large energies.","In fact, if particles with very high energy (compared to the temperature) are free streaming, we can use them to build hot non-hydrodynamic waves, which live longer than any hydrodynamic wave.","Since many standard cross-sections in quantum field theory vanish at high energies, the existence of these non-thermal long-lived waves is a rather general feature of relativistic systems."],"url":"http://arxiv.org/abs/2404.12327v1","category":"nucl-th"}
{"created":"2024-04-18 16:54:05","title":"Two-loop integrals for $t \\bar{t} +$jet production at hadron colliders in the leading colour approximation","abstract":"We compute the differential equations for the two remaining integral topologies contributing to the leading colour two-loop amplitudes for $pp \\rightarrow t\\bar{t}j$. We derive differential equations for the master integrals by solving the integration-by-parts identities over finite fields. Of the two systems of differential equations, one is presented in canonical '${\\rm d} \\log$' form, while the other is found to have an elliptic sector. For the elliptic topology we identify the relevant elliptic curve, and present the differential equations in a more general form which depends quadratically on $\\epsilon$ and contains non-logarithmic one-forms in addition to the canonical ${\\rm d} \\log$'s. We solve the systems of differential equations numerically using generalised series expansions with the boundary terms obtained using the auxiliary mass flow method. A summary of all one-loop and two-loop planar topologies is presented including the list of alphabet letters for the '${\\rm d} \\log$' form systems and high-precision boundary values.","sentences":["We compute the differential equations for the two remaining integral topologies contributing to the leading colour two-loop amplitudes for $pp \\rightarrow t\\bar{t}j$. We derive differential equations for the master integrals by solving the integration-by-parts identities over finite fields.","Of the two systems of differential equations, one is presented in canonical '${\\rm d} \\log$' form, while the other is found to have an elliptic sector.","For the elliptic topology we identify the relevant elliptic curve, and present the differential equations in a more general form which depends quadratically on $\\epsilon$ and contains non-logarithmic one-forms in addition to the canonical ${\\rm d} \\log$'s.","We solve the systems of differential equations numerically using generalised series expansions with the boundary terms obtained using the auxiliary mass flow method.","A summary of all one-loop and two-loop planar topologies is presented including the list of alphabet letters for the '${\\rm d} \\log$' form systems and high-precision boundary values."],"url":"http://arxiv.org/abs/2404.12325v1","category":"hep-ph"}
{"created":"2024-04-18 16:53:08","title":"Generalizable Face Landmarking Guided by Conditional Face Warping","abstract":"As a significant step for human face modeling, editing, and generation, face landmarking aims at extracting facial keypoints from images. A generalizable face landmarker is required in practice because real-world facial images, e.g., the avatars in animations and games, are often stylized in various ways. However, achieving generalizable face landmarking is challenging due to the diversity of facial styles and the scarcity of labeled stylized faces. In this study, we propose a simple but effective paradigm to learn a generalizable face landmarker based on labeled real human faces and unlabeled stylized faces. Our method learns the face landmarker as the key module of a conditional face warper. Given a pair of real and stylized facial images, the conditional face warper predicts a warping field from the real face to the stylized one, in which the face landmarker predicts the ending points of the warping field and provides us with high-quality pseudo landmarks for the corresponding stylized facial images. Applying an alternating optimization strategy, we learn the face landmarker to minimize $i)$ the discrepancy between the stylized faces and the warped real ones and $ii)$ the prediction errors of both real and pseudo landmarks. Experiments on various datasets show that our method outperforms existing state-of-the-art domain adaptation methods in face landmarking tasks, leading to a face landmarker with better generalizability. Code is available at https://plustwo0.github.io/project-face-landmarker}{https://plustwo0.github.io/project-face-landmarker.","sentences":["As a significant step for human face modeling, editing, and generation, face landmarking aims at extracting facial keypoints from images.","A generalizable face landmarker is required in practice because real-world facial images, e.g., the avatars in animations and games, are often stylized in various ways.","However, achieving generalizable face landmarking is challenging due to the diversity of facial styles and the scarcity of labeled stylized faces.","In this study, we propose a simple but effective paradigm to learn a generalizable face landmarker based on labeled real human faces and unlabeled stylized faces.","Our method learns the face landmarker as the key module of a conditional face warper.","Given a pair of real and stylized facial images, the conditional face warper predicts a warping field from the real face to the stylized one, in which the face landmarker predicts the ending points of the warping field and provides us with high-quality pseudo landmarks for the corresponding stylized facial images.","Applying an alternating optimization strategy, we learn the face landmarker to minimize $i)$ the discrepancy between the stylized faces and the warped real ones and $ii)$ the prediction errors of both real and pseudo landmarks.","Experiments on various datasets show that our method outperforms existing state-of-the-art domain adaptation methods in face landmarking tasks, leading to a face landmarker with better generalizability.","Code is available at https://plustwo0.github.io/project-face-landmarker}{https://plustwo0.github.io/project-face-landmarker."],"url":"http://arxiv.org/abs/2404.12322v1","category":"cs.CV"}
{"created":"2024-04-18 16:52:41","title":"Marginal Analysis of Count Time Series in the Presence of Missing Observations","abstract":"Time series in real-world applications often have missing observations, making typical analytical methods unsuitable. One method for dealing with missing data is the concept of amplitude modulation. While this principle works with any data, here, missing data for unbounded and bounded count time series are investigated, where tailor-made dispersion and skewness statistics are used for model diagnostics. General closed-form asymptotic formulas are derived for such statistics with only weak assumptions on the underlying process. Moreover, closed-form formulas are derived for the popular special cases of Poisson and binomial autoregressive processes, always under the assumption that missingness occurs. The finite-sample performances of the considered asymptotic approximations are analyzed with simulations. The practical application of the corresponding dispersion and skewness tests under missing data is demonstrated with three real-data examples.","sentences":["Time series in real-world applications often have missing observations, making typical analytical methods unsuitable.","One method for dealing with missing data is the concept of amplitude modulation.","While this principle works with any data, here, missing data for unbounded and bounded count time series are investigated, where tailor-made dispersion and skewness statistics are used for model diagnostics.","General closed-form asymptotic formulas are derived for such statistics with only weak assumptions on the underlying process.","Moreover, closed-form formulas are derived for the popular special cases of Poisson and binomial autoregressive processes, always under the assumption that missingness occurs.","The finite-sample performances of the considered asymptotic approximations are analyzed with simulations.","The practical application of the corresponding dispersion and skewness tests under missing data is demonstrated with three real-data examples."],"url":"http://arxiv.org/abs/2404.12319v1","category":"stat.ME"}
{"created":"2024-04-18 16:52:36","title":"Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment","abstract":"Aligning language models (LMs) based on human-annotated preference data is a crucial step in obtaining practical and performant LM-based systems. However, multilingual human preference data are difficult to obtain at scale, making it challenging to extend this framework to diverse languages. In this work, we evaluate a simple approach for zero-shot cross-lingual alignment, where a reward model is trained on preference data in one source language and directly applied to other target languages. On summarization and open-ended dialog generation, we show that this method is consistently successful under comprehensive evaluation settings, including human evaluation: cross-lingually aligned models are preferred by humans over unaligned models on up to >70% of evaluation instances. We moreover find that a different-language reward model sometimes yields better aligned models than a same-language reward model. We also identify best practices when there is no language-specific data for even supervised finetuning, another component in alignment.","sentences":["Aligning language models (LMs) based on human-annotated preference data is a crucial step in obtaining practical and performant LM-based systems.","However, multilingual human preference data are difficult to obtain at scale, making it challenging to extend this framework to diverse languages.","In this work, we evaluate a simple approach for zero-shot cross-lingual alignment, where a reward model is trained on preference data in one source language and directly applied to other target languages.","On summarization and open-ended dialog generation, we show that this method is consistently successful under comprehensive evaluation settings, including human evaluation: cross-lingually aligned models are preferred by humans over unaligned models on up to >70% of evaluation instances.","We moreover find that a different-language reward model sometimes yields better aligned models than a same-language reward model.","We also identify best practices when there is no language-specific data for even supervised finetuning, another component in alignment."],"url":"http://arxiv.org/abs/2404.12318v1","category":"cs.CL"}
{"created":"2024-04-18 16:51:23","title":"Large Language Models for Synthetic Participatory Planning of Shared Automated Electric Mobility Systems","abstract":"Unleashing the synergies of rapidly evolving mobility technologies in a multi-stakeholder landscape presents unique challenges and opportunities for addressing urban transportation problems. This paper introduces a novel synthetic participatory method, critically leveraging large language models (LLMs) to create digital avatars representing diverse stakeholders to plan shared automated electric mobility systems (SAEMS). These calibratable agents collaboratively identify objectives, envision and evaluate SAEMS alternatives, and strategize implementation under risks and constraints. The results of a Montreal case study indicate that a structured and parameterized workflow provides outputs with high controllability and comprehensiveness on an SAEMS plan than generated using a single LLM-enabled expert agent. Consequently, the approach provides a promising avenue for cost-efficiently improving the inclusivity and interpretability of multi-objective transportation planning, suggesting a paradigm shift in how we envision and strategize for sustainable and equitable transportation systems.","sentences":["Unleashing the synergies of rapidly evolving mobility technologies in a multi-stakeholder landscape presents unique challenges and opportunities for addressing urban transportation problems.","This paper introduces a novel synthetic participatory method, critically leveraging large language models (LLMs) to create digital avatars representing diverse stakeholders to plan shared automated electric mobility systems (SAEMS).","These calibratable agents collaboratively identify objectives, envision and evaluate SAEMS alternatives, and strategize implementation under risks and constraints.","The results of a Montreal case study indicate that a structured and parameterized workflow provides outputs with high controllability and comprehensiveness on an SAEMS plan than generated using a single LLM-enabled expert agent.","Consequently, the approach provides a promising avenue for cost-efficiently improving the inclusivity and interpretability of multi-objective transportation planning, suggesting a paradigm shift in how we envision and strategize for sustainable and equitable transportation systems."],"url":"http://arxiv.org/abs/2404.12317v1","category":"cs.CE"}
{"created":"2024-04-18 16:51:15","title":"Neutrinoless double beta decay in the minimal type-I seesaw model: mass-dependent nuclear matrix element, current limits and future sensitivities","abstract":"In this work we discuss the neutrino mass dependent nuclear matrix element (NME) of the neutrinoless double beta decay process and derive the limit on the parameter space of the minimal Type-I seesaw model from the current available experimental data as well as the future sensitivities from the next-generation experiments. Both the explicit many-body calculations and naive extrapolations of the mass dependent NME are employed in the current work. The uncertainties of the theoretical nuclear structure models are taken into account. By combining the latest experimental data from $^{76}$Ge-based experiments, GERDA and MAJORANA, the $^{130}$Te-based experiment, CUORE and the $^{136}$Xe-based experiments, KamLAND-Zen and EXO-200, the bounds on the parameter space of the minimal Type-I seesaw model are obtained and compared with the limits from other experimental probes. Sensitivities for future experiments utilizing $^{76}$Ge-based (LEGEND-1000), $^{82}$Se-based (SuperNEMO), $^{130}$Te based (SNO+II) and $^{136}$Xe-based (nEXO), with a ten-year exposure, are also derived.","sentences":["In this work we discuss the neutrino mass dependent nuclear matrix element (NME) of the neutrinoless double beta decay process and derive the limit on the parameter space of the minimal Type-I seesaw model from the current available experimental data as well as the future sensitivities from the next-generation experiments.","Both the explicit many-body calculations and naive extrapolations of the mass dependent NME are employed in the current work.","The uncertainties of the theoretical nuclear structure models are taken into account.","By combining the latest experimental data from $^{76}$Ge-based experiments, GERDA and MAJORANA, the $^{130}$Te-based experiment, CUORE and the $^{136}$Xe-based experiments, KamLAND-Zen and EXO-200, the bounds on the parameter space of the minimal Type-I seesaw model are obtained and compared with the limits from other experimental probes.","Sensitivities for future experiments utilizing $^{76}$Ge-based (LEGEND-1000), $^{82}$Se-based (SuperNEMO), $^{130}$Te based (SNO+II) and $^{136}$Xe-based (nEXO), with a ten-year exposure, are also derived."],"url":"http://arxiv.org/abs/2404.12316v1","category":"hep-ph"}
{"created":"2024-04-18 16:50:46","title":"Guided Discrete Diffusion for Electronic Health Record Generation","abstract":"Electronic health records (EHRs) are a pivotal data source that enables numerous applications in computational medicine, e.g., disease progression prediction, clinical trial design, and health economics and outcomes research. Despite wide usability, their sensitive nature raises privacy and confidentially concerns, which limit potential use cases. To tackle these challenges, we explore the use of generative models to synthesize artificial, yet realistic EHRs. While diffusion-based methods have recently demonstrated state-of-the-art performance in generating other data modalities and overcome the training instability and mode collapse issues that plague previous GAN-based approaches, their applications in EHR generation remain underexplored. The discrete nature of tabular medical code data in EHRs poses challenges for high-quality data generation, especially for continuous diffusion models. To this end, we introduce a novel tabular EHR generation method, EHR-D3PM, which enables both unconditional and conditional generation using the discrete diffusion model. Our experiments demonstrate that EHR-D3PM significantly outperforms existing generative baselines on comprehensive fidelity and utility metrics while maintaining less membership vulnerability risks. Furthermore, we show EHR-D3PM is effective as a data augmentation method and enhances performance on downstream tasks when combined with real data.","sentences":["Electronic health records (EHRs) are a pivotal data source that enables numerous applications in computational medicine, e.g., disease progression prediction, clinical trial design, and health economics and outcomes research.","Despite wide usability, their sensitive nature raises privacy and confidentially concerns, which limit potential use cases.","To tackle these challenges, we explore the use of generative models to synthesize artificial, yet realistic EHRs.","While diffusion-based methods have recently demonstrated state-of-the-art performance in generating other data modalities and overcome the training instability and mode collapse issues that plague previous GAN-based approaches, their applications in EHR generation remain underexplored.","The discrete nature of tabular medical code data in EHRs poses challenges for high-quality data generation, especially for continuous diffusion models.","To this end, we introduce a novel tabular EHR generation method, EHR-D3PM, which enables both unconditional and conditional generation using the discrete diffusion model.","Our experiments demonstrate that EHR-D3PM significantly outperforms existing generative baselines on comprehensive fidelity and utility metrics while maintaining less membership vulnerability risks.","Furthermore, we show EHR-D3PM is effective as a data augmentation method and enhances performance on downstream tasks when combined with real data."],"url":"http://arxiv.org/abs/2404.12314v1","category":"cs.LG"}
{"created":"2024-04-18 16:47:46","title":"Grothendieck prelopologies: towards a closed monoidal sheaf category","abstract":"In this paper, we present a generalization of Grothendieck pretopologies -- suited for semicartesian categories with equalizers $C$ -- leading to a closed monoidal category of sheaves, instead of closed cartesian category. This is proved through a different sheafification process, which is the left adjoint functor of the suitable inclusion functor but does not preserve all finite limits. If the monoidal structure in $C$ is given by the categorical product, all constructions coincide with those for Grothendieck toposes. The motivation for such generalization stems from a certain notion of sheaves on quantales that does not form a topos.","sentences":["In this paper, we present a generalization of Grothendieck pretopologies -- suited for semicartesian categories with equalizers $C$ -- leading to a closed monoidal category of sheaves, instead of closed cartesian category.","This is proved through a different sheafification process, which is the left adjoint functor of the suitable inclusion functor but does not preserve all finite limits.","If the monoidal structure in $C$ is given by the categorical product, all constructions coincide with those for Grothendieck toposes.","The motivation for such generalization stems from a certain notion of sheaves on quantales that does not form a topos."],"url":"http://arxiv.org/abs/2404.12313v1","category":"math.CT"}
{"created":"2024-04-18 16:46:08","title":"A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to Functional Conditional Moment Equations","abstract":"We study minimax optimization problems defined over infinite-dimensional function classes. In particular, we restrict the functions to the class of overparameterized two-layer neural networks and study (i) the convergence of the gradient descent-ascent algorithm and (ii) the representation learning of the neural network. As an initial step, we consider the minimax optimization problem stemming from estimating a functional equation defined by conditional expectations via adversarial estimation, where the objective function is quadratic in the functional space. For this problem, we establish convergence under the mean-field regime by considering the continuous-time and infinite-width limit of the optimization dynamics. Under this regime, gradient descent-ascent corresponds to a Wasserstein gradient flow over the space of probability measures defined over the space of neural network parameters. We prove that the Wasserstein gradient flow converges globally to a stationary point of the minimax objective at a $\\mathcal{O}(T^{-1} + \\alpha^{-1} ) $ sublinear rate, and additionally finds the solution to the functional equation when the regularizer of the minimax objective is strongly convex. Here $T$ denotes the time and $\\alpha$ is a scaling parameter of the neural network. In terms of representation learning, our results show that the feature representation induced by the neural networks is allowed to deviate from the initial one by the magnitude of $\\mathcal{O}(\\alpha^{-1})$, measured in terms of the Wasserstein distance. Finally, we apply our general results to concrete examples including policy evaluation, nonparametric instrumental variable regression, and asset pricing.","sentences":["We study minimax optimization problems defined over infinite-dimensional function classes.","In particular, we restrict the functions to the class of overparameterized two-layer neural networks and study (i) the convergence of the gradient descent-ascent algorithm and (ii) the representation learning of the neural network.","As an initial step, we consider the minimax optimization problem stemming from estimating a functional equation defined by conditional expectations via adversarial estimation, where the objective function is quadratic in the functional space.","For this problem, we establish convergence under the mean-field regime by considering the continuous-time and infinite-width limit of the optimization dynamics.","Under this regime, gradient descent-ascent corresponds to a Wasserstein gradient flow over the space of probability measures defined over the space of neural network parameters.","We prove that the Wasserstein gradient flow converges globally to a stationary point of the minimax objective at a $\\mathcal{O}(T^{-1} + \\alpha^{-1} ) $ sublinear rate, and additionally finds the solution to the functional equation when the regularizer of the minimax objective is strongly convex.","Here $T$ denotes the time and $\\alpha$ is a scaling parameter of the neural network.","In terms of representation learning, our results show that the feature representation induced by the neural networks is allowed to deviate from the initial one by the magnitude of $\\mathcal{O}(\\alpha^{-1})$, measured in terms of the Wasserstein distance.","Finally, we apply our general results to concrete examples including policy evaluation, nonparametric instrumental variable regression, and asset pricing."],"url":"http://arxiv.org/abs/2404.12312v1","category":"cs.LG"}
{"created":"2024-04-18 16:39:19","title":"A Multiwavelength Survey of Nearby M dwarfs: Optical and Near-Ultraviolet Flares and Activity with Contemporaneous TESS, Kepler/K2, \\textit{Swift}, and HST Observations","abstract":"We present a comprehensive multiwavelength investigation into flares and activity in nearby M~dwarf stars. We leverage the most extensive contemporaneous dataset obtained through the Transiting Exoplanet Sky Survey (TESS), Kepler/K2, the Neil Gehrels Swift Observatory (\\textit{Swift}), and the Hubble Space Telescope (HST), spanning the optical and near-ultraviolet (NUV) regimes. In total, we observed 213 NUV flares on 24 nearby M dwarfs, with $\\sim$27\\% of them having detected optical counterparts, and found that all optical flares had NUV counterparts. We explore NUV/optical energy fractionation in M dwarf flares. Our findings reveal a slight decrease in the ratio of optical to NUV energies with increasing NUV energies, a trend in agreement with prior investigations on G-K stars' flares at higher energies. Our analysis yields an average NUV fraction of flaring time for M0-M3 dwarfs of 2.1\\%, while for M4-M6 dwarfs, it is 5\\%. We present an empirical relationship between NUV and optical flare energies and compare to predictions from radiative-hydrodynamic and blackbody models. We conducted a comparison of the flare frequency distribution (FFDs) of NUV and optical flares, revealing the FFDs of both NUV and optical flares exhibit comparable slopes across all spectral subtypes. NUV flares on stars affect the atmospheric chemistry, the radiation environment, and the overall potential to sustain life on any exoplanets they host. We find that early and mid-M dwarfs (M0-M5) have the potential to generate NUV flares capable of initiating abiogenesis.","sentences":["We present a comprehensive multiwavelength investigation into flares and activity in nearby M~dwarf stars.","We leverage the most extensive contemporaneous dataset obtained through the Transiting Exoplanet Sky Survey (TESS), Kepler/K2, the Neil Gehrels Swift Observatory (\\textit{Swift}), and the Hubble Space Telescope (HST), spanning the optical and near-ultraviolet (NUV) regimes.","In total, we observed 213 NUV flares on 24 nearby M dwarfs, with $\\sim$27\\% of them having detected optical counterparts, and found that all optical flares had NUV counterparts.","We explore NUV/optical energy fractionation in M dwarf flares.","Our findings reveal a slight decrease in the ratio of optical to NUV energies with increasing NUV energies, a trend in agreement with prior investigations on G-K stars' flares at higher energies.","Our analysis yields an average NUV fraction of flaring time for M0-M3 dwarfs of 2.1\\%, while for M4-M6 dwarfs, it is 5\\%.","We present an empirical relationship between NUV and optical flare energies and compare to predictions from radiative-hydrodynamic and blackbody models.","We conducted a comparison of the flare frequency distribution (FFDs) of NUV and optical flares, revealing the FFDs of both NUV and optical flares exhibit comparable slopes across all spectral subtypes.","NUV flares on stars affect the atmospheric chemistry, the radiation environment, and the overall potential to sustain life on any exoplanets they host.","We find that early and mid-M dwarfs (M0-M5) have the potential to generate NUV flares capable of initiating abiogenesis."],"url":"http://arxiv.org/abs/2404.12310v1","category":"astro-ph.SR"}
{"created":"2024-04-18 16:38:02","title":"iRAG: An Incremental Retrieval Augmented Generation System for Videos","abstract":"Retrieval augmented generation (RAG) systems combine the strengths of language generation and information retrieval to power many real-world applications like chatbots. Use of RAG for combined understanding of multimodal data such as text, images and videos is appealing but two critical limitations exist: one-time, upfront capture of all content in large multimodal data as text descriptions entails high processing times, and not all information in the rich multimodal data is typically in the text descriptions. Since the user queries are not known apriori, developing a system for multimodal to text conversion and interactive querying of multimodal data is challenging.   To address these limitations, we propose iRAG, which augments RAG with a novel incremental workflow to enable interactive querying of large corpus of multimodal data. Unlike traditional RAG, iRAG quickly indexes large repositories of multimodal data, and in the incremental workflow, it uses the index to opportunistically extract more details from select portions of the multimodal data to retrieve context relevant to an interactive user query. Such an incremental workflow avoids long multimodal to text conversion times, overcomes information loss issues by doing on-demand query-specific extraction of details in multimodal data, and ensures high quality of responses to interactive user queries that are often not known apriori. To the best of our knowledge, iRAG is the first system to augment RAG with an incremental workflow to support efficient interactive querying of large, real-world multimodal data. Experimental results on real-world long videos demonstrate 23x to 25x faster video to text ingestion, while ensuring that quality of responses to interactive user queries is comparable to responses from a traditional RAG where all video data is converted to text upfront before any querying.","sentences":["Retrieval augmented generation (RAG) systems combine the strengths of language generation and information retrieval to power many real-world applications like chatbots.","Use of RAG for combined understanding of multimodal data such as text, images and videos is appealing but two critical limitations exist: one-time, upfront capture of all content in large multimodal data as text descriptions entails high processing times, and not all information in the rich multimodal data is typically in the text descriptions.","Since the user queries are not known apriori, developing a system for multimodal to text conversion and interactive querying of multimodal data is challenging.   ","To address these limitations, we propose iRAG, which augments RAG with a novel incremental workflow to enable interactive querying of large corpus of multimodal data.","Unlike traditional RAG, iRAG quickly indexes large repositories of multimodal data, and in the incremental workflow, it uses the index to opportunistically extract more details from select portions of the multimodal data to retrieve context relevant to an interactive user query.","Such an incremental workflow avoids long multimodal to text conversion times, overcomes information loss issues by doing on-demand query-specific extraction of details in multimodal data, and ensures high quality of responses to interactive user queries that are often not known apriori.","To the best of our knowledge, iRAG is the first system to augment RAG with an incremental workflow to support efficient interactive querying of large, real-world multimodal data.","Experimental results on real-world long videos demonstrate 23x to 25x faster video to text ingestion, while ensuring that quality of responses to interactive user queries is comparable to responses from a traditional RAG where all video data is converted to text upfront before any querying."],"url":"http://arxiv.org/abs/2404.12309v1","category":"cs.CV"}
{"created":"2024-04-18 16:30:04","title":"Crepant Transformation Conjecture For the Grassmannian Flop","abstract":"We prove an explicit form of the Crepant Transformation Conjecture for Grassmannian flops. Our approach uses abelianization to first relate the restrictions of the Lagrangian cones to degree-2 classes, and then deduces the general result using ``explicit reconstruction'' (also known as the method of big I-functions).","sentences":["We prove an explicit form of the Crepant Transformation Conjecture for Grassmannian flops.","Our approach uses abelianization to first relate the restrictions of the Lagrangian cones to degree-2 classes, and then deduces the general result using ``explicit reconstruction'' (also known as the method of big I-functions)."],"url":"http://arxiv.org/abs/2404.12302v1","category":"math.AG"}
{"created":"2024-04-18 16:26:07","title":"Understanding Fermionic Generalized Symmetries","abstract":"We explore new aspects of internal fermionic shifting symmetries, present in physical systems such as free Dirac spinors and p-form tensor-spinor fields. We propose a novel procedure to gauge these global symmetries, which also introduces a new St\\\"uckelberg mechanism to give a mass to free fermionic fields. Furthermore, we find new magnetic fermionic symmetries in these physical systems whose charged objects are disorder operators. For the case of a Dirac spinor, we discuss an dual description, where the magnetic symmetry acts on the holonomies of a dual 2-form tensor-spinor. Further generalizations such as higher-group-like structures are also discussed.","sentences":["We explore new aspects of internal fermionic shifting symmetries, present in physical systems such as free Dirac spinors and p-form tensor-spinor fields.","We propose a novel procedure to gauge these global symmetries, which also introduces a new St\\\"uckelberg mechanism to give a mass to free fermionic fields.","Furthermore, we find new magnetic fermionic symmetries in these physical systems whose charged objects are disorder operators.","For the case of a Dirac spinor, we discuss an dual description, where the magnetic symmetry acts on the holonomies of a dual 2-form tensor-spinor.","Further generalizations such as higher-group-like structures are also discussed."],"url":"http://arxiv.org/abs/2404.12301v1","category":"hep-th"}
{"created":"2024-04-18 16:24:12","title":"Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair","abstract":"In Simultaneous Machine Translation (SiMT) systems, training with a simultaneous interpretation (SI) corpus is an effective method for achieving high-quality yet low-latency systems. However, it is very challenging to curate such a corpus due to limitations in the abilities of annotators, and hence, existing SI corpora are limited. Therefore, we propose a method to convert existing speech translation corpora into interpretation-style data, maintaining the original word order and preserving the entire source content using Large Language Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in text-to-text and speech-to-text settings with the LLM-SI-Corpus reduces latencies while maintaining the same level of quality as the models trained with offline datasets. The LLM-SI-Corpus is available at \\url{https://github.com/yusuke1997/LLM-SI-Corpus}.","sentences":["In Simultaneous Machine Translation (SiMT) systems, training with a simultaneous interpretation (SI) corpus is an effective method for achieving high-quality yet low-latency systems.","However, it is very challenging to curate such a corpus due to limitations in the abilities of annotators, and hence, existing SI corpora are limited.","Therefore, we propose a method to convert existing speech translation corpora into interpretation-style data, maintaining the original word order and preserving the entire source content using Large Language Models (LLM-SI-Corpus).","We demonstrate that fine-tuning SiMT models in text-to-text and speech-to-text settings with the LLM-SI-Corpus reduces latencies while maintaining the same level of quality as the models trained with offline datasets.","The LLM-SI-Corpus is available at \\url{https://github.com/yusuke1997/LLM-SI-Corpus}."],"url":"http://arxiv.org/abs/2404.12299v1","category":"cs.CL"}
{"created":"2024-04-18 16:16:02","title":"floZ: Evidence estimation from posterior samples with normalizing flows","abstract":"We propose a novel method (floZ), based on normalizing flows, for estimating the Bayesian evidence (and its numerical uncertainty) from a set of samples drawn from the unnormalized posterior distribution. We validate it on distributions whose evidence is known analytically, up to 15 parameter space dimensions, and compare with two state-of-the-art techniques for estimating the evidence: nested sampling (which computes the evidence as its main target) and a k-nearest-neighbors technique that produces evidence estimates from posterior samples. Provided representative samples from the target posterior are available, our method is more robust to posterior distributions with sharp features, especially in higher dimensions. It has wide applicability, e.g., to estimate the evidence from variational inference, Markov-chain Monte Carlo samples, or any other method that delivers samples from the unnormalized posterior density.","sentences":["We propose a novel method (floZ), based on normalizing flows, for estimating the Bayesian evidence (and its numerical uncertainty) from a set of samples drawn from the unnormalized posterior distribution.","We validate it on distributions whose evidence is known analytically, up to 15 parameter space dimensions, and compare with two state-of-the-art techniques for estimating the evidence: nested sampling (which computes the evidence as its main target) and a k-nearest-neighbors technique that produces evidence estimates from posterior samples.","Provided representative samples from the target posterior are available, our method is more robust to posterior distributions with sharp features, especially in higher dimensions.","It has wide applicability, e.g., to estimate the evidence from variational inference, Markov-chain Monte Carlo samples, or any other method that delivers samples from the unnormalized posterior density."],"url":"http://arxiv.org/abs/2404.12294v1","category":"stat.ML"}
{"created":"2024-04-18 16:11:17","title":"Augmenting emotion features in irony detection with Large language modeling","abstract":"This study introduces a novel method for irony detection, applying Large Language Models (LLMs) with prompt-based learning to facilitate emotion-centric text augmentation. Traditional irony detection techniques typically fall short due to their reliance on static linguistic features and predefined knowledge bases, often overlooking the nuanced emotional dimensions integral to irony. In contrast, our methodology augments the detection process by integrating subtle emotional cues, augmented through LLMs, into three benchmark pre-trained NLP models - BERT, T5, and GPT-2 - which are widely recognized as foundational in irony detection. We assessed our method using the SemEval-2018 Task 3 dataset and observed substantial enhancements in irony detection capabilities.","sentences":["This study introduces a novel method for irony detection, applying Large Language Models (LLMs) with prompt-based learning to facilitate emotion-centric text augmentation.","Traditional irony detection techniques typically fall short due to their reliance on static linguistic features and predefined knowledge bases, often overlooking the nuanced emotional dimensions integral to irony.","In contrast, our methodology augments the detection process by integrating subtle emotional cues, augmented through LLMs, into three benchmark pre-trained NLP models - BERT, T5, and GPT-2 - which are widely recognized as foundational in irony detection.","We assessed our method using the SemEval-2018 Task 3 dataset and observed substantial enhancements in irony detection capabilities."],"url":"http://arxiv.org/abs/2404.12291v1","category":"cs.CL"}
{"created":"2024-04-18 16:10:38","title":"Resilience through Scene Context in Visual Referring Expression Generation","abstract":"Scene context is well known to facilitate humans' perception of visible objects. In this paper, we investigate the role of context in Referring Expression Generation (REG) for objects in images, where existing research has often focused on distractor contexts that exert pressure on the generator. We take a new perspective on scene context in REG and hypothesize that contextual information can be conceived of as a resource that makes REG models more resilient and facilitates the generation of object descriptions, and object types in particular. We train and test Transformer-based REG models with target representations that have been artificially obscured with noise to varying degrees. We evaluate how properties of the models' visual context affect their processing and performance. Our results show that even simple scene contexts make models surprisingly resilient to perturbations, to the extent that they can identify referent types even when visual information about the target is completely missing.","sentences":["Scene context is well known to facilitate humans' perception of visible objects.","In this paper, we investigate the role of context in Referring Expression Generation (REG) for objects in images, where existing research has often focused on distractor contexts that exert pressure on the generator.","We take a new perspective on scene context in REG and hypothesize that contextual information can be conceived of as a resource that makes REG models more resilient and facilitates the generation of object descriptions, and object types in particular.","We train and test Transformer-based REG models with target representations that have been artificially obscured with noise to varying degrees.","We evaluate how properties of the models' visual context affect their processing and performance.","Our results show that even simple scene contexts make models surprisingly resilient to perturbations, to the extent that they can identify referent types even when visual information about the target is completely missing."],"url":"http://arxiv.org/abs/2404.12289v1","category":"cs.CL"}
{"created":"2024-04-18 16:07:52","title":"Lifting maps between graphs to embeddings","abstract":"In this paper, we study conditions for the existence of an embedding $\\widetilde{f} \\colon P \\to Q \\times \\mathbb{R}$ such that $f = \\mathrm{pr}_Q \\circ \\widetilde{f}$, where $f \\colon P \\to Q$ is a piecewise linear map between polyhedra. Our focus is on non-degenerate maps between graphs, where non-degeneracy means that the preimages of points are finite sets.   We introduce combinatorial techniques and establish necessary and sufficient conditions for the general case. Using these results, we demonstrate that the problem of the existence of a lifting reduces to testing the satisfiability of a 3-CNF formula. Additionally, we construct a counterexample to a result by V. Po\\'{e}naru on lifting of smooth immersions to embeddings.   Furthermore, by establishing connections between the stated problem and the approximability by embeddings, we deduce that, in the case of generic maps from a tree to a segment, a weaker condition becomes sufficient for the existence of a lifting.","sentences":["In this paper, we study conditions for the existence of an embedding $\\widetilde{f} \\colon P \\to Q \\times \\mathbb{R}$ such that $f = \\mathrm{pr}_Q \\circ \\widetilde{f}$, where $f \\colon P \\to Q$ is a piecewise linear map between polyhedra.","Our focus is on non-degenerate maps between graphs, where non-degeneracy means that the preimages of points are finite sets.   ","We introduce combinatorial techniques and establish necessary and sufficient conditions for the general case.","Using these results, we demonstrate that the problem of the existence of a lifting reduces to testing the satisfiability of a 3-CNF formula.","Additionally, we construct a counterexample to a result by V. Po\\'{e}naru on lifting of smooth immersions to embeddings.   ","Furthermore, by establishing connections between the stated problem and the approximability by embeddings, we deduce that, in the case of generic maps from a tree to a segment, a weaker condition becomes sufficient for the existence of a lifting."],"url":"http://arxiv.org/abs/2404.12287v1","category":"math.GT"}
{"created":"2024-04-18 16:04:14","title":"Performance Evaluation of Segment Anything Model with Variational Prompting for Application to Non-Visible Spectrum Imagery","abstract":"The Segment Anything Model (SAM) is a deep neural network foundational model designed to perform instance segmentation which has gained significant popularity given its zero-shot segmentation ability. SAM operates by generating masks based on various input prompts such as text, bounding boxes, points, or masks, introducing a novel methodology to overcome the constraints posed by dataset-specific scarcity. While SAM is trained on an extensive dataset, comprising ~11M images, it mostly consists of natural photographic images with only very limited images from other modalities. Whilst the rapid progress in visual infrared surveillance and X-ray security screening imaging technologies, driven forward by advances in deep learning, has significantly enhanced the ability to detect, classify and segment objects with high accuracy, it is not evident if the SAM zero-shot capabilities can be transferred to such modalities. This work assesses SAM capabilities in segmenting objects of interest in the X-ray/infrared modalities. Our approach reuses the pre-trained SAM with three different prompts: bounding box, centroid and random points. We present quantitative/qualitative results to showcase the performance on selected datasets. Our results show that SAM can segment objects in the X-ray modality when given a box prompt, but its performance varies for point prompts. Specifically, SAM performs poorly in segmenting slender objects and organic materials, such as plastic bottles. We find that infrared objects are also challenging to segment with point prompts given the low-contrast nature of this modality. This study shows that while SAM demonstrates outstanding zero-shot capabilities with box prompts, its performance ranges from moderate to poor for point prompts, indicating that special consideration on the cross-modal generalisation of SAM is needed when considering use on X-ray/infrared imagery.","sentences":["The Segment Anything Model (SAM) is a deep neural network foundational model designed to perform instance segmentation which has gained significant popularity given its zero-shot segmentation ability.","SAM operates by generating masks based on various input prompts such as text, bounding boxes, points, or masks, introducing a novel methodology to overcome the constraints posed by dataset-specific scarcity.","While SAM is trained on an extensive dataset, comprising ~11M images, it mostly consists of natural photographic images with only very limited images from other modalities.","Whilst the rapid progress in visual infrared surveillance and X-ray security screening imaging technologies, driven forward by advances in deep learning, has significantly enhanced the ability to detect, classify and segment objects with high accuracy, it is not evident if the SAM zero-shot capabilities can be transferred to such modalities.","This work assesses SAM capabilities in segmenting objects of interest in the X-ray/infrared modalities.","Our approach reuses the pre-trained SAM with three different prompts: bounding box, centroid and random points.","We present quantitative/qualitative results to showcase the performance on selected datasets.","Our results show that SAM can segment objects in the X-ray modality when given a box prompt, but its performance varies for point prompts.","Specifically, SAM performs poorly in segmenting slender objects and organic materials, such as plastic bottles.","We find that infrared objects are also challenging to segment with point prompts given the low-contrast nature of this modality.","This study shows that while SAM demonstrates outstanding zero-shot capabilities with box prompts, its performance ranges from moderate to poor for point prompts, indicating that special consideration on the cross-modal generalisation of SAM is needed when considering use on X-ray/infrared imagery."],"url":"http://arxiv.org/abs/2404.12285v1","category":"cs.CV"}
{"created":"2024-04-18 16:02:02","title":"A hybrid boundary integral-PDE approach for the approximation of the demagnetization potential in micromagnetics","abstract":"The demagnetization field in micromagnetism is given as the gradient of a potential which solves a partial differential equation (PDE) posed in R^d. In its most general form, this PDE is supplied with continuity condition on the boundary of the magnetic domain and the equation includes a discontinuity in the gradient of the potential over the boundary. Typical numerical algorithms to solve this problem relies on the representation of the potential via the Green's function, where a volume and a boundary integral terms need to be accurately approximated. From a computational point of view, the volume integral dominates the computational cost and can be difficult to approximate due to the singularities of the Green's function. In this article, we propose a hybrid model, where the overall potential can be approximated by solving two uncoupled PDEs posed in bounded domains, whereby the boundary conditions of one of the PDEs is obtained by a low cost boundary integral. Moreover, we provide a convergence analysis of the method under two separate theoretical settings; periodic magnetisation, and high-frequency magnetisation. Numerical examples are given to verify the convergence rates.","sentences":["The demagnetization field in micromagnetism is given as the gradient of a potential which solves a partial differential equation (PDE) posed in R^d.","In its most general form, this PDE is supplied with continuity condition on the boundary of the magnetic domain and the equation includes a discontinuity in the gradient of the potential over the boundary.","Typical numerical algorithms to solve this problem relies on the representation of the potential via the Green's function, where a volume and a boundary integral terms need to be accurately approximated.","From a computational point of view, the volume integral dominates the computational cost and can be difficult to approximate due to the singularities of the Green's function.","In this article, we propose a hybrid model, where the overall potential can be approximated by solving two uncoupled PDEs posed in bounded domains, whereby the boundary conditions of one of the PDEs is obtained by a low cost boundary integral.","Moreover, we provide a convergence analysis of the method under two separate theoretical settings; periodic magnetisation, and high-frequency magnetisation.","Numerical examples are given to verify the convergence rates."],"url":"http://arxiv.org/abs/2404.12284v1","category":"math.NA"}
{"created":"2024-04-18 15:57:19","title":"RISE: 3D Perception Makes Real-World Robot Imitation Simple and Effective","abstract":"Precise robot manipulations require rich spatial information in imitation learning. Image-based policies model object positions from fixed cameras, which are sensitive to camera view changes. Policies utilizing 3D point clouds usually predict keyframes rather than continuous actions, posing difficulty in dynamic and contact-rich scenarios. To utilize 3D perception efficiently, we present RISE, an end-to-end baseline for real-world imitation learning, which predicts continuous actions directly from single-view point clouds. It compresses the point cloud to tokens with a sparse 3D encoder. After adding sparse positional encoding, the tokens are featurized using a transformer. Finally, the features are decoded into robot actions by a diffusion head. Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency. Experiments also demonstrate that RISE is more general and robust to environmental change compared with previous baselines. Project website: rise-policy.github.io.","sentences":["Precise robot manipulations require rich spatial information in imitation learning.","Image-based policies model object positions from fixed cameras, which are sensitive to camera view changes.","Policies utilizing 3D point clouds usually predict keyframes rather than continuous actions, posing difficulty in dynamic and contact-rich scenarios.","To utilize 3D perception efficiently, we present RISE, an end-to-end baseline for real-world imitation learning, which predicts continuous actions directly from single-view point clouds.","It compresses the point cloud to tokens with a sparse 3D encoder.","After adding sparse positional encoding, the tokens are featurized using a transformer.","Finally, the features are decoded into robot actions by a diffusion head.","Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency.","Experiments also demonstrate that RISE is more general and robust to environmental change compared with previous baselines.","Project website: rise-policy.github.io."],"url":"http://arxiv.org/abs/2404.12281v1","category":"cs.RO"}
{"created":"2024-04-18 15:52:42","title":"DF-DM: A foundational process model for multimodal data fusion in the artificial intelligence era","abstract":"In the big data era, integrating diverse data modalities poses significant challenges, particularly in complex fields like healthcare. This paper introduces a new process model for multimodal Data Fusion for Data Mining, integrating embeddings and the Cross-Industry Standard Process for Data Mining with the existing Data Fusion Information Group model. Our model aims to decrease computational costs, complexity, and bias while improving efficiency and reliability. We also propose \"disentangled dense fusion\", a novel embedding fusion method designed to optimize mutual information and facilitate dense inter-modality feature interaction, thereby minimizing redundant information.   We demonstrate the model's efficacy through three use cases: predicting diabetic retinopathy using retinal images and patient metadata, domestic violence prediction employing satellite imagery, internet, and census data, and identifying clinical and demographic features from radiography images and clinical notes. The model achieved a Macro F1 score of 0.92 in diabetic retinopathy prediction, an R-squared of 0.854 and sMAPE of 24.868 in domestic violence prediction, and a macro AUC of 0.92 and 0.99 for disease prediction and sex classification, respectively, in radiological analysis.   These results underscore the Data Fusion for Data Mining model's potential to significantly impact multimodal data processing, promoting its adoption in diverse, resource-constrained settings.","sentences":["In the big data era, integrating diverse data modalities poses significant challenges, particularly in complex fields like healthcare.","This paper introduces a new process model for multimodal Data Fusion for Data Mining, integrating embeddings and the Cross-Industry Standard Process for Data Mining with the existing Data Fusion Information Group model.","Our model aims to decrease computational costs, complexity, and bias while improving efficiency and reliability.","We also propose \"disentangled dense fusion\", a novel embedding fusion method designed to optimize mutual information and facilitate dense inter-modality feature interaction, thereby minimizing redundant information.   ","We demonstrate the model's efficacy through three use cases: predicting diabetic retinopathy using retinal images and patient metadata, domestic violence prediction employing satellite imagery, internet, and census data, and identifying clinical and demographic features from radiography images and clinical notes.","The model achieved a Macro F1 score of 0.92 in diabetic retinopathy prediction, an R-squared of 0.854 and sMAPE of 24.868 in domestic violence prediction, and a macro AUC of 0.92 and 0.99 for disease prediction and sex classification, respectively, in radiological analysis.   ","These results underscore the Data Fusion for Data Mining model's potential to significantly impact multimodal data processing, promoting its adoption in diverse, resource-constrained settings."],"url":"http://arxiv.org/abs/2404.12278v1","category":"cs.AI"}
{"created":"2024-04-18 15:47:00","title":"Advancing the Robustness of Large Language Models through Self-Denoised Smoothing","abstract":"Although large language models (LLMs) have achieved significant success, their vulnerability to adversarial perturbations, including recent jailbreak attacks, has raised considerable concerns. However, the increasing size of these models and their limited access make improving their robustness a challenging task. Among various defense strategies, randomized smoothing has shown great potential for LLMs, as it does not require full access to the model's parameters or fine-tuning via adversarial training. However, randomized smoothing involves adding noise to the input before model prediction, and the final model's robustness largely depends on the model's performance on these noise corrupted data. Its effectiveness is often limited by the model's sub-optimal performance on noisy data. To address this issue, we propose to leverage the multitasking nature of LLMs to first denoise the noisy inputs and then to make predictions based on these denoised versions. We call this procedure self-denoised smoothing. Unlike previous denoised smoothing techniques in computer vision, which require training a separate model to enhance the robustness of LLMs, our method offers significantly better efficiency and flexibility. Our experimental results indicate that our method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments (i.e., jailbreak attacks). Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise","sentences":["Although large language models (LLMs) have achieved significant success, their vulnerability to adversarial perturbations, including recent jailbreak attacks, has raised considerable concerns.","However, the increasing size of these models and their limited access make improving their robustness a challenging task.","Among various defense strategies, randomized smoothing has shown great potential for LLMs, as it does not require full access to the model's parameters or fine-tuning via adversarial training.","However, randomized smoothing involves adding noise to the input before model prediction, and the final model's robustness largely depends on the model's performance on these noise corrupted data.","Its effectiveness is often limited by the model's sub-optimal performance on noisy data.","To address this issue, we propose to leverage the multitasking nature of LLMs to first denoise the noisy inputs and then to make predictions based on these denoised versions.","We call this procedure self-denoised smoothing.","Unlike previous denoised smoothing techniques in computer vision, which require training a separate model to enhance the robustness of LLMs, our method offers significantly better efficiency and flexibility.","Our experimental results indicate that our method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments (i.e., jailbreak attacks).","Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise"],"url":"http://arxiv.org/abs/2404.12274v1","category":"cs.CL"}
{"created":"2024-04-18 15:46:26","title":"FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom","abstract":"Federated Learning (FL) has emerged as a promising solution for collaborative training of large language models (LLMs). However, the integration of LLMs into FL introduces new challenges, particularly concerning the evaluation of LLMs. Traditional evaluation methods that rely on labeled test sets and similarity-based metrics cover only a subset of the acceptable answers, thereby failing to accurately reflect the performance of LLMs on generative tasks. Meanwhile, although automatic evaluation methods that leverage advanced LLMs present potential, they face critical risks of data leakage due to the need to transmit data to external servers and suboptimal performance on downstream tasks due to the lack of domain knowledge. To address these issues, we propose a Federated Evaluation framework of Large Language Models, named FedEval-LLM, that provides reliable performance measurements of LLMs on downstream tasks without the reliance on labeled test sets and external tools, thus ensuring strong privacy-preserving capability. FedEval-LLM leverages a consortium of personalized LLMs from participants as referees to provide domain knowledge and collective evaluation capability, thus aligning to the respective downstream tasks and mitigating uncertainties and biases associated with a single referee. Experimental results demonstrate a significant improvement in the evaluation capability of personalized evaluation models on downstream tasks. When applied to FL, these evaluation models exhibit strong agreement with human preference and RougeL-score on meticulously curated test sets. FedEval-LLM effectively overcomes the limitations of traditional metrics and the reliance on external services, making it a promising framework for the evaluation of LLMs within collaborative training scenarios.","sentences":["Federated Learning (FL) has emerged as a promising solution for collaborative training of large language models (LLMs).","However, the integration of LLMs into FL introduces new challenges, particularly concerning the evaluation of LLMs.","Traditional evaluation methods that rely on labeled test sets and similarity-based metrics cover only a subset of the acceptable answers, thereby failing to accurately reflect the performance of LLMs on generative tasks.","Meanwhile, although automatic evaluation methods that leverage advanced LLMs present potential, they face critical risks of data leakage due to the need to transmit data to external servers and suboptimal performance on downstream tasks due to the lack of domain knowledge.","To address these issues, we propose a Federated Evaluation framework of Large Language Models, named FedEval-LLM, that provides reliable performance measurements of LLMs on downstream tasks without the reliance on labeled test sets and external tools, thus ensuring strong privacy-preserving capability.","FedEval-LLM leverages a consortium of personalized LLMs from participants as referees to provide domain knowledge and collective evaluation capability, thus aligning to the respective downstream tasks and mitigating uncertainties and biases associated with a single referee.","Experimental results demonstrate a significant improvement in the evaluation capability of personalized evaluation models on downstream tasks.","When applied to FL, these evaluation models exhibit strong agreement with human preference and RougeL-score on meticulously curated test sets.","FedEval-LLM effectively overcomes the limitations of traditional metrics and the reliance on external services, making it a promising framework for the evaluation of LLMs within collaborative training scenarios."],"url":"http://arxiv.org/abs/2404.12273v1","category":"cs.AI"}
{"created":"2024-04-18 15:45:27","title":"Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences","abstract":"Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.","sentences":["Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs.","Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation.","We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements.","Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions.","While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades.","A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment.","In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria.","What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs.","We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants."],"url":"http://arxiv.org/abs/2404.12272v1","category":"cs.HC"}
{"created":"2024-04-18 15:44:03","title":"Emergent Anti-Ferroelectric Ordering and the Coupling of Liquid Crystalline and Polar Order","abstract":"Polar liquid crystals possess three dimensional orientational order coupled with unidirectional electric polarity, yielding fluid ferroelectrics. Such polar phases are generated by rod-like molecules with large electric dipole moments. 2,5-Disubstituted 1,3-dioxane is commonly employed as a polar motif in said systems, and here we show this to suffer from thermal instability as a consequence of equatorial-trans to axial-trans isomerism at elevated temperatures. We utilise isosteric building blocks as potential replacements for the 1,3- dioxane unit, and in doing so we obtain new examples of fluid ferroelectric systems. For binary mixtures of certain composition, we observe the emergence of a new fluid antiferroelectric phase - a finding not observed for either of the parent molecules. Our study also reveals a critical tipping point for the emergence of polar order in otherwise apolar systems. These results hint at the possibility for uncovering new highly ordered polar LC phases and delineate distinct transition mechanisms in orientational and polar ordering.","sentences":["Polar liquid crystals possess three dimensional orientational order coupled with unidirectional electric polarity, yielding fluid ferroelectrics.","Such polar phases are generated by rod-like molecules with large electric dipole moments.","2,5-Disubstituted 1,3-dioxane is commonly employed as a polar motif in said systems, and here we show this to suffer from thermal instability as a consequence of equatorial-trans to axial-trans isomerism at elevated temperatures.","We utilise isosteric building blocks as potential replacements for the 1,3- dioxane unit, and in doing so we obtain new examples of fluid ferroelectric systems.","For binary mixtures of certain composition, we observe the emergence of a new fluid antiferroelectric phase - a finding not observed for either of the parent molecules.","Our study also reveals a critical tipping point for the emergence of polar order in otherwise apolar systems.","These results hint at the possibility for uncovering new highly ordered polar LC phases and delineate distinct transition mechanisms in orientational and polar ordering."],"url":"http://arxiv.org/abs/2404.12271v1","category":"cond-mat.soft"}
{"created":"2024-04-18 15:41:27","title":"How Population Diversity Influences the Efficiency of Crossover","abstract":"Our theoretical understanding of crossover is limited by our ability to analyze how population diversity evolves. In this study, we provide one of the first rigorous analyses of population diversity and optimization time in a setting where large diversity and large population sizes are required to speed up progress. We give a formal and general criterion which amount of diversity is necessary and sufficient to speed up the $(\\mu+1)$ Genetic Algorithm on LeadingOnes. We show that the naturally evolving diversity falls short of giving a substantial speed-up for any $\\mu=O(\\sqrt{n}/\\log^2 n)$. On the other hand, we show that even for $\\mu=2$, if we simply break ties in favor of diversity then this increases diversity so much that optimization is accelerated by a constant factor.","sentences":["Our theoretical understanding of crossover is limited by our ability to analyze how population diversity evolves.","In this study, we provide one of the first rigorous analyses of population diversity and optimization time in a setting where large diversity and large population sizes are required to speed up progress.","We give a formal and general criterion which amount of diversity is necessary and sufficient to speed up the $(\\mu+1)$ Genetic Algorithm on LeadingOnes.","We show that the naturally evolving diversity falls short of giving a substantial speed-up for any $\\mu=O(\\sqrt{n}/\\log^2 n)$.","On the other hand, we show that even for $\\mu=2$, if we simply break ties in favor of diversity then this increases diversity so much that optimization is accelerated by a constant factor."],"url":"http://arxiv.org/abs/2404.12268v1","category":"cs.NE"}
{"created":"2024-04-18 15:38:14","title":"Physics-integrated generative modeling using attentive planar normalizing flow based variational autoencoder","abstract":"Physics-integrated generative modeling is a class of hybrid or grey-box modeling in which we augment the the data-driven model with the physics knowledge governing the data distribution. The use of physics knowledge allows the generative model to produce output in a controlled way, so that the output, by construction, complies with the physical laws. It imparts improved generalization ability to extrapolate beyond the training distribution as well as improved interpretability because the model is partly grounded in firm domain knowledge. In this work, we aim to improve the fidelity of reconstruction and robustness to noise in the physics integrated generative model. To this end, we use variational-autoencoder as a generative model. To improve the reconstruction results of the decoder, we propose to learn the latent posterior distribution of both the physics as well as the trainable data-driven components using planar normalizng flow. Normalizng flow based posterior distribution harnesses the inherent dynamical structure of the data distribution, hence the learned model gets closer to the true underlying data distribution. To improve the robustness of generative model against noise injected in the model, we propose a modification in the encoder part of the normalizing flow based VAE. We designed the encoder to incorporate scaled dot product attention based contextual information in the noisy latent vector which will mitigate the adverse effect of noise in the latent vector and make the model more robust. We empirically evaluated our models on human locomotion dataset [33] and the results validate the efficacy of our proposed models in terms of improvement in reconstruction quality as well as robustness against noise injected in the model.","sentences":["Physics-integrated generative modeling is a class of hybrid or grey-box modeling in which we augment the the data-driven model with the physics knowledge governing the data distribution.","The use of physics knowledge allows the generative model to produce output in a controlled way, so that the output, by construction, complies with the physical laws.","It imparts improved generalization ability to extrapolate beyond the training distribution as well as improved interpretability because the model is partly grounded in firm domain knowledge.","In this work, we aim to improve the fidelity of reconstruction and robustness to noise in the physics integrated generative model.","To this end, we use variational-autoencoder as a generative model.","To improve the reconstruction results of the decoder, we propose to learn the latent posterior distribution of both the physics as well as the trainable data-driven components using planar normalizng flow.","Normalizng flow based posterior distribution harnesses the inherent dynamical structure of the data distribution, hence the learned model gets closer to the true underlying data distribution.","To improve the robustness of generative model against noise injected in the model, we propose a modification in the encoder part of the normalizing flow based VAE.","We designed the encoder to incorporate scaled dot product attention based contextual information in the noisy latent vector which will mitigate the adverse effect of noise in the latent vector and make the model more robust.","We empirically evaluated our models on human locomotion dataset [33] and the results validate the efficacy of our proposed models in terms of improvement in reconstruction quality as well as robustness against noise injected in the model."],"url":"http://arxiv.org/abs/2404.12267v1","category":"cs.LG"}
{"created":"2024-04-18 15:36:26","title":"A higher-dimensional version of F\u00e1ry's theorem","abstract":"We prove a generalization of Istvan F\\'ary's celebrated theorem to higher dimension.","sentences":["We prove a generalization of Istvan F\\'ary's celebrated theorem to higher dimension."],"url":"http://arxiv.org/abs/2404.12265v1","category":"math.GT"}
{"created":"2024-04-18 15:36:02","title":"On the problem of optimal fluid transport in capillaries","abstract":"In this note, we revisit the problem of the pressure-driven transport of a meniscus through a narrow cylindrical capillary or pore. This generic process finds many applications in science and technology. As it is known that Direct Numerical Simulations of moving contact line problems are highly demanding in terms of computational costs, simplified models in the form of ordinary differential equations offer an interesting alternative to perform a mathematical optimization of the flow. Blake and De Coninck studied the pressure-driven transport of a meniscus and identified two major competing mechanisms. While a hydrophilic surface is favorable to enhance the spontaneous imbibition into the pore, the friction is known to be significantly reduced on a hydrophobic surface. Blake and De Coninck showed that, depending on the applied pressure difference, there exists an optimal wettability that minimizes the time required to move the meniscus over a certain distance. We revisit this problem and derive analytical solutions in the limiting cases of negligible inertia and negligible contact line friction.","sentences":["In this note, we revisit the problem of the pressure-driven transport of a meniscus through a narrow cylindrical capillary or pore.","This generic process finds many applications in science and technology.","As it is known that Direct Numerical Simulations of moving contact line problems are highly demanding in terms of computational costs, simplified models in the form of ordinary differential equations offer an interesting alternative to perform a mathematical optimization of the flow.","Blake and De Coninck studied the pressure-driven transport of a meniscus and identified two major competing mechanisms.","While a hydrophilic surface is favorable to enhance the spontaneous imbibition into the pore, the friction is known to be significantly reduced on a hydrophobic surface.","Blake and De Coninck showed that, depending on the applied pressure difference, there exists an optimal wettability that minimizes the time required to move the meniscus over a certain distance.","We revisit this problem and derive analytical solutions in the limiting cases of negligible inertia and negligible contact line friction."],"url":"http://arxiv.org/abs/2404.12263v1","category":"physics.flu-dyn"}
{"created":"2024-04-18 15:32:35","title":"Tree-Based Nonlinear Reduced Modeling","abstract":"This paper is concerned with model order reduction of parametric Partial Differential Equations (PDEs) using tree-based library approximations. Classical approaches are formulated for PDEs on Hilbert spaces and involve one single linear space to approximate the set of PDE solutions. Here, we develop reduced models relying on a collection of linear or nonlinear approximation spaces called a library, and which can also be formulated on general metric spaces. To build the spaces of the library, we rely on greedy algorithms involving different splitting strategies which lead to a hierarchical tree-based representation. We illustrate through numerical examples that the proposed strategies have a much wider range of applicability in terms of the parametric PDEs that can successfully be addressed. While the classical approach is very efficient for elliptic problems with strong coercivity, we show that the tree-based library approaches can deal with diffusion problems with weak coercivity, convection-diffusion problems, and with transport-dominated PDEs posed on general metric spaces such as the $L^2$-Wasserstein space.","sentences":["This paper is concerned with model order reduction of parametric Partial Differential Equations (PDEs) using tree-based library approximations.","Classical approaches are formulated for PDEs on Hilbert spaces and involve one single linear space to approximate the set of PDE solutions.","Here, we develop reduced models relying on a collection of linear or nonlinear approximation spaces called a library, and which can also be formulated on general metric spaces.","To build the spaces of the library, we rely on greedy algorithms involving different splitting strategies which lead to a hierarchical tree-based representation.","We illustrate through numerical examples that the proposed strategies have a much wider range of applicability in terms of the parametric PDEs that can successfully be addressed.","While the classical approach is very efficient for elliptic problems with strong coercivity, we show that the tree-based library approaches can deal with diffusion problems with weak coercivity, convection-diffusion problems, and with transport-dominated PDEs posed on general metric spaces such as the $L^2$-Wasserstein space."],"url":"http://arxiv.org/abs/2404.12262v1","category":"math.NA"}
{"created":"2024-04-18 15:28:34","title":"Alleviating Catastrophic Forgetting in Facial Expression Recognition with Emotion-Centered Models","abstract":"Facial expression recognition is a pivotal component in machine learning, facilitating various applications. However, convolutional neural networks (CNNs) are often plagued by catastrophic forgetting, impeding their adaptability. The proposed method, emotion-centered generative replay (ECgr), tackles this challenge by integrating synthetic images from generative adversarial networks. Moreover, ECgr incorporates a quality assurance algorithm to ensure the fidelity of generated images. This dual approach enables CNNs to retain past knowledge while learning new tasks, enhancing their performance in emotion recognition. The experimental results on four diverse facial expression datasets demonstrate that incorporating images generated by our pseudo-rehearsal method enhances training on the targeted dataset and the source dataset while making the CNN retain previously learned knowledge.","sentences":["Facial expression recognition is a pivotal component in machine learning, facilitating various applications.","However, convolutional neural networks (CNNs) are often plagued by catastrophic forgetting, impeding their adaptability.","The proposed method, emotion-centered generative replay (ECgr), tackles this challenge by integrating synthetic images from generative adversarial networks.","Moreover, ECgr incorporates a quality assurance algorithm to ensure the fidelity of generated images.","This dual approach enables CNNs to retain past knowledge while learning new tasks, enhancing their performance in emotion recognition.","The experimental results on four diverse facial expression datasets demonstrate that incorporating images generated by our pseudo-rehearsal method enhances training on the targeted dataset and the source dataset while making the CNN retain previously learned knowledge."],"url":"http://arxiv.org/abs/2404.12260v1","category":"cs.CV"}
{"created":"2024-04-18 15:26:02","title":"Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM","abstract":"Data analysts have long sought to turn unstructured text data into meaningful concepts. Though common, topic modeling and clustering focus on lower-level keywords and require significant interpretative work. We introduce concept induction, a computational process that instead produces high-level concepts, defined by explicit inclusion criteria, from unstructured text. For a dataset of toxic online comments, where a state-of-the-art BERTopic model outputs \"women, power, female,\" concept induction produces high-level concepts such as \"Criticism of traditional gender roles\" and \"Dismissal of women's concerns.\" We present LLooM, a concept induction algorithm that leverages large language models to iteratively synthesize sampled text and propose human-interpretable concepts of increasing generality. We then instantiate LLooM in a mixed-initiative text analysis tool, enabling analysts to shift their attention from interpreting topics to engaging in theory-driven analysis. Through technical evaluations and four analysis scenarios ranging from literature review to content moderation, we find that LLooM's concepts improve upon the prior art of topic models in terms of quality and data coverage. In expert case studies, LLooM helped researchers to uncover new insights even from familiar datasets, for example by suggesting a previously unnoticed concept of attacks on out-party stances in a political social media dataset.","sentences":["Data analysts have long sought to turn unstructured text data into meaningful concepts.","Though common, topic modeling and clustering focus on lower-level keywords and require significant interpretative work.","We introduce concept induction, a computational process that instead produces high-level concepts, defined by explicit inclusion criteria, from unstructured text.","For a dataset of toxic online comments, where a state-of-the-art BERTopic model outputs \"women, power, female,\" concept induction produces high-level concepts such as \"Criticism of traditional gender roles\" and \"Dismissal of women's concerns.\"","We present LLooM, a concept induction algorithm that leverages large language models to iteratively synthesize sampled text and propose human-interpretable concepts of increasing generality.","We then instantiate LLooM in a mixed-initiative text analysis tool, enabling analysts to shift their attention from interpreting topics to engaging in theory-driven analysis.","Through technical evaluations and four analysis scenarios ranging from literature review to content moderation, we find that LLooM's concepts improve upon the prior art of topic models in terms of quality and data coverage.","In expert case studies, LLooM helped researchers to uncover new insights even from familiar datasets, for example by suggesting a previously unnoticed concept of attacks on out-party stances in a political social media dataset."],"url":"http://arxiv.org/abs/2404.12259v1","category":"cs.HC"}
{"created":"2024-04-18 15:23:37","title":"Food Portion Estimation via 3D Object Scaling","abstract":"Image-based methods to analyze food images have alleviated the user burden and biases associated with traditional methods. However, accurate portion estimation remains a major challenge due to the loss of 3D information in the 2D representation of foods captured by smartphone cameras or wearable devices. In this paper, we propose a new framework to estimate both food volume and energy from 2D images by leveraging the power of 3D food models and physical reference in the eating scene. Our method estimates the pose of the camera and the food object in the input image and recreates the eating occasion by rendering an image of a 3D model of the food with the estimated poses. We also introduce a new dataset, SimpleFood45, which contains 2D images of 45 food items and associated annotations including food volume, weight, and energy. Our method achieves an average error of 31.10 kCal (17.67%) on this dataset, outperforming existing portion estimation methods.","sentences":["Image-based methods to analyze food images have alleviated the user burden and biases associated with traditional methods.","However, accurate portion estimation remains a major challenge due to the loss of 3D information in the 2D representation of foods captured by smartphone cameras or wearable devices.","In this paper, we propose a new framework to estimate both food volume and energy from 2D images by leveraging the power of 3D food models and physical reference in the eating scene.","Our method estimates the pose of the camera and the food object in the input image and recreates the eating occasion by rendering an image of a 3D model of the food with the estimated poses.","We also introduce a new dataset, SimpleFood45, which contains 2D images of 45 food items and associated annotations including food volume, weight, and energy.","Our method achieves an average error of 31.10 kCal (17.67%) on this dataset, outperforming existing portion estimation methods."],"url":"http://arxiv.org/abs/2404.12257v1","category":"cs.CV"}
{"created":"2024-04-18 15:22:29","title":"An Online Spatial-Temporal Graph Trajectory Planner for Autonomous Vehicles","abstract":"The autonomous driving industry is expected to grow by over 20 times in the coming decade and, thus, motivate researchers to delve into it. The primary focus of their research is to ensure safety, comfort, and efficiency. An autonomous vehicle has several modules responsible for one or more of the aforementioned items. Among these modules, the trajectory planner plays a pivotal role in the safety of the vehicle and the comfort of its passengers. The module is also responsible for respecting kinematic constraints and any applicable road constraints. In this paper, a novel online spatial-temporal graph trajectory planner is introduced to generate safe and comfortable trajectories. First, a spatial-temporal graph is constructed using the autonomous vehicle, its surrounding vehicles, and virtual nodes along the road with respect to the vehicle itself. Next, the graph is forwarded into a sequential network to obtain the desired states. To support the planner, a simple behavioral layer is also presented that determines kinematic constraints for the planner. Furthermore, a novel potential function is also proposed to train the network. Finally, the proposed planner is tested on three different complex driving tasks, and the performance is compared with two frequently used methods. The results show that the proposed planner generates safe and feasible trajectories while achieving similar or longer distances in the forward direction and comparable comfort ride.","sentences":["The autonomous driving industry is expected to grow by over 20 times in the coming decade and, thus, motivate researchers to delve into it.","The primary focus of their research is to ensure safety, comfort, and efficiency.","An autonomous vehicle has several modules responsible for one or more of the aforementioned items.","Among these modules, the trajectory planner plays a pivotal role in the safety of the vehicle and the comfort of its passengers.","The module is also responsible for respecting kinematic constraints and any applicable road constraints.","In this paper, a novel online spatial-temporal graph trajectory planner is introduced to generate safe and comfortable trajectories.","First, a spatial-temporal graph is constructed using the autonomous vehicle, its surrounding vehicles, and virtual nodes along the road with respect to the vehicle itself.","Next, the graph is forwarded into a sequential network to obtain the desired states.","To support the planner, a simple behavioral layer is also presented that determines kinematic constraints for the planner.","Furthermore, a novel potential function is also proposed to train the network.","Finally, the proposed planner is tested on three different complex driving tasks, and the performance is compared with two frequently used methods.","The results show that the proposed planner generates safe and feasible trajectories while achieving similar or longer distances in the forward direction and comparable comfort ride."],"url":"http://arxiv.org/abs/2404.12256v1","category":"cs.RO"}
{"created":"2024-04-18 15:21:51","title":"Omnidirectional 3D printing of PEDOT:PSS aerogels with tunable electromechanical performance for unconventional stretchable interconnects and thermoelectrics","abstract":"The next generation of soft electronics will expand to the third dimension. This will require the integration of mechanically-compliant three-dimensional functional structures with stretchable materials. This study demonstrates omnidirectional direct ink writing (DIW) of Poly(3,4-ethylenedioxythiophene) polystyrene sulfonate (PEDOT:PSS) aerogels with tunable electrical and mechanical performance, which can be integrated with soft substrates. Several PEDOT:PSS hydrogels were formulated for DIW and freeze-dried directly on stretchable substrates to form integrated aerogels displaying high shape fidelity and minimal shrinkage. The effect of additives and processing in the PEDOT:PSS hydro and aerogels morphology, and the link with their electromechanical properties was elucidated. This technology demonstrated 3D-structured stretchable interconnects and planar thermoelectric generators (TEGs) for skin electronics, as well as vertically-printed high aspect ratio thermoelectric pillars with a high ZT value of 3.2 10^-3 and ultra-low thermal conductivity of 0.065 W/(m K). Despite their comparatively low ZT, the aerogel pillars outpowered their dense counterparts in realistic energy harvesting scenarios where contact resistances cannot be ignored, and produced up to 26 nW/cm2 (corresponding to a gravimetric power density of 0.76 mW/kg) for a difference of temperature of 15 K. This work suggests promising advancements in soft and energy-efficiency electronic systems relevant to soft robotics and wearable.","sentences":["The next generation of soft electronics will expand to the third dimension.","This will require the integration of mechanically-compliant three-dimensional functional structures with stretchable materials.","This study demonstrates omnidirectional direct ink writing (DIW) of Poly(3,4-ethylenedioxythiophene) polystyrene sulfonate (PEDOT:PSS) aerogels with tunable electrical and mechanical performance, which can be integrated with soft substrates.","Several PEDOT:","PSS hydrogels were formulated for DIW and freeze-dried directly on stretchable substrates to form integrated aerogels displaying high shape fidelity and minimal shrinkage.","The effect of additives and processing in the PEDOT:PSS hydro and aerogels morphology, and the link with their electromechanical properties was elucidated.","This technology demonstrated 3D-structured stretchable interconnects and planar thermoelectric generators (TEGs) for skin electronics, as well as vertically-printed high aspect ratio thermoelectric pillars with a high ZT value of 3.2 10^-3 and ultra-low thermal conductivity of 0.065 W/(m K).","Despite their comparatively low ZT, the aerogel pillars outpowered their dense counterparts in realistic energy harvesting scenarios where contact resistances cannot be ignored, and produced up to 26 nW/cm2 (corresponding to a gravimetric power density of 0.76 mW/kg) for a difference of temperature of 15 K. This work suggests promising advancements in soft and energy-efficiency electronic systems relevant to soft robotics and wearable."],"url":"http://arxiv.org/abs/2404.12254v1","category":"physics.app-ph"}
{"created":"2024-04-18 15:18:14","title":"Dynamic Modality and View Selection for Multimodal Emotion Recognition with Missing Modalities","abstract":"The study of human emotions, traditionally a cornerstone in fields like psychology and neuroscience, has been profoundly impacted by the advent of artificial intelligence (AI). Multiple channels, such as speech (voice) and facial expressions (image), are crucial in understanding human emotions. However, AI's journey in multimodal emotion recognition (MER) is marked by substantial technical challenges. One significant hurdle is how AI models manage the absence of a particular modality - a frequent occurrence in real-world situations. This study's central focus is assessing the performance and resilience of two strategies when confronted with the lack of one modality: a novel multimodal dynamic modality and view selection and a cross-attention mechanism. Results on the RECOLA dataset show that dynamic selection-based methods are a promising approach for MER. In the missing modalities scenarios, all dynamic selection-based methods outperformed the baseline. The study concludes by emphasizing the intricate interplay between audio and video modalities in emotion prediction, showcasing the adaptability of dynamic selection methods in handling missing modalities.","sentences":["The study of human emotions, traditionally a cornerstone in fields like psychology and neuroscience, has been profoundly impacted by the advent of artificial intelligence (AI).","Multiple channels, such as speech (voice) and facial expressions (image), are crucial in understanding human emotions.","However, AI's journey in multimodal emotion recognition (MER) is marked by substantial technical challenges.","One significant hurdle is how AI models manage the absence of a particular modality - a frequent occurrence in real-world situations.","This study's central focus is assessing the performance and resilience of two strategies when confronted with the lack of one modality: a novel multimodal dynamic modality and view selection and a cross-attention mechanism.","Results on the RECOLA dataset show that dynamic selection-based methods are a promising approach for MER.","In the missing modalities scenarios, all dynamic selection-based methods outperformed the baseline.","The study concludes by emphasizing the intricate interplay between audio and video modalities in emotion prediction, showcasing the adaptability of dynamic selection methods in handling missing modalities."],"url":"http://arxiv.org/abs/2404.12251v1","category":"cs.LG"}
{"created":"2024-04-18 15:13:00","title":"An approach to Hamiltonian Floer theory for maps from surfaces","abstract":"In $n$-dimensional classical field theory one studies maps from $n$-dimensional manifolds in such a way that classical mechanics is recovered for $n=1$. In previous papers we have shown that the standard polysymplectic framework in which field theory is described, is not suitable for variational techniques. In this paper, we introduce for $n=2$ a Lagrange-Hamilton formalism that allows us to define a generalization of Hamiltonian Floer theory. As an application, we prove a cuplength estimate for our Hamiltonian equations that yields a lower bound on the number of solutions to Laplace equations with nonlinearity. We also discuss the relation with holomorphic Floer theory.","sentences":["In $n$-dimensional classical field theory one studies maps from $n$-dimensional manifolds in such a way that classical mechanics is recovered for $n=1$. In previous papers we have shown that the standard polysymplectic framework in which field theory is described, is not suitable for variational techniques.","In this paper, we introduce for $n=2$ a Lagrange-Hamilton formalism that allows us to define a generalization of Hamiltonian Floer theory.","As an application, we prove a cuplength estimate for our Hamiltonian equations that yields a lower bound on the number of solutions to Laplace equations with nonlinearity.","We also discuss the relation with holomorphic Floer theory."],"url":"http://arxiv.org/abs/2404.12249v1","category":"math.SG"}
{"created":"2024-04-18 15:12:52","title":"The quantum gravity seeds for laws of nature","abstract":"We discuss the challenges that the standard (Humean and non-Humean) accounts of laws face within the framework of quantum gravity where space and time may not be fundamental. This paper identifies core (meta)physical features that cut across a number of quantum gravity approaches and formalisms and that provide seeds for articulating updated conceptions that could account for QG laws not involving any spatio-temporal notions. To this aim, we will in particular highlight the constitutive roles of quantum entanglement, quantum transition amplitudes and quantum causal histories. These features also stress the fruitful overlap between quantum gravity and quantum information theory.","sentences":["We discuss the challenges that the standard (Humean and non-Humean) accounts of laws face within the framework of quantum gravity where space and time may not be fundamental.","This paper identifies core (meta)physical features that cut across a number of quantum gravity approaches and formalisms and that provide seeds for articulating updated conceptions that could account for QG laws not involving any spatio-temporal notions.","To this aim, we will in particular highlight the constitutive roles of quantum entanglement, quantum transition amplitudes and quantum causal histories.","These features also stress the fruitful overlap between quantum gravity and quantum information theory."],"url":"http://arxiv.org/abs/2404.12248v1","category":"physics.hist-ph"}
{"created":"2024-04-18 15:11:53","title":"How Thick is the Air-Water Interface? -- A Direct Experimental Measurement of the Decay Length of the Interfacial Structural Anisotropy","abstract":"The air-water interface is a highly prevalent phase boundary with a far-reaching impact on natural and industrial processes. Water molecules behave differently at the interface compared to the bulk, exhibiting anisotropic orientational distributions, reduced intermolecular connectivity in the hydrogen bond network, and significantly slower dynamics. Despite many decades of research, the thickness of the structural anisotropy in the interfacial layer remains controversial, with a direct experimental measurement being absent. In this study, we utilise an advancement in non-linear vibrational spectroscopy to gain access to this important parameter. Combining phase-resolved sum- and difference-frequency generation (SFG and DFG) responses, we directly measure the decay in structural anisotropy of the air-water interface. We find a decay length of ~6-8\\r{A}, in excellent agreement with depth-resolved SFG spectra calculated from ab initio parameterised molecular dynamics (MD) simulations. The result reveals surprisingly short anisotropic orientational correlations from the interfacial layer that are even shorter than in the bulk. Furthermore, the recorded SFG and DFG responses are decomposed into a vibrationally resonant and non-resonant contribution through isotopic exchange measurements. Through their separate analysis, we show that the resonant response is a sensitive probe of the structural anisotropy at the interface whereas the non-resonant contribution contains a significant isotropic contribution from the bulk and therefore only partially reports on the interfacial structure. This finding places stringent restrictions on the insight available through both purely non-resonant and second-order intensity studies.","sentences":["The air-water interface is a highly prevalent phase boundary with a far-reaching impact on natural and industrial processes.","Water molecules behave differently at the interface compared to the bulk, exhibiting anisotropic orientational distributions, reduced intermolecular connectivity in the hydrogen bond network, and significantly slower dynamics.","Despite many decades of research, the thickness of the structural anisotropy in the interfacial layer remains controversial, with a direct experimental measurement being absent.","In this study, we utilise an advancement in non-linear vibrational spectroscopy to gain access to this important parameter.","Combining phase-resolved sum- and difference-frequency generation (SFG and DFG) responses, we directly measure the decay in structural anisotropy of the air-water interface.","We find a decay length of ~6-8\\r{A}, in excellent agreement with depth-resolved SFG spectra calculated from ab initio parameterised molecular dynamics (MD) simulations.","The result reveals surprisingly short anisotropic orientational correlations from the interfacial layer that are even shorter than in the bulk.","Furthermore, the recorded SFG and DFG responses are decomposed into a vibrationally resonant and non-resonant contribution through isotopic exchange measurements.","Through their separate analysis, we show that the resonant response is a sensitive probe of the structural anisotropy at the interface whereas the non-resonant contribution contains a significant isotropic contribution from the bulk and therefore only partially reports on the interfacial structure.","This finding places stringent restrictions on the insight available through both purely non-resonant and second-order intensity studies."],"url":"http://arxiv.org/abs/2404.12247v1","category":"physics.chem-ph"}
{"created":"2024-04-18 15:05:55","title":"PyTOaCNN: Topology optimization using an adaptive convolutional neural network in Python","abstract":"This paper introduces an adaptive convolutional neural network (CNN) architecture capable of automating various topology optimization (TO) problems with diverse underlying physics. The proposed architecture has an encoder-decoder-type structure with dense layers added at the bottleneck region to capture complex geometrical features. The network is trained using datasets obtained by the problem-specific open-source TO codes. Tensorflow and Keras are the main libraries employed to develop and to train the model. Effectiveness and robustness of the proposed adaptive CNN model are demonstrated through its performance in compliance minimization problems involving constant and design-dependent loads and in addressing bulk modulus optimization. Once trained, the model takes user's input of the volume fraction as an image and instantly generates an output image of optimized design. The proposed CNN produces high-quality results resembling those obtained via open-source TO codes with negligible performance and volume fraction errors. The paper includes complete associated Python code (Appendix A) for the proposed CNN architecture and explains each part of the code to facilitate reproducibility and ease of learning.","sentences":["This paper introduces an adaptive convolutional neural network (CNN) architecture capable of automating various topology optimization (TO) problems with diverse underlying physics.","The proposed architecture has an encoder-decoder-type structure with dense layers added at the bottleneck region to capture complex geometrical features.","The network is trained using datasets obtained by the problem-specific open-source TO codes.","Tensorflow and Keras are the main libraries employed to develop and to train the model.","Effectiveness and robustness of the proposed adaptive CNN model are demonstrated through its performance in compliance minimization problems involving constant and design-dependent loads and in addressing bulk modulus optimization.","Once trained, the model takes user's input of the volume fraction as an image and instantly generates an output image of optimized design.","The proposed CNN produces high-quality results resembling those obtained via open-source TO codes with negligible performance and volume fraction errors.","The paper includes complete associated Python code (Appendix A) for the proposed CNN architecture and explains each part of the code to facilitate reproducibility and ease of learning."],"url":"http://arxiv.org/abs/2404.12244v1","category":"cs.CE"}
{"created":"2024-04-18 15:05:09","title":"A non-singular universe out of Hayward black hole","abstract":"We construct a (quantum mechanically) modified model for the Oppenheimer-Snyder collapse scenario where the exterior of the collapsing dust ball is a Hayward black hole spacetime and the interior is a dust Friedmann-Robertson-Walker cosmology. This interior cosmology is entirely determined by the junction conditions with the exterior black hole. It turns out to be non-singular, displaying a power-law contraction which precedes a de Sitter phase or, reversely, a power-law expansion followed by a de Sitter era. We also analyse the global causal structure and the viability of the model.","sentences":["We construct a (quantum mechanically) modified model for the Oppenheimer-Snyder collapse scenario where the exterior of the collapsing dust ball is a Hayward black hole spacetime and the interior is a dust Friedmann-Robertson-Walker cosmology.","This interior cosmology is entirely determined by the junction conditions with the exterior black hole.","It turns out to be non-singular, displaying a power-law contraction which precedes a de Sitter phase or, reversely, a power-law expansion followed by a de Sitter era.","We also analyse the global causal structure and the viability of the model."],"url":"http://arxiv.org/abs/2404.12243v1","category":"gr-qc"}
{"created":"2024-04-18 15:02:35","title":"CMNEE: A Large-Scale Document-Level Event Extraction Dataset based on Open-Source Chinese Military News","abstract":"Extracting structured event knowledge, including event triggers and corresponding arguments, from military texts is fundamental to many applications, such as intelligence analysis and decision assistance. However, event extraction in the military field faces the data scarcity problem, which impedes the research of event extraction models in this domain. To alleviate this problem, we propose CMNEE, a large-scale, document-level open-source Chinese Military News Event Extraction dataset. It contains 17,000 documents and 29,223 events, which are all manually annotated based on a pre-defined schema for the military domain including 8 event types and 11 argument role types. We designed a two-stage, multi-turns annotation strategy to ensure the quality of CMNEE and reproduced several state-of-the-art event extraction models with a systematic evaluation. The experimental results on CMNEE fall shorter than those on other domain datasets obviously, which demonstrates that event extraction for military domain poses unique challenges and requires further research efforts. Our code and data can be obtained from https://github.com/Mzzzhu/CMNEE.","sentences":["Extracting structured event knowledge, including event triggers and corresponding arguments, from military texts is fundamental to many applications, such as intelligence analysis and decision assistance.","However, event extraction in the military field faces the data scarcity problem, which impedes the research of event extraction models in this domain.","To alleviate this problem, we propose CMNEE, a large-scale, document-level open-source Chinese Military News Event Extraction dataset.","It contains 17,000 documents and 29,223 events, which are all manually annotated based on a pre-defined schema for the military domain including 8 event types and 11 argument role types.","We designed a two-stage, multi-turns annotation strategy to ensure the quality of CMNEE and reproduced several state-of-the-art event extraction models with a systematic evaluation.","The experimental results on CMNEE fall shorter than those on other domain datasets obviously, which demonstrates that event extraction for military domain poses unique challenges and requires further research efforts.","Our code and data can be obtained from https://github.com/Mzzzhu/CMNEE."],"url":"http://arxiv.org/abs/2404.12242v1","category":"cs.CL"}
{"created":"2024-04-18 15:01:00","title":"Introducing v0.5 of the AI Safety Benchmark from MLCommons","abstract":"This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. We have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark.","sentences":["This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group.","The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models.","We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users).","We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark.","We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024.","The v1.0 benchmark will provide meaningful insights into the safety of AI systems.","However, the v0.5 benchmark should not be used to assess the safety of AI systems.","We have sought to fully document the limitations, flaws, and challenges of v0.5.","This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts.","There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark."],"url":"http://arxiv.org/abs/2404.12241v1","category":"cs.CL"}
{"created":"2024-04-18 15:00:59","title":"A Time-Inhomogeneous Markov Model for Resource Availability under Sparse Observations","abstract":"Accurate spatio-temporal information about the current situation is crucial for smart city applications such as modern routing algorithms. Often, this information describes the state of stationary resources, e.g. the availability of parking bays, charging stations or the amount of people waiting for a vehicle to pick them up near a given location. To exploit this kind of information, predicting future states of the monitored resources is often mandatory because a resource might change its state within the time until it is needed. To train an accurate predictive model, it is often not possible to obtain a continuous time series on the state of the resource. For example, the information might be collected from traveling agents visiting the resource with an irregular frequency. Thus, it is necessary to develop methods which work on sparse observations for training and prediction. In this paper, we propose time-inhomogeneous discrete Markov models to allow accurate prediction even when the frequency of observation is very rare. Our new model is able to blend recent observations with historic data and also provide useful probabilistic estimates for future states. Since resources availability in a city is typically time-dependent, our Markov model is time-inhomogeneous and cyclic within a predefined time interval. To train our model, we propose a modified Baum-Welch algorithm. Evaluations on real-world datasets of parking bay availability show that our new method indeed yields good results compared to methods being trained on complete data and non-cyclic variants.","sentences":["Accurate spatio-temporal information about the current situation is crucial for smart city applications such as modern routing algorithms.","Often, this information describes the state of stationary resources, e.g. the availability of parking bays, charging stations or the amount of people waiting for a vehicle to pick them up near a given location.","To exploit this kind of information, predicting future states of the monitored resources is often mandatory because a resource might change its state within the time until it is needed.","To train an accurate predictive model, it is often not possible to obtain a continuous time series on the state of the resource.","For example, the information might be collected from traveling agents visiting the resource with an irregular frequency.","Thus, it is necessary to develop methods which work on sparse observations for training and prediction.","In this paper, we propose time-inhomogeneous discrete Markov models to allow accurate prediction even when the frequency of observation is very rare.","Our new model is able to blend recent observations with historic data and also provide useful probabilistic estimates for future states.","Since resources availability in a city is typically time-dependent, our Markov model is time-inhomogeneous and cyclic within a predefined time interval.","To train our model, we propose a modified Baum-Welch algorithm.","Evaluations on real-world datasets of parking bay availability show that our new method indeed yields good results compared to methods being trained on complete data and non-cyclic variants."],"url":"http://arxiv.org/abs/2404.12240v1","category":"cs.AI"}
{"created":"2024-04-18 14:51:55","title":"De-DSI: Decentralised Differentiable Search Index","abstract":"This study introduces De-DSI, a novel framework that fuses large language models (LLMs) with genuine decentralization for information retrieval, particularly employing the differentiable search index (DSI) concept in a decentralized setting. Focused on efficiently connecting novel user queries with document identifiers without direct document access, De-DSI operates solely on query-docid pairs. To enhance scalability, an ensemble of DSI models is introduced, where the dataset is partitioned into smaller shards for individual model training. This approach not only maintains accuracy by reducing the number of data each model needs to handle but also facilitates scalability by aggregating outcomes from multiple models. This aggregation uses a beam search to identify top docids and applies a softmax function for score normalization, selecting documents with the highest scores for retrieval. The decentralized implementation demonstrates that retrieval success is comparable to centralized methods, with the added benefit of the possibility of distributing computational complexity across the network. This setup also allows for the retrieval of multimedia items through magnet links, eliminating the need for platforms or intermediaries.","sentences":["This study introduces De-DSI, a novel framework that fuses large language models (LLMs) with genuine decentralization for information retrieval, particularly employing the differentiable search index (DSI) concept in a decentralized setting.","Focused on efficiently connecting novel user queries with document identifiers without direct document access, De-DSI operates solely on query-docid pairs.","To enhance scalability, an ensemble of DSI models is introduced, where the dataset is partitioned into smaller shards for individual model training.","This approach not only maintains accuracy by reducing the number of data each model needs to handle but also facilitates scalability by aggregating outcomes from multiple models.","This aggregation uses a beam search to identify top docids and applies a softmax function for score normalization, selecting documents with the highest scores for retrieval.","The decentralized implementation demonstrates that retrieval success is comparable to centralized methods, with the added benefit of the possibility of distributing computational complexity across the network.","This setup also allows for the retrieval of multimedia items through magnet links, eliminating the need for platforms or intermediaries."],"url":"http://arxiv.org/abs/2404.12237v1","category":"cs.IR"}
{"created":"2024-04-18 14:51:42","title":"Beyond Average: Individualized Visual Scanpath Prediction","abstract":"Understanding how attention varies across individuals has significant scientific and societal impacts. However, existing visual scanpath models treat attention uniformly, neglecting individual differences. To bridge this gap, this paper focuses on individualized scanpath prediction (ISP), a new attention modeling task that aims to accurately predict how different individuals shift their attention in diverse visual tasks. It proposes an ISP method featuring three novel technical components: (1) an observer encoder to characterize and integrate an observer's unique attention traits, (2) an observer-centric feature integration approach that holistically combines visual features, task guidance, and observer-specific characteristics, and (3) an adaptive fixation prioritization mechanism that refines scanpath predictions by dynamically prioritizing semantic feature maps based on individual observers' attention traits. These novel components allow scanpath models to effectively address the attention variations across different observers. Our method is generally applicable to different datasets, model architectures, and visual tasks, offering a comprehensive tool for transforming general scanpath models into individualized ones. Comprehensive evaluations using value-based and ranking-based metrics verify the method's effectiveness and generalizability.","sentences":["Understanding how attention varies across individuals has significant scientific and societal impacts.","However, existing visual scanpath models treat attention uniformly, neglecting individual differences.","To bridge this gap, this paper focuses on individualized scanpath prediction (ISP), a new attention modeling task that aims to accurately predict how different individuals shift their attention in diverse visual tasks.","It proposes an ISP method featuring three novel technical components: (1) an observer encoder to characterize and integrate an observer's unique attention traits, (2) an observer-centric feature integration approach that holistically combines visual features, task guidance, and observer-specific characteristics, and (3) an adaptive fixation prioritization mechanism that refines scanpath predictions by dynamically prioritizing semantic feature maps based on individual observers' attention traits.","These novel components allow scanpath models to effectively address the attention variations across different observers.","Our method is generally applicable to different datasets, model architectures, and visual tasks, offering a comprehensive tool for transforming general scanpath models into individualized ones.","Comprehensive evaluations using value-based and ranking-based metrics verify the method's effectiveness and generalizability."],"url":"http://arxiv.org/abs/2404.12235v1","category":"cs.CV"}
{"created":"2024-04-18 14:50:55","title":"Constraining Higgs sectors of BSM models -- the case of 95 GeV \"Higgs\"","abstract":"In view of lack of the direct experimental evidence for a Beyond the Standard Model (BSM) physics, accommodating a SM-like Higgs boson is on the most important constraints that a BSM model must fulfill. Already for some time the FlexibleSUSY spectrum generator generator allowed for a reliable prediction of masses and decay patterns of the BSM Higgs boson in a large class of user defined supersymmetric and non-supersymmetric models. However, no easy way to compare those predictions with experimental data existed. To that end we present here an interface between FlexibleSUSY and HiggsTools, a computer program assessing in a statistically meaningful way consistency of a BSM Higgs sector with experiments.   Motivated by the recent ATLAS and CMS observation of the di-photon excess at a mass of $\\sim$95 GeV we demonstrate the capabilities of our framework by investigating whether the observed low mass excesses around 95 GeV seen in the data can be explained as the lightest scalar of the Minimal R-symmetric Supersymmetric Standard Model, without spoiling the SM-like properties of the second-to-lightest state. We also briefly comment on the light dark matter candidate which is a necessarily ingredient of such a setup.","sentences":["In view of lack of the direct experimental evidence for a Beyond the Standard Model (BSM) physics, accommodating a SM-like Higgs boson is on the most important constraints that a BSM model must fulfill.","Already for some time the FlexibleSUSY spectrum generator generator allowed for a reliable prediction of masses and decay patterns of the BSM Higgs boson in a large class of user defined supersymmetric and non-supersymmetric models.","However, no easy way to compare those predictions with experimental data existed.","To that end we present here an interface between FlexibleSUSY and HiggsTools, a computer program assessing in a statistically meaningful way consistency of a BSM Higgs sector with experiments.   ","Motivated by the recent ATLAS and CMS observation of the di-photon excess at a mass of $\\sim$95 GeV we demonstrate the capabilities of our framework by investigating whether the observed low mass excesses around 95 GeV seen in the data can be explained as the lightest scalar of the Minimal R-symmetric Supersymmetric Standard Model, without spoiling the SM-like properties of the second-to-lightest state.","We also briefly comment on the light dark matter candidate which is a necessarily ingredient of such a setup."],"url":"http://arxiv.org/abs/2404.12233v1","category":"hep-ph"}
{"created":"2024-04-18 14:47:42","title":"Estimates for the quantized tensor train ranks for the power functions","abstract":"In this work we provide theoretical estimates for the ranks of the power functions $f(k) = k^{-\\alpha}$, $\\alpha>1$ in the quantized tensor train (QTT) format for $k = 1, 2, 3, \\ldots, 2^{d}$. Such functions and their several generalizations (e.~g. $f(k) = k^{-\\alpha} \\cdot e^{-\\lambda k}, \\lambda > 0$) play an important role in studies of the asymptotic solutions of the aggregation-fragmentation kinetic equations. In order to support the constructed theory we verify the values of QTT-ranks of these functions in practice with the use of the TTSVD procedure and show an agreement between the numerical and analytical results.","sentences":["In this work we provide theoretical estimates for the ranks of the power functions $f(k) = k^{-\\alpha}$, $\\alpha>1$ in the quantized tensor train (QTT) format for $k = 1, 2, 3, \\ldots, 2^{d}$. Such functions and their several generalizations (e.~g. $f(k) = k^{-\\alpha} \\cdot e^{-\\lambda k}, \\lambda > 0$) play an important role in studies of the asymptotic solutions of the aggregation-fragmentation kinetic equations.","In order to support the constructed theory we verify the values of QTT-ranks of these functions in practice with the use of the TTSVD procedure and show an agreement between the numerical and analytical results."],"url":"http://arxiv.org/abs/2404.12230v1","category":"math.NA"}
{"created":"2024-04-18 14:44:08","title":"Relationship Discovery for Drug Recommendation","abstract":"Medication recommendation systems are designed to deliver personalized drug suggestions that are closely aligned with individual patient needs. Previous studies have primarily concentrated on developing medication embeddings, achieving significant progress. Nonetheless, these approaches often fall short in accurately reflecting individual patient profiles, mainly due to challenges in distinguishing between various patient conditions and the inability to establish precise correlations between specific conditions and appropriate medications. In response to these issues, we introduce DisMed, a model that focuses on patient conditions to enhance personalization. DisMed employs causal inference to discern clear, quantifiable causal links. It then examines patient conditions in depth, recognizing and adapting to the evolving nuances of these conditions, and mapping them directly to corresponding medications. Additionally, DisMed leverages data from multiple patient visits to propose combinations of medications. Comprehensive testing on real-world datasets demonstrates that DisMed not only improves the customization of patient profiles but also surpasses leading models in both precision and safety.","sentences":["Medication recommendation systems are designed to deliver personalized drug suggestions that are closely aligned with individual patient needs.","Previous studies have primarily concentrated on developing medication embeddings, achieving significant progress.","Nonetheless, these approaches often fall short in accurately reflecting individual patient profiles, mainly due to challenges in distinguishing between various patient conditions and the inability to establish precise correlations between specific conditions and appropriate medications.","In response to these issues, we introduce DisMed, a model that focuses on patient conditions to enhance personalization.","DisMed employs causal inference to discern clear, quantifiable causal links.","It then examines patient conditions in depth, recognizing and adapting to the evolving nuances of these conditions, and mapping them directly to corresponding medications.","Additionally, DisMed leverages data from multiple patient visits to propose combinations of medications.","Comprehensive testing on real-world datasets demonstrates that DisMed not only improves the customization of patient profiles but also surpasses leading models in both precision and safety."],"url":"http://arxiv.org/abs/2404.12228v1","category":"cs.AI"}
{"created":"2024-04-18 14:38:32","title":"Length Generalization of Causal Transformers without Position Encoding","abstract":"Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length. We identify a connection between the failure of NoPE's generalization and the distraction of attention distributions. We propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size. Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms. The source code is publicly accessible","sentences":["Generalizing to longer sentences is important for recent Transformer-based language models.","Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge.","In this paper, we study the length generalization property of NoPE.","We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length.","We identify a connection between the failure of NoPE's generalization and the distraction of attention distributions.","We propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size.","Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms.","The source code is publicly accessible"],"url":"http://arxiv.org/abs/2404.12224v1","category":"cs.CL"}
{"created":"2024-04-18 14:38:05","title":"Images of Kerr-MOG black holes surrounded by geometrically thick magnetized equilibrium tori","abstract":"We adopt general relativistic ray-tracing (GRRT) schemes to study images of Kerr-MOG black holes surrounded by geometrically thick magnetized equilibrium tori, which belong to steady-state solutions of thick accretion disks within the framework of general relativistic magnetohydrodynamics (GRMHD). The black hole possesses an extra dimensionless MOG parameter described its deviation from usual Kerr one. Our results show that the presence of the MOG parameter leads to smaller disks in size, but enhances the total flux density and peak brightness in their images. Combining with observation data of black hole M87* from the Event Horizon Telescope (EHT), we make a constraint on parameters of the Kerr-MOG black hole and find that the presence of the MOG parameter broadens the allowable range of black hole spin.","sentences":["We adopt general relativistic ray-tracing (GRRT) schemes to study images of Kerr-MOG black holes surrounded by geometrically thick magnetized equilibrium tori, which belong to steady-state solutions of thick accretion disks within the framework of general relativistic magnetohydrodynamics (GRMHD).","The black hole possesses an extra dimensionless MOG parameter described its deviation from usual Kerr one.","Our results show that the presence of the MOG parameter leads to smaller disks in size, but enhances the total flux density and peak brightness in their images.","Combining with observation data of black hole M87* from the Event Horizon Telescope (EHT), we make a constraint on parameters of the Kerr-MOG black hole and find that the presence of the MOG parameter broadens the allowable range of black hole spin."],"url":"http://arxiv.org/abs/2404.12223v1","category":"gr-qc"}
{"created":"2024-04-18 14:31:54","title":"Hybrid Dynamics Modeling and Trajectory Planning for a Cable-Trailer System with a Quadruped Robot","abstract":"Inspired by the utilization of dogs in sled-pulling for transportation, we introduce a cable-trailer system with a quadruped robot. The motion planning of the proposed robot system presents challenges arising from the nonholonomic constraints of the trailer, system underactuation, and hybrid interaction through the cable. To tackle these challenges, we develop a hybrid dynamics model that accounts for the cable's taut/slack status. Since it is computationally intense to directly optimize the trajectory, we first propose a search algorithm to compute a sub-optimal trajectory as the initial solution. Then, a novel collision avoidance constraint based on the geometric shapes of objects is proposed to formulate the trajectory optimization problem for the hybrid system. The proposed trajectory planning method is implemented on a Unitree A1 quadruped robot with a customized cable-trailer and validated through experiments.","sentences":["Inspired by the utilization of dogs in sled-pulling for transportation, we introduce a cable-trailer system with a quadruped robot.","The motion planning of the proposed robot system presents challenges arising from the nonholonomic constraints of the trailer, system underactuation, and hybrid interaction through the cable.","To tackle these challenges, we develop a hybrid dynamics model that accounts for the cable's taut/slack status.","Since it is computationally intense to directly optimize the trajectory, we first propose a search algorithm to compute a sub-optimal trajectory as the initial solution.","Then, a novel collision avoidance constraint based on the geometric shapes of objects is proposed to formulate the trajectory optimization problem for the hybrid system.","The proposed trajectory planning method is implemented on a Unitree A1 quadruped robot with a customized cable-trailer and validated through experiments."],"url":"http://arxiv.org/abs/2404.12220v1","category":"cs.RO"}
{"created":"2024-04-18 14:30:46","title":"A Quadrature Approach for General-Purpose Batch Bayesian Optimization via Probabilistic Lifting","abstract":"Parallelisation in Bayesian optimisation is a common strategy but faces several challenges: the need for flexibility in acquisition functions and kernel choices, flexibility dealing with discrete and continuous variables simultaneously, model misspecification, and lastly fast massive parallelisation. To address these challenges, we introduce a versatile and modular framework for batch Bayesian optimisation via probabilistic lifting with kernel quadrature, called SOBER, which we present as a Python library based on GPyTorch/BoTorch. Our framework offers the following unique benefits: (1) Versatility in downstream tasks under a unified approach. (2) A gradient-free sampler, which does not require the gradient of acquisition functions, offering domain-agnostic sampling (e.g., discrete and mixed variables, non-Euclidean space). (3) Flexibility in domain prior distribution. (4) Adaptive batch size (autonomous determination of the optimal batch size). (5) Robustness against a misspecified reproducing kernel Hilbert space. (6) Natural stopping criterion.","sentences":["Parallelisation in Bayesian optimisation is a common strategy but faces several challenges: the need for flexibility in acquisition functions and kernel choices, flexibility dealing with discrete and continuous variables simultaneously, model misspecification, and lastly fast massive parallelisation.","To address these challenges, we introduce a versatile and modular framework for batch Bayesian optimisation via probabilistic lifting with kernel quadrature, called SOBER, which we present as a Python library based on GPyTorch/BoTorch.","Our framework offers the following unique benefits: (1) Versatility in downstream tasks under a unified approach.","(2) A gradient-free sampler, which does not require the gradient of acquisition functions, offering domain-agnostic sampling (e.g., discrete and mixed variables, non-Euclidean space).","(3) Flexibility in domain prior distribution.","(4) Adaptive batch size (autonomous determination of the optimal batch size).","(5) Robustness against a misspecified reproducing kernel Hilbert space.","(6) Natural stopping criterion."],"url":"http://arxiv.org/abs/2404.12219v1","category":"cs.LG"}
{"created":"2024-04-18 14:22:18","title":"Broadband microwave signal generation with programmable chirp shapes via low-speed electronics-controlled phase-modulated optical loop","abstract":"Broadband microwave signals with customized chirp shapes are highly captivating in practical applications. Compared with electronic technology, photonic solutions are superior in bandwidth, but suffer from flexible and rapid manipulation of chirp shape. Here, we demonstrate a novel concept for generating broadband microwave signals with programmable chirp shapes. It is realized on a simple fiber-optic platform involving a continuous-wave laser, a phase-modulated optical loop, low-speed electronics at MHz, and an optical coherent receiver. Microwave signals with bandwidths beyond tens of GHz, even up to hundreds of GHz, can be generated, where the chirp shape is identical to the low-frequency driving waveform of the phase-modulated optical loop. Besides, signal parameters, such as bandwidth, center frequency, and temporal duration, can be reconfigured in real time. In the experiment, highly-coherent microwave signals with various customized chirp shapes are generated, where the time resolution for programming the chirp shape is 649 ps. The center frequency and bandwidth tuning ranges exceed 21 GHz, and the temporal duration is tuned in the range of 9 ns to 180 ns.","sentences":["Broadband microwave signals with customized chirp shapes are highly captivating in practical applications.","Compared with electronic technology, photonic solutions are superior in bandwidth, but suffer from flexible and rapid manipulation of chirp shape.","Here, we demonstrate a novel concept for generating broadband microwave signals with programmable chirp shapes.","It is realized on a simple fiber-optic platform involving a continuous-wave laser, a phase-modulated optical loop, low-speed electronics at MHz, and an optical coherent receiver.","Microwave signals with bandwidths beyond tens of GHz, even up to hundreds of GHz, can be generated, where the chirp shape is identical to the low-frequency driving waveform of the phase-modulated optical loop.","Besides, signal parameters, such as bandwidth, center frequency, and temporal duration, can be reconfigured in real time.","In the experiment, highly-coherent microwave signals with various customized chirp shapes are generated, where the time resolution for programming the chirp shape is 649 ps.","The center frequency and bandwidth tuning ranges exceed 21 GHz, and the temporal duration is tuned in the range of 9 ns to 180 ns."],"url":"http://arxiv.org/abs/2404.12217v1","category":"physics.optics"}
{"created":"2024-04-18 14:20:30","title":"ProTA: Probabilistic Token Aggregation for Text-Video Retrieval","abstract":"Text-video retrieval aims to find the most relevant cross-modal samples for a given query. Recent methods focus on modeling the whole spatial-temporal relations. However, since video clips contain more diverse content than captions, the model aligning these asymmetric video-text pairs has a high risk of retrieving many false positive results. In this paper, we propose Probabilistic Token Aggregation (\\textit{ProTA}) to handle cross-modal interaction with content asymmetry. Specifically, we propose dual partial-related aggregation to disentangle and re-aggregate token representations in both low-dimension and high-dimension spaces. We propose token-based probabilistic alignment to generate token-level probabilistic representation and maintain the feature representation diversity. In addition, an adaptive contrastive loss is proposed to learn compact cross-modal distribution space. Based on extensive experiments, \\textit{ProTA} achieves significant improvements on MSR-VTT (50.9%), LSMDC (25.8%), and DiDeMo (47.2%).","sentences":["Text-video retrieval aims to find the most relevant cross-modal samples for a given query.","Recent methods focus on modeling the whole spatial-temporal relations.","However, since video clips contain more diverse content than captions, the model aligning these asymmetric video-text pairs has a high risk of retrieving many false positive results.","In this paper, we propose Probabilistic Token Aggregation (\\textit{ProTA}) to handle cross-modal interaction with content asymmetry.","Specifically, we propose dual partial-related aggregation to disentangle and re-aggregate token representations in both low-dimension and high-dimension spaces.","We propose token-based probabilistic alignment to generate token-level probabilistic representation and maintain the feature representation diversity.","In addition, an adaptive contrastive loss is proposed to learn compact cross-modal distribution space.","Based on extensive experiments, \\textit{ProTA} achieves significant improvements on MSR-VTT (50.9%), LSMDC (25.8%), and DiDeMo (47.2%)."],"url":"http://arxiv.org/abs/2404.12216v1","category":"cs.CV"}
{"created":"2024-04-18 14:20:08","title":"Income Shocks and their Transmission into Consumption","abstract":"This article reviews the economics literature of, primarily, the last 20 years, that studies the link between income shocks and consumption fluctuations at the household level. We identify three broad approaches through which researchers estimate the consumption response to income shocks: 1.) structural methods in which a fully or partially specified model helps identify the consumption response to income shocks from the data; 2.) natural experiments in which the consumption response of one group who receives an income shock is compared to another group who does not; 3.) elicitation surveys in which consumers are asked how they expect to react to various hypothetical events.","sentences":["This article reviews the economics literature of, primarily, the last 20 years, that studies the link between income shocks and consumption fluctuations at the household level.","We identify three broad approaches through which researchers estimate the consumption response to income shocks: 1.)","structural methods in which a fully or partially specified model helps identify the consumption response to income shocks from the data; 2.) natural experiments in which the consumption response of one group who receives an income shock is compared to another group who does not; 3.)","elicitation surveys in which consumers are asked how they expect to react to various hypothetical events."],"url":"http://arxiv.org/abs/2404.12214v1","category":"econ.GN"}
{"created":"2024-04-18 14:19:45","title":"Investigating Variance Definitions for Mirror Descent with Relative Smoothness","abstract":"Mirror Descent is a popular algorithm, that extends Gradients Descent (GD) beyond the Euclidean geometry. One of its benefits is to enable strong convergence guarantees through smooth-like analyses, even for objectives with exploding or vanishing curvature. This is achieved through the introduction of the notion of relative smoothness, which holds in many of the common use-cases of Mirror descent. While basic deterministic results extend well to the relative setting, most existing stochastic analyses require additional assumptions on the mirror, such as strong convexity (in the usual sense), to ensure bounded variance. In this work, we revisit Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduce a new (less restrictive) definition of variance which can generally be bounded (globally) under mild regularity assumptions. We then investigate this notion in more details, and show that it naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, we leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.","sentences":["Mirror Descent is a popular algorithm, that extends Gradients Descent (GD) beyond the Euclidean geometry.","One of its benefits is to enable strong convergence guarantees through smooth-like analyses, even for objectives with exploding or vanishing curvature.","This is achieved through the introduction of the notion of relative smoothness, which holds in many of the common use-cases of Mirror descent.","While basic deterministic results extend well to the relative setting, most existing stochastic analyses require additional assumptions on the mirror, such as strong convexity (in the usual sense), to ensure bounded variance.","In this work, we revisit Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduce a new (less restrictive) definition of variance which can generally be bounded (globally) under mild regularity assumptions.","We then investigate this notion in more details, and show that it naturally leads to strong convergence guarantees for stochastic mirror descent.","Finally, we leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance."],"url":"http://arxiv.org/abs/2404.12213v1","category":"math.OC"}
{"created":"2024-04-18 14:14:07","title":"Partial-to-Partial Shape Matching with Geometric Consistency","abstract":"Finding correspondences between 3D shapes is an important and long-standing problem in computer vision, graphics and beyond. A prominent challenge are partial-to-partial shape matching settings, which occur when the shapes to match are only observed incompletely (e.g. from 3D scanning). Although partial-to-partial matching is a highly relevant setting in practice, it is rarely explored. Our work bridges the gap between existing (rather artificial) 3D full shape matching and partial-to-partial real-world settings by exploiting geometric consistency as a strong constraint. We demonstrate that it is indeed possible to solve this challenging problem in a variety of settings. For the first time, we achieve geometric consistency for partial-to-partial matching, which is realized by a novel integer non-linear program formalism building on triangle product spaces, along with a new pruning algorithm based on linear integer programming. Further, we generate a new inter-class dataset for partial-to-partial shape-matching. We show that our method outperforms current SOTA methods on both an established intra-class dataset and our novel inter-class dataset.","sentences":["Finding correspondences between 3D shapes is an important and long-standing problem in computer vision, graphics and beyond.","A prominent challenge are partial-to-partial shape matching settings, which occur when the shapes to match are only observed incompletely (e.g. from 3D scanning).","Although partial-to-partial matching is a highly relevant setting in practice, it is rarely explored.","Our work bridges the gap between existing (rather artificial) 3D full shape matching and partial-to-partial real-world settings by exploiting geometric consistency as a strong constraint.","We demonstrate that it is indeed possible to solve this challenging problem in a variety of settings.","For the first time, we achieve geometric consistency for partial-to-partial matching, which is realized by a novel integer non-linear program formalism building on triangle product spaces, along with a new pruning algorithm based on linear integer programming.","Further, we generate a new inter-class dataset for partial-to-partial shape-matching.","We show that our method outperforms current SOTA methods on both an established intra-class dataset and our novel inter-class dataset."],"url":"http://arxiv.org/abs/2404.12209v1","category":"cs.CV"}
{"created":"2024-04-18 14:07:08","title":"GraFIQs: Face Image Quality Assessment Using Gradient Magnitudes","abstract":"Face Image Quality Assessment (FIQA) estimates the utility of face images for automated face recognition (FR) systems. We propose in this work a novel approach to assess the quality of face images based on inspecting the required changes in the pre-trained FR model weights to minimize differences between testing samples and the distribution of the FR training dataset. To achieve that, we propose quantifying the discrepancy in Batch Normalization statistics (BNS), including mean and variance, between those recorded during FR training and those obtained by processing testing samples through the pretrained FR model. We then generate gradient magnitudes of pretrained FR weights by backpropagating the BNS through the pretrained model. The cumulative absolute sum of these gradient magnitudes serves as the FIQ for our approach. Through comprehensive experimentation, we demonstrate the effectiveness of our training-free and quality labeling-free approach, achieving competitive performance to recent state-of-theart FIQA approaches without relying on quality labeling, the need to train regression networks, specialized architectures, or designing and optimizing specific loss functions.","sentences":["Face Image Quality Assessment (FIQA) estimates the utility of face images for automated face recognition (FR) systems.","We propose in this work a novel approach to assess the quality of face images based on inspecting the required changes in the pre-trained FR model weights to minimize differences between testing samples and the distribution of the FR training dataset.","To achieve that, we propose quantifying the discrepancy in Batch Normalization statistics (BNS), including mean and variance, between those recorded during FR training and those obtained by processing testing samples through the pretrained FR model.","We then generate gradient magnitudes of pretrained FR weights by backpropagating the BNS through the pretrained model.","The cumulative absolute sum of these gradient magnitudes serves as the FIQ for our approach.","Through comprehensive experimentation, we demonstrate the effectiveness of our training-free and quality labeling-free approach, achieving competitive performance to recent state-of-theart FIQA approaches without relying on quality labeling, the need to train regression networks, specialized architectures, or designing and optimizing specific loss functions."],"url":"http://arxiv.org/abs/2404.12203v1","category":"cs.CV"}
{"created":"2024-04-18 14:07:01","title":"Reflections on the denialism of the Earth's curvature based on the general public participation in the collective reproduction of Eratosthenes' experiment","abstract":"In this paper, we propose the replication of scientific experiments, with the participation of the general public, as one of the possible strategies to confront science deniers. Using the social media of the Federal University of Lavras (UFLA), the project entitled : The Magic of Physics and the Universe, invited the public to reproduce the experiment carried out by the Greek Eratosthenes over 2,100 years ago, which he proved that the Earth is spherical. We present a report of this experience to suggest that it is possible for citizens to experience scientific knowledge and observe the phenomena in a simple way, in order to develop a critical interpretation of false information that circulates in society. This proposal distinctively geeks to deconstruct the denial of the Earth's curvature through direct and collective participation of citizens in Eratosthenes' experiment, which permit more reflection on how the scientific method used to study this subject works.","sentences":["In this paper, we propose the replication of scientific experiments, with the participation of the general public, as one of the possible strategies to confront science deniers.","Using the social media of the Federal University of Lavras (UFLA), the project entitled : The Magic of Physics and the Universe, invited the public to reproduce the experiment carried out by the Greek Eratosthenes over 2,100 years ago, which he proved that the Earth is spherical.","We present a report of this experience to suggest that it is possible for citizens to experience scientific knowledge and observe the phenomena in a simple way, in order to develop a critical interpretation of false information that circulates in society.","This proposal distinctively geeks to deconstruct the denial of the Earth's curvature through direct and collective participation of citizens in Eratosthenes' experiment, which permit more reflection on how the scientific method used to study this subject works."],"url":"http://arxiv.org/abs/2404.12202v1","category":"physics.soc-ph"}
{"created":"2024-04-18 14:02:43","title":"Axion Dark Matter from Heavy Quarks","abstract":"We propose simple scenarios where the observed dark matter abundance arises from decays and scatterings of heavy quarks through freeze-in of an axion-like particle with mass in the $10 {\\rm \\, keV} - 1 {\\rm \\, MeV}$ range. These models can be tested by future X-ray telescopes, and in some cases will be almost entirely probed by searches for two-body decays $K \\to \\pi + {\\rm invis.}$ at NA62. As a byproduct, we discuss the cancellation of IR divergencies in flavor-violating scattering processes relevant for thermal axion production, and derive the general contribution to axion-photon couplings from all three light quarks.","sentences":["We propose simple scenarios where the observed dark matter abundance arises from decays and scatterings of heavy quarks through freeze-in of an axion-like particle with mass in the $10 {\\rm \\, keV} - 1 {\\rm \\, MeV}$ range.","These models can be tested by future X-ray telescopes, and in some cases will be almost entirely probed by searches for two-body decays $K \\to \\pi + {\\rm invis.}$ at NA62.","As a byproduct, we discuss the cancellation of IR divergencies in flavor-violating scattering processes relevant for thermal axion production, and derive the general contribution to axion-photon couplings from all three light quarks."],"url":"http://arxiv.org/abs/2404.12199v1","category":"hep-ph"}
{"created":"2024-04-18 14:01:05","title":"Mathematical analysis of a model-constrained inverse problem for the reconstruction of early states of prostate cancer growth","abstract":"The availability of cancer measurements over time enables the personalised assessment of tumour growth and therapeutic response dynamics. However, many tumours are treated after diagnosis without collecting longitudinal data, and cancer monitoring protocols may include infrequent measurements. To facilitate the estimation of disease dynamics and better guide ensuing clinical decisions, we investigate an inverse problem enabling the reconstruction of earlier tumour states by using a single spatial tumour dataset and a biomathematical model describing disease dynamics. We focus on prostate cancer, since aggressive cases of this disease are usually treated after diagnosis. We describe tumour dynamics with a phase-field model driven by a generic nutrient ruled by reaction-diffusion dynamics. The model is completed with another reaction-diffusion equation for the local production of prostate-specific antigen, which is a key prostate cancer biomarker. We first improve previous well-posedness results by further showing that the solution operator is continuously Fr\\'echet differentiable. We then analyse the backward inverse problem concerning the reconstruction of earlier tumour states starting from measurements of the model variables at the final time. Since this problem is severely ill-posed, only very weak conditional stability of logarithmic type can be recovered from the terminal data. However, by restricting the unknowns to a compact subset of a finite-dimensional subspace, we can derive an optimal Lipschitz stability estimate.","sentences":["The availability of cancer measurements over time enables the personalised assessment of tumour growth and therapeutic response dynamics.","However, many tumours are treated after diagnosis without collecting longitudinal data, and cancer monitoring protocols may include infrequent measurements.","To facilitate the estimation of disease dynamics and better guide ensuing clinical decisions, we investigate an inverse problem enabling the reconstruction of earlier tumour states by using a single spatial tumour dataset and a biomathematical model describing disease dynamics.","We focus on prostate cancer, since aggressive cases of this disease are usually treated after diagnosis.","We describe tumour dynamics with a phase-field model driven by a generic nutrient ruled by reaction-diffusion dynamics.","The model is completed with another reaction-diffusion equation for the local production of prostate-specific antigen, which is a key prostate cancer biomarker.","We first improve previous well-posedness results by further showing that the solution operator is continuously Fr\\'echet differentiable.","We then analyse the backward inverse problem concerning the reconstruction of earlier tumour states starting from measurements of the model variables at the final time.","Since this problem is severely ill-posed, only very weak conditional stability of logarithmic type can be recovered from the terminal data.","However, by restricting the unknowns to a compact subset of a finite-dimensional subspace, we can derive an optimal Lipschitz stability estimate."],"url":"http://arxiv.org/abs/2404.12198v1","category":"math.AP"}
{"created":"2024-04-18 13:57:28","title":"Ward Identities in a Two-Dimensional Gravitational Model: Anomalous Amplitude Revisited Using a Completely Regularization-Independent Mathematical Strategy","abstract":"We present a detailed investigation of the anomalous gravitational amplitude in a simple two-dimensional model with Weyl fermions. We employ a mathematical strategy that completely avoids any regularization prescription for handling divergent perturbative amplitudes. This strategy relies solely on the validity of the linearity of the integration operation and avoids modifying the amplitudes during intermediate calculations, unlike studies using regularization methods. Additionally, we adopt arbitrary routings for internal loop momenta, representing the most general analysis scenario. As expected, we show that surface terms play a crucial role in both preserving the symmetry properties of the amplitude and ensuring the mathematical consistency of the results. Notably, our final perturbative amplitude can be converted into the form obtained using any specific regularization prescription. We consider three common scenarios, one of which recovers the traditional results for gravitational anomalies. However, we demonstrate that this scenario inevitably breaks the linearity of integration, leading to an undesirable mathematical situation. This clean and transparent conclusion, enabled by the general nature of our strategy, would not be apparent in similar studies using regularization techniques.","sentences":["We present a detailed investigation of the anomalous gravitational amplitude in a simple two-dimensional model with Weyl fermions.","We employ a mathematical strategy that completely avoids any regularization prescription for handling divergent perturbative amplitudes.","This strategy relies solely on the validity of the linearity of the integration operation and avoids modifying the amplitudes during intermediate calculations, unlike studies using regularization methods.","Additionally, we adopt arbitrary routings for internal loop momenta, representing the most general analysis scenario.","As expected, we show that surface terms play a crucial role in both preserving the symmetry properties of the amplitude and ensuring the mathematical consistency of the results.","Notably, our final perturbative amplitude can be converted into the form obtained using any specific regularization prescription.","We consider three common scenarios, one of which recovers the traditional results for gravitational anomalies.","However, we demonstrate that this scenario inevitably breaks the linearity of integration, leading to an undesirable mathematical situation.","This clean and transparent conclusion, enabled by the general nature of our strategy, would not be apparent in similar studies using regularization techniques."],"url":"http://arxiv.org/abs/2404.12196v1","category":"hep-th"}
{"created":"2024-04-18 13:57:18","title":"OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data","abstract":"Instruction fine-tuning pretrained LLMs for diverse downstream tasks has demonstrated remarkable success and has captured the interest of both academics and practitioners. To ensure such fine-tuned LLMs align with human preferences, techniques such as RLHF and DPO have emerged. At the same time, there is increasing interest in smaller parameter counts for models. In this work, using OpenLLaMA 3Bv2 as a base model, we describe the recipe used to fine-tune the OpenBezoar family of models. In this recipe: We first generate synthetic instruction fine-tuning data using an open and commercially non-restrictive instruction fine-tuned variant of the Falcon-40B model under three schemes based on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a seed dataset) and Orca (with the Flan Collection as a seed dataset), then filter these generations using GPT-4 as a human proxy. We then perform cost-effective QLoRA-based supervised fine-tuning sequentially with each scheme. The resulting checkpoint is further fine-tuned with a subset of the HH-RLHF dataset to minimize distribution shift prior to using the DPO loss to obtain the final checkpoint. Evaluation is done with the LM Eval Harness tasks/metrics as well as on MT-Bench using the \"LLM-as-a-judge\" framework with Claude 2.1, with the finding that the final checkpoint, \"OpenBezoar-HH-RLHF-DPO\", demonstrates superior performance over many models at the 3B parameter scale, even outperforming the top model in one of the categories on the Huggingface Open LLM Leaderboard. We release \"OpenBezoar-SFT\", \"OpenBezoar-HH-RLHF-SFT\", \"OpenBezoar-HH-RLHF-DPO\" checkpoints, alongside our generated datasets on HuggingFace at https://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc and our codebase at https://bitbucket.org/paladinanalytics/workspace/projects/OP.","sentences":["Instruction fine-tuning pretrained LLMs for diverse downstream tasks has demonstrated remarkable success and has captured the interest of both academics and practitioners.","To ensure such fine-tuned LLMs align with human preferences, techniques such as RLHF and DPO have emerged.","At the same time, there is increasing interest in smaller parameter counts for models.","In this work, using OpenLLaMA 3Bv2 as a base model, we describe the recipe used to fine-tune the OpenBezoar family of models.","In this recipe: We first generate synthetic instruction fine-tuning data using an open and commercially non-restrictive instruction fine-tuned variant of the Falcon-40B model under three schemes based on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a seed dataset) and Orca (with the Flan Collection as a seed dataset), then filter these generations using GPT-4 as a human proxy.","We then perform cost-effective QLoRA-based supervised fine-tuning sequentially with each scheme.","The resulting checkpoint is further fine-tuned with a subset of the HH-RLHF dataset to minimize distribution shift prior to using the DPO loss to obtain the final checkpoint.","Evaluation is done with the LM Eval Harness tasks/metrics as well as on MT-Bench using the \"LLM-as-a-judge\" framework with Claude 2.1, with the finding that the final checkpoint, \"OpenBezoar-HH-RLHF-DPO\", demonstrates superior performance over many models at the 3B parameter scale, even outperforming the top model in one of the categories on the Huggingface Open LLM Leaderboard.","We release \"OpenBezoar-SFT\", \"OpenBezoar-HH-RLHF-SFT\", \"OpenBezoar-HH-RLHF-DPO\" checkpoints, alongside our generated datasets on HuggingFace at https://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc and our codebase at https://bitbucket.org/paladinanalytics/workspace/projects/OP."],"url":"http://arxiv.org/abs/2404.12195v1","category":"cs.CL"}
{"created":"2024-04-18 13:56:44","title":"Revisiting Buchdahl transformations: New static and rotating black holes in vacuum, double copy, and hairy extensions","abstract":"This paper investigates Buchdahl transformations within the framework of Einstein and Einstein-Scalar theories. Specifically, we establish that the recently proposed Schwarzschild-Levi-Civita spacetime can be obtained by means of a Buchdahl transformation of the Schwarschild metric along the spacelike Killing vector. The study extends Buchdahl's original theorem by combining it with the Kerr-Schild representation. In doing so, we construct new vacuum-rotating black holes in higher dimensions which can be viewed as the Levi-Civita extensions of the Myers-Perry geometries. Furthermore, it demonstrates that the double copy scheme within these new generated geometries still holds, providing an example of an algebraically general double copy framework. In the context of the Einstein-Scalar system, the paper extends the corresponding Buchdahl theorem to scenarios where a static vacuum seed configuration, transformed with respect to a spacelike Killing vector, generates a hairy black hole spacetime. We analyze the geometrical features of these spacetimes and investigate how a change of frame, via conformal transformations, leads to a new family of black hole spacetimes within the Einstein-Conformal-Scalar system.","sentences":["This paper investigates Buchdahl transformations within the framework of Einstein and Einstein-Scalar theories.","Specifically, we establish that the recently proposed Schwarzschild-Levi-Civita spacetime can be obtained by means of a Buchdahl transformation of the Schwarschild metric along the spacelike Killing vector.","The study extends Buchdahl's original theorem by combining it with the Kerr-Schild representation.","In doing so, we construct new vacuum-rotating black holes in higher dimensions which can be viewed as the Levi-Civita extensions of the Myers-Perry geometries.","Furthermore, it demonstrates that the double copy scheme within these new generated geometries still holds, providing an example of an algebraically general double copy framework.","In the context of the Einstein-Scalar system, the paper extends the corresponding Buchdahl theorem to scenarios where a static vacuum seed configuration, transformed with respect to a spacelike Killing vector, generates a hairy black hole spacetime.","We analyze the geometrical features of these spacetimes and investigate how a change of frame, via conformal transformations, leads to a new family of black hole spacetimes within the Einstein-Conformal-Scalar system."],"url":"http://arxiv.org/abs/2404.12194v1","category":"gr-qc"}
{"created":"2024-04-18 13:56:22","title":"Portrait comparison of binary and weighted Skill Relatedness Networks","abstract":"In this paper we compare Skill-Relatedness Networks (SRNs) for selected countries, that is to say statistically significant inter-industrial interactions representing latent skills exchanges derived from observed labor flows, a kind of industry spaces. Using data from Argentina (ARG), Germany (DEU) and Sweden (SWE), we compare their SRNs utilizing an information-theoretic method that permits to compare networks of \"non-aligned\" nodes, which is the case of interest. For each SRN we extract its portrait, a fingerprint of structural measures of the distributions of their shortest paths, and calculate their pairwise divergences. This allows us also to contrast differences in structural (binary) connectivity with differences in the information provided by the (weighted) skill relatedness indicator (SR). We find that, in the case of ARG, structural connectivity is very different from their counterpart in DEU and SWE, but through the glass of SR the distances analyzed are all substantially smaller and more alike. These results qualify the role of the SR indicator as revealing some hidden dimension different from connectivity alone, providing empirical support to the suggestion that industry spaces may differ across countries.","sentences":["In this paper we compare Skill-Relatedness Networks (SRNs) for selected countries, that is to say statistically significant inter-industrial interactions representing latent skills exchanges derived from observed labor flows, a kind of industry spaces.","Using data from Argentina (ARG), Germany (DEU) and Sweden (SWE), we compare their SRNs utilizing an information-theoretic method that permits to compare networks of \"non-aligned\" nodes, which is the case of interest.","For each SRN we extract its portrait, a fingerprint of structural measures of the distributions of their shortest paths, and calculate their pairwise divergences.","This allows us also to contrast differences in structural (binary) connectivity with differences in the information provided by the (weighted) skill relatedness indicator (SR).","We find that, in the case of ARG, structural connectivity is very different from their counterpart in DEU and SWE, but through the glass of SR the distances analyzed are all substantially smaller and more alike.","These results qualify the role of the SR indicator as revealing some hidden dimension different from connectivity alone, providing empirical support to the suggestion that industry spaces may differ across countries."],"url":"http://arxiv.org/abs/2404.12193v1","category":"physics.soc-ph"}
{"created":"2024-04-18 13:56:03","title":"Aligning Actions and Walking to LLM-Generated Textual Descriptions","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, including data augmentation and synthetic data generation. This work explores the use of LLMs to generate rich textual descriptions for motion sequences, encompassing both actions and walking patterns. We leverage the expressive power of LLMs to align motion representations with high-level linguistic cues, addressing two distinct tasks: action recognition and retrieval of walking sequences based on appearance attributes. For action recognition, we employ LLMs to generate textual descriptions of actions in the BABEL-60 dataset, facilitating the alignment of motion sequences with linguistic representations. In the domain of gait analysis, we investigate the impact of appearance attributes on walking patterns by generating textual descriptions of motion sequences from the DenseGait dataset using LLMs. These descriptions capture subtle variations in walking styles influenced by factors such as clothing choices and footwear. Our approach demonstrates the potential of LLMs in augmenting structured motion attributes and aligning multi-modal representations. The findings contribute to the advancement of comprehensive motion understanding and open up new avenues for leveraging LLMs in multi-modal alignment and data augmentation for motion analysis. We make the code publicly available at https://github.com/Radu1999/WalkAndText","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, including data augmentation and synthetic data generation.","This work explores the use of LLMs to generate rich textual descriptions for motion sequences, encompassing both actions and walking patterns.","We leverage the expressive power of LLMs to align motion representations with high-level linguistic cues, addressing two distinct tasks: action recognition and retrieval of walking sequences based on appearance attributes.","For action recognition, we employ LLMs to generate textual descriptions of actions in the BABEL-60 dataset, facilitating the alignment of motion sequences with linguistic representations.","In the domain of gait analysis, we investigate the impact of appearance attributes on walking patterns by generating textual descriptions of motion sequences from the DenseGait dataset using LLMs.","These descriptions capture subtle variations in walking styles influenced by factors such as clothing choices and footwear.","Our approach demonstrates the potential of LLMs in augmenting structured motion attributes and aligning multi-modal representations.","The findings contribute to the advancement of comprehensive motion understanding and open up new avenues for leveraging LLMs in multi-modal alignment and data augmentation for motion analysis.","We make the code publicly available at https://github.com/Radu1999/WalkAndText"],"url":"http://arxiv.org/abs/2404.12192v1","category":"cs.CV"}
{"created":"2024-04-18 13:54:23","title":"TMM$-$Sim: A Versatile Tool for Optical Simulation of Thin$-$Film Solar Cells","abstract":"The Transfer Matrix Method (TMM) has become a prominent tool for the optical simulation of thin$-$film solar cells, particularly among researchers specializing in organic semiconductors and perovskite materials. As the commercial viability of these solar cells continues to advance, driven by rapid developments in materials and production processes, the importance of optical simulation has grown significantly. By leveraging optical simulation, researchers can gain profound insights into photovoltaic phenomena, empowering the implementation of device optimization strategies to achieve enhanced performance. However, existing TMM$-$based packages exhibit limitations, such as requiring programming expertise, licensing fees, or lack of support for bilayer device simulation. In response to these gaps and challenges, we present the TMM Simulator (TMM$-$Sim), an intuitive and user$-$friendly tool to calculate essential photovoltaic parameters, including the optical electric field profile, exciton generation profile, fraction of light absorbed per layer, photocurrent, external quantum efficiency, internal quantum efficiency, and parasitic losses. An additional advantage of TMM$-$Sim lies in its capacity to generate outcomes suitable as input parameters for electro$-$optical device simulations. In this work, we offer a comprehensive guide, outlining a step$-$by$-$step process to use TMM$-$Sim, and provide a thorough analysis of the results. TMM$-$Sim is freely available, accessible through our web server (nanocalc.org), or downloadable from the TMM$-$Sim repository (for \\textit{Unix}, \\textit{Windows}, and \\textit{macOS}) on \\textit{GitHub}. With its user$-$friendly interface and powerful capabilities, TMM$-$Sim aims to facilitate and accelerate research in thin$-$film solar cells, fostering advancements in renewable energy technologies.","sentences":["The Transfer Matrix Method (TMM) has become a prominent tool for the optical simulation of thin$-$film solar cells, particularly among researchers specializing in organic semiconductors and perovskite materials.","As the commercial viability of these solar cells continues to advance, driven by rapid developments in materials and production processes, the importance of optical simulation has grown significantly.","By leveraging optical simulation, researchers can gain profound insights into photovoltaic phenomena, empowering the implementation of device optimization strategies to achieve enhanced performance.","However, existing TMM$-$based packages exhibit limitations, such as requiring programming expertise, licensing fees, or lack of support for bilayer device simulation.","In response to these gaps and challenges, we present the TMM Simulator (TMM$-$Sim), an intuitive and user$-$friendly tool to calculate essential photovoltaic parameters, including the optical electric field profile, exciton generation profile, fraction of light absorbed per layer, photocurrent, external quantum efficiency, internal quantum efficiency, and parasitic losses.","An additional advantage of TMM$-$Sim lies in its capacity to generate outcomes suitable as input parameters for electro$-$optical device simulations.","In this work, we offer a comprehensive guide, outlining a step$-$by$-$step process to use TMM$-$Sim, and provide a thorough analysis of the results.","TMM$-$Sim is freely available, accessible through our web server (nanocalc.org), or downloadable from the TMM$-$Sim repository (for \\textit{Unix}, \\textit{Windows}, and \\textit{macOS}) on \\textit{GitHub}.","With its user$-$friendly interface and powerful capabilities, TMM$-$Sim aims to facilitate and accelerate research in thin$-$film solar cells, fostering advancements in renewable energy technologies."],"url":"http://arxiv.org/abs/2404.12191v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 13:53:32","title":"Estimating the Hessian Matrix of Ranking Objectives for Stochastic Learning to Rank with Gradient Boosted Trees","abstract":"Stochastic learning to rank (LTR) is a recent branch in the LTR field that concerns the optimization of probabilistic ranking models. Their probabilistic behavior enables certain ranking qualities that are impossible with deterministic models. For example, they can increase the diversity of displayed documents, increase fairness of exposure over documents, and better balance exploitation and exploration through randomization. A core difficulty in LTR is gradient estimation, for this reason, existing stochastic LTR methods have been limited to differentiable ranking models (e.g., neural networks). This is in stark contrast with the general field of LTR where Gradient Boosted Decision Trees (GBDTs) have long been considered the state-of-the-art.   In this work, we address this gap by introducing the first stochastic LTR method for GBDTs. Our main contribution is a novel estimator for the second-order derivatives, i.e., the Hessian matrix, which is a requirement for effective GBDTs. To efficiently compute both the first and second-order derivatives simultaneously, we incorporate our estimator into the existing PL-Rank framework, which was originally designed for first-order derivatives only. Our experimental results indicate that stochastic LTR without the Hessian has extremely poor performance, whilst the performance is competitive with the current state-of-the-art with our estimated Hessian. Thus, through the contribution of our novel Hessian estimation method, we have successfully introduced GBDTs to stochastic LTR.","sentences":["Stochastic learning to rank (LTR) is a recent branch in the LTR field that concerns the optimization of probabilistic ranking models.","Their probabilistic behavior enables certain ranking qualities that are impossible with deterministic models.","For example, they can increase the diversity of displayed documents, increase fairness of exposure over documents, and better balance exploitation and exploration through randomization.","A core difficulty in LTR is gradient estimation, for this reason, existing stochastic LTR methods have been limited to differentiable ranking models (e.g., neural networks).","This is in stark contrast with the general field of LTR where Gradient Boosted Decision Trees (GBDTs) have long been considered the state-of-the-art.   ","In this work, we address this gap by introducing the first stochastic LTR method for GBDTs.","Our main contribution is a novel estimator for the second-order derivatives, i.e., the Hessian matrix, which is a requirement for effective GBDTs.","To efficiently compute both the first and second-order derivatives simultaneously, we incorporate our estimator into the existing PL-Rank framework, which was originally designed for first-order derivatives only.","Our experimental results indicate that stochastic LTR without the Hessian has extremely poor performance, whilst the performance is competitive with the current state-of-the-art with our estimated Hessian.","Thus, through the contribution of our novel Hessian estimation method, we have successfully introduced GBDTs to stochastic LTR."],"url":"http://arxiv.org/abs/2404.12190v1","category":"cs.LG"}
{"created":"2024-04-18 13:50:20","title":"Multi-material topology optimization of an electric machine considering demagnetization","abstract":"We consider the topology optimization problem of a 2d permanent magnet synchronous machine in magnetostatic operation with demagnetization. This amounts to a PDE-constrained multi-material design optimization problem with an additional pointwise state constraint. Using a generic framework we can incorporate this additional constraint and compute the corresponding topological derivative. We present and discuss optimization results obtained by a multi-material level set algorithm.","sentences":["We consider the topology optimization problem of a 2d permanent magnet synchronous machine in magnetostatic operation with demagnetization.","This amounts to a PDE-constrained multi-material design optimization problem with an additional pointwise state constraint.","Using a generic framework we can incorporate this additional constraint and compute the corresponding topological derivative.","We present and discuss optimization results obtained by a multi-material level set algorithm."],"url":"http://arxiv.org/abs/2404.12188v1","category":"math.OC"}
{"created":"2024-04-18 13:47:53","title":"An Adaptive Metaheuristic Framework for Changing Environments","abstract":"The rapidly changing landscapes of modern optimization problems require algorithms that can be adapted in real-time. This paper introduces an Adaptive Metaheuristic Framework (AMF) designed for dynamic environments. It is capable of intelligently adapting to changes in the problem parameters. The AMF combines a dynamic representation of problems, a real-time sensing system, and adaptive techniques to navigate continuously changing optimization environments. Through a simulated dynamic optimization problem, the AMF's capability is demonstrated to detect environmental changes and proactively adjust its search strategy. This framework utilizes a differential evolution algorithm that is improved with an adaptation module that adjusts solutions in response to detected changes. The capability of the AMF to adjust is tested through a series of iterations, demonstrating its resilience and robustness in sustaining solution quality despite the problem's development. The effectiveness of AMF is demonstrated through a series of simulations on a dynamic optimization problem. Robustness and agility characterize the algorithm's performance, as evidenced by the presented fitness evolution and solution path visualizations. The findings show that AMF is a practical solution to dynamic optimization and a major step forward in the creation of algorithms that can handle the unpredictability of real-world problems.","sentences":["The rapidly changing landscapes of modern optimization problems require algorithms that can be adapted in real-time.","This paper introduces an Adaptive Metaheuristic Framework (AMF) designed for dynamic environments.","It is capable of intelligently adapting to changes in the problem parameters.","The AMF combines a dynamic representation of problems, a real-time sensing system, and adaptive techniques to navigate continuously changing optimization environments.","Through a simulated dynamic optimization problem, the AMF's capability is demonstrated to detect environmental changes and proactively adjust its search strategy.","This framework utilizes a differential evolution algorithm that is improved with an adaptation module that adjusts solutions in response to detected changes.","The capability of the AMF to adjust is tested through a series of iterations, demonstrating its resilience and robustness in sustaining solution quality despite the problem's development.","The effectiveness of AMF is demonstrated through a series of simulations on a dynamic optimization problem.","Robustness and agility characterize the algorithm's performance, as evidenced by the presented fitness evolution and solution path visualizations.","The findings show that AMF is a practical solution to dynamic optimization and a major step forward in the creation of algorithms that can handle the unpredictability of real-world problems."],"url":"http://arxiv.org/abs/2404.12185v1","category":"cs.AI"}
{"created":"2024-04-18 13:32:29","title":"Designing a sector-coupled European energy system robust to 60 years of historical weather data","abstract":"As energy systems transform to rely on renewable energy and electrification, they encounter stronger year-to-year variability in energy supply and demand. However, most infrastructure planning is based on a single weather year, resulting in a lack of robustness. In this paper, we optimize energy infrastructure for a European energy system designed for net-zero CO$_2$ emissions in 62 different weather years. Subsequently, we fix the capacity layouts and simulate their operation in every weather year, to evaluate resource adequacy and CO$_2$ emissions abatement. We show that interannual weather variability causes variation of $\\pm$10\\% in total system cost. The most expensive capacity layout obtains the lowest net CO$_2$ emissions but not the highest resource adequacy. Instead, capacity layouts designed with years including compound weather events result in a more robust and cost-effective design. Deploying CO$_2$-emitting backup generation is a cost-effective robustness measure, which only increase CO$_2$ emissions marginally as the average CO$_2$ emissions remain less than 1\\% of 1990 levels. Our findings highlight how extreme weather years drive investments in robustness measures, making them compatible with all weather conditions within six decades of historical weather data.","sentences":["As energy systems transform to rely on renewable energy and electrification, they encounter stronger year-to-year variability in energy supply and demand.","However, most infrastructure planning is based on a single weather year, resulting in a lack of robustness.","In this paper, we optimize energy infrastructure for a European energy system designed for net-zero CO$_2$ emissions in 62 different weather years.","Subsequently, we fix the capacity layouts and simulate their operation in every weather year, to evaluate resource adequacy and CO$_2$ emissions abatement.","We show that interannual weather variability causes variation of $\\pm$10\\% in total system cost.","The most expensive capacity layout obtains the lowest net CO$_2$ emissions but not the highest resource adequacy.","Instead, capacity layouts designed with years including compound weather events result in a more robust and cost-effective design.","Deploying CO$_2$-emitting backup generation is a cost-effective robustness measure, which only increase CO$_2$ emissions marginally as the average CO$_2$ emissions remain less than 1\\% of 1990 levels.","Our findings highlight how extreme weather years drive investments in robustness measures, making them compatible with all weather conditions within six decades of historical weather data."],"url":"http://arxiv.org/abs/2404.12178v1","category":"physics.soc-ph"}
{"created":"2024-04-18 13:27:29","title":"How to Benchmark Vision Foundation Models for Semantic Segmentation?","abstract":"Recent vision foundation models (VFMs) have demonstrated proficiency in various tasks but require supervised fine-tuning to perform the task of semantic segmentation effectively. Benchmarking their performance is essential for selecting current models and guiding future model developments for this task. The lack of a standardized benchmark complicates comparisons. Therefore, the primary objective of this paper is to study how VFMs should be benchmarked for semantic segmentation. To do so, various VFMs are fine-tuned under various settings, and the impact of individual settings on the performance ranking and training time is assessed. Based on the results, the recommendation is to fine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear decoder, as these settings are representative of using a larger model, more advanced decoder and smaller patch size, while reducing training time by more than 13 times. Using multiple datasets for training and evaluation is also recommended, as the performance ranking across datasets and domain shifts varies. Linear probing, a common practice for some VFMs, is not recommended, as it is not representative of end-to-end fine-tuning. The benchmarking setup recommended in this paper enables a performance analysis of VFMs for semantic segmentation. The findings of such an analysis reveal that pretraining with promptable segmentation is not beneficial, whereas masked image modeling (MIM) with abstract representations is crucial, even more important than the type of supervision used. The code for efficiently fine-tuning VFMs for semantic segmentation can be accessed through the project page at: https://tue-mps.github.io/benchmark-vfm-ss/.","sentences":["Recent vision foundation models (VFMs) have demonstrated proficiency in various tasks but require supervised fine-tuning to perform the task of semantic segmentation effectively.","Benchmarking their performance is essential for selecting current models and guiding future model developments for this task.","The lack of a standardized benchmark complicates comparisons.","Therefore, the primary objective of this paper is to study how VFMs should be benchmarked for semantic segmentation.","To do so, various VFMs are fine-tuned under various settings, and the impact of individual settings on the performance ranking and training time is assessed.","Based on the results, the recommendation is to fine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear decoder, as these settings are representative of using a larger model, more advanced decoder and smaller patch size, while reducing training time by more than 13 times.","Using multiple datasets for training and evaluation is also recommended, as the performance ranking across datasets and domain shifts varies.","Linear probing, a common practice for some VFMs, is not recommended, as it is not representative of end-to-end fine-tuning.","The benchmarking setup recommended in this paper enables a performance analysis of VFMs for semantic segmentation.","The findings of such an analysis reveal that pretraining with promptable segmentation is not beneficial, whereas masked image modeling (MIM) with abstract representations is crucial, even more important than the type of supervision used.","The code for efficiently fine-tuning VFMs for semantic segmentation can be accessed through the project page at: https://tue-mps.github.io/benchmark-vfm-ss/."],"url":"http://arxiv.org/abs/2404.12172v1","category":"cs.CV"}
{"created":"2024-04-18 13:23:38","title":"Secure Semantic Communication for Image Transmission in the Presence of Eavesdroppers","abstract":"Semantic communication (SemCom) has emerged as a key technology for the forthcoming sixth-generation (6G) network, attributed to its enhanced communication efficiency and robustness against channel noise. However, the open nature of wireless channels renders them vulnerable to eavesdropping, posing a serious threat to privacy. To address this issue, we propose a novel secure semantic communication (SemCom) approach for image transmission, which integrates steganography technology to conceal private information within non-private images (host images). Specifically, we propose an invertible neural network (INN)-based signal steganography approach, which embeds channel input signals of a private image into those of a host image before transmission. This ensures that the original private image can be reconstructed from the received signals at the legitimate receiver, while the eavesdropper can only decode the information of the host image. Simulation results demonstrate that the proposed approach maintains comparable reconstruction quality of both host and private images at the legitimate receiver, compared to scenarios without any secure mechanisms. Experiments also show that the eavesdropper is only able to reconstruct host images, showcasing the enhanced security provided by our approach.","sentences":["Semantic communication (SemCom) has emerged as a key technology for the forthcoming sixth-generation (6G) network, attributed to its enhanced communication efficiency and robustness against channel noise.","However, the open nature of wireless channels renders them vulnerable to eavesdropping, posing a serious threat to privacy.","To address this issue, we propose a novel secure semantic communication (SemCom) approach for image transmission, which integrates steganography technology to conceal private information within non-private images (host images).","Specifically, we propose an invertible neural network (INN)-based signal steganography approach, which embeds channel input signals of a private image into those of a host image before transmission.","This ensures that the original private image can be reconstructed from the received signals at the legitimate receiver, while the eavesdropper can only decode the information of the host image.","Simulation results demonstrate that the proposed approach maintains comparable reconstruction quality of both host and private images at the legitimate receiver, compared to scenarios without any secure mechanisms.","Experiments also show that the eavesdropper is only able to reconstruct host images, showcasing the enhanced security provided by our approach."],"url":"http://arxiv.org/abs/2404.12170v1","category":"eess.SP"}
{"created":"2024-04-18 13:23:05","title":"Shotit: compute-efficient image-to-video search engine for the cloud","abstract":"With the rapid growth of information technology, users are exposed to a massive amount of data online, including image, music, and video. This has led to strong needs to provide effective corresponsive search services such as image, music, and video search services. Most of them are operated based on keywords, namely using keywords to find related image, music, and video. Additionally, there are image-to-image search services that enable users to find similar images using one input image. Given that videos are essentially composed of image frames, then similar videos can be searched by one input image or screenshot. We want to target this scenario and provide an efficient method and implementation in this paper.   We present Shotit, a cloud-native image-to-video search engine that tailors this search scenario in a compute-efficient approach. One main limitation faced in this scenario is the scale of its dataset. A typical image-to-image search engine only handles one-to-one relationships, colloquially, one image corresponds to another single image. But image-to-video proliferates. Take a 24-min length video as an example, it will generate roughly 20,000 image frames. As the number of videos grows, the scale of the dataset explodes exponentially. In this case, a compute-efficient approach ought to be considered, and the system design should cater to the cloud-native trend. Choosing an emerging technology - vector database as its backbone, Shotit fits these two metrics performantly. Experiments for two different datasets, a 50 thousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary TV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with object storage reveal the effectiveness of Shotit. A demo regarding the Blender Open Movie dataset is illustrated within this paper.","sentences":["With the rapid growth of information technology, users are exposed to a massive amount of data online, including image, music, and video.","This has led to strong needs to provide effective corresponsive search services such as image, music, and video search services.","Most of them are operated based on keywords, namely using keywords to find related image, music, and video.","Additionally, there are image-to-image search services that enable users to find similar images using one input image.","Given that videos are essentially composed of image frames, then similar videos can be searched by one input image or screenshot.","We want to target this scenario and provide an efficient method and implementation in this paper.   ","We present Shotit, a cloud-native image-to-video search engine that tailors this search scenario in a compute-efficient approach.","One main limitation faced in this scenario is the scale of its dataset.","A typical image-to-image search engine only handles one-to-one relationships, colloquially, one image corresponds to another single image.","But image-to-video proliferates.","Take a 24-min length video as an example, it will generate roughly 20,000 image frames.","As the number of videos grows, the scale of the dataset explodes exponentially.","In this case, a compute-efficient approach ought to be considered, and the system design should cater to the cloud-native trend.","Choosing an emerging technology - vector database as its backbone, Shotit fits these two metrics performantly.","Experiments for two different datasets, a 50 thousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary TV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with object storage reveal the effectiveness of Shotit.","A demo regarding the Blender Open Movie dataset is illustrated within this paper."],"url":"http://arxiv.org/abs/2404.12169v1","category":"cs.MM"}
{"created":"2024-04-18 13:22:56","title":"Real-World Efficient Blind Motion Deblurring via Blur Pixel Discretization","abstract":"As recent advances in mobile camera technology have enabled the capability to capture high-resolution images, such as 4K images, the demand for an efficient deblurring model handling large motion has increased. In this paper, we discover that the image residual errors, i.e., blur-sharp pixel differences, can be grouped into some categories according to their motion blur type and how complex their neighboring pixels are. Inspired by this, we decompose the deblurring (regression) task into blur pixel discretization (pixel-level blur classification) and discrete-to-continuous conversion (regression with blur class map) tasks. Specifically, we generate the discretized image residual errors by identifying the blur pixels and then transform them to a continuous form, which is computationally more efficient than naively solving the original regression problem with continuous values. Here, we found that the discretization result, i.e., blur segmentation map, remarkably exhibits visual similarity with the image residual errors. As a result, our efficient model shows comparable performance to state-of-the-art methods in realistic benchmarks, while our method is up to 10 times computationally more efficient.","sentences":["As recent advances in mobile camera technology have enabled the capability to capture high-resolution images, such as 4K images, the demand for an efficient deblurring model handling large motion has increased.","In this paper, we discover that the image residual errors, i.e., blur-sharp pixel differences, can be grouped into some categories according to their motion blur type and how complex their neighboring pixels are.","Inspired by this, we decompose the deblurring (regression) task into blur pixel discretization (pixel-level blur classification) and discrete-to-continuous conversion (regression with blur class map) tasks.","Specifically, we generate the discretized image residual errors by identifying the blur pixels and then transform them to a continuous form, which is computationally more efficient than naively solving the original regression problem with continuous values.","Here, we found that the discretization result, i.e., blur segmentation map, remarkably exhibits visual similarity with the image residual errors.","As a result, our efficient model shows comparable performance to state-of-the-art methods in realistic benchmarks, while our method is up to 10 times computationally more efficient."],"url":"http://arxiv.org/abs/2404.12168v1","category":"cs.CV"}
{"created":"2024-04-18 13:19:26","title":"Quantum trajectory entanglement in various unravelings of Markovian dynamics","abstract":"The cost of classical simulations of quantum many-body dynamics is often determined by the amount of entanglement in the system. In this paper, we study entanglement in stochastic quantum trajectory approaches that solve master equations describing open quantum system dynamics. First, we introduce and compare adaptive trajectory unravelings of master equations. Specifically, building on Ref. [Phys. Rev. Lett. 128, 243601 (2022)], we study several greedy algorithms that generate trajectories with a low average entanglement entropy. Second, we consider various conventional unravelings of a one-dimensional open random Brownian circuit and locate the transition points from area- to volume-law-entangled trajectories. Third, we compare various trajectory unravelings using matrix product states with a direct integration of the master equation using matrix product operators. We provide concrete examples of dynamics, for which the simulation cost of stochastic trajectories is exponentially smaller than the one of matrix product operators.","sentences":["The cost of classical simulations of quantum many-body dynamics is often determined by the amount of entanglement in the system.","In this paper, we study entanglement in stochastic quantum trajectory approaches that solve master equations describing open quantum system dynamics.","First, we introduce and compare adaptive trajectory unravelings of master equations.","Specifically, building on Ref.","[Phys. Rev. Lett.","128, 243601 (2022)], we study several greedy algorithms that generate trajectories with a low average entanglement entropy.","Second, we consider various conventional unravelings of a one-dimensional open random Brownian circuit and locate the transition points from area- to volume-law-entangled trajectories.","Third, we compare various trajectory unravelings using matrix product states with a direct integration of the master equation using matrix product operators.","We provide concrete examples of dynamics, for which the simulation cost of stochastic trajectories is exponentially smaller than the one of matrix product operators."],"url":"http://arxiv.org/abs/2404.12167v1","category":"quant-ph"}
{"created":"2024-04-18 13:15:44","title":"Stability Certificates for Receding Horizon Games","abstract":"Game-theoretic MPC (or Receding Horizon Games) is an emerging control methodology for multi-agent systems that generates control actions by solving a dynamic game with coupling constraints in a receding-horizon fashion. This control paradigm has recently received an increasing attention in various application fields, including robotics, autonomous driving, traffic networks, and energy grids, due to its ability to model the competitive nature of self-interested agents with shared resources while incorporating future predictions, dynamic models, and constraints into the decision-making process. In this work, we present the first formal stability analysis based on dissipativity and monotone operator theory that is valid also for non-potential games. Specifically, we derive LMI-based certificates that ensure asymptotic stability and are numerically verifiable. Moreover, we show that, if the agents have decoupled dynamics, the numerical verification can be performed in a scalable manner. Finally, we present tuning guidelines for the agents' cost function weights to fulfill the certificates and, thus, ensure stability.","sentences":["Game-theoretic MPC (or Receding Horizon Games) is an emerging control methodology for multi-agent systems that generates control actions by solving a dynamic game with coupling constraints in a receding-horizon fashion.","This control paradigm has recently received an increasing attention in various application fields, including robotics, autonomous driving, traffic networks, and energy grids, due to its ability to model the competitive nature of self-interested agents with shared resources while incorporating future predictions, dynamic models, and constraints into the decision-making process.","In this work, we present the first formal stability analysis based on dissipativity and monotone operator theory that is valid also for non-potential games.","Specifically, we derive LMI-based certificates that ensure asymptotic stability and are numerically verifiable.","Moreover, we show that, if the agents have decoupled dynamics, the numerical verification can be performed in a scalable manner.","Finally, we present tuning guidelines for the agents' cost function weights to fulfill the certificates and, thus, ensure stability."],"url":"http://arxiv.org/abs/2404.12165v1","category":"eess.SY"}
{"created":"2024-04-18 13:15:34","title":"Energetic particle acceleration and transport with the novel Icarus$+$PARADISE model","abstract":"With the rise of satellites and mankind's growing dependence on technology, there is an increasing awareness of space weather phenomena related to high-energy particles. Shock waves driven by coronal mass ejections (CMEs) and corotating interaction regions (CIRs) occasionally act as potent particle accelerators, generating hazardous solar energetic particles (SEPs) that pose risks to satellite electronics and astronauts. Numerical simulation tools capable of modelling and predicting large SEP events are thus highly demanded. We introduce the new Icarus$+$PARADISE model as an advancement of the previous EUHFORIA$+$PARADISE model. Icarus, based on the MPI-AMRVAC framework, is a three-dimensional magnetohydrodynamic code that models solar wind configurations from 0.1 au onwards, encompassing transient structures like CMEs or CIRs. Differing from EUHFORIA's uniform-only grid, Icarus incorporates solution adaptive mesh refinement (AMR) and grid stretching. The particle transport code PARADISE propagates energetic particles as test particles through these solar wind configurations by solving the focused transport equation in a stochastic manner. We validate our new model by reproducing EUHFORIA+PARADISE results. This is done by modelling the acceleration and transport of energetic particles in a synthetic solar wind configuration containing an embedded CIR. Subsequently, we illustrate how the simulation results vary with grid resolution by employing different levels of AMR. The resulting intensity profiles illustrate increased particle acceleration with higher levels of AMR in the shock region, better capturing the effects of the shock.","sentences":["With the rise of satellites and mankind's growing dependence on technology, there is an increasing awareness of space weather phenomena related to high-energy particles.","Shock waves driven by coronal mass ejections (CMEs) and corotating interaction regions (CIRs) occasionally act as potent particle accelerators, generating hazardous solar energetic particles (SEPs) that pose risks to satellite electronics and astronauts.","Numerical simulation tools capable of modelling and predicting large SEP events are thus highly demanded.","We introduce the new Icarus$+$PARADISE model as an advancement of the previous EUHFORIA$+$PARADISE model.","Icarus, based on the MPI-AMRVAC framework, is a three-dimensional magnetohydrodynamic code that models solar wind configurations from 0.1 au onwards, encompassing transient structures like CMEs or CIRs.","Differing from EUHFORIA's uniform-only grid, Icarus incorporates solution adaptive mesh refinement (AMR) and grid stretching.","The particle transport code PARADISE propagates energetic particles as test particles through these solar wind configurations by solving the focused transport equation in a stochastic manner.","We validate our new model by reproducing EUHFORIA+PARADISE results.","This is done by modelling the acceleration and transport of energetic particles in a synthetic solar wind configuration containing an embedded CIR.","Subsequently, we illustrate how the simulation results vary with grid resolution by employing different levels of AMR.","The resulting intensity profiles illustrate increased particle acceleration with higher levels of AMR in the shock region, better capturing the effects of the shock."],"url":"http://arxiv.org/abs/2404.12164v1","category":"physics.space-ph"}
{"created":"2024-04-18 13:15:08","title":"Hyperbolic spaces that detect all strongly-contracting directions","abstract":"Given a geodesic metric space $X$, we construct a corresponding hyperbolic space, which we call the contraction space, that detects all strongly contracting directions in the following sense; a geodesic in $X$ is strongly contracting if and only if its parametrized image in the contraction space is a quasi-geodesic. If a finitely generated group $G$ acts geometrically on $X$, then all strongly-contracting elements act as WPD elements on the contraction space. If the space $X$ is CAT(0), or more generally Morse-dichotomous, that is if all Morse geodesics are strongly-contracting, then all generalized loxodromics act as WPD elements, implying that the action is what we call ``universally WPD''.","sentences":["Given a geodesic metric space $X$, we construct a corresponding hyperbolic space, which we call the contraction space, that detects all strongly contracting directions in the following sense; a geodesic in $X$ is strongly contracting if and only if its parametrized image in the contraction space is a quasi-geodesic.","If a finitely generated group $G$ acts geometrically on $X$, then all strongly-contracting elements act as WPD elements on the contraction space.","If the space $X$ is CAT(0), or more generally Morse-dichotomous, that is if all Morse geodesics are strongly-contracting, then all generalized loxodromics act as WPD elements, implying that the action is what we call ``universally WPD''."],"url":"http://arxiv.org/abs/2404.12162v1","category":"math.GR"}
{"created":"2024-04-18 13:10:21","title":"A general alternating direction implicit iteration method for solving complex symmetric linear systems","abstract":"We have introduced the generalized alternating direction implicit iteration (GADI) method for solving large sparse complex symmetric linear systems and proved its convergence properties. Additionally, some numerical results have demonstrated the effectiveness of this algorithm. Furthermore, as an application of the GADI method in solving complex symmetric linear systems, we utilized the flattening operator and Kronecker product properties to solve Lyapunov and Riccati equations with complex coefficients using the GADI method. In solving the Riccati equation, we combined inner and outer iterations, first simplifying the Riccati equation into a Lyapunov equation using the Newton method, and then applying the GADI method for solution. Finally, we provided convergence analysis of the method and corresponding numerical results.","sentences":["We have introduced the generalized alternating direction implicit iteration (GADI) method for solving large sparse complex symmetric linear systems and proved its convergence properties.","Additionally, some numerical results have demonstrated the effectiveness of this algorithm.","Furthermore, as an application of the GADI method in solving complex symmetric linear systems, we utilized the flattening operator and Kronecker product properties to solve Lyapunov and Riccati equations with complex coefficients using the GADI method.","In solving the Riccati equation, we combined inner and outer iterations, first simplifying the Riccati equation into a Lyapunov equation using the Newton method, and then applying the GADI method for solution.","Finally, we provided convergence analysis of the method and corresponding numerical results."],"url":"http://arxiv.org/abs/2404.12160v1","category":"math.NA"}
{"created":"2024-04-18 13:09:34","title":"Localization in boundary-driven lattice models","abstract":"Several systems may display an equilibrium condensation transition, where a finite fraction of a conserved quantity is spatially localized. The presence of two conservation laws may induce the emergence of such transition in an out-of-equilibrium setup, where boundaries are attached to two different and subcritical heat baths. We study this phenomenon in a class of stochastic lattice models, where the local energy is a general convex function of the local mass, mass and energy being both globally conserved in the isolated system. We obtain exact results for the nonequilibrium steady state (spatial profiles, mass and energy currents, Onsager coefficients) and we highlight important differences between equilibrium and out-of-equilibrium condensation.","sentences":["Several systems may display an equilibrium condensation transition, where a finite fraction of a conserved quantity is spatially localized.","The presence of two conservation laws may induce the emergence of such transition in an out-of-equilibrium setup, where boundaries are attached to two different and subcritical heat baths.","We study this phenomenon in a class of stochastic lattice models, where the local energy is a general convex function of the local mass, mass and energy being both globally conserved in the isolated system.","We obtain exact results for the nonequilibrium steady state (spatial profiles, mass and energy currents, Onsager coefficients) and we highlight important differences between equilibrium and out-of-equilibrium condensation."],"url":"http://arxiv.org/abs/2404.12159v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-18 13:05:22","title":"Protecting gauge symmetries in the the dynamics of SU(3) lattice gauge theories","abstract":"Quantum simulation of the dynamics of a lattice gauge theory demands imposing on-site constraints. Ideally, the dynamics remain confined within the physical Hilbert space, where all the states satisfy those constraints. For non-Abelian gauge theories, implementing local constraints is non-trivial, as is keeping the dynamics confined in the physical Hilbert space, considering the erroneous quantum devices. SU(3) gauge group, albeit crucial for describing the strong interaction of nature, is notorious for studying via Hamiltonian simulation. This work presents a couple of symmetry protection protocols for simulating the exact dynamics of SU(3) gauge theory in $1+1$ dimension. The first protocol doesn't require imposing any local symmetry but relies on protecting global symmetries, which are Abelian with a preferred choice of framework, namely the loop-string-hadron framework. Generalization to a higher dimension is possible, however, the protection scheme needs to be local for that case but is still Abelian and thus advantageous. The symmetry protection schemes presented here are important steps towards quantum simulating the full theory of quantum chromodynamics.","sentences":["Quantum simulation of the dynamics of a lattice gauge theory demands imposing on-site constraints.","Ideally, the dynamics remain confined within the physical Hilbert space, where all the states satisfy those constraints.","For non-Abelian gauge theories, implementing local constraints is non-trivial, as is keeping the dynamics confined in the physical Hilbert space, considering the erroneous quantum devices.","SU(3) gauge group, albeit crucial for describing the strong interaction of nature, is notorious for studying via Hamiltonian simulation.","This work presents a couple of symmetry protection protocols for simulating the exact dynamics of SU(3) gauge theory in $1+1$ dimension.","The first protocol doesn't require imposing any local symmetry but relies on protecting global symmetries, which are Abelian with a preferred choice of framework, namely the loop-string-hadron framework.","Generalization to a higher dimension is possible, however, the protection scheme needs to be local for that case but is still Abelian and thus advantageous.","The symmetry protection schemes presented here are important steps towards quantum simulating the full theory of quantum chromodynamics."],"url":"http://arxiv.org/abs/2404.12158v1","category":"hep-lat"}
{"created":"2024-04-18 12:58:55","title":"StyleBooth: Image Style Editing with Multimodal Instruction","abstract":"Given an original image, image editing aims to generate an image that align with the provided instruction. The challenges are to accept multimodal inputs as instructions and a scarcity of high-quality training data, including crucial triplets of source/target image pairs and multimodal (text and image) instructions. In this paper, we focus on image style editing and present StyleBooth, a method that proposes a comprehensive framework for image editing and a feasible strategy for building a high-quality style editing dataset. We integrate encoded textual instruction and image exemplar as a unified condition for diffusion model, enabling the editing of original image following multimodal instructions. Furthermore, by iterative style-destyle tuning and editing and usability filtering, the StyleBooth dataset provides content-consistent stylized/plain image pairs in various categories of styles. To show the flexibility of StyleBooth, we conduct experiments on diverse tasks, such as text-based style editing, exemplar-based style editing and compositional style editing. The results demonstrate that the quality and variety of training data significantly enhance the ability to preserve content and improve the overall quality of generated images in editing tasks. Project page can be found at https://ali-vilab.github.io/stylebooth-page/.","sentences":["Given an original image, image editing aims to generate an image that align with the provided instruction.","The challenges are to accept multimodal inputs as instructions and a scarcity of high-quality training data, including crucial triplets of source/target image pairs and multimodal (text and image) instructions.","In this paper, we focus on image style editing and present StyleBooth, a method that proposes a comprehensive framework for image editing and a feasible strategy for building a high-quality style editing dataset.","We integrate encoded textual instruction and image exemplar as a unified condition for diffusion model, enabling the editing of original image following multimodal instructions.","Furthermore, by iterative style-destyle tuning and editing and usability filtering, the StyleBooth dataset provides content-consistent stylized/plain image pairs in various categories of styles.","To show the flexibility of StyleBooth, we conduct experiments on diverse tasks, such as text-based style editing, exemplar-based style editing and compositional style editing.","The results demonstrate that the quality and variety of training data significantly enhance the ability to preserve content and improve the overall quality of generated images in editing tasks.","Project page can be found at https://ali-vilab.github.io/stylebooth-page/."],"url":"http://arxiv.org/abs/2404.12154v1","category":"cs.CV"}
{"created":"2024-04-18 12:58:36","title":"FecTek: Enhancing Term Weight in Lexicon-Based Retrieval with Feature Context and Term-level Knowledge","abstract":"Lexicon-based retrieval has gained siginificant popularity in text retrieval due to its efficient and robust performance. To further enhance performance of lexicon-based retrieval, researchers have been diligently incorporating state-of-the-art methodologies like Neural retrieval and text-level contrastive learning approaches. Nonetheless, despite the promising outcomes, current lexicon-based retrieval methods have received limited attention in exploring the potential benefits of feature context representations and term-level knowledge guidance. In this paper, we introduce an innovative method by introducing FEature Context and TErm-level Knowledge modules(FecTek). To effectively enrich the feature context representations of term weight, the Feature Context Module (FCM) is introduced, which leverages the power of BERT's representation to determine dynamic weights for each element in the embedding. Additionally, we develop a term-level knowledge guidance module (TKGM) for effectively utilizing term-level knowledge to intelligently guide the modeling process of term weight. Evaluation of the proposed method on MS Marco benchmark demonstrates its superiority over the previous state-of-the-art approaches.","sentences":["Lexicon-based retrieval has gained siginificant popularity in text retrieval due to its efficient and robust performance.","To further enhance performance of lexicon-based retrieval, researchers have been diligently incorporating state-of-the-art methodologies like Neural retrieval and text-level contrastive learning approaches.","Nonetheless, despite the promising outcomes, current lexicon-based retrieval methods have received limited attention in exploring the potential benefits of feature context representations and term-level knowledge guidance.","In this paper, we introduce an innovative method by introducing FEature Context and TErm-level Knowledge modules(FecTek).","To effectively enrich the feature context representations of term weight, the Feature Context Module (FCM) is introduced, which leverages the power of BERT's representation to determine dynamic weights for each element in the embedding.","Additionally, we develop a term-level knowledge guidance module (TKGM) for effectively utilizing term-level knowledge to intelligently guide the modeling process of term weight.","Evaluation of the proposed method on MS Marco benchmark demonstrates its superiority over the previous state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.12152v1","category":"cs.CL"}
{"created":"2024-04-18 12:55:18","title":"Aligning language models with human preferences","abstract":"Language models (LMs) trained on vast quantities of text data can acquire sophisticated skills such as generating summaries, answering questions or generating code. However, they also manifest behaviors that violate human preferences, e.g., they can generate offensive content, falsehoods or perpetuate social biases. In this thesis, I explore several approaches to aligning LMs with human preferences. First, I argue that aligning LMs can be seen as Bayesian inference: conditioning a prior (base, pretrained LM) on evidence about human preferences (Chapter 2). Conditioning on human preferences can be implemented in numerous ways. In Chapter 3, I investigate the relation between two approaches to finetuning pretrained LMs using feedback given by a scoring function: reinforcement learning from human feedback (RLHF) and distribution matching. I show that RLHF can be seen as a special case of distribution matching but distributional matching is strictly more general. In chapter 4, I show how to extend the distribution matching to conditional language models. Finally, in chapter 5 I explore a different root: conditioning an LM on human preferences already during pretraining. I show that involving human feedback from the very start tends to be more effective than using it only during supervised finetuning. Overall, these results highlight the room for alignment techniques different from and complementary to RLHF.","sentences":["Language models (LMs) trained on vast quantities of text data can acquire sophisticated skills such as generating summaries, answering questions or generating code.","However, they also manifest behaviors that violate human preferences, e.g., they can generate offensive content, falsehoods or perpetuate social biases.","In this thesis, I explore several approaches to aligning LMs with human preferences.","First, I argue that aligning LMs can be seen as Bayesian inference: conditioning a prior (base, pretrained LM) on evidence about human preferences (Chapter 2).","Conditioning on human preferences can be implemented in numerous ways.","In Chapter 3, I investigate the relation between two approaches to finetuning pretrained LMs using feedback given by a scoring function: reinforcement learning from human feedback (RLHF) and distribution matching.","I show that RLHF can be seen as a special case of distribution matching but distributional matching is strictly more general.","In chapter 4, I show how to extend the distribution matching to conditional language models.","Finally, in chapter 5 I explore a different root: conditioning an LM on human preferences already during pretraining.","I show that involving human feedback from the very start tends to be more effective than using it only during supervised finetuning.","Overall, these results highlight the room for alignment techniques different from and complementary to RLHF."],"url":"http://arxiv.org/abs/2404.12150v1","category":"cs.LG"}
{"created":"2024-04-18 12:54:25","title":"AccidentBlip2: Accident Detection With Multi-View MotionBlip2","abstract":"Multimodal Large Language Models (MLLMs) have shown outstanding capabilities in many areas of multimodal reasoning. Therefore, we use the reasoning ability of Multimodal Large Language Models for environment description and scene understanding in complex transportation environments. In this paper, we propose AccidentBlip2, a multimodal large language model that can predict in real time whether an accident risk will occur. Our approach involves feature extraction based on the temporal scene of the six-view surround view graphs and temporal inference using the temporal blip framework through the vision transformer. We then input the generated temporal token into the MLLMs for inference to determine whether an accident will occur or not. Since AccidentBlip2 does not rely on any BEV images and LiDAR, the number of inference parameters and the inference cost of MLLMs can be significantly reduced, and it also does not incur a large training overhead during training. AccidentBlip2 outperforms existing solutions on the DeepAccident dataset and can also provide a reference solution for end-to-end automated driving accident prediction.","sentences":["Multimodal Large Language Models (MLLMs) have shown outstanding capabilities in many areas of multimodal reasoning.","Therefore, we use the reasoning ability of Multimodal Large Language Models for environment description and scene understanding in complex transportation environments.","In this paper, we propose AccidentBlip2, a multimodal large language model that can predict in real time whether an accident risk will occur.","Our approach involves feature extraction based on the temporal scene of the six-view surround view graphs and temporal inference using the temporal blip framework through the vision transformer.","We then input the generated temporal token into the MLLMs for inference to determine whether an accident will occur or not.","Since AccidentBlip2 does not rely on any BEV images and LiDAR, the number of inference parameters and the inference cost of MLLMs can be significantly reduced, and it also does not incur a large training overhead during training.","AccidentBlip2 outperforms existing solutions on the DeepAccident dataset and can also provide a reference solution for end-to-end automated driving accident prediction."],"url":"http://arxiv.org/abs/2404.12149v1","category":"cs.AI"}
{"created":"2024-04-18 12:48:17","title":"From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency","abstract":"The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what \"understanding\" means for a language model and how it compares to human understanding. This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say. In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning. Specifically, we focus on consistency across languages as well as paraphrases. Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks. We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks. We find that the model's multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding. We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding.","sentences":["The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what \"understanding\" means for a language model and how it compares to human understanding.","This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say.","In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning.","Specifically, we focus on consistency across languages as well as paraphrases.","Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks.","We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks.","We find that the model's multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding.","We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding."],"url":"http://arxiv.org/abs/2404.12145v1","category":"cs.CL"}
{"created":"2024-04-18 12:44:35","title":"The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action","abstract":"Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to 'algorithmic positive action' under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of fairness-aware algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to 'not do harm' towards a positive obligation to actively 'do no harm' as a more adequate framework for algorithmic decision-making and fair ml-interventions.","sentences":["Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems.","While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to 'algorithmic positive action' under European Union (EU) non-discrimination law.","As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions.","In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action.","Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of fairness-aware algorithmic decision-making.","Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making.","Consequently, we suggest moving away from a duty to 'not do harm' towards a positive obligation to actively 'do no harm' as a more adequate framework for algorithmic decision-making and fair ml-interventions."],"url":"http://arxiv.org/abs/2404.12143v1","category":"cs.AI"}
{"created":"2024-04-18 12:43:39","title":"MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space","abstract":"Generative models for structure-based drug design (SBDD) have shown promising results in recent years. Existing works mainly focus on how to generate molecules with higher binding affinity, ignoring the feasibility prerequisites for generated 3D poses and resulting in false positives. We conduct thorough studies on key factors of ill-conformational problems when applying autoregressive methods and diffusion to SBDD, including mode collapse and hybrid continuous-discrete space. In this paper, we introduce \\ours, the first SBDD model that operates in the continuous parameter space, together with a novel noise reduced sampling strategy. Empirical results show that our model consistently achieves superior performance in binding affinity with more stable 3D structure, demonstrating our ability to accurately model interatomic interactions. To our best knowledge, MolCRAFT is the first to achieve reference-level Vina Scores (-6.59 kcal/mol), outperforming other strong baselines by a wide margin (-0.84 kcal/mol). Code is available at https://github.com/AlgoMole/MolCRAFT.","sentences":["Generative models for structure-based drug design (SBDD) have shown promising results in recent years.","Existing works mainly focus on how to generate molecules with higher binding affinity, ignoring the feasibility prerequisites for generated 3D poses and resulting in false positives.","We conduct thorough studies on key factors of ill-conformational problems when applying autoregressive methods and diffusion to SBDD, including mode collapse and hybrid continuous-discrete space.","In this paper, we introduce \\ours, the first SBDD model that operates in the continuous parameter space, together with a novel noise reduced sampling strategy.","Empirical results show that our model consistently achieves superior performance in binding affinity with more stable 3D structure, demonstrating our ability to accurately model interatomic interactions.","To our best knowledge, MolCRAFT is the first to achieve reference-level Vina Scores (-6.59 kcal/mol), outperforming other strong baselines by a wide margin (-0.84 kcal/mol).","Code is available at https://github.com/AlgoMole/MolCRAFT."],"url":"http://arxiv.org/abs/2404.12141v1","category":"q-bio.BM"}
{"created":"2024-04-18 12:41:44","title":"Data reconstruction of the dynamical connection function in $f(Q)$ cosmology","abstract":"We employ Hubble data and Gaussian Processes in order to reconstruct the dynamical connection function in $f(Q)$ cosmology beyond the coincident gauge. In particular, there exist three branches of connections that satisfy the torsionless and curvatureless conditions, parameterized by a new dynamical function $\\gamma$. We express the redshift dependence of $\\gamma$ in terms of the $H(z)$ function and the $f(Q)$ form and parameters, and then we reconstruct it using 55 $H(z)$ observation data. Firstly, we investigate the case where ordinary conservation law holds, and we reconstruct the $f(Q)$ function, which is very well described by a quadratic correction on top of Symmetric Teleparallel Equivalent of General Relativity. Proceeding to the general case, we consider two of the most studied $f(Q)$ models of the literature, namely the square-root and the exponential one. In both cases we reconstruct $\\gamma(z)$, and we show that according to AIC and BIC information criteria its inclusion is favoured compared to both $\\Lambda$CDM paradigm, as well as to the same $f(Q)$ models under the coincident gauge. This feature acts as an indication that $f(Q)$ cosmology should be studied beyond the coincident gauge.","sentences":["We employ Hubble data and Gaussian Processes in order to reconstruct the dynamical connection function in $f(Q)$ cosmology beyond the coincident gauge.","In particular, there exist three branches of connections that satisfy the torsionless and curvatureless conditions, parameterized by a new dynamical function $\\gamma$.","We express the redshift dependence of $\\gamma$ in terms of the $H(z)$ function and the $f(Q)$ form and parameters, and then we reconstruct it using 55 $H(z)$ observation data.","Firstly, we investigate the case where ordinary conservation law holds, and we reconstruct the $f(Q)$ function, which is very well described by a quadratic correction on top of Symmetric Teleparallel Equivalent of General Relativity.","Proceeding to the general case, we consider two of the most studied $f(Q)$ models of the literature, namely the square-root and the exponential one.","In both cases we reconstruct $\\gamma(z)$, and we show that according to AIC and BIC information criteria its inclusion is favoured compared to both $\\Lambda$CDM paradigm, as well as to the same $f(Q)$ models under the coincident gauge.","This feature acts as an indication that $f(Q)$ cosmology should be studied beyond the coincident gauge."],"url":"http://arxiv.org/abs/2404.12140v1","category":"astro-ph.CO"}
{"created":"2024-04-18 12:40:59","title":"Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?","abstract":"Can Large Language Models substitute humans in making important decisions? Recent research has unveiled the potential of LLMs to role-play assigned personas, mimicking their knowledge and linguistic habits. However, imitative decision-making requires a more nuanced understanding of personas. In this paper, we benchmark the ability of LLMs in persona-driven decision-making. Specifically, we investigate whether LLMs can predict characters' decisions provided with the preceding stories in high-quality novels. Leveraging character analyses written by literary experts, we construct a dataset LIFECHOICE comprising 1,401 character decision points from 395 books. Then, we conduct comprehensive experiments on LIFECHOICE, with various LLMs and methods for LLM role-playing. The results demonstrate that state-of-the-art LLMs exhibit promising capabilities in this task, yet there is substantial room for improvement. Hence, we further propose the CHARMAP method, which achieves a 6.01% increase in accuracy via persona-based memory retrieval. We will make our datasets and code publicly available.","sentences":["Can Large Language Models substitute humans in making important decisions?","Recent research has unveiled the potential of LLMs to role-play assigned personas, mimicking their knowledge and linguistic habits.","However, imitative decision-making requires a more nuanced understanding of personas.","In this paper, we benchmark the ability of LLMs in persona-driven decision-making.","Specifically, we investigate whether LLMs can predict characters' decisions provided with the preceding stories in high-quality novels.","Leveraging character analyses written by literary experts, we construct a dataset LIFECHOICE comprising 1,401 character decision points from 395 books.","Then, we conduct comprehensive experiments on LIFECHOICE, with various LLMs and methods for LLM role-playing.","The results demonstrate that state-of-the-art LLMs exhibit promising capabilities in this task, yet there is substantial room for improvement.","Hence, we further propose the CHARMAP method, which achieves a 6.01% increase in accuracy via persona-based memory retrieval.","We will make our datasets and code publicly available."],"url":"http://arxiv.org/abs/2404.12138v1","category":"cs.AI"}
{"created":"2024-04-18 12:40:51","title":"Estimation of subcritical Galton Watson processes with correlated immigration","abstract":"We consider an observed subcritical Galton Watson process $\\{Y_n,\\ n\\in \\mathbb{Z} \\}$ with correlated stationary immigration process $\\{\\epsilon_n,\\ n\\in \\mathbb{Z} \\}$. Two situations are presented. The first one is when $\\mbox{Cov}(\\epsilon_0,\\epsilon_k)=0$ for $k$ larger than some $k_0$: a consistent estimator for the reproduction and mean immigration rates is given, and a central limit theorem is proved. The second one is when $\\{\\epsilon_n,\\ n\\in \\mathbb{Z} \\}$ has general correlation structure: under mixing assumptions, we exhibit an estimator for the the logarithm of the reproduction rate and we prove that it converges in quadratic mean with explicit speed. In addition, when the mixing coefficients decrease fast enough, we provide and prove a two terms expansion for the estimator. Numerical illustrations are provided.","sentences":["We consider an observed subcritical Galton Watson process $\\{Y_n,\\ n\\in \\mathbb{Z} \\}$ with correlated stationary immigration process $\\{\\epsilon_n,\\ n\\in \\mathbb{Z} \\}$. Two situations are presented.","The first one is when $\\mbox{Cov}(\\epsilon_0,\\epsilon_k)=0$ for $k$ larger than some $k_0$: a consistent estimator for the reproduction and mean immigration rates is given, and a central limit theorem is proved.","The second one is when $\\{\\epsilon_n,\\ n\\in \\mathbb{Z} \\}$ has general correlation structure: under mixing assumptions, we exhibit an estimator for the the logarithm of the reproduction rate and we prove that it converges in quadratic mean with explicit speed.","In addition, when the mixing coefficients decrease fast enough, we provide and prove a two terms expansion for the estimator.","Numerical illustrations are provided."],"url":"http://arxiv.org/abs/2404.12137v1","category":"math.ST"}
{"created":"2024-04-18 12:35:24","title":"Warped Time Series Anomaly Detection","abstract":"This paper addresses the problem of detecting time series outliers, focusing on systems with repetitive behavior, such as industrial robots operating on production lines.Notable challenges arise from the fact that a task performed multiple times may exhibit different duration in each repetition and that the time series reported by the sensors are irregularly sampled because of data gaps. The anomaly detection approach presented in this paper consists of three stages.The first stage identifies the repetitive cycles in the lengthy time series and segments them into individual time series corresponding to one task cycle, while accounting for possible temporal distortions.The second stage computes a prototype for the cycles using a GPU-based barycenter algorithm, specifically tailored for very large time series.The third stage uses the prototype to detect abnormal cycles by computing an anomaly score for each cycle.The overall approach, named WarpEd Time Series ANomaly Detection (WETSAND), makes use of the Dynamic Time Warping algorithm and its variants because they are suited to the distorted nature of the time series.The experiments show that \\wetsand scales to large signals, computes human-friendly prototypes, works with very little data, and outperforms some general purpose anomaly detection approaches such as autoencoders.","sentences":["This paper addresses the problem of detecting time series outliers, focusing on systems with repetitive behavior, such as industrial robots operating on production lines.","Notable challenges arise from the fact that a task performed multiple times may exhibit different duration in each repetition and that the time series reported by the sensors are irregularly sampled because of data gaps.","The anomaly detection approach presented in this paper consists of three stages.","The first stage identifies the repetitive cycles in the lengthy time series and segments them into individual time series corresponding to one task cycle, while accounting for possible temporal distortions.","The second stage computes a prototype for the cycles using a GPU-based barycenter algorithm, specifically tailored for very large time series.","The third stage uses the prototype to detect abnormal cycles by computing an anomaly score for each cycle.","The overall approach, named WarpEd","Time Series ANomaly Detection (WETSAND), makes use of the Dynamic Time Warping algorithm and its variants because they are suited to the distorted nature of the time series.","The experiments show that \\wetsand scales to large signals, computes human-friendly prototypes, works with very little data, and outperforms some general purpose anomaly detection approaches such as autoencoders."],"url":"http://arxiv.org/abs/2404.12134v1","category":"cs.AI"}
{"created":"2024-04-18 12:35:17","title":"On Target Detection in the Presence of Clutter in Joint Communication and Sensing Cellular Networks","abstract":"Recent works on joint communication and sensing (JCAS) cellular networks have proposed to use time division mode (TDM) and concurrent mode (CM), as alternative methods for sharing the resources between communication and sensing signals. While the performance of these JCAS schemes for object tracking and parameter estimation has been studied in previous works, their performance on target detection in the presence of clutter has not been analyzed. In this paper, we propose a detection scheme for estimating the number of targets in JCAS cellular networks that employ TDM or CM resource sharing. The proposed detection method allows for the presence of clutter and/or temporally correlated noise. This scheme is studied with respect to the JCAS trade-off parameters that allow to control the time slots in TDM and the power resources in CM allocated to sensing and communications. The performance of two fundamental transmit beamforming schemes, typical for JCAS, is compared in terms of the receiver operating characteristics curves. Our results indicate that in general the TDM scheme gives a somewhat better detection performance compared to the CM scheme, although both schemes outperform existing approaches provided that their respective trade-off parameters are tuned properly.","sentences":["Recent works on joint communication and sensing (JCAS) cellular networks have proposed to use time division mode (TDM) and concurrent mode (CM), as alternative methods for sharing the resources between communication and sensing signals.","While the performance of these JCAS schemes for object tracking and parameter estimation has been studied in previous works, their performance on target detection in the presence of clutter has not been analyzed.","In this paper, we propose a detection scheme for estimating the number of targets in JCAS cellular networks that employ TDM or CM resource sharing.","The proposed detection method allows for the presence of clutter and/or temporally correlated noise.","This scheme is studied with respect to the JCAS trade-off parameters that allow to control the time slots in TDM and the power resources in CM allocated to sensing and communications.","The performance of two fundamental transmit beamforming schemes, typical for JCAS, is compared in terms of the receiver operating characteristics curves.","Our results indicate that in general the TDM scheme gives a somewhat better detection performance compared to the CM scheme, although both schemes outperform existing approaches provided that their respective trade-off parameters are tuned properly."],"url":"http://arxiv.org/abs/2404.12133v1","category":"cs.IT"}
{"created":"2024-04-18 12:31:48","title":"One-Shot Sequential Federated Learning for Non-IID Data by Enhancing Local Model Diversity","abstract":"Traditional federated learning mainly focuses on parallel settings (PFL), which can suffer significant communication and computation costs. In contrast, one-shot and sequential federated learning (SFL) have emerged as innovative paradigms to alleviate these costs. However, the issue of non-IID (Independent and Identically Distributed) data persists as a significant challenge in one-shot and SFL settings, exacerbated by the restricted communication between clients. In this paper, we improve the one-shot sequential federated learning for non-IID data by proposing a local model diversity-enhancing strategy. Specifically, to leverage the potential of local model diversity for improving model performance, we introduce a local model pool for each client that comprises diverse models generated during local training, and propose two distance measurements to further enhance the model diversity and mitigate the effect of non-IID data. Consequently, our proposed framework can improve the global model performance while maintaining low communication costs. Extensive experiments demonstrate that our method exhibits superior performance to existing one-shot PFL methods and achieves better accuracy compared with state-of-the-art one-shot SFL methods on both label-skew and domain-shift tasks (e.g., 6%+ accuracy improvement on the CIFAR-10 dataset).","sentences":["Traditional federated learning mainly focuses on parallel settings (PFL), which can suffer significant communication and computation costs.","In contrast, one-shot and sequential federated learning (SFL) have emerged as innovative paradigms to alleviate these costs.","However, the issue of non-IID (Independent and Identically Distributed) data persists as a significant challenge in one-shot and SFL settings, exacerbated by the restricted communication between clients.","In this paper, we improve the one-shot sequential federated learning for non-IID data by proposing a local model diversity-enhancing strategy.","Specifically, to leverage the potential of local model diversity for improving model performance, we introduce a local model pool for each client that comprises diverse models generated during local training, and propose two distance measurements to further enhance the model diversity and mitigate the effect of non-IID data.","Consequently, our proposed framework can improve the global model performance while maintaining low communication costs.","Extensive experiments demonstrate that our method exhibits superior performance to existing one-shot PFL methods and achieves better accuracy compared with state-of-the-art one-shot SFL methods on both label-skew and domain-shift tasks (e.g., 6%+ accuracy improvement on the CIFAR-10 dataset)."],"url":"http://arxiv.org/abs/2404.12130v1","category":"cs.LG"}
{"created":"2024-04-18 12:30:36","title":"Recursive stochastic differential games with non-Lipschitzian generators and viscosity solutions of Hamilton-Jacobi-Bellman-Isaacs equation","abstract":"This investigation is dedicated to a two-player zero-sum stochastic differential game (SDG), where a cost function is characterized by a backward stochastic differential equation (BSDE) with a continuous and monotonic generator regarding the first unknown variable, which possesses immense applicability in financial engineering. A verification theorem by virtue of classical solution of derived Hamilton-Jacobi-Bellman-Isaacs (HJBI) equation is given. The dynamic programming principle (DPP) and unique weak (viscosity) solvability of HJBI equation are formulated through comparison theorem for BSDEs with monotonic generators and stability of viscosity solution. Some new regularity properties of value function are presented. Finally, we propose three concrete examples, which are concerned with resp., classical, and viscosity solution of HJBI equation, as well as a financial application where an investor with a non-Lipschitzian Epstein-Zin utility deals with market friction to maximize her utility preference.","sentences":["This investigation is dedicated to a two-player zero-sum stochastic differential game (SDG), where a cost function is characterized by a backward stochastic differential equation (BSDE) with a continuous and monotonic generator regarding the first unknown variable, which possesses immense applicability in financial engineering.","A verification theorem by virtue of classical solution of derived Hamilton-Jacobi-Bellman-Isaacs (HJBI) equation is given.","The dynamic programming principle (DPP) and unique weak (viscosity) solvability of HJBI equation are formulated through comparison theorem for BSDEs with monotonic generators and stability of viscosity solution.","Some new regularity properties of value function are presented.","Finally, we propose three concrete examples, which are concerned with resp., classical, and viscosity solution of HJBI equation, as well as a financial application where an investor with a non-Lipschitzian Epstein-Zin utility deals with market friction to maximize her utility preference."],"url":"http://arxiv.org/abs/2404.12129v1","category":"math.OC"}
{"created":"2024-04-18 12:28:50","title":"Personalized Forgetting Mechanism with Concept-Driven Knowledge Tracing","abstract":"Knowledge Tracing (KT) aims to trace changes in students' knowledge states throughout their entire learning process by analyzing their historical learning data and predicting their future learning performance. Existing forgetting curve theory based knowledge tracing models only consider the general forgetting caused by time intervals, ignoring the individualization of students and the causal relationship of the forgetting process. To address these problems, we propose a Concept-driven Personalized Forgetting knowledge tracing model (CPF) which integrates hierarchical relationships between knowledge concepts and incorporates students' personalized cognitive abilities. First, we integrate the students' personalized capabilities into both the learning and forgetting processes to explicitly distinguish students' individual learning gains and forgetting rates according to their cognitive abilities. Second, we take into account the hierarchical relationships between knowledge points and design a precursor-successor knowledge concept matrix to simulate the causal relationship in the forgetting process, while also integrating the potential impact of forgetting prior knowledge points on subsequent ones. The proposed personalized forgetting mechanism can not only be applied to the learning of specifc knowledge concepts but also the life-long learning process. Extensive experimental results on three public datasets show that our CPF outperforms current forgetting curve theory based methods in predicting student performance, demonstrating CPF can better simulate changes in students' knowledge status through the personalized forgetting mechanism.","sentences":["Knowledge Tracing (KT) aims to trace changes in students' knowledge states throughout their entire learning process by analyzing their historical learning data and predicting their future learning performance.","Existing forgetting curve theory based knowledge tracing models only consider the general forgetting caused by time intervals, ignoring the individualization of students and the causal relationship of the forgetting process.","To address these problems, we propose a Concept-driven Personalized Forgetting knowledge tracing model (CPF) which integrates hierarchical relationships between knowledge concepts and incorporates students' personalized cognitive abilities.","First, we integrate the students' personalized capabilities into both the learning and forgetting processes to explicitly distinguish students' individual learning gains and forgetting rates according to their cognitive abilities.","Second, we take into account the hierarchical relationships between knowledge points and design a precursor-successor knowledge concept matrix to simulate the causal relationship in the forgetting process, while also integrating the potential impact of forgetting prior knowledge points on subsequent ones.","The proposed personalized forgetting mechanism can not only be applied to the learning of specifc knowledge concepts but also the life-long learning process.","Extensive experimental results on three public datasets show that our CPF outperforms current forgetting curve theory based methods in predicting student performance, demonstrating CPF can better simulate changes in students' knowledge status through the personalized forgetting mechanism."],"url":"http://arxiv.org/abs/2404.12127v1","category":"cs.AI"}
{"created":"2024-04-18 12:25:57","title":"Improving inference on neutron star properties using information from binary merger remnants","abstract":"The gravitational-wave signal GW170817 is a result of a binary neutron star coalescence event. The observations of electromagnetic counterparts suggest that the event didn't led to the prompt formation of a black-hole. In this work, we first classify the GW170817 LIGO-Virgo data sample into prompt collapse to a black-hole using the $q$-dependent threshold mass fits and then remove these cases from the data sample. We find that the cases without a prompt black-hole formation do not support radii $ <$ 10 km unlike the LIGO-Virgo data sample. This is consistent with the maximum mass constraint, based on the binary pulsar J0348+0432, imposed LIGO-Virgo data sample. Additionally, we find that the cases without the prompt collapse to a black-hole improve the uncertainty range of neutron star radii from 3.3 km to 2.6 km for the data sample without the mass constraint and from 2.8 km to 2.5 km for the data sample with the mass constraint, implying improved constraints on the neutron star radii and hence the equation-of-state.","sentences":["The gravitational-wave signal GW170817 is a result of a binary neutron star coalescence event.","The observations of electromagnetic counterparts suggest that the event didn't led to the prompt formation of a black-hole.","In this work, we first classify the GW170817 LIGO-Virgo data sample into prompt collapse to a black-hole using the $q$-dependent threshold mass fits and then remove these cases from the data sample.","We find that the cases without a prompt black-hole formation do not support radii $ <$ 10 km unlike the LIGO-Virgo data sample.","This is consistent with the maximum mass constraint, based on the binary pulsar J0348+0432, imposed LIGO-Virgo data sample.","Additionally, we find that the cases without the prompt collapse to a black-hole improve the uncertainty range of neutron star radii from 3.3 km to 2.6 km for the data sample without the mass constraint and from 2.8 km to 2.5 km for the data sample with the mass constraint, implying improved constraints on the neutron star radii and hence the equation-of-state."],"url":"http://arxiv.org/abs/2404.12126v1","category":"gr-qc"}
{"created":"2024-04-18 12:25:46","title":"Intelligence Education made in Europe","abstract":"Global conflicts and trouble spots have thrown the world into turmoil. Intelligence services have never been as necessary as they are today when it comes to providing political decision-makers with concrete, accurate, and up-to-date decision-making knowledge. This requires a common co-operation, a common working language and a common understanding of each other. The best way to create this \"intelligence community\" is through a harmonized intelligence education.   In this paper, we show how joint intelligence education can succeed. We draw on the experience of Germany, where all intelligence services and the Bundeswehr are academically educated together in a single degree program that lays the foundations for a common working language. We also show how these experiences have been successfully transferred to a European level, namely to ICE, the Intelligence College in Europe. Our experience has shown that three aspects are particularly important: firstly, interdisciplinarity or better, transdisciplinarity, secondly, the integration of IT knowhow and thirdly, the development and learning of methodological skills. Using the example of the cyber intelligence module with a special focus on data-driven decision support, additionally with its many points of reference to numerous other academic modules, we show how the specific analytic methodology presented is embedded in our specific European teaching context.","sentences":["Global conflicts and trouble spots have thrown the world into turmoil.","Intelligence services have never been as necessary as they are today when it comes to providing political decision-makers with concrete, accurate, and up-to-date decision-making knowledge.","This requires a common co-operation, a common working language and a common understanding of each other.","The best way to create this \"intelligence community\" is through a harmonized intelligence education.   ","In this paper, we show how joint intelligence education can succeed.","We draw on the experience of Germany, where all intelligence services and the Bundeswehr are academically educated together in a single degree program that lays the foundations for a common working language.","We also show how these experiences have been successfully transferred to a European level, namely to ICE, the Intelligence College in Europe.","Our experience has shown that three aspects are particularly important: firstly, interdisciplinarity or better, transdisciplinarity, secondly, the integration of IT knowhow and thirdly, the development and learning of methodological skills.","Using the example of the cyber intelligence module with a special focus on data-driven decision support, additionally with its many points of reference to numerous other academic modules, we show how the specific analytic methodology presented is embedded in our specific European teaching context."],"url":"http://arxiv.org/abs/2404.12125v1","category":"cs.CY"}
{"created":"2024-04-18 12:23:13","title":"Dialogues between Astrobiology and Earth Pedagogy: a proposal to the continued training of science teachers","abstract":"Introduction and Objective. Assuming the need to rethink Education and its function as an element of social transformation and generator of a profound ethical change and promoter of other ways of being on planet Earth, this study seeks to understand the possible dialogues between Earth Pedagogy and Astrobiology in the construction and proposition of continuing education processes for science teachers. Methodology. A mini-course was promoted aimed at teachers and an attempt was made to report the construction process of the training process and, using Content Analysis, to analyze the contributions pointed out by the participants to their training and teaching practice. Results and Conclusion. The dialogue between knowledge in the continuing education of teachers contributes with fundamental elements to teaching practice, covering cognitive, emotional, individual and collective aspects, as well as highlighting the role of the university in promoting initiatives that bring Education and Complexity together.","sentences":["Introduction and Objective.","Assuming the need to rethink Education and its function as an element of social transformation and generator of a profound ethical change and promoter of other ways of being on planet Earth, this study seeks to understand the possible dialogues between Earth Pedagogy and Astrobiology in the construction and proposition of continuing education processes for science teachers.","Methodology.","A mini-course was promoted aimed at teachers and an attempt was made to report the construction process of the training process and, using Content Analysis, to analyze the contributions pointed out by the participants to their training and teaching practice.","Results and Conclusion.","The dialogue between knowledge in the continuing education of teachers contributes with fundamental elements to teaching practice, covering cognitive, emotional, individual and collective aspects, as well as highlighting the role of the university in promoting initiatives that bring Education and Complexity together."],"url":"http://arxiv.org/abs/2404.12124v1","category":"physics.ed-ph"}
{"created":"2024-04-18 12:13:09","title":"Fortify the Guardian, Not the Treasure: Resilient Adversarial Detectors","abstract":"This paper presents RADAR-Robust Adversarial Detection via Adversarial Retraining-an approach designed to enhance the robustness of adversarial detectors against adaptive attacks, while maintaining classifier performance. An adaptive attack is one where the attacker is aware of the defenses and adapts their strategy accordingly. Our proposed method leverages adversarial training to reinforce the ability to detect attacks, without compromising clean accuracy. During the training phase, we integrate into the dataset adversarial examples, which were optimized to fool both the classifier and the adversarial detector, enabling the adversarial detector to learn and adapt to potential attack scenarios. Experimental evaluations on the CIFAR-10 and SVHN datasets demonstrate that our proposed algorithm significantly improves a detector's ability to accurately identify adaptive adversarial attacks -- without sacrificing clean accuracy.","sentences":["This paper presents RADAR-Robust Adversarial Detection via Adversarial Retraining-an approach designed to enhance the robustness of adversarial detectors against adaptive attacks, while maintaining classifier performance.","An adaptive attack is one where the attacker is aware of the defenses and adapts their strategy accordingly.","Our proposed method leverages adversarial training to reinforce the ability to detect attacks, without compromising clean accuracy.","During the training phase, we integrate into the dataset adversarial examples, which were optimized to fool both the classifier and the adversarial detector, enabling the adversarial detector to learn and adapt to potential attack scenarios.","Experimental evaluations on the CIFAR-10 and SVHN datasets demonstrate that our proposed algorithm significantly improves a detector's ability to accurately identify adaptive adversarial attacks -- without sacrificing clean accuracy."],"url":"http://arxiv.org/abs/2404.12120v1","category":"cs.CV"}
{"created":"2024-04-18 12:05:25","title":"Ore sets, denominator sets and the left regular left quotient ring of a ring","abstract":"The aim of the papers is to describe the left regular left quotient ring ${}'Q(R)$ and the right regular right quotient ring $Q'(R)$ for the following algebras $R$: $\\mS_n=\\mS_1^{\\t n}$ is the algebra of one-sided inverses, where $\\mS_1=K\\langle x,y\\, | \\, yx=1\\rangle$, $\\CI_n=K\\langle \\der_1, \\ldots, \\der_n,\\int_1,\\ldots, \\int_n\\rangle$ is the algebra of scalar integro-differential operators and the Jacobian algebra $\\mA_1=K\\langle x,\\der, (\\der x)^{-1}\\rangle$. The sets of left and right regular elements of the algebras $\\mS_1$, $\\CI_1$, $\\mA_1$ and $\\mI_1=K\\langle x, \\der,\\int\\rangle$. A progress is made on the following conjecture, \\cite{Clas-lreg-quot}: $${}'Q(\\mI_n)\\simeq Q(A_n)\\;\\; {\\rm   where}\\;\\; \\mI_n =K\\bigg\\langle x_1,\\ldots , x_n, \\der_1, \\ldots, \\der_n,\\int_1,\\ldots, \\int_n\\bigg\\rangle$$ is the algebra of polynomial integro-differential operators and $Q(A_n)$ is the classical quotient ring (of fractions) of the $n$'th Weyl algebra $A_n$, i.e. a criterion is given when the isomorphism holds. We produce several general constructions of left Ore and left denominator sets that appear naturally in applications and are of independent interest and use them to produce explicit left denominator sets that give the localization ring isomorphic to ${}'Q(\\mS_n)$ or ${}'Q(\\mI_n)$ or ${}'Q(\\mA_n)$ where $\\mA_n:=\\mA_1^{\\t n}$. Several characterizations of one-sided regular elements of a ring are given in module-theoretic and one-sided-ideal-theoretic way.","sentences":["The aim of the papers is to describe the left regular left quotient ring ${}'Q(R)$ and the right regular right quotient ring $Q'(R)$ for the following algebras $R$: $\\mS_n=\\mS_1^{\\t n}$ is the algebra of one-sided inverses, where $\\mS_1=K\\langle x,y\\, | \\, yx=1\\rangle$, $\\CI_n=K\\langle \\der_1, \\ldots, \\der_n,\\int_1,\\ldots, \\int_n\\rangle$ is the algebra of scalar integro-differential operators and the Jacobian algebra $\\mA_1=K\\langle x,\\der, (\\der x)^{-1}\\rangle$.","The sets of left and right regular elements of the algebras $\\mS_1$, $\\CI_1$, $\\mA_1$ and $\\mI_1=K\\langle x, \\der,\\int\\rangle$. A progress is made on the following conjecture, \\cite{Clas-lreg-quot}: $${}'Q(\\mI_n)\\simeq Q(A_n)\\;\\; {\\rm   where}\\;\\; \\mI_n =K\\bigg\\langle x_1,\\ldots , x_n, \\der_1, \\ldots, \\der_n,\\int_1,\\ldots, \\int_n\\bigg\\rangle$$ is the algebra of polynomial integro-differential operators and $Q(A_n)$ is the classical quotient ring (of fractions) of the $n$'th Weyl algebra $A_n$, i.e. a criterion is given when the isomorphism holds.","We produce several general constructions of left Ore and left denominator sets that appear naturally in applications and are of independent interest and use them to produce explicit left denominator sets that give the localization ring isomorphic to ${}'Q(\\mS_n)$ or ${}'Q(\\mI_n)$ or ${}'Q(\\mA_n)$ where $\\mA_n:=\\mA_1^{\\t n}$. Several characterizations of one-sided regular elements of a ring are given in module-theoretic and one-sided-ideal-theoretic way."],"url":"http://arxiv.org/abs/2404.12116v1","category":"math.RA"}
{"created":"2024-04-18 11:59:50","title":"Caging in Motion: Characterizing Robustness in Manipulation through Energy Margin and Dynamic Caging Analysis","abstract":"To develop robust manipulation policies, quantifying robustness is essential. Evaluating robustness in general dexterous manipulation, nonetheless, poses significant challenges due to complex hybrid dynamics, combinatorial explosion of possible contact interactions, global geometry, etc. This paper introduces ``caging in motion'', an approach for analyzing manipulation robustness through energy margins and caging-based analysis. Our method assesses manipulation robustness by measuring the energy margin to failure and extends traditional caging concepts for a global analysis of dynamic manipulation. This global analysis is facilitated by a kinodynamic planning framework that naturally integrates global geometry, contact changes, and robot compliance. We validate the effectiveness of our approach in the simulation and real-world experiments of multiple dynamic manipulation scenarios, highlighting its potential to predict manipulation success and robustness.","sentences":["To develop robust manipulation policies, quantifying robustness is essential.","Evaluating robustness in general dexterous manipulation, nonetheless, poses significant challenges due to complex hybrid dynamics, combinatorial explosion of possible contact interactions, global geometry, etc.","This paper introduces ``caging in motion'', an approach for analyzing manipulation robustness through energy margins and caging-based analysis.","Our method assesses manipulation robustness by measuring the energy margin to failure and extends traditional caging concepts for a global analysis of dynamic manipulation.","This global analysis is facilitated by a kinodynamic planning framework that naturally integrates global geometry, contact changes, and robot compliance.","We validate the effectiveness of our approach in the simulation and real-world experiments of multiple dynamic manipulation scenarios, highlighting its potential to predict manipulation success and robustness."],"url":"http://arxiv.org/abs/2404.12115v1","category":"cs.RO"}
{"created":"2024-04-18 11:59:07","title":"Recurrence formulae for spectral determinants","abstract":"We develop a unified method to study spectral determinants for several different manifolds, including spheres and hemispheres, and projective spaces. This is a direct consequence of an approach based on deriving recursion relations for the corresponding zeta functions, which we are then able to solve explicitly. Apart from new applications such as hemispheres, we also believe that the resulting formulae in the cases for which expressions for the determinant were already known are simpler and easier to compute in general, when compared to those resulting from other approaches.","sentences":["We develop a unified method to study spectral determinants for several different manifolds, including spheres and hemispheres, and projective spaces.","This is a direct consequence of an approach based on deriving recursion relations for the corresponding zeta functions, which we are then able to solve explicitly.","Apart from new applications such as hemispheres, we also believe that the resulting formulae in the cases for which expressions for the determinant were already known are simpler and easier to compute in general, when compared to those resulting from other approaches."],"url":"http://arxiv.org/abs/2404.12114v1","category":"math.SP"}
{"created":"2024-04-18 11:57:03","title":"Cahn-Hilliard equations with singular potential, reaction term and pure phase initial datum","abstract":"We consider local and nonlocal Cahn-Hilliard equations with constant mobility and singular potentials including, e.g., the Flory-Huggins potential, subject to no-flux (or periodic) boundary conditions. The main goal is to show that the presence of a suitable class of reaction terms allows to establish the existence of a weak solution to the corresponding initial and boundary value problem even though the initial condition is a pure state. In other words, the separation process takes place even in presence of a pure phase, provided that it is triggered by a convenient reaction term. This fact was already observed by the authors in a previous contribution devoted to a specific biological model. In this context, we generalize the previously-mentioned concept by examining the essential assumptions required for the reaction term to apply the new strategy. Also, we explore the scenario involving the nonlocal Cahn-Hilliard equation and provide illustrative examples that contextualize within our abstract framework.","sentences":["We consider local and nonlocal Cahn-Hilliard equations with constant mobility and singular potentials including, e.g., the Flory-Huggins potential, subject to no-flux (or periodic) boundary conditions.","The main goal is to show that the presence of a suitable class of reaction terms allows to establish the existence of a weak solution to the corresponding initial and boundary value problem even though the initial condition is a pure state.","In other words, the separation process takes place even in presence of a pure phase, provided that it is triggered by a convenient reaction term.","This fact was already observed by the authors in a previous contribution devoted to a specific biological model.","In this context, we generalize the previously-mentioned concept by examining the essential assumptions required for the reaction term to apply the new strategy.","Also, we explore the scenario involving the nonlocal Cahn-Hilliard equation and provide illustrative examples that contextualize within our abstract framework."],"url":"http://arxiv.org/abs/2404.12113v1","category":"math.AP"}
{"created":"2024-04-18 11:56:55","title":"Generalized derivations of BiHom-supertrialgebras","abstract":"In this note, our goal is to describe the concept of generalized derivations in the context of BiHom-supertrialgebras. We provide a comprehensive analysis of the properties and applications of these generalized derivations, including their relationship with other algebraic structures. We also explore various examples and applications of BiHom-supertrialgebras in different fields of mathematics and physics. Our findings contribute to a deeper understanding of the algebraic properties and applications of BiHom-supertrialgebras, and pave the way for further research in this area.","sentences":["In this note, our goal is to describe the concept of generalized derivations in the context of BiHom-supertrialgebras.","We provide a comprehensive analysis of the properties and applications of these generalized derivations, including their relationship with other algebraic structures.","We also explore various examples and applications of BiHom-supertrialgebras in different fields of mathematics and physics.","Our findings contribute to a deeper understanding of the algebraic properties and applications of BiHom-supertrialgebras, and pave the way for further research in this area."],"url":"http://arxiv.org/abs/2404.12112v1","category":"math.RA"}
{"created":"2024-04-18 11:40:26","title":"Scale Separation Effects on Simulations of Plasma Turbulence","abstract":"Understanding plasma turbulence requires a synthesis of experiments, observations, theory, and simulations. In the case of kinetic plasmas such as the solar wind, the lack of collisions renders the fluid closures such as viscosity meaningless and one needs to resort to higher order fluid models or kinetic models. Typically, the computational expense in such models is managed by simulating artificial values of certain parameters such as the ratio of the Alfv\\'en speed to the speed of light ($v_A/c$) or the relative mass ratio of ions and electrons ($m_i/m_e$). Although, typically care is taken to use values as close as possible to realistic values within the computational constraints, these artificial values could potentially introduce unphysical effects. These unphysical effects could be significant at sub-ion scales, where kinetic effects are the most important. In this paper, we use the ten-moment fluid model in the Gkeyll framework to perform controlled numerical experiments, systematically varying the ion-electron mass ratio from a small value down to the realistic proton-electron mass ratio. We show that the unphysical mass ratio has a significant effect on the kinetic range dynamics as well as the heating of both the plasma species. The dissipative process for both ions and electrons become more compressive in nature, although the ions remain nearly incompressible in all cases. The electrons move from being dominated by incompressive viscous like heating/dissipation, to very compressive heating/dissipation dominated by compressions/rarefactions. While the heating change is significant for the electrons, a mass ratio of $m_i/m_e \\sim 250$ captures the asymptotic behaviour of electron heating.","sentences":["Understanding plasma turbulence requires a synthesis of experiments, observations, theory, and simulations.","In the case of kinetic plasmas such as the solar wind, the lack of collisions renders the fluid closures such as viscosity meaningless and one needs to resort to higher order fluid models or kinetic models.","Typically, the computational expense in such models is managed by simulating artificial values of certain parameters such as the ratio of the Alfv\\'en speed to the speed of light ($v_A/c$) or the relative mass ratio of ions and electrons ($m_i/m_e$).","Although, typically care is taken to use values as close as possible to realistic values within the computational constraints, these artificial values could potentially introduce unphysical effects.","These unphysical effects could be significant at sub-ion scales, where kinetic effects are the most important.","In this paper, we use the ten-moment fluid model in the Gkeyll framework to perform controlled numerical experiments, systematically varying the ion-electron mass ratio from a small value down to the realistic proton-electron mass ratio.","We show that the unphysical mass ratio has a significant effect on the kinetic range dynamics as well as the heating of both the plasma species.","The dissipative process for both ions and electrons become more compressive in nature, although the ions remain nearly incompressible in all cases.","The electrons move from being dominated by incompressive viscous like heating/dissipation, to very compressive heating/dissipation dominated by compressions/rarefactions.","While the heating change is significant for the electrons, a mass ratio of $m_i/m_e \\sim 250$ captures the asymptotic behaviour of electron heating."],"url":"http://arxiv.org/abs/2404.12105v1","category":"physics.plasm-ph"}
{"created":"2024-04-18 11:38:25","title":"Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image Models","abstract":"The burgeoning landscape of text-to-image models, exemplified by innovations such as Midjourney and DALLE 3, has revolutionized content creation across diverse sectors. However, these advancements bring forth critical ethical concerns, particularly with the misuse of open-source models to generate content that violates societal norms. Addressing this, we introduce Ethical-Lens, a framework designed to facilitate the value-aligned usage of text-to-image tools without necessitating internal model revision. Ethical-Lens ensures value alignment in text-to-image models across toxicity and bias dimensions by refining user commands and rectifying model outputs. Systematic evaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess alignment capability. Our experiments reveal that Ethical-Lens enhances alignment capabilities to levels comparable with or superior to commercial models like DALLE 3, ensuring user-generated content adheres to ethical standards while maintaining image quality. This study indicates the potential of Ethical-Lens to ensure the sustainable development of open-source text-to-image tools and their beneficial integration into society. Our code is available at https://github.com/yuzhu-cai/Ethical-Lens.","sentences":["The burgeoning landscape of text-to-image models, exemplified by innovations such as Midjourney and DALLE 3, has revolutionized content creation across diverse sectors.","However, these advancements bring forth critical ethical concerns, particularly with the misuse of open-source models to generate content that violates societal norms.","Addressing this, we introduce Ethical-Lens, a framework designed to facilitate the value-aligned usage of text-to-image tools without necessitating internal model revision.","Ethical-Lens ensures value alignment in text-to-image models across toxicity and bias dimensions by refining user commands and rectifying model outputs.","Systematic evaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess alignment capability.","Our experiments reveal that Ethical-Lens enhances alignment capabilities to levels comparable with or superior to commercial models like DALLE 3, ensuring user-generated content adheres to ethical standards while maintaining image quality.","This study indicates the potential of Ethical-Lens to ensure the sustainable development of open-source text-to-image tools and their beneficial integration into society.","Our code is available at https://github.com/yuzhu-cai/Ethical-Lens."],"url":"http://arxiv.org/abs/2404.12104v1","category":"cs.CV"}
{"created":"2024-04-18 11:36:20","title":"Functional formulation of quantum theory of a scalar field in a metric with Lorentzian and Euclidean signatures","abstract":"We study the Schr\\\"odinger equation in quantum field theory (QFT) in its functional formulation. In this approach quantum correlation functions can be expressed as classical expectation values over (complex) stochastic processes. We obtain a stochastic representation of the Schr\\\"odinger time evolution on Wentzel-Kramers-Brillouin (WKB) states by means of the Wiener integral. We discuss QFT in a flat expanding metric and in de Sitter space-time. We calculate the evolution kernel in an expanding flat metric in the real time formulation. We discuss a field interaction in pseudoRiemannian and Riemannian metrics showing that an inversion of the signature leads to some substantial simplifications of the singularity problems in QFT.","sentences":["We study the Schr\\\"odinger equation in quantum field theory (QFT) in its functional formulation.","In this approach quantum correlation functions can be expressed as classical expectation values over (complex) stochastic processes.","We obtain a stochastic representation of the Schr\\\"odinger time evolution on Wentzel-Kramers-Brillouin (WKB) states by means of the Wiener integral.","We discuss QFT in a flat expanding metric and in de Sitter space-time.","We calculate the evolution kernel in an expanding flat metric in the real time formulation.","We discuss a field interaction in pseudoRiemannian and Riemannian metrics showing that an inversion of the signature leads to some substantial simplifications of the singularity problems in QFT."],"url":"http://arxiv.org/abs/2404.12102v1","category":"hep-th"}
{"created":"2024-04-18 11:33:23","title":"Generic free fermions with nearest neighbour interactions","abstract":"Generic free fermions are free fermions with a single particle spectrum that satisfies the $q$ no resonance condition, i.e., where equal sums of single-particle energies are unique. This property guaranties that they have no degeneracies and gives them relaxation properties more similar to those of generic systems. In this article we provide a minimal example of a generic free fermionic model with nearest neighbour interactions -- a tight-binding model with complex hopping. Using some standard results from number theory we prove that this model fulfils the $q$ no resonance condition when the number of lattice sites is prime. Whenever this is not the case one can recover the $q$ no resonance condition by adding hopping terms between sites corresponding to the divisors of the number of sites. We further discuss its many-body spectral statistics and show that local probes -- like the ratio of consecutive level spacings -- look very similar to what is expected for the Poisson statistics. We however demonstrate that free fermion models can never have Poisson statistics with an analysis of the moments of the spectral form factor.","sentences":["Generic free fermions are free fermions with a single particle spectrum that satisfies the $q$ no resonance condition, i.e., where equal sums of single-particle energies are unique.","This property guaranties that they have no degeneracies and gives them relaxation properties more similar to those of generic systems.","In this article we provide a minimal example of a generic free fermionic model with nearest neighbour interactions -- a tight-binding model with complex hopping.","Using some standard results from number theory we prove that this model fulfils the $q$ no resonance condition when the number of lattice sites is prime.","Whenever this is not the case one can recover the $q$ no resonance condition by adding hopping terms between sites corresponding to the divisors of the number of sites.","We further discuss its many-body spectral statistics and show that local probes -- like the ratio of consecutive level spacings -- look very similar to what is expected for the Poisson statistics.","We however demonstrate that free fermion models can never have Poisson statistics with an analysis of the moments of the spectral form factor."],"url":"http://arxiv.org/abs/2404.12100v1","category":"quant-ph"}
{"created":"2024-04-18 11:30:12","title":"On structures of BiHom-Superdialgebras and their derivations","abstract":"BiHom-superdialgebras are clear generalization of Hom-superdialgebras. The purpose of this note is to describe and to survey structures of BiHom-superdialgebras. Then we derive derivations of BiHomsuperdialgebras.","sentences":["BiHom-superdialgebras are clear generalization of Hom-superdialgebras.","The purpose of this note is to describe and to survey structures of BiHom-superdialgebras.","Then we derive derivations of BiHomsuperdialgebras."],"url":"http://arxiv.org/abs/2404.12098v1","category":"math.RA"}
{"created":"2024-04-18 11:17:58","title":"X-Light: Cross-City Traffic Signal Control Using Transformer on Transformer as Meta Multi-Agent Reinforcement Learner","abstract":"The effectiveness of traffic light control has been significantly improved by current reinforcement learning-based approaches via better cooperation among multiple traffic lights. However, a persisting issue remains: how to obtain a multi-agent traffic signal control algorithm with remarkable transferability across diverse cities? In this paper, we propose a Transformer on Transformer (TonT) model for cross-city meta multi-agent traffic signal control, named as X-Light: We input the full Markov Decision Process trajectories, and the Lower Transformer aggregates the states, actions, rewards among the target intersection and its neighbors within a city, and the Upper Transformer learns the general decision trajectories across different cities. This dual-level approach bolsters the model's robust generalization and transferability. Notably, when directly transferring to unseen scenarios, ours surpasses all baseline methods with +7.91% on average, and even +16.3% in some cases, yielding the best results.","sentences":["The effectiveness of traffic light control has been significantly improved by current reinforcement learning-based approaches via better cooperation among multiple traffic lights.","However, a persisting issue remains: how to obtain a multi-agent traffic signal control algorithm with remarkable transferability across diverse cities?","In this paper, we propose a Transformer on Transformer (TonT) model for cross-city meta multi-agent traffic signal control, named as X-Light: We input the full Markov Decision Process trajectories, and the Lower Transformer aggregates the states, actions, rewards among the target intersection and its neighbors within a city, and the Upper Transformer learns the general decision trajectories across different cities.","This dual-level approach bolsters the model's robust generalization and transferability.","Notably, when directly transferring to unseen scenarios, ours surpasses all baseline methods with +7.91% on average, and even +16.3% in some cases, yielding the best results."],"url":"http://arxiv.org/abs/2404.12090v1","category":"cs.AI"}
{"created":"2024-04-18 11:17:39","title":"An Overview of Electromagnetic Illusions: Empowering Smart Environments with Reconfigurable Metasurfaces","abstract":"This study delves into the innovative landscape of metasurfaces, with a particular focus on their role in achieving EM illusion (EMI) a facet of paramount significance. The control of EM waves assumes a pivotal role in mitigating issues such as signal degradation, interference, and reduced communication range. Furthermore, the engineering of waves serves as a foundational element in achieving invisibility or minimized detectability. This survey unravels the theoretical underpinnings and practical designs of EMI coatings, which have been harnessed to develop functional metasurfaces. EMI, practically achieved through engineered coatings, confers a strategic advantage by either reducing the radar cross-section of objects or creating misleading footprints. In addition to illustrating the outstanding achievements in reconfigurable cloaking, this study culminates in the proposal of a novel approach, suggesting the emergence of EMI without the need for physically coating the device to conceal and thus proposing the concept of a smart EMI environment. This groundbreaking work opens a new way for engineers and researchers to unlock exotic and versatile designs that build on reconfigurable intelligent surfaces (RIS). Crucially the designs enabled by the proposed approach, present a wide array of applications, encompassing camouflaging, deceptive sensing, radar cognition control, and defence security, among others. In essence, this research stands as a beacon guiding the exploration of uncharted territories in wave control through smart EMI environments, with profound implications spanning basic academic research in RIS through advanced security technologies and communication systems.","sentences":["This study delves into the innovative landscape of metasurfaces, with a particular focus on their role in achieving EM illusion (EMI) a facet of paramount significance.","The control of EM waves assumes a pivotal role in mitigating issues such as signal degradation, interference, and reduced communication range.","Furthermore, the engineering of waves serves as a foundational element in achieving invisibility or minimized detectability.","This survey unravels the theoretical underpinnings and practical designs of EMI coatings, which have been harnessed to develop functional metasurfaces.","EMI, practically achieved through engineered coatings, confers a strategic advantage by either reducing the radar cross-section of objects or creating misleading footprints.","In addition to illustrating the outstanding achievements in reconfigurable cloaking, this study culminates in the proposal of a novel approach, suggesting the emergence of EMI without the need for physically coating the device to conceal and thus proposing the concept of a smart EMI environment.","This groundbreaking work opens a new way for engineers and researchers to unlock exotic and versatile designs that build on reconfigurable intelligent surfaces (RIS).","Crucially the designs enabled by the proposed approach, present a wide array of applications, encompassing camouflaging, deceptive sensing, radar cognition control, and defence security, among others.","In essence, this research stands as a beacon guiding the exploration of uncharted territories in wave control through smart EMI environments, with profound implications spanning basic academic research in RIS through advanced security technologies and communication systems."],"url":"http://arxiv.org/abs/2404.12089v1","category":"eess.SP"}
{"created":"2024-04-18 11:13:36","title":"Optimizing the diffusion of overdamped Langevin dynamics","abstract":"This is a preleminary work. Overdamped Langevin dynamics are reversible stochastic differential equations which are commonly used to sample probability measures in high dimensional spaces, such as the ones appearing in computational statistical physics and Bayesian inference. By varying the diffusion coefficient, there are in fact infinitely many reversible overdamped Langevin dynamics which preserve the target probability measure at hand. This suggests to optimize the diffusion coefficient in order to increase the convergence rate of the dynamics, as measured by the spectral gap of the generator associated with the stochastic differential equation. We analytically study this problem here, obtaining in particular necessary conditions on the optimal diffusion coefficient. We also derive an explicit expression of the optimal diffusion in some homogenized limit. Numerical results, both relying on discretizations of the spectral gap problem and Monte Carlo simulations of the stochastic dynamics, demonstrate the increased quality of the sampling arising from an appropriate choice of the diffusion coefficient.","sentences":["This is a preleminary work.","Overdamped Langevin dynamics are reversible stochastic differential equations which are commonly used to sample probability measures in high dimensional spaces, such as the ones appearing in computational statistical physics and Bayesian inference.","By varying the diffusion coefficient, there are in fact infinitely many reversible overdamped Langevin dynamics which preserve the target probability measure at hand.","This suggests to optimize the diffusion coefficient in order to increase the convergence rate of the dynamics, as measured by the spectral gap of the generator associated with the stochastic differential equation.","We analytically study this problem here, obtaining in particular necessary conditions on the optimal diffusion coefficient.","We also derive an explicit expression of the optimal diffusion in some homogenized limit.","Numerical results, both relying on discretizations of the spectral gap problem and Monte Carlo simulations of the stochastic dynamics, demonstrate the increased quality of the sampling arising from an appropriate choice of the diffusion coefficient."],"url":"http://arxiv.org/abs/2404.12087v1","category":"math.NA"}
{"created":"2024-04-18 11:13:35","title":"Preserving Nature's Ledger: Blockchains in Biodiversity Conservation","abstract":"In the contemporary era, biodiversity conservation emerges as a paramount challenge, necessitating innovative approaches to monitoring, preserving, and enhancing the natural world. This paper explores the integration of blockchain technology in biodiversity conservation, offering a novel perspective on how digital resilience can be built within ecological contexts. Blockchain, with its decentralized and immutable ledger and tokenization affordances, presents a groundbreaking solution for the accurate monitoring and tracking of environmental assets, thereby addressing the critical need for transparency and trust in conservation efforts. Unlike previous more theoretical approaches, by addressing the research question of how blockchain supports digital resilience in biodiversity conservation, this study presents a grounded framework that justifies which blockchain features are essential to decipher specific data contribution and data leveraging processes in an effort to protect our planet's biodiversity, while boosting potential economic benefits for all actors involved, from local farmers, to hardware vendors and artificial intelligence experts, to investors and regular users, volunteers and donors.","sentences":["In the contemporary era, biodiversity conservation emerges as a paramount challenge, necessitating innovative approaches to monitoring, preserving, and enhancing the natural world.","This paper explores the integration of blockchain technology in biodiversity conservation, offering a novel perspective on how digital resilience can be built within ecological contexts.","Blockchain, with its decentralized and immutable ledger and tokenization affordances, presents a groundbreaking solution for the accurate monitoring and tracking of environmental assets, thereby addressing the critical need for transparency and trust in conservation efforts.","Unlike previous more theoretical approaches, by addressing the research question of how blockchain supports digital resilience in biodiversity conservation, this study presents a grounded framework that justifies which blockchain features are essential to decipher specific data contribution and data leveraging processes in an effort to protect our planet's biodiversity, while boosting potential economic benefits for all actors involved, from local farmers, to hardware vendors and artificial intelligence experts, to investors and regular users, volunteers and donors."],"url":"http://arxiv.org/abs/2404.12086v1","category":"cs.CY"}
{"created":"2024-04-18 11:09:33","title":"Big Bang equivalence of $f(R)$ gravity with no cosmological lithium problem","abstract":"$f(R)$ gravity is one of the serious alternatives of general relativity having large range of astronomical consequences. In this work, we study Big Bang Nucleosynthesis (BBN) in $f(R)$ gravity theory. We consider modification to gravity due to the existence of primordial black holes in the radiation era which introduce additional degrees of freedom known as scalarons. We calculate the light element abundances by using the BBN code PArthENoPE. It is found that for a range of scalaron mass $(2.2-3.5) \\times 10^4$ eV, the abundance of lithium is lowered by $3-4$ times the value predicted by general relativistic BBN which is a level desired to address the cosmological lithium problem. For the above scalaron mass range helium abundance is within observed bound. However, deuterium abundance is found to be increased by $3-6$ times the observed primordial abundance which calls for high efficiency of stellar formation and evolution processes for destruction of the same. A novel relation between scalaron mass and black hole mass has been used to estimate that the above scalaron mass range corresponds to primordial black holes of sub-planetary mass ($\\sim 10^{19}$ g) serving as one of the potential non-baryonic dark matter candidates. We infer Big Bang equivalence of power law $f(R)$ gravity with primordial black holes.","sentences":["$f(R)$ gravity is one of the serious alternatives of general relativity having large range of astronomical consequences.","In this work, we study Big Bang Nucleosynthesis (BBN) in $f(R)$ gravity theory.","We consider modification to gravity due to the existence of primordial black holes in the radiation era which introduce additional degrees of freedom known as scalarons.","We calculate the light element abundances by using the BBN code PArthENoPE.","It is found that for a range of scalaron mass $(2.2-3.5) \\times 10^4$ eV, the abundance of lithium is lowered by $3-4$ times the value predicted by general relativistic BBN which is a level desired to address the cosmological lithium problem.","For the above scalaron mass range helium abundance is within observed bound.","However, deuterium abundance is found to be increased by $3-6$ times the observed primordial abundance which calls for high efficiency of stellar formation and evolution processes for destruction of the same.","A novel relation between scalaron mass and black hole mass has been used to estimate that the above scalaron mass range corresponds to primordial black holes of sub-planetary mass ($\\sim 10^{19}$ g) serving as one of the potential non-baryonic dark matter candidates.","We infer Big Bang equivalence of power law $f(R)$ gravity with primordial black holes."],"url":"http://arxiv.org/abs/2404.12084v1","category":"astro-ph.CO"}
{"created":"2024-04-18 11:09:12","title":"The Generic Circular Triangle-Free Graph","abstract":"In this paper, we introduce the generic circular triangle-free graph $\\mathbb C_3$ and propose a finite axiomatization of its first order theory. In particular, our main results show that a countable graph $G$ embeds into $\\mathbb C_3$ if and only if it is a $\\{K_3, K_1 + 2K_2, K_1+C_5, C_6\\}$-free graph. As a byproduct of this result, we obtain a geometric characterization of finite $\\{K_3, K_1 + 2K_2, K_1+C_5, C_6\\}$-free graphs, and the (finite) list of minimal obstructions of unit Helly circular-arc graphs with independence number strictly less than three.   The circular chromatic number $\\chi_c(G)$ is a refinement of the classical chromatic number $\\chi(G)$. We construct $\\mathbb C_3$ so that a graph $G$ has circular chromatic number strictly less than three if and only if $G$ maps homomorphically to $\\mathbb C_3$. We build on our main results to show that $\\chi_c(G) < 3$ if and only if $G$ can be extended to a $\\{K_3, K_1 + 2K_2, K_1+C_5, C_6\\}$-free graph, and in turn, we use this result to reprove an old characterization of $\\chi_c(G) < 3$ due to Brandt (1999). Finally, we answer a question recently asked by Guzm\\'an-Pro, Hell, and Hern\\'andez-Cruz by showing that the problem of deciding for a given finite graph $G$ whether $\\chi_c(G) < 3$ is NP-complete.","sentences":["In this paper, we introduce the generic circular triangle-free graph $\\mathbb C_3$ and propose a finite axiomatization of its first order theory.","In particular, our main results show that a countable graph $G$ embeds into $\\mathbb C_3$ if and only if it is a $\\{K_3, K_1 + 2K_2, K_1+C_5, C_6\\}$-free graph.","As a byproduct of this result, we obtain a geometric characterization of finite $\\{K_3, K_1 + 2K_2, K_1+C_5, C_6\\}$-free graphs, and the (finite) list of minimal obstructions of unit Helly circular-arc graphs with independence number strictly less than three.   ","The circular chromatic number $\\chi_c(G)$ is a refinement of the classical chromatic number $\\chi(G)$. We construct $\\mathbb C_3$ so that a graph $G$ has circular chromatic number strictly less than three if and only if $G$ maps homomorphically to $\\mathbb C_3$. We build on our main results to show that $\\chi_c(G) < 3$ if and only if $G$ can be extended to a $\\{K_3, K_1 + 2K_2, K_1+C_5, C_6\\}$-free graph, and in turn, we use this result to reprove an old characterization of $\\chi_c(G) <","3$ due to Brandt (1999).","Finally, we answer a question recently asked by Guzm\\'an-Pro, Hell, and Hern\\'andez-Cruz by showing that the problem of deciding for a given finite graph $G$ whether $\\chi_c(G) < 3$ is NP-complete."],"url":"http://arxiv.org/abs/2404.12082v1","category":"math.CO"}
{"created":"2024-04-18 11:05:15","title":"MaskCD: A Remote Sensing Change Detection Network Based on Mask Classification","abstract":"Change detection (CD) from remote sensing (RS) images using deep learning has been widely investigated in the literature. It is typically regarded as a pixel-wise labeling task that aims to classify each pixel as changed or unchanged. Although per-pixel classification networks in encoder-decoder structures have shown dominance, they still suffer from imprecise boundaries and incomplete object delineation at various scenes. For high-resolution RS images, partly or totally changed objects are more worthy of attention rather than a single pixel. Therefore, we revisit the CD task from the mask prediction and classification perspective and propose MaskCD to detect changed areas by adaptively generating categorized masks from input image pairs. Specifically, it utilizes a cross-level change representation perceiver (CLCRP) to learn multiscale change-aware representations and capture spatiotemporal relations from encoded features by exploiting deformable multihead self-attention (DeformMHSA). Subsequently, a masked-attention-based detection transformers (MA-DETR) decoder is developed to accurately locate and identify changed objects based on masked attention and self-attention mechanisms. It reconstructs the desired changed objects by decoding the pixel-wise representations into learnable mask proposals and making final predictions from these candidates. Experimental results on five benchmark datasets demonstrate the proposed approach outperforms other state-of-the-art models. Codes and pretrained models are available online (https://github.com/EricYu97/MaskCD).","sentences":["Change detection (CD) from remote sensing (RS) images using deep learning has been widely investigated in the literature.","It is typically regarded as a pixel-wise labeling task that aims to classify each pixel as changed or unchanged.","Although per-pixel classification networks in encoder-decoder structures have shown dominance, they still suffer from imprecise boundaries and incomplete object delineation at various scenes.","For high-resolution RS images, partly or totally changed objects are more worthy of attention rather than a single pixel.","Therefore, we revisit the CD task from the mask prediction and classification perspective and propose MaskCD to detect changed areas by adaptively generating categorized masks from input image pairs.","Specifically, it utilizes a cross-level change representation perceiver (CLCRP) to learn multiscale change-aware representations and capture spatiotemporal relations from encoded features by exploiting deformable multihead self-attention (DeformMHSA).","Subsequently, a masked-attention-based detection transformers (MA-DETR) decoder is developed to accurately locate and identify changed objects based on masked attention and self-attention mechanisms.","It reconstructs the desired changed objects by decoding the pixel-wise representations into learnable mask proposals and making final predictions from these candidates.","Experimental results on five benchmark datasets demonstrate the proposed approach outperforms other state-of-the-art models.","Codes and pretrained models are available online (https://github.com/EricYu97/MaskCD)."],"url":"http://arxiv.org/abs/2404.12081v1","category":"cs.CV"}
{"created":"2024-04-18 11:02:01","title":"Trajectory Planning for Autonomous Vehicle Using Iterative Reward Prediction in Reinforcement Learning","abstract":"Traditional trajectory planning methods for autonomous vehicles have several limitations. Heuristic and explicit simple rules make trajectory lack generality and complex motion. One of the approaches to resolve the above limitations of traditional trajectory planning methods is trajectory planning using reinforcement learning. However, reinforcement learning suffers from instability of learning and prior works of trajectory planning using reinforcement learning didn't consider the uncertainties. In this paper, we propose a trajectory planning method for autonomous vehicles using reinforcement learning. The proposed method includes iterative reward prediction method that stabilizes the learning process, and uncertainty propagation method that makes the reinforcement learning agent to be aware of the uncertainties. The proposed method is experimented in the CARLA simulator. Compared to the baseline method, we have reduced the collision rate by 60.17%, and increased the average reward to 30.82 times.","sentences":["Traditional trajectory planning methods for autonomous vehicles have several limitations.","Heuristic and explicit simple rules make trajectory lack generality and complex motion.","One of the approaches to resolve the above limitations of traditional trajectory planning methods is trajectory planning using reinforcement learning.","However, reinforcement learning suffers from instability of learning and prior works of trajectory planning using reinforcement learning didn't consider the uncertainties.","In this paper, we propose a trajectory planning method for autonomous vehicles using reinforcement learning.","The proposed method includes iterative reward prediction method that stabilizes the learning process, and uncertainty propagation method that makes the reinforcement learning agent to be aware of the uncertainties.","The proposed method is experimented in the CARLA simulator.","Compared to the baseline method, we have reduced the collision rate by 60.17%, and increased the average reward to 30.82 times."],"url":"http://arxiv.org/abs/2404.12079v1","category":"cs.RO"}
{"created":"2024-04-18 10:59:54","title":"TIMIT Speaker Profiling: A Comparison of Multi-task learning and Single-task learning Approaches","abstract":"This study employs deep learning techniques to explore four speaker profiling tasks on the TIMIT dataset, namely gender classification, accent classification, age estimation, and speaker identification, highlighting the potential and challenges of multi-task learning versus single-task models. The motivation for this research is twofold: firstly, to empirically assess the advantages and drawbacks of multi-task learning over single-task models in the context of speaker profiling; secondly, to emphasize the undiminished significance of skillful feature engineering for speaker recognition tasks. The findings reveal challenges in accent classification, and multi-task learning is found advantageous for tasks of similar complexity. Non-sequential features are favored for speaker recognition, but sequential ones can serve as starting points for complex models. The study underscores the necessity of meticulous experimentation and parameter tuning for deep learning models.","sentences":["This study employs deep learning techniques to explore four speaker profiling tasks on the TIMIT dataset, namely gender classification, accent classification, age estimation, and speaker identification, highlighting the potential and challenges of multi-task learning versus single-task models.","The motivation for this research is twofold: firstly, to empirically assess the advantages and drawbacks of multi-task learning over single-task models in the context of speaker profiling; secondly, to emphasize the undiminished significance of skillful feature engineering for speaker recognition tasks.","The findings reveal challenges in accent classification, and multi-task learning is found advantageous for tasks of similar complexity.","Non-sequential features are favored for speaker recognition, but sequential ones can serve as starting points for complex models.","The study underscores the necessity of meticulous experimentation and parameter tuning for deep learning models."],"url":"http://arxiv.org/abs/2404.12077v1","category":"cs.SD"}
{"created":"2024-04-18 10:59:04","title":"Evolutionary Multi-Objective Optimisation for Fairness-Aware Self Adjusting Memory Classifiers in Data Streams","abstract":"This paper introduces a novel approach, evolutionary multi-objective optimisation for fairness-aware self-adjusting memory classifiers, designed to enhance fairness in machine learning algorithms applied to data stream classification. With the growing concern over discrimination in algorithmic decision-making, particularly in dynamic data stream environments, there is a need for methods that ensure fair treatment of individuals across sensitive attributes like race or gender. The proposed approach addresses this challenge by integrating the strengths of the self-adjusting memory K-Nearest-Neighbour algorithm with evolutionary multi-objective optimisation. This combination allows the new approach to efficiently manage concept drift in streaming data and leverage the flexibility of evolutionary multi-objective optimisation to maximise accuracy and minimise discrimination simultaneously. We demonstrate the effectiveness of the proposed approach through extensive experiments on various datasets, comparing its performance against several baseline methods in terms of accuracy and fairness metrics. Our results show that the proposed approach maintains competitive accuracy and significantly reduces discrimination, highlighting its potential as a robust solution for fairness-aware data stream classification. Further analyses also confirm the effectiveness of the strategies to trigger evolutionary multi-objective optimisation and adapt classifiers in the proposed approach.","sentences":["This paper introduces a novel approach, evolutionary multi-objective optimisation for fairness-aware self-adjusting memory classifiers, designed to enhance fairness in machine learning algorithms applied to data stream classification.","With the growing concern over discrimination in algorithmic decision-making, particularly in dynamic data stream environments, there is a need for methods that ensure fair treatment of individuals across sensitive attributes like race or gender.","The proposed approach addresses this challenge by integrating the strengths of the self-adjusting memory K-Nearest-Neighbour algorithm with evolutionary multi-objective optimisation.","This combination allows the new approach to efficiently manage concept drift in streaming data and leverage the flexibility of evolutionary multi-objective optimisation to maximise accuracy and minimise discrimination simultaneously.","We demonstrate the effectiveness of the proposed approach through extensive experiments on various datasets, comparing its performance against several baseline methods in terms of accuracy and fairness metrics.","Our results show that the proposed approach maintains competitive accuracy and significantly reduces discrimination, highlighting its potential as a robust solution for fairness-aware data stream classification.","Further analyses also confirm the effectiveness of the strategies to trigger evolutionary multi-objective optimisation and adapt classifiers in the proposed approach."],"url":"http://arxiv.org/abs/2404.12076v1","category":"cs.AI"}
{"created":"2024-04-18 10:57:32","title":"E-Vote Your Conscience: Perceptions of Coercion and Vote Buying, and the Usability of Fake Credentials in Online Voting","abstract":"Online voting is attractive for convenience and accessibility, but is more susceptible to voter coercion and vote buying than in-person voting. One mitigation is to give voters fake voting credentials that they can yield to a coercer. Fake credentials appear identical to real ones, but cast votes that are silently omitted from the final tally. An important unanswered question is how ordinary voters perceive such a mitigation: whether they could understand and use fake credentials, and whether the coercion risks justify the costs of mitigation. We present the first systematic study of these questions, involving 150 diverse individuals in Boston, Massachusetts. All participants \"registered\" and \"voted\" in a mock election: 120 were exposed to coercion resistance via fake credentials, the rest forming a control group. Of the 120 participants exposed to fake credentials, 96% understood their use. 53% reported that they would create fake credentials in a real-world voting scenario, given the opportunity. 10% mistakenly voted with a fake credential, however. 22% reported either personal experience with or direct knowledge of coercion or vote-buying incidents. These latter participants rated the coercion-resistant system essentially as trustworthy as in-person voting via hand-marked paper ballots. Of the 150 total participants to use the system, 87% successfully created their credentials without assistance; 83% both successfully created and properly used their credentials. Participants give a System Usability Scale score of 70.4, which is slightly above the industry's average score of 68. Our findings appear to support the importance of the coercion problem in general, and the promise of fake credentials as a possible mitigation, but user error rates remain an important usability challenge for future work.","sentences":["Online voting is attractive for convenience and accessibility, but is more susceptible to voter coercion and vote buying than in-person voting.","One mitigation is to give voters fake voting credentials that they can yield to a coercer.","Fake credentials appear identical to real ones, but cast votes that are silently omitted from the final tally.","An important unanswered question is how ordinary voters perceive such a mitigation: whether they could understand and use fake credentials, and whether the coercion risks justify the costs of mitigation.","We present the first systematic study of these questions, involving 150 diverse individuals in Boston, Massachusetts.","All participants \"registered\" and \"voted\" in a mock election: 120 were exposed to coercion resistance via fake credentials, the rest forming a control group.","Of the 120 participants exposed to fake credentials, 96% understood their use.","53% reported that they would create fake credentials in a real-world voting scenario, given the opportunity.","10% mistakenly voted with a fake credential, however.","22% reported either personal experience with or direct knowledge of coercion or vote-buying incidents.","These latter participants rated the coercion-resistant system essentially as trustworthy as in-person voting via hand-marked paper ballots.","Of the 150 total participants to use the system, 87% successfully created their credentials without assistance; 83% both successfully created and properly used their credentials.","Participants give a System Usability Scale score of 70.4, which is slightly above the industry's average score of 68.","Our findings appear to support the importance of the coercion problem in general, and the promise of fake credentials as a possible mitigation, but user error rates remain an important usability challenge for future work."],"url":"http://arxiv.org/abs/2404.12075v1","category":"cs.HC"}
{"created":"2024-04-18 10:57:14","title":"A Flexible Architecture for Web-based GIS Applications using Docker and Graph Databases","abstract":"Regional planning processes and associated redevelopment projects can be complex due to the vast amount of diverse data involved. However, all of this data shares a common geographical reference, especially in the renaturation of former open-cast mining areas. To ensure safety, it is crucial to maintain a comprehensive overview of the interrelated data and draw accurate conclusions. This requires special tools and can be a very time-consuming process. A geographical information system (GIS) is well-suited for this purpose, but even a GIS has limitations when dealing with multiple data types and sources. Additional tools are often necessary to process and view all the data, which can complicate the planning process. Our paper describes a system architecture that addresses the aforementioned issues and provides a simple, yet flexible tool for these activities. The architecture is based on microservices using Docker and is divided into a backend and a frontend. The backend simplifies and generalizes the integration of different data types, while a graph database is used to link relevant data and reveal potential new relationships between them. Finally, a modern web frontend displays the data and relationships.","sentences":["Regional planning processes and associated redevelopment projects can be complex due to the vast amount of diverse data involved.","However, all of this data shares a common geographical reference, especially in the renaturation of former open-cast mining areas.","To ensure safety, it is crucial to maintain a comprehensive overview of the interrelated data and draw accurate conclusions.","This requires special tools and can be a very time-consuming process.","A geographical information system (GIS) is well-suited for this purpose, but even a GIS has limitations when dealing with multiple data types and sources.","Additional tools are often necessary to process and view all the data, which can complicate the planning process.","Our paper describes a system architecture that addresses the aforementioned issues and provides a simple, yet flexible tool for these activities.","The architecture is based on microservices using Docker and is divided into a backend and a frontend.","The backend simplifies and generalizes the integration of different data types, while a graph database is used to link relevant data and reveal potential new relationships between them.","Finally, a modern web frontend displays the data and relationships."],"url":"http://arxiv.org/abs/2404.12074v1","category":"cs.HC"}
{"created":"2024-04-18 10:54:38","title":"Van Hove singularities in the density of states of a chaotic dynamical system","abstract":"The statistics of a chaotic recursion relation can be predicted by constructing an associated sequence of periodic elliptic operators. For such operators, the density of states is well understood, can be computed straightforwardly and explicit formulas can often be derived. The example studied here is a non-linear recursion relation which can be related to a sequence of periodic operators generated by a Fibonacci tiling rule. This link is used to derive an explicit formula for the limiting distribution of orbits of the non-linear recursion relation. This distribution contains characteristic features of the associated operators' densities of states, such as Van Hove singularities near to critical values.","sentences":["The statistics of a chaotic recursion relation can be predicted by constructing an associated sequence of periodic elliptic operators.","For such operators, the density of states is well understood, can be computed straightforwardly and explicit formulas can often be derived.","The example studied here is a non-linear recursion relation which can be related to a sequence of periodic operators generated by a Fibonacci tiling rule.","This link is used to derive an explicit formula for the limiting distribution of orbits of the non-linear recursion relation.","This distribution contains characteristic features of the associated operators' densities of states, such as Van Hove singularities near to critical values."],"url":"http://arxiv.org/abs/2404.12073v1","category":"nlin.CD"}
{"created":"2024-04-18 10:45:47","title":"Towards an Approximation Theory of Observable Operator Models","abstract":"Observable operator models (OOMs) offer a powerful framework for modelling stochastic processes, surpassing the traditional hidden Markov models (HMMs) in generality and efficiency. However, using OOMs to model infinite-dimensional processes poses significant theoretical challenges. This article explores a rigorous approach to developing an approximation theory for OOMs of infinite-dimensional processes. Building upon foundational work outlined in an unpublished tutorial [Jae98], an inner product structure on the space of future distributions is rigorously established and the continuity of observable operators with respect to the associated 2-norm is proven. The original theorem proven in this thesis describes a fundamental obstacle in making an infinite-dimensional space of future distributions into a Hilbert space. The presented findings lay the groundwork for future research in approximating observable operators of infinite-dimensional processes, while a remedy to the encountered obstacle is suggested.","sentences":["Observable operator models (OOMs) offer a powerful framework for modelling stochastic processes, surpassing the traditional hidden Markov models (HMMs) in generality and efficiency.","However, using OOMs to model infinite-dimensional processes poses significant theoretical challenges.","This article explores a rigorous approach to developing an approximation theory for OOMs of infinite-dimensional processes.","Building upon foundational work outlined in an unpublished tutorial [Jae98], an inner product structure on the space of future distributions is rigorously established and the continuity of observable operators with respect to the associated 2-norm is proven.","The original theorem proven in this thesis describes a fundamental obstacle in making an infinite-dimensional space of future distributions into a Hilbert space.","The presented findings lay the groundwork for future research in approximating observable operators of infinite-dimensional processes, while a remedy to the encountered obstacle is suggested."],"url":"http://arxiv.org/abs/2404.12070v1","category":"math.PR"}
{"created":"2024-04-18 10:39:44","title":"Does dark energy really revive using DESI 2024 data?","abstract":"We investigate the impact of the Dark Energy Spectroscopic Instrument (DESI) 2024 data on dark energy scenarios. We thus analyze three typologies of models, the first in which the cosmic speed up is related to thermodynamics, the second associated with Taylor expansions of the barotropic factor, whereas the third based on \\emph{ad hoc} dark energy parameterizations. In this respect, we perform Monte Carlo Markov chain analyses, adopting the Metropolis-Hastings algorithm, of 12 models. To do so, we first work at the background, inferring \\emph{a posteriori} kinematic quantities associated with each model. Afterwards, we obtain early time predictions, computing departures on the growth evolution with respect to the model that better fits DESI data. We find that the best model to fit data \\emph{is not} the Chevallier-Polarski-Linder (CPL) parametrization, but rather a more complicated log-corrected dark energy contribution. To check the goodness of our findings, we further directly fit the product, $r_d h_0$, concluding that $r_d h_0$ is anticorrelated with the mass. This treatment is worked out by removing a precise data point placed at $z=0.51$. Surprisingly, in this case the results again align with the $\\Lambda$CDM model, \\emph{indicating that the possible tension between the concordance paradigm and the CPL model can be severely alleviated}. We conclude that future data points will be essential to clarify whether dynamical dark energy is really in tension with the $\\Lambda$CDM model.","sentences":["We investigate the impact of the Dark Energy Spectroscopic Instrument (DESI) 2024 data on dark energy scenarios.","We thus analyze three typologies of models, the first in which the cosmic speed up is related to thermodynamics, the second associated with Taylor expansions of the barotropic factor, whereas the third based on \\emph{ad hoc} dark energy parameterizations.","In this respect, we perform Monte Carlo Markov chain analyses, adopting the Metropolis-Hastings algorithm, of 12 models.","To do so, we first work at the background, inferring \\emph{a posteriori} kinematic quantities associated with each model.","Afterwards, we obtain early time predictions, computing departures on the growth evolution with respect to the model that better fits DESI data.","We find that the best model to fit data \\emph{is not} the Chevallier-Polarski-Linder (CPL) parametrization, but rather a more complicated log-corrected dark energy contribution.","To check the goodness of our findings, we further directly fit the product, $r_d h_0$, concluding that $r_d h_0$ is anticorrelated with the mass.","This treatment is worked out by removing a precise data point placed at $z=0.51$. Surprisingly, in this case the results again align with the $\\Lambda$CDM model, \\emph{indicating that the possible tension between the concordance paradigm and the CPL model can be severely alleviated}.","We conclude that future data points will be essential to clarify whether dynamical dark energy is really in tension with the $\\Lambda$CDM model."],"url":"http://arxiv.org/abs/2404.12068v1","category":"astro-ph.CO"}
{"created":"2024-04-18 10:31:15","title":"Subordinators and generalized heat kernels: Random time change and long time dynamics","abstract":"This paper focuses on studying the long-time dynamics of the subordination process for a range of linear evolution equations, with a special emphasis on the fractional heat equation. By treating inverse subordinators as random time variables and employing the subordination principle to solve forward Kolmogorov equations, we explore the behavior of the solutions over extended periods. We provide a detailed description of the specific classes of subordinators suitable for conducting asymptotic analysis. Our findings not only extend existing research, but also enhance the results previously presented in [9, 10].","sentences":["This paper focuses on studying the long-time dynamics of the subordination process for a range of linear evolution equations, with a special emphasis on the fractional heat equation.","By treating inverse subordinators as random time variables and employing the subordination principle to solve forward Kolmogorov equations, we explore the behavior of the solutions over extended periods.","We provide a detailed description of the specific classes of subordinators suitable for conducting asymptotic analysis.","Our findings not only extend existing research, but also enhance the results previously presented in [9, 10]."],"url":"http://arxiv.org/abs/2404.12067v1","category":"math.AP"}
{"created":"2024-04-18 10:25:42","title":"RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models","abstract":"The escalating challenge of misinformation, particularly in the context of political discourse, necessitates advanced solutions for fact-checking. We introduce innovative approaches to enhance the reliability and efficiency of multimodal fact-checking through the integration of Large Language Models (LLMs) with Retrieval-augmented Generation (RAG)- based advanced reasoning techniques. This work proposes two novel methodologies, Chain of RAG (CoRAG) and Tree of RAG (ToRAG). The approaches are designed to handle multimodal claims by reasoning the next questions that need to be answered based on previous evidence. Our approaches improve the accuracy of veracity predictions and the generation of explanations over the traditional fact-checking approach of sub-question generation with chain of thought veracity prediction. By employing multimodal LLMs adept at analyzing both text and images, this research advances the capability of automated systems in identifying and countering misinformation.","sentences":["The escalating challenge of misinformation, particularly in the context of political discourse, necessitates advanced solutions for fact-checking.","We introduce innovative approaches to enhance the reliability and efficiency of multimodal fact-checking through the integration of Large Language Models (LLMs) with Retrieval-augmented Generation (RAG)- based advanced reasoning techniques.","This work proposes two novel methodologies, Chain of RAG (CoRAG) and Tree of RAG (ToRAG).","The approaches are designed to handle multimodal claims by reasoning the next questions that need to be answered based on previous evidence.","Our approaches improve the accuracy of veracity predictions and the generation of explanations over the traditional fact-checking approach of sub-question generation with chain of thought veracity prediction.","By employing multimodal LLMs adept at analyzing both text and images, this research advances the capability of automated systems in identifying and countering misinformation."],"url":"http://arxiv.org/abs/2404.12065v1","category":"cs.CL"}
{"created":"2024-04-18 10:20:37","title":"MIDGET: Music Conditioned 3D Dance Generation","abstract":"In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.","sentences":["In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm.","To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction.","We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset.","Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music."],"url":"http://arxiv.org/abs/2404.12062v1","category":"cs.SD"}
{"created":"2024-04-18 10:12:19","title":"Constructing metric spaces from systems of walls","abstract":"We give a general procedure for constructing metric spaces from systems of partitions. This generalises and provides analogues of Sageev's construction of dual CAT(0) cube complexes for the settings of hyperbolic and injective metric spaces.   As applications, we produce a ``universal'' hyperbolic action for groups with strongly contracting elements, and show that many groups with ``coarsely cubical'' features admit geometric actions on injective metric spaces. In an appendix with Davide Spriano, we show that a large class of groups have an infinite-dimensional space of quasimorphisms.","sentences":["We give a general procedure for constructing metric spaces from systems of partitions.","This generalises and provides analogues of Sageev's construction of dual CAT(0) cube complexes for the settings of hyperbolic and injective metric spaces.   ","As applications, we produce a ``universal'' hyperbolic action for groups with strongly contracting elements, and show that many groups with ``coarsely cubical'' features admit geometric actions on injective metric spaces.","In an appendix with Davide Spriano, we show that a large class of groups have an infinite-dimensional space of quasimorphisms."],"url":"http://arxiv.org/abs/2404.12057v1","category":"math.GR"}
{"created":"2024-04-18 10:12:18","title":"Deconstructing Human-AI Collaboration: Agency, Interaction, and Adaptation","abstract":"As full AI-based automation remains out of reach in most real-world applications, the focus has instead shifted to leveraging the strengths of both human and AI agents, creating effective collaborative systems. The rapid advances in this area have yielded increasingly more complex systems and frameworks, while the nuance of their characterization has gotten more vague. Similarly, the existing conceptual models no longer capture the elaborate processes of these systems nor describe the entire scope of their collaboration paradigms. In this paper, we propose a new unified set of dimensions through which to analyze and describe human-AI systems. Our conceptual model is centered around three high-level aspects - agency, interaction, and adaptation - and is developed through a multi-step process. Firstly, an initial design space is proposed by surveying the literature and consolidating existing definitions and conceptual frameworks. Secondly, this model is iteratively refined and validated by conducting semi-structured interviews with nine researchers in this field. Lastly, to illustrate the applicability of our design space, we utilize it to provide a structured description of selected human-AI systems.","sentences":["As full AI-based automation remains out of reach in most real-world applications, the focus has instead shifted to leveraging the strengths of both human and AI agents, creating effective collaborative systems.","The rapid advances in this area have yielded increasingly more complex systems and frameworks, while the nuance of their characterization has gotten more vague.","Similarly, the existing conceptual models no longer capture the elaborate processes of these systems nor describe the entire scope of their collaboration paradigms.","In this paper, we propose a new unified set of dimensions through which to analyze and describe human-AI systems.","Our conceptual model is centered around three high-level aspects - agency, interaction, and adaptation - and is developed through a multi-step process.","Firstly, an initial design space is proposed by surveying the literature and consolidating existing definitions and conceptual frameworks.","Secondly, this model is iteratively refined and validated by conducting semi-structured interviews with nine researchers in this field.","Lastly, to illustrate the applicability of our design space, we utilize it to provide a structured description of selected human-AI systems."],"url":"http://arxiv.org/abs/2404.12056v1","category":"cs.HC"}
{"created":"2024-04-18 10:06:58","title":"A method for non-linear inversion of the stellar structure applied to gravity-mode pulsators","abstract":"We present a method for a non-linear asteroseismic inversion suitable for gravity-mode pulsators and apply it to slowly pulsating B-type (SPB) stars. Our inversion method is based on the iterative improvement of a parameterised static stellar structure model, which in turn is based on constraints from the observed oscillation periods. We present tests to demonstrate that the method is successful in recovering the properties of artificial targets both inside and outside the parameter space. We also present a test of our method on the well-studied SPB star KIC 7760680. We believe that this method is promising for carrying out detailed analyses of observations of SPB and $\\gamma$ Dor stars and will provide complementary information to evolutionary models.","sentences":["We present a method for a non-linear asteroseismic inversion suitable for gravity-mode pulsators and apply it to slowly pulsating B-type (SPB) stars.","Our inversion method is based on the iterative improvement of a parameterised static stellar structure model, which in turn is based on constraints from the observed oscillation periods.","We present tests to demonstrate that the method is successful in recovering the properties of artificial targets both inside and outside the parameter space.","We also present a test of our method on the well-studied SPB star KIC 7760680.","We believe that this method is promising for carrying out detailed analyses of observations of SPB and $\\gamma$ Dor stars and will provide complementary information to evolutionary models."],"url":"http://arxiv.org/abs/2404.12052v1","category":"astro-ph.SR"}
{"created":"2024-04-18 10:02:41","title":"Celestial CFT from $H_3^+$-WZW Model","abstract":"Recently, there has been a growing interest in celestial holography, which is holography in asymptotic flat spacetimes. This holographic duality exhibits numerous mysterious and fruitful features, particularly on the dual CFT side. In this paper, we present the candidate of dual CFT of Minkowski spacetime extracted from $SL(2,\\mathbb{C})/SU(2)\\cong H^+_3$ Wess-Zumino-Witten (WZW) model, the simplest non-compact CFT. We demonstrate that it reproduces the well-known principal series and correlation functions dual to the bulk scattering amplitudes.","sentences":["Recently, there has been a growing interest in celestial holography, which is holography in asymptotic flat spacetimes.","This holographic duality exhibits numerous mysterious and fruitful features, particularly on the dual CFT side.","In this paper, we present the candidate of dual CFT of Minkowski spacetime extracted from $SL(2,\\mathbb{C})/SU(2)\\cong H^+_3$ Wess-Zumino-Witten (WZW) model, the simplest non-compact CFT.","We demonstrate that it reproduces the well-known principal series and correlation functions dual to the bulk scattering amplitudes."],"url":"http://arxiv.org/abs/2404.12049v1","category":"hep-th"}
{"created":"2024-04-18 10:01:08","title":"Self-Adjusting Evolutionary Algorithms Are Slow on Multimodal Landscapes","abstract":"The one-fifth rule and its generalizations are a classical parameter control mechanism in discrete domains. They have also been transferred to control the offspring population size of the $(1, \\lambda)$-EA. This has been shown to work very well for hill-climbing, and combined with a restart mechanism it was recently shown by Hevia Fajardo and Sudholt to improve performance on the multi-modal problem Cliff drastically.   In this work we show that the positive results do not extend to other types of local optima. On the distorted OneMax benchmark, the self-adjusting $(1, \\lambda)$-EA is slowed down just as elitist algorithms because self-adaptation prevents the algorithm from escaping from local optima. This makes the self-adaptive algorithm considerably worse than good static parameter choices, which do allow to escape from local optima efficiently. We show this theoretically and complement the result with empirical runtime results.","sentences":["The one-fifth rule and its generalizations are a classical parameter control mechanism in discrete domains.","They have also been transferred to control the offspring population size of the $(1, \\lambda)$-EA.","This has been shown to work very well for hill-climbing, and combined with a restart mechanism it was recently shown by Hevia Fajardo and Sudholt to improve performance on the multi-modal problem Cliff drastically.   ","In this work we show that the positive results do not extend to other types of local optima.","On the distorted OneMax benchmark, the self-adjusting $(1, \\lambda)$-EA is slowed down just as elitist algorithms because self-adaptation prevents the algorithm from escaping from local optima.","This makes the self-adaptive algorithm considerably worse than good static parameter choices, which do allow to escape from local optima efficiently.","We show this theoretically and complement the result with empirical runtime results."],"url":"http://arxiv.org/abs/2404.12047v1","category":"cs.NE"}
{"created":"2024-04-18 09:58:51","title":"RAM: Towards an Ever-Improving Memory System by Learning from Communications","abstract":"We introduce RAM, an innovative RAG-based framework with an ever-improving memory. Inspired by humans' pedagogical process, RAM utilizes recursively reasoning-based retrieval and experience reflections to continually update the memory and learn from users' communicative feedback, namely communicative learning. Extensive experiments with both simulated and real users demonstrate significant improvements over traditional RAG and self-knowledge methods, particularly excelling in handling false premise and multi-hop questions. Furthermore, RAM exhibits promising adaptability to various feedback and retrieval method chain types, showcasing its potential for advancing AI capabilities in dynamic knowledge acquisition and lifelong learning.","sentences":["We introduce RAM, an innovative RAG-based framework with an ever-improving memory.","Inspired by humans' pedagogical process, RAM utilizes recursively reasoning-based retrieval and experience reflections to continually update the memory and learn from users' communicative feedback, namely communicative learning.","Extensive experiments with both simulated and real users demonstrate significant improvements over traditional RAG and self-knowledge methods, particularly excelling in handling false premise and multi-hop questions.","Furthermore, RAM exhibits promising adaptability to various feedback and retrieval method chain types, showcasing its potential for advancing AI capabilities in dynamic knowledge acquisition and lifelong learning."],"url":"http://arxiv.org/abs/2404.12045v1","category":"cs.AI"}
{"created":"2024-04-18 09:52:18","title":"Can We Catch the Elephant? The Evolvement of Hallucination Evaluation on Natural Language Generation: A Survey","abstract":"Hallucination in Natural Language Generation (NLG) is like the elephant in the room, obvious but often overlooked until recent achievements significantly improved the fluency and grammatical accuracy of generated text. For Large Language Models (LLMs), hallucinations can happen in various downstream tasks and casual conversations, which need accurate assessment to enhance reliability and safety. However, current studies on hallucination evaluation vary greatly, and people still find it difficult to sort out and select the most appropriate evaluation methods. Moreover, as NLP research gradually shifts to the domain of LLMs, it brings new challenges to this direction. This paper provides a comprehensive survey on the evolvement of hallucination evaluation methods, aiming to address three key aspects: 1) Diverse definitions and granularity of facts; 2) The categories of automatic evaluators and their applicability; 3) Unresolved issues and future directions.","sentences":["Hallucination in Natural Language Generation (NLG) is like the elephant in the room, obvious but often overlooked until recent achievements significantly improved the fluency and grammatical accuracy of generated text.","For Large Language Models (LLMs), hallucinations can happen in various downstream tasks and casual conversations, which need accurate assessment to enhance reliability and safety.","However, current studies on hallucination evaluation vary greatly, and people still find it difficult to sort out and select the most appropriate evaluation methods.","Moreover, as NLP research gradually shifts to the domain of LLMs, it brings new challenges to this direction.","This paper provides a comprehensive survey on the evolvement of hallucination evaluation methods, aiming to address three key aspects: 1) Diverse definitions and granularity of facts; 2) The categories of automatic evaluators and their applicability; 3) Unresolved issues and future directions."],"url":"http://arxiv.org/abs/2404.12041v1","category":"cs.CL"}
{"created":"2024-04-18 09:44:56","title":"Data-free Knowledge Distillation for Fine-grained Visual Categorization","abstract":"Data-free knowledge distillation (DFKD) is a promising approach for addressing issues related to model compression, security privacy, and transmission restrictions. Although the existing methods exploiting DFKD have achieved inspiring achievements in coarse-grained classification, in practical applications involving fine-grained classification tasks that require more detailed distinctions between similar categories, sub-optimal results are obtained. To address this issue, we propose an approach called DFKD-FGVC that extends DFKD to fine-grained visual categorization~(FGVC) tasks. Our approach utilizes an adversarial distillation framework with attention generator, mixed high-order attention distillation, and semantic feature contrast learning. Specifically, we introduce a spatial-wise attention mechanism to the generator to synthesize fine-grained images with more details of discriminative parts. We also utilize the mixed high-order attention mechanism to capture complex interactions among parts and the subtle differences among discriminative features of the fine-grained categories, paying attention to both local features and semantic context relationships. Moreover, we leverage the teacher and student models of the distillation framework to contrast high-level semantic feature maps in the hyperspace, comparing variances of different categories. We evaluate our approach on three widely-used FGVC benchmarks (Aircraft, Cars196, and CUB200) and demonstrate its superior performance.","sentences":["Data-free knowledge distillation (DFKD) is a promising approach for addressing issues related to model compression, security privacy, and transmission restrictions.","Although the existing methods exploiting DFKD have achieved inspiring achievements in coarse-grained classification, in practical applications involving fine-grained classification tasks that require more detailed distinctions between similar categories, sub-optimal results are obtained.","To address this issue, we propose an approach called DFKD-FGVC that extends DFKD to fine-grained visual categorization~(FGVC) tasks.","Our approach utilizes an adversarial distillation framework with attention generator, mixed high-order attention distillation, and semantic feature contrast learning.","Specifically, we introduce a spatial-wise attention mechanism to the generator to synthesize fine-grained images with more details of discriminative parts.","We also utilize the mixed high-order attention mechanism to capture complex interactions among parts and the subtle differences among discriminative features of the fine-grained categories, paying attention to both local features and semantic context relationships.","Moreover, we leverage the teacher and student models of the distillation framework to contrast high-level semantic feature maps in the hyperspace, comparing variances of different categories.","We evaluate our approach on three widely-used FGVC benchmarks (Aircraft, Cars196, and CUB200) and demonstrate its superior performance."],"url":"http://arxiv.org/abs/2404.12037v1","category":"cs.CV"}
{"created":"2024-04-18 09:42:24","title":"Monitoring Unmanned Aircraft: Specification, Integration, and Lessons-learned","abstract":"This paper reports on the integration of runtime monitoring into fully-electric aircraft designed by Volocopter, a German aircraft manufacturer of electric multi-rotor helicopters. The runtime monitor recognizes hazardous situations and system faults. Since the correct operation of the monitor is critical for the safety of the aircraft, the development of the monitor must follow strict aeronautical standards. This includes the integration of the monitor into different development environments, such as log-file analysis, hardware/software-in-the-loop testing, and test flights. We have used the stream-based monitoring framework RTLola to generate monitors for a range of requirements. In this paper, we present representative monitoring specifications and our lessons learned from integrating the generated monitors. Our main finding is that the specification and the integration need to be decoupled, because the specification remains stable throughout the development process, whereas the different development stages require a separate integration of the monitor into each environment. We achieve this decoupling with a novel abstraction layer in the monitoring framework that adapts the monitor to each environment without affecting the core component generated from the specification. The decoupling of the integration has also allowed us to react quickly to the frequent changes in the hardware and software environment of the monitor due to the fast-paced development of the aircraft in a startup company.","sentences":["This paper reports on the integration of runtime monitoring into fully-electric aircraft designed by Volocopter, a German aircraft manufacturer of electric multi-rotor helicopters.","The runtime monitor recognizes hazardous situations and system faults.","Since the correct operation of the monitor is critical for the safety of the aircraft, the development of the monitor must follow strict aeronautical standards.","This includes the integration of the monitor into different development environments, such as log-file analysis, hardware/software-in-the-loop testing, and test flights.","We have used the stream-based monitoring framework RTLola to generate monitors for a range of requirements.","In this paper, we present representative monitoring specifications and our lessons learned from integrating the generated monitors.","Our main finding is that the specification and the integration need to be decoupled, because the specification remains stable throughout the development process, whereas the different development stages require a separate integration of the monitor into each environment.","We achieve this decoupling with a novel abstraction layer in the monitoring framework that adapts the monitor to each environment without affecting the core component generated from the specification.","The decoupling of the integration has also allowed us to react quickly to the frequent changes in the hardware and software environment of the monitor due to the fast-paced development of the aircraft in a startup company."],"url":"http://arxiv.org/abs/2404.12035v1","category":"cs.SE"}
{"created":"2024-04-18 09:41:05","title":"Generation and annihilation of three dimensional magnetic nulls in extrapolated solar coronal magnetic field: Data-based Implicit Large Eddy Simulation","abstract":"Three-dimensional magnetic nulls are the points where magnetic field vanishes and are preferential sites for magnetic reconnection: a process which converts magnetic energy into heat and accelerates charged particles along with a rearrangement of magnetic field lines. In the solar corona, the reconnections manifest as coronal transients including solar flares, coronal mass ejections and coronal jets. The nulls are generally found to be collocated with complex active regions on the solar photosphere. Extrapolation of magnetic field from corresponding photospheric magnetogram indicate an abundance of these nulls in solar atmosphere. Nevertheless, their generation is still not well understood. Recently, Maurya et al. (2023) have demonstrated magnetic reconnection to be a cause for generation and annihilation of magnetic nulls through magnetohydrodynamics simulation, where the initial magnetic field is idealized to have a single radial null. This article further extends the study in a more realistic scenario where the initial magnetic field is constructed by extrapolating photospheric magnetogram data and hence, incorporates field line complexities inherent to a complex active region. For the purpose, the active region NOAA 11977 hosting a C6.6 class flare is selected. The simulation is initiated using non-force-free extrapolated magnetic field from the photospheric vector magnetogram at around 02:48:00 UT on 17 February 2014, 16 minutes before the flare peak. The generation, annihilation and dynamics of nulls are explored by a complimentary usage of trilinear null detection technique and tracing of magnetic field line dynamics. It is found that the nulls can spontaneously generate/annihilate in pairs while preserving the topological degree and can have observational implications like footpoint brightenings. Magnetic reconnection is found to be the cause of such generation and annihilation.","sentences":["Three-dimensional magnetic nulls are the points where magnetic field vanishes and are preferential sites for magnetic reconnection: a process which converts magnetic energy into heat and accelerates charged particles along with a rearrangement of magnetic field lines.","In the solar corona, the reconnections manifest as coronal transients including solar flares, coronal mass ejections and coronal jets.","The nulls are generally found to be collocated with complex active regions on the solar photosphere.","Extrapolation of magnetic field from corresponding photospheric magnetogram indicate an abundance of these nulls in solar atmosphere.","Nevertheless, their generation is still not well understood.","Recently, Maurya et al. (2023) have demonstrated magnetic reconnection to be a cause for generation and annihilation of magnetic nulls through magnetohydrodynamics simulation, where the initial magnetic field is idealized to have a single radial null.","This article further extends the study in a more realistic scenario where the initial magnetic field is constructed by extrapolating photospheric magnetogram data and hence, incorporates field line complexities inherent to a complex active region.","For the purpose, the active region NOAA 11977 hosting a C6.6 class flare is selected.","The simulation is initiated using non-force-free extrapolated magnetic field from the photospheric vector magnetogram at around 02:48:00 UT on 17 February 2014, 16 minutes before the flare peak.","The generation, annihilation and dynamics of nulls are explored by a complimentary usage of trilinear null detection technique and tracing of magnetic field line dynamics.","It is found that the nulls can spontaneously generate/annihilate in pairs while preserving the topological degree and can have observational implications like footpoint brightenings.","Magnetic reconnection is found to be the cause of such generation and annihilation."],"url":"http://arxiv.org/abs/2404.12034v1","category":"astro-ph.SR"}
{"created":"2024-04-18 09:32:11","title":"A variational approach to a fuzzy Boltzmann equation","abstract":"We study a fuzzy Boltzmann equation, where particles interact via delocalised collisions, in contrast to classical Boltzmann equations. We discuss the existence and uniqueness of solutions and provide a natural variational characterisation by casting the fuzzy Boltzmann equation into the framework of GENERIC systems (General Equations for Non-Equilibrium Reversible-Irreversible Coupling).","sentences":["We study a fuzzy Boltzmann equation, where particles interact via delocalised collisions, in contrast to classical Boltzmann equations.","We discuss the existence and uniqueness of solutions and provide a natural variational characterisation by casting the fuzzy Boltzmann equation into the framework of GENERIC systems (General Equations for Non-Equilibrium Reversible-Irreversible Coupling)."],"url":"http://arxiv.org/abs/2404.12032v1","category":"math.AP"}
{"created":"2024-04-18 09:31:03","title":"MLS-Track: Multilevel Semantic Interaction in RMOT","abstract":"The new trend in multi-object tracking task is to track objects of interest using natural language. However, the scarcity of paired prompt-instance data hinders its progress. To address this challenge, we propose a high-quality yet low-cost data generation method base on Unreal Engine 5 and construct a brand-new benchmark dataset, named Refer-UE-City, which primarily includes scenes from intersection surveillance videos, detailing the appearance and actions of people and vehicles. Specifically, it provides 14 videos with a total of 714 expressions, and is comparable in scale to the Refer-KITTI dataset. Additionally, we propose a multi-level semantic-guided multi-object framework called MLS-Track, where the interaction between the model and text is enhanced layer by layer through the introduction of Semantic Guidance Module (SGM) and Semantic Correlation Branch (SCB). Extensive experiments on Refer-UE-City and Refer-KITTI datasets demonstrate the effectiveness of our proposed framework and it achieves state-of-the-art performance. Code and datatsets will be available.","sentences":["The new trend in multi-object tracking task is to track objects of interest using natural language.","However, the scarcity of paired prompt-instance data hinders its progress.","To address this challenge, we propose a high-quality yet low-cost data generation method base on Unreal Engine 5 and construct a brand-new benchmark dataset, named Refer-UE-City, which primarily includes scenes from intersection surveillance videos, detailing the appearance and actions of people and vehicles.","Specifically, it provides 14 videos with a total of 714 expressions, and is comparable in scale to the Refer-KITTI dataset.","Additionally, we propose a multi-level semantic-guided multi-object framework called MLS-Track, where the interaction between the model and text is enhanced layer by layer through the introduction of Semantic Guidance Module (SGM) and Semantic Correlation Branch (SCB).","Extensive experiments on Refer-UE-City and Refer-KITTI datasets demonstrate the effectiveness of our proposed framework and it achieves state-of-the-art performance.","Code and datatsets will be available."],"url":"http://arxiv.org/abs/2404.12031v1","category":"cs.CV"}
{"created":"2024-04-18 09:29:08","title":"Mapping back and forth between model predictive control and neural networks","abstract":"Model predictive control (MPC) for linear systems with quadratic costs and linear constraints is shown to admit an exact representation as an implicit neural network. A method to \"unravel\" the implicit neural network of MPC into an explicit one is also introduced. As well as building links between model-based and data-driven control, these results emphasize the capability of implicit neural networks for representing solutions of optimisation problems, as such problems are themselves implicitly defined functions.","sentences":["Model predictive control (MPC) for linear systems with quadratic costs and linear constraints is shown to admit an exact representation as an implicit neural network.","A method to \"unravel\" the implicit neural network of MPC into an explicit one is also introduced.","As well as building links between model-based and data-driven control, these results emphasize the capability of implicit neural networks for representing solutions of optimisation problems, as such problems are themselves implicitly defined functions."],"url":"http://arxiv.org/abs/2404.12030v1","category":"eess.SY"}
{"created":"2024-04-18 09:26:23","title":"Deep learning to detect gravitational waves from binary close encounters: Fast parameter estimation using normalizing flows","abstract":"A yet undetected class of GW signals is represented by the close encounters between compact objects in highly-eccentric e~1 orbits, that can occur in binary systems formed in dense environments such as globular clusters. The expected gravitational signals are short-duration pulses that would repeat over a much longer time scale in case of multiple passages at periastron. These sources represent a unique opportunity of exploring astrophysical formation channels as well as a different way of testing GR. In the case of binary systems containing neutron stars, the observation of these sources could help to constrain the EOS, thanks to the signature left in the GW signal by the f-modes excitation that can occur during the encounter. The detection and PE of these signals is challenging given the short duration of expected signals and the sensitivities of current ground-based GW interferometers. We present a novel approach that exploits Probabilistic ML. We have used Conditional Normalizing Flows to model complex probability distributions and therefore infer posterior distributions for the source parameters. Fast detection and PE is very important as it could trigger electromagnetic follow-up campaigns and offer the possibility to study these events in a multimessenger context. To develop and test the algorithm, we have focused on the simulations of single bursts emission obtained using the Effective Fly-by formalism and embedded in the noise of aLIGO and Virgo during O3. Our proposed model outperforms standard Bayesian methods in accuracy and is 5 orders of magnitude faster, being able to produce 5x10^4 posterior samples in just 0.5s. The results are extremely promising and constitute the first successful attempt for a fast and complete parameter estimation of binary CEs using deep learning, offering a new approach to study the evolution of orbital parameters of compact binary systems.","sentences":["A yet undetected class of GW signals is represented by the close encounters between compact objects in highly-eccentric e~1 orbits, that can occur in binary systems formed in dense environments such as globular clusters.","The expected gravitational signals are short-duration pulses that would repeat over a much longer time scale in case of multiple passages at periastron.","These sources represent a unique opportunity of exploring astrophysical formation channels as well as a different way of testing GR.","In the case of binary systems containing neutron stars, the observation of these sources could help to constrain the EOS, thanks to the signature left in the GW signal by the f-modes excitation that can occur during the encounter.","The detection and PE of these signals is challenging given the short duration of expected signals and the sensitivities of current ground-based GW interferometers.","We present a novel approach that exploits Probabilistic ML.","We have used Conditional Normalizing Flows to model complex probability distributions and therefore infer posterior distributions for the source parameters.","Fast detection and PE is very important as it could trigger electromagnetic follow-up campaigns and offer the possibility to study these events in a multimessenger context.","To develop and test the algorithm, we have focused on the simulations of single bursts emission obtained using the Effective Fly-by formalism and embedded in the noise of aLIGO and Virgo during O3.","Our proposed model outperforms standard Bayesian methods in accuracy and is 5 orders of magnitude faster, being able to produce 5x10^4 posterior samples in just 0.5s.","The results are extremely promising and constitute the first successful attempt for a fast and complete parameter estimation of binary CEs using deep learning, offering a new approach to study the evolution of orbital parameters of compact binary systems."],"url":"http://arxiv.org/abs/2404.12028v1","category":"gr-qc"}
{"created":"2024-04-18 09:21:16","title":"Meta-Auxiliary Learning for Micro-Expression Recognition","abstract":"Micro-expressions (MEs) are involuntary movements revealing people's hidden feelings, which has attracted numerous interests for its objectivity in emotion detection. However, despite its wide applications in various scenarios, micro-expression recognition (MER) remains a challenging problem in real life due to three reasons, including (i) data-level: lack of data and imbalanced classes, (ii) feature-level: subtle, rapid changing, and complex features of MEs, and (iii) decision-making-level: impact of individual differences. To address these issues, we propose a dual-branch meta-auxiliary learning method, called LightmanNet, for fast and robust micro-expression recognition. Specifically, LightmanNet learns general MER knowledge from limited data through a dual-branch bi-level optimization process: (i) In the first level, it obtains task-specific MER knowledge by learning in two branches, where the first branch is for learning MER features via primary MER tasks, while the other branch is for guiding the model obtain discriminative features via auxiliary tasks, i.e., image alignment between micro-expressions and macro-expressions since their resemblance in both spatial and temporal behavioral patterns. The two branches of learning jointly constrain the model of learning meaningful task-specific MER knowledge while avoiding learning noise or superficial connections between MEs and emotions that may damage its generalization ability. (ii) In the second level, LightmanNet further refines the learned task-specific knowledge, improving model generalization and efficiency. Extensive experiments on various benchmark datasets demonstrate the superior robustness and efficiency of LightmanNet.","sentences":["Micro-expressions (MEs) are involuntary movements revealing people's hidden feelings, which has attracted numerous interests for its objectivity in emotion detection.","However, despite its wide applications in various scenarios, micro-expression recognition (MER) remains a challenging problem in real life due to three reasons, including (i) data-level: lack of data and imbalanced classes, (ii) feature-level: subtle, rapid changing, and complex features of MEs, and (iii) decision-making-level: impact of individual differences.","To address these issues, we propose a dual-branch meta-auxiliary learning method, called LightmanNet, for fast and robust micro-expression recognition.","Specifically, LightmanNet learns general MER knowledge from limited data through a dual-branch bi-level optimization process: (i) In the first level, it obtains task-specific MER knowledge by learning in two branches, where the first branch is for learning MER features via primary MER tasks, while the other branch is for guiding the model obtain discriminative features via auxiliary tasks, i.e., image alignment between micro-expressions and macro-expressions since their resemblance in both spatial and temporal behavioral patterns.","The two branches of learning jointly constrain the model of learning meaningful task-specific MER knowledge while avoiding learning noise or superficial connections between MEs and emotions that may damage its generalization ability.","(ii) In the second level, LightmanNet further refines the learned task-specific knowledge, improving model generalization and efficiency.","Extensive experiments on various benchmark datasets demonstrate the superior robustness and efficiency of LightmanNet."],"url":"http://arxiv.org/abs/2404.12024v1","category":"cs.CV"}
{"created":"2024-04-18 09:17:46","title":"Context-Aware Orchestration of Energy-Efficient Gossip Learning Schemes","abstract":"Fully distributed learning schemes such as Gossip Learning (GL) are gaining momentum due to their scalability and effectiveness even in dynamic settings. However, they often imply a high utilization of communication and computing resources, whose energy footprint may jeopardize the learning process, particularly on battery-operated IoT devices. To address this issue, we present Optimized Gossip Learning (OGL)}, a distributed training approach based on the combination of GL with adaptive optimization of the learning process, which allows for achieving a target accuracy while minimizing the energy consumption of the learning process. We propose a data-driven approach to OGL management that relies on optimizing in real-time for each node the number of training epochs and the choice of which model to exchange with neighbors based on patterns of node contacts, models' quality, and available resources at each node. Our approach employs a DNN model for dynamic tuning of the aforementioned parameters, trained by an infrastructure-based orchestrator function. We performed our assessments on two different datasets, leveraging time-varying random graphs and a measurement-based dynamic urban scenario. Results suggest that our approach is highly efficient and effective in a broad spectrum of network scenarios.","sentences":["Fully distributed learning schemes such as Gossip Learning (GL) are gaining momentum due to their scalability and effectiveness even in dynamic settings.","However, they often imply a high utilization of communication and computing resources, whose energy footprint may jeopardize the learning process, particularly on battery-operated IoT devices.","To address this issue, we present Optimized Gossip Learning (OGL)}, a distributed training approach based on the combination of GL with adaptive optimization of the learning process, which allows for achieving a target accuracy while minimizing the energy consumption of the learning process.","We propose a data-driven approach to OGL management that relies on optimizing in real-time for each node the number of training epochs and the choice of which model to exchange with neighbors based on patterns of node contacts, models' quality, and available resources at each node.","Our approach employs a DNN model for dynamic tuning of the aforementioned parameters, trained by an infrastructure-based orchestrator function.","We performed our assessments on two different datasets, leveraging time-varying random graphs and a measurement-based dynamic urban scenario.","Results suggest that our approach is highly efficient and effective in a broad spectrum of network scenarios."],"url":"http://arxiv.org/abs/2404.12023v1","category":"cs.NI"}
{"created":"2024-04-18 09:17:06","title":"Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration","abstract":"Large language models (LLMs) have recently shown remarkable performance across a wide range of tasks. However, the substantial number of parameters in LLMs contributes to significant latency during model inference. This is particularly evident when utilizing autoregressive decoding methods, which generate one token in a single forward process, thereby not fully capitalizing on the parallel computing capabilities of GPUs. In this paper, we propose a novel parallel decoding approach, namely \\textit{hidden transfer}, which decodes multiple successive tokens simultaneously in a single forward pass. The idea is to transfer the intermediate hidden states of the previous context to the \\textit{pseudo} hidden states of the future tokens to be generated, and then the pseudo hidden states will pass the following transformer layers thereby assimilating more semantic information and achieving superior predictive accuracy of the future tokens.   Besides, we use the novel tree attention mechanism to simultaneously generate and verify multiple candidates of output sequences, which ensure the lossless generation and further improves the generation efficiency of our method. Experiments demonstrate the effectiveness of our method. We conduct a lot of analytic experiments to prove our motivation. In terms of acceleration metrics, we outperform all the single-model acceleration techniques, including Medusa and Self-Speculative decoding.","sentences":["Large language models (LLMs) have recently shown remarkable performance across a wide range of tasks.","However, the substantial number of parameters in LLMs contributes to significant latency during model inference.","This is particularly evident when utilizing autoregressive decoding methods, which generate one token in a single forward process, thereby not fully capitalizing on the parallel computing capabilities of GPUs.","In this paper, we propose a novel parallel decoding approach, namely \\textit{hidden transfer}, which decodes multiple successive tokens simultaneously in a single forward pass.","The idea is to transfer the intermediate hidden states of the previous context to the \\textit{pseudo} hidden states of the future tokens to be generated, and then the pseudo hidden states will pass the following transformer layers thereby assimilating more semantic information and achieving superior predictive accuracy of the future tokens.   ","Besides, we use the novel tree attention mechanism to simultaneously generate and verify multiple candidates of output sequences, which ensure the lossless generation and further improves the generation efficiency of our method.","Experiments demonstrate the effectiveness of our method.","We conduct a lot of analytic experiments to prove our motivation.","In terms of acceleration metrics, we outperform all the single-model acceleration techniques, including Medusa and Self-Speculative decoding."],"url":"http://arxiv.org/abs/2404.12022v1","category":"cs.CL"}
{"created":"2024-04-18 09:16:41","title":"First 2D electron density measurements using Coherence Imaging Spectroscopy in the MAST-U Super-X divertor","abstract":"2D profiles of electron density and neutral temperature are inferred from multi-delay Coherence Imaging Spectroscopy data of divertor plasmas using a non-linear inversion technique. The inference is based on imaging the spectral line-broadening of Balmer lines and can differentiate between the Doppler and Stark broadening components by measuring the fringe contrast at multiple interferometric delays simultaneously. The model has been applied to images generated from simulated density profiles to evaluate its performance. Typical mean absolute errors of 30 percent are achieved, which are consistent with Monte Carlo uncertainty propagation accounting for noise, uncertainties in the calibrations, and in the model inputs. The analysis has been tested on experimental data from the MAST-U Super-X divertor, where it infers typical electron densities of 2-3 $10^{19}$ m$^{-3}$ and neutral temperatures of 0-2 eV during beam-heated L-mode discharges. The results are shown to be in reasonable agreement with the other available diagnostics.","sentences":["2D profiles of electron density and neutral temperature are inferred from multi-delay Coherence Imaging Spectroscopy data of divertor plasmas using a non-linear inversion technique.","The inference is based on imaging the spectral line-broadening of Balmer lines and can differentiate between the Doppler and Stark broadening components by measuring the fringe contrast at multiple interferometric delays simultaneously.","The model has been applied to images generated from simulated density profiles to evaluate its performance.","Typical mean absolute errors of 30 percent are achieved, which are consistent with Monte Carlo uncertainty propagation accounting for noise, uncertainties in the calibrations, and in the model inputs.","The analysis has been tested on experimental data from the MAST-U Super-X divertor, where it infers typical electron densities of 2-3 $10^{19}$ m$^{-3}$ and neutral temperatures of 0-2 eV during beam-heated L-mode discharges.","The results are shown to be in reasonable agreement with the other available diagnostics."],"url":"http://arxiv.org/abs/2404.12021v1","category":"physics.plasm-ph"}
{"created":"2024-04-18 09:16:02","title":"Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering","abstract":"Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, \\textit{MUSIC-AVQA-R}, crafted in two steps: rephrasing questions within the test split of a public dataset (\\textit{MUSIC-AVQA}) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on both datasets, especially obtaining a significant improvement of 9.68\\% on the proposed dataset. Extensive ablation experiments are conducted on these two datasets to validate the effectiveness of the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset.","sentences":["Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs.","Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness.","Furthermore, current datasets may not provide a precise diagnostic for these methods.","To tackle these challenges, firstly, we propose a novel dataset, \\textit{MUSIC-AVQA-R}, crafted in two steps: rephrasing questions within the test split of a public dataset (\\textit{MUSIC-AVQA}) and subsequently introducing distribution shifts to split questions.","The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions.","Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning.","Experimental results show that this architecture achieves state-of-the-art performance on both datasets, especially obtaining a significant improvement of 9.68\\% on the proposed dataset.","Extensive ablation experiments are conducted on these two datasets to validate the effectiveness of the debiasing strategy.","Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset."],"url":"http://arxiv.org/abs/2404.12020v1","category":"cs.CV"}
{"created":"2024-04-18 09:11:57","title":"Automated Real-Time Inspection in Indoor and Outdoor 3D Environments with Cooperative Aerial Robots","abstract":"This work introduces a cooperative inspection system designed to efficiently control and coordinate a team of distributed heterogeneous UAV agents for the inspection of 3D structures in cluttered, unknown spaces. Our proposed approach employs a two-stage innovative methodology. Initially, it leverages the complementary sensing capabilities of the robots to cooperatively map the unknown environment. It then generates optimized, collision-free inspection paths, thereby ensuring comprehensive coverage of the structure's surface area. The effectiveness of our system is demonstrated through qualitative and quantitative results from extensive Gazebo-based simulations that closely replicate real-world inspection scenarios, highlighting its ability to thoroughly inspect real-world-like 3D structures.","sentences":["This work introduces a cooperative inspection system designed to efficiently control and coordinate a team of distributed heterogeneous UAV agents for the inspection of 3D structures in cluttered, unknown spaces.","Our proposed approach employs a two-stage innovative methodology.","Initially, it leverages the complementary sensing capabilities of the robots to cooperatively map the unknown environment.","It then generates optimized, collision-free inspection paths, thereby ensuring comprehensive coverage of the structure's surface area.","The effectiveness of our system is demonstrated through qualitative and quantitative results from extensive Gazebo-based simulations that closely replicate real-world inspection scenarios, highlighting its ability to thoroughly inspect real-world-like 3D structures."],"url":"http://arxiv.org/abs/2404.12018v1","category":"cs.RO"}
{"created":"2024-04-18 09:11:08","title":"Acceleration of electromagnetic shower development and enhancement of light yield in oriented scintillating crystals","abstract":"We observed a substantial increase of the scintillation light output of lead tungstate (PbWO$_4$) at a small incidence angle with respect to two main lattice axes. This reflects the acceleration of electromagnetic shower development that occurs in the crystalline Strong Field. We measured the scintillation light generated by $120$-$\\mathrm{GeV}$ electrons and $10$-$100$-$\\mathrm{GeV}$ $\\gamma$ rays on thick samples. This result deepens the knowledge of the shower development mechanisms in crystal scintillators and could pave the way to the development of innovative accelerator- and space-borne calorimeters.","sentences":["We observed a substantial increase of the scintillation light output of lead tungstate (PbWO$_4$) at a small incidence angle with respect to two main lattice axes.","This reflects the acceleration of electromagnetic shower development that occurs in the crystalline Strong Field.","We measured the scintillation light generated by $120$-$\\mathrm{GeV}$ electrons and $10$-$100$-$\\mathrm{GeV}$ $\\gamma$ rays on thick samples.","This result deepens the knowledge of the shower development mechanisms in crystal scintillators and could pave the way to the development of innovative accelerator- and space-borne calorimeters."],"url":"http://arxiv.org/abs/2404.12016v1","category":"hep-ex"}
{"created":"2024-04-18 09:06:05","title":"What does CLIP know about peeling a banana?","abstract":"Humans show an innate capability to identify tools to support specific actions. The association between objects parts and the actions they facilitate is usually named affordance. Being able to segment objects parts depending on the tasks they afford is crucial to enable intelligent robots to use objects of daily living. Traditional supervised learning methods for affordance segmentation require costly pixel-level annotations, while weakly supervised approaches, though less demanding, still rely on object-interaction examples and support a closed set of actions. These limitations hinder scalability, may introduce biases, and usually restrict models to a limited set of predefined actions. This paper proposes AffordanceCLIP, to overcome these limitations by leveraging the implicit affordance knowledge embedded within large pre-trained Vision-Language models like CLIP. We experimentally demonstrate that CLIP, although not explicitly trained for affordances detection, retains valuable information for the task. Our AffordanceCLIP achieves competitive zero-shot performance compared to methods with specialized training, while offering several advantages: i) it works with any action prompt, not just a predefined set; ii) it requires training only a small number of additional parameters compared to existing solutions and iii) eliminates the need for direct supervision on action-object pairs, opening new perspectives for functionality-based reasoning of models.","sentences":["Humans show an innate capability to identify tools to support specific actions.","The association between objects parts and the actions they facilitate is usually named affordance.","Being able to segment objects parts depending on the tasks they afford is crucial to enable intelligent robots to use objects of daily living.","Traditional supervised learning methods for affordance segmentation require costly pixel-level annotations, while weakly supervised approaches, though less demanding, still rely on object-interaction examples and support a closed set of actions.","These limitations hinder scalability, may introduce biases, and usually restrict models to a limited set of predefined actions.","This paper proposes AffordanceCLIP, to overcome these limitations by leveraging the implicit affordance knowledge embedded within large pre-trained Vision-Language models like CLIP.","We experimentally demonstrate that CLIP, although not explicitly trained for affordances detection, retains valuable information for the task.","Our AffordanceCLIP achieves competitive zero-shot performance compared to methods with specialized training, while offering several advantages: i) it works with any action prompt, not just a predefined set; ii) it requires training only a small number of additional parameters compared to existing solutions and iii) eliminates the need for direct supervision on action-object pairs, opening new perspectives for functionality-based reasoning of models."],"url":"http://arxiv.org/abs/2404.12015v1","category":"cs.CV"}
{"created":"2024-04-18 09:04:15","title":"Sequential Compositional Generalization in Multimodal Models","abstract":"The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks. However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting. Our study aims to address this by examining sequential compositional generalization using \\textsc{CompAct} (\\underline{Comp}ositional \\underline{Act}ivities)\\footnote{Project Page: \\url{http://cyberiada.github.io/CompAct}}, a carefully constructed, perceptually grounded dataset set within a rich backdrop of egocentric kitchen activity videos. Each instance in our dataset is represented with a combination of raw video footage, naturally occurring sound, and crowd-sourced step-by-step descriptions. More importantly, our setup ensures that the individual concepts are consistently distributed across training and evaluation sets, while their compositions are novel in the evaluation set. We conduct a comprehensive assessment of several unimodal and multimodal models. Our findings reveal that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts. This highlights the importance of multimodality while charting a trajectory for future research in this domain.","sentences":["The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks.","However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting.","Our study aims to address this by examining sequential compositional generalization using \\textsc{CompAct} (\\underline{Comp}ositional \\underline{Act}ivities)\\footnote{Project Page: \\url{http://cyberiada.github.io/CompAct}}, a carefully constructed, perceptually grounded dataset set within a rich backdrop of egocentric kitchen activity videos.","Each instance in our dataset is represented with a combination of raw video footage, naturally occurring sound, and crowd-sourced step-by-step descriptions.","More importantly, our setup ensures that the individual concepts are consistently distributed across training and evaluation sets, while their compositions are novel in the evaluation set.","We conduct a comprehensive assessment of several unimodal and multimodal models.","Our findings reveal that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts.","This highlights the importance of multimodality while charting a trajectory for future research in this domain."],"url":"http://arxiv.org/abs/2404.12013v1","category":"cs.CL"}
{"created":"2024-04-18 09:03:09","title":"Pseudo-random generators using linear feedback shift registers with output extraction","abstract":"The use of three extractors, fed by linear feedback shift registers (LFSR) for generating pseudo-random bit streams is investigated. Specifically, a standard LFSR is combined with a von Neumann extractor, a modified LFSR, extended by the all-zero state, is combined with an output logic, which translates every three bits from the LFSR into up to two output bits and a run extraction of the input bit stream into single output bits are investigated. The latter two achieve better efficiency in using bits from the primary bit stream, the last one reaches 50\\%. Compared to other generator logics, the three extractors investigated are less performant in terms of their cryptographic strength. However, the focus of this report is on the quality of the pseudo-random bit stream in comparison to really random bits and on the efficiency of using the bits of the primary stream from the LFSR and generating valid output bits, while fulfilling a minimum cryptographic strength only, beyond that of the pure LFSR.","sentences":["The use of three extractors, fed by linear feedback shift registers (LFSR) for generating pseudo-random bit streams is investigated.","Specifically, a standard LFSR is combined with a von Neumann extractor, a modified LFSR, extended by the all-zero state, is combined with an output logic, which translates every three bits from the LFSR into up to two output bits and a run extraction of the input bit stream into single output bits are investigated.","The latter two achieve better efficiency in using bits from the primary bit stream, the last one reaches 50\\%.","Compared to other generator logics, the three extractors investigated are less performant in terms of their cryptographic strength.","However, the focus of this report is on the quality of the pseudo-random bit stream in comparison to really random bits and on the efficiency of using the bits of the primary stream from the LFSR and generating valid output bits, while fulfilling a minimum cryptographic strength only, beyond that of the pure LFSR."],"url":"http://arxiv.org/abs/2404.12011v1","category":"cs.CR"}
{"created":"2024-04-18 09:02:45","title":"ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused with High-Quality Lexical and Syntactic Diversity","abstract":"Paraphrase generation is a pivotal task in natural language processing (NLP). Existing datasets in the domain lack syntactic and lexical diversity, resulting in paraphrases that closely resemble the source sentences. Moreover, these datasets often contain hate speech and noise, and may unintentionally include non-English language sentences. This research introduces ParaFusion, a large-scale, high-quality English paraphrase dataset developed using Large Language Models (LLM) to address these challenges. ParaFusion augments existing datasets with high-quality data, significantly enhancing both lexical and syntactic diversity while maintaining close semantic similarity. It also mitigates the presence of hate speech and reduces noise, ensuring a cleaner and more focused English dataset. Results show that ParaFusion offers at least a 25% improvement in both syntactic and lexical diversity, measured across several metrics for each data source. The paper also aims to set a gold standard for paraphrase evaluation as it contains one of the most comprehensive evaluation strategies to date. The results underscore the potential of ParaFusion as a valuable resource for improving NLP applications.","sentences":["Paraphrase generation is a pivotal task in natural language processing (NLP).","Existing datasets in the domain lack syntactic and lexical diversity, resulting in paraphrases that closely resemble the source sentences.","Moreover, these datasets often contain hate speech and noise, and may unintentionally include non-English language sentences.","This research introduces ParaFusion, a large-scale, high-quality English paraphrase dataset developed using Large Language Models (LLM) to address these challenges.","ParaFusion augments existing datasets with high-quality data, significantly enhancing both lexical and syntactic diversity while maintaining close semantic similarity.","It also mitigates the presence of hate speech and reduces noise, ensuring a cleaner and more focused English dataset.","Results show that ParaFusion offers at least a 25% improvement in both syntactic and lexical diversity, measured across several metrics for each data source.","The paper also aims to set a gold standard for paraphrase evaluation as it contains one of the most comprehensive evaluation strategies to date.","The results underscore the potential of ParaFusion as a valuable resource for improving NLP applications."],"url":"http://arxiv.org/abs/2404.12010v1","category":"cs.CL"}
{"created":"2024-04-18 08:59:32","title":"How Do Recommendation Models Amplify Popularity Bias? An Analysis from the Spectral Perspective","abstract":"Recommendation Systems (RS) are often plagued by popularity bias. Specifically,when recommendation models are trained on long-tailed datasets, they not only inherit this bias but often exacerbate it. This effect undermines both the precision and fairness of RS and catalyzes the so-called Matthew Effect. Despite the widely recognition of this issue, the fundamental causes remain largely elusive. In our research, we delve deeply into popularity bias amplification. Our comprehensive theoretical and empirical investigations lead to two core insights: 1) Item popularity is memorized in the principal singular vector of the score matrix predicted by the recommendation model; 2) The dimension collapse phenomenon amplifies the impact of principal singular vector on model predictions, intensifying the popularity bias. Based on these insights, we propose a novel method to mitigate this bias by imposing penalties on the magnitude of the principal singular value. Considering the heavy computational burden in directly evaluating the gradient of the principal singular value, we develop an efficient algorithm that harnesses the inherent properties of the singular vector. Extensive experiments across seven real-world datasets and three testing scenarios have been conducted to validate the superiority of our method.","sentences":["Recommendation Systems (RS) are often plagued by popularity bias.","Specifically,when recommendation models are trained on long-tailed datasets, they not only inherit this bias but often exacerbate it.","This effect undermines both the precision and fairness of RS and catalyzes the so-called Matthew Effect.","Despite the widely recognition of this issue, the fundamental causes remain largely elusive.","In our research, we delve deeply into popularity bias amplification.","Our comprehensive theoretical and empirical investigations lead to two core insights: 1) Item popularity is memorized in the principal singular vector of the score matrix predicted by the recommendation model; 2) The dimension collapse phenomenon amplifies the impact of principal singular vector on model predictions, intensifying the popularity bias.","Based on these insights, we propose a novel method to mitigate this bias by imposing penalties on the magnitude of the principal singular value.","Considering the heavy computational burden in directly evaluating the gradient of the principal singular value, we develop an efficient algorithm that harnesses the inherent properties of the singular vector.","Extensive experiments across seven real-world datasets and three testing scenarios have been conducted to validate the superiority of our method."],"url":"http://arxiv.org/abs/2404.12008v1","category":"cs.IR"}
{"created":"2024-04-18 08:59:31","title":"On the Canonical Bundle Formula and Adjunction for Generalized Kaehler Pairs","abstract":"In this article we prove analogs of Kawamata's canonical bundle formula, Kawamata subadjunction and plt/lc inversion of adjunction for generalized pairs on Kaehler varieties. We also show that a conjecture of BDPPin dimension n-1 implies that the cone theorem holds for any n-dimensional Kaehler generalized klt pair. Along the way, we obtain more complete versions of some results due to Collins-Tosatti and Cao-Hoering.","sentences":["In this article we prove analogs of Kawamata's canonical bundle formula, Kawamata subadjunction and plt/lc inversion of adjunction for generalized pairs on Kaehler varieties.","We also show that a conjecture of BDPPin dimension n-1 implies that the cone theorem holds for any n-dimensional Kaehler generalized klt pair.","Along the way, we obtain more complete versions of some results due to Collins-Tosatti and Cao-Hoering."],"url":"http://arxiv.org/abs/2404.12007v1","category":"math.AG"}
{"created":"2024-04-18 08:55:59","title":"Generation of relativistic electrons at the termination shock in the solar flare region","abstract":"Solar flares are accompanied by an enhanced emission of electromagnetic waves from the radio up to the gamma-ray range. The associated hard X-ray (HXR) and microwave radiation is generated by energetic electrons, which carry a substantial part of the energy released during a flare. The flare is generally understood as a manifestation of magnetic reconnection in the corona. The so-called standard CSHKP model is one of the most widely accepted models for eruptive flares. The solar flare on September 10, 2017 offers a unique opportunity to study this model. The observations from the Expanded Owens Valley Solar Array (EOVSA) show that 1.6x10^4 electrons with energies >300 keV were generated in the flare region. There are signatures in solar radio and extreme ultraviolet observations as well as numerical simulations that a termination shock (TS) appears in the magnetic reconnection outflow region. Electrons accelerated at the TS can be considered to generate the loop-top HXR sources. In contrast to previous studies, we investigate whether the heating of the plasma at the TS provides enough relativistic electrons needed for the HXR and microwave emission observed during the X8.2 solar flare on September 10, 2017. We studied the heating of the plasma at the TS by evaluating the jump in the temperature across the shock by means of the Rankine-Hugoniot relationships under coronal circumstances measured during that event. The part of relativistic electrons was calculated in the heated downstream region. In the magnetic reconnection outflow region, the plasma is strongly heated at the TS. Thus, there are enough energetic electrons in the tail of the electron distribution function needed for the microwave and HXR emission observed during that event. The generation of relativistic electrons at the TS is a possible mechanism to explain the enhanced microwave and HXR radiation emitted during flares.","sentences":["Solar flares are accompanied by an enhanced emission of electromagnetic waves from the radio up to the gamma-ray range.","The associated hard X-ray (HXR) and microwave radiation is generated by energetic electrons, which carry a substantial part of the energy released during a flare.","The flare is generally understood as a manifestation of magnetic reconnection in the corona.","The so-called standard CSHKP model is one of the most widely accepted models for eruptive flares.","The solar flare on September 10, 2017 offers a unique opportunity to study this model.","The observations from the Expanded Owens Valley Solar Array (EOVSA) show that 1.6x10^4 electrons with energies >300 keV were generated in the flare region.","There are signatures in solar radio and extreme ultraviolet observations as well as numerical simulations that a termination shock (TS) appears in the magnetic reconnection outflow region.","Electrons accelerated at the TS can be considered to generate the loop-top HXR sources.","In contrast to previous studies, we investigate whether the heating of the plasma at the TS provides enough relativistic electrons needed for the HXR and microwave emission observed during the X8.2 solar flare on September 10, 2017.","We studied the heating of the plasma at the TS by evaluating the jump in the temperature across the shock by means of the Rankine-Hugoniot relationships under coronal circumstances measured during that event.","The part of relativistic electrons was calculated in the heated downstream region.","In the magnetic reconnection outflow region, the plasma is strongly heated at the TS.","Thus, there are enough energetic electrons in the tail of the electron distribution function needed for the microwave and HXR emission observed during that event.","The generation of relativistic electrons at the TS is a possible mechanism to explain the enhanced microwave and HXR radiation emitted during flares."],"url":"http://arxiv.org/abs/2404.12005v1","category":"astro-ph.SR"}
{"created":"2024-04-18 08:53:25","title":"How does the $X(3872)$ show up in $e^+e^-$ collisions: dip versus peak","abstract":"We demonstrate that the dip observed near the total energy of 3872 MeV in the recent cross section data from the BESIII Collaboration for $e^+e^-\\to J/\\psi\\pi^+\\pi^- $ admits a natural explanation as a coupled-channel effect: it is a consequence of unitarity and a strong $S$-wave $D\\bar D^*$ attraction that generates the state $X(3872)$. We anticipate the appearance of a similar dip in the $e^+e^-\\to J/\\psi\\pi^+\\pi^-\\pi^0$ final state near the $D^*\\bar{D}^*$ threshold driven by the same general mechanism, then to be interpreted as a signature of the predicted spin-two partner of the $X(3872)$.","sentences":["We demonstrate that the dip observed near the total energy of 3872 MeV in the recent cross section data from the BESIII Collaboration for $e^+e^-\\to J/\\psi\\pi^+\\pi^- $ admits a natural explanation as a coupled-channel effect: it is a consequence of unitarity and a strong $S$-wave $D\\bar D^*$ attraction that generates the state $X(3872)$.","We anticipate the appearance of a similar dip in the $e^+e^-\\to J/\\psi\\pi^+\\pi^-\\pi^0$ final state near the $D^*\\bar{D}^*$ threshold driven by the same general mechanism, then to be interpreted as a signature of the predicted spin-two partner of the $X(3872)$."],"url":"http://arxiv.org/abs/2404.12003v1","category":"hep-ph"}
{"created":"2024-04-18 08:51:14","title":"How far are AI-powered programming assistants from meeting developers' needs?","abstract":"Recent In-IDE AI coding assistant tools (ACATs) like GitHub Copilot have significantly impacted developers' coding habits. While some studies have examined their effectiveness, there lacks in-depth investigation into the actual assistance process. To bridge this gap, we simulate real development scenarios encompassing three typical types of software development tasks and recruit 27 computer science students to investigate their behavior with three popular ACATs. Our goal is to comprehensively assess ACATs' effectiveness, explore characteristics of recommended code, identify reasons for modifications, and understand users' challenges and expectations. To facilitate the study, we develop an experimental platform that includes a data collection plugin for VSCode IDE and provides functions for screen recording, code evaluation, and automatic generation of personalized interview and survey questions. Through analysis of the collected data, we find that ACATs generally enhance task completion rates, reduce time, improve code quality, and increase self-perceived productivity. However, the improvement is influenced by both the nature of coding tasks and users' experience level. Notably, for experienced participants, the use of ACATs may even increase completion time. We observe that \"edited line completion\" is the most frequently recommended way, while \"comments completion\" and \"string completion\" have the lowest acceptance rates. The primary reasons for modifying recommended code are disparities between output formats and requirements, flawed logic, and inconsistent code styles. In terms of challenges and expectations, optimization of service access and help documentation is also concerned by participants except for functionality and performance. Our study provides valuable insights into the effectiveness and usability of ACATs, informing further improvements in their design and implementation.","sentences":["Recent In-IDE AI coding assistant tools (ACATs) like GitHub Copilot have significantly impacted developers' coding habits.","While some studies have examined their effectiveness, there lacks in-depth investigation into the actual assistance process.","To bridge this gap, we simulate real development scenarios encompassing three typical types of software development tasks and recruit 27 computer science students to investigate their behavior with three popular ACATs.","Our goal is to comprehensively assess ACATs' effectiveness, explore characteristics of recommended code, identify reasons for modifications, and understand users' challenges and expectations.","To facilitate the study, we develop an experimental platform that includes a data collection plugin for VSCode IDE and provides functions for screen recording, code evaluation, and automatic generation of personalized interview and survey questions.","Through analysis of the collected data, we find that ACATs generally enhance task completion rates, reduce time, improve code quality, and increase self-perceived productivity.","However, the improvement is influenced by both the nature of coding tasks and users' experience level.","Notably, for experienced participants, the use of ACATs may even increase completion time.","We observe that \"edited line completion\" is the most frequently recommended way, while \"comments completion\" and \"string completion\" have the lowest acceptance rates.","The primary reasons for modifying recommended code are disparities between output formats and requirements, flawed logic, and inconsistent code styles.","In terms of challenges and expectations, optimization of service access and help documentation is also concerned by participants except for functionality and performance.","Our study provides valuable insights into the effectiveness and usability of ACATs, informing further improvements in their design and implementation."],"url":"http://arxiv.org/abs/2404.12000v1","category":"cs.SE"}
{"created":"2024-04-18 08:49:38","title":"Token-level Direct Preference Optimization","abstract":"Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at https://github.com/Vance0124/Token-level-Direct-Preference-Optimization.","sentences":["Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions.","This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models.","However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion.","In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level.","Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity.","Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling.","Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity.","Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods.","Our code is open-sourced at https://github.com/Vance0124/Token-level-Direct-Preference-Optimization."],"url":"http://arxiv.org/abs/2404.11999v1","category":"cs.CL"}
{"created":"2024-04-18 17:52:05","title":"Multichannel-GaAsP-photomultiplier-based fiber bundle ISM-STED microscope","abstract":"The usage of a GaAsP (gallium arsenide phosphide) photomultiplier for microscopical imaging allows the evaluation of low-light luminescent objects. We designed a setup for collecting a confocal microscopic image signal, which is divided into 14 equal-sized input channels. The division is achieved with a beamsplitter and two fiber bundles consisting of seven fibers each. Re-imaging the confocal pinhole by such a densely packed fiber bundle permits the utilization of a photon re-localization approach to overcome the optical resolution limit. The center fiber creates a real-time image, while the outer fibers enable a higher-resolution image via an image scanning microscope (ISM) signal calculation. The fiber bundles are enclosed in a fused silica capillary and are drawn out to create one solid fiber bundle. During the drawing process, the fiber bundles are tapered down to an outer diameter size of 400 {\\mu}m, with each fiber having a less than 0.3 Airy unit diameter. For the photomultiplier interface, all fibers of both fiber bundles are integrated into a v-groove array, with each fiber representing a detection input, which is followed by projection optics for imaging onto the multichannel detector. The resulting confocal super-resolution microscope is suitable for the application of time-correlated single photon counting (TCSPC) techniques such as fluorescence lifetime imaging (FLIM), time-resolved anisotropy, or F\\\"orster resonance energy transfer (FRET) imaging","sentences":["The usage of a GaAsP (gallium arsenide phosphide) photomultiplier for microscopical imaging allows the evaluation of low-light luminescent objects.","We designed a setup for collecting a confocal microscopic image signal, which is divided into 14 equal-sized input channels.","The division is achieved with a beamsplitter and two fiber bundles consisting of seven fibers each.","Re-imaging the confocal pinhole by such a densely packed fiber bundle permits the utilization of a photon re-localization approach to overcome the optical resolution limit.","The center fiber creates a real-time image, while the outer fibers enable a higher-resolution image via an image scanning microscope (ISM) signal calculation.","The fiber bundles are enclosed in a fused silica capillary and are drawn out to create one solid fiber bundle.","During the drawing process, the fiber bundles are tapered down to an outer diameter size of 400 {\\mu}m, with each fiber having a less than 0.3 Airy unit diameter.","For the photomultiplier interface, all fibers of both fiber bundles are integrated into a v-groove array, with each fiber representing a detection input, which is followed by projection optics for imaging onto the multichannel detector.","The resulting confocal super-resolution microscope is suitable for the application of time-correlated single photon counting (TCSPC) techniques such as fluorescence lifetime imaging (FLIM), time-resolved anisotropy, or F\\\"orster resonance energy transfer (FRET) imaging"],"url":"http://arxiv.org/abs/2404.12370v1","category":"physics.optics"}
{"created":"2024-04-18 16:55:33","title":"Subtraction and Addition of Propagating Photons by Two-Level Emitters","abstract":"Coherent manipulation of quantum states of light is key to photonic quantum information processing. In this Letter, we show that a passive two-level nonlinearity suffices to implement non-Gaussian quantum operations on propagating field modes. In particular, the collective light-matter interaction can efficiently extract a single photon from a multi-photon input wave packet to an orthogonal temporal mode. We accurately describe the single-photon subtraction process by elements of an intuitive quantum-trajectory model. By employing this process, quantum information protocols gain orders of magnitude improved efficiency over heralded schemes with linear optics. The reverse process can be used to add photons one-by-one to a single wave-packet mode and compose arbitrarily large Fock states with a finite total success probability $>96.7\\%$.","sentences":["Coherent manipulation of quantum states of light is key to photonic quantum information processing.","In this Letter, we show that a passive two-level nonlinearity suffices to implement non-Gaussian quantum operations on propagating field modes.","In particular, the collective light-matter interaction can efficiently extract a single photon from a multi-photon input wave packet to an orthogonal temporal mode.","We accurately describe the single-photon subtraction process by elements of an intuitive quantum-trajectory model.","By employing this process, quantum information protocols gain orders of magnitude improved efficiency over heralded schemes with linear optics.","The reverse process can be used to add photons one-by-one to a single wave-packet mode and compose arbitrarily large Fock states with a finite total success probability $>96.7\\%$."],"url":"http://arxiv.org/abs/2404.12328v1","category":"quant-ph"}
{"created":"2024-04-18 16:35:38","title":"ASID: Active Exploration for System Identification in Robotic Manipulation","abstract":"Model-free control strategies such as reinforcement learning have shown the ability to learn control strategies without requiring an accurate model or simulator of the world. While this is appealing due to the lack of modeling requirements, such methods can be sample inefficient, making them impractical in many real-world domains. On the other hand, model-based control techniques leveraging accurate simulators can circumvent these challenges and use a large amount of cheap simulation data to learn controllers that can effectively transfer to the real world. The challenge with such model-based techniques is the requirement for an extremely accurate simulation, requiring both the specification of appropriate simulation assets and physical parameters. This requires considerable human effort to design for every environment being considered. In this work, we propose a learning system that can leverage a small amount of real-world data to autonomously refine a simulation model and then plan an accurate control strategy that can be deployed in the real world. Our approach critically relies on utilizing an initial (possibly inaccurate) simulator to design effective exploration policies that, when deployed in the real world, collect high-quality data. We demonstrate the efficacy of this paradigm in identifying articulation, mass, and other physical parameters in several challenging robotic manipulation tasks, and illustrate that only a small amount of real-world data can allow for effective sim-to-real transfer. Project website at https://weirdlabuw.github.io/asid","sentences":["Model-free control strategies such as reinforcement learning have shown the ability to learn control strategies without requiring an accurate model or simulator of the world.","While this is appealing due to the lack of modeling requirements, such methods can be sample inefficient, making them impractical in many real-world domains.","On the other hand, model-based control techniques leveraging accurate simulators can circumvent these challenges and use a large amount of cheap simulation data to learn controllers that can effectively transfer to the real world.","The challenge with such model-based techniques is the requirement for an extremely accurate simulation, requiring both the specification of appropriate simulation assets and physical parameters.","This requires considerable human effort to design for every environment being considered.","In this work, we propose a learning system that can leverage a small amount of real-world data to autonomously refine a simulation model and then plan an accurate control strategy that can be deployed in the real world.","Our approach critically relies on utilizing an initial (possibly inaccurate) simulator to design effective exploration policies that, when deployed in the real world, collect high-quality data.","We demonstrate the efficacy of this paradigm in identifying articulation, mass, and other physical parameters in several challenging robotic manipulation tasks, and illustrate that only a small amount of real-world data can allow for effective sim-to-real transfer.","Project website at https://weirdlabuw.github.io/asid"],"url":"http://arxiv.org/abs/2404.12308v1","category":"cs.RO"}
{"created":"2024-04-18 15:54:48","title":"VELOcities of CEpheids (VELOCE) I. High-precision radial velocities of Cepheids","abstract":"This first VELOCE data release comprises 18,225 high-precision RV measurements of 258 bona fide classical Cepheids on both hemispheres collected mainly between 2010 and 2022, alongside 1161 additional observations of 164 other stars. The median per-observation RV uncertainty is 0.037 km/s, and some reach 0.002 km/s. Non-variable standard stars characterize RV zero-point stability and provide a base for future cross-calibrations. We determined zero-point differences between VELOCE and 31 literature data sets using template fitting and measured linear period changes of 146 Cepheids. Seventy six spectroscopic binary Cepheids and 14 candidates are identified using VELOCE data alone and are investigated in detail in a companion paper (VELOCE II). Several new insights into Cepheid pulsations were obtained, including: a) the most detailed description of the Hertzsprung progression by RVs; b) the identification of double-peaked bumps in the RV curve; c) clear evidence that virtually all Cepheids feature spectroscopic variability signals that lead to modulated RV variability. We identified 36 such stars, of which 4 also exhibit orbital motion. Linear radius variations depend strongly on pulsation period and a steep increase in slope of the $\\Delta$R/p versus logP-relation is found near 10d, challenging the existence of a tight relation between Baade-Wesselink projection factors and pulsation periods. We investigated the accuracy of RV time series measurements, v$_\\gamma$, and RV amplitudes published in Gaia's DR3 and determined an average offset of 0.65 \\pm 0.11 km/s relative to VELOCE. We recommend adopting a single set of template correlation parameters for distinct classes of large-amplitude variable stars to avoid systematic offsets in v$_\\gamma$ among stars belonging to the same class. Peak-to-peak amplitudes of Gaia RVs exhibit significant (16%) dispersion compared to VELOCE. [abridged]","sentences":["This first VELOCE data release comprises 18,225 high-precision RV measurements of 258 bona fide classical Cepheids on both hemispheres collected mainly between 2010 and 2022, alongside 1161 additional observations of 164 other stars.","The median per-observation RV uncertainty is 0.037 km/s, and some reach 0.002 km/s. Non-variable standard stars characterize RV zero-point stability and provide a base for future cross-calibrations.","We determined zero-point differences between VELOCE and 31 literature data sets using template fitting and measured linear period changes of 146 Cepheids.","Seventy six spectroscopic binary Cepheids and 14 candidates are identified using VELOCE data alone and are investigated in detail in a companion paper (VELOCE II).","Several new insights into Cepheid pulsations were obtained, including: a) the most detailed description of the Hertzsprung progression by RVs; b) the identification of double-peaked bumps in the RV curve; c) clear evidence that virtually all Cepheids feature spectroscopic variability signals that lead to modulated RV variability.","We identified 36 such stars, of which 4 also exhibit orbital motion.","Linear radius variations depend strongly on pulsation period and a steep increase in slope of the $\\Delta$R/p versus logP-relation is found near 10d, challenging the existence of a tight relation between Baade-Wesselink projection factors and pulsation periods.","We investigated the accuracy of RV time series measurements, v$_\\gamma$, and RV amplitudes published in Gaia's DR3 and determined an average offset of 0.65 \\pm 0.11 km/s relative to VELOCE.","We recommend adopting a single set of template correlation parameters for distinct classes of large-amplitude variable stars to avoid systematic offsets in v$_\\gamma$ among stars belonging to the same class.","Peak-to-peak amplitudes of Gaia RVs exhibit significant (16%) dispersion compared to VELOCE.","[abridged]"],"url":"http://arxiv.org/abs/2404.12280v1","category":"astro-ph.SR"}
{"created":"2024-04-18 14:27:41","title":"Traveling strings of active dipolar colloids","abstract":"We study an intriguing new type of self-assembled active colloidal polymer system in 3D. It is obtained from a suspension of Janus particles in an electric field that induces parallel dipoles in the particles as well as self-propulsion in the plane perpendicular to the field. At low packing fractions, in experiment, the particles self-assemble into 3D columns that are self-propelled in 2D. Explicit numerical simulations combining dipolar interactions and active self-propulsion find an activity dependent transition to a string phase by increasing dipole strength. We classify the collective dynamics of strings as a function of rotational and translational diffusion. Using an anisotropic version of the Rouse model of polymers with active driving, we analytically compute the strings' collective dynamics and centre of mass motion, which matches simulations and is consistent with experimental data. We also discover long range correlations of the fluctuations along the string contour that grow with the active persistence time, a purely active effect that disappears in the thermal limit.","sentences":["We study an intriguing new type of self-assembled active colloidal polymer system in 3D.","It is obtained from a suspension of Janus particles in an electric field that induces parallel dipoles in the particles as well as self-propulsion in the plane perpendicular to the field.","At low packing fractions, in experiment, the particles self-assemble into 3D columns that are self-propelled in 2D. Explicit numerical simulations combining dipolar interactions and active self-propulsion find an activity dependent transition to a string phase by increasing dipole strength.","We classify the collective dynamics of strings as a function of rotational and translational diffusion.","Using an anisotropic version of the Rouse model of polymers with active driving, we analytically compute the strings' collective dynamics and centre of mass motion, which matches simulations and is consistent with experimental data.","We also discover long range correlations of the fluctuations along the string contour that grow with the active persistence time, a purely active effect that disappears in the thermal limit."],"url":"http://arxiv.org/abs/2404.12218v1","category":"cond-mat.soft"}
{"created":"2024-04-18 13:27:42","title":"Coupled states of cold 174-Yb atoms in a high-finesse cavity","abstract":"We experimentally and theoretically study the formation of dressed states emerging from strong collective coupling of the narrow intercombination line of Yb atoms to a single mode of a high-finesse optical cavity. By permanently trapping and cooling the Yb atoms during their interaction with the cavity, we gain continuous experimental access to the dressed states. This allows us to detect both their field and their atomic properties, by simultaneously measuring the steady-state cavity transmission and free-space fluorescence. By varying the cavity and probe frequencies, we observe coupled atom-cavity states with atom number-dependent splitting, the hallmark of collective strong coupling of the atoms with the single cavity mode. We find additional fluorescence output at atomic resonance, which we explain by the effects of dephasing and inhomogeneous broadening. We compare our experimental results with a theoretical model and find good qualitative agreement.","sentences":["We experimentally and theoretically study the formation of dressed states emerging from strong collective coupling of the narrow intercombination line of Yb atoms to a single mode of a high-finesse optical cavity.","By permanently trapping and cooling the Yb atoms during their interaction with the cavity, we gain continuous experimental access to the dressed states.","This allows us to detect both their field and their atomic properties, by simultaneously measuring the steady-state cavity transmission and free-space fluorescence.","By varying the cavity and probe frequencies, we observe coupled atom-cavity states with atom number-dependent splitting, the hallmark of collective strong coupling of the atoms with the single cavity mode.","We find additional fluorescence output at atomic resonance, which we explain by the effects of dephasing and inhomogeneous broadening.","We compare our experimental results with a theoretical model and find good qualitative agreement."],"url":"http://arxiv.org/abs/2404.12173v1","category":"quant-ph"}
{"created":"2024-04-18 12:41:33","title":"Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models","abstract":"Vision-Language Pre-training (VLP) models like CLIP have achieved remarkable success in computer vision and particularly demonstrated superior robustness to distribution shifts of 2D images. However, their robustness under 3D viewpoint variations is still limited, which can hinder the development for real-world applications. This paper successfully addresses this concern while keeping VLPs' original performance by breaking through two primary obstacles: 1) the scarcity of training data and 2) the suboptimal fine-tuning paradigms. To combat data scarcity, we build the Multi-View Caption (MVCap) dataset -- a comprehensive collection of over four million multi-view image-text pairs across more than 100K objects, providing more potential for VLP models to develop generalizable viewpoint-invariant representations. To address the limitations of existing paradigms in performance trade-offs and training efficiency, we design a novel fine-tuning framework named Omniview-Tuning (OVT). Specifically, OVT introduces a Cross-Viewpoint Alignment objective through a minimax-like optimization strategy, which effectively aligns representations of identical objects from diverse viewpoints without causing overfitting. Additionally, OVT fine-tunes VLP models in a parameter-efficient manner, leading to minimal computational cost. Extensive experiments on various VLP models with different architectures validate that OVT significantly improves the models' resilience to viewpoint shifts and keeps the original performance, establishing a pioneering standard for boosting the viewpoint invariance of VLP models.","sentences":["Vision-Language Pre-training (VLP) models like CLIP have achieved remarkable success in computer vision and particularly demonstrated superior robustness to distribution shifts of 2D images.","However, their robustness under 3D viewpoint variations is still limited, which can hinder the development for real-world applications.","This paper successfully addresses this concern while keeping VLPs' original performance by breaking through two primary obstacles: 1) the scarcity of training data and 2) the suboptimal fine-tuning paradigms.","To combat data scarcity, we build the Multi-View Caption (MVCap) dataset -- a comprehensive collection of over four million multi-view image-text pairs across more than 100K objects, providing more potential for VLP models to develop generalizable viewpoint-invariant representations.","To address the limitations of existing paradigms in performance trade-offs and training efficiency, we design a novel fine-tuning framework named Omniview-Tuning (OVT).","Specifically, OVT introduces a Cross-Viewpoint Alignment objective through a minimax-like optimization strategy, which effectively aligns representations of identical objects from diverse viewpoints without causing overfitting.","Additionally, OVT fine-tunes VLP models in a parameter-efficient manner, leading to minimal computational cost.","Extensive experiments on various VLP models with different architectures validate that OVT significantly improves the models' resilience to viewpoint shifts and keeps the original performance, establishing a pioneering standard for boosting the viewpoint invariance of VLP models."],"url":"http://arxiv.org/abs/2404.12139v1","category":"cs.CV"}
{"created":"2024-04-18 12:33:57","title":"Enhancing Suicide Risk Assessment: A Speech-Based Automated Approach in Emergency Medicine","abstract":"The delayed access to specialized psychiatric assessments and care for patients at risk of suicidal tendencies in emergency departments creates a notable gap in timely intervention, hindering the provision of adequate mental health support during critical situations. To address this, we present a non-invasive, speech-based approach for automatic suicide risk assessment. For our study, we have collected a novel dataset of speech recordings from $20$ patients from which we extract three sets of features, including wav2vec, interpretable speech and acoustic features, and deep learning-based spectral representations. We proceed by conducting a binary classification to assess suicide risk in a leave-one-subject-out fashion. Our most effective speech model achieves a balanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech model with a series of patients' metadata, such as the history of suicide attempts or access to firearms, improves the overall result. The metadata integration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute improvement of $28.2\\,\\%$, demonstrating the efficacy of our proposed approaches for automatic suicide risk assessment in emergency medicine.","sentences":["The delayed access to specialized psychiatric assessments and care for patients at risk of suicidal tendencies in emergency departments creates a notable gap in timely intervention, hindering the provision of adequate mental health support during critical situations.","To address this, we present a non-invasive, speech-based approach for automatic suicide risk assessment.","For our study, we have collected a novel dataset of speech recordings from $20$ patients from which we extract three sets of features, including wav2vec, interpretable speech and acoustic features, and deep learning-based spectral representations.","We proceed by conducting a binary classification to assess suicide risk in a leave-one-subject-out fashion.","Our most effective speech model achieves a balanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech model with a series of patients' metadata, such as the history of suicide attempts or access to firearms, improves the overall result.","The metadata integration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute improvement of $28.2\\,\\%$, demonstrating the efficacy of our proposed approaches for automatic suicide risk assessment in emergency medicine."],"url":"http://arxiv.org/abs/2404.12132v1","category":"cs.SD"}
{"created":"2024-04-18 11:48:24","title":"Impact of non-reciprocal interactions on colloidal self-assembly with tunable anisotropy","abstract":"Non-reciprocal (NR) effective interactions violating Newton's third law occur in many biological systems, but can also be engineered in synthetic, colloidal systems. Recent research has shown that such NR interactions can have tremendous effects on the overall collective behaviour and pattern formation, but can also influence aggregation processes on the particle scale. Here we focus on the impact of non-reciprocity on the self-assembly of an (originally passive) colloidal system with anisotropic interactions whose character is tunable by external fields. In the absence of non-reciprocity, that is, under equilibrium conditions, the colloids form aggregates with extremely long life times [Kogler et al., Soft Matter 11, 7356 (2015)], indicating kinetic trapping. Here we study, based on Brownian Dynamics (BD) simulations in 2D, a NR version of this model consisting of two species with reciprocal isotropic, but NR anisotropic interactions. We find that NR induces an effective propulsion of particle pairs and small aggregates forming at initial stages of self-assembly, an indication of the NR-induced non-equilibrium. The shape and stability of these initial clusters strongly depends on the degree of anisotropy. At longer times we find, for weak NR interactions, large (even system-spanning) clusters where single particles can escape and enter at the boundaries, in stark contrast to the small rigid aggregates appearing at the same time in the passive case. In this sense, weak NR shortcuts the aggregation. Increasing the degree of NR (and thus, propulsion), we even observe large-scale phase separation if the interactions are weakly anisotropic. In contrast, system with strong NR and anisotropy remain essentially disordered. Overall, NR interactions are shown to destabilize the rigid aggregates interrupting self-assembly in the passive case, helping the system to overcome kinetic barriers.","sentences":["Non-reciprocal (NR) effective interactions violating Newton's third law occur in many biological systems, but can also be engineered in synthetic, colloidal systems.","Recent research has shown that such NR interactions can have tremendous effects on the overall collective behaviour and pattern formation, but can also influence aggregation processes on the particle scale.","Here we focus on the impact of non-reciprocity on the self-assembly of an (originally passive) colloidal system with anisotropic interactions whose character is tunable by external fields.","In the absence of non-reciprocity, that is, under equilibrium conditions, the colloids form aggregates with extremely long life times","[Kogler et al., Soft Matter 11, 7356 (2015)], indicating kinetic trapping.","Here we study, based on Brownian Dynamics (BD) simulations in 2D, a NR version of this model consisting of two species with reciprocal isotropic, but NR anisotropic interactions.","We find that NR induces an effective propulsion of particle pairs and small aggregates forming at initial stages of self-assembly, an indication of the NR-induced non-equilibrium.","The shape and stability of these initial clusters strongly depends on the degree of anisotropy.","At longer times we find, for weak NR interactions, large (even system-spanning) clusters where single particles can escape and enter at the boundaries, in stark contrast to the small rigid aggregates appearing at the same time in the passive case.","In this sense, weak NR shortcuts the aggregation.","Increasing the degree of NR (and thus, propulsion), we even observe large-scale phase separation if the interactions are weakly anisotropic.","In contrast, system with strong NR and anisotropy remain essentially disordered.","Overall, NR interactions are shown to destabilize the rigid aggregates interrupting self-assembly in the passive case, helping the system to overcome kinetic barriers."],"url":"http://arxiv.org/abs/2404.12108v1","category":"cond-mat.soft"}
{"created":"2024-04-18 09:53:49","title":"Using Real-world Bug Bounty Programs in Secure Coding Course: Experience Report","abstract":"To keep up with the growing number of cyber-attacks and associated threats, there is an ever-increasing demand for cybersecurity professionals and new methods and technologies. Training new cybersecurity professionals is a challenging task due to the broad scope of the area. One particular field where there is a shortage of experts is Ethical Hacking. Due to its complexity, it often faces educational constraints. Recognizing these challenges, we propose a solution: integrating a real-world bug bounty programme into cybersecurity curriculum. This innovative approach aims to fill the gap in practical cybersecurity education and also brings additional positive benefits. To evaluate our idea, we include the proposed solution to a secure coding course for IT-oriented faculty. We let students choose to participate in a bug bounty programme as an option for the semester assignment in a secure coding course. We then collected responses from the students to evaluate the outcomes (improved skills, reported vulnerabilities, a better relationship with security, etc.). Evaluation of the assignment showed that students enjoyed solving such real-world problems, could find real vulnerabilities, and that it helped raise their skills and cybersecurity awareness. Participation in real bug bounty programmes also positively affects the security level of the tested products. We also discuss the potential risks of this approach and how to mitigate them.","sentences":["To keep up with the growing number of cyber-attacks and associated threats, there is an ever-increasing demand for cybersecurity professionals and new methods and technologies.","Training new cybersecurity professionals is a challenging task due to the broad scope of the area.","One particular field where there is a shortage of experts is Ethical Hacking.","Due to its complexity, it often faces educational constraints.","Recognizing these challenges, we propose a solution: integrating a real-world bug bounty programme into cybersecurity curriculum.","This innovative approach aims to fill the gap in practical cybersecurity education and also brings additional positive benefits.","To evaluate our idea, we include the proposed solution to a secure coding course for IT-oriented faculty.","We let students choose to participate in a bug bounty programme as an option for the semester assignment in a secure coding course.","We then collected responses from the students to evaluate the outcomes (improved skills, reported vulnerabilities, a better relationship with security, etc.).","Evaluation of the assignment showed that students enjoyed solving such real-world problems, could find real vulnerabilities, and that it helped raise their skills and cybersecurity awareness.","Participation in real bug bounty programmes also positively affects the security level of the tested products.","We also discuss the potential risks of this approach and how to mitigate them."],"url":"http://arxiv.org/abs/2404.12043v1","category":"cs.CR"}
{"created":"2024-04-18 09:03:39","title":"Packing measure of the linear Gauss system","abstract":"For every $k \\in \\mathbb{N}$ let $f_k:[\\frac{1}{k+1}, \\frac{1}{k}] \\to [0,1]$ be decreasing, linear functions such that $f_k(\\frac{1}{k+1}) = 1$ and $f_k(\\frac{1}{k}) = 0$, $k = 1, 2, \\dots$. We define iterated function system (IFS) $S_n$ by limiting the collection of functions $f_k$ to first n, meaning $S_n = \\{f_k \\}_{k=1}^n$. Let $J_n$ denote the limit set of $S_n$. Then $\\lim\\limits_{n\\to \\infty} \\mathcal{P}_{h_n}(J_n) = 2$, where $h_n$ is the packing dimension of $J_n$ and $\\mathcal{P}_{h_n}$ is the corresponding packing measure.","sentences":["For every $k \\in \\mathbb{N}$ let $f_k:[\\frac{1}{k+1}, \\frac{1}{k}] \\to","[0,1]$ be decreasing, linear functions such that $f_k(\\frac{1}{k+1})","= 1$ and $f_k(\\frac{1}{k})","= 0$, $k = 1, 2, \\dots$.","We define iterated function system (IFS) $S_n$ by limiting the collection of functions $f_k$ to first n, meaning $S_n = \\{f_k \\}_{k=1}^n$. Let $J_n$ denote the limit set of $S_n$. Then $\\lim\\limits_{n\\to \\infty} \\mathcal{P}_{h_n}(J_n) = 2$, where $h_n$ is the packing dimension of $J_n$ and $\\mathcal{P}_{h_n}$ is the corresponding packing measure."],"url":"http://arxiv.org/abs/2404.12012v1","category":"math.DS"}
{"created":"2024-04-18 08:44:52","title":"DST-GTN: Dynamic Spatio-Temporal Graph Transformer Network for Traffic Forecasting","abstract":"Accurate traffic forecasting is essential for effective urban planning and congestion management. Deep learning (DL) approaches have gained colossal success in traffic forecasting but still face challenges in capturing the intricacies of traffic dynamics. In this paper, we identify and address this challenges by emphasizing that spatial features are inherently dynamic and change over time. A novel in-depth feature representation, called Dynamic Spatio-Temporal (Dyn-ST) features, is introduced, which encapsulates spatial characteristics across varying times. Moreover, a Dynamic Spatio-Temporal Graph Transformer Network (DST-GTN) is proposed by capturing Dyn-ST features and other dynamic adjacency relations between intersections. The DST-GTN can model dynamic ST relationships between nodes accurately and refine the representation of global and local ST characteristics by adopting adaptive weights in low-pass and all-pass filters, enabling the extraction of Dyn-ST features from traffic time-series data. Through numerical experiments on public datasets, the DST-GTN achieves state-of-the-art performance for a range of traffic forecasting tasks and demonstrates enhanced stability.","sentences":["Accurate traffic forecasting is essential for effective urban planning and congestion management.","Deep learning (DL) approaches have gained colossal success in traffic forecasting but still face challenges in capturing the intricacies of traffic dynamics.","In this paper, we identify and address this challenges by emphasizing that spatial features are inherently dynamic and change over time.","A novel in-depth feature representation, called Dynamic Spatio-Temporal (Dyn-ST) features, is introduced, which encapsulates spatial characteristics across varying times.","Moreover, a Dynamic Spatio-Temporal Graph Transformer Network (DST-GTN) is proposed by capturing Dyn-ST features and other dynamic adjacency relations between intersections.","The DST-GTN can model dynamic ST relationships between nodes accurately and refine the representation of global and local ST characteristics by adopting adaptive weights in low-pass and all-pass filters, enabling the extraction of Dyn-ST features from traffic time-series data.","Through numerical experiments on public datasets, the DST-GTN achieves state-of-the-art performance for a range of traffic forecasting tasks and demonstrates enhanced stability."],"url":"http://arxiv.org/abs/2404.11996v1","category":"cs.AI"}
{"created":"2024-04-18 08:33:49","title":"The Fireball of November 24, 1970, as the Most Probable Source of the Ischgl Meteorite","abstract":"In June 1976, a pristine meteorite stone weighing approximately 1 kg, fully covered with a fresh black fusion crust, was collected on a mountain road in the high-altitude Alpine environment. The recovery took place while clearing the remnants of a snow avalanche, 2 km northwest of Ischgl in Austria. Subsequent to its retrieval, the specimen remained in the finder's private residence without undergoing any scientific examination or identification until 2008, when it was brought to the University of Innsbruck. The sample was classified as a well-preserved LL6 chondrite, with a W0 weathering grade, implying a relatively short time between the meteorite fall and its retrieval. To investigate the potential connection between the Ischgl meteorite and a recorded fireball event, we have reviewed all documented fireballs ever photographed by German fireball camera stations. This examination led us to identify the fireball EN241170 observed in Germany by ten different European Network stations on the night of November 23/24, 1970, as the most likely candidate. We employed state-of-the-art techniques to reconstruct the fireball's trajectory, and to reproduce both its luminous and dark flight phases in detail. We find that the determined strewn field and the generated heat map closely align with the recovery location of the Ischgl meteorite. Furthermore, the measured radionuclide data reported here indicate that the pre-atmospheric size of the Ischgl meteoroid is consistent with the mass estimate inferred from our deceleration analysis along the trajectory. Our findings strongly support the conclusion that the Ischgl meteorite originated from the EN241170 fireball, effectively establishing it as a confirmed meteorite fall. This discovery enables to determine, along with the physical properties, also the heliocentric orbit and cosmic history of the Ischgl meteorite.","sentences":["In June 1976, a pristine meteorite stone weighing approximately 1 kg, fully covered with a fresh black fusion crust, was collected on a mountain road in the high-altitude Alpine environment.","The recovery took place while clearing the remnants of a snow avalanche, 2 km northwest of Ischgl in Austria.","Subsequent to its retrieval, the specimen remained in the finder's private residence without undergoing any scientific examination or identification until 2008, when it was brought to the University of Innsbruck.","The sample was classified as a well-preserved LL6 chondrite, with a W0 weathering grade, implying a relatively short time between the meteorite fall and its retrieval.","To investigate the potential connection between the Ischgl meteorite and a recorded fireball event, we have reviewed all documented fireballs ever photographed by German fireball camera stations.","This examination led us to identify the fireball EN241170 observed in Germany by ten different European Network stations on the night of November 23/24, 1970, as the most likely candidate.","We employed state-of-the-art techniques to reconstruct the fireball's trajectory, and to reproduce both its luminous and dark flight phases in detail.","We find that the determined strewn field and the generated heat map closely align with the recovery location of the Ischgl meteorite.","Furthermore, the measured radionuclide data reported here indicate that the pre-atmospheric size of the Ischgl meteoroid is consistent with the mass estimate inferred from our deceleration analysis along the trajectory.","Our findings strongly support the conclusion that the Ischgl meteorite originated from the EN241170 fireball, effectively establishing it as a confirmed meteorite fall.","This discovery enables to determine, along with the physical properties, also the heliocentric orbit and cosmic history of the Ischgl meteorite."],"url":"http://arxiv.org/abs/2404.11989v1","category":"astro-ph.EP"}
{"created":"2024-04-18 08:33:35","title":"The Emerging AI Divide in the United States","abstract":"The digital divide describes disparities in access to and usage of digital tooling between social and economic groups. Emerging generative artificial intelligence tools, which strongly affect productivity, could magnify the impact of these divides. However, the affordability, multi-modality, and multilingual capabilities of these tools could also make them more accessible to diverse users in comparison with previous forms of digital tooling. In this study, we characterize spatial differences in U.S. residents' knowledge of a new generative AI tool, ChatGPT, through an analysis of state- and county-level search query data. In the first six months after the tool's release, we observe the highest rates of users searching for ChatGPT in West Coast states and persistently low rates of search in Appalachian and Gulf states. Counties with the highest rates of search are relatively more urbanized and have proportionally more educated, more economically advantaged, and more Asian residents in comparison with other counties or with the U.S. average. In multilevel models adjusting for socioeconomic and demographic factors as well as industry makeup, education is the strongest positive predictor of rates of search for generative AI tooling. Although generative AI technologies may be novel, early differences in uptake appear to be following familiar paths of digital marginalization.","sentences":["The digital divide describes disparities in access to and usage of digital tooling between social and economic groups.","Emerging generative artificial intelligence tools, which strongly affect productivity, could magnify the impact of these divides.","However, the affordability, multi-modality, and multilingual capabilities of these tools could also make them more accessible to diverse users in comparison with previous forms of digital tooling.","In this study, we characterize spatial differences in U.S. residents' knowledge of a new generative AI tool, ChatGPT, through an analysis of state- and county-level search query data.","In the first six months after the tool's release, we observe the highest rates of users searching for ChatGPT in West Coast states and persistently low rates of search in Appalachian and Gulf states.","Counties with the highest rates of search are relatively more urbanized and have proportionally more educated, more economically advantaged, and more Asian residents in comparison with other counties or with the U.S. average.","In multilevel models adjusting for socioeconomic and demographic factors as well as industry makeup, education is the strongest positive predictor of rates of search for generative AI tooling.","Although generative AI technologies may be novel, early differences in uptake appear to be following familiar paths of digital marginalization."],"url":"http://arxiv.org/abs/2404.11988v1","category":"cs.AI"}
{"created":"2024-04-18 08:20:30","title":"Unsupervised learning approach to quantum wavepacket dynamics from coupled temporal-spatial correlations","abstract":"Understanding complex quantum dynamics in realistic materials requires insight into the underlying correlations dominating the interactions between the participating particles. Due to the wealth of information involved in these processes, applying artificial intelligence methods is compelling. Yet, unsupervised data-driven approaches typically focus on maximal variations of the individual components, rather than considering the correlations between them. Here we present an approach that recognizes correlation patterns to explore convoluted dynamical processes. Our scheme is using singular value decomposition (SVD) to extract dynamical features, unveiling the internal temporal-spatial interrelations that generate the dynamical mechanisms. We apply our approach to study light-induced wavepacket propagation in organic crystals, of interest for applications in material based quantum computing and quantum information science. We show how transformation from the input momentum and time coordinates onto a new correlation-induced coordinate space allows direct recognition of the relaxation and dephasing components dominating the dynamics and demonstrate their dependence on the initial pulse shape. Entanglement of the dynamical features is suggested as a pathway to reproduce the information required for further explainability of these mechanisms. Our method offers a route for elucidating complex dynamical processes using unsupervised AI-based analysis in multi-component systems.","sentences":["Understanding complex quantum dynamics in realistic materials requires insight into the underlying correlations dominating the interactions between the participating particles.","Due to the wealth of information involved in these processes, applying artificial intelligence methods is compelling.","Yet, unsupervised data-driven approaches typically focus on maximal variations of the individual components, rather than considering the correlations between them.","Here we present an approach that recognizes correlation patterns to explore convoluted dynamical processes.","Our scheme is using singular value decomposition (SVD) to extract dynamical features, unveiling the internal temporal-spatial interrelations that generate the dynamical mechanisms.","We apply our approach to study light-induced wavepacket propagation in organic crystals, of interest for applications in material based quantum computing and quantum information science.","We show how transformation from the input momentum and time coordinates onto a new correlation-induced coordinate space allows direct recognition of the relaxation and dephasing components dominating the dynamics and demonstrate their dependence on the initial pulse shape.","Entanglement of the dynamical features is suggested as a pathway to reproduce the information required for further explainability of these mechanisms.","Our method offers a route for elucidating complex dynamical processes using unsupervised AI-based analysis in multi-component systems."],"url":"http://arxiv.org/abs/2404.11980v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 08:14:40","title":"Corpus Christi: Establishing Replicability when Sharing the Bread is Not Allowed","abstract":"In this paper, we provide practical tools to improve the scientific soundness of firmware corpora beyond the state of the art. We identify binary analysis challenges that significantly impact corpus creation. We use them to derive a framework of key corpus requirements that nurture the scientific goals of replicability and representativeness. We apply the framework to 44 top tier papers and collect 704 data points to show that there is currently no common ground on corpus creation. We discover in otherwise excellent work, that incomplete documentation and inflated corpus sizes blur visions on representativeness and hinder replicability. Our results show that the strict framework provides useful and practical guidelines that can identify miniscule step stones in corpus creation with significant impact on soundness.   Finally, we show that it is possible to meet all requirements: We provide a new corpus called LFwC. It is designed for large-scale static analyses on Linux-based firmware and consists of 10,913 high-quality images, covering 2,365 network appliances. We share rich meta data and scripts for replicability with the community. We verify unpacking, perform deduplication, identify contents, and provide bug ground truth. We identify ISAs and Linux kernels. All samples can be unpacked with the open source tool FACT.","sentences":["In this paper, we provide practical tools to improve the scientific soundness of firmware corpora beyond the state of the art.","We identify binary analysis challenges that significantly impact corpus creation.","We use them to derive a framework of key corpus requirements that nurture the scientific goals of replicability and representativeness.","We apply the framework to 44 top tier papers and collect 704 data points to show that there is currently no common ground on corpus creation.","We discover in otherwise excellent work, that incomplete documentation and inflated corpus sizes blur visions on representativeness and hinder replicability.","Our results show that the strict framework provides useful and practical guidelines that can identify miniscule step stones in corpus creation with significant impact on soundness.   ","Finally, we show that it is possible to meet all requirements: We provide a new corpus called LFwC.","It is designed for large-scale static analyses on Linux-based firmware and consists of 10,913 high-quality images, covering 2,365 network appliances.","We share rich meta data and scripts for replicability with the community.","We verify unpacking, perform deduplication, identify contents, and provide bug ground truth.","We identify ISAs and Linux kernels.","All samples can be unpacked with the open source tool FACT."],"url":"http://arxiv.org/abs/2404.11977v1","category":"cs.CR"}
{"created":"2024-04-18 08:05:23","title":"Device (In)Dependence of Deep Learning-based Image Age Approximation","abstract":"The goal of temporal image forensic is to approximate the age of a digital image relative to images from the same device. Usually, this is based on traces left during the image acquisition pipeline. For example, several methods exist that exploit the presence of in-field sensor defects for this purpose. In addition to these 'classical' methods, there is also an approach in which a Convolutional Neural Network (CNN) is trained to approximate the image age. One advantage of a CNN is that it independently learns the age features used. This would make it possible to exploit other (different) age traces in addition to the known ones (i.e., in-field sensor defects). In a previous work, we have shown that the presence of strong in-field sensor defects is irrelevant for a CNN to predict the age class. Based on this observation, the question arises how device (in)dependent the learned features are. In this work, we empirically asses this by training a network on images from a single device and then apply the trained model to images from different devices. This evaluation is performed on 14 different devices, including 10 devices from the publicly available 'Northumbria Temporal Image Forensics' database. These 10 different devices are based on five different device pairs (i.e., with the identical camera model).","sentences":["The goal of temporal image forensic is to approximate the age of a digital image relative to images from the same device.","Usually, this is based on traces left during the image acquisition pipeline.","For example, several methods exist that exploit the presence of in-field sensor defects for this purpose.","In addition to these 'classical' methods, there is also an approach in which a Convolutional Neural Network (CNN) is trained to approximate the image age.","One advantage of a CNN is that it independently learns the age features used.","This would make it possible to exploit other (different) age traces in addition to the known ones (i.e., in-field sensor defects).","In a previous work, we have shown that the presence of strong in-field sensor defects is irrelevant for a CNN to predict the age class.","Based on this observation, the question arises how device (in)dependent the learned features are.","In this work, we empirically asses this by training a network on images from a single device and then apply the trained model to images from different devices.","This evaluation is performed on 14 different devices, including 10 devices from the publicly available 'Northumbria Temporal Image Forensics' database.","These 10 different devices are based on five different device pairs (i.e., with the identical camera model)."],"url":"http://arxiv.org/abs/2404.11974v1","category":"eess.IV"}
{"created":"2024-04-18 08:01:20","title":"Exploring the landscape of large language models: Foundations, techniques, and challenges","abstract":"In this review paper, we delve into the realm of Large Language Models (LLMs), covering their foundational principles, diverse applications, and nuanced training processes. The article sheds light on the mechanics of in-context learning and a spectrum of fine-tuning approaches, with a special focus on methods that optimize efficiency in parameter usage. Additionally, it explores how LLMs can be more closely aligned with human preferences through innovative reinforcement learning frameworks and other novel methods that incorporate human feedback. The article also examines the emerging technique of retrieval augmented generation, integrating external knowledge into LLMs. The ethical dimensions of LLM deployment are discussed, underscoring the need for mindful and responsible application. Concluding with a perspective on future research trajectories, this review offers a succinct yet comprehensive overview of the current state and emerging trends in the evolving landscape of LLMs, serving as an insightful guide for both researchers and practitioners in artificial intelligence.","sentences":["In this review paper, we delve into the realm of Large Language Models (LLMs), covering their foundational principles, diverse applications, and nuanced training processes.","The article sheds light on the mechanics of in-context learning and a spectrum of fine-tuning approaches, with a special focus on methods that optimize efficiency in parameter usage.","Additionally, it explores how LLMs can be more closely aligned with human preferences through innovative reinforcement learning frameworks and other novel methods that incorporate human feedback.","The article also examines the emerging technique of retrieval augmented generation, integrating external knowledge into LLMs.","The ethical dimensions of LLM deployment are discussed, underscoring the need for mindful and responsible application.","Concluding with a perspective on future research trajectories, this review offers a succinct yet comprehensive overview of the current state and emerging trends in the evolving landscape of LLMs, serving as an insightful guide for both researchers and practitioners in artificial intelligence."],"url":"http://arxiv.org/abs/2404.11973v1","category":"cs.AI"}
{"created":"2024-04-18 07:50:10","title":"From Language Models to Practical Self-Improving Computer Agents","abstract":"We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities. Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself. We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.","sentences":["We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks.","As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities.","Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself.","We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks.","Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities.","The agent effectively uses these various tools to solve problems including automated software development and web-based tasks."],"url":"http://arxiv.org/abs/2404.11964v1","category":"cs.AI"}
{"created":"2024-04-18 07:48:00","title":"\u00a9Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model","abstract":"This paper addresses the contentious issue of copyright infringement in images generated by text-to-image models, sparking debates among AI developers, content creators, and legal entities. State-of-the-art models create high-quality content without crediting original creators, causing concern in the artistic community. To mitigate this, we propose the \\copyright Plug-in Authorization framework, introducing three operations: addition, extraction, and combination. Addition involves training a \\copyright plug-in for specific copyright, facilitating proper credit attribution. Extraction allows creators to reclaim copyright from infringing models, and combination enables users to merge different \\copyright plug-ins. These operations act as permits, incentivizing fair use and providing flexibility in authorization. We present innovative approaches,\"Reverse LoRA\" for extraction and \"EasyMerge\" for seamless combination. Experiments in artist-style replication and cartoon IP recreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable solution for human copyright protection in the age of generative AIs.","sentences":["This paper addresses the contentious issue of copyright infringement in images generated by text-to-image models, sparking debates among AI developers, content creators, and legal entities.","State-of-the-art models create high-quality content without crediting original creators, causing concern in the artistic community.","To mitigate this, we propose the \\copyright Plug-in Authorization framework, introducing three operations: addition, extraction, and combination.","Addition involves training a \\copyright plug-in for specific copyright, facilitating proper credit attribution.","Extraction allows creators to reclaim copyright from infringing models, and combination enables users to merge different \\copyright plug-ins.","These operations act as permits, incentivizing fair use and providing flexibility in authorization.","We present innovative approaches,\"Reverse LoRA\" for extraction and \"EasyMerge\" for seamless combination.","Experiments in artist-style replication and cartoon IP recreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable solution for human copyright protection in the age of generative AIs."],"url":"http://arxiv.org/abs/2404.11962v1","category":"cs.AI"}
{"created":"2024-04-18 07:42:46","title":"Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers","abstract":"The most recent pointwise Large Language Model (LLM) rankers have achieved remarkable ranking results. However, these rankers are hindered by two major drawbacks: (1) they fail to follow a standardized comparison guidance during the ranking process, and (2) they struggle with comprehensive considerations when dealing with complicated passages. To address these shortcomings, we propose to build a ranker that generates ranking scores based on a set of criteria from various perspectives. These criteria are intended to direct each perspective in providing a distinct yet synergistic evaluation. Our research, which examines eight datasets from the BEIR benchmark demonstrates that incorporating this multi-perspective criteria ensemble approach markedly enhanced the performance of pointwise LLM rankers.","sentences":["The most recent pointwise Large Language Model (LLM) rankers have achieved remarkable ranking results.","However, these rankers are hindered by two major drawbacks: (1) they fail to follow a standardized comparison guidance during the ranking process, and (2) they struggle with comprehensive considerations when dealing with complicated passages.","To address these shortcomings, we propose to build a ranker that generates ranking scores based on a set of criteria from various perspectives.","These criteria are intended to direct each perspective in providing a distinct yet synergistic evaluation.","Our research, which examines eight datasets from the BEIR benchmark demonstrates that incorporating this multi-perspective criteria ensemble approach markedly enhanced the performance of pointwise LLM rankers."],"url":"http://arxiv.org/abs/2404.11960v1","category":"cs.IR"}
{"created":"2024-04-18 07:07:38","title":"Sketch-guided Image Inpainting with Partial Discrete Diffusion Process","abstract":"In this work, we study the task of sketch-guided image inpainting. Unlike the well-explored natural language-guided image inpainting, which excels in capturing semantic details, the relatively less-studied sketch-guided inpainting offers greater user control in specifying the object's shape and pose to be inpainted. As one of the early solutions to this task, we introduce a novel partial discrete diffusion process (PDDP). The forward pass of the PDDP corrupts the masked regions of the image and the backward pass reconstructs these masked regions conditioned on hand-drawn sketches using our proposed sketch-guided bi-directional transformer. The proposed novel transformer module accepts two inputs -- the image containing the masked region to be inpainted and the query sketch to model the reverse diffusion process. This strategy effectively addresses the domain gap between sketches and natural images, thereby, enhancing the quality of inpainting results. In the absence of a large-scale dataset specific to this task, we synthesize a dataset from the MS-COCO to train and extensively evaluate our proposed framework against various competent approaches in the literature. The qualitative and quantitative results and user studies establish that the proposed method inpaints realistic objects that fit the context in terms of the visual appearance of the provided sketch. To aid further research, we have made our code publicly available at https://github.com/vl2g/Sketch-Inpainting .","sentences":["In this work, we study the task of sketch-guided image inpainting.","Unlike the well-explored natural language-guided image inpainting, which excels in capturing semantic details, the relatively less-studied sketch-guided inpainting offers greater user control in specifying the object's shape and pose to be inpainted.","As one of the early solutions to this task, we introduce a novel partial discrete diffusion process (PDDP).","The forward pass of the PDDP corrupts the masked regions of the image and the backward pass reconstructs these masked regions conditioned on hand-drawn sketches using our proposed sketch-guided bi-directional transformer.","The proposed novel transformer module accepts two inputs -- the image containing the masked region to be inpainted and the query sketch to model the reverse diffusion process.","This strategy effectively addresses the domain gap between sketches and natural images, thereby, enhancing the quality of inpainting results.","In the absence of a large-scale dataset specific to this task, we synthesize a dataset from the MS-COCO to train and extensively evaluate our proposed framework against various competent approaches in the literature.","The qualitative and quantitative results and user studies establish that the proposed method inpaints realistic objects that fit the context in terms of the visual appearance of the provided sketch.","To aid further research, we have made our code publicly available at https://github.com/vl2g/Sketch-Inpainting ."],"url":"http://arxiv.org/abs/2404.11949v1","category":"cs.CV"}
{"created":"2024-04-18 06:58:02","title":"S4TP: Social-Suitable and Safety-Sensitive Trajectory Planning for Autonomous Vehicles","abstract":"In public roads, autonomous vehicles (AVs) face the challenge of frequent interactions with human-driven vehicles (HDVs), which render uncertain driving behavior due to varying social characteristics among humans. To effectively assess the risks prevailing in the vicinity of AVs in social interactive traffic scenarios and achieve safe autonomous driving, this article proposes a social-suitable and safety-sensitive trajectory planning (S4TP) framework. Specifically, S4TP integrates the Social-Aware Trajectory Prediction (SATP) and Social-Aware Driving Risk Field (SADRF) modules. SATP utilizes Transformers to effectively encode the driving scene and incorporates an AV's planned trajectory during the prediction decoding process. SADRF assesses the expected surrounding risk degrees during AVs-HDVs interactions, each with different social characteristics, visualized as two-dimensional heat maps centered on the AV. SADRF models the driving intentions of the surrounding HDVs and predicts trajectories based on the representation of vehicular interactions. S4TP employs an optimization-based approach for motion planning, utilizing the predicted HDVs'trajectories as input. With the integration of SADRF, S4TP executes real-time online optimization of the planned trajectory of AV within lowrisk regions, thus improving the safety and the interpretability of the planned trajectory. We have conducted comprehensive tests of the proposed method using the SMARTS simulator. Experimental results in complex social scenarios, such as unprotected left turn intersections, merging, cruising, and overtaking, validate the superiority of our proposed S4TP in terms of safety and rationality. S4TP achieves a pass rate of 100% across all scenarios, surpassing the current state-of-the-art methods Fanta of 98.25% and Predictive-Decision of 94.75%.","sentences":["In public roads, autonomous vehicles (AVs) face the challenge of frequent interactions with human-driven vehicles (HDVs), which render uncertain driving behavior due to varying social characteristics among humans.","To effectively assess the risks prevailing in the vicinity of AVs in social interactive traffic scenarios and achieve safe autonomous driving, this article proposes a social-suitable and safety-sensitive trajectory planning (S4TP) framework.","Specifically, S4TP integrates the Social-Aware Trajectory Prediction (SATP) and Social-Aware Driving Risk Field (SADRF) modules.","SATP utilizes Transformers to effectively encode the driving scene and incorporates an AV's planned trajectory during the prediction decoding process.","SADRF assesses the expected surrounding risk degrees during AVs-HDVs interactions, each with different social characteristics, visualized as two-dimensional heat maps centered on the AV.","SADRF models the driving intentions of the surrounding HDVs and predicts trajectories based on the representation of vehicular interactions.","S4TP employs an optimization-based approach for motion planning, utilizing the predicted HDVs'trajectories as input.","With the integration of SADRF, S4TP executes real-time online optimization of the planned trajectory of AV within lowrisk regions, thus improving the safety and the interpretability of the planned trajectory.","We have conducted comprehensive tests of the proposed method using the SMARTS simulator.","Experimental results in complex social scenarios, such as unprotected left turn intersections, merging, cruising, and overtaking, validate the superiority of our proposed S4TP in terms of safety and rationality.","S4TP achieves a pass rate of 100% across all scenarios, surpassing the current state-of-the-art methods Fanta of 98.25% and Predictive-Decision of 94.75%."],"url":"http://arxiv.org/abs/2404.11946v1","category":"cs.RO"}
{"created":"2024-04-18 06:36:43","title":"Comprehensive Review and New Analysis Software for Single-file Pedestrian Experiments","abstract":"This paper offers a comprehensive examination of single-file experiments within the field of pedestrian dynamics, providing a thorough review from both theoretical and analytical perspectives. It begins by tracing the historical context of single-file movement studies in pedestrian dynamics. Then, the significance of understanding the fundamental relationships among density, speed, and flow in pedestrian dynamics through the lens of simple single-file systems is explored in depth. Furthermore, we investigate various traffic systems involving human or non-human entities such as ants, mice, bicycles, and cars, and provide insights. We explore the types of experimental setups, data collection methods, and influential factors that affect pedestrian movement. We also define and explain the common concepts concerning single-file movement, particularly in experimental research. Finally, we present a Python tool named ``SingleFileMovementAnalysis'' designed for analyzing single-file experimental data, specifically head trajectories. This tool provides a cohesive approach to preparing and calculating movement metrics like speed, density, and headway. The article aims to stimulate further research and underscore the areas where future researchers can contribute to advancing and enhancing single-file studies.","sentences":["This paper offers a comprehensive examination of single-file experiments within the field of pedestrian dynamics, providing a thorough review from both theoretical and analytical perspectives.","It begins by tracing the historical context of single-file movement studies in pedestrian dynamics.","Then, the significance of understanding the fundamental relationships among density, speed, and flow in pedestrian dynamics through the lens of simple single-file systems is explored in depth.","Furthermore, we investigate various traffic systems involving human or non-human entities such as ants, mice, bicycles, and cars, and provide insights.","We explore the types of experimental setups, data collection methods, and influential factors that affect pedestrian movement.","We also define and explain the common concepts concerning single-file movement, particularly in experimental research.","Finally, we present a Python tool named ``SingleFileMovementAnalysis'' designed for analyzing single-file experimental data, specifically head trajectories.","This tool provides a cohesive approach to preparing and calculating movement metrics like speed, density, and headway.","The article aims to stimulate further research and underscore the areas where future researchers can contribute to advancing and enhancing single-file studies."],"url":"http://arxiv.org/abs/2404.11937v1","category":"physics.soc-ph"}
{"created":"2024-04-18 06:35:37","title":"LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights","abstract":"Latent Diffusion Models (LDMs) have emerged as powerful generative models, known for delivering remarkable results under constrained computational resources. However, deploying LDMs on resource-limited devices remains a complex issue, presenting challenges such as memory consumption and inference speed. To address this issue, we introduce LD-Pruner, a novel performance-preserving structured pruning method for compressing LDMs. Traditional pruning methods for deep neural networks are not tailored to the unique characteristics of LDMs, such as the high computational cost of training and the absence of a fast, straightforward and task-agnostic method for evaluating model performance. Our method tackles these challenges by leveraging the latent space during the pruning process, enabling us to effectively quantify the impact of pruning on model performance, independently of the task at hand. This targeted pruning of components with minimal impact on the output allows for faster convergence during training, as the model has less information to re-learn, thereby addressing the high computational cost of training. Consequently, our approach achieves a compressed model that offers improved inference speed and reduced parameter count, while maintaining minimal performance degradation. We demonstrate the effectiveness of our approach on three different tasks: text-to-image (T2I) generation, Unconditional Image Generation (UIG) and Unconditional Audio Generation (UAG). Notably, we reduce the inference time of Stable Diffusion (SD) by 34.9% while simultaneously improving its FID by 5.2% on MS-COCO T2I benchmark. This work paves the way for more efficient pruning methods for LDMs, enhancing their applicability.","sentences":["Latent Diffusion Models (LDMs) have emerged as powerful generative models, known for delivering remarkable results under constrained computational resources.","However, deploying LDMs on resource-limited devices remains a complex issue, presenting challenges such as memory consumption and inference speed.","To address this issue, we introduce LD-Pruner, a novel performance-preserving structured pruning method for compressing LDMs.","Traditional pruning methods for deep neural networks are not tailored to the unique characteristics of LDMs, such as the high computational cost of training and the absence of a fast, straightforward and task-agnostic method for evaluating model performance.","Our method tackles these challenges by leveraging the latent space during the pruning process, enabling us to effectively quantify the impact of pruning on model performance, independently of the task at hand.","This targeted pruning of components with minimal impact on the output allows for faster convergence during training, as the model has less information to re-learn, thereby addressing the high computational cost of training.","Consequently, our approach achieves a compressed model that offers improved inference speed and reduced parameter count, while maintaining minimal performance degradation.","We demonstrate the effectiveness of our approach on three different tasks: text-to-image (T2I) generation, Unconditional Image Generation (UIG) and Unconditional Audio Generation (UAG).","Notably, we reduce the inference time of Stable Diffusion (SD) by 34.9% while simultaneously improving its FID by 5.2% on MS-COCO T2I benchmark.","This work paves the way for more efficient pruning methods for LDMs, enhancing their applicability."],"url":"http://arxiv.org/abs/2404.11936v1","category":"cs.LG"}
{"created":"2024-04-18 06:20:50","title":"CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment","abstract":"Multilingual proficiency presents a significant challenge for large language models (LLMs). English-centric models are usually suboptimal in other languages, particularly those that are linguistically distant from English. This performance discrepancy mainly stems from the imbalanced distribution of training data across languages during pre-training and instruction tuning stages. To address this problem, we propose a novel approach called CrossIn, which utilizes a mixed composition of cross-lingual instruction tuning data. Our method leverages the compressed representation shared by various languages to efficiently enhance the model's task-solving capabilities and multilingual proficiency within a single process. In addition, we introduce a multi-task and multi-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental results demonstrate that our method substantially improves performance across tasks and languages, and we provide extensive insights into the impact of cross-lingual data volume and the integration of translation data on enhancing multilingual consistency and accuracy.","sentences":["Multilingual proficiency presents a significant challenge for large language models (LLMs).","English-centric models are usually suboptimal in other languages, particularly those that are linguistically distant from English.","This performance discrepancy mainly stems from the imbalanced distribution of training data across languages during pre-training and instruction tuning stages.","To address this problem, we propose a novel approach called CrossIn, which utilizes a mixed composition of cross-lingual instruction tuning data.","Our method leverages the compressed representation shared by various languages to efficiently enhance the model's task-solving capabilities and multilingual proficiency within a single process.","In addition, we introduce a multi-task and multi-faceted benchmark to evaluate the effectiveness of CrossIn.","Experimental results demonstrate that our method substantially improves performance across tasks and languages, and we provide extensive insights into the impact of cross-lingual data volume and the integration of translation data on enhancing multilingual consistency and accuracy."],"url":"http://arxiv.org/abs/2404.11932v1","category":"cs.CL"}
{"created":"2024-04-18 06:18:48","title":"A Symmetric Regressor for MRI-Based Assessment of Striatal Dopamine Transporter Uptake in Parkinson's Disease","abstract":"Dopamine transporter (DAT) imaging is commonly used for monitoring Parkinson's disease (PD), where striatal DAT uptake amount is computed to assess PD severity. However, DAT imaging has a high cost and the risk of radiance exposure and is not available in general clinics. Recently, MRI patch of the nigral region has been proposed as a safer and easier alternative. This paper proposes a symmetric regressor for predicting the DAT uptake amount from the nigral MRI patch. Acknowledging the symmetry between the right and left nigrae, the proposed regressor incorporates a paired input-output model that simultaneously predicts the DAT uptake amounts for both the right and left striata. Moreover, it employs a symmetric loss that imposes a constraint on the difference between right-to-left predictions, resembling the high correlation in DAT uptake amounts in the two lateral sides. Additionally, we propose a symmetric Monte-Carlo (MC) dropout method for providing a fruitful uncertainty estimate of the DAT uptake prediction, which utilizes the above symmetry. We evaluated the proposed approach on 734 nigral patches, which demonstrated significantly improved performance of the symmetric regressor compared with the standard regressors while giving better explainability and feature representation. The symmetric MC dropout also gave precise uncertainty ranges with a high probability of including the true DAT uptake amounts within the range.","sentences":["Dopamine transporter (DAT) imaging is commonly used for monitoring Parkinson's disease (PD), where striatal DAT uptake amount is computed to assess PD severity.","However, DAT imaging has a high cost and the risk of radiance exposure and is not available in general clinics.","Recently, MRI patch of the nigral region has been proposed as a safer and easier alternative.","This paper proposes a symmetric regressor for predicting the DAT uptake amount from the nigral MRI patch.","Acknowledging the symmetry between the right and left nigrae, the proposed regressor incorporates a paired input-output model that simultaneously predicts the DAT uptake amounts for both the right and left striata.","Moreover, it employs a symmetric loss that imposes a constraint on the difference between right-to-left predictions, resembling the high correlation in DAT uptake amounts in the two lateral sides.","Additionally, we propose a symmetric Monte-Carlo (MC) dropout method for providing a fruitful uncertainty estimate of the DAT uptake prediction, which utilizes the above symmetry.","We evaluated the proposed approach on 734 nigral patches, which demonstrated significantly improved performance of the symmetric regressor compared with the standard regressors while giving better explainability and feature representation.","The symmetric MC dropout also gave precise uncertainty ranges with a high probability of including the true DAT uptake amounts within the range."],"url":"http://arxiv.org/abs/2404.11929v1","category":"eess.IV"}
{"created":"2024-04-18 06:15:58","title":"Search for $e^+e^-\\to K^+ K^- \u03c8(3770)$ at center-of-mass energies from 4.84 to 4.95 GeV","abstract":"Using $e^+e^-$ collision data, corresponding to an integrated luminosity of $892\\,\\rm pb^{-1}$ collected at center-of-mass energies from 4.84 to 4.95\\,GeV with the BESIII detector, we search for the process $e^+e^-\\to K^+ K^- \\psi(3770)$ by reconstructing two charged kaons and one $D$ meson from $\\psi(3770)$. No significant signal of $e^+e^-\\to K^+ K^- \\psi(3770)$ is found and the upper limits of the Born cross sections are reported at 90\\% confidence level.","sentences":["Using $e^+e^-$ collision data, corresponding to an integrated luminosity of $892\\,\\rm pb^{-1}$ collected at center-of-mass energies from 4.84 to 4.95\\,GeV with the BESIII detector, we search for the process $e^+e^-\\to K^+ K^- \\psi(3770)$ by reconstructing two charged kaons and one $D$ meson from $\\psi(3770)$. No significant signal of $e^+e^-\\to K^+ K^- \\psi(3770)$ is found and the upper limits of the Born cross sections are reported at 90\\% confidence level."],"url":"http://arxiv.org/abs/2404.11927v1","category":"hep-ex"}
{"created":"2024-04-18 06:02:54","title":"EdgeFusion: On-Device Text-to-Image Generation","abstract":"The intensive computational burden of Stable Diffusion (SD) for text-to-image generation poses a significant hurdle for its practical application. To tackle this challenge, recent research focuses on methods to reduce sampling steps, such as Latent Consistency Model (LCM), and on employing architectural optimizations, including pruning and knowledge distillation. Diverging from existing approaches, we uniquely start with a compact SD variant, BK-SDM. We observe that directly applying LCM to BK-SDM with commonly used crawled datasets yields unsatisfactory results. It leads us to develop two strategies: (1) leveraging high-quality image-text pairs from leading generative models and (2) designing an advanced distillation process tailored for LCM. Through our thorough exploration of quantization, profiling, and on-device deployment, we achieve rapid generation of photo-realistic, text-aligned images in just two steps, with latency under one second on resource-limited edge devices.","sentences":["The intensive computational burden of Stable Diffusion (SD) for text-to-image generation poses a significant hurdle for its practical application.","To tackle this challenge, recent research focuses on methods to reduce sampling steps, such as Latent Consistency Model (LCM), and on employing architectural optimizations, including pruning and knowledge distillation.","Diverging from existing approaches, we uniquely start with a compact SD variant, BK-SDM.","We observe that directly applying LCM to BK-SDM with commonly used crawled datasets yields unsatisfactory results.","It leads us to develop two strategies: (1) leveraging high-quality image-text pairs from leading generative models and (2) designing an advanced distillation process tailored for LCM.","Through our thorough exploration of quantization, profiling, and on-device deployment, we achieve rapid generation of photo-realistic, text-aligned images in just two steps, with latency under one second on resource-limited edge devices."],"url":"http://arxiv.org/abs/2404.11925v1","category":"cs.LG"}
{"created":"2024-04-18 06:02:12","title":"Toward Short-Term Glucose Prediction Solely Based on CGM Time Series","abstract":"The global diabetes epidemic highlights the importance of maintaining good glycemic control. Glucose prediction is a fundamental aspect of diabetes management, facilitating real-time decision-making. Recent research has introduced models focusing on long-term glucose trend prediction, which are unsuitable for real-time decision-making and result in delayed responses. Conversely, models designed to respond to immediate glucose level changes cannot analyze glucose variability comprehensively. Moreover, contemporary research generally integrates various physiological parameters (e.g. insulin doses, food intake, etc.), which inevitably raises data privacy concerns. To bridge such a research gap, we propose TimeGlu -- an end-to-end pipeline for short-term glucose prediction solely based on CGM time series data. We implement four baseline methods to conduct a comprehensive comparative analysis of the model's performance. Through extensive experiments on two contrasting datasets (CGM Glucose and Colas dataset), TimeGlu achieves state-of-the-art performance without the need for additional personal data from patients, providing effective guidance for real-world diabetic glucose management.","sentences":["The global diabetes epidemic highlights the importance of maintaining good glycemic control.","Glucose prediction is a fundamental aspect of diabetes management, facilitating real-time decision-making.","Recent research has introduced models focusing on long-term glucose trend prediction, which are unsuitable for real-time decision-making and result in delayed responses.","Conversely, models designed to respond to immediate glucose level changes cannot analyze glucose variability comprehensively.","Moreover, contemporary research generally integrates various physiological parameters (e.g. insulin doses, food intake, etc.), which inevitably raises data privacy concerns.","To bridge such a research gap, we propose TimeGlu -- an end-to-end pipeline for short-term glucose prediction solely based on CGM time series data.","We implement four baseline methods to conduct a comprehensive comparative analysis of the model's performance.","Through extensive experiments on two contrasting datasets (CGM Glucose and Colas dataset), TimeGlu achieves state-of-the-art performance without the need for additional personal data from patients, providing effective guidance for real-world diabetic glucose management."],"url":"http://arxiv.org/abs/2404.11924v1","category":"cs.AI"}
{"created":"2024-04-18 05:48:15","title":"Expected Coordinate Improvement for High-Dimensional Bayesian Optimization","abstract":"Bayesian optimization (BO) algorithm is very popular for solving low-dimensional expensive optimization problems. Extending Bayesian optimization to high dimension is a meaningful but challenging task. One of the major challenges is that it is difficult to find good infill solutions as the acquisition functions are also high-dimensional. In this work, we propose the expected coordinate improvement (ECI) criterion for high-dimensional Bayesian optimization. The proposed ECI criterion measures the potential improvement we can get by moving the current best solution along one coordinate. The proposed approach selects the coordinate with the highest ECI value to refine in each iteration and covers all the coordinates gradually by iterating over the coordinates. The greatest advantage of the proposed ECI-BO (expected coordinate improvement based Bayesian optimization) algorithm over the standard BO algorithm is that the infill selection problem of the proposed algorithm is always a one-dimensional problem thus can be easily solved. Numerical experiments show that the proposed algorithm can achieve significantly better results than the standard BO algorithm and competitive results when compared with five state-of-the-art high-dimensional BOs. This work provides a simple but efficient approach for high-dimensional Bayesian optimization.","sentences":["Bayesian optimization (BO) algorithm is very popular for solving low-dimensional expensive optimization problems.","Extending Bayesian optimization to high dimension is a meaningful but challenging task.","One of the major challenges is that it is difficult to find good infill solutions as the acquisition functions are also high-dimensional.","In this work, we propose the expected coordinate improvement (ECI) criterion for high-dimensional Bayesian optimization.","The proposed ECI criterion measures the potential improvement we can get by moving the current best solution along one coordinate.","The proposed approach selects the coordinate with the highest ECI value to refine in each iteration and covers all the coordinates gradually by iterating over the coordinates.","The greatest advantage of the proposed ECI-BO (expected coordinate improvement based Bayesian optimization) algorithm over the standard BO algorithm is that the infill selection problem of the proposed algorithm is always a one-dimensional problem thus can be easily solved.","Numerical experiments show that the proposed algorithm can achieve significantly better results than the standard BO algorithm and competitive results when compared with five state-of-the-art high-dimensional BOs.","This work provides a simple but efficient approach for high-dimensional Bayesian optimization."],"url":"http://arxiv.org/abs/2404.11917v1","category":"cs.LG"}
{"created":"2024-04-18 05:43:50","title":"SKIP: Skill-Localized Prompt Tuning for Inference Speed Boost-Up","abstract":"Prompt-tuning methods have shown comparable performance as parameter-efficient fine-tuning (PEFT) methods in various natural language understanding tasks. However, existing prompt tuning methods still utilize the entire model architecture; thus, they fail to accelerate inference speed in the application. In this paper, we propose a novel approach called SKIll-localized Prompt tuning (SKIP), which is extremely efficient in inference time. Our method significantly enhances inference efficiency by investigating and utilizing a skill-localized subnetwork in a language model. Surprisingly, our method improves the inference speed up to 160% while pruning 52% of the parameters. Furthermore, we demonstrate that our method is applicable across various transformer-based architectures, thereby confirming its practicality and scalability.","sentences":["Prompt-tuning methods have shown comparable performance as parameter-efficient fine-tuning (PEFT) methods in various natural language understanding tasks.","However, existing prompt tuning methods still utilize the entire model architecture; thus, they fail to accelerate inference speed in the application.","In this paper, we propose a novel approach called SKIll-localized Prompt tuning (SKIP), which is extremely efficient in inference time.","Our method significantly enhances inference efficiency by investigating and utilizing a skill-localized subnetwork in a language model.","Surprisingly, our method improves the inference speed up to 160% while pruning 52% of the parameters.","Furthermore, we demonstrate that our method is applicable across various transformer-based architectures, thereby confirming its practicality and scalability."],"url":"http://arxiv.org/abs/2404.11916v1","category":"cs.CL"}
{"created":"2024-04-18 05:15:20","title":"Sampling-based Pareto Optimization for Chance-constrained Monotone Submodular Problems","abstract":"Recently surrogate functions based on the tail inequalities were developed to evaluate the chance constraints in the context of evolutionary computation and several Pareto optimization algorithms using these surrogates were successfully applied in optimizing chance-constrained monotone submodular problems. However, the difference in performance between algorithms using the surrogates and those employing the direct sampling-based evaluation remains unclear. Within the paper, a sampling-based method is proposed to directly evaluate the chance constraint. Furthermore, to address the problems with more challenging settings, an enhanced GSEMO algorithm integrated with an adaptive sliding window, called ASW-GSEMO, is introduced. In the experiments, the ASW-GSEMO employing the sampling-based approach is tested on the chance-constrained version of the maximum coverage problem with different settings. Its results are compared with those from other algorithms using different surrogate functions. The experimental findings indicate that the ASW-GSEMO with the sampling-based evaluation approach outperforms other algorithms, highlighting that the performances of algorithms using different evaluation methods are comparable. Additionally, the behaviors of ASW-GSEMO are visualized to explain the distinctions between it and the algorithms utilizing the surrogate functions.","sentences":["Recently surrogate functions based on the tail inequalities were developed to evaluate the chance constraints in the context of evolutionary computation and several Pareto optimization algorithms using these surrogates were successfully applied in optimizing chance-constrained monotone submodular problems.","However, the difference in performance between algorithms using the surrogates and those employing the direct sampling-based evaluation remains unclear.","Within the paper, a sampling-based method is proposed to directly evaluate the chance constraint.","Furthermore, to address the problems with more challenging settings, an enhanced GSEMO algorithm integrated with an adaptive sliding window, called ASW-GSEMO, is introduced.","In the experiments, the ASW-GSEMO employing the sampling-based approach is tested on the chance-constrained version of the maximum coverage problem with different settings.","Its results are compared with those from other algorithms using different surrogate functions.","The experimental findings indicate that the ASW-GSEMO with the sampling-based evaluation approach outperforms other algorithms, highlighting that the performances of algorithms using different evaluation methods are comparable.","Additionally, the behaviors of ASW-GSEMO are visualized to explain the distinctions between it and the algorithms utilizing the surrogate functions."],"url":"http://arxiv.org/abs/2404.11907v1","category":"cs.AI"}
{"created":"2024-04-18 05:00:53","title":"Enhancing Financial Inclusion and Regulatory Challenges: A Critical Analysis of Digital Banks and Alternative Lenders Through Digital Platforms, Machine Learning, and Large Language Models Integration","abstract":"This paper explores the dual impact of digital banks and alternative lenders on financial inclusion and the regulatory challenges posed by their business models. It discusses the integration of digital platforms, machine learning (ML), and Large Language Models (LLMs) in enhancing financial services accessibility for underserved populations. Through a detailed analysis of operational frameworks and technological infrastructures, this research identifies key mechanisms that facilitate broader financial access and mitigate traditional barriers. Additionally, the paper addresses significant regulatory concerns involving data privacy, algorithmic bias, financial stability, and consumer protection. Employing a mixed-methods approach, which combines quantitative financial data analysis with qualitative insights from industry experts, this paper elucidates the complexities of leveraging digital technology to foster financial inclusivity. The findings underscore the necessity of evolving regulatory frameworks that harmonize innovation with comprehensive risk management. This paper concludes with policy recommendations for regulators, financial institutions, and technology providers, aiming to cultivate a more inclusive and stable financial ecosystem through prudent digital technology integration.","sentences":["This paper explores the dual impact of digital banks and alternative lenders on financial inclusion and the regulatory challenges posed by their business models.","It discusses the integration of digital platforms, machine learning (ML), and Large Language Models (LLMs) in enhancing financial services accessibility for underserved populations.","Through a detailed analysis of operational frameworks and technological infrastructures, this research identifies key mechanisms that facilitate broader financial access and mitigate traditional barriers.","Additionally, the paper addresses significant regulatory concerns involving data privacy, algorithmic bias, financial stability, and consumer protection.","Employing a mixed-methods approach, which combines quantitative financial data analysis with qualitative insights from industry experts, this paper elucidates the complexities of leveraging digital technology to foster financial inclusivity.","The findings underscore the necessity of evolving regulatory frameworks that harmonize innovation with comprehensive risk management.","This paper concludes with policy recommendations for regulators, financial institutions, and technology providers, aiming to cultivate a more inclusive and stable financial ecosystem through prudent digital technology integration."],"url":"http://arxiv.org/abs/2404.11898v1","category":"cs.AI"}
{"created":"2024-04-18 04:47:27","title":"Rendering Participating Media Using Path Graphs","abstract":"Rendering volumetric scattering media, including clouds, fog, smoke, and other complex materials, is crucial for realism in computer graphics. Traditional path tracing, while unbiased, requires many long path samples to converge in scenes with scattering media, and a lot of work is wasted by paths that make a negligible contribution to the image. Methods to make better use of the information learned during path tracing range from photon mapping to radiance caching, but struggle to support the full range of heterogeneous scattering media. This paper introduces a new volumetric rendering algorithm that extends and adapts the previous \\emph{path graph} surface rendering algorithm. Our method leverages the information collected through multiple-scattering transport paths to compute lower-noise estimates, increasing computational efficiency by reducing the required sample count. Our key contributions include an extended path graph for participating media and new aggregation and propagation operators for efficient path reuse in volumes. Compared to previous methods, our approach significantly boosts convergence in scenes with challenging volumetric light transport, including heterogeneous media with high scattering albedos and dense, forward-scattering translucent materials, under complex lighting conditions.","sentences":["Rendering volumetric scattering media, including clouds, fog, smoke, and other complex materials, is crucial for realism in computer graphics.","Traditional path tracing, while unbiased, requires many long path samples to converge in scenes with scattering media, and a lot of work is wasted by paths that make a negligible contribution to the image.","Methods to make better use of the information learned during path tracing range from photon mapping to radiance caching, but struggle to support the full range of heterogeneous scattering media.","This paper introduces a new volumetric rendering algorithm that extends and adapts the previous \\emph{path graph} surface rendering algorithm.","Our method leverages the information collected through multiple-scattering transport paths to compute lower-noise estimates, increasing computational efficiency by reducing the required sample count.","Our key contributions include an extended path graph for participating media and new aggregation and propagation operators for efficient path reuse in volumes.","Compared to previous methods, our approach significantly boosts convergence in scenes with challenging volumetric light transport, including heterogeneous media with high scattering albedos and dense, forward-scattering translucent materials, under complex lighting conditions."],"url":"http://arxiv.org/abs/2404.11894v1","category":"cs.GR"}
{"created":"2024-04-18 04:36:37","title":"Large Language Models Can Plan Your Travels Rigorously with Formal Verification Tools","abstract":"The recent advancements of Large Language Models (LLMs), with their abundant world knowledge and capabilities of tool-using and reasoning, fostered many LLM planning algorithms. However, LLMs have not shown to be able to accurately solve complex combinatorial optimization problems. In Xie et al. (2024), the authors proposed TravelPlanner, a U.S. domestic travel planning benchmark, and showed that LLMs themselves cannot make travel plans that satisfy user requirements with a best success rate of 0.6%. In this work, we propose a framework that enables LLMs to formally formulate and solve the travel planning problem as a satisfiability modulo theory (SMT) problem and use SMT solvers interactively and automatically solve the combinatorial search problem. The SMT solvers guarantee the satisfiable of input constraints and the LLMs can enable a language-based interaction with our framework. When the input constraints cannot be satisfiable, our LLM-based framework will interactively offer suggestions to users to modify their travel requirements via automatic reasoning using the SMT solvers. We evaluate our framework with TravelPlanner and achieve a success rate of 97%. We also create a separate dataset that contain international travel benchmarks and use both dataset to evaluate the effectiveness of our interactive planning framework when the initial user queries cannot be satisfied. Our framework could generate valid plans with an average success rate of 78.6% for our dataset and 85.0% for TravelPlanner according to diverse humans preferences.","sentences":["The recent advancements of Large Language Models (LLMs), with their abundant world knowledge and capabilities of tool-using and reasoning, fostered many LLM planning algorithms.","However, LLMs have not shown to be able to accurately solve complex combinatorial optimization problems.","In Xie et al. (2024), the authors proposed TravelPlanner, a U.S. domestic travel planning benchmark, and showed that LLMs themselves cannot make travel plans that satisfy user requirements with a best success rate of 0.6%.","In this work, we propose a framework that enables LLMs to formally formulate and solve the travel planning problem as a satisfiability modulo theory (SMT) problem and use SMT solvers interactively and automatically solve the combinatorial search problem.","The SMT solvers guarantee the satisfiable of input constraints and the LLMs can enable a language-based interaction with our framework.","When the input constraints cannot be satisfiable, our LLM-based framework will interactively offer suggestions to users to modify their travel requirements via automatic reasoning using the SMT solvers.","We evaluate our framework with TravelPlanner and achieve a success rate of 97%.","We also create a separate dataset that contain international travel benchmarks and use both dataset to evaluate the effectiveness of our interactive planning framework when the initial user queries cannot be satisfied.","Our framework could generate valid plans with an average success rate of 78.6% for our dataset and 85.0% for TravelPlanner according to diverse humans preferences."],"url":"http://arxiv.org/abs/2404.11891v1","category":"cs.AI"}
{"created":"2024-04-18 04:30:18","title":"FCNCP: A Coupled Nonnegative CANDECOMP/PARAFAC Decomposition Based on Federated Learning","abstract":"In the field of brain science, data sharing across servers is becoming increasingly challenging due to issues such as industry competition, privacy security, and administrative procedure policies and regulations. Therefore, there is an urgent need to develop new methods for data analysis and processing that enable scientific collaboration without data sharing. In view of this, this study proposes to study and develop a series of efficient non-negative coupled tensor decomposition algorithm frameworks based on federated learning called FCNCP for the EEG data arranged on different servers. It combining the good discriminative performance of tensor decomposition in high-dimensional data representation and decomposition, the advantages of coupled tensor decomposition in cross-sample tensor data analysis, and the features of federated learning for joint modelling in distributed servers. The algorithm utilises federation learning to establish coupling constraints for data distributed across different servers. In the experiments, firstly, simulation experiments are carried out using simulated data, and stable and consistent decomposition results are obtained, which verify the effectiveness of the proposed algorithms in this study. Then the FCNCP algorithm was utilised to decompose the fifth-order event-related potential (ERP) tensor data collected by applying proprioceptive stimuli on the left and right hands. It was found that contralateral stimulation induced more symmetrical components in the activation areas of the left and right hemispheres. The conclusions drawn are consistent with the interpretations of related studies in cognitive neuroscience, demonstrating that the method can efficiently process higher-order EEG data and that some key hidden information can be preserved.","sentences":["In the field of brain science, data sharing across servers is becoming increasingly challenging due to issues such as industry competition, privacy security, and administrative procedure policies and regulations.","Therefore, there is an urgent need to develop new methods for data analysis and processing that enable scientific collaboration without data sharing.","In view of this, this study proposes to study and develop a series of efficient non-negative coupled tensor decomposition algorithm frameworks based on federated learning called FCNCP for the EEG data arranged on different servers.","It combining the good discriminative performance of tensor decomposition in high-dimensional data representation and decomposition, the advantages of coupled tensor decomposition in cross-sample tensor data analysis, and the features of federated learning for joint modelling in distributed servers.","The algorithm utilises federation learning to establish coupling constraints for data distributed across different servers.","In the experiments, firstly, simulation experiments are carried out using simulated data, and stable and consistent decomposition results are obtained, which verify the effectiveness of the proposed algorithms in this study.","Then the FCNCP algorithm was utilised to decompose the fifth-order event-related potential (ERP) tensor data collected by applying proprioceptive stimuli on the left and right hands.","It was found that contralateral stimulation induced more symmetrical components in the activation areas of the left and right hemispheres.","The conclusions drawn are consistent with the interpretations of related studies in cognitive neuroscience, demonstrating that the method can efficiently process higher-order EEG data and that some key hidden information can be preserved."],"url":"http://arxiv.org/abs/2404.11890v1","category":"math.NA"}
{"created":"2024-04-18 04:25:21","title":"The Dog Walking Theory: Rethinking Convergence in Federated Learning","abstract":"Federated learning (FL) is a collaborative learning paradigm that allows different clients to train one powerful global model without sharing their private data. Although FL has demonstrated promising results in various applications, it is known to suffer from convergence issues caused by the data distribution shift across different clients, especially on non-independent and identically distributed (non-IID) data. In this paper, we study the convergence of FL on non-IID data and propose a novel \\emph{Dog Walking Theory} to formulate and identify the missing element in existing research. The Dog Walking Theory describes the process of a dog walker leash walking multiple dogs from one side of the park to the other. The goal of the dog walker is to arrive at the right destination while giving the dogs enough exercise (i.e., space exploration). In FL, the server is analogous to the dog walker while the clients are analogous to the dogs. This analogy allows us to identify one crucial yet missing element in existing FL algorithms: the leash that guides the exploration of the clients. To address this gap, we propose a novel FL algorithm \\emph{FedWalk} that leverages an external easy-to-converge task at the server side as a \\emph{leash task} to guide the local training of the clients. We theoretically analyze the convergence of FedWalk with respect to data heterogeneity (between server and clients) and task discrepancy (between the leash and the original tasks). Experiments on multiple benchmark datasets demonstrate the superiority of FedWalk over state-of-the-art FL methods under both IID and non-IID settings.","sentences":["Federated learning (FL) is a collaborative learning paradigm that allows different clients to train one powerful global model without sharing their private data.","Although FL has demonstrated promising results in various applications, it is known to suffer from convergence issues caused by the data distribution shift across different clients, especially on non-independent and identically distributed (non-IID) data.","In this paper, we study the convergence of FL on non-IID data and propose a novel \\emph{Dog Walking Theory} to formulate and identify the missing element in existing research.","The Dog Walking Theory describes the process of a dog walker leash walking multiple dogs from one side of the park to the other.","The goal of the dog walker is to arrive at the right destination while giving the dogs enough exercise (i.e., space exploration).","In FL, the server is analogous to the dog walker while the clients are analogous to the dogs.","This analogy allows us to identify one crucial yet missing element in existing FL algorithms: the leash that guides the exploration of the clients.","To address this gap, we propose a novel FL algorithm \\emph{FedWalk} that leverages an external easy-to-converge task at the server side as a \\emph{leash task} to guide the local training of the clients.","We theoretically analyze the convergence of FedWalk with respect to data heterogeneity (between server and clients) and task discrepancy (between the leash and the original tasks).","Experiments on multiple benchmark datasets demonstrate the superiority of FedWalk over state-of-the-art FL methods under both IID and non-IID settings."],"url":"http://arxiv.org/abs/2404.11888v1","category":"cs.LG"}
{"created":"2024-04-18 03:52:19","title":"Hybrid Navigation Acceptability and Safety","abstract":"Autonomous vessels have emerged as a prominent and accepted solution, particularly in the naval defence sector. However, achieving full autonomy for marine vessels demands the development of robust and reliable control and guidance systems that can handle various encounters with manned and unmanned vessels while operating effectively under diverse weather and sea conditions. A significant challenge in this pursuit is ensuring the autonomous vessels' compliance with the International Regulations for Preventing Collisions at Sea (COLREGs). These regulations present a formidable hurdle for the human-level understanding by autonomous systems as they were originally designed from common navigation practices created since the mid-19th century. Their ambiguous language assumes experienced sailors' interpretation and execution, and therefore demands a high-level (cognitive) understanding of language and agent intentions. These capabilities surpass the current state-of-the-art in intelligent systems. This position paper highlights the critical requirements for a trustworthy control and guidance system, exploring the complexity of adapting COLREGs for safe vessel-on-vessel encounters considering autonomous maritime technology competing and/or cooperating with manned vessels.","sentences":["Autonomous vessels have emerged as a prominent and accepted solution, particularly in the naval defence sector.","However, achieving full autonomy for marine vessels demands the development of robust and reliable control and guidance systems that can handle various encounters with manned and unmanned vessels while operating effectively under diverse weather and sea conditions.","A significant challenge in this pursuit is ensuring the autonomous vessels' compliance with the International Regulations for Preventing Collisions at Sea (COLREGs).","These regulations present a formidable hurdle for the human-level understanding by autonomous systems as they were originally designed from common navigation practices created since the mid-19th century.","Their ambiguous language assumes experienced sailors' interpretation and execution, and therefore demands a high-level (cognitive) understanding of language and agent intentions.","These capabilities surpass the current state-of-the-art in intelligent systems.","This position paper highlights the critical requirements for a trustworthy control and guidance system, exploring the complexity of adapting COLREGs for safe vessel-on-vessel encounters considering autonomous maritime technology competing and/or cooperating with manned vessels."],"url":"http://arxiv.org/abs/2404.11882v1","category":"eess.SY"}
{"created":"2024-04-18 03:22:02","title":"Concept Induction using LLMs: a user experiment for assessment","abstract":"Explainable Artificial Intelligence (XAI) poses a significant challenge in providing transparent and understandable insights into complex AI models. Traditional post-hoc algorithms, while useful, often struggle to deliver interpretable explanations. Concept-based models offer a promising avenue by incorporating explicit representations of concepts to enhance interpretability. However, existing research on automatic concept discovery methods is often limited by lower-level concepts, costly human annotation requirements, and a restricted domain of background knowledge. In this study, we explore the potential of a Large Language Model (LLM), specifically GPT-4, by leveraging its domain knowledge and common-sense capability to generate high-level concepts that are meaningful as explanations for humans, for a specific setting of image classification. We use minimal textual object information available in the data via prompting to facilitate this process. To evaluate the output, we compare the concepts generated by the LLM with two other methods: concepts generated by humans and the ECII heuristic concept induction system. Since there is no established metric to determine the human understandability of concepts, we conducted a human study to assess the effectiveness of the LLM-generated concepts. Our findings indicate that while human-generated explanations remain superior, concepts derived from GPT-4 are more comprehensible to humans compared to those generated by ECII.","sentences":["Explainable Artificial Intelligence (XAI) poses a significant challenge in providing transparent and understandable insights into complex AI models.","Traditional post-hoc algorithms, while useful, often struggle to deliver interpretable explanations.","Concept-based models offer a promising avenue by incorporating explicit representations of concepts to enhance interpretability.","However, existing research on automatic concept discovery methods is often limited by lower-level concepts, costly human annotation requirements, and a restricted domain of background knowledge.","In this study, we explore the potential of a Large Language Model (LLM), specifically GPT-4, by leveraging its domain knowledge and common-sense capability to generate high-level concepts that are meaningful as explanations for humans, for a specific setting of image classification.","We use minimal textual object information available in the data via prompting to facilitate this process.","To evaluate the output, we compare the concepts generated by the LLM with two other methods: concepts generated by humans and the ECII heuristic concept induction system.","Since there is no established metric to determine the human understandability of concepts, we conducted a human study to assess the effectiveness of the LLM-generated concepts.","Our findings indicate that while human-generated explanations remain superior, concepts derived from GPT-4 are more comprehensible to humans compared to those generated by ECII."],"url":"http://arxiv.org/abs/2404.11875v1","category":"cs.AI"}
{"created":"2024-04-18 03:21:28","title":"SNP: Structured Neuron-level Pruning to Preserve Attention Scores","abstract":"Multi-head self-attention (MSA) is a key component of Vision Transformers (ViTs), which have achieved great success in various vision tasks. However, their high computational cost and memory footprint hinder their deployment on resource-constrained devices. Conventional pruning approaches can only compress and accelerate the MSA module using head pruning, although the head is not an atomic unit. To address this issue, we propose a novel graph-aware neuron-level pruning method, Structured Neuron-level Pruning (SNP). SNP prunes neurons with less informative attention scores and eliminates redundancy among heads. Specifically, it prunes graphically connected query and key layers having the least informative attention scores while preserving the overall attention scores. Value layers, which can be pruned independently, are pruned to eliminate inter-head redundancy. Our proposed method effectively compresses and accelerates Transformer-based models for both edge devices and server processors. For instance, the DeiT-Small with SNP runs 3.1$\\times$ faster than the original model and achieves performance that is 21.94\\% faster and 1.12\\% higher than the DeiT-Tiny. Additionally, SNP combine successfully with conventional head or block pruning approaches. SNP with head pruning could compress the DeiT-Base by 80\\% of the parameters and computational costs and achieve 3.85$\\times$ faster inference speed on RTX3090 and 4.93$\\times$ on Jetson Nano.","sentences":["Multi-head self-attention (MSA) is a key component of Vision Transformers (ViTs), which have achieved great success in various vision tasks.","However, their high computational cost and memory footprint hinder their deployment on resource-constrained devices.","Conventional pruning approaches can only compress and accelerate the MSA module using head pruning, although the head is not an atomic unit.","To address this issue, we propose a novel graph-aware neuron-level pruning method, Structured Neuron-level Pruning (SNP).","SNP prunes neurons with less informative attention scores and eliminates redundancy among heads.","Specifically, it prunes graphically connected query and key layers having the least informative attention scores while preserving the overall attention scores.","Value layers, which can be pruned independently, are pruned to eliminate inter-head redundancy.","Our proposed method effectively compresses and accelerates Transformer-based models for both edge devices and server processors.","For instance, the DeiT-Small with SNP runs 3.1$\\times$ faster than the original model and achieves performance that is 21.94\\% faster and 1.12\\% higher than the DeiT-Tiny.","Additionally, SNP combine successfully with conventional head or block pruning approaches.","SNP with head pruning could compress the DeiT-Base by 80\\% of the parameters and computational costs and achieve 3.85$\\times$ faster inference speed on RTX3090 and 4.93$\\times$ on Jetson Nano."],"url":"http://arxiv.org/abs/2404.11630v1","category":"cs.CV"}
{"created":"2024-04-18 03:17:45","title":"Using a Local Surrogate Model to Interpret Temporal Shifts in Global Annual Data","abstract":"This paper focuses on explaining changes over time in globally-sourced, annual temporal data, with the specific objective of identifying pivotal factors that contribute to these temporal shifts. Leveraging such analytical frameworks can yield transformative impacts, including the informed refinement of public policy and the identification of key drivers affecting a country's economic evolution. We employ Local Interpretable Model-agnostic Explanations (LIME) to shed light on national happiness indices, economic freedom, and population metrics, spanning variable time frames. Acknowledging the presence of missing values, we employ three imputation approaches to generate robust multivariate time-series datasets apt for LIME's input requirements. Our methodology's efficacy is substantiated through a series of empirical evaluations involving multiple datasets. These evaluations include comparative analyses against random feature selection, correlation with real-world events as elucidated by LIME, and validation through Individual Conditional Expectation (ICE) plots, a state-of-the-art technique proficient in feature importance detection.","sentences":["This paper focuses on explaining changes over time in globally-sourced, annual temporal data, with the specific objective of identifying pivotal factors that contribute to these temporal shifts.","Leveraging such analytical frameworks can yield transformative impacts, including the informed refinement of public policy and the identification of key drivers affecting a country's economic evolution.","We employ Local Interpretable Model-agnostic Explanations (LIME) to shed light on national happiness indices, economic freedom, and population metrics, spanning variable time frames.","Acknowledging the presence of missing values, we employ three imputation approaches to generate robust multivariate time-series datasets apt for LIME's input requirements.","Our methodology's efficacy is substantiated through a series of empirical evaluations involving multiple datasets.","These evaluations include comparative analyses against random feature selection, correlation with real-world events as elucidated by LIME, and validation through Individual Conditional Expectation (ICE) plots, a state-of-the-art technique proficient in feature importance detection."],"url":"http://arxiv.org/abs/2404.11874v1","category":"cs.LG"}
{"created":"2024-04-18 02:31:50","title":"Effective medium theory for Van-Der-Waals heterostructures","abstract":"We derive the electromagnetic medium equivalent to a collection of all-dielectric nano-particles (enjoying high refractive indices) distributed locally non-periodically in a smooth domain $\\Omega$. Such distributions are used to model well known structures in material sciences as the Van-der-Waals heterostructures. Since the nano-particles are all-dielectric, then the permittivity remains unchanged while the permeability is altered by this effective medium. This equivalent medium describes, in particular, the effective medium of 2 dimensional type Van-der-Waals heterostructures. These structures are 3 dimensional which are build as superposition of identical (2D)-sheets each supporting locally non-periodic distributions of nano-particles. An explicit form of this effective medium is provided for the particular case of honeycomb heterostructures.   At the mathematical analysis level, we propose a new approach to derive the effective medium when the subwavelength nano-particles are distributed non-periodically. The first step consists in deriving the point interaction approximation, also called the Foldy-Lax approximation. The scattered field is given as a superposition of dipoles (or poles for other models) multiplied by the elements of a vector which is itself solution of an algebraic system. This step is done regardless of the way how the particles are distributed. As a second step, which is the new and critical step, we rewrite this algebraic system according to the way how these nano-particles are locally distributed. The new algebraic system will then fix the related continuous Lippmann Schwinger system which, in its turn, indicates naturally the equivalent medium.","sentences":["We derive the electromagnetic medium equivalent to a collection of all-dielectric nano-particles (enjoying high refractive indices) distributed locally non-periodically in a smooth domain $\\Omega$. Such distributions are used to model well known structures in material sciences as the Van-der-Waals heterostructures.","Since the nano-particles are all-dielectric, then the permittivity remains unchanged while the permeability is altered by this effective medium.","This equivalent medium describes, in particular, the effective medium of 2 dimensional type Van-der-Waals heterostructures.","These structures are 3 dimensional which are build as superposition of identical (2D)-sheets each supporting locally non-periodic distributions of nano-particles.","An explicit form of this effective medium is provided for the particular case of honeycomb heterostructures.   ","At the mathematical analysis level, we propose a new approach to derive the effective medium when the subwavelength nano-particles are distributed non-periodically.","The first step consists in deriving the point interaction approximation, also called the Foldy-Lax approximation.","The scattered field is given as a superposition of dipoles (or poles for other models) multiplied by the elements of a vector which is itself solution of an algebraic system.","This step is done regardless of the way how the particles are distributed.","As a second step, which is the new and critical step, we rewrite this algebraic system according to the way how these nano-particles are locally distributed.","The new algebraic system will then fix the related continuous Lippmann Schwinger system which, in its turn, indicates naturally the equivalent medium."],"url":"http://arxiv.org/abs/2404.11859v1","category":"math.AP"}
{"created":"2024-04-18 02:15:40","title":"SGRU: A High-Performance Structured Gated Recurrent Unit for Traffic Flow Prediction","abstract":"Traffic flow prediction is an essential task in constructing smart cities and is a typical Multivariate Time Series (MTS) Problem. Recent research has abandoned Gated Recurrent Units (GRU) and utilized dilated convolutions or temporal slicing for feature extraction, and they have the following drawbacks: (1) Dilated convolutions fail to capture the features of adjacent time steps, resulting in the loss of crucial transitional data. (2) The connections within the same temporal slice are strong, while the connections between different temporal slices are too loose. In light of these limitations, we emphasize the importance of analyzing a complete time series repeatedly and the crucial role of GRU in MTS. Therefore, we propose SGRU: Structured Gated Recurrent Units, which involve structured GRU layers and non-linear units, along with multiple layers of time embedding to enhance the model's fitting performance. We evaluate our approach on four publicly available California traffic datasets: PeMS03, PeMS04, PeMS07, and PeMS08 for regression prediction. Experimental results demonstrate that our model outperforms baseline models with average improvements of 11.7%, 18.6%, 18.5%, and 12.0% respectively.","sentences":["Traffic flow prediction is an essential task in constructing smart cities and is a typical Multivariate Time Series (MTS) Problem.","Recent research has abandoned Gated Recurrent Units (GRU) and utilized dilated convolutions or temporal slicing for feature extraction, and they have the following drawbacks: (1) Dilated convolutions fail to capture the features of adjacent time steps, resulting in the loss of crucial transitional data.","(2) The connections within the same temporal slice are strong, while the connections between different temporal slices are too loose.","In light of these limitations, we emphasize the importance of analyzing a complete time series repeatedly and the crucial role of GRU in MTS.","Therefore, we propose SGRU: Structured Gated Recurrent Units, which involve structured GRU layers and non-linear units, along with multiple layers of time embedding to enhance the model's fitting performance.","We evaluate our approach on four publicly available California traffic datasets: PeMS03, PeMS04, PeMS07, and PeMS08 for regression prediction.","Experimental results demonstrate that our model outperforms baseline models with average improvements of 11.7%, 18.6%, 18.5%, and 12.0% respectively."],"url":"http://arxiv.org/abs/2404.11854v1","category":"cs.AI"}
{"created":"2024-04-18 01:48:28","title":"Challenging Negative Gender Stereotypes: A Study on the Effectiveness of Automated Counter-Stereotypes","abstract":"Gender stereotypes are pervasive beliefs about individuals based on their gender that play a significant role in shaping societal attitudes, behaviours, and even opportunities. Recognizing the negative implications of gender stereotypes, particularly in online communications, this study investigates eleven strategies to automatically counter-act and challenge these views. We present AI-generated gender-based counter-stereotypes to (self-identified) male and female study participants and ask them to assess their offensiveness, plausibility, and potential effectiveness. The strategies of counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) emerged as the most robust approaches, while humour, perspective-taking, counter-examples, and empathy for the speaker were perceived as less effective. Also, the differences in ratings were more pronounced for stereotypes about the different targets than between the genders of the raters. Alarmingly, many AI-generated counter-stereotypes were perceived as offensive and/or implausible. Our analysis and the collected dataset offer foundational insight into counter-stereotype generation, guiding future efforts to develop strategies that effectively challenge gender stereotypes in online interactions.","sentences":["Gender stereotypes are pervasive beliefs about individuals based on their gender that play a significant role in shaping societal attitudes, behaviours, and even opportunities.","Recognizing the negative implications of gender stereotypes, particularly in online communications, this study investigates eleven strategies to automatically counter-act and challenge these views.","We present AI-generated gender-based counter-stereotypes to (self-identified) male and female study participants and ask them to assess their offensiveness, plausibility, and potential effectiveness.","The strategies of counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) emerged as the most robust approaches, while humour, perspective-taking, counter-examples, and empathy for the speaker were perceived as less effective.","Also, the differences in ratings were more pronounced for stereotypes about the different targets than between the genders of the raters.","Alarmingly, many AI-generated counter-stereotypes were perceived as offensive and/or implausible.","Our analysis and the collected dataset offer foundational insight into counter-stereotype generation, guiding future efforts to develop strategies that effectively challenge gender stereotypes in online interactions."],"url":"http://arxiv.org/abs/2404.11845v1","category":"cs.CL"}
{"created":"2024-04-18 01:45:46","title":"Generating synthetic electroretinogram waveforms using Artificial Intelligence to improve classification of retinal conditions in under-represented populations","abstract":"Visual electrophysiology is often used clinically to determine functional changes associated with retinal or neurological conditions. The full-field flash electroretinogram (ERG) assesses the global contribution of the outer and inner retinal layers initiated by the rods and cone pathways depending on the state of retinal adaptation. Within clinical centers reference normative data are used to compare with clinical cases that may be rare or underpowered in a specific demographic. To bolster either reference or case datasets the application of synthetic ERG waveforms may offer benefits to disease classification and case-control studies. In this study and as a proof of concept, artificial intelligence (AI) to generate synthetic signals using Generative Adversarial Networks is deployed to up-scale male participants within an ISCEV reference dataset containing 32 participants, with waveforms from the right and left eye. Random Forest Classifiers further improved classification for sex within the group from a balanced accuracy of 0.72 to 0.83 with the added synthetic male waveforms. This is the first study to demonstrate the generation of synthetic ERG waveforms to improve machine learning classification modelling with electroretinogram waveforms.","sentences":["Visual electrophysiology is often used clinically to determine functional changes associated with retinal or neurological conditions.","The full-field flash electroretinogram (ERG) assesses the global contribution of the outer and inner retinal layers initiated by the rods and cone pathways depending on the state of retinal adaptation.","Within clinical centers reference normative data are used to compare with clinical cases that may be rare or underpowered in a specific demographic.","To bolster either reference or case datasets the application of synthetic ERG waveforms may offer benefits to disease classification and case-control studies.","In this study and as a proof of concept, artificial intelligence (AI) to generate synthetic signals using Generative Adversarial Networks is deployed to up-scale male participants within an ISCEV reference dataset containing 32 participants, with waveforms from the right and left eye.","Random Forest Classifiers further improved classification for sex within the group from a balanced accuracy of 0.72 to 0.83 with the added synthetic male waveforms.","This is the first study to demonstrate the generation of synthetic ERG waveforms to improve machine learning classification modelling with electroretinogram waveforms."],"url":"http://arxiv.org/abs/2404.11842v1","category":"q-bio.NC"}
{"created":"2024-04-18 01:36:03","title":"AI-Empowered RIS-Assisted Networks: CV-Enabled RIS Selection and DNN-Enabled Transmission","abstract":"This paper investigates artificial intelligence (AI) empowered schemes for reconfigurable intelligent surface (RIS) assisted networks from the perspective of fast implementation. We formulate a weighted sum-rate maximization problem for a multi-RIS-assisted network. To avoid huge channel estimation overhead due to activate all RISs, we propose a computer vision (CV) enabled RIS selection scheme based on a single shot multi-box detector. To realize real-time resource allocation, a deep neural network (DNN) enabled transmit design is developed to learn the optimal mapping from channel information to transmit beamformers and phase shift matrix. Numerical results illustrate that the CV module is able to select of RIS with the best propagation condition. The well-trained DNN achieves similar sum-rate performance to the existing alternative optimization method but with much smaller inference time.","sentences":["This paper investigates artificial intelligence (AI) empowered schemes for reconfigurable intelligent surface (RIS) assisted networks from the perspective of fast implementation.","We formulate a weighted sum-rate maximization problem for a multi-RIS-assisted network.","To avoid huge channel estimation overhead due to activate all RISs, we propose a computer vision (CV) enabled RIS selection scheme based on a single shot multi-box detector.","To realize real-time resource allocation, a deep neural network (DNN) enabled transmit design is developed to learn the optimal mapping from channel information to transmit beamformers and phase shift matrix.","Numerical results illustrate that the CV module is able to select of RIS with the best propagation condition.","The well-trained DNN achieves similar sum-rate performance to the existing alternative optimization method but with much smaller inference time."],"url":"http://arxiv.org/abs/2404.11836v1","category":"eess.SP"}
{"created":"2024-04-18 01:31:19","title":"CAUS: A Dataset for Question Generation based on Human Cognition Leveraging Large Language Models","abstract":"We introduce the CAUS (Curious About Uncertain Scene) dataset, designed to enable Large Language Models, specifically GPT-4, to emulate human cognitive processes for resolving uncertainties. Leveraging this dataset, we investigate the potential of LLMs to engage in questioning effectively. Our approach involves providing scene descriptions embedded with uncertainties to stimulate the generation of reasoning and queries. The queries are then classified according to multi-dimensional criteria. All procedures are facilitated by a collaborative system involving both LLMs and human researchers. Our results demonstrate that GPT-4 can effectively generate pertinent questions and grasp their nuances, particularly when given appropriate context and instructions. The study suggests that incorporating human-like questioning into AI models improves their ability to manage uncertainties, paving the way for future advancements in Artificial Intelligence (AI).","sentences":["We introduce the CAUS (Curious About Uncertain Scene) dataset, designed to enable Large Language Models, specifically GPT-4, to emulate human cognitive processes for resolving uncertainties.","Leveraging this dataset, we investigate the potential of LLMs to engage in questioning effectively.","Our approach involves providing scene descriptions embedded with uncertainties to stimulate the generation of reasoning and queries.","The queries are then classified according to multi-dimensional criteria.","All procedures are facilitated by a collaborative system involving both LLMs and human researchers.","Our results demonstrate that GPT-4 can effectively generate pertinent questions and grasp their nuances, particularly when given appropriate context and instructions.","The study suggests that incorporating human-like questioning into AI models improves their ability to manage uncertainties, paving the way for future advancements in Artificial Intelligence (AI)."],"url":"http://arxiv.org/abs/2404.11835v1","category":"cs.AI"}
{"created":"2024-04-18 01:27:29","title":"Planning with Language Models Through The Lens of Efficiency","abstract":"We analyse the cost of using LLMs for planning and highlight that recent trends are profoundly uneconomical. We propose a significantly more efficient approach and argue for a responsible use of compute resources; urging research community to investigate LLM-based approaches that upholds efficiency.","sentences":["We analyse the cost of using LLMs for planning and highlight that recent trends are profoundly uneconomical.","We propose a significantly more efficient approach and argue for a responsible use of compute resources; urging research community to investigate LLM-based approaches that upholds efficiency."],"url":"http://arxiv.org/abs/2404.11833v1","category":"cs.AI"}
{"created":"2024-04-18 01:15:41","title":"AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence","abstract":"As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity.","sentences":["As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas.","To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum.","This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework.","Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric.","Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness.","AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity."],"url":"http://arxiv.org/abs/2404.11826v1","category":"cs.CL"}
{"created":"2024-04-18 01:10:24","title":"TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation","abstract":"Recent advancements in Text-to-image (T2I) generation have witnessed a shift from adapting text to fixed backgrounds to creating images around text. Traditional approaches are often limited to generate layouts within static images for effective text placement. Our proposed approach, TextCenGen, introduces a dynamic adaptation of the blank region for text-friendly image generation, emphasizing text-centric design and visual harmony generation. Our method employs force-directed attention guidance in T2I models to generate images that strategically reserve whitespace for pre-defined text areas, even for text or icons at the golden ratio. Observing how cross-attention maps affect object placement, we detect and repel conflicting objects using a force-directed graph approach, combined with a Spatial Excluding Cross-Attention Constraint for smooth attention in whitespace areas. As a novel task in graphic design, experiments indicate that TextCenGen outperforms existing methods with more harmonious compositions. Furthermore, our method significantly enhances T2I model outcomes on our specially collected prompt datasets, catering to varied text positions. These results demonstrate the efficacy of TextCenGen in creating more harmonious and integrated text-image compositions.","sentences":["Recent advancements in Text-to-image (T2I) generation have witnessed a shift from adapting text to fixed backgrounds to creating images around text.","Traditional approaches are often limited to generate layouts within static images for effective text placement.","Our proposed approach, TextCenGen, introduces a dynamic adaptation of the blank region for text-friendly image generation, emphasizing text-centric design and visual harmony generation.","Our method employs force-directed attention guidance in T2I models to generate images that strategically reserve whitespace for pre-defined text areas, even for text or icons at the golden ratio.","Observing how cross-attention maps affect object placement, we detect and repel conflicting objects using a force-directed graph approach, combined with a Spatial Excluding Cross-Attention Constraint for smooth attention in whitespace areas.","As a novel task in graphic design, experiments indicate that TextCenGen outperforms existing methods with more harmonious compositions.","Furthermore, our method significantly enhances T2I model outcomes on our specially collected prompt datasets, catering to varied text positions.","These results demonstrate the efficacy of TextCenGen in creating more harmonious and integrated text-image compositions."],"url":"http://arxiv.org/abs/2404.11824v1","category":"cs.CV"}
{"created":"2024-04-18 00:53:13","title":"Applicability of Eliashberg theory for systems with electron-phonon and electron-electron interaction: a comparative analysis","abstract":"We present a comparative analysis of the validity of Eliashberg theory for the cases of fermions interacting with an Einstein phonon and with soft nematic fluctuations near an Ising-nematic/Ising-ferromagnetic quantum-critical point (QCP). In both cases, Eliashberg theory is obtained by neglecting vertex corrections. For the phonon case, the reasoning to neglect vertex corrections is the Migdal ``fast electron/slow boson'' argument because the phonon velocity is much smaller than the Fermi velocity, $v_F$. The same argument allows one to compute the fermionic self-energy within Eliashberg theory perturbatively rather than self-consistently. For the nematic case, the velocity of a collective boson is comparable to $v_F$ and this argument apparently does not work. Nonetheless, we argue that while two-loop vertex corrections near a nematic QCP are not small parametrically, they are small numerically. At the same time, perturbative calculation of the fermionic self-energy can be rigorously justified when the fermion-boson coupling is small compared to the Fermi energy. Furthermore, we argue that for the electron-phonon case Eliashberg theory breaks down at some distance from where the dressed Debye frequency would vanish, while for the nematic case it holds all the way to a QCP. From this perspective, Eliashberg theory for the nematic case actually works better than for the electron-phonon case.","sentences":["We present a comparative analysis of the validity of Eliashberg theory for the cases of fermions interacting with an Einstein phonon and with soft nematic fluctuations near an Ising-nematic/Ising-ferromagnetic quantum-critical point (QCP).","In both cases, Eliashberg theory is obtained by neglecting vertex corrections.","For the phonon case, the reasoning to neglect vertex corrections is the Migdal ``fast electron/slow boson'' argument because the phonon velocity is much smaller than the Fermi velocity, $v_F$. The same argument allows one to compute the fermionic self-energy within Eliashberg theory perturbatively rather than self-consistently.","For the nematic case, the velocity of a collective boson is comparable to $v_F$ and this argument apparently does not work.","Nonetheless, we argue that while two-loop vertex corrections near a nematic QCP are not small parametrically, they are small numerically.","At the same time, perturbative calculation of the fermionic self-energy can be rigorously justified when the fermion-boson coupling is small compared to the Fermi energy.","Furthermore, we argue that for the electron-phonon case Eliashberg theory breaks down at some distance from where the dressed Debye frequency would vanish, while for the nematic case it holds all the way to a QCP.","From this perspective, Eliashberg theory for the nematic case actually works better than for the electron-phonon case."],"url":"http://arxiv.org/abs/2404.11820v1","category":"cond-mat.str-el"}
{"created":"2024-04-18 00:18:07","title":"Cross-model Mutual Learning for Exemplar-based Medical Image Segmentation","abstract":"Medical image segmentation typically demands extensive dense annotations for model training, which is both time-consuming and skill-intensive. To mitigate this burden, exemplar-based medical image segmentation methods have been introduced to achieve effective training with only one annotated image. In this paper, we introduce a novel Cross-model Mutual learning framework for Exemplar-based Medical image Segmentation (CMEMS), which leverages two models to mutually excavate implicit information from unlabeled data at multiple granularities. CMEMS can eliminate confirmation bias and enable collaborative training to learn complementary information by enforcing consistency at different granularities across models. Concretely, cross-model image perturbation based mutual learning is devised by using weakly perturbed images to generate high-confidence pseudo-labels, supervising predictions of strongly perturbed images across models. This approach enables joint pursuit of prediction consistency at the image granularity. Moreover, cross-model multi-level feature perturbation based mutual learning is designed by letting pseudo-labels supervise predictions from perturbed multi-level features with different resolutions, which can broaden the perturbation space and enhance the robustness of our framework. CMEMS is jointly trained using exemplar data, synthetic data, and unlabeled data in an end-to-end manner. Experimental results on two medical image datasets indicate that the proposed CMEMS outperforms the state-of-the-art segmentation methods with extremely limited supervision.","sentences":["Medical image segmentation typically demands extensive dense annotations for model training, which is both time-consuming and skill-intensive.","To mitigate this burden, exemplar-based medical image segmentation methods have been introduced to achieve effective training with only one annotated image.","In this paper, we introduce a novel Cross-model Mutual learning framework for Exemplar-based Medical image Segmentation (CMEMS), which leverages two models to mutually excavate implicit information from unlabeled data at multiple granularities.","CMEMS can eliminate confirmation bias and enable collaborative training to learn complementary information by enforcing consistency at different granularities across models.","Concretely, cross-model image perturbation based mutual learning is devised by using weakly perturbed images to generate high-confidence pseudo-labels, supervising predictions of strongly perturbed images across models.","This approach enables joint pursuit of prediction consistency at the image granularity.","Moreover, cross-model multi-level feature perturbation based mutual learning is designed by letting pseudo-labels supervise predictions from perturbed multi-level features with different resolutions, which can broaden the perturbation space and enhance the robustness of our framework.","CMEMS is jointly trained using exemplar data, synthetic data, and unlabeled data in an end-to-end manner.","Experimental results on two medical image datasets indicate that the proposed CMEMS outperforms the state-of-the-art segmentation methods with extremely limited supervision."],"url":"http://arxiv.org/abs/2404.11812v1","category":"cs.CV"}
{"created":"2024-04-18 00:17:01","title":"Physics-informed active learning for accelerating quantum chemical simulations","abstract":"Quantum chemical simulations can be greatly accelerated by constructing machine learning potentials, which is often done using active learning (AL). The usefulness of the constructed potentials is often limited by the high effort required and their insufficient robustness in the simulations. Here we introduce the end-to-end AL for constructing robust data-efficient potentials with affordable investment of time and resources and minimum human interference. Our AL protocol is based on the physics-informed sampling of training points, automatic selection of initial data, and uncertainty quantification. The versatility of this protocol is shown in our implementation of quasi-classical molecular dynamics for simulating vibrational spectra, conformer search of a key biochemical molecule, and time-resolved mechanism of the Diels-Alder reaction. These investigations took us days instead of weeks of pure quantum chemical calculations on a high-performance computing cluster.","sentences":["Quantum chemical simulations can be greatly accelerated by constructing machine learning potentials, which is often done using active learning (AL).","The usefulness of the constructed potentials is often limited by the high effort required and their insufficient robustness in the simulations.","Here we introduce the end-to-end AL for constructing robust data-efficient potentials with affordable investment of time and resources and minimum human interference.","Our AL protocol is based on the physics-informed sampling of training points, automatic selection of initial data, and uncertainty quantification.","The versatility of this protocol is shown in our implementation of quasi-classical molecular dynamics for simulating vibrational spectra, conformer search of a key biochemical molecule, and time-resolved mechanism of the Diels-Alder reaction.","These investigations took us days instead of weeks of pure quantum chemical calculations on a high-performance computing cluster."],"url":"http://arxiv.org/abs/2404.11811v1","category":"physics.chem-ph"}
{"created":"2024-04-17 23:49:00","title":"TempBEV: Improving Learned BEV Encoders with Combined Image and BEV Space Temporal Aggregation","abstract":"Autonomous driving requires an accurate representation of the environment. A strategy toward high accuracy is to fuse data from several sensors. Learned Bird's-Eye View (BEV) encoders can achieve this by mapping data from individual sensors into one joint latent space. For cost-efficient camera-only systems, this provides an effective mechanism to fuse data from multiple cameras with different views. Accuracy can further be improved by aggregating sensor information over time. This is especially important in monocular camera systems to account for the lack of explicit depth and velocity measurements. Thereby, the effectiveness of developed BEV encoders crucially depends on the operators used to aggregate temporal information and on the used latent representation spaces. We analyze BEV encoders proposed in the literature and compare their effectiveness, quantifying the effects of aggregation operators and latent representations. While most existing approaches aggregate temporal information either in image or in BEV latent space, our analyses and performance comparisons suggest that these latent representations exhibit complementary strengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which integrates aggregated temporal information from both latent spaces. We consider subsequent image frames as stereo through time and leverage methods from optical flow estimation for temporal stereo encoding. Empirical evaluation on the NuScenes dataset shows a significant improvement by TempBEV over the baseline for 3D object detection and BEV segmentation. The ablation uncovers a strong synergy of joint temporal aggregation in the image and BEV latent space. These results indicate the overall effectiveness of our approach and make a strong case for aggregating temporal information in both image and BEV latent spaces.","sentences":["Autonomous driving requires an accurate representation of the environment.","A strategy toward high accuracy is to fuse data from several sensors.","Learned Bird's-Eye View (BEV) encoders can achieve this by mapping data from individual sensors into one joint latent space.","For cost-efficient camera-only systems, this provides an effective mechanism to fuse data from multiple cameras with different views.","Accuracy can further be improved by aggregating sensor information over time.","This is especially important in monocular camera systems to account for the lack of explicit depth and velocity measurements.","Thereby, the effectiveness of developed BEV encoders crucially depends on the operators used to aggregate temporal information and on the used latent representation spaces.","We analyze BEV encoders proposed in the literature and compare their effectiveness, quantifying the effects of aggregation operators and latent representations.","While most existing approaches aggregate temporal information either in image or in BEV latent space, our analyses and performance comparisons suggest that these latent representations exhibit complementary strengths.","Therefore, we develop a novel temporal BEV encoder, TempBEV, which integrates aggregated temporal information from both latent spaces.","We consider subsequent image frames as stereo through time and leverage methods from optical flow estimation for temporal stereo encoding.","Empirical evaluation on the NuScenes dataset shows a significant improvement by TempBEV over the baseline for 3D object detection and BEV segmentation.","The ablation uncovers a strong synergy of joint temporal aggregation in the image and BEV latent space.","These results indicate the overall effectiveness of our approach and make a strong case for aggregating temporal information in both image and BEV latent spaces."],"url":"http://arxiv.org/abs/2404.11803v1","category":"cs.CV"}
{"created":"2024-04-17 23:41:48","title":"Developing Situational Awareness for Joint Action with Autonomous Vehicles","abstract":"Unanswered questions about how human-AV interaction designers can support rider's informational needs hinders Autonomous Vehicles (AV) adoption. To achieve joint human-AV action goals - such as safe transportation, trust, or learning from an AV - sufficient situational awareness must be held by the human, AV, and human-AV system collectively. We present a systems-level framework that integrates cognitive theories of joint action and situational awareness as a means to tailor communications that meet the criteria necessary for goal success. This framework is based on four components of the shared situation: AV traits, action goals, subject-specific traits and states, and the situated driving context. AV communications should be tailored to these factors and be sensitive when they change. This framework can be useful for understanding individual, shared, and distributed human-AV situational awareness and designing for future AV communications that meet the informational needs and goals of diverse groups and in diverse driving contexts.","sentences":["Unanswered questions about how human-AV interaction designers can support rider's informational needs hinders Autonomous Vehicles (AV) adoption.","To achieve joint human-AV action goals - such as safe transportation, trust, or learning from an AV - sufficient situational awareness must be held by the human, AV, and human-AV system collectively.","We present a systems-level framework that integrates cognitive theories of joint action and situational awareness as a means to tailor communications that meet the criteria necessary for goal success.","This framework is based on four components of the shared situation: AV traits, action goals, subject-specific traits and states, and the situated driving context.","AV communications should be tailored to these factors and be sensitive when they change.","This framework can be useful for understanding individual, shared, and distributed human-AV situational awareness and designing for future AV communications that meet the informational needs and goals of diverse groups and in diverse driving contexts."],"url":"http://arxiv.org/abs/2404.11800v1","category":"cs.HC"}
{"created":"2024-04-17 23:30:48","title":"When are Foundation Models Effective? Understanding the Suitability for Pixel-Level Classification Using Multispectral Imagery","abstract":"Foundation models, i.e., very large deep learning models, have demonstrated impressive performances in various language and vision tasks that are otherwise difficult to reach using smaller-size models. The major success of GPT-type of language models is particularly exciting and raises expectations on the potential of foundation models in other domains including satellite remote sensing. In this context, great efforts have been made to build foundation models to test their capabilities in broader applications, and examples include Prithvi by NASA-IBM, Segment-Anything-Model, ViT, etc. This leads to an important question: Are foundation models always a suitable choice for different remote sensing tasks, and when or when not? This work aims to enhance the understanding of the status and suitability of foundation models for pixel-level classification using multispectral imagery at moderate resolution, through comparisons with traditional machine learning (ML) and regular-size deep learning models. Interestingly, the results reveal that in many scenarios traditional ML models still have similar or better performance compared to foundation models, especially for tasks where texture is less useful for classification. On the other hand, deep learning models did show more promising results for tasks where labels partially depend on texture (e.g., burn scar), while the difference in performance between foundation models and deep learning models is not obvious. The results conform with our analysis: The suitability of foundation models depend on the alignment between the self-supervised learning tasks and the real downstream tasks, and the typical masked autoencoder paradigm is not necessarily suitable for many remote sensing problems.","sentences":["Foundation models, i.e., very large deep learning models, have demonstrated impressive performances in various language and vision tasks that are otherwise difficult to reach using smaller-size models.","The major success of GPT-type of language models is particularly exciting and raises expectations on the potential of foundation models in other domains including satellite remote sensing.","In this context, great efforts have been made to build foundation models to test their capabilities in broader applications, and examples include Prithvi by NASA-IBM, Segment-Anything-Model, ViT, etc.","This leads to an important question: Are foundation models always a suitable choice for different remote sensing tasks, and when or when not?","This work aims to enhance the understanding of the status and suitability of foundation models for pixel-level classification using multispectral imagery at moderate resolution, through comparisons with traditional machine learning (ML) and regular-size deep learning models.","Interestingly, the results reveal that in many scenarios traditional ML models still have similar or better performance compared to foundation models, especially for tasks where texture is less useful for classification.","On the other hand, deep learning models did show more promising results for tasks where labels partially depend on texture (e.g., burn scar), while the difference in performance between foundation models and deep learning models is not obvious.","The results conform with our analysis: The suitability of foundation models depend on the alignment between the self-supervised learning tasks and the real downstream tasks, and the typical masked autoencoder paradigm is not necessarily suitable for many remote sensing problems."],"url":"http://arxiv.org/abs/2404.11797v1","category":"cs.CV"}
{"created":"2024-04-17 23:14:46","title":"Symplectic Weiss calculi","abstract":"We provide two candidates for symplectic Weiss calculus based on two different, but closely related, collections of groups. In the case of the non-compact symplectic groups, i.e., automorphism groups of vector spaces with symplectic forms, we show that the calculus deformation retracts onto unitary calculus as a corollary of the fact that Weiss calculus only depends on the homotopy type of the groupoid core of the diagram category. In the case of the compact symplectic groups, i.e., automorphism groups of quaternion vector spaces, we provide a comparison with the other known versions of Weiss calculus analogous to the comparisons of calculi of the second named author, and classify certain stably trivial quaternion vector bundles over finite cell complexes in a range, using elementary results on convergence of Weiss calculi.","sentences":["We provide two candidates for symplectic Weiss calculus based on two different, but closely related, collections of groups.","In the case of the non-compact symplectic groups, i.e., automorphism groups of vector spaces with symplectic forms, we show that the calculus deformation retracts onto unitary calculus as a corollary of the fact that Weiss calculus only depends on the homotopy type of the groupoid core of the diagram category.","In the case of the compact symplectic groups, i.e., automorphism groups of quaternion vector spaces, we provide a comparison with the other known versions of Weiss calculus analogous to the comparisons of calculi of the second named author, and classify certain stably trivial quaternion vector bundles over finite cell complexes in a range, using elementary results on convergence of Weiss calculi."],"url":"http://arxiv.org/abs/2404.11796v1","category":"math.AT"}
{"created":"2024-04-17 23:10:11","title":"Prompt-Driven Feature Diffusion for Open-World Semi-Supervised Learning","abstract":"In this paper, we present a novel approach termed Prompt-Driven Feature Diffusion (PDFD) within a semi-supervised learning framework for Open World Semi-Supervised Learning (OW-SSL). At its core, PDFD deploys an efficient feature-level diffusion model with the guidance of class-specific prompts to support discriminative feature representation learning and feature generation, tackling the challenge of the non-availability of labeled data for unseen classes in OW-SSL. In particular, PDFD utilizes class prototypes as prompts in the diffusion model, leveraging their class-discriminative and semantic generalization ability to condition and guide the diffusion process across all the seen and unseen classes. Furthermore, PDFD incorporates a class-conditional adversarial loss for diffusion model training, ensuring that the features generated via the diffusion process can be discriminatively aligned with the class-conditional features of the real data. Additionally, the class prototypes of the unseen classes are computed using only unlabeled instances with confident predictions within a semi-supervised learning framework. We conduct extensive experiments to evaluate the proposed PDFD. The empirical results show PDFD exhibits remarkable performance enhancements over many state-of-the-art existing methods.","sentences":["In this paper, we present a novel approach termed Prompt-Driven Feature Diffusion (PDFD) within a semi-supervised learning framework for Open World Semi-Supervised Learning (OW-SSL).","At its core, PDFD deploys an efficient feature-level diffusion model with the guidance of class-specific prompts to support discriminative feature representation learning and feature generation, tackling the challenge of the non-availability of labeled data for unseen classes in OW-SSL.","In particular, PDFD utilizes class prototypes as prompts in the diffusion model, leveraging their class-discriminative and semantic generalization ability to condition and guide the diffusion process across all the seen and unseen classes.","Furthermore, PDFD incorporates a class-conditional adversarial loss for diffusion model training, ensuring that the features generated via the diffusion process can be discriminatively aligned with the class-conditional features of the real data.","Additionally, the class prototypes of the unseen classes are computed using only unlabeled instances with confident predictions within a semi-supervised learning framework.","We conduct extensive experiments to evaluate the proposed PDFD.","The empirical results show PDFD exhibits remarkable performance enhancements over many state-of-the-art existing methods."],"url":"http://arxiv.org/abs/2404.11795v1","category":"cs.LG"}
{"created":"2024-04-17 23:00:29","title":"Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key Point Generation and Introducing an Automatic Coverage Evaluation Metric","abstract":"The proliferation of social media platforms has given rise to the amount of online debates and arguments. Consequently, the need for automatic summarization methods for such debates is imperative, however this area of summarization is rather understudied. The Key Point Analysis (KPA) task formulates argument summarization as representing the summary of a large collection of arguments in the form of concise sentences in bullet-style format, called key points. A sub-task of KPA, called Key Point Generation (KPG), focuses on generating these key points given the arguments. This paper introduces a novel extractive approach for key point generation, that outperforms previous state-of-the-art methods for the task. Our method utilizes an extractive clustering based approach that offers concise, high quality generated key points with higher coverage of reference summaries, and less redundant outputs. In addition, we show that the existing evaluation metrics for summarization such as ROUGE are incapable of differentiating between generated key points of different qualities. To this end, we propose a new evaluation metric for assessing the generated key points by their coverage. Our code can be accessed online.","sentences":["The proliferation of social media platforms has given rise to the amount of online debates and arguments.","Consequently, the need for automatic summarization methods for such debates is imperative, however this area of summarization is rather understudied.","The Key Point Analysis (KPA) task formulates argument summarization as representing the summary of a large collection of arguments in the form of concise sentences in bullet-style format, called key points.","A sub-task of KPA, called Key Point Generation (KPG), focuses on generating these key points given the arguments.","This paper introduces a novel extractive approach for key point generation, that outperforms previous state-of-the-art methods for the task.","Our method utilizes an extractive clustering based approach that offers concise, high quality generated key points with higher coverage of reference summaries, and less redundant outputs.","In addition, we show that the existing evaluation metrics for summarization such as ROUGE are incapable of differentiating between generated key points of different qualities.","To this end, we propose a new evaluation metric for assessing the generated key points by their coverage.","Our code can be accessed online."],"url":"http://arxiv.org/abs/2404.11793v1","category":"cs.CL"}
{"created":"2024-04-17 23:00:03","title":"Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study","abstract":"This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models. Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality. We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components. We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning.","sentences":["This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG).","Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models.","Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality.","We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components.","We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning."],"url":"http://arxiv.org/abs/2404.11792v1","category":"cs.AI"}
{"created":"2024-04-17 22:44:45","title":"An Invitation to Resolvent Analysis","abstract":"Resolvent analysis is a powerful tool that can reveal the linear amplification mechanisms between the forcing inputs and the response outputs about a base flow. These mechanisms can be revealed in terms of a pair of forcing and response modes and the associated gains (amplification magnitude) in the order of energy contents at a given frequency. The linear relationship that ties the forcing and the response is represented through the resolvent operator (transfer function), which is constructed through spatially discretizing the linearized Navier-Stokes operator. One of the unique strengths of resolvent analysis is its ability to analyze statistically stationary turbulent flows. In light of the increasing interest in using resolvent analysis to study a variety of flows, we offer this guide in hopes of removing the hurdle for students and researchers to initiate the development of a resolvent analysis code and its applications to their problems of interest. To achieve this goal, we discuss various aspects of resolvent analysis and its role in identifying dominant flow structures about the base flow. The discussion in this paper revolves around the compressible Navier-Stokes equations in the most general manner. We cover essential considerations ranging from selecting the base flow and appropriate energy norms to the intricacies of constructing the linear operator and performing eigenvalue and singular value decompositions. Throughout the paper, we offer details and know-how that may not be available to readers in a collective manner elsewhere. Towards the end of this paper, examples are offered to demonstrate the practical applicability of resolvent analysis, aiming to guide readers through its implementation and inspire further extensions. We invite readers to consider resolvent analysis as a companion for their research endeavors.","sentences":["Resolvent analysis is a powerful tool that can reveal the linear amplification mechanisms between the forcing inputs and the response outputs about a base flow.","These mechanisms can be revealed in terms of a pair of forcing and response modes and the associated gains (amplification magnitude) in the order of energy contents at a given frequency.","The linear relationship that ties the forcing and the response is represented through the resolvent operator (transfer function), which is constructed through spatially discretizing the linearized Navier-Stokes operator.","One of the unique strengths of resolvent analysis is its ability to analyze statistically stationary turbulent flows.","In light of the increasing interest in using resolvent analysis to study a variety of flows, we offer this guide in hopes of removing the hurdle for students and researchers to initiate the development of a resolvent analysis code and its applications to their problems of interest.","To achieve this goal, we discuss various aspects of resolvent analysis and its role in identifying dominant flow structures about the base flow.","The discussion in this paper revolves around the compressible Navier-Stokes equations in the most general manner.","We cover essential considerations ranging from selecting the base flow and appropriate energy norms to the intricacies of constructing the linear operator and performing eigenvalue and singular value decompositions.","Throughout the paper, we offer details and know-how that may not be available to readers in a collective manner elsewhere.","Towards the end of this paper, examples are offered to demonstrate the practical applicability of resolvent analysis, aiming to guide readers through its implementation and inspire further extensions.","We invite readers to consider resolvent analysis as a companion for their research endeavors."],"url":"http://arxiv.org/abs/2404.11789v1","category":"physics.flu-dyn"}
{"created":"2024-04-17 22:38:52","title":"Intelligent mechanical metamaterials towards learning static and dynamic behaviors","abstract":"The exploration of intelligent machines has recently spurred the development of physical neural networks, a class of intelligent metamaterials capable of learning, whether in silico or in situ, from observed data. In this study, we introduce a back-propagation framework for lattice-based mechanical neural networks (MNNs) to achieve prescribed static and dynamic performance. This approach leverages the steady states of nodes for back-propagation, efficiently updating the learning degrees of freedom without prior knowledge of input loading. One-dimensional MNNs, trained with back-propagation in silico, can exhibit the desired behaviors on demand function as intelligent mechanical machines. The framework is then employed for the precise morphing control of the two-dimensional MNNs subjected to different static loads. Moreover, the intelligent MNNs are trained to execute classical machine learning tasks such as regression to tackle various deformation control tasks. Finally, the disordered MNNs are constructed and trained to demonstrate pre-programmed wave bandgap control ability, illustrating the versatility of the proposed approach as a platform for physical learning. Our approach presents an efficient pathway for the design of intelligent mechanical metamaterials for a wide range of static and dynamic target functionalities, positioning them as powerful engines for physical learning.","sentences":["The exploration of intelligent machines has recently spurred the development of physical neural networks, a class of intelligent metamaterials capable of learning, whether in silico or in situ, from observed data.","In this study, we introduce a back-propagation framework for lattice-based mechanical neural networks (MNNs) to achieve prescribed static and dynamic performance.","This approach leverages the steady states of nodes for back-propagation, efficiently updating the learning degrees of freedom without prior knowledge of input loading.","One-dimensional MNNs, trained with back-propagation in silico, can exhibit the desired behaviors on demand function as intelligent mechanical machines.","The framework is then employed for the precise morphing control of the two-dimensional MNNs subjected to different static loads.","Moreover, the intelligent MNNs are trained to execute classical machine learning tasks such as regression to tackle various deformation control tasks.","Finally, the disordered MNNs are constructed and trained to demonstrate pre-programmed wave bandgap control ability, illustrating the versatility of the proposed approach as a platform for physical learning.","Our approach presents an efficient pathway for the design of intelligent mechanical metamaterials for a wide range of static and dynamic target functionalities, positioning them as powerful engines for physical learning."],"url":"http://arxiv.org/abs/2404.11785v1","category":"physics.app-ph"}
{"created":"2024-04-17 22:12:41","title":"REQUAL-LM: Reliability and Equity through Aggregation in Large Language Models","abstract":"The extensive scope of large language models (LLMs) across various domains underscores the critical importance of responsibility in their application, beyond natural language processing. In particular, the randomized nature of LLMs, coupled with inherent biases and historical stereotypes in data, raises critical concerns regarding reliability and equity. Addressing these challenges are necessary before using LLMs for applications with societal impact. Towards addressing this gap, we introduce REQUAL-LM, a novel method for finding reliable and equitable LLM outputs through aggregation. Specifically, we develop a Monte Carlo method based on repeated sampling to find a reliable output close to the mean of the underlying distribution of possible outputs. We formally define the terms such as reliability and bias, and design an equity-aware aggregation to minimize harmful bias while finding a highly reliable output. REQUAL-LM does not require specialized hardware, does not impose a significant computing load, and uses LLMs as a blackbox. This design choice enables seamless scalability alongside the rapid advancement of LLM technologies. Our system does not require retraining the LLMs, which makes it deployment ready and easy to adapt. Our comprehensive experiments using various tasks and datasets demonstrate that REQUAL- LM effectively mitigates bias and selects a more equitable response, specifically the outputs that properly represents minority groups.","sentences":["The extensive scope of large language models (LLMs) across various domains underscores the critical importance of responsibility in their application, beyond natural language processing.","In particular, the randomized nature of LLMs, coupled with inherent biases and historical stereotypes in data, raises critical concerns regarding reliability and equity.","Addressing these challenges are necessary before using LLMs for applications with societal impact.","Towards addressing this gap, we introduce REQUAL-LM, a novel method for finding reliable and equitable LLM outputs through aggregation.","Specifically, we develop a Monte Carlo method based on repeated sampling to find a reliable output close to the mean of the underlying distribution of possible outputs.","We formally define the terms such as reliability and bias, and design an equity-aware aggregation to minimize harmful bias while finding a highly reliable output.","REQUAL-LM does not require specialized hardware, does not impose a significant computing load, and uses LLMs as a blackbox.","This design choice enables seamless scalability alongside the rapid advancement of LLM technologies.","Our system does not require retraining the LLMs, which makes it deployment ready and easy to adapt.","Our comprehensive experiments using various tasks and datasets demonstrate that REQUAL- LM effectively mitigates bias and selects a more equitable response, specifically the outputs that properly represents minority groups."],"url":"http://arxiv.org/abs/2404.11782v1","category":"cs.CL"}
{"created":"2024-04-17 22:09:31","title":"Asymmetric canonical correlation analysis of Riemannian and high-dimensional data","abstract":"In this paper, we introduce a novel statistical model for the integrative analysis of Riemannian-valued functional data and high-dimensional data. We apply this model to explore the dependence structure between each subject's dynamic functional connectivity -- represented by a temporally indexed collection of positive definite covariance matrices -- and high-dimensional data representing lifestyle, demographic, and psychometric measures. Specifically, we employ a reformulation of canonical correlation analysis that enables efficient control of the complexity of the functional canonical directions using tangent space sieve approximations. Additionally, we enforce an interpretable group structure on the high-dimensional canonical directions via a sparsity-promoting penalty. The proposed method shows improved empirical performance over alternative approaches and comes with theoretical guarantees. Its application to data from the Human Connectome Project reveals a dominant mode of covariation between dynamic functional connectivity and lifestyle, demographic, and psychometric measures. This mode aligns with results from static connectivity studies but reveals a unique temporal non-stationary pattern that such studies fail to capture.","sentences":["In this paper, we introduce a novel statistical model for the integrative analysis of Riemannian-valued functional data and high-dimensional data.","We apply this model to explore the dependence structure between each subject's dynamic functional connectivity -- represented by a temporally indexed collection of positive definite covariance matrices -- and high-dimensional data representing lifestyle, demographic, and psychometric measures.","Specifically, we employ a reformulation of canonical correlation analysis that enables efficient control of the complexity of the functional canonical directions using tangent space sieve approximations.","Additionally, we enforce an interpretable group structure on the high-dimensional canonical directions via a sparsity-promoting penalty.","The proposed method shows improved empirical performance over alternative approaches and comes with theoretical guarantees.","Its application to data from the Human Connectome Project reveals a dominant mode of covariation between dynamic functional connectivity and lifestyle, demographic, and psychometric measures.","This mode aligns with results from static connectivity studies but reveals a unique temporal non-stationary pattern that such studies fail to capture."],"url":"http://arxiv.org/abs/2404.11781v1","category":"stat.ME"}
{"created":"2024-04-17 21:56:27","title":"Behavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommendation Systems","abstract":"Large Language Models (LLMs) have demonstrated great potential in Conversational Recommender Systems (CRS). However, the application of LLMs to CRS has exposed a notable discrepancy in behavior between LLM-based CRS and human recommenders: LLMs often appear inflexible and passive, frequently rushing to complete the recommendation task without sufficient inquiry.This behavior discrepancy can lead to decreased accuracy in recommendations and lower user satisfaction. Despite its importance, existing studies in CRS lack a study about how to measure such behavior discrepancy. To fill this gap, we propose Behavior Alignment, a new evaluation metric to measure how well the recommendation strategies made by a LLM-based CRS are consistent with human recommenders'. Our experiment results show that the new metric is better aligned with human preferences and can better differentiate how systems perform than existing evaluation metrics. As Behavior Alignment requires explicit and costly human annotations on the recommendation strategies, we also propose a classification-based method to implicitly measure the Behavior Alignment based on the responses. The evaluation results confirm the robustness of the method.","sentences":["Large Language Models (LLMs) have demonstrated great potential in Conversational Recommender Systems (CRS).","However, the application of LLMs to CRS has exposed a notable discrepancy in behavior between LLM-based CRS and human recommenders: LLMs often appear inflexible and passive, frequently rushing to complete the recommendation task without sufficient inquiry.","This behavior discrepancy can lead to decreased accuracy in recommendations and lower user satisfaction.","Despite its importance, existing studies in CRS lack a study about how to measure such behavior discrepancy.","To fill this gap, we propose Behavior Alignment, a new evaluation metric to measure how well the recommendation strategies made by a LLM-based CRS are consistent with human recommenders'.","Our experiment results show that the new metric is better aligned with human preferences and can better differentiate how systems perform than existing evaluation metrics.","As Behavior Alignment requires explicit and costly human annotations on the recommendation strategies, we also propose a classification-based method to implicitly measure the Behavior Alignment based on the responses.","The evaluation results confirm the robustness of the method."],"url":"http://arxiv.org/abs/2404.11773v1","category":"cs.IR"}
{"created":"2024-04-17 21:54:05","title":"IoT-Driven Cloud-based Energy and Environment Monitoring System for Manufacturing Industry","abstract":"This research focused on the development of a cost-effective IoT solution for energy and environment monitoring geared towards manufacturing industries. The proposed system is developed using open-source software that can be easily deployed in any manufacturing environment. The system collects real-time temperature, humidity, and energy data from different devices running on different communication such as TCP/IP, Modbus, etc., and the data is transferred wirelessly using an MQTT client to a database working as a cloud storage solution. The collected data is then visualized and analyzed using a website running on a host machine working as a web client.","sentences":["This research focused on the development of a cost-effective IoT solution for energy and environment monitoring geared towards manufacturing industries.","The proposed system is developed using open-source software that can be easily deployed in any manufacturing environment.","The system collects real-time temperature, humidity, and energy data from different devices running on different communication such as TCP/IP, Modbus, etc., and the data is transferred wirelessly using an MQTT client to a database working as a cloud storage solution.","The collected data is then visualized and analyzed using a website running on a host machine working as a web client."],"url":"http://arxiv.org/abs/2404.11771v1","category":"eess.SY"}
{"created":"2024-04-17 21:53:01","title":"Event-Based Eye Tracking. AIS 2024 Challenge Survey","abstract":"This survey reviews the AIS 2024 Event-Based Eye Tracking (EET) Challenge. The task of the challenge focuses on processing eye movement recorded with event cameras and predicting the pupil center of the eye. The challenge emphasizes efficient eye tracking with event cameras to achieve good task accuracy and efficiency trade-off. During the challenge period, 38 participants registered for the Kaggle competition, and 8 teams submitted a challenge factsheet. The novel and diverse methods from the submitted factsheets are reviewed and analyzed in this survey to advance future event-based eye tracking research.","sentences":["This survey reviews the AIS 2024 Event-Based Eye Tracking (EET) Challenge.","The task of the challenge focuses on processing eye movement recorded with event cameras and predicting the pupil center of the eye.","The challenge emphasizes efficient eye tracking with event cameras to achieve good task accuracy and efficiency trade-off.","During the challenge period, 38 participants registered for the Kaggle competition, and 8 teams submitted a challenge factsheet.","The novel and diverse methods from the submitted factsheets are reviewed and analyzed in this survey to advance future event-based eye tracking research."],"url":"http://arxiv.org/abs/2404.11770v1","category":"cs.CV"}
{"created":"2024-04-17 21:17:48","title":"Improved Generalization Bounds for Communication Efficient Federated Learning","abstract":"This paper focuses on reducing the communication cost of federated learning by exploring generalization bounds and representation learning. We first characterize a tighter generalization bound for one-round federated learning based on local clients' generalizations and heterogeneity of data distribution (non-iid scenario). We also characterize a generalization bound in R-round federated learning and its relation to the number of local updates (local stochastic gradient descents (SGDs)). Then, based on our generalization bound analysis and our representation learning interpretation of this analysis, we show for the first time that less frequent aggregations, hence more local updates, for the representation extractor (usually corresponds to initial layers) leads to the creation of more generalizable models, particularly for non-iid scenarios. We design a novel Federated Learning with Adaptive Local Steps (FedALS) algorithm based on our generalization bound and representation learning analysis. FedALS employs varying aggregation frequencies for different parts of the model, so reduces the communication cost. The paper is followed with experimental results showing the effectiveness of FedALS.","sentences":["This paper focuses on reducing the communication cost of federated learning by exploring generalization bounds and representation learning.","We first characterize a tighter generalization bound for one-round federated learning based on local clients' generalizations and heterogeneity of data distribution (non-iid scenario).","We also characterize a generalization bound in R-round federated learning and its relation to the number of local updates (local stochastic gradient descents (SGDs)).","Then, based on our generalization bound analysis and our representation learning interpretation of this analysis, we show for the first time that less frequent aggregations, hence more local updates, for the representation extractor (usually corresponds to initial layers) leads to the creation of more generalizable models, particularly for non-iid scenarios.","We design a novel Federated Learning with Adaptive Local Steps (FedALS) algorithm based on our generalization bound and representation learning analysis.","FedALS employs varying aggregation frequencies for different parts of the model, so reduces the communication cost.","The paper is followed with experimental results showing the effectiveness of FedALS."],"url":"http://arxiv.org/abs/2404.11754v1","category":"cs.LG"}
{"created":"2024-04-17 21:09:13","title":"Mapping Violence: Developing an Extensive Framework to Build a Bangla Sectarian Expression Dataset from Social Media Interactions","abstract":"Communal violence in online forums has become extremely prevalent in South Asia, where many communities of different cultures coexist and share resources. These societies exhibit a phenomenon characterized by strong bonds within their own groups and animosity towards others, leading to conflicts that frequently escalate into violent confrontations. To address this issue, we have developed the first comprehensive framework for the automatic detection of communal violence markers in online Bangla content accompanying the largest collection (13K raw sentences) of social media interactions that fall under the definition of four major violence class and their 16 coarse expressions. Our workflow introduces a 7-step expert annotation process incorporating insights from social scientists, linguists, and psychologists. By presenting data statistics and benchmarking performance using this dataset, we have determined that, aside from the category of Non-communal violence, Religio-communal violence is particularly pervasive in Bangla text. Moreover, we have substantiated the effectiveness of fine-tuning language models in identifying violent comments by conducting preliminary benchmarking on the state-of-the-art Bangla deep learning model.","sentences":["Communal violence in online forums has become extremely prevalent in South Asia, where many communities of different cultures coexist and share resources.","These societies exhibit a phenomenon characterized by strong bonds within their own groups and animosity towards others, leading to conflicts that frequently escalate into violent confrontations.","To address this issue, we have developed the first comprehensive framework for the automatic detection of communal violence markers in online Bangla content accompanying the largest collection (13K raw sentences) of social media interactions that fall under the definition of four major violence class and their 16 coarse expressions.","Our workflow introduces a 7-step expert annotation process incorporating insights from social scientists, linguists, and psychologists.","By presenting data statistics and benchmarking performance using this dataset, we have determined that, aside from the category of Non-communal violence, Religio-communal violence is particularly pervasive in Bangla text.","Moreover, we have substantiated the effectiveness of fine-tuning language models in identifying violent comments by conducting preliminary benchmarking on the state-of-the-art Bangla deep learning model."],"url":"http://arxiv.org/abs/2404.11752v1","category":"cs.CL"}
{"created":"2024-04-17 20:51:13","title":"Incremental Bootstrapping and Classification of Structured Scenes in a Fuzzy Ontology","abstract":"We foresee robots that bootstrap knowledge representations and use them for classifying relevant situations and making decisions based on future observations. Particularly for assistive robots, the bootstrapping mechanism might be supervised by humans who should not repeat a training phase several times and should be able to refine the taught representation. We consider robots that bootstrap structured representations to classify some intelligible categories. Such a structure should be incrementally bootstrapped, i.e., without invalidating the identified category models when a new additional category is considered. To tackle this scenario, we presented the Scene Identification and Tagging (SIT) algorithm, which bootstraps structured knowledge representation in a crisp OWL-DL ontology. Over time, SIT bootstraps a graph representing scenes, sub-scenes and similar scenes. Then, SIT can classify new scenes within the bootstrapped graph through logic-based reasoning. However, SIT has issues with sensory data because its crisp implementation is not robust to perception noises. This paper presents a reformulation of SIT within the fuzzy domain, which exploits a fuzzy DL ontology to overcome the robustness issues. By comparing the performances of fuzzy and crisp implementations of SIT, we show that fuzzy SIT is robust, preserves the properties of its crisp formulation, and enhances the bootstrapped representations. On the contrary, the fuzzy implementation of SIT leads to less intelligible knowledge representations than the one bootstrapped in the crisp domain.","sentences":["We foresee robots that bootstrap knowledge representations and use them for classifying relevant situations and making decisions based on future observations.","Particularly for assistive robots, the bootstrapping mechanism might be supervised by humans who should not repeat a training phase several times and should be able to refine the taught representation.","We consider robots that bootstrap structured representations to classify some intelligible categories.","Such a structure should be incrementally bootstrapped, i.e., without invalidating the identified category models when a new additional category is considered.","To tackle this scenario, we presented the Scene Identification and Tagging (SIT) algorithm, which bootstraps structured knowledge representation in a crisp OWL-DL ontology.","Over time, SIT bootstraps a graph representing scenes, sub-scenes and similar scenes.","Then, SIT can classify new scenes within the bootstrapped graph through logic-based reasoning.","However, SIT has issues with sensory data because its crisp implementation is not robust to perception noises.","This paper presents a reformulation of SIT within the fuzzy domain, which exploits a fuzzy DL ontology to overcome the robustness issues.","By comparing the performances of fuzzy and crisp implementations of SIT, we show that fuzzy SIT is robust, preserves the properties of its crisp formulation, and enhances the bootstrapped representations.","On the contrary, the fuzzy implementation of SIT leads to less intelligible knowledge representations than the one bootstrapped in the crisp domain."],"url":"http://arxiv.org/abs/2404.11744v1","category":"cs.AI"}
{"created":"2024-04-17 20:50:28","title":"Meta-Decomposition: Dynamic Segmentation Approach Selection in IoT-based Activity Recognition","abstract":"Internet of Things (IoT) devices generate heterogeneous data over time; and relying solely on individual data points is inadequate for accurate analysis.   Segmentation is a common preprocessing step in many IoT applications, including IoT-based activity recognition, aiming to address the limitations of individual events and streamline the process. However, this step introduces at least two families of uncontrollable biases. The first is caused by the changes made by the segmentation process on the initial problem space, such as dividing the input data into 60 seconds windows. The second category of biases results from the segmentation process itself, including the fixation of the segmentation method and its parameters.   To address these biases, we propose to redefine the segmentation problem as a special case of a decomposition problem, including three key components: a decomposer, resolutions, and a composer.   The inclusion of the composer task in the segmentation process facilitates an assessment of the relationship between the original problem and the problem after the segmentation. Therefore, It leads to an improvement in the evaluation process and, consequently, in the selection of the appropriate segmentation method.   Then, we formally introduce our novel meta-decomposition or learning-to-decompose approach. It reduces the segmentation biases by considering the segmentation as a hyperparameter to be optimized by the outer learning problem. Therefore, meta-decomposition improves the overall system performance by dynamically selecting the appropriate segmentation method without including the mentioned biases. Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposal.","sentences":["Internet of Things (IoT) devices generate heterogeneous data over time; and relying solely on individual data points is inadequate for accurate analysis.   ","Segmentation is a common preprocessing step in many IoT applications, including IoT-based activity recognition, aiming to address the limitations of individual events and streamline the process.","However, this step introduces at least two families of uncontrollable biases.","The first is caused by the changes made by the segmentation process on the initial problem space, such as dividing the input data into 60 seconds windows.","The second category of biases results from the segmentation process itself, including the fixation of the segmentation method and its parameters.   ","To address these biases, we propose to redefine the segmentation problem as a special case of a decomposition problem, including three key components: a decomposer, resolutions, and a composer.   ","The inclusion of the composer task in the segmentation process facilitates an assessment of the relationship between the original problem and the problem after the segmentation.","Therefore, It leads to an improvement in the evaluation process and, consequently, in the selection of the appropriate segmentation method.   ","Then, we formally introduce our novel meta-decomposition or learning-to-decompose approach.","It reduces the segmentation biases by considering the segmentation as a hyperparameter to be optimized by the outer learning problem.","Therefore, meta-decomposition improves the overall system performance by dynamically selecting the appropriate segmentation method without including the mentioned biases.","Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposal."],"url":"http://arxiv.org/abs/2404.11742v1","category":"cs.AI"}
{"created":"2024-04-17 20:48:15","title":"Simulating Cloud Environments of Connected Vehicles for Anomaly Detection","abstract":"The emergence of connected vehicles is driven by increasing customer and regulatory demands. To meet these, more complex software applications, some of which require service-based cloud and edge backends, are developed. When new software is deployed however, the high complexity and interdependencies between components can lead to unforeseen side effects in other system parts. As such, it becomes more challenging to recognize whether deviations to the intended system behavior are occurring, ultimately resulting in higher monitoring efforts and slower responses to errors. To overcome this problem, a simulation of the cloud environment running in parallel to the system is proposed. This approach enables the live comparison between simulated and real cloud behavior. Therefore, a concept is developed mirroring the existing cloud system into a simulation. To collect the necessary data, an observability platform is presented, capturing telemetry and architecture information. Subsequently, a simulation environment is designed that converts the architecture into a simulation model and simulates its dynamic workload by utilizing captured communication data. The proposed concept is evaluated in a real-world application scenario for electric vehicle charging: Vehicles can apply for an unoccupied charging station at a cloud service backend, the latter which manages all incoming requests and performs the assignment. Benchmarks are conducted by comparing the collected telemetry data with the simulated results under different loads and injected faults. The results show that regular cloud behavior is mirrored well by the simulation and that misbehavior due to fault injection is well visible, indicating that simulations are a promising data source for anomaly detection in connected vehicle cloud environments during operation.","sentences":["The emergence of connected vehicles is driven by increasing customer and regulatory demands.","To meet these, more complex software applications, some of which require service-based cloud and edge backends, are developed.","When new software is deployed however, the high complexity and interdependencies between components can lead to unforeseen side effects in other system parts.","As such, it becomes more challenging to recognize whether deviations to the intended system behavior are occurring, ultimately resulting in higher monitoring efforts and slower responses to errors.","To overcome this problem, a simulation of the cloud environment running in parallel to the system is proposed.","This approach enables the live comparison between simulated and real cloud behavior.","Therefore, a concept is developed mirroring the existing cloud system into a simulation.","To collect the necessary data, an observability platform is presented, capturing telemetry and architecture information.","Subsequently, a simulation environment is designed that converts the architecture into a simulation model and simulates its dynamic workload by utilizing captured communication data.","The proposed concept is evaluated in a real-world application scenario for electric vehicle charging: Vehicles can apply for an unoccupied charging station at a cloud service backend, the latter which manages all incoming requests and performs the assignment.","Benchmarks are conducted by comparing the collected telemetry data with the simulated results under different loads and injected faults.","The results show that regular cloud behavior is mirrored well by the simulation and that misbehavior due to fault injection is well visible, indicating that simulations are a promising data source for anomaly detection in connected vehicle cloud environments during operation."],"url":"http://arxiv.org/abs/2404.11740v1","category":"cs.ET"}
{"created":"2024-04-17 20:31:05","title":"Missed Connections: Lateral Thinking Puzzles for Large Language Models","abstract":"The Connections puzzle published each day by the New York Times tasks players with dividing a bank of sixteen words into four groups of four words that each relate to a common theme. Solving the puzzle requires both common linguistic knowledge (i.e. definitions and typical usage) as well as, in many cases, lateral or abstract thinking. This is because the four categories ascend in complexity, with the most challenging category often requiring thinking about words in uncommon ways or as parts of larger phrases. We investigate the capacity for automated AI systems to play Connections and explore the game's potential as an automated benchmark for abstract reasoning and a way to measure the semantic information encoded by data-driven linguistic systems. In particular, we study both a sentence-embedding baseline and modern large language models (LLMs). We report their accuracy on the task, measure the impacts of chain-of-thought prompting, and discuss their failure modes. Overall, we find that the Connections task is challenging yet feasible, and a strong test-bed for future work.","sentences":["The Connections puzzle published each day by the New York Times tasks players with dividing a bank of sixteen words into four groups of four words that each relate to a common theme.","Solving the puzzle requires both common linguistic knowledge (i.e. definitions and typical usage) as well as, in many cases, lateral or abstract thinking.","This is because the four categories ascend in complexity, with the most challenging category often requiring thinking about words in uncommon ways or as parts of larger phrases.","We investigate the capacity for automated AI systems to play Connections and explore the game's potential as an automated benchmark for abstract reasoning and a way to measure the semantic information encoded by data-driven linguistic systems.","In particular, we study both a sentence-embedding baseline and modern large language models (LLMs).","We report their accuracy on the task, measure the impacts of chain-of-thought prompting, and discuss their failure modes.","Overall, we find that the Connections task is challenging yet feasible, and a strong test-bed for future work."],"url":"http://arxiv.org/abs/2404.11730v1","category":"cs.CL"}
{"created":"2024-04-17 20:28:15","title":"Deep Learning for Video-Based Assessment of Endotracheal Intubation Skills","abstract":"Endotracheal intubation (ETI) is an emergency procedure performed in civilian and combat casualty care settings to establish an airway. Objective and automated assessment of ETI skills is essential for the training and certification of healthcare providers. However, the current approach is based on manual feedback by an expert, which is subjective, time- and resource-intensive, and is prone to poor inter-rater reliability and halo effects. This work proposes a framework to evaluate ETI skills using single and multi-view videos. The framework consists of two stages. First, a 2D convolutional autoencoder (AE) and a pre-trained self-supervision network extract features from videos. Second, a 1D convolutional enhanced with a cross-view attention module takes the features from the AE as input and outputs predictions for skill evaluation. The ETI datasets were collected in two phases. In the first phase, ETI is performed by two subject cohorts: Experts and Novices. In the second phase, novice subjects perform ETI under time pressure, and the outcome is either Successful or Unsuccessful. A third dataset of videos from a single head-mounted camera for Experts and Novices is also analyzed. The study achieved an accuracy of 100% in identifying Expert/Novice trials in the initial phase. In the second phase, the model showed 85% accuracy in classifying Successful/Unsuccessful procedures. Using head-mounted cameras alone, the model showed a 96% accuracy on Expert and Novice classification while maintaining an accuracy of 85% on classifying successful and unsuccessful. In addition, GradCAMs are presented to explain the differences between Expert and Novice behavior and Successful and Unsuccessful trials. The approach offers a reliable and objective method for automated assessment of ETI skills.","sentences":["Endotracheal intubation (ETI) is an emergency procedure performed in civilian and combat casualty care settings to establish an airway.","Objective and automated assessment of ETI skills is essential for the training and certification of healthcare providers.","However, the current approach is based on manual feedback by an expert, which is subjective, time- and resource-intensive, and is prone to poor inter-rater reliability and halo effects.","This work proposes a framework to evaluate ETI skills using single and multi-view videos.","The framework consists of two stages.","First, a 2D convolutional autoencoder (AE) and a pre-trained self-supervision network extract features from videos.","Second, a 1D convolutional enhanced with a cross-view attention module takes the features from the AE as input and outputs predictions for skill evaluation.","The ETI datasets were collected in two phases.","In the first phase, ETI is performed by two subject cohorts: Experts and Novices.","In the second phase, novice subjects perform ETI under time pressure, and the outcome is either Successful or Unsuccessful.","A third dataset of videos from a single head-mounted camera for Experts and Novices is also analyzed.","The study achieved an accuracy of 100% in identifying Expert/Novice trials in the initial phase.","In the second phase, the model showed 85% accuracy in classifying Successful/Unsuccessful procedures.","Using head-mounted cameras alone, the model showed a 96% accuracy on Expert and Novice classification while maintaining an accuracy of 85% on classifying successful and unsuccessful.","In addition, GradCAMs are presented to explain the differences between Expert and Novice behavior and Successful and Unsuccessful trials.","The approach offers a reliable and objective method for automated assessment of ETI skills."],"url":"http://arxiv.org/abs/2404.11727v1","category":"cs.CV"}
{"created":"2024-04-17 20:13:37","title":"GEOBIND: Binding Text, Image, and Audio through Satellite Images","abstract":"In remote sensing, we are interested in modeling various modalities for some geographic location. Several works have focused on learning the relationship between a location and type of landscape, habitability, audio, textual descriptions, etc. Recently, a common way to approach these problems is to train a deep-learning model that uses satellite images to infer some unique characteristics of the location. In this work, we present a deep-learning model, GeoBind, that can infer about multiple modalities, specifically text, image, and audio, from satellite imagery of a location. To do this, we use satellite images as the binding element and contrastively align all other modalities to the satellite image data. Our training results in a joint embedding space with multiple types of data: satellite image, ground-level image, audio, and text. Furthermore, our approach does not require a single complex dataset that contains all the modalities mentioned above. Rather it only requires multiple satellite-image paired data. While we only align three modalities in this paper, we present a general framework that can be used to create an embedding space with any number of modalities by using satellite images as the binding element. Our results show that, unlike traditional unimodal models, GeoBind is versatile and can reason about multiple modalities for a given satellite image input.","sentences":["In remote sensing, we are interested in modeling various modalities for some geographic location.","Several works have focused on learning the relationship between a location and type of landscape, habitability, audio, textual descriptions, etc.","Recently, a common way to approach these problems is to train a deep-learning model that uses satellite images to infer some unique characteristics of the location.","In this work, we present a deep-learning model, GeoBind, that can infer about multiple modalities, specifically text, image, and audio, from satellite imagery of a location.","To do this, we use satellite images as the binding element and contrastively align all other modalities to the satellite image data.","Our training results in a joint embedding space with multiple types of data: satellite image, ground-level image, audio, and text.","Furthermore, our approach does not require a single complex dataset that contains all the modalities mentioned above.","Rather it only requires multiple satellite-image paired data.","While we only align three modalities in this paper, we present a general framework that can be used to create an embedding space with any number of modalities by using satellite images as the binding element.","Our results show that, unlike traditional unimodal models, GeoBind is versatile and can reason about multiple modalities for a given satellite image input."],"url":"http://arxiv.org/abs/2404.11720v1","category":"cs.AI"}
{"created":"2024-04-17 20:12:45","title":"Detecting gravitational wave signals using a flexible model for the amplitude and frequency evolution","abstract":"We currently lack good waveform models for many gravitational wave sources. Examples where models are lacking include neutron star post merger signals, core collapse supernovae, and signals of unknown origin. Wavelet based techniques have proven effective at detecting and characterizing these signals. Here we introduce a new method that uses collections of evolving amplitude-frequency tracks, or \"voices\", to model generic gravitational wave signals. The analysis is implemented using trans-dimensional Bayesian inference, building on the earlier wavelet-based BayesWave algorithm. The new algorithm, BayesWaveVoices, outperforms the original for long duration signals.","sentences":["We currently lack good waveform models for many gravitational wave sources.","Examples where models are lacking include neutron star post merger signals, core collapse supernovae, and signals of unknown origin.","Wavelet based techniques have proven effective at detecting and characterizing these signals.","Here we introduce a new method that uses collections of evolving amplitude-frequency tracks, or \"voices\", to model generic gravitational wave signals.","The analysis is implemented using trans-dimensional Bayesian inference, building on the earlier wavelet-based BayesWave algorithm.","The new algorithm, BayesWaveVoices, outperforms the original for long duration signals."],"url":"http://arxiv.org/abs/2404.11719v1","category":"gr-qc"}
{"created":"2024-04-17 20:11:32","title":"How often are errors in natural language reasoning due to paraphrastic variability?","abstract":"Large language models have been shown to behave inconsistently in response to meaning-preserving paraphrastic inputs. At the same time, researchers evaluate the knowledge and reasoning abilities of these models with test evaluations that do not disaggregate the effect of paraphrastic variability on performance. We propose a metric for evaluating the paraphrastic consistency of natural language reasoning models based on the probability of a model achieving the same correctness on two paraphrases of the same problem. We mathematically connect this metric to the proportion of a model's variance in correctness attributable to paraphrasing. To estimate paraphrastic consistency, we collect ParaNLU, a dataset of 7,782 human-written and validated paraphrased reasoning problems constructed on top of existing benchmark datasets for defeasible and abductive natural language inference. Using ParaNLU, we measure the paraphrastic consistency of several model classes and show that consistency dramatically increases with pretraining but not finetuning. All models tested exhibited room for improvement in paraphrastic consistency.","sentences":["Large language models have been shown to behave inconsistently in response to meaning-preserving paraphrastic inputs.","At the same time, researchers evaluate the knowledge and reasoning abilities of these models with test evaluations that do not disaggregate the effect of paraphrastic variability on performance.","We propose a metric for evaluating the paraphrastic consistency of natural language reasoning models based on the probability of a model achieving the same correctness on two paraphrases of the same problem.","We mathematically connect this metric to the proportion of a model's variance in correctness attributable to paraphrasing.","To estimate paraphrastic consistency, we collect ParaNLU, a dataset of 7,782 human-written and validated paraphrased reasoning problems constructed on top of existing benchmark datasets for defeasible and abductive natural language inference.","Using ParaNLU, we measure the paraphrastic consistency of several model classes and show that consistency dramatically increases with pretraining but not finetuning.","All models tested exhibited room for improvement in paraphrastic consistency."],"url":"http://arxiv.org/abs/2404.11717v1","category":"cs.CL"}
{"created":"2024-04-17 20:10:43","title":"A Survey on Semantic Modeling for Building Energy Management","abstract":"Buildings account for a substantial portion of global energy consumption. Reducing buildings' energy usage primarily involves obtaining data from building systems and environment, which are instrumental in assessing and optimizing the building's performance. However, as devices from various manufacturers represent their data in unique ways, this disparity introduces challenges for semantic interoperability and creates obstacles in developing scalable building applications. This survey explores the leading semantic modeling techniques deployed for energy management in buildings. Furthermore, it aims to offer tangible use cases for applying semantic models, shedding light on the pivotal concepts and limitations intrinsic to each model. Our findings will assist researchers in discerning the appropriate circumstances and methodologies for employing these models in various use cases.","sentences":["Buildings account for a substantial portion of global energy consumption.","Reducing buildings' energy usage primarily involves obtaining data from building systems and environment, which are instrumental in assessing and optimizing the building's performance.","However, as devices from various manufacturers represent their data in unique ways, this disparity introduces challenges for semantic interoperability and creates obstacles in developing scalable building applications.","This survey explores the leading semantic modeling techniques deployed for energy management in buildings.","Furthermore, it aims to offer tangible use cases for applying semantic models, shedding light on the pivotal concepts and limitations intrinsic to each model.","Our findings will assist researchers in discerning the appropriate circumstances and methodologies for employing these models in various use cases."],"url":"http://arxiv.org/abs/2404.11716v1","category":"cs.AI"}
{"created":"2024-04-17 19:55:58","title":"Implementation and Evaluation of a Gradient Descent-Trained Defensible Blackboard Architecture System","abstract":"A variety of forms of artificial intelligence systems have been developed. Two well-known techniques are neural networks and rule-fact expert systems. The former can be trained from presented data while the latter is typically developed by human domain experts. A combined implementation that uses gradient descent to train a rule-fact expert system has been previously proposed. A related system type, the Blackboard Architecture, adds an actualization capability to expert systems. This paper proposes and evaluates the incorporation of a defensible-style gradient descent training capability into the Blackboard Architecture. It also introduces the use of activation functions for defensible artificial intelligence systems and implements and evaluates a new best path-based training algorithm.","sentences":["A variety of forms of artificial intelligence systems have been developed.","Two well-known techniques are neural networks and rule-fact expert systems.","The former can be trained from presented data while the latter is typically developed by human domain experts.","A combined implementation that uses gradient descent to train a rule-fact expert system has been previously proposed.","A related system type, the Blackboard Architecture, adds an actualization capability to expert systems.","This paper proposes and evaluates the incorporation of a defensible-style gradient descent training capability into the Blackboard Architecture.","It also introduces the use of activation functions for defensible artificial intelligence systems and implements and evaluates a new best path-based training algorithm."],"url":"http://arxiv.org/abs/2404.11714v1","category":"cs.AI"}
{"created":"2024-04-17 19:16:32","title":"Pretraining Billion-scale Geospatial Foundational Models on Frontier","abstract":"As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases. On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning. Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators. Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature. Such geospatial data poses unique challenges opening up new opportunities to develop FMs. We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data. We studied from end-to-end the performance and impact in the solution by scaling the model size. Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model. Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library. Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters. By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications.","sentences":["As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases.","On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning.","Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators.","Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature.","Such geospatial data poses unique challenges opening up new opportunities to develop FMs.","We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data.","We studied from end-to-end the performance and impact in the solution by scaling the model size.","Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model.","Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library.","Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters.","By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications."],"url":"http://arxiv.org/abs/2404.11706v1","category":"cs.AI"}
{"created":"2024-04-17 18:55:41","title":"A Secure and Trustworthy Network Architecture for Federated Learning Healthcare Applications","abstract":"Federated Learning (FL) has emerged as a promising approach for privacy-preserving machine learning, particularly in sensitive domains such as healthcare. In this context, the TRUSTroke project aims to leverage FL to assist clinicians in ischemic stroke prediction. This paper provides an overview of the TRUSTroke FL network infrastructure. The proposed architecture adopts a client-server model with a central Parameter Server (PS). We introduce a Docker-based design for the client nodes, offering a flexible solution for implementing FL processes in clinical settings. The impact of different communication protocols (HTTP or MQTT) on FL network operation is analyzed, with MQTT selected for its suitability in FL scenarios. A control plane to support the main operations required by FL processes is also proposed. The paper concludes with an analysis of security aspects of the FL architecture, addressing potential threats and proposing mitigation strategies to increase the trustworthiness level.","sentences":["Federated Learning (FL) has emerged as a promising approach for privacy-preserving machine learning, particularly in sensitive domains such as healthcare.","In this context, the TRUSTroke project aims to leverage FL to assist clinicians in ischemic stroke prediction.","This paper provides an overview of the TRUSTroke FL network infrastructure.","The proposed architecture adopts a client-server model with a central Parameter Server (PS).","We introduce a Docker-based design for the client nodes, offering a flexible solution for implementing FL processes in clinical settings.","The impact of different communication protocols (HTTP or MQTT) on FL network operation is analyzed, with MQTT selected for its suitability in FL scenarios.","A control plane to support the main operations required by FL processes is also proposed.","The paper concludes with an analysis of security aspects of the FL architecture, addressing potential threats and proposing mitigation strategies to increase the trustworthiness level."],"url":"http://arxiv.org/abs/2404.11698v1","category":"cs.AI"}
{"created":"2024-04-17 18:49:52","title":"Graviton topology","abstract":"Over the past three decades, it has been shown that discrete and continuous media can support topologically nontrivial modes. Recently, it was shown that the same is true of the vacuum, namely, right (R) and left (L) circularly polarized photons are topologically nontrivial. Here, we study the topology of another class of massless particles, namely the gravitons. Working in the transverse-traceless gauge and the limit of weak gravity, we show that the collection of all gravitons forms a rank-two vector bundle over the lightcone. We prove the graviton bundle is topologically trivial, allowing us to discover a globally smooth basis for gravitons. It has often been assumed that there exists such a global basis consisting of linear polarized gravitons. We prove that this stronger assumption is false--the graviton bundle has no linearly polarized subbundles. While the total graviton bundle can be decomposed into trivial line bundles, it also breaks apart into two nontrivial $\\mathrm{SO}(3)$ invariant subbundles, consisting of the R and L gravitons. Unlike the bundles in the trivial decomposition, the R and L gravitons are in fact irreducible bundle representations of the Poincar\\'{e} group, and are thus elementary particles. The nontrivial topologies of the R and L gravitons are fully characterized by the Chern numbers $\\mp 4$. These topologies differ from those of the R and L photons, which are characterized by the Chern numbers $\\mp 2$. This nontrivial topology obstructs the splitting of graviton angular momentum into spin and orbital angular momentum.","sentences":["Over the past three decades, it has been shown that discrete and continuous media can support topologically nontrivial modes.","Recently, it was shown that the same is true of the vacuum, namely, right (R) and left (L) circularly polarized photons are topologically nontrivial.","Here, we study the topology of another class of massless particles, namely the gravitons.","Working in the transverse-traceless gauge and the limit of weak gravity, we show that the collection of all gravitons forms a rank-two vector bundle over the lightcone.","We prove the graviton bundle is topologically trivial, allowing us to discover a globally smooth basis for gravitons.","It has often been assumed that there exists such a global basis consisting of linear polarized gravitons.","We prove that this stronger assumption is false--the graviton bundle has no linearly polarized subbundles.","While the total graviton bundle can be decomposed into trivial line bundles, it also breaks apart into two nontrivial $\\mathrm{SO}(3)$ invariant subbundles, consisting of the R and L gravitons.","Unlike the bundles in the trivial decomposition, the R and L gravitons are in fact irreducible bundle representations of the Poincar\\'{e} group, and are thus elementary particles.","The nontrivial topologies of the R and L gravitons are fully characterized by the Chern numbers $\\mp 4$.","These topologies differ from those of the R and L photons, which are characterized by the Chern numbers $\\mp 2$.","This nontrivial topology obstructs the splitting of graviton angular momentum into spin and orbital angular momentum."],"url":"http://arxiv.org/abs/2404.11696v1","category":"math-ph"}
{"created":"2024-04-17 18:17:50","title":"Cross-Problem Learning for Solving Vehicle Routing Problems","abstract":"Existing neural heuristics often train a deep architecture from scratch for each specific vehicle routing problem (VRP), ignoring the transferable knowledge across different VRP variants. This paper proposes the cross-problem learning to assist heuristics training for different downstream VRP variants. Particularly, we modularize neural architectures for complex VRPs into 1) the backbone Transformer for tackling the travelling salesman problem (TSP), and 2) the additional lightweight modules for processing problem-specific features in complex VRPs. Accordingly, we propose to pre-train the backbone Transformer for TSP, and then apply it in the process of fine-tuning the Transformer models for each target VRP variant. On the one hand, we fully fine-tune the trained backbone Transformer and problem-specific modules simultaneously. On the other hand, we only fine-tune small adapter networks along with the modules, keeping the backbone Transformer still. Extensive experiments on typical VRPs substantiate that 1) the full fine-tuning achieves significantly better performance than the one trained from scratch, and 2) the adapter-based fine-tuning also delivers comparable performance while being notably parameter-efficient. Furthermore, we empirically demonstrate the favorable effect of our method in terms of cross-distribution application and versatility.","sentences":["Existing neural heuristics often train a deep architecture from scratch for each specific vehicle routing problem (VRP), ignoring the transferable knowledge across different VRP variants.","This paper proposes the cross-problem learning to assist heuristics training for different downstream VRP variants.","Particularly, we modularize neural architectures for complex VRPs into 1) the backbone Transformer for tackling the travelling salesman problem (TSP), and 2) the additional lightweight modules for processing problem-specific features in complex VRPs.","Accordingly, we propose to pre-train the backbone Transformer for TSP, and then apply it in the process of fine-tuning the Transformer models for each target VRP variant.","On the one hand, we fully fine-tune the trained backbone Transformer and problem-specific modules simultaneously.","On the other hand, we only fine-tune small adapter networks along with the modules, keeping the backbone Transformer still.","Extensive experiments on typical VRPs substantiate that 1) the full fine-tuning achieves significantly better performance than the one trained from scratch, and 2) the adapter-based fine-tuning also delivers comparable performance while being notably parameter-efficient.","Furthermore, we empirically demonstrate the favorable effect of our method in terms of cross-distribution application and versatility."],"url":"http://arxiv.org/abs/2404.11677v1","category":"cs.AI"}
{"created":"2024-04-17 18:12:05","title":"A Study of Undefined Behavior Across Foreign Function Boundaries in Rust Libraries","abstract":"The Rust programming language restricts aliasing and mutability to provide static safety guarantees, which developers rely on to write secure and performant applications. However, Rust is frequently used to interoperate with other languages that have far weaker restrictions. These languages support cyclic and self-referential design patterns that conflict with current models of Rust's operational semantics, representing a potentially significant source of undefined behavior that no current tools can detect. We created MiriLLI, a tool which uses existing Rust and LLVM interpreters to jointly execute multi-language Rust applications. We used our tool in a large-scale study of Rust libraries that call foreign functions, and we found 45 instances of undefined or undesirable behavior. These include four bugs from libraries that had over 10,000 daily downloads on average, one from a component of the GNU Compiler Collection (GCC), and one from a library maintained by the Rust Project. Most of these errors were caused by incompatible aliasing and initialization patterns, incorrect foreign function bindings, and invalid type conversion. The majority of aliasing violations were caused by unsound operations in Rust, but they occurred in foreign code. The Rust community must invest in new tools for validating multi-language programs to ensure that developers can easily detect and fix these errors.","sentences":["The Rust programming language restricts aliasing and mutability to provide static safety guarantees, which developers rely on to write secure and performant applications.","However, Rust is frequently used to interoperate with other languages that have far weaker restrictions.","These languages support cyclic and self-referential design patterns that conflict with current models of Rust's operational semantics, representing a potentially significant source of undefined behavior that no current tools can detect.","We created MiriLLI, a tool which uses existing Rust and LLVM interpreters to jointly execute multi-language Rust applications.","We used our tool in a large-scale study of Rust libraries that call foreign functions, and we found 45 instances of undefined or undesirable behavior.","These include four bugs from libraries that had over 10,000 daily downloads on average, one from a component of the GNU Compiler Collection (GCC), and one from a library maintained by the Rust Project.","Most of these errors were caused by incompatible aliasing and initialization patterns, incorrect foreign function bindings, and invalid type conversion.","The majority of aliasing violations were caused by unsound operations in Rust, but they occurred in foreign code.","The Rust community must invest in new tools for validating multi-language programs to ensure that developers can easily detect and fix these errors."],"url":"http://arxiv.org/abs/2404.11671v1","category":"cs.SE"}
{"created":"2024-04-17 18:04:37","title":"Deep Dependency Networks and Advanced Inference Schemes for Multi-Label Classification","abstract":"We present a unified framework called deep dependency networks (DDNs) that combines dependency networks and deep learning architectures for multi-label classification, with a particular emphasis on image and video data. The primary advantage of dependency networks is their ease of training, in contrast to other probabilistic graphical models like Markov networks. In particular, when combined with deep learning architectures, they provide an intuitive, easy-to-use loss function for multi-label classification. A drawback of DDNs compared to Markov networks is their lack of advanced inference schemes, necessitating the use of Gibbs sampling. To address this challenge, we propose novel inference schemes based on local search and integer linear programming for computing the most likely assignment to the labels given observations. We evaluate our novel methods on three video datasets (Charades, TACoS, Wetlab) and three image datasets (MS-COCO, PASCAL VOC, NUS-WIDE), comparing their performance with (a) basic neural architectures and (b) neural architectures combined with Markov networks equipped with advanced inference and learning techniques. Our results demonstrate the superiority of our new DDN methods over the two competing approaches.","sentences":["We present a unified framework called deep dependency networks (DDNs) that combines dependency networks and deep learning architectures for multi-label classification, with a particular emphasis on image and video data.","The primary advantage of dependency networks is their ease of training, in contrast to other probabilistic graphical models like Markov networks.","In particular, when combined with deep learning architectures, they provide an intuitive, easy-to-use loss function for multi-label classification.","A drawback of DDNs compared to Markov networks is their lack of advanced inference schemes, necessitating the use of Gibbs sampling.","To address this challenge, we propose novel inference schemes based on local search and integer linear programming for computing the most likely assignment to the labels given observations.","We evaluate our novel methods on three video datasets (Charades, TACoS, Wetlab) and three image datasets (MS-COCO, PASCAL VOC, NUS-WIDE), comparing their performance with (a) basic neural architectures and (b) neural architectures combined with Markov networks equipped with advanced inference and learning techniques.","Our results demonstrate the superiority of our new DDN methods over the two competing approaches."],"url":"http://arxiv.org/abs/2404.11667v1","category":"cs.LG"}
{"created":"2024-04-17 18:00:46","title":"Designing an Intelligent Parcel Management System using IoT & Machine Learning","abstract":"Parcels delivery is a critical activity in railways. More importantly, each parcel must be thoroughly checked and sorted according to its destination address. We require an efficient and robust IoT system capable of doing all of these tasks with great precision and minimal human interaction. This paper discusses, We created a fully-fledged solution using IoT and machine learning to assist trains in performing this operation efficiently. In this study, we covered the product, which consists mostly of two phases. Scanning is the first step, followed by sorting. During the scanning process, the parcel will be passed through three scanners that will look for explosives, drugs, and any dangerous materials in the parcel and will trash it if any of the tests fail. When the scanning step is over, the parcel moves on to the sorting phase, where we use QR codes to retrieve the details of the parcels and sort them properly. The simulation of the system is done using the blender software. Our research shows that our procedure significantly improves accuracy as well as the assessment of cutting-edge technology and existing techniques.","sentences":["Parcels delivery is a critical activity in railways.","More importantly, each parcel must be thoroughly checked and sorted according to its destination address.","We require an efficient and robust IoT system capable of doing all of these tasks with great precision and minimal human interaction.","This paper discusses, We created a fully-fledged solution using IoT and machine learning to assist trains in performing this operation efficiently.","In this study, we covered the product, which consists mostly of two phases.","Scanning is the first step, followed by sorting.","During the scanning process, the parcel will be passed through three scanners that will look for explosives, drugs, and any dangerous materials in the parcel and will trash it if any of the tests fail.","When the scanning step is over, the parcel moves on to the sorting phase, where we use QR codes to retrieve the details of the parcels and sort them properly.","The simulation of the system is done using the blender software.","Our research shows that our procedure significantly improves accuracy as well as the assessment of cutting-edge technology and existing techniques."],"url":"http://arxiv.org/abs/2404.11661v1","category":"cs.CY"}
{"created":"2024-04-17 18:00:06","title":"The impact of stellar bars on quenching star formation: insights from a spatially resolved analysis in the local Universe","abstract":"Stellar bars are common morphological structures in the local Universe: according to optical and NIR surveys, they are present in about two-thirds of disc galaxies. These elongated structures are also believed to play a crucial role in secular evolutionary processes, since they are able to efficiently redistribute gas, stars and angular momentum within their hosts, although it remains unclear whether they enhance or suppress star formation. A useful tool to investigate such an ambiguity is the Main Sequence (MS) relation, which tightly links stellar mass ($M_{\\star}$) and star formation rate (SFR). The main goal of this work is to explore star formation processes in barred galaxies, in order to assess the relevance of possible bar quenching effects on the typical log-linear trend of the resolved MS. To this purpose, we carry out a spatially resolved analysis on sub-kpc scales for a sample of six nearby barred galaxies. Multi-wavelength photometric data (from far-UV to far-IR) are collected from the DustPedia database and a panchromatic Spectral Energy Distribution (SED) fitting procedure is applied on square apertures of fixed angular size (8\" $\\times$ 8\"), making use of the magphys code. For each galaxy we obtain the distributions of stellar mass and star formation rate surface densities and relate them in the $\\log \\Sigma_{\\star}$ - $\\log \\Sigma_{\\rm SFR}$ plane deriving the spatially resolved MS relation. Although significant galaxy-to-galaxy variations are in place, we infer the presence of a common anti-correlation track in correspondence with the bar-hosting region, which shows systematically lower values of SFR. Such a central quiescent signature can be interpreted as the result of a bar-driven depletion of gas reservoirs and a consequent halt of star formation. This seems to point in the direction of an inside-out quenching scenario.","sentences":["Stellar bars are common morphological structures in the local Universe: according to optical and NIR surveys, they are present in about two-thirds of disc galaxies.","These elongated structures are also believed to play a crucial role in secular evolutionary processes, since they are able to efficiently redistribute gas, stars and angular momentum within their hosts, although it remains unclear whether they enhance or suppress star formation.","A useful tool to investigate such an ambiguity is the Main Sequence (MS) relation, which tightly links stellar mass ($M_{\\star}$) and star formation rate (SFR).","The main goal of this work is to explore star formation processes in barred galaxies, in order to assess the relevance of possible bar quenching effects on the typical log-linear trend of the resolved MS.","To this purpose, we carry out a spatially resolved analysis on sub-kpc scales for a sample of six nearby barred galaxies.","Multi-wavelength photometric data (from far-UV to far-IR) are collected from the DustPedia database and a panchromatic Spectral Energy Distribution (SED) fitting procedure is applied on square apertures of fixed angular size (8\" $\\times$ 8\"), making use of the magphys code.","For each galaxy we obtain the distributions of stellar mass and star formation rate surface densities and relate them in the $\\log \\Sigma_{\\star}$ - $\\log \\Sigma_{\\rm SFR}$ plane deriving the spatially resolved MS relation.","Although significant galaxy-to-galaxy variations are in place, we infer the presence of a common anti-correlation track in correspondence with the bar-hosting region, which shows systematically lower values of SFR.","Such a central quiescent signature can be interpreted as the result of a bar-driven depletion of gas reservoirs and a consequent halt of star formation.","This seems to point in the direction of an inside-out quenching scenario."],"url":"http://arxiv.org/abs/2404.11656v1","category":"astro-ph.GA"}
{"created":"2024-04-18 17:59:53","title":"Moving Object Segmentation: All You Need Is SAM (and Flow)","abstract":"The objective of this paper is motion segmentation -- discovering and segmenting the moving objects in a video. This is a much studied area with numerous careful,and sometimes complex, approaches and training schemes including: self-supervised learning, learning from synthetic datasets, object-centric representations, amodal representations, and many more. Our interest in this paper is to determine if the Segment Anything model (SAM) can contribute to this task. We investigate two models for combining SAM with optical flow that harness the segmentation power of SAM with the ability of flow to discover and group moving objects. In the first model, we adapt SAM to take optical flow, rather than RGB, as an input. In the second, SAM takes RGB as an input, and flow is used as a segmentation prompt. These surprisingly simple methods, without any further modifications, outperform all previous approaches by a considerable margin in both single and multi-object benchmarks. We also extend these frame-level segmentations to sequence-level segmentations that maintain object identity. Again, this simple model outperforms previous methods on multiple video object segmentation benchmarks.","sentences":["The objective of this paper is motion segmentation -- discovering and segmenting the moving objects in a video.","This is a much studied area with numerous careful,and sometimes complex, approaches and training schemes including: self-supervised learning, learning from synthetic datasets, object-centric representations, amodal representations, and many more.","Our interest in this paper is to determine if the Segment Anything model (SAM) can contribute to this task.","We investigate two models for combining SAM with optical flow that harness the segmentation power of SAM with the ability of flow to discover and group moving objects.","In the first model, we adapt SAM to take optical flow, rather than RGB, as an input.","In the second, SAM takes RGB as an input, and flow is used as a segmentation prompt.","These surprisingly simple methods, without any further modifications, outperform all previous approaches by a considerable margin in both single and multi-object benchmarks.","We also extend these frame-level segmentations to sequence-level segmentations that maintain object identity.","Again, this simple model outperforms previous methods on multiple video object segmentation benchmarks."],"url":"http://arxiv.org/abs/2404.12389v1","category":"cs.CV"}
{"created":"2024-04-18 17:57:53","title":"Matching the Statistical Query Lower Bound for k-sparse Parity Problems with Stochastic Gradient Descent","abstract":"The $k$-parity problem is a classical problem in computational complexity and algorithmic theory, serving as a key benchmark for understanding computational classes. In this paper, we solve the $k$-parity problem with stochastic gradient descent (SGD) on two-layer fully-connected neural networks. We demonstrate that SGD can efficiently solve the $k$-sparse parity problem on a $d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of $\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, thus matching the established $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Our theoretical analysis begins by constructing a good neural network capable of correctly solving the $k$-parity problem. We then demonstrate how a trained neural network with SGD can effectively approximate this good network, solving the $k$-parity problem with small statistical errors. Our theoretical results and findings are supported by empirical evidence, showcasing the efficiency and efficacy of our approach.","sentences":["The $k$-parity problem is a classical problem in computational complexity and algorithmic theory, serving as a key benchmark for understanding computational classes.","In this paper, we solve the $k$-parity problem with stochastic gradient descent (SGD) on two-layer fully-connected neural networks.","We demonstrate that SGD can efficiently solve the $k$-sparse parity problem on a $d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of $\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, thus matching the established $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models.","Our theoretical analysis begins by constructing a good neural network capable of correctly solving the $k$-parity problem.","We then demonstrate how a trained neural network with SGD can effectively approximate this good network, solving the $k$-parity problem with small statistical errors.","Our theoretical results and findings are supported by empirical evidence, showcasing the efficiency and efficacy of our approach."],"url":"http://arxiv.org/abs/2404.12376v1","category":"cs.LG"}
{"created":"2024-04-18 17:52:14","title":"Long-lived oscillations of false and true vacuum states in neutral atom systems","abstract":"Metastable false vacuum states arise in a range of quantum systems and can be observed in various dynamical scenarios, including decay, bubble nucleation, and long-lived oscillations. False vacuum phenomenology has been examined in quantum many-body systems, notably in 1D ferromagnetic Ising spin systems and superfluids. In this paper, we study long-lived oscillations of false and true vacuum states in 1D antiferromagnetic neutral atom chains with long-range Rydberg interactions. We use a staggered local detuning field to achieve confinement. Using theoretical and numerical models, we identify novel spectral signatures of quasiparticle oscillations distinct to antiferromagnetic neutral atom systems and interpret them using a classical energy model of deconfinement from Rydberg tails. Finally, we evaluate the experimental accessibility of our proposed setup on current neutral-atom platforms and discuss experimental feasibility and constraints.","sentences":["Metastable false vacuum states arise in a range of quantum systems and can be observed in various dynamical scenarios, including decay, bubble nucleation, and long-lived oscillations.","False vacuum phenomenology has been examined in quantum many-body systems, notably in 1D ferromagnetic Ising spin systems and superfluids.","In this paper, we study long-lived oscillations of false and true vacuum states in 1D antiferromagnetic neutral atom chains with long-range Rydberg interactions.","We use a staggered local detuning field to achieve confinement.","Using theoretical and numerical models, we identify novel spectral signatures of quasiparticle oscillations distinct to antiferromagnetic neutral atom systems and interpret them using a classical energy model of deconfinement from Rydberg tails.","Finally, we evaluate the experimental accessibility of our proposed setup on current neutral-atom platforms and discuss experimental feasibility and constraints."],"url":"http://arxiv.org/abs/2404.12371v1","category":"quant-ph"}
{"created":"2024-04-18 17:49:02","title":"Accounting for AI and Users Shaping One Another: The Role of Mathematical Models","abstract":"As AI systems enter into a growing number of societal domains, these systems increasingly shape and are shaped by user preferences, opinions, and behaviors. However, the design of AI systems rarely accounts for how AI and users shape one another. In this position paper, we argue for the development of formal interaction models which mathematically specify how AI and users shape one another. Formal interaction models can be leveraged to (1) specify interactions for implementation, (2) monitor interactions through empirical analysis, (3) anticipate societal impacts via counterfactual analysis, and (4) control societal impacts via interventions. The design space of formal interaction models is vast, and model design requires careful consideration of factors such as style, granularity, mathematical complexity, and measurability. Using content recommender systems as a case study, we critically examine the nascent literature of formal interaction models with respect to these use-cases and design axes. More broadly, we call for the community to leverage formal interaction models when designing, evaluating, or auditing any AI system which interacts with users.","sentences":["As AI systems enter into a growing number of societal domains, these systems increasingly shape and are shaped by user preferences, opinions, and behaviors.","However, the design of AI systems rarely accounts for how AI and users shape one another.","In this position paper, we argue for the development of formal interaction models which mathematically specify how AI and users shape one another.","Formal interaction models can be leveraged to (1) specify interactions for implementation, (2) monitor interactions through empirical analysis, (3) anticipate societal impacts via counterfactual analysis, and (4) control societal impacts via interventions.","The design space of formal interaction models is vast, and model design requires careful consideration of factors such as style, granularity, mathematical complexity, and measurability.","Using content recommender systems as a case study, we critically examine the nascent literature of formal interaction models with respect to these use-cases and design axes.","More broadly, we call for the community to leverage formal interaction models when designing, evaluating, or auditing any AI system which interacts with users."],"url":"http://arxiv.org/abs/2404.12366v1","category":"cs.LG"}
{"created":"2024-04-18 17:45:19","title":"Transformer tricks: Removing weights for skipless transformers","abstract":"He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights. However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention). The latter schemes are used by many popular LLMs such as Llama 2, Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes mathematically equivalent versions that are suitable for MQA and GQA. For example, removing Q and P from a skipless version of Mistral-7B would remove 15% of its weights (and thus reduce its compute and memory complexity). See arXiv:2402.13388 and https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.","sentences":["He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights.","However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention).","The latter schemes are used by many popular LLMs such as Llama 2, Mistral, Mixtral, PaLM, and Gemma.","Therefore, this micro-paper proposes mathematically equivalent versions that are suitable for MQA and GQA.","For example, removing Q and P from a skipless version of Mistral-7B would remove 15% of its weights (and thus reduce its compute and memory complexity).","See arXiv:2402.13388","and https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks."],"url":"http://arxiv.org/abs/2404.12362v1","category":"cs.LG"}
{"created":"2024-04-18 17:28:32","title":"Fully non-linear elliptic equations on complex manifolds","abstract":"In this paper, we study a broad class of fully nonlinear elliptic equations on Hermitian manifolds. On one hand, under the optimal structural assumptions we derive $C^{2,\\alpha}$-estimate for solutions of the equations on closed Hermitian manifolds. On the other hand, we treat the Dirichlet problem. In both cases, we prove the existence theorems with unbounded condition.","sentences":["In this paper, we study a broad class of fully nonlinear elliptic equations on Hermitian manifolds.","On one hand, under the optimal structural assumptions we derive $C^{2,\\alpha}$-estimate for solutions of the equations on closed Hermitian manifolds.","On the other hand, we treat the Dirichlet problem.","In both cases, we prove the existence theorems with unbounded condition."],"url":"http://arxiv.org/abs/2404.12350v1","category":"math.AP"}
{"created":"2024-04-18 17:01:34","title":"Normative Requirements Operationalization with Large Language Models","abstract":"Normative non-functional requirements specify constraints that a system must observe in order to avoid violations of social, legal, ethical, empathetic, and cultural norms. As these requirements are typically defined by non-technical system stakeholders with different expertise and priorities (ethicists, lawyers, social scientists, etc.), ensuring their well-formedness and consistency is very challenging. Recent research has tackled this challenge using a domain-specific language to specify normative requirements as rules whose consistency can then be analysed with formal methods. In this paper, we propose a complementary approach that uses Large Language Models to extract semantic relationships between abstract representations of system capabilities. These relations, which are often assumed implicitly by non-technical stakeholders (e.g., based on common sense or domain knowledge), are then used to enrich the automated reasoning techniques for eliciting and analyzing the consistency of normative requirements. We show the effectiveness of our approach to normative requirements elicitation and operationalization through a range of real-world case studies.","sentences":["Normative non-functional requirements specify constraints that a system must observe in order to avoid violations of social, legal, ethical, empathetic, and cultural norms.","As these requirements are typically defined by non-technical system stakeholders with different expertise and priorities (ethicists, lawyers, social scientists, etc.), ensuring their well-formedness and consistency is very challenging.","Recent research has tackled this challenge using a domain-specific language to specify normative requirements as rules whose consistency can then be analysed with formal methods.","In this paper, we propose a complementary approach that uses Large Language Models to extract semantic relationships between abstract representations of system capabilities.","These relations, which are often assumed implicitly by non-technical stakeholders (e.g., based on common sense or domain knowledge), are then used to enrich the automated reasoning techniques for eliciting and analyzing the consistency of normative requirements.","We show the effectiveness of our approach to normative requirements elicitation and operationalization through a range of real-world case studies."],"url":"http://arxiv.org/abs/2404.12335v1","category":"cs.SE"}
{"created":"2024-04-18 16:58:21","title":"On the roles of stellar rotation and binarity in NGC 2423's main-sequence turnoff region","abstract":"Research has shown that many young and intermediate-age clusters (younger than $\\sim$2 Gyr) have extended main sequences and main-sequence turnoffs (eMSTOs), which cannot be adequately described by a single isochrone. The reason for the extended main sequences is now known, with the most probable cause being the fast rotation of stars. However, a significant fraction of slowly rotating stars form a younger stellar population than their fast-rotating counterparts, leading to speculation that they have undergone thorough rotational mixing processes internally. One speculation is that a considerable number of slowly rotating stars reside in close binary systems, where tidal forces from companion stars are the cause of their rotational deceleration. In this work, we report a relatively old open star cluster in the Milky Way, NGC 2423 ($\\sim$1 Gyrs old), which exhibits an apparent eMSTO. As anticipated, many characteristics of NGC 2423 indicate that its eMSTO is driven by stellar rotations. Our calculations indicate that if slowly rotating stars commonly have a close companion star, they should exhibit significant differences in radial velocities observationally, and binary systems that can be tidally locked within the age of NGC 2423 should have a mass ratio close to 1. However, none of these predictions align with our observations. Interestingly, among the only two equal-mass binary systems in the observed region for which spectroscopic data could be obtained, we discovered that one of them is a tidally locked binary system. This further suggests the validity of our numerical simulation results.","sentences":["Research has shown that many young and intermediate-age clusters (younger than $\\sim$2 Gyr) have extended main sequences and main-sequence turnoffs (eMSTOs), which cannot be adequately described by a single isochrone.","The reason for the extended main sequences is now known, with the most probable cause being the fast rotation of stars.","However, a significant fraction of slowly rotating stars form a younger stellar population than their fast-rotating counterparts, leading to speculation that they have undergone thorough rotational mixing processes internally.","One speculation is that a considerable number of slowly rotating stars reside in close binary systems, where tidal forces from companion stars are the cause of their rotational deceleration.","In this work, we report a relatively old open star cluster in the Milky Way, NGC 2423 ($\\sim$1 Gyrs old), which exhibits an apparent eMSTO.","As anticipated, many characteristics of NGC 2423 indicate that its eMSTO is driven by stellar rotations.","Our calculations indicate that if slowly rotating stars commonly have a close companion star, they should exhibit significant differences in radial velocities observationally, and binary systems that can be tidally locked within the age of NGC 2423 should have a mass ratio close to 1.","However, none of these predictions align with our observations.","Interestingly, among the only two equal-mass binary systems in the observed region for which spectroscopic data could be obtained, we discovered that one of them is a tidally locked binary system.","This further suggests the validity of our numerical simulation results."],"url":"http://arxiv.org/abs/2404.12331v1","category":"astro-ph.SR"}
{"created":"2024-04-18 16:56:07","title":"Practical Considerations for Discrete-Time Implementations of Continuous-Time Control Barrier Function-Based Safety Filters","abstract":"Safety filters based on control barrier functions (CBFs) have become a popular method to guarantee safety for uncertified control policies, e.g., as resulting from reinforcement learning. Here, safety is defined as staying in a pre-defined set, the safe set, that adheres to the system's state constraints, e.g., as given by lane boundaries for a self-driving vehicle. In this paper, we examine one commonly overlooked problem that arises in practical implementations of continuous-time CBF-based safety filters. In particular, we look at the issues caused by discrete-time implementations of the continuous-time CBF-based safety filter, especially for cases where the magnitude of the Lie derivative of the CBF with respect to the control input is zero or close to zero. When overlooked, this filter can result in undesirable chattering effects or constraint violations. In this work, we propose three mitigation strategies that allow us to use a continuous-time safety filter in a discrete-time implementation with a local relative degree. Using these strategies in augmented CBF-based safety filters, we achieve safety for all states in the safe set by either using an additional penalty term in the safety filtering objective or modifying the CBF such that those undesired states are not encountered during closed-loop operation. We demonstrate the presented issue and validate our three proposed mitigation strategies in simulation and on a real-world quadrotor.","sentences":["Safety filters based on control barrier functions (CBFs) have become a popular method to guarantee safety for uncertified control policies, e.g., as resulting from reinforcement learning.","Here, safety is defined as staying in a pre-defined set, the safe set, that adheres to the system's state constraints, e.g., as given by lane boundaries for a self-driving vehicle.","In this paper, we examine one commonly overlooked problem that arises in practical implementations of continuous-time CBF-based safety filters.","In particular, we look at the issues caused by discrete-time implementations of the continuous-time CBF-based safety filter, especially for cases where the magnitude of the Lie derivative of the CBF with respect to the control input is zero or close to zero.","When overlooked, this filter can result in undesirable chattering effects or constraint violations.","In this work, we propose three mitigation strategies that allow us to use a continuous-time safety filter in a discrete-time implementation with a local relative degree.","Using these strategies in augmented CBF-based safety filters, we achieve safety for all states in the safe set by either using an additional penalty term in the safety filtering objective or modifying the CBF such that those undesired states are not encountered during closed-loop operation.","We demonstrate the presented issue and validate our three proposed mitigation strategies in simulation and on a real-world quadrotor."],"url":"http://arxiv.org/abs/2404.12329v1","category":"eess.SY"}
{"created":"2024-04-18 16:53:03","title":"Area laws and thermalization from classical entropies in a Bose-Einstein condensate","abstract":"The scaling of local quantum entropies is of utmost interest for characterizing quantum fields, many-body systems, and gravity. Despite their importance, theoretically and experimentally accessing quantum entropies is challenging as they are nonlinear functionals of the underlying quantum state. Here, we show that suitably chosen classical entropies capture the very same features as their quantum analogs for an experimentally relevant setting. We describe the post-quench dynamics of a multi-well spin-1 Bose-Einstein condensate from an initial product state via measurement distributions of spin observables and estimate the corresponding entropies using the asymptotically unbiased k-nearest neighbor method. We observe the dynamical build-up of quantum correlations signaled by an area law, as well as local thermalization revealed by a transition to a volume law, both in regimes characterized by non-Gaussian distributions. We emphasize that all relevant features can be observed at small sample numbers without assuming a specific functional form of the distributions, rendering our method directly applicable to a large variety of models and experimental platforms.","sentences":["The scaling of local quantum entropies is of utmost interest for characterizing quantum fields, many-body systems, and gravity.","Despite their importance, theoretically and experimentally accessing quantum entropies is challenging as they are nonlinear functionals of the underlying quantum state.","Here, we show that suitably chosen classical entropies capture the very same features as their quantum analogs for an experimentally relevant setting.","We describe the post-quench dynamics of a multi-well spin-1","Bose-Einstein condensate from an initial product state via measurement distributions of spin observables and estimate the corresponding entropies using the asymptotically unbiased k-nearest neighbor method.","We observe the dynamical build-up of quantum correlations signaled by an area law, as well as local thermalization revealed by a transition to a volume law, both in regimes characterized by non-Gaussian distributions.","We emphasize that all relevant features can be observed at small sample numbers without assuming a specific functional form of the distributions, rendering our method directly applicable to a large variety of models and experimental platforms."],"url":"http://arxiv.org/abs/2404.12321v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-18 16:52:56","title":"Area laws from classical entropies","abstract":"The area law-like scaling of local quantum entropies is the central characteristic of the entanglement inherent in quantum fields, many-body systems, and spacetime. Whilst the area law is primarily associated with the entanglement structure of the underlying quantum state, we here show that it equally manifests in classical entropies over measurement distributions when vacuum contributions dictated by the uncertainty principle are subtracted. Using the examples of the Gaussian ground and thermal states, but also the non-Gaussian particle state of a relativistic scalar field, we present analytical and numerical area laws for the entropies of various distributions and unveil how quantities of widespread interest such as the central charge and the (local) temperature are encoded in classical observables. With our approach, quantum entropies are no longer necessary to probe quantum phenomena, thereby rendering area laws and other quantum features directly accessible to theoretical models of high complexity as well as state-of-the-art experiments.","sentences":["The area law-like scaling of local quantum entropies is the central characteristic of the entanglement inherent in quantum fields, many-body systems, and spacetime.","Whilst the area law is primarily associated with the entanglement structure of the underlying quantum state, we here show that it equally manifests in classical entropies over measurement distributions when vacuum contributions dictated by the uncertainty principle are subtracted.","Using the examples of the Gaussian ground and thermal states, but also the non-Gaussian particle state of a relativistic scalar field, we present analytical and numerical area laws for the entropies of various distributions and unveil how quantities of widespread interest such as the central charge and the (local) temperature are encoded in classical observables.","With our approach, quantum entropies are no longer necessary to probe quantum phenomena, thereby rendering area laws and other quantum features directly accessible to theoretical models of high complexity as well as state-of-the-art experiments."],"url":"http://arxiv.org/abs/2404.12320v1","category":"quant-ph"}
{"created":"2024-04-18 16:51:12","title":"Adjoint Sensitivities of Chaotic Flows without Adjoint Solvers: A Data-Driven Approach","abstract":"In one calculation, adjoint sensitivity analysis provides the gradient of a quantity of interest with respect to all system's parameters. Conventionally, adjoint solvers need to be implemented by differentiating computational models, which can be a cumbersome task and is code-specific. To propose an adjoint solver that is not code-specific, we develop a data-driven strategy. We demonstrate its application on the computation of gradients of long-time averages of chaotic flows. First, we deploy a parameter-aware echo state network (ESN) to accurately forecast and simulate the dynamics of a dynamical system for a range of system's parameters. Second, we derive the adjoint of the parameter-aware ESN. Finally, we combine the parameter-aware ESN with its adjoint version to compute the sensitivities to the system parameters. We showcase the method on a prototypical chaotic system. Because adjoint sensitivities in chaotic regimes diverge for long integration times, we analyse the application of ensemble adjoint method to the ESN. We find that the adjoint sensitivities obtained from the ESN match closely with the original system. This work opens possibilities for sensitivity analysis without code-specific adjoint solvers.","sentences":["In one calculation, adjoint sensitivity analysis provides the gradient of a quantity of interest with respect to all system's parameters.","Conventionally, adjoint solvers need to be implemented by differentiating computational models, which can be a cumbersome task and is code-specific.","To propose an adjoint solver that is not code-specific, we develop a data-driven strategy.","We demonstrate its application on the computation of gradients of long-time averages of chaotic flows.","First, we deploy a parameter-aware echo state network (ESN) to accurately forecast and simulate the dynamics of a dynamical system for a range of system's parameters.","Second, we derive the adjoint of the parameter-aware ESN.","Finally, we combine the parameter-aware ESN with its adjoint version to compute the sensitivities to the system parameters.","We showcase the method on a prototypical chaotic system.","Because adjoint sensitivities in chaotic regimes diverge for long integration times, we analyse the application of ensemble adjoint method to the ESN.","We find that the adjoint sensitivities obtained from the ESN match closely with the original system.","This work opens possibilities for sensitivity analysis without code-specific adjoint solvers."],"url":"http://arxiv.org/abs/2404.12315v1","category":"cs.LG"}
{"created":"2024-04-18 16:35:18","title":"Investigation of Spin-Pumping and -Transport in the Ni80Fe20/Pt/Co Asymmetric Trilayer","abstract":"FM1/NM/FM2 trilayers have garnered considerable attention because of their potential in spintronic applications. A thorough investigation of the spin transport properties of these trilayers is therefore important. Asymmetric trilayers, particularly those including Platinum (Pt) as a spacer are less explored. Pt mediates exchange coupling between the two FM layers and thus offers a unique platform to investigate the spin-transport properties under indirect exchange coupling conditions through the spin-pumping mechanism. Our analytical focus on the acoustic mode of the ferromagnetic resonance spectrum, facilitated by the distinct magnetizations of the Ni80Fe20 and Co layers, allows for the isolation of individual layer resonances. The derived spin-pumping induced damping of the Ni80Fe20 and Co layers reveals a direct dependence on the Pt spacer thickness. Furthermore, fitting of the weighted average of the damping parameters to the spin-pumping induced damping of acoustic mode reveals that the observed FMR spectra is indeed a result of the in-phase precession of the magnetizations in two FM layers. The extracted effective spin-mixing conductance varies with the FM/NM interface, specifically 1.72x10^19 m^(-2) at the Ni80Fe20/Pt and 4.07x10^19 m^(-2) at the Co/Pt interface, indicating a strong correlation with interfacial characteristics. Additionally, we deduce the spin diffusion length in Pt to be between 1.02 and 1.55 nm and calculate the interfacial spin transparency and spin current densities, highlighting significant disparities between the Ni80Fe20/Pt and Co/Pt interfaces. This detailed analysis enhances our understanding of spin transport in Ni80Fe20/Pt/Co trilayers. It offers insights important for advancing spintronic device design and lays the groundwork for future theoretical investigations of trilayer system.","sentences":["FM1/NM/FM2 trilayers have garnered considerable attention because of their potential in spintronic applications.","A thorough investigation of the spin transport properties of these trilayers is therefore important.","Asymmetric trilayers, particularly those including Platinum (Pt) as a spacer are less explored.","Pt mediates exchange coupling between the two FM layers and thus offers a unique platform to investigate the spin-transport properties under indirect exchange coupling conditions through the spin-pumping mechanism.","Our analytical focus on the acoustic mode of the ferromagnetic resonance spectrum, facilitated by the distinct magnetizations of the Ni80Fe20 and Co layers, allows for the isolation of individual layer resonances.","The derived spin-pumping induced damping of the Ni80Fe20 and Co layers reveals a direct dependence on the Pt spacer thickness.","Furthermore, fitting of the weighted average of the damping parameters to the spin-pumping induced damping of acoustic mode reveals that the observed FMR spectra is indeed a result of the in-phase precession of the magnetizations in two FM layers.","The extracted effective spin-mixing conductance varies with the FM/NM interface, specifically 1.72x10^19 m^(-2) at the Ni80Fe20/Pt and 4.07x10^19 m^(-2) at the Co/Pt interface, indicating a strong correlation with interfacial characteristics.","Additionally, we deduce the spin diffusion length in Pt to be between 1.02 and 1.55 nm and calculate the interfacial spin transparency and spin current densities, highlighting significant disparities between the Ni80Fe20/Pt and Co/Pt interfaces.","This detailed analysis enhances our understanding of spin transport in Ni80Fe20/Pt/Co trilayers.","It offers insights important for advancing spintronic device design and lays the groundwork for future theoretical investigations of trilayer system."],"url":"http://arxiv.org/abs/2404.12307v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 16:32:44","title":"SAFLA: Semantic-aware Full Lifecycle Assurance Designed for Intent-Driven Networks","abstract":"Intent-driven Networks (IDNs) are crucial in enhancing network management efficiency by enabling the translation of high-level intents into executable configurations via a top-down approach. The escalating complexity of network architectures, however, has led to a semantic gap between these intents and their actual configurations, posing significant challenges to the accuracy and reliability of IDNs. While existing methodologies attempt to address this gap through a bottom-up analysis of network metadata, they often fall short, focusing primarily on intent extraction or reasoning without fully leveraging insights to tackle the inherent challenges of IDNs. To mitigate this, we introduce SAFLA, a semantic-aware framework specifically designed to assure the full lifecycle of intents within IDNs. By seamlessly integrating top-down and bottom-up approaches, SAFLA not only provides comprehensive intent assurance but also effectively bridges the semantic gap. This integration facilitates a self-healing mechanism, substantially reducing the need for manual intervention even in dynamically changing network environments. Experimental results demonstrate the framework's feasibility and efficiency, confirming its capacity to quickly adapt intents in response to network changes, thus marking an important advancement in the field of IDNs.","sentences":["Intent-driven Networks (IDNs) are crucial in enhancing network management efficiency by enabling the translation of high-level intents into executable configurations via a top-down approach.","The escalating complexity of network architectures, however, has led to a semantic gap between these intents and their actual configurations, posing significant challenges to the accuracy and reliability of IDNs.","While existing methodologies attempt to address this gap through a bottom-up analysis of network metadata, they often fall short, focusing primarily on intent extraction or reasoning without fully leveraging insights to tackle the inherent challenges of IDNs.","To mitigate this, we introduce SAFLA, a semantic-aware framework specifically designed to assure the full lifecycle of intents within IDNs.","By seamlessly integrating top-down and bottom-up approaches, SAFLA not only provides comprehensive intent assurance but also effectively bridges the semantic gap.","This integration facilitates a self-healing mechanism, substantially reducing the need for manual intervention even in dynamically changing network environments.","Experimental results demonstrate the framework's feasibility and efficiency, confirming its capacity to quickly adapt intents in response to network changes, thus marking an important advancement in the field of IDNs."],"url":"http://arxiv.org/abs/2404.12305v1","category":"cs.NI"}
{"created":"2024-04-18 16:23:53","title":"Dirac Spectral Density in $N_f = 2+1$ QCD at $T = 230$ MeV","abstract":"We compute the renormalized Dirac spectral density in $N_f = 2+1$ QCD at physical quark masses, temperature $T = 230$ MeV and system size $L_s = 3.4$ fm. To that end, we perform a point-wise continuum limit of the staggered density in lattice QCD with staggered quarks. We find, for the first time, that a clear infrared structure (IR peak) emerges in the density of Dirac operator describing dynamical quarks. Features of this structure are consistent with those previously attributed to the recently-proposed IR phase of thermal QCD. Our results (i) provide solid evidence that these IR features are stable and physical; (ii) improve the upper bound for IR-phase transition temperature $T_{\\mathrm{IR}}$ so that the new window is $200 < T_{\\mathrm{IR}} < 230\\,$MeV; (iii) support non-restoration of anomalous U$_{\\mathrm A}$(1) symmetry (chiral limit) below $T \\!=\\! 230\\,$MeV.","sentences":["We compute the renormalized Dirac spectral density in $N_f = 2+1$ QCD at physical quark masses, temperature $T = 230$ MeV and system size $L_s = 3.4$ fm.","To that end, we perform a point-wise continuum limit of the staggered density in lattice QCD with staggered quarks.","We find, for the first time, that a clear infrared structure (IR peak) emerges in the density of Dirac operator describing dynamical quarks.","Features of this structure are consistent with those previously attributed to the recently-proposed IR phase of thermal QCD.","Our results (i) provide solid evidence that these IR features are stable and physical; (ii) improve the upper bound for IR-phase transition temperature $T_{\\mathrm{IR}}$ so that the new window is $200 < T_{\\mathrm{IR}} < 230\\,$MeV; (iii) support non-restoration of anomalous U$_{\\mathrm A}$(1) symmetry (chiral limit) below $T \\!=\\! 230\\,$MeV."],"url":"http://arxiv.org/abs/2404.12298v1","category":"hep-lat"}
{"created":"2024-04-18 16:22:45","title":"Extended unitarity and absence of skin effect in periodically driven systems","abstract":"One of the most striking features of non-Hermitian quasiperiodic systems with arbitrarily small asymmetry in the hopping amplitudes and open boundaries is the accumulation of all the bulk eigenstates at one of the edges of the system, termed in literature as the skin effect, below a critical strength of the potential. In this Letter, we uncover that a time-periodic drive in such systems can eliminate the SE up to a finite strength of this asymmetry. Remarkably, the critical value for the onset of SE is independent of the driving frequency and approaches to the static behavior in the thermodynamic limit. We find that the absence of SE is intricately linked to the emergence of extended unitarity in the delocalized phase, providing dynamical stability to the system. Interestingly, under periodic boundary condition, our non-Hermitian system can be mapped to a Hermitian analogue in the large driving frequency limit that leads to the extended unitarity irrespective of the hopping asymmetry and the strength of the quasiperiodic potential, in stark contrast to the static limit. Additionally, we numerically verify that this behavior persists Based on our findings, we propose a possible experimental realization of our driven system, which could be used as a switch to control the light funneling mechanism.","sentences":["One of the most striking features of non-Hermitian quasiperiodic systems with arbitrarily small asymmetry in the hopping amplitudes and open boundaries is the accumulation of all the bulk eigenstates at one of the edges of the system, termed in literature as the skin effect, below a critical strength of the potential.","In this Letter, we uncover that a time-periodic drive in such systems can eliminate the SE up to a finite strength of this asymmetry.","Remarkably, the critical value for the onset of SE is independent of the driving frequency and approaches to the static behavior in the thermodynamic limit.","We find that the absence of SE is intricately linked to the emergence of extended unitarity in the delocalized phase, providing dynamical stability to the system.","Interestingly, under periodic boundary condition, our non-Hermitian system can be mapped to a Hermitian analogue in the large driving frequency limit that leads to the extended unitarity irrespective of the hopping asymmetry and the strength of the quasiperiodic potential, in stark contrast to the static limit.","Additionally, we numerically verify that this behavior persists Based on our findings, we propose a possible experimental realization of our driven system, which could be used as a switch to control the light funneling mechanism."],"url":"http://arxiv.org/abs/2404.12297v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-18 16:19:10","title":"Long Duration Battery Sizing, Siting, and Operation Under Wildfire Risk Using Progressive Hedging","abstract":"Battery sizing and siting problems are computationally challenging due to the need to make long-term planning decisions that are cognizant of short-term operational decisions. This paper considers sizing, siting, and operating batteries in a power grid to maximize their benefits, including price arbitrage and load shed mitigation, during both normal operations and periods with high wildfire ignition risk. We formulate a multi-scenario optimization problem for long duration battery storage while considering the possibility of load shedding during Public Safety Power Shutoff (PSPS) events that de-energize lines to mitigate severe wildfire ignition risk. To enable a computationally scalable solution of this problem with many scenarios of wildfire risk and power injection variability, we develop a customized temporal decomposition method based on a progressive hedging framework. Extending traditional progressive hedging techniques, we consider coupling in both placement variables across all scenarios and state-of-charge variables at temporal boundaries. This enforces consistency across scenarios while enabling parallel computations despite both spatial and temporal coupling. The proposed decomposition facilitates efficient and scalable modeling of a full year of hourly operational decisions to inform the sizing and siting of batteries. With this decomposition, we model a year of hourly operational decisions to inform optimal battery placement for a 240-bus WECC model in under 70 minutes of wall-clock time.","sentences":["Battery sizing and siting problems are computationally challenging due to the need to make long-term planning decisions that are cognizant of short-term operational decisions.","This paper considers sizing, siting, and operating batteries in a power grid to maximize their benefits, including price arbitrage and load shed mitigation, during both normal operations and periods with high wildfire ignition risk.","We formulate a multi-scenario optimization problem for long duration battery storage while considering the possibility of load shedding during Public Safety Power Shutoff (PSPS) events that de-energize lines to mitigate severe wildfire ignition risk.","To enable a computationally scalable solution of this problem with many scenarios of wildfire risk and power injection variability, we develop a customized temporal decomposition method based on a progressive hedging framework.","Extending traditional progressive hedging techniques, we consider coupling in both placement variables across all scenarios and state-of-charge variables at temporal boundaries.","This enforces consistency across scenarios while enabling parallel computations despite both spatial and temporal coupling.","The proposed decomposition facilitates efficient and scalable modeling of a full year of hourly operational decisions to inform the sizing and siting of batteries.","With this decomposition, we model a year of hourly operational decisions to inform optimal battery placement for a 240-bus WECC model in under 70 minutes of wall-clock time."],"url":"http://arxiv.org/abs/2404.12296v1","category":"eess.SY"}
{"created":"2024-04-18 16:18:41","title":"When Medical Imaging Met Self-Attention: A Love Story That Didn't Quite Work Out","abstract":"A substantial body of research has focused on developing systems that assist medical professionals during labor-intensive early screening processes, many based on convolutional deep-learning architectures. Recently, multiple studies explored the application of so-called self-attention mechanisms in the vision domain. These studies often report empirical improvements over fully convolutional approaches on various datasets and tasks. To evaluate this trend for medical imaging, we extend two widely adopted convolutional architectures with different self-attention variants on two different medical datasets. With this, we aim to specifically evaluate the possible advantages of additional self-attention. We compare our models with similarly sized convolutional and attention-based baselines and evaluate performance gains statistically. Additionally, we investigate how including such layers changes the features learned by these models during the training. Following a hyperparameter search, and contrary to our expectations, we observe no significant improvement in balanced accuracy over fully convolutional models. We also find that important features, such as dermoscopic structures in skin lesion images, are still not learned by employing self-attention. Finally, analyzing local explanations, we confirm biased feature usage. We conclude that merely incorporating attention is insufficient to surpass the performance of existing fully convolutional methods.","sentences":["A substantial body of research has focused on developing systems that assist medical professionals during labor-intensive early screening processes, many based on convolutional deep-learning architectures.","Recently, multiple studies explored the application of so-called self-attention mechanisms in the vision domain.","These studies often report empirical improvements over fully convolutional approaches on various datasets and tasks.","To evaluate this trend for medical imaging, we extend two widely adopted convolutional architectures with different self-attention variants on two different medical datasets.","With this, we aim to specifically evaluate the possible advantages of additional self-attention.","We compare our models with similarly sized convolutional and attention-based baselines and evaluate performance gains statistically.","Additionally, we investigate how including such layers changes the features learned by these models during the training.","Following a hyperparameter search, and contrary to our expectations, we observe no significant improvement in balanced accuracy over fully convolutional models.","We also find that important features, such as dermoscopic structures in skin lesion images, are still not learned by employing self-attention.","Finally, analyzing local explanations, we confirm biased feature usage.","We conclude that merely incorporating attention is insufficient to surpass the performance of existing fully convolutional methods."],"url":"http://arxiv.org/abs/2404.12295v1","category":"cs.CV"}
{"created":"2024-04-18 16:13:58","title":"Singular-limit analysis of gradient descent with noise injection","abstract":"We study the limiting dynamics of a large class of noisy gradient descent systems in the overparameterized regime. In this regime the set of global minimizers of the loss is large, and when initialized in a neighbourhood of this zero-loss set a noisy gradient descent algorithm slowly evolves along this set. In some cases this slow evolution has been related to better generalisation properties. We characterize this evolution for the broad class of noisy gradient descent systems in the limit of small step size. Our results show that the structure of the noise affects not just the form of the limiting process, but also the time scale at which the evolution takes place. We apply the theory to Dropout, label noise and classical SGD (minibatching) noise, and show that these evolve on different two time scales. Classical SGD even yields a trivial evolution on both time scales, implying that additional noise is required for regularization. The results are inspired by the training of neural networks, but the theorems apply to noisy gradient descent of any loss that has a non-trivial zero-loss set.","sentences":["We study the limiting dynamics of a large class of noisy gradient descent systems in the overparameterized regime.","In this regime the set of global minimizers of the loss is large, and when initialized in a neighbourhood of this zero-loss set a noisy gradient descent algorithm slowly evolves along this set.","In some cases this slow evolution has been related to better generalisation properties.","We characterize this evolution for the broad class of noisy gradient descent systems in the limit of small step size.","Our results show that the structure of the noise affects not just the form of the limiting process, but also the time scale at which the evolution takes place.","We apply the theory to Dropout, label noise and classical SGD (minibatching) noise, and show that these evolve on different two time scales.","Classical SGD even yields a trivial evolution on both time scales, implying that additional noise is required for regularization.","The results are inspired by the training of neural networks, but the theorems apply to noisy gradient descent of any loss that has a non-trivial zero-loss set."],"url":"http://arxiv.org/abs/2404.12293v1","category":"cs.LG"}
{"created":"2024-04-18 16:10:06","title":"The genetic basis and evolution of meiotic recombination rate variation -- what have we learned?","abstract":"Meiotic recombination is the exchange of DNA between homologous chromosomes, through chromosomal crossover and gene-conversion events. It is a fundamental feature of sex, and an important driver of diversity in eukaryotic genomes. The toolbox of recombination is remarkably conserved, yet meiotic genes show substantial variation even between closely related species. Furthermore, the rate and distribution of recombination is diverse across eukaryotes, both within and between genomic regions (i.e. \"hotspots\"), chromosomes, individuals, sexes, populations, and species. In recent decades, major advances have been made in understanding recombination rate variation, in terms of measuring it, identifying its genetic architecture and evolutionary potential, and understanding the complex dynamics of recombination landscapes. In this perspective, written for the 40th anniversary of the journal Molecular Biology and Evolution, I explore what we have learned and are still learning about the genetic basis and evolution of recombination rates, and present open questions for future research.","sentences":["Meiotic recombination is the exchange of DNA between homologous chromosomes, through chromosomal crossover and gene-conversion events.","It is a fundamental feature of sex, and an important driver of diversity in eukaryotic genomes.","The toolbox of recombination is remarkably conserved, yet meiotic genes show substantial variation even between closely related species.","Furthermore, the rate and distribution of recombination is diverse across eukaryotes, both within and between genomic regions (i.e. \"hotspots\"), chromosomes, individuals, sexes, populations, and species.","In recent decades, major advances have been made in understanding recombination rate variation, in terms of measuring it, identifying its genetic architecture and evolutionary potential, and understanding the complex dynamics of recombination landscapes.","In this perspective, written for the 40th anniversary of the journal Molecular Biology and Evolution, I explore what we have learned and are still learning about the genetic basis and evolution of recombination rates, and present open questions for future research."],"url":"http://arxiv.org/abs/2404.12288v1","category":"q-bio.PE"}
{"created":"2024-04-18 15:58:31","title":"Investigating Guiding Information for Adaptive Collocation Point Sampling in PINNs","abstract":"Physics-informed neural networks (PINNs) provide a means of obtaining approximate solutions of partial differential equations and systems through the minimisation of an objective function which includes the evaluation of a residual function at a set of collocation points within the domain. The quality of a PINNs solution depends upon numerous parameters, including the number and distribution of these collocation points. In this paper we consider a number of strategies for selecting these points and investigate their impact on the overall accuracy of the method. In particular, we suggest that no single approach is likely to be ``optimal'' but we show how a number of important metrics can have an impact in improving the quality of the results obtained when using a fixed number of residual evaluations. We illustrate these approaches through the use of two benchmark test problems: Burgers' equation and the Allen-Cahn equation.","sentences":["Physics-informed neural networks (PINNs) provide a means of obtaining approximate solutions of partial differential equations and systems through the minimisation of an objective function which includes the evaluation of a residual function at a set of collocation points within the domain.","The quality of a PINNs solution depends upon numerous parameters, including the number and distribution of these collocation points.","In this paper we consider a number of strategies for selecting these points and investigate their impact on the overall accuracy of the method.","In particular, we suggest that no single approach is likely to be ``optimal'' but we show how a number of important metrics can have an impact in improving the quality of the results obtained when using a fixed number of residual evaluations.","We illustrate these approaches through the use of two benchmark test problems: Burgers' equation and the Allen-Cahn equation."],"url":"http://arxiv.org/abs/2404.12282v1","category":"cs.LG"}
{"created":"2024-04-18 15:37:45","title":"Theory of Mobility Rings in Non-Hermitian Systems","abstract":"Through Avila global theorem, we analytically study the non-Hermitian mobility edge. The results show that the mobility edge in non-Hermitian systems has a ring structure, which we named as \"mobility ring\". Furthermore, we carry out numerical analysis of the eigenenergy spectra in several typical cases, and the consistence of the numerical results with the analytical expression proves the correctness and universality of the mobility ring theory. Further, based on the analytical expression, we discuss the properties of multiple mobility rings. Finally, we compare the results of mobility rings with that of dual transformations, and find that although the self-dual method can give the interval of real eigenvalues corresponding to the extended states, it can not fully display the mobility edge information in the complex plane. The mobility ring theory proposed in this paper is universal for all non-Hermitian systems.","sentences":["Through Avila global theorem, we analytically study the non-Hermitian mobility edge.","The results show that the mobility edge in non-Hermitian systems has a ring structure, which we named as \"mobility ring\".","Furthermore, we carry out numerical analysis of the eigenenergy spectra in several typical cases, and the consistence of the numerical results with the analytical expression proves the correctness and universality of the mobility ring theory.","Further, based on the analytical expression, we discuss the properties of multiple mobility rings.","Finally, we compare the results of mobility rings with that of dual transformations, and find that although the self-dual method can give the interval of real eigenvalues corresponding to the extended states, it can not fully display the mobility edge information in the complex plane.","The mobility ring theory proposed in this paper is universal for all non-Hermitian systems."],"url":"http://arxiv.org/abs/2404.12266v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-18 15:28:49","title":"Design And Flight Testing Of LQRi Attitude Control For Quadcopter UAV","abstract":"This paper presents the design, implementation, and flight test results of linear quadratic integral regulator (LQRi) based attitude control for a quadcopter UAV. We present the derivation of the mathematical model for the kinematics and dynamics of the UAV, along with the linearized state space representation of the system about hover conditions. LQR and LQRi controllers are then designed to stabilize the UAV in hover conditions and to track desired attitude commands. The controllers are then implemented onboard the Pixhawk flight controller and flight test results are discussed. Finally, the code related to this paper has been published open-source for replication and further research","sentences":["This paper presents the design, implementation, and flight test results of linear quadratic integral regulator (LQRi) based attitude control for a quadcopter UAV.","We present the derivation of the mathematical model for the kinematics and dynamics of the UAV, along with the linearized state space representation of the system about hover conditions.","LQR and LQRi controllers are then designed to stabilize the UAV in hover conditions and to track desired attitude commands.","The controllers are then implemented onboard the Pixhawk flight controller and flight test results are discussed.","Finally, the code related to this paper has been published open-source for replication and further research"],"url":"http://arxiv.org/abs/2404.12261v1","category":"cs.RO"}
{"created":"2024-04-18 15:25:59","title":"DeepLocalization: Using change point detection for Temporal Action Localization","abstract":"In this study, we introduce DeepLocalization, an innovative framework devised for the real-time localization of actions tailored explicitly for monitoring driver behavior. Utilizing the power of advanced deep learning methodologies, our objective is to tackle the critical issue of distracted driving-a significant factor contributing to road accidents. Our strategy employs a dual approach: leveraging Graph-Based Change-Point Detection for pinpointing actions in time alongside a Video Large Language Model (Video-LLM) for precisely categorizing activities. Through careful prompt engineering, we customize the Video-LLM to adeptly handle driving activities' nuances, ensuring its classification efficacy even with sparse data. Engineered to be lightweight, our framework is optimized for consumer-grade GPUs, making it vastly applicable in practical scenarios. We subjected our method to rigorous testing on the SynDD2 dataset, a complex benchmark for distracted driving behaviors, where it demonstrated commendable performance-achieving 57.5% accuracy in event classification and 51% in event detection. These outcomes underscore the substantial promise of DeepLocalization in accurately identifying diverse driver behaviors and their temporal occurrences, all within the bounds of limited computational resources.","sentences":["In this study, we introduce DeepLocalization, an innovative framework devised for the real-time localization of actions tailored explicitly for monitoring driver behavior.","Utilizing the power of advanced deep learning methodologies, our objective is to tackle the critical issue of distracted driving-a significant factor contributing to road accidents.","Our strategy employs a dual approach: leveraging Graph-Based Change-Point Detection for pinpointing actions in time alongside a Video Large Language Model (Video-LLM) for precisely categorizing activities.","Through careful prompt engineering, we customize the Video-LLM to adeptly handle driving activities' nuances, ensuring its classification efficacy even with sparse data.","Engineered to be lightweight, our framework is optimized for consumer-grade GPUs, making it vastly applicable in practical scenarios.","We subjected our method to rigorous testing on the SynDD2 dataset, a complex benchmark for distracted driving behaviors, where it demonstrated commendable performance-achieving 57.5% accuracy in event classification and 51% in event detection.","These outcomes underscore the substantial promise of DeepLocalization in accurately identifying diverse driver behaviors and their temporal occurrences, all within the bounds of limited computational resources."],"url":"http://arxiv.org/abs/2404.12258v1","category":"cs.CV"}
{"created":"2024-04-18 15:22:22","title":"Star-by-star dynamical evolution of the physical pair of the Collinder 135 and UBC 7 open clusters","abstract":"In a previous paper using Gaia DR2 data, we demonstrated that the two closely situated open clusters Collinder 135 and UBC 7 might have formed together about 50 Myr ago. In this work, we performed star-by-star dynamical modelling of the evolution of the open clusters Collinder 135 and UBC 7 from their supposed initial state to their present-day state, reproducing observational distributions of members. Modelling of the Collinder 135 and UBC 7 dynamical evolution was done using the high-order parallel N-body code \\phi-GPU with up-to-date stellar evolution. Membership and characteristics of the clusters were acquired based on Gaia DR3 data. The comparison of the present-day radial cumulative star count obtained from the N-body simulations with the current observational data gave us full consistency of the model with observational data, especially in the central 8 pc, where 80% of the stars reside. The proper motion velocity components obtained from the N-body simulations of the stars are also quite consistent with the observed distributions and error bars. These results show that our numerical modelling is able to reproduce the open clusters' current complex 6D observed phase-space distributions with a high level of confidence. Thus, the model demonstrates that the hypothesis of a common origin of Collinder 135 and UBC 7 complies with present-day observational data.","sentences":["In a previous paper using Gaia DR2 data, we demonstrated that the two closely situated open clusters Collinder 135 and UBC 7 might have formed together about 50 Myr ago.","In this work, we performed star-by-star dynamical modelling of the evolution of the open clusters Collinder 135 and UBC 7 from their supposed initial state to their present-day state, reproducing observational distributions of members.","Modelling of the Collinder 135 and UBC 7 dynamical evolution was done using the high-order parallel N-body code \\phi-GPU with up-to-date stellar evolution.","Membership and characteristics of the clusters were acquired based on Gaia DR3 data.","The comparison of the present-day radial cumulative star count obtained from the N-body simulations with the current observational data gave us full consistency of the model with observational data, especially in the central 8 pc, where 80% of the stars reside.","The proper motion velocity components obtained from the N-body simulations of the stars are also quite consistent with the observed distributions and error bars.","These results show that our numerical modelling is able to reproduce the open clusters' current complex 6D observed phase-space distributions with a high level of confidence.","Thus, the model demonstrates that the hypothesis of a common origin of Collinder 135 and UBC 7 complies with present-day observational data."],"url":"http://arxiv.org/abs/2404.12255v1","category":"astro-ph.GA"}
{"created":"2024-04-18 15:21:34","title":"Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing","abstract":"Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.","sentences":["Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning.","Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities.","However, these approaches are inherently constrained by data availability and quality.","In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards.","Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious.","In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations.","Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks.","AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback.","Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs."],"url":"http://arxiv.org/abs/2404.12253v1","category":"cs.CL"}
{"created":"2024-04-18 15:11:02","title":"Blind Localization and Clustering of Anomalies in Textures","abstract":"Anomaly detection and localization in images is a growing field in computer vision. In this area, a seemingly understudied problem is anomaly clustering, i.e., identifying and grouping different types of anomalies in a fully unsupervised manner. In this work, we propose a novel method for clustering anomalies in largely stationary images (textures) in a blind setting. That is, the input consists of normal and anomalous images without distinction and without labels. What contributes to the difficulty of the task is that anomalous regions are often small and may present only subtle changes in appearance, which can be easily overshadowed by the genuine variance in the texture. Moreover, each anomaly type may have a complex appearance distribution. We introduce a novel scheme for solving this task using a combination of blind anomaly localization and contrastive learning. By identifying the anomalous regions with high fidelity, we can restrict our focus to those regions of interest; then, contrastive learning is employed to increase the separability of different anomaly types and reduce the intra-class variation. Our experiments show that the proposed solution yields significantly better results compared to prior work, setting a new state of the art. Project page: https://reality.tf.fau.de/pub/ardelean2024blind.html.","sentences":["Anomaly detection and localization in images is a growing field in computer vision.","In this area, a seemingly understudied problem is anomaly clustering, i.e., identifying and grouping different types of anomalies in a fully unsupervised manner.","In this work, we propose a novel method for clustering anomalies in largely stationary images (textures) in a blind setting.","That is, the input consists of normal and anomalous images without distinction and without labels.","What contributes to the difficulty of the task is that anomalous regions are often small and may present only subtle changes in appearance, which can be easily overshadowed by the genuine variance in the texture.","Moreover, each anomaly type may have a complex appearance distribution.","We introduce a novel scheme for solving this task using a combination of blind anomaly localization and contrastive learning.","By identifying the anomalous regions with high fidelity, we can restrict our focus to those regions of interest; then, contrastive learning is employed to increase the separability of different anomaly types and reduce the intra-class variation.","Our experiments show that the proposed solution yields significantly better results compared to prior work, setting a new state of the art.","Project page: https://reality.tf.fau.de/pub/ardelean2024blind.html."],"url":"http://arxiv.org/abs/2404.12246v1","category":"cs.CV"}
{"created":"2024-04-18 14:58:29","title":"The formation of transiting circumplanetary debris discs from the disruption of satellite systems during planet-planet scattering","abstract":"Several stars show deep transits consistent with discs of roughly 1 Solar radius seen at moderate inclinations, likely surrounding planets on eccentric orbits. We show that this configuration arises naturally as a result of planet-planet scattering when the planets possess satellite systems. Planet-planet scattering explains the orbital eccentricities of the discs' host bodies, while the close encounters during scattering lead to the exchange of satellites between planets and/or their destabilisation. This leads to collisions between satellites and their tidal disruption close to the planet. Both of these events lead to large quantities of debris being produced, which in time will settle into a disc such as those observed. The mass of debris required is comparable to a Ceres-sized satellite. Through N-body simulations of planets with clones of the Galilean satellite system undergoing scattering, we show that 90 percent of planets undergoing scattering will possess debris from satellite destruction. Extrapolating to smaller numbers of satellites suggests that tens of percent of such planets should still possess circumplanetary debris discs. The debris trails arising from these events are often tilted at tens of degrees to the planetary orbit, consistent with the inclinations of the observed discs. Disruption of satellite systems during scattering thus simultaneously explains the existence of debris, the tilt of the discs, and the eccentricity of the planets they orbit.","sentences":["Several stars show deep transits consistent with discs of roughly 1 Solar radius seen at moderate inclinations, likely surrounding planets on eccentric orbits.","We show that this configuration arises naturally as a result of planet-planet scattering when the planets possess satellite systems.","Planet-planet scattering explains the orbital eccentricities of the discs' host bodies, while the close encounters during scattering lead to the exchange of satellites between planets and/or their destabilisation.","This leads to collisions between satellites and their tidal disruption close to the planet.","Both of these events lead to large quantities of debris being produced, which in time will settle into a disc such as those observed.","The mass of debris required is comparable to a Ceres-sized satellite.","Through N-body simulations of planets with clones of the Galilean satellite system undergoing scattering, we show that 90 percent of planets undergoing scattering will possess debris from satellite destruction.","Extrapolating to smaller numbers of satellites suggests that tens of percent of such planets should still possess circumplanetary debris discs.","The debris trails arising from these events are often tilted at tens of degrees to the planetary orbit, consistent with the inclinations of the observed discs.","Disruption of satellite systems during scattering thus simultaneously explains the existence of debris, the tilt of the discs, and the eccentricity of the planets they orbit."],"url":"http://arxiv.org/abs/2404.12239v1","category":"astro-ph.EP"}
{"created":"2024-04-18 14:51:35","title":"Quantitative homogenization and hydrodynamic limit of non-gradient exclusion process","abstract":"For the non-gradient exclusion process, we prove its approximation rate of diffusion matrix/conductivity by local functions. The proof follows the quantitative homogenization theory developed by Armstrong, Kuusi, Mourrat and Smart, while the new challenge here is the hard core constraint of particle number on every site. Therefore, a coarse-grained method is proposed to lift the configuration to a larger space without exclusion, and a gradient coupling between two systems is applied to capture the spatial cancellation. Moreover, the approximation rate of conductivity is uniform with respect to the density via the regularity of the local corrector. As an application, we integrate this result in the work by Funaki, Uchiyama and Yau [IMA Vol. Math. Appl., 77 (1996), pp. 1-40.] and yield a quantitative hydrodynamic limit. In particular, our new approach avoids to show the characterization of closed forms. We also discuss the possible extensions in the presence of disorder on the bonds.","sentences":["For the non-gradient exclusion process, we prove its approximation rate of diffusion matrix/conductivity by local functions.","The proof follows the quantitative homogenization theory developed by Armstrong, Kuusi, Mourrat and Smart, while the new challenge here is the hard core constraint of particle number on every site.","Therefore, a coarse-grained method is proposed to lift the configuration to a larger space without exclusion, and a gradient coupling between two systems is applied to capture the spatial cancellation.","Moreover, the approximation rate of conductivity is uniform with respect to the density via the regularity of the local corrector.","As an application, we integrate this result in the work by Funaki, Uchiyama and Yau [IMA Vol.","Math.","Appl., 77 (1996), pp. 1-40.]","and yield a quantitative hydrodynamic limit.","In particular, our new approach avoids to show the characterization of closed forms.","We also discuss the possible extensions in the presence of disorder on the bonds."],"url":"http://arxiv.org/abs/2404.12234v1","category":"math.PR"}
{"created":"2024-04-18 14:49:45","title":"Model geometries of porous materials","abstract":"We describe a method for modeling the geometry of porous materials. The approach enables the independent selection of crucial parameters, including porosity, pore size distribution, pore shape, and connectivity. Consequently, it can effectively model a wide range of porous systems. Due to the diverse and systematic variation possibilities, the method is suitable for developing and optimizing porous structures. The geometries can be exported as triangular meshes, facilitating their immediate use in numerical simulation and further digital processing. We showcase the method's capabilities by minimizing the foam structure's thermal conductivity through geometry optimization.","sentences":["We describe a method for modeling the geometry of porous materials.","The approach enables the independent selection of crucial parameters, including porosity, pore size distribution, pore shape, and connectivity.","Consequently, it can effectively model a wide range of porous systems.","Due to the diverse and systematic variation possibilities, the method is suitable for developing and optimizing porous structures.","The geometries can be exported as triangular meshes, facilitating their immediate use in numerical simulation and further digital processing.","We showcase the method's capabilities by minimizing the foam structure's thermal conductivity through geometry optimization."],"url":"http://arxiv.org/abs/2404.12232v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 14:41:33","title":"A cooperative strategy for diagnosing the root causes of quality requirement violations in multiagent systems","abstract":"Many modern software systems are built as a set of autonomous software components (also called agents) that collaborate with each other and are situated in an environment. To keep these multiagent systems operational under abnormal circumstances, it is crucial to make them resilient. Existing solutions are often centralised and rely on information manually provided by experts at design time, making such solutions rigid and limiting the autonomy and adaptability of the system. In this work, we propose a cooperative strategy focused on the identification of the root causes of quality requirement violations in multiagent systems. This strategy allows agents to cooperate with each other in order to identify whether these violations come from service providers, associated components, or the communication infrastructure. From this identification process, agents are able to adapt their behaviour in order to mitigate and solve existing abnormalities with the aim of normalising system operation. This strategy consists of an interaction protocol that, together with the proposed algorithms, allow agents playing the protocol roles to diagnose problems to be repaired. We evaluate our proposal with the implementation of a service-oriented system. The results demonstrate that our solution enables the correct identification of different sources of failures, favouring the selection of the most suitable actions to be taken to overcome abnormal situations.","sentences":["Many modern software systems are built as a set of autonomous software components (also called agents) that collaborate with each other and are situated in an environment.","To keep these multiagent systems operational under abnormal circumstances, it is crucial to make them resilient.","Existing solutions are often centralised and rely on information manually provided by experts at design time, making such solutions rigid and limiting the autonomy and adaptability of the system.","In this work, we propose a cooperative strategy focused on the identification of the root causes of quality requirement violations in multiagent systems.","This strategy allows agents to cooperate with each other in order to identify whether these violations come from service providers, associated components, or the communication infrastructure.","From this identification process, agents are able to adapt their behaviour in order to mitigate and solve existing abnormalities with the aim of normalising system operation.","This strategy consists of an interaction protocol that, together with the proposed algorithms, allow agents playing the protocol roles to diagnose problems to be repaired.","We evaluate our proposal with the implementation of a service-oriented system.","The results demonstrate that our solution enables the correct identification of different sources of failures, favouring the selection of the most suitable actions to be taken to overcome abnormal situations."],"url":"http://arxiv.org/abs/2404.12226v1","category":"cs.SE"}
{"created":"2024-04-18 14:12:05","title":"Attosecond dynamics of electron scattering by an absorbing layer","abstract":"Attosecond dynamics of electron reflection from a thin film is studied based on a one-dimensional jellium model. Following the Eisenbud-Wigner-Smith concept, the reflection time delay $\\Delta\\tau_{\\rm R}$ is calculated as the energy derivative of the phase of the complex reflection amplitude $r$. For a purely elastic scattering by a jellium slab of a finite thickness $d$ the transmission probability $T$ oscillates with the momentum $K$ in the solid with a period $\\pi/d$, and $\\Delta\\tau_{\\rm R}$ closely follows these oscillations. The reflection delay averaged over an energy interval grows with $d$, but in the limit of $d\\to\\infty$ the amplitude $r$ becomes real, so $\\Delta\\tau_{\\rm R}$ vanishes. This picture changes substantially with the inclusion of an absorbing potential $-iV_{\\rm i}$: As expected, for a sufficiently thick slab the reflection amplitude now tends to its asymptotic value for a semi-infinite crystal. Interestingly, for $V_{\\rm i} \\ne 0$, around the $T(E)$ maxima, the $\\Delta\\tau_{\\rm R}(E)$ curve strongly deviates from $T(E)$, showing a narrow dip just at the $\\Delta\\tau_{\\rm R}(E)$ maximum for $V_{\\rm i}=0$. An analytical theory of this counterintuitive behavior is developed.","sentences":["Attosecond dynamics of electron reflection from a thin film is studied based on a one-dimensional jellium model.","Following the Eisenbud-Wigner-Smith concept, the reflection time delay $\\Delta\\tau_{\\rm R}$ is calculated as the energy derivative of the phase of the complex reflection amplitude $r$. For a purely elastic scattering by a jellium slab of a finite thickness $d$ the transmission probability $T$ oscillates with the momentum $K$ in the solid with a period $\\pi/d$, and $\\Delta\\tau_{\\rm R}$ closely follows these oscillations.","The reflection delay averaged over an energy interval grows with $d$, but in the limit of $d\\to\\infty$ the amplitude $r$ becomes real, so $\\Delta\\tau_{\\rm R}$ vanishes.","This picture changes substantially with the inclusion of an absorbing potential $-iV_{\\rm i}$: As expected, for a sufficiently thick slab the reflection amplitude now tends to its asymptotic value for a semi-infinite crystal.","Interestingly, for $V_{\\rm i} \\ne 0$, around the $T(E)$ maxima, the $\\Delta\\tau_{\\rm R}(E)$ curve strongly deviates from $T(E)$, showing a narrow dip just at the $\\Delta\\tau_{\\rm R}(E)$ maximum for $V_{\\rm i}=0$.","An analytical theory of this counterintuitive behavior is developed."],"url":"http://arxiv.org/abs/2404.12206v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 14:04:45","title":"Infinite sumsets of the form $B+B$ in sets with large density","abstract":"For a set $A \\subset \\mathbb{N}$ we characterize in terms of its density when there exists an infinite set $B \\subset \\mathbb{N}$ and $t \\in \\{0,1\\}$ such that $B+B \\subset A-t$, where $B+B : =\\{b_1+b_2\\colon b_1,b_2 \\in B\\}$. Specifically, when the lower density $\\underline{d}(A) >1/2$ or the upper density $\\overline{d}(A)> 3/4$, the existence of such a set $B\\subset \\mathbb{N}$ and $t\\in \\{0,1\\}$ is assured. Furthermore, whenever $\\underline{d}(A) > 3/4$ or $\\overline{d}(A)>5/6$, we show that the shift $t$ is unnecessary and we also provide examples to show that these bounds are sharp. Finally, we construct a syndetic three-coloring of the natural numbers that does not contain a monochromatic $B+B+t$ for any infinite set $B \\subset \\mathbb{N}$ and number $t \\in \\mathbb{N}$.","sentences":["For a set $A \\subset \\mathbb{N}$ we characterize in terms of its density when there exists an infinite set $B \\subset \\mathbb{N}$ and $t \\in \\{0,1\\}$ such that $B+B \\subset A-t$, where $B+B : =\\{b_1+b_2\\colon b_1,b_2 \\in B\\}$.","Specifically, when the lower density $\\underline{d}(A) >1/2$ or the upper density $\\overline{d}(A)> 3/4$, the existence of such a set $B\\subset \\mathbb{N}$ and $t\\in \\{0,1\\}$ is assured.","Furthermore, whenever $\\underline{d}(A) > 3/4$ or $\\overline{d}(A)>5/6$, we show that the shift $t$ is unnecessary and we also provide examples to show that these bounds are sharp.","Finally, we construct a syndetic three-coloring of the natural numbers that does not contain a monochromatic $B+B+t$ for any infinite set $B \\subset \\mathbb{N}$ and number $t \\in \\mathbb{N}$."],"url":"http://arxiv.org/abs/2404.12201v1","category":"math.DS"}
{"created":"2024-04-18 13:49:09","title":"Stability-informed Bayesian Optimization for MPC Cost Function Learning","abstract":"Designing predictive controllers towards optimal closed-loop performance while maintaining safety and stability is challenging. This work explores closed-loop learning for predictive control parameters under imperfect information while considering closed-loop stability. We employ constrained Bayesian optimization to learn a model predictive controller's (MPC) cost function parametrized as a feedforward neural network, optimizing closed-loop behavior as well as minimizing model-plant mismatch. Doing so offers a high degree of freedom and, thus, the opportunity for efficient and global optimization towards the desired and optimal closed-loop behavior. We extend this framework by stability constraints on the learned controller parameters, exploiting the optimal value function of the underlying MPC as a Lyapunov candidate. The effectiveness of the proposed approach is underlined in simulations, highlighting its performance and safety capabilities.","sentences":["Designing predictive controllers towards optimal closed-loop performance while maintaining safety and stability is challenging.","This work explores closed-loop learning for predictive control parameters under imperfect information while considering closed-loop stability.","We employ constrained Bayesian optimization to learn a model predictive controller's (MPC) cost function parametrized as a feedforward neural network, optimizing closed-loop behavior as well as minimizing model-plant mismatch.","Doing so offers a high degree of freedom and, thus, the opportunity for efficient and global optimization towards the desired and optimal closed-loop behavior.","We extend this framework by stability constraints on the learned controller parameters, exploiting the optimal value function of the underlying MPC as a Lyapunov candidate.","The effectiveness of the proposed approach is underlined in simulations, highlighting its performance and safety capabilities."],"url":"http://arxiv.org/abs/2404.12187v1","category":"eess.SY"}
{"created":"2024-04-18 13:49:07","title":"Privacy-Preserving UCB Decision Process Verification via zk-SNARKs","abstract":"With the increasingly widespread application of machine learning, how to strike a balance between protecting the privacy of data and algorithm parameters and ensuring the verifiability of machine learning has always been a challenge. This study explores the intersection of reinforcement learning and data privacy, specifically addressing the Multi-Armed Bandit (MAB) problem with the Upper Confidence Bound (UCB) algorithm. We introduce zkUCB, an innovative algorithm that employs the Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARKs) to enhance UCB. zkUCB is carefully designed to safeguard the confidentiality of training data and algorithmic parameters, ensuring transparent UCB decision-making. Experiments highlight zkUCB's superior performance, attributing its enhanced reward to judicious quantization bit usage that reduces information entropy in the decision-making process. zkUCB's proof size and verification time scale linearly with the execution steps of zkUCB. This showcases zkUCB's adept balance between data security and operational efficiency. This approach contributes significantly to the ongoing discourse on reinforcing data privacy in complex decision-making processes, offering a promising solution for privacy-sensitive applications.","sentences":["With the increasingly widespread application of machine learning, how to strike a balance between protecting the privacy of data and algorithm parameters and ensuring the verifiability of machine learning has always been a challenge.","This study explores the intersection of reinforcement learning and data privacy, specifically addressing the Multi-Armed Bandit (MAB) problem with the Upper Confidence Bound (UCB) algorithm.","We introduce zkUCB, an innovative algorithm that employs the Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARKs) to enhance UCB.","zkUCB is carefully designed to safeguard the confidentiality of training data and algorithmic parameters, ensuring transparent UCB decision-making.","Experiments highlight zkUCB's superior performance, attributing its enhanced reward to judicious quantization bit usage that reduces information entropy in the decision-making process.","zkUCB's proof size and verification time scale linearly with the execution steps of zkUCB.","This showcases zkUCB's adept balance between data security and operational efficiency.","This approach contributes significantly to the ongoing discourse on reinforcing data privacy in complex decision-making processes, offering a promising solution for privacy-sensitive applications."],"url":"http://arxiv.org/abs/2404.12186v1","category":"cs.LG"}
{"created":"2024-04-18 13:47:17","title":"Boolean Matching Reversible Circuits: Algorithm and Complexity","abstract":"Boolean matching is an important problem in logic synthesis and verification. Despite being well-studied for conventional Boolean circuits, its treatment for reversible logic circuits remains largely, if not completely, missing. This work provides the first such study. Given two (black-box) reversible logic circuits that are promised to be matchable, we check their equivalences under various input/output negation and permutation conditions subject to the availability/unavailability of their inverse circuits. Notably, among other results, we show that the equivalence up to input negation and permutation is solvable in quantum polynomial time, while its classical complexity is exponential. This result is arguably the first demonstration of quantum exponential speedup in solving design automation problems. Also, as a negative result, we show that the equivalence up to both input and output negations is not solvable in quantum polynomial time unless UNIQUE-SAT is, which is unlikely. This work paves the theoretical foundation of Boolean matching reversible circuits for potential applications, e.g., in quantum circuit synthesis.","sentences":["Boolean matching is an important problem in logic synthesis and verification.","Despite being well-studied for conventional Boolean circuits, its treatment for reversible logic circuits remains largely, if not completely, missing.","This work provides the first such study.","Given two (black-box) reversible logic circuits that are promised to be matchable, we check their equivalences under various input/output negation and permutation conditions subject to the availability/unavailability of their inverse circuits.","Notably, among other results, we show that the equivalence up to input negation and permutation is solvable in quantum polynomial time, while its classical complexity is exponential.","This result is arguably the first demonstration of quantum exponential speedup in solving design automation problems.","Also, as a negative result, we show that the equivalence up to both input and output negations is not solvable in quantum polynomial time unless UNIQUE-SAT is, which is unlikely.","This work paves the theoretical foundation of Boolean matching reversible circuits for potential applications, e.g., in quantum circuit synthesis."],"url":"http://arxiv.org/abs/2404.12184v1","category":"quant-ph"}
{"created":"2024-04-18 13:41:14","title":"Abnormal solutions of Bethe--Salpeter equation with massless and massive exchanges","abstract":"We summarize the main properties of the so called ''abnormal solutions'' of the Wick--Cutkosky model, i.e. two massive scalar particles interacting via massless scalar exchange (\"photons\"), within the Bethe--Salpeter equation. These solutions do not exist in the non-relativistic limit, in spite of having very small binding energies. They present a genuine many-body character dominated by photons, with a norm of the valence constituent wave function (two-body norm) that vanishes in the limit of zero binding energy.   We present new results concerning the massive-exchange case, in particular determine under which conditions is it possible to obtain such peculiar solutions without spoiling the model by tachyonic states ($M^2<0$).","sentences":["We summarize the main properties of the so called ''abnormal solutions'' of the Wick--Cutkosky model, i.e. two massive scalar particles interacting via massless scalar exchange (\"photons\"), within the Bethe--Salpeter equation.","These solutions do not exist in the non-relativistic limit, in spite of having very small binding energies.","They present a genuine many-body character dominated by photons, with a norm of the valence constituent wave function (two-body norm) that vanishes in the limit of zero binding energy.   ","We present new results concerning the massive-exchange case, in particular determine under which conditions is it possible to obtain such peculiar solutions without spoiling the model by tachyonic states ($M^2<0$)."],"url":"http://arxiv.org/abs/2404.12182v1","category":"hep-ph"}
{"created":"2024-04-18 13:39:00","title":"Microwave seeding time crystal in Floquet driven Rydberg atoms","abstract":"Crystal seeding enables a deeper understanding of phase behavior, leading to the development of methods for controlling and manipulating phase transitions in various applications such as materials synthesis, crystallization processes, and phase transformation engineering. How to seed a crystalline in time domain is an open question, which is of great significant and may provide an avenue to understand and control time-dependent quantum many-body physics. Here, we utilize a microwave pulse as a seed to induce the formation of a discrete time crystal in Floquet driven Rydberg atoms. In the experiment, the periodic driving on Rydberg states acts as a seeded crystalline order in subspace, which triggers the time-translation symmetry breaking across the entire ensemble. The behavior of the emergent time crystal is elaborately linked to alterations in the seed, such as the relative phase shift and the frequency difference, which result in phase dependent seeding and corresponding shift in periodicity of the time crystal, leading to embryonic synchronization. This result opens up new possibilities for studying and harnessing time-dependent quantum many-body phenomena, offering insights into the behavior of complex many-body systems under seeding.","sentences":["Crystal seeding enables a deeper understanding of phase behavior, leading to the development of methods for controlling and manipulating phase transitions in various applications such as materials synthesis, crystallization processes, and phase transformation engineering.","How to seed a crystalline in time domain is an open question, which is of great significant and may provide an avenue to understand and control time-dependent quantum many-body physics.","Here, we utilize a microwave pulse as a seed to induce the formation of a discrete time crystal in Floquet driven Rydberg atoms.","In the experiment, the periodic driving on Rydberg states acts as a seeded crystalline order in subspace, which triggers the time-translation symmetry breaking across the entire ensemble.","The behavior of the emergent time crystal is elaborately linked to alterations in the seed, such as the relative phase shift and the frequency difference, which result in phase dependent seeding and corresponding shift in periodicity of the time crystal, leading to embryonic synchronization.","This result opens up new possibilities for studying and harnessing time-dependent quantum many-body phenomena, offering insights into the behavior of complex many-body systems under seeding."],"url":"http://arxiv.org/abs/2404.12180v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-18 13:31:53","title":"Practical GHz single-cavity all-fiber dual-comb laser for high-speed spectroscopy","abstract":"Dual-comb spectroscopy (DCS) with few-GHz tooth spacing that provides the optimal trade-off between spectral resolution and refresh rate is a powerful tool for measuring and analyzing rapidly evolving transient events. Despite such an exciting opportunity, existing technologies compromise either the spectral resolution or refresh rate, leaving few-GHz DCS with robust design largely unmet for frontier applications. In this work, we demonstrate a novel GHz DCS by exploring the multimode interference-mediated spectral filtering effect in an all-fiber ultrashort cavity configuration. The GHz single-cavity all-fiber dual-comb source is seeded by a dual-wavelength mode-locked fiber laser operating at fundamental repetition rates of about 1.0 GHz differing by 148 kHz, which has an excellent stability in the free-running state that the Allan deviation is only 101.7 mHz for an average time of 1 second. Thanks to the large repetition rate difference between the asynchronous dichromatic pulse trains, the GHz DCS enables a refresh time as short as 6.75 us, making it promising for studying nonrepeatable transient phenomena in real time. To this end, the practicality of the present GHz DCS is validated by successfully capturing the 'shock waves' of balloon and firecracker explosions outdoors. This GHz single-cavity all-fiber dual-comb system promises a noteworthy improvement in acquisition speed and reliability without sacrificing measurement accuracy, anticipated as a practical tool for high-speed applications.","sentences":["Dual-comb spectroscopy (DCS) with few-GHz tooth spacing that provides the optimal trade-off between spectral resolution and refresh rate is a powerful tool for measuring and analyzing rapidly evolving transient events.","Despite such an exciting opportunity, existing technologies compromise either the spectral resolution or refresh rate, leaving few-GHz DCS with robust design largely unmet for frontier applications.","In this work, we demonstrate a novel GHz DCS by exploring the multimode interference-mediated spectral filtering effect in an all-fiber ultrashort cavity configuration.","The GHz single-cavity all-fiber dual-comb source is seeded by a dual-wavelength mode-locked fiber laser operating at fundamental repetition rates of about 1.0 GHz differing by 148 kHz, which has an excellent stability in the free-running state that the Allan deviation is only 101.7 mHz for an average time of 1 second.","Thanks to the large repetition rate difference between the asynchronous dichromatic pulse trains, the GHz DCS enables a refresh time as short as 6.75 us, making it promising for studying nonrepeatable transient phenomena in real time.","To this end, the practicality of the present GHz DCS is validated by successfully capturing the 'shock waves' of balloon and firecracker explosions outdoors.","This GHz single-cavity all-fiber dual-comb system promises a noteworthy improvement in acquisition speed and reliability without sacrificing measurement accuracy, anticipated as a practical tool for high-speed applications."],"url":"http://arxiv.org/abs/2404.12176v1","category":"physics.optics"}
{"created":"2024-04-18 13:31:30","title":"Quasiparticle cooling algorithms for quantum many-body state preparation","abstract":"Probing correlated states of many-body systems is one of the central tasks for quantum simulators and processors. A promising approach to state preparation is to realize desired correlated states as steady states of engineered dissipative evolution. A recent experiment with a Google superconducting quantum processor [X. Mi et al., Science 383, 1332 (2024)] demonstrated a cooling algorithm utilizing auxiliary degrees of freedom that are periodically reset to remove quasiparticles from the system, thereby driving it towards the ground state. We develop a kinetic theory framework to describe quasiparticle cooling dynamics, and employ it to compare the efficiency of different cooling algorithms. In particular, we introduce a protocol where coupling to auxiliaries is modulated in time to minimize heating processes, and demonstrate that it allows a high-fidelity preparation of ground states in different quantum phases. We verify the validity of the kinetic theory description by an extensive comparison with numerical simulations of a 1d transverse-field Ising model using a solvable model and tensor-network techniques. Further, the effect of noise, which limits efficiency of variational quantum algorithms in near-term quantum processors, can be naturally described within the kinetic theory. We investigate the steady state quasiparticle population as a function of noise strength, and establish maximum noise values for achieving high-fidelity ground states. This work establishes quasiparticle cooling algorithms as a practical, robust method for many-body state preparation on near-term quantum processors.","sentences":["Probing correlated states of many-body systems is one of the central tasks for quantum simulators and processors.","A promising approach to state preparation is to realize desired correlated states as steady states of engineered dissipative evolution.","A recent experiment with a Google superconducting quantum processor [X. Mi et al., Science 383, 1332 (2024)] demonstrated a cooling algorithm utilizing auxiliary degrees of freedom that are periodically reset to remove quasiparticles from the system, thereby driving it towards the ground state.","We develop a kinetic theory framework to describe quasiparticle cooling dynamics, and employ it to compare the efficiency of different cooling algorithms.","In particular, we introduce a protocol where coupling to auxiliaries is modulated in time to minimize heating processes, and demonstrate that it allows a high-fidelity preparation of ground states in different quantum phases.","We verify the validity of the kinetic theory description by an extensive comparison with numerical simulations of a 1d transverse-field Ising model using a solvable model and tensor-network techniques.","Further, the effect of noise, which limits efficiency of variational quantum algorithms in near-term quantum processors, can be naturally described within the kinetic theory.","We investigate the steady state quasiparticle population as a function of noise strength, and establish maximum noise values for achieving high-fidelity ground states.","This work establishes quasiparticle cooling algorithms as a practical, robust method for many-body state preparation on near-term quantum processors."],"url":"http://arxiv.org/abs/2404.12175v1","category":"quant-ph"}
{"created":"2024-04-18 13:31:05","title":"Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?","abstract":"The increasing threat of disinformation calls for automating parts of the fact-checking pipeline. Identifying text segments requiring fact-checking is known as claim detection (CD) and claim check-worthiness detection (CW), the latter incorporating complex domain-specific criteria of worthiness and often framed as a ranking task. Zero- and few-shot LLM prompting is an attractive option for both tasks, as it bypasses the need for labeled datasets and allows verbalized claim and worthiness criteria to be directly used for prompting. We evaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets from diverse domains, each utilizing a different worthiness criterion. We investigate two key aspects: (1) how best to distill factuality and worthiness criteria into a prompt and (2) what amount of context to provide for each claim. To this end, we experiment with varying the level of prompt verbosity and the amount of contextual information provided to the model. Our results show that optimal prompt verbosity is domain-dependent, adding context does not improve performance, and confidence scores can be directly used to produce reliable check-worthiness rankings.","sentences":["The increasing threat of disinformation calls for automating parts of the fact-checking pipeline.","Identifying text segments requiring fact-checking is known as claim detection (CD) and claim check-worthiness detection (CW), the latter incorporating complex domain-specific criteria of worthiness and often framed as a ranking task.","Zero- and few-shot LLM prompting is an attractive option for both tasks, as it bypasses the need for labeled datasets and allows verbalized claim and worthiness criteria to be directly used for prompting.","We evaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets from diverse domains, each utilizing a different worthiness criterion.","We investigate two key aspects: (1) how best to distill factuality and worthiness criteria into a prompt and (2) what amount of context to provide for each claim.","To this end, we experiment with varying the level of prompt verbosity and the amount of contextual information provided to the model.","Our results show that optimal prompt verbosity is domain-dependent, adding context does not improve performance, and confidence scores can be directly used to produce reliable check-worthiness rankings."],"url":"http://arxiv.org/abs/2404.12174v1","category":"cs.CL"}
{"created":"2024-04-18 12:56:54","title":"Vortex motion in reconfigurable three-dimensional superconducting nanoarchitectures","abstract":"When materials are patterned in three dimensions, there exist opportunities to tailor and create functionalities associated with an increase in complexity, the breaking of symmetries, and the introduction of curvature and non-trivial topologies. For superconducting nanostructures, the extension to the third dimension may trigger the emergence of new physical phenomena, as well as advances in technologies. Here, we harness three-dimensional (3D) nanopatterning to fabricate and control the emergent properties of a 3D superconducting nanostructure. Not only are we able to demonstrate the existence and motion of superconducting vortices in 3D but, with simulations, we show that the confinement leads to a well-defined bending of the vortices within the volume of the structure. Moreover, we experimentally observe a strong geometrical anisotropy of the critical field, through which we achieve the reconfigurable coexistence of superconducting and normal states in our 3D superconducting architecture, and the local definition of weak links. In this way, we uncover an intermediate regime of nanosuperconductivity, where the vortex state is truly three-dimensional and can be designed and manipulated by geometrical confinement. This insight into the influence of 3D geometries on superconducting properties offers a route to local reconfigurable control for future computing devices, sensors, and quantum technologies.","sentences":["When materials are patterned in three dimensions, there exist opportunities to tailor and create functionalities associated with an increase in complexity, the breaking of symmetries, and the introduction of curvature and non-trivial topologies.","For superconducting nanostructures, the extension to the third dimension may trigger the emergence of new physical phenomena, as well as advances in technologies.","Here, we harness three-dimensional (3D) nanopatterning to fabricate and control the emergent properties of a 3D superconducting nanostructure.","Not only are we able to demonstrate the existence and motion of superconducting vortices in 3D but, with simulations, we show that the confinement leads to a well-defined bending of the vortices within the volume of the structure.","Moreover, we experimentally observe a strong geometrical anisotropy of the critical field, through which we achieve the reconfigurable coexistence of superconducting and normal states in our 3D superconducting architecture, and the local definition of weak links.","In this way, we uncover an intermediate regime of nanosuperconductivity, where the vortex state is truly three-dimensional and can be designed and manipulated by geometrical confinement.","This insight into the influence of 3D geometries on superconducting properties offers a route to local reconfigurable control for future computing devices, sensors, and quantum technologies."],"url":"http://arxiv.org/abs/2404.12151v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-18 12:35:39","title":"mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture","abstract":"The escalating complexity of micro-services architecture in cloud-native technologies poses significant challenges for maintaining system stability and efficiency. To conduct root cause analysis (RCA) and resolution of alert events, we propose a pioneering framework, multi-Agent Blockchain-inspired Collaboration for root cause analysis in micro-services architecture (mABC), to revolutionize the AI for IT operations (AIOps) domain, where multiple agents based on the powerful large language models (LLMs) perform blockchain-inspired voting to reach a final agreement following a standardized process for processing tasks and queries provided by Agent Workflow. Specifically, seven specialized agents derived from Agent Workflow each provide valuable insights towards root cause analysis based on their expertise and the intrinsic software knowledge of LLMs collaborating within a decentralized chain. To avoid potential instability issues in LLMs and fully leverage the transparent and egalitarian advantages inherent in a decentralized structure, mABC adopts a decision-making process inspired by blockchain governance principles while considering the contribution index and expertise index of each agent. Experimental results on the public benchmark AIOps challenge dataset and our created train-ticket dataset demonstrate superior performance in accurately identifying root causes and formulating effective solutions, compared to previous strong baselines. The ablation study further highlights the significance of each component within mABC, with Agent Workflow, multi-agent, and blockchain-inspired voting being crucial for achieving optimal performance. mABC offers a comprehensive automated root cause analysis and resolution in micro-services architecture and achieves a significant improvement in the AIOps domain compared to existing baselines","sentences":["The escalating complexity of micro-services architecture in cloud-native technologies poses significant challenges for maintaining system stability and efficiency.","To conduct root cause analysis (RCA) and resolution of alert events, we propose a pioneering framework, multi-Agent Blockchain-inspired Collaboration for root cause analysis in micro-services architecture (mABC), to revolutionize the AI for IT operations (AIOps) domain, where multiple agents based on the powerful large language models (LLMs) perform blockchain-inspired voting to reach a final agreement following a standardized process for processing tasks and queries provided by Agent Workflow.","Specifically, seven specialized agents derived from Agent Workflow each provide valuable insights towards root cause analysis based on their expertise and the intrinsic software knowledge of LLMs collaborating within a decentralized chain.","To avoid potential instability issues in LLMs and fully leverage the transparent and egalitarian advantages inherent in a decentralized structure, mABC adopts a decision-making process inspired by blockchain governance principles while considering the contribution index and expertise index of each agent.","Experimental results on the public benchmark AIOps challenge dataset and our created train-ticket dataset demonstrate superior performance in accurately identifying root causes and formulating effective solutions, compared to previous strong baselines.","The ablation study further highlights the significance of each component within mABC, with Agent Workflow, multi-agent, and blockchain-inspired voting being crucial for achieving optimal performance.","mABC offers a comprehensive automated root cause analysis and resolution in micro-services architecture and achieves a significant improvement in the AIOps domain compared to existing baselines"],"url":"http://arxiv.org/abs/2404.12135v1","category":"cs.MA"}
{"created":"2024-04-18 12:33:17","title":"Continued-fraction characterization of Stieltjes moment sequences with support in $[\u03be, \\infty)$","abstract":"We give a continued-fraction characterization of Stieltjes moment sequences for which there exists a representing measure with support in $[\\xi, \\infty)$. The proof is elementary.","sentences":["We give a continued-fraction characterization of Stieltjes moment sequences for which there exists a representing measure with support in $[\\xi, \\infty)$. The proof is elementary."],"url":"http://arxiv.org/abs/2404.12131v1","category":"math.CA"}
{"created":"2024-04-18 12:29:03","title":"Optimizing Intensive Database Tasks Through Caching Proxy Mechanisms","abstract":"Web caching is essential for the World Wide Web, saving processing power, bandwidth, and reducing latency. Many proxy caching solutions focus on buffering data from the main server, neglecting cacheable information meant for server writes. Existing systems addressing this issue are often intrusive, requiring modifications to the main application for integration. We identify opportunities for enhancement in conventional caching proxies. This paper explores, designs, and implements a potential prototype for such an application. Our focus is on harnessing a faster bulk-data-write approach compared to single-data-write within the context of relational databases. If a (upload) request matches a specified cacheable URL, then the data will be extracted and buffered on the local disk for later bulk-write. In contrast with already existing caching proxies, Squid, for example, in a similar uploading scenario, the request would simply get redirected, leaving out potential gains such as minimized processing power, lower server load, and bandwidth. After prototyping and testing the suggested application against Squid, concerning data uploads with 1, 100, 1.000, ..., and 100.000 requests, we consistently observed query execution improvements ranging from 5 to 9 times. This enhancement was achieved through buffering and bulk-writing the data, the extent of which depended on the specific test conditions.","sentences":["Web caching is essential for the World Wide Web, saving processing power, bandwidth, and reducing latency.","Many proxy caching solutions focus on buffering data from the main server, neglecting cacheable information meant for server writes.","Existing systems addressing this issue are often intrusive, requiring modifications to the main application for integration.","We identify opportunities for enhancement in conventional caching proxies.","This paper explores, designs, and implements a potential prototype for such an application.","Our focus is on harnessing a faster bulk-data-write approach compared to single-data-write within the context of relational databases.","If a (upload) request matches a specified cacheable URL, then the data will be extracted and buffered on the local disk for later bulk-write.","In contrast with already existing caching proxies, Squid, for example, in a similar uploading scenario, the request would simply get redirected, leaving out potential gains such as minimized processing power, lower server load, and bandwidth.","After prototyping and testing the suggested application against Squid, concerning data uploads with 1, 100, 1.000, ..., and 100.000 requests, we consistently observed query execution improvements ranging from 5 to 9 times.","This enhancement was achieved through buffering and bulk-writing the data, the extent of which depended on the specific test conditions."],"url":"http://arxiv.org/abs/2404.12128v1","category":"cs.DB"}
{"created":"2024-04-18 12:22:19","title":"Robust and Adaptive Deep Reinforcement Learning for Enhancing Flow Control around a Square Cylinder with Varying Reynolds Numbers","abstract":"The present study applies a Deep Reinforcement Learning (DRL) algorithm to Active Flow Control (AFC) of a two-dimensional flow around a confined square cylinder. Specifically, the Soft Actor-Critic (SAC) algorithm is employed to modulate the flow of a pair of synthetic jets placed on the upper and lower surfaces of the confined squared cylinder in flow configurations characterized by Re of 100, 200, 300, and 400. The investigation starts with an analysis of the baseline flow in the absence of active control. It is observed that at Re = 100 and Re = 200, the vortex shedding exhibits mono-frequency characteristics. Conversely, at Re = 300 and Re = 400, the vortex shedding is dominated by multiple frequencies, which is indicative of more complex flow features. With the application of the SAC algorithm, we demonstrate the capability of DRL-based control in effectively suppressing vortex shedding, while significantly diminishing drag and fluctuations in lift. Quantitatively, the data-driven active control strategy results in a drag reduction of approximately 14.4%, 26.4%, 38.9%, and 47.0% for Re = 100, 200, 300, and 400, respectively. To understand the underlying control mechanism, we also present detailed flow field comparisons, which showcase the adaptability of DRL in devising distinct control strategies tailored to the dynamic conditions at varying Re. These findings substantiate the proficiency of DRL in controlling chaotic, multi-frequency dominated vortex shedding phenomena, underscoring the robustness of DRL in complex AFC problems.","sentences":["The present study applies a Deep Reinforcement Learning (DRL) algorithm to Active Flow Control (AFC) of a two-dimensional flow around a confined square cylinder.","Specifically, the Soft Actor-Critic (SAC) algorithm is employed to modulate the flow of a pair of synthetic jets placed on the upper and lower surfaces of the confined squared cylinder in flow configurations characterized by Re of 100, 200, 300, and 400.","The investigation starts with an analysis of the baseline flow in the absence of active control.","It is observed that at Re = 100 and Re = 200, the vortex shedding exhibits mono-frequency characteristics.","Conversely, at Re = 300 and Re = 400, the vortex shedding is dominated by multiple frequencies, which is indicative of more complex flow features.","With the application of the SAC algorithm, we demonstrate the capability of DRL-based control in effectively suppressing vortex shedding, while significantly diminishing drag and fluctuations in lift.","Quantitatively, the data-driven active control strategy results in a drag reduction of approximately 14.4%, 26.4%, 38.9%, and 47.0% for Re = 100, 200, 300, and 400, respectively.","To understand the underlying control mechanism, we also present detailed flow field comparisons, which showcase the adaptability of DRL in devising distinct control strategies tailored to the dynamic conditions at varying Re.","These findings substantiate the proficiency of DRL in controlling chaotic, multi-frequency dominated vortex shedding phenomena, underscoring the robustness of DRL in complex AFC problems."],"url":"http://arxiv.org/abs/2404.12123v1","category":"physics.flu-dyn"}
{"created":"2024-04-18 12:11:18","title":"Quantum thermodynamics of the spin-boson model using the principle of minimal dissipation","abstract":"A recently developed approach to the thermodynamics of open quantum systems, on the basis of the principle of minimal dissipation, is applied to the spin-boson model. Employing a numerically exact quantum dynamical treatment based on the hierarchical equations of motion (HEOM) method, we investigate the influence of the environment on quantities such as work, heat and entropy production in a range of parameters which go beyond the weak-coupling limit and include both the non-adiabatic and the adiabatic regimes. The results reveal significant differences to the weak-coupling forms of work, heat and entropy production, which are analyzed in some detail.","sentences":["A recently developed approach to the thermodynamics of open quantum systems, on the basis of the principle of minimal dissipation, is applied to the spin-boson model.","Employing a numerically exact quantum dynamical treatment based on the hierarchical equations of motion (HEOM) method, we investigate the influence of the environment on quantities such as work, heat and entropy production in a range of parameters which go beyond the weak-coupling limit and include both the non-adiabatic and the adiabatic regimes.","The results reveal significant differences to the weak-coupling forms of work, heat and entropy production, which are analyzed in some detail."],"url":"http://arxiv.org/abs/2404.12118v1","category":"quant-ph"}
{"created":"2024-04-18 11:54:10","title":"Optical anisotropy of the kagome magnet FeSn: Dominant role of excitations between kagome and Sn layers","abstract":"Antiferromagnetic FeSn is considered to be a close realization of the ideal two-dimensional (2D) kagome lattice, hosting Dirac cones, van Hove singularities, and flat bands, as it comprises Fe$_3$Sn kagome layers well separated by Sn buffer layers. We observe a pronounced optical anisotropy, with the low-energy optical conductivity being surprisingly higher perpendicular to the kagome planes than along the layers. This finding contradicts the prevalent picture of dominantly 2D electronic structure for FeSn. Our material-specific theory reproduces the measured conductivity spectra remarkarbly well. A site-specific decomposition of the optical response to individual excitation channels shows that the optical conductivity for polarizations both parallel and perpendicular to the kagome plane is dominated by interlayer transitions between kagome layers and adjacent Sn-based layers. Moreover, the matrix elements corresponding to these transitions are highly anisotropic, leading to larger out-of-plane conductivity. Our results evidence the crucial role of interstitial layers in charge dynamics even in seemingly 2D systems.","sentences":["Antiferromagnetic FeSn is considered to be a close realization of the ideal two-dimensional (2D) kagome lattice, hosting Dirac cones, van Hove singularities, and flat bands, as it comprises Fe$_3$Sn kagome layers well separated by Sn buffer layers.","We observe a pronounced optical anisotropy, with the low-energy optical conductivity being surprisingly higher perpendicular to the kagome planes than along the layers.","This finding contradicts the prevalent picture of dominantly 2D electronic structure for FeSn.","Our material-specific theory reproduces the measured conductivity spectra remarkarbly well.","A site-specific decomposition of the optical response to individual excitation channels shows that the optical conductivity for polarizations both parallel and perpendicular to the kagome plane is dominated by interlayer transitions between kagome layers and adjacent Sn-based layers.","Moreover, the matrix elements corresponding to these transitions are highly anisotropic, leading to larger out-of-plane conductivity.","Our results evidence the crucial role of interstitial layers in charge dynamics even in seemingly 2D systems."],"url":"http://arxiv.org/abs/2404.12111v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 11:53:19","title":"Saturation of the compression of two interacting magnetic flux tubes evidenced in the laboratory","abstract":"Interactions between magnetic fields advected by matter play a fundamental role in the Universe at all possible scales. A crucial role these interactions play is in making turbulent fields highly anisotropic, leading to observed ordered fields. These in turn, are important evolutionary factors for all the systems within and around. Despite scant evidence, due to the difficulty in measuring even near-Earth events, the magnetic field compression factor in these interactions, measured at very varied scales, is limited to a few. However, compressing matter in which a magnetic field is embedded, results in compression up to several thousands. Here we show, using laboratory experiments and matching three-dimensional hybrid simulations, that there is indeed a very effective saturation of the compression when two independent magnetic fields are advected by plasmas encounter. We found that the observed saturation is linked to a build-up of the magnetic pressure at the inflows encounter point, which decelerates them and thereby stops further compression. Moreover, the growth of an electric field, induced by the incoming flows and the magnetic field, acts in redirecting the inflows transversely, further hampering field compression.","sentences":["Interactions between magnetic fields advected by matter play a fundamental role in the Universe at all possible scales.","A crucial role these interactions play is in making turbulent fields highly anisotropic, leading to observed ordered fields.","These in turn, are important evolutionary factors for all the systems within and around.","Despite scant evidence, due to the difficulty in measuring even near-Earth events, the magnetic field compression factor in these interactions, measured at very varied scales, is limited to a few.","However, compressing matter in which a magnetic field is embedded, results in compression up to several thousands.","Here we show, using laboratory experiments and matching three-dimensional hybrid simulations, that there is indeed a very effective saturation of the compression when two independent magnetic fields are advected by plasmas encounter.","We found that the observed saturation is linked to a build-up of the magnetic pressure at the inflows encounter point, which decelerates them and thereby stops further compression.","Moreover, the growth of an electric field, induced by the incoming flows and the magnetic field, acts in redirecting the inflows transversely, further hampering field compression."],"url":"http://arxiv.org/abs/2404.12110v1","category":"physics.plasm-ph"}
{"created":"2024-04-18 11:47:46","title":"Effective Individual Fairest Community Search over Heterogeneous Information Networks","abstract":"Community search over heterogeneous information networks has been applied to wide domains, such as activity organization and team formation. From these scenarios, the members of a group with the same treatment often have different levels of activity and workloads, which causes unfairness in the treatment between active members and inactive members (called individual unfairness). However, existing works do not pay attention to individual fairness and do not sufficiently consider the rich semantics of HINs (e.g., high-order structure), which disables complex queries. To fill the gap, we formally define the issue of individual fairest community search over HINs (denoted as IFCS), which aims to find a set of vertices from the HIN that own the same type, close relationships, and small difference of activity level and has been demonstrated to be NP-hard. To do this, we first develop an exploration-based filter that reduces the search space of the community effectively. Further, to avoid repeating computation and prune unfair communities in advance, we propose a message-based scheme and a lower bound-based scheme. At last, we conduct extensive experiments on four real-world datasets to demonstrate the effectiveness and efficiency of our proposed algorithms, which achieve at least X3 times faster than the baseline solution.","sentences":["Community search over heterogeneous information networks has been applied to wide domains, such as activity organization and team formation.","From these scenarios, the members of a group with the same treatment often have different levels of activity and workloads, which causes unfairness in the treatment between active members and inactive members (called individual unfairness).","However, existing works do not pay attention to individual fairness and do not sufficiently consider the rich semantics of HINs (e.g., high-order structure), which disables complex queries.","To fill the gap, we formally define the issue of individual fairest community search over HINs (denoted as IFCS), which aims to find a set of vertices from the HIN that own the same type, close relationships, and small difference of activity level and has been demonstrated to be NP-hard.","To do this, we first develop an exploration-based filter that reduces the search space of the community effectively.","Further, to avoid repeating computation and prune unfair communities in advance, we propose a message-based scheme and a lower bound-based scheme.","At last, we conduct extensive experiments on four real-world datasets to demonstrate the effectiveness and efficiency of our proposed algorithms, which achieve at least X3 times faster than the baseline solution."],"url":"http://arxiv.org/abs/2404.12107v1","category":"cs.DB"}
{"created":"2024-04-18 11:43:57","title":"Landau-Lifschitz magnets: exact thermodynamics and transport","abstract":"The classical Landau--Lifshitz equation -- the simplest model of a ferromagnet -- provides an archetypal example for studying transport phenomena. In one-spatial dimension, integrability enables the classification of the spectrum of linear and nonlinear modes. An exact characterization of finite-temperature thermodynamics and transport has nonetheless remained elusive. We present an exact description of thermodynamic equilibrium states in terms of interacting modes. This is achieved by retrieving the classical Landau--Lifschitz model through the semiclassical limit of the integrable quantum spin-$S$ anisotropic Heisenberg chain at the level of the thermodynamic Bethe ansatz description. In the axial regime, the mode spectrum comprises solitons with unconventional statistics, whereas in the planar regime we additionally find two special types of modes of radiative and solitonic type. The obtained framework paves the way for analytical study of unconventional transport properties: as an example we study the finite-temperature spin Drude weight, finding excellent agreement with Monte Carlo simulations.","sentences":["The classical Landau--Lifshitz equation -- the simplest model of a ferromagnet -- provides an archetypal example for studying transport phenomena.","In one-spatial dimension, integrability enables the classification of the spectrum of linear and nonlinear modes.","An exact characterization of finite-temperature thermodynamics and transport has nonetheless remained elusive.","We present an exact description of thermodynamic equilibrium states in terms of interacting modes.","This is achieved by retrieving the classical Landau--Lifschitz model through the semiclassical limit of the integrable quantum spin-$S$ anisotropic Heisenberg chain at the level of the thermodynamic Bethe ansatz description.","In the axial regime, the mode spectrum comprises solitons with unconventional statistics, whereas in the planar regime we additionally find two special types of modes of radiative and solitonic type.","The obtained framework paves the way for analytical study of unconventional transport properties: as an example we study the finite-temperature spin Drude weight, finding excellent agreement with Monte Carlo simulations."],"url":"http://arxiv.org/abs/2404.12106v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-18 11:33:48","title":"Dynamics Over Homogeneous Spaces","abstract":"We present the Euler-Lagrange and Hamilton's equations for a system whose configuration space is a unified product Lie group $G=M\\bowtie_{\\gamma} H$, for some $\\gamma:M\\times M \\to H$. By reduction, then, we obtain the Euler-Lagrange type and Hamilton's type equations of the same form for the quotient space $M\\cong G/H$, although it is not necessarily a Lie group. We observe, through further reduction, that it is possible to formulate the Euler-Poincar\\'{e} type and Lie-Poisson type equations on the corresponding quotient $\\mathfrak{m}\\cong \\mathfrak{g}/\\mathfrak{h}$ of Lie algebras, which is not a priori a Lie algebra. Moreover, we realize the $n$th order iterated tangent group $T^{(n)}G$ of a Lie group $G$ as an extension of the $n$th order tangent group $T^nG$ of the same type. More precisely, $\\mathfrak{g}$ being the Lie algebra of $G$, $T^{(n)}G \\cong \\mathfrak{g}^{\\times \\,2^n-1-n} \\bowtie_\\gamma T^nG$ for some $\\gamma:\\mathfrak{g}^{\\times \\,2^n-1-n} \\times \\mathfrak{g}^{\\times \\,2^n-1-n} \\to T^nG$. We thus obtain the $n$th order Euler-Lagrange (and then the $n$th order Euler-Poincar\\'e) equations over $T^nG$ by reduction from those on $T(T^{n-1}G)$. Finally, we illustrate our results in the realm of the Kepler problem, and the non-linear tokamak plasma dynamics.","sentences":["We present the Euler-Lagrange and Hamilton's equations for a system whose configuration space is a unified product Lie group $G=M\\bowtie_{\\gamma} H$, for some $\\gamma:M\\times M \\to H$. By reduction, then, we obtain the Euler-Lagrange type and Hamilton's type equations of the same form for the quotient space $M\\cong G/H$, although it is not necessarily a Lie group.","We observe, through further reduction, that it is possible to formulate the Euler-Poincar\\'{e} type and Lie-Poisson type equations on the corresponding quotient $\\mathfrak{m}\\cong \\mathfrak{g}/\\mathfrak{h}$ of Lie algebras, which is not a priori a Lie algebra.","Moreover, we realize the $n$th order iterated tangent group $T^{(n)}G$ of a Lie group $G$ as an extension of the $n$th order tangent group $T^nG$ of the same type.","More precisely, $\\mathfrak{g}$ being the Lie algebra of $G$,","$T^{(n)}G \\cong \\mathfrak{g}^{\\times \\,2^n-1-n} \\bowtie_\\gamma T^nG$ for some $\\gamma:\\mathfrak{g}^{\\times \\,2^n-1-n} \\times \\mathfrak{g}^{\\times \\,2^n-1-n} \\to T^nG$. We thus obtain the $n$th order Euler-Lagrange (and then the $n$th order Euler-Poincar\\'e) equations over $T^nG$ by reduction from those on $T(T^{n-1}G)$.","Finally, we illustrate our results in the realm of the Kepler problem, and the non-linear tokamak plasma dynamics."],"url":"http://arxiv.org/abs/2404.12101v1","category":"math.DG"}
{"created":"2024-04-18 11:29:43","title":"MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast Adaptation of Neural Predictive Models","abstract":"In this paper, we consider the problem of reference tracking in uncertain nonlinear systems. A neural State-Space Model (NSSM) is used to approximate the nonlinear system, where a deep encoder network learns the nonlinearity from data, and a state-space component captures the temporal relationship. This transforms the nonlinear system into a linear system in a latent space, enabling the application of model predictive control (MPC) to determine effective control actions. Our objective is to design the optimal controller using limited data from the \\textit{target system} (the system of interest). To this end, we employ an implicit model-agnostic meta-learning (iMAML) framework that leverages information from \\textit{source systems} (systems that share similarities with the target system) to expedite training in the target system and enhance its control performance. The framework consists of two phases: the (offine) meta-training phase learns a aggregated NSSM using data from source systems, and the (online) meta-inference phase quickly adapts this aggregated model to the target system using only a few data points and few online training iterations, based on local loss function gradients. The iMAML algorithm exploits the implicit function theorem to exactly compute the gradient during training, without relying on the entire optimization path. By focusing solely on the optimal solution, rather than the path, we can meta-train with less storage complexity and fewer approximations than other contemporary meta-learning algorithms. We demonstrate through numerical examples that our proposed method can yield accurate predictive models by adaptation, resulting in a downstream MPC that outperforms several baselines.","sentences":["In this paper, we consider the problem of reference tracking in uncertain nonlinear systems.","A neural State-Space Model (NSSM) is used to approximate the nonlinear system, where a deep encoder network learns the nonlinearity from data, and a state-space component captures the temporal relationship.","This transforms the nonlinear system into a linear system in a latent space, enabling the application of model predictive control (MPC) to determine effective control actions.","Our objective is to design the optimal controller using limited data from the \\textit{target system} (the system of interest).","To this end, we employ an implicit model-agnostic meta-learning (iMAML) framework that leverages information from \\textit{source systems} (systems that share similarities with the target system) to expedite training in the target system and enhance its control performance.","The framework consists of two phases: the (offine) meta-training phase learns a aggregated NSSM using data from source systems, and the (online) meta-inference phase quickly adapts this aggregated model to the target system using only a few data points and few online training iterations, based on local loss function gradients.","The iMAML algorithm exploits the implicit function theorem to exactly compute the gradient during training, without relying on the entire optimization path.","By focusing solely on the optimal solution, rather than the path, we can meta-train with less storage complexity and fewer approximations than other contemporary meta-learning algorithms.","We demonstrate through numerical examples that our proposed method can yield accurate predictive models by adaptation, resulting in a downstream MPC that outperforms several baselines."],"url":"http://arxiv.org/abs/2404.12097v1","category":"eess.SY"}
{"created":"2024-04-18 11:24:12","title":"Evaluating the Security of Merkle Trees in the Internet of Things: An Analysis of Data Falsification Probabilities","abstract":"Addressing the critical challenge of ensuring data integrity in decentralized systems, this paper delves into the underexplored area of data falsification probabilities within Merkle Trees, which are pivotal in blockchain and Internet of Things (IoT) technologies. Despite their widespread use, a comprehensive understanding of the probabilistic aspects of data security in these structures remains a gap in current research. Our study aims to bridge this gap by developing a theoretical framework to calculate the probability of data falsification, taking into account various scenarios based on the length of the Merkle path and hash length. The research progresses from the derivation of an exact formula for falsification probability to an approximation suitable for cases with significantly large hash lengths. Empirical experiments validate the theoretical models, exploring simulations with diverse hash lengths and Merkle path lengths. The findings reveal a decrease in falsification probability with increasing hash length and an inverse relationship with longer Merkle paths. A numerical analysis quantifies the discrepancy between exact and approximate probabilities, underscoring the conditions for the effective application of the approximation. This work offers crucial insights into optimizing Merkle Tree structures for bolstering security in blockchain and IoT systems, achieving a balance between computational efficiency and data integrity.","sentences":["Addressing the critical challenge of ensuring data integrity in decentralized systems, this paper delves into the underexplored area of data falsification probabilities within Merkle Trees, which are pivotal in blockchain and Internet of Things (IoT) technologies.","Despite their widespread use, a comprehensive understanding of the probabilistic aspects of data security in these structures remains a gap in current research.","Our study aims to bridge this gap by developing a theoretical framework to calculate the probability of data falsification, taking into account various scenarios based on the length of the Merkle path and hash length.","The research progresses from the derivation of an exact formula for falsification probability to an approximation suitable for cases with significantly large hash lengths.","Empirical experiments validate the theoretical models, exploring simulations with diverse hash lengths and Merkle path lengths.","The findings reveal a decrease in falsification probability with increasing hash length and an inverse relationship with longer Merkle paths.","A numerical analysis quantifies the discrepancy between exact and approximate probabilities, underscoring the conditions for the effective application of the approximation.","This work offers crucial insights into optimizing Merkle Tree structures for bolstering security in blockchain and IoT systems, achieving a balance between computational efficiency and data integrity."],"url":"http://arxiv.org/abs/2404.12093v1","category":"cs.CR"}
{"created":"2024-04-18 11:13:17","title":"Commutative Algebra and Algebraic Geometry using OSCAR","abstract":"We give illustrative examples of how the computer algebra system OSCAR can support research in commutative algebra and algebraic geometry. We start with a thorough introduction to Groebner basis techniques, with particular emphasis on the computation of syzygies, then apply these techniques to deal with ideal and ring theoretic concepts such as primary decomposition and normalization, and finally use them for geometric case studies which concern curves and surfaces, both from a local and global point of view.","sentences":["We give illustrative examples of how the computer algebra system OSCAR can support research in commutative algebra and algebraic geometry.","We start with a thorough introduction to Groebner basis techniques, with particular emphasis on the computation of syzygies, then apply these techniques to deal with ideal and ring theoretic concepts such as primary decomposition and normalization, and finally use them for geometric case studies which concern curves and surfaces, both from a local and global point of view."],"url":"http://arxiv.org/abs/2404.12085v1","category":"math.AG"}
{"created":"2024-04-18 11:09:25","title":"MambaPupil: Bidirectional Selective Recurrent model for Event-based Eye tracking","abstract":"Event-based eye tracking has shown great promise with the high temporal resolution and low redundancy provided by the event camera. However, the diversity and abruptness of eye movement patterns, including blinking, fixating, saccades, and smooth pursuit, pose significant challenges for eye localization. To achieve a stable event-based eye-tracking system, this paper proposes a bidirectional long-term sequence modeling and time-varying state selection mechanism to fully utilize contextual temporal information in response to the variability of eye movements. Specifically, the MambaPupil network is proposed, which consists of the multi-layer convolutional encoder to extract features from the event representations, a bidirectional Gated Recurrent Unit (GRU), and a Linear Time-Varying State Space Module (LTV-SSM), to selectively capture contextual correlation from the forward and backward temporal relationship. Furthermore, the Bina-rep is utilized as a compact event representation, and the tailor-made data augmentation, called as Event-Cutout, is proposed to enhance the model's robustness by applying spatial random masking to the event image. The evaluation on the ThreeET-plus benchmark shows the superior performance of the MambaPupil, which secured the 1st place in CVPR'2024 AIS Event-based Eye Tracking challenge.","sentences":["Event-based eye tracking has shown great promise with the high temporal resolution and low redundancy provided by the event camera.","However, the diversity and abruptness of eye movement patterns, including blinking, fixating, saccades, and smooth pursuit, pose significant challenges for eye localization.","To achieve a stable event-based eye-tracking system, this paper proposes a bidirectional long-term sequence modeling and time-varying state selection mechanism to fully utilize contextual temporal information in response to the variability of eye movements.","Specifically, the MambaPupil network is proposed, which consists of the multi-layer convolutional encoder to extract features from the event representations, a bidirectional Gated Recurrent Unit (GRU), and a Linear Time-Varying State Space Module (LTV-SSM), to selectively capture contextual correlation from the forward and backward temporal relationship.","Furthermore, the Bina-rep is utilized as a compact event representation, and the tailor-made data augmentation, called as Event-Cutout, is proposed to enhance the model's robustness by applying spatial random masking to the event image.","The evaluation on the ThreeET-plus benchmark shows the superior performance of the MambaPupil, which secured the 1st place in CVPR'2024 AIS Event-based Eye Tracking challenge."],"url":"http://arxiv.org/abs/2404.12083v1","category":"cs.CV"}
{"created":"2024-04-18 11:02:18","title":"A Mathematical Formalisation of the \u03b3-contraction Problem","abstract":"Networks play an ubiquitous role in computer science and real-world applications, offering multiple kind of information that can be retrieved with adequate methods. With the continuous growing in the amount of data available, networks are becoming larger day by day. Consequently, the tasks that were easily achievable on smaller networks, often becomes impractical on huge amount of data, either due to the high computational cost or due to the impracticality to visualise corresponding data. Using distinctive node features to group large amount of connected data into a limited number of clusters, hence represented by a representative per cluster, proves to be a valuable approach. The resulting contracted graphs are more manageable in size and can reveal previously hidden characteristics of the original networks. Furthermore, in many real-world use cases, a definition of cluster is intrinsic with the data, eventually obtained with the injection of some expert knowledge represent by a categorical function. Clusters then results in set of connected vertices taking the same values in a finite set C. In the recent literature, Lombardi and Onofri proposed a novel, fast, and easily parallelisable approach under the name of $\\gamma$-contraction to contract a graph given a categorical function. In this work, we formally define such approach by providing a rigorous mathematical definition of the problem, which, to the best of our knowledge, was missing in the existing literature. Specifically, we explore the variadic nature of the contraction operation and use it to introduce the weaker version of the colour contraction, under the name of $\\beta$-contraction, that the algorithmic solution exploits. We finally dive into the details of the algorithm and we provide a full assesment on its convergence complexity relying on two constructive proofs that deeply unveil its mode of operation.","sentences":["Networks play an ubiquitous role in computer science and real-world applications, offering multiple kind of information that can be retrieved with adequate methods.","With the continuous growing in the amount of data available, networks are becoming larger day by day.","Consequently, the tasks that were easily achievable on smaller networks, often becomes impractical on huge amount of data, either due to the high computational cost or due to the impracticality to visualise corresponding data.","Using distinctive node features to group large amount of connected data into a limited number of clusters, hence represented by a representative per cluster, proves to be a valuable approach.","The resulting contracted graphs are more manageable in size and can reveal previously hidden characteristics of the original networks.","Furthermore, in many real-world use cases, a definition of cluster is intrinsic with the data, eventually obtained with the injection of some expert knowledge represent by a categorical function.","Clusters then results in set of connected vertices taking the same values in a finite set C. In the recent literature, Lombardi and Onofri proposed a novel, fast, and easily parallelisable approach under the name of $\\gamma$-contraction to contract a graph given a categorical function.","In this work, we formally define such approach by providing a rigorous mathematical definition of the problem, which, to the best of our knowledge, was missing in the existing literature.","Specifically, we explore the variadic nature of the contraction operation and use it to introduce the weaker version of the colour contraction, under the name of $\\beta$-contraction, that the algorithmic solution exploits.","We finally dive into the details of the algorithm and we provide a full assesment on its convergence complexity relying on two constructive proofs that deeply unveil its mode of operation."],"url":"http://arxiv.org/abs/2404.12080v1","category":"cs.DS"}
{"created":"2024-04-18 11:00:58","title":"The port-Hamiltonian structure of continuum mechanics","abstract":"In this paper we present a novel approach to the geometric formulation of solid and fluid mechanics within the port-Hamiltonian framework, which extends the standard Hamiltonian formulation to non-conservative and open dynamical systems. Leveraging Dirac structures, instead of symplectic or Poisson structures, this formalism allows the incorporation of energy exchange within the spatial domain or through its boundary, which allows for a more comprehensive description of continuum mechanics. Building upon our recent work in describing nonlinear elasticity using exterior calculus and bundle-valued differential forms, this paper focuses on the systematic derivation of port-Hamiltonian models for solid and fluid mechanics in the material, spatial, and convective representations using Hamiltonian reduction theory. This paper also discusses constitutive relations for stress within this framework including hyper-elasticity, for both finite- and infinite-strains, as well as viscous fluid flow governed by the Navier-Stokes equations.","sentences":["In this paper we present a novel approach to the geometric formulation of solid and fluid mechanics within the port-Hamiltonian framework, which extends the standard Hamiltonian formulation to non-conservative and open dynamical systems.","Leveraging Dirac structures, instead of symplectic or Poisson structures, this formalism allows the incorporation of energy exchange within the spatial domain or through its boundary, which allows for a more comprehensive description of continuum mechanics.","Building upon our recent work in describing nonlinear elasticity using exterior calculus and bundle-valued differential forms, this paper focuses on the systematic derivation of port-Hamiltonian models for solid and fluid mechanics in the material, spatial, and convective representations using Hamiltonian reduction theory.","This paper also discusses constitutive relations for stress within this framework including hyper-elasticity, for both finite- and infinite-strains, as well as viscous fluid flow governed by the Navier-Stokes equations."],"url":"http://arxiv.org/abs/2404.12078v1","category":"math-ph"}
{"created":"2024-04-18 10:54:14","title":"Transport of orbital currents in systems with strong intervalley coupling: the case of Kekul\u00e9 distorted graphene","abstract":"We show that orbital currents can describe the transport of orbital magnetic moments of Bloch states in models where the formalism based on valley current is not applicable. As a case study, we consider Kekul\\'e distorted graphene. We begin by analyzing the band structure in detail and obtain the orbital magnetic moment operator for this model within the framework of the modern theory of magnetism. Despite the simultaneous presence of time-reversal and spatial-inversion symmetries, such operator may be defined, although its expectation value at a given energy is zero. Nevertheless, its presence can be exposed by the application of an external magnetic field. We then proceed to study the transport of these quantities. In the Kekul\\'e-$O$ distorted graphene model, the strong coupling between different valleys prevents the definition of a bulk valley current. However, the formalism of the orbital Hall effect together with the non-Abelian description of the magnetic moment operator can be directly applied to describe its transport in these types of models. We show that the Kekul\\'e-$O$ distorted graphene model exhibits an orbital Hall insulating plateau whose height is inversely proportional to the energy band gap produced by intervalley coupling. Our results strengthen the perspective of using the orbital Hall effect formalism as a preferable alternative to the valley Hall effect","sentences":["We show that orbital currents can describe the transport of orbital magnetic moments of Bloch states in models where the formalism based on valley current is not applicable.","As a case study, we consider Kekul\\'e distorted graphene.","We begin by analyzing the band structure in detail and obtain the orbital magnetic moment operator for this model within the framework of the modern theory of magnetism.","Despite the simultaneous presence of time-reversal and spatial-inversion symmetries, such operator may be defined, although its expectation value at a given energy is zero.","Nevertheless, its presence can be exposed by the application of an external magnetic field.","We then proceed to study the transport of these quantities.","In the Kekul\\'e-$O$ distorted graphene model, the strong coupling between different valleys prevents the definition of a bulk valley current.","However, the formalism of the orbital Hall effect together with the non-Abelian description of the magnetic moment operator can be directly applied to describe its transport in these types of models.","We show that the Kekul\\'e-$O$ distorted graphene model exhibits an orbital Hall insulating plateau whose height is inversely proportional to the energy band gap produced by intervalley coupling.","Our results strengthen the perspective of using the orbital Hall effect formalism as a preferable alternative to the valley Hall effect"],"url":"http://arxiv.org/abs/2404.12072v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-18 10:52:32","title":"Complexity-Aware Theoretical Performance Analysis of SDM MIMO Equalizers","abstract":"We propose a theoretical framework to compute, rapidly and accurately, the signal-to-noise ratio at the output of spatial-division multiplexing (SDM) linear MIMO equalizers with arbitrary numbers of spatial modes and filter taps and demonstrate three orders of magnitude of speed-up compared to Monte Carlo simulations.","sentences":["We propose a theoretical framework to compute, rapidly and accurately, the signal-to-noise ratio at the output of spatial-division multiplexing (SDM) linear MIMO equalizers with arbitrary numbers of spatial modes and filter taps and demonstrate three orders of magnitude of speed-up compared to Monte Carlo simulations."],"url":"http://arxiv.org/abs/2404.12071v1","category":"cs.IT"}
{"created":"2024-04-18 10:21:28","title":"FastVPINNs: Tensor-Driven Acceleration of VPINNs for Complex Geometries","abstract":"Variational Physics-Informed Neural Networks (VPINNs) utilize a variational loss function to solve partial differential equations, mirroring Finite Element Analysis techniques. Traditional hp-VPINNs, while effective for high-frequency problems, are computationally intensive and scale poorly with increasing element counts, limiting their use in complex geometries. This work introduces FastVPINNs, a tensor-based advancement that significantly reduces computational overhead and improves scalability. Using optimized tensor operations, FastVPINNs achieve a 100-fold reduction in the median training time per epoch compared to traditional hp-VPINNs. With proper choice of hyperparameters, FastVPINNs surpass conventional PINNs in both speed and accuracy, especially in problems with high-frequency solutions. Demonstrated effectiveness in solving inverse problems on complex domains underscores FastVPINNs' potential for widespread application in scientific and engineering challenges, opening new avenues for practical implementations in scientific machine learning.","sentences":["Variational Physics-Informed Neural Networks (VPINNs) utilize a variational loss function to solve partial differential equations, mirroring Finite Element Analysis techniques.","Traditional hp-VPINNs, while effective for high-frequency problems, are computationally intensive and scale poorly with increasing element counts, limiting their use in complex geometries.","This work introduces FastVPINNs, a tensor-based advancement that significantly reduces computational overhead and improves scalability.","Using optimized tensor operations, FastVPINNs achieve a 100-fold reduction in the median training time per epoch compared to traditional hp-VPINNs.","With proper choice of hyperparameters, FastVPINNs surpass conventional PINNs in both speed and accuracy, especially in problems with high-frequency solutions.","Demonstrated effectiveness in solving inverse problems on complex domains underscores FastVPINNs' potential for widespread application in scientific and engineering challenges, opening new avenues for practical implementations in scientific machine learning."],"url":"http://arxiv.org/abs/2404.12063v1","category":"cs.LG"}
{"created":"2024-04-18 10:06:00","title":"emrQA-msquad: A Medical Dataset Structured with the SQuAD V2.0 Framework, Enriched with emrQA Medical Information","abstract":"Machine Reading Comprehension (MRC) holds a pivotal role in shaping Medical Question Answering Systems (QAS) and transforming the landscape of accessing and applying medical information. However, the inherent challenges in the medical field, such as complex terminology and question ambiguity, necessitate innovative solutions. One key solution involves integrating specialized medical datasets and creating dedicated datasets. This strategic approach enhances the accuracy of QAS, contributing to advancements in clinical decision-making and medical research. To address the intricacies of medical terminology, a specialized dataset was integrated, exemplified by a novel Span extraction dataset derived from emrQA but restructured into 163,695 questions and 4,136 manually obtained answers, this new dataset was called emrQA-msquad dataset. Additionally, for ambiguous questions, a dedicated medical dataset for the Span extraction task was introduced, reinforcing the system's robustness. The fine-tuning of models such as BERT, RoBERTa, and Tiny RoBERTa for medical contexts significantly improved response accuracy within the F1-score range of 0.75 to 1.00 from 10.1% to 37.4%, 18.7% to 44.7% and 16.0% to 46.8%, respectively. Finally, emrQA-msquad dataset is publicy available at https://huggingface.co/datasets/Eladio/emrqa-msquad.","sentences":["Machine Reading Comprehension (MRC) holds a pivotal role in shaping Medical Question Answering Systems (QAS) and transforming the landscape of accessing and applying medical information.","However, the inherent challenges in the medical field, such as complex terminology and question ambiguity, necessitate innovative solutions.","One key solution involves integrating specialized medical datasets and creating dedicated datasets.","This strategic approach enhances the accuracy of QAS, contributing to advancements in clinical decision-making and medical research.","To address the intricacies of medical terminology, a specialized dataset was integrated, exemplified by a novel Span extraction dataset derived from emrQA but restructured into 163,695 questions and 4,136 manually obtained answers, this new dataset was called emrQA-msquad dataset.","Additionally, for ambiguous questions, a dedicated medical dataset for the Span extraction task was introduced, reinforcing the system's robustness.","The fine-tuning of models such as BERT, RoBERTa, and Tiny RoBERTa for medical contexts significantly improved response accuracy within the F1-score range of 0.75 to 1.00 from 10.1% to 37.4%, 18.7% to 44.7% and 16.0% to 46.8%, respectively.","Finally, emrQA-msquad dataset is publicy available at https://huggingface.co/datasets/Eladio/emrqa-msquad."],"url":"http://arxiv.org/abs/2404.12050v1","category":"cs.CL"}
{"created":"2024-04-18 09:55:36","title":"Resolved magnetohydrodynamic wave lensing in the solar corona","abstract":"Electromagnetic wave lensing, a common physical phenomenon recognized in visible light for centuries, finds extensive applications in manipulating light in optical systems such as telescopes and cameras. Magnetohydrodynamic wave is a common perturbation phenomenon in the corona. By using high spatio-temporal resolution observations from the Solar Dynamics Observatory, here, we report the observation of a magnetohydrodynamic wave lensing in the highly ionized and magnetized coronal plasma, where quasi-periodic wavefronts emanated from a flare converged at a specific point after traversing a coronal hole. The entire process resembles an electromagnetic wave lensing from the source to the focus. Meanwhile, the magnetohydrodynamic wave lensing is well reproduced through a magnetohydrodynamic numerical simulation with full spatio-temporal resolution. We further investigate potential applications for coronal seismology, as the lensing process encodes information on the Alfv\\'en speed, in conjunction with favorable geometric and density variations.","sentences":["Electromagnetic wave lensing, a common physical phenomenon recognized in visible light for centuries, finds extensive applications in manipulating light in optical systems such as telescopes and cameras.","Magnetohydrodynamic wave is a common perturbation phenomenon in the corona.","By using high spatio-temporal resolution observations from the Solar Dynamics Observatory, here, we report the observation of a magnetohydrodynamic wave lensing in the highly ionized and magnetized coronal plasma, where quasi-periodic wavefronts emanated from a flare converged at a specific point after traversing a coronal hole.","The entire process resembles an electromagnetic wave lensing from the source to the focus.","Meanwhile, the magnetohydrodynamic wave lensing is well reproduced through a magnetohydrodynamic numerical simulation with full spatio-temporal resolution.","We further investigate potential applications for coronal seismology, as the lensing process encodes information on the Alfv\\'en speed, in conjunction with favorable geometric and density variations."],"url":"http://arxiv.org/abs/2404.12044v1","category":"astro-ph.SR"}
{"created":"2024-04-18 09:52:50","title":"Exploring Boundaries and Intensities in Offensive and Hate Speech: Unveiling the Complex Spectrum of Social Media Discourse","abstract":"The prevalence of digital media and evolving sociopolitical dynamics have significantly amplified the dissemination of hateful content. Existing studies mainly focus on classifying texts into binary categories, often overlooking the continuous spectrum of offensiveness and hatefulness inherent in the text. In this research, we present an extensive benchmark dataset for Amharic, comprising 8,258 tweets annotated for three distinct tasks: category classification, identification of hate targets, and rating offensiveness and hatefulness intensities. Our study highlights that a considerable majority of tweets belong to the less offensive and less hate intensity levels, underscoring the need for early interventions by stakeholders. The prevalence of ethnic and political hatred targets, with significant overlaps in our dataset, emphasizes the complex relationships within Ethiopia's sociopolitical landscape. We build classification and regression models and investigate the efficacy of models in handling these tasks. Our results reveal that hate and offensive speech can not be addressed by a simplistic binary classification, instead manifesting as variables across a continuous range of values. The Afro-XLMR-large model exhibits the best performances achieving F1-scores of 75.30%, 70.59%, and 29.42% for the category, target, and regression tasks, respectively. The 80.22% correlation coefficient of the Afro-XLMR-large model indicates strong alignments.","sentences":["The prevalence of digital media and evolving sociopolitical dynamics have significantly amplified the dissemination of hateful content.","Existing studies mainly focus on classifying texts into binary categories, often overlooking the continuous spectrum of offensiveness and hatefulness inherent in the text.","In this research, we present an extensive benchmark dataset for Amharic, comprising 8,258 tweets annotated for three distinct tasks: category classification, identification of hate targets, and rating offensiveness and hatefulness intensities.","Our study highlights that a considerable majority of tweets belong to the less offensive and less hate intensity levels, underscoring the need for early interventions by stakeholders.","The prevalence of ethnic and political hatred targets, with significant overlaps in our dataset, emphasizes the complex relationships within Ethiopia's sociopolitical landscape.","We build classification and regression models and investigate the efficacy of models in handling these tasks.","Our results reveal that hate and offensive speech can not be addressed by a simplistic binary classification, instead manifesting as variables across a continuous range of values.","The Afro-XLMR-large model exhibits the best performances achieving F1-scores of 75.30%, 70.59%, and 29.42% for the category, target, and regression tasks, respectively.","The 80.22% correlation coefficient of the Afro-XLMR-large model indicates strong alignments."],"url":"http://arxiv.org/abs/2404.12042v1","category":"cs.CL"}
{"created":"2024-04-18 09:51:04","title":"String Junctions Revisited","abstract":"Recent measurements at the LHC have revealed heavy-flavour baryon fractions much larger than those observed at LEP, with e.g., LambdaC+/D0 and LambdaB0/B0 reaching ~ 0.5 at low pT. One scenario that has been at least partly successful in predicting observed trends is QCD colour reconnections with string junctions. In previous work, however, the limit of a low-pT heavy quark was not well defined. We reconsider the string equations of motion for junction systems in this limit, and find that the junction effectively becomes bound to the heavy quark, a scenario we refer to as a \"pearl on a string\". We extend string-junction fragmentation in Pythia with a dedicated modelling of this limit for both light- and heavy-quark \"pearls\".","sentences":["Recent measurements at the LHC have revealed heavy-flavour baryon fractions much larger than those observed at LEP, with e.g., LambdaC+/D0 and LambdaB0/B0 reaching ~ 0.5 at low pT. One scenario that has been at least partly successful in predicting observed trends is QCD colour reconnections with string junctions.","In previous work, however, the limit of a low-pT heavy quark was not well defined.","We reconsider the string equations of motion for junction systems in this limit, and find that the junction effectively becomes bound to the heavy quark, a scenario we refer to as a \"pearl on a string\".","We extend string-junction fragmentation in Pythia with a dedicated modelling of this limit for both light- and heavy-quark \"pearls\"."],"url":"http://arxiv.org/abs/2404.12040v1","category":"hep-ph"}
{"created":"2024-04-18 09:46:44","title":"Constraints on the finite volume two-nucleon spectrum at $m_\u03c0\\approx 806$ MeV","abstract":"The low-energy finite-volume spectrum of the two-nucleon system at a pion mass of $m_\\pi \\approx 806$ MeV is studied with lattice quantum chromodynamics (LQCD) using variational methods. The interpolating-operator sets used in [Phys.Rev.D 107 (2023) 9, 094508] are extended by including a complete basis of local hexaquark operators, as well as plane-wave dibaryon operators built from products of both positive- and negative-parity nucleon operators. Results are presented for the isosinglet and isotriplet two-nucleon channels. In both channels, these results provide compelling evidence for the presence of an additional state in the low-energy spectrum not present in a non-interacting two-nucleon system. Within the space of operators that are considered in this work, the strongest overlap of this state is onto hexaquark operators, and variational analyses based on operator sets containing only dibaryon operators are insensitive to this state over the temporal extent studied here. The consequences of these studies for the LQCD understanding of the two-nucleon spectrum are investigated.","sentences":["The low-energy finite-volume spectrum of the two-nucleon system at a pion mass of $m_\\pi \\approx 806$ MeV is studied with lattice quantum chromodynamics (LQCD) using variational methods.","The interpolating-operator sets used in [Phys.Rev.D 107 (2023) 9, 094508] are extended by including a complete basis of local hexaquark operators, as well as plane-wave dibaryon operators built from products of both positive- and negative-parity nucleon operators.","Results are presented for the isosinglet and isotriplet two-nucleon channels.","In both channels, these results provide compelling evidence for the presence of an additional state in the low-energy spectrum not present in a non-interacting two-nucleon system.","Within the space of operators that are considered in this work, the strongest overlap of this state is onto hexaquark operators, and variational analyses based on operator sets containing only dibaryon operators are insensitive to this state over the temporal extent studied here.","The consequences of these studies for the LQCD understanding of the two-nucleon spectrum are investigated."],"url":"http://arxiv.org/abs/2404.12039v1","category":"hep-lat"}
{"created":"2024-04-18 09:43:03","title":"Exploring the Premelting Transition through Molecular Simulations Powered by Neural Network Potentials","abstract":"The system has addressed the error of \"Bad character(s) in field Abstract\" for no reason. Please refer to manuscript for the full abstract.","sentences":["The system has addressed the error of \"Bad character(s) in field Abstract\" for no reason.","Please refer to manuscript for the full abstract."],"url":"http://arxiv.org/abs/2404.12036v1","category":"physics.comp-ph"}
{"created":"2024-04-18 09:33:31","title":"Quantum Optical Approach to the $K$ Nearest Neighbour Algorithm","abstract":"We construct a hybrid quantum-classical approach for the $K$-Nearest Neighbour algorithm, where the information is embedded in a phase-distributed multimode coherent state with the assistance of a single photon. The task of finding the closeness between the data points is delivered by the quantum optical computer, while the sorting and class assignment are performed by a classical computer. We provide the quantum optical architecture corresponding to our algorithm. The subordinate optical network is validated by numerical simulation. We also optimize the computational resources of the algorithm in the context of space, energy requirements and gate complexity. Applications are presented for diverse and well-known public benchmarks and synthesized data sets.","sentences":["We construct a hybrid quantum-classical approach for the $K$-Nearest Neighbour algorithm, where the information is embedded in a phase-distributed multimode coherent state with the assistance of a single photon.","The task of finding the closeness between the data points is delivered by the quantum optical computer, while the sorting and class assignment are performed by a classical computer.","We provide the quantum optical architecture corresponding to our algorithm.","The subordinate optical network is validated by numerical simulation.","We also optimize the computational resources of the algorithm in the context of space, energy requirements and gate complexity.","Applications are presented for diverse and well-known public benchmarks and synthesized data sets."],"url":"http://arxiv.org/abs/2404.12033v1","category":"quant-ph"}
{"created":"2024-04-18 09:24:41","title":"Transport scaling in porous media convection","abstract":"We present a theory to describe the Nusselt number ($Nu$), corresponding to the heat or mass flux, as a function of the Rayleigh--Darcy number ($Ra$), the ratio of buoyant driving force over diffusive dissipation, in convective porous media flows. First, we derive exact relationships within the system for the kinetic energy and the thermal dissipation rate. Second, by segregating the thermal dissipation rate into contributions from the boundary layer and the bulk, which is inspired by the ideas of the Grossmann and Lohse theory (J. Fluid Mech., vol. 407, 2000; Phys. Rev. Lett., vol. 86, 2001), we derive the scaling relation for $Nu$ as a function of $Ra$ and provide a robust theoretical explanation to the empirical relations proposed in previous studies. Specifically, by incorporating the length scale of the flow structure into the theory, we demonstrate why heat or mass transport differs between two-dimensional and three-dimensional porous media convection. Our model is in excellent agreement with the data obtained from numerical simulations, affirming its validity and predictive capabilities.","sentences":["We present a theory to describe the Nusselt number ($Nu$), corresponding to the heat or mass flux, as a function of the Rayleigh--Darcy number ($Ra$), the ratio of buoyant driving force over diffusive dissipation, in convective porous media flows.","First, we derive exact relationships within the system for the kinetic energy and the thermal dissipation rate.","Second, by segregating the thermal dissipation rate into contributions from the boundary layer and the bulk, which is inspired by the ideas of the Grossmann and Lohse theory (J. Fluid Mech., vol. 407, 2000; Phys. Rev. Lett., vol. 86, 2001), we derive the scaling relation for $Nu$ as a function of $Ra$ and provide a robust theoretical explanation to the empirical relations proposed in previous studies.","Specifically, by incorporating the length scale of the flow structure into the theory, we demonstrate why heat or mass transport differs between two-dimensional and three-dimensional porous media convection.","Our model is in excellent agreement with the data obtained from numerical simulations, affirming its validity and predictive capabilities."],"url":"http://arxiv.org/abs/2404.12026v1","category":"physics.flu-dyn"}
{"created":"2024-04-18 09:22:08","title":"PID Tuning using Cross-Entropy Deep Learning: a Lyapunov Stability Analysis","abstract":"Underwater Unmanned Vehicles (UUVs) have to constantly compensate for the external disturbing forces acting on their body. Adaptive Control theory is commonly used there to grant the control law some flexibility in its response to process variation. Today, learning-based (LB) adaptive methods are leading the field where model-based control structures are combined with deep model-free learning algorithms. This work proposes experiments and metrics to empirically study the stability of such a controller. We perform this stability analysis on a LB adaptive control system whose adaptive parameters are determined using a Cross-Entropy Deep Learning method.","sentences":["Underwater Unmanned Vehicles (UUVs) have to constantly compensate for the external disturbing forces acting on their body.","Adaptive Control theory is commonly used there to grant the control law some flexibility in its response to process variation.","Today, learning-based (LB) adaptive methods are leading the field where model-based control structures are combined with deep model-free learning algorithms.","This work proposes experiments and metrics to empirically study the stability of such a controller.","We perform this stability analysis on a LB adaptive control system whose adaptive parameters are determined using a Cross-Entropy Deep Learning method."],"url":"http://arxiv.org/abs/2404.12025v1","category":"eess.SY"}
{"created":"2024-04-18 09:04:39","title":"Enhance Robustness of Language Models Against Variation Attack through Graph Integration","abstract":"The widespread use of pre-trained language models (PLMs) in natural language processing (NLP) has greatly improved performance outcomes. However, these models' vulnerability to adversarial attacks (e.g., camouflaged hints from drug dealers), particularly in the Chinese language with its rich character diversity/variation and complex structures, hatches vital apprehension. In this study, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE), to increase the robustness of PLMs against character variation attacks in Chinese content. CHANGE presents a novel approach for incorporating a Chinese character variation graph into the PLMs. Through designing different supplementary tasks utilizing the graph structure, CHANGE essentially enhances PLMs' interpretation of adversarially manipulated text. Experiments conducted in a multitude of NLP tasks show that CHANGE outperforms current language models in combating against adversarial attacks and serves as a valuable contribution to robust language model research. These findings contribute to the groundwork on robust language models and highlight the substantial potential of graph-guided pre-training strategies for real-world applications.","sentences":["The widespread use of pre-trained language models (PLMs) in natural language processing (NLP) has greatly improved performance outcomes.","However, these models' vulnerability to adversarial attacks (e.g., camouflaged hints from drug dealers), particularly in the Chinese language with its rich character diversity/variation and complex structures, hatches vital apprehension.","In this study, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE), to increase the robustness of PLMs against character variation attacks in Chinese content.","CHANGE presents a novel approach for incorporating a Chinese character variation graph into the PLMs.","Through designing different supplementary tasks utilizing the graph structure, CHANGE essentially enhances PLMs' interpretation of adversarially manipulated text.","Experiments conducted in a multitude of NLP tasks show that CHANGE outperforms current language models in combating against adversarial attacks and serves as a valuable contribution to robust language model research.","These findings contribute to the groundwork on robust language models and highlight the substantial potential of graph-guided pre-training strategies for real-world applications."],"url":"http://arxiv.org/abs/2404.12014v1","category":"cs.CL"}
{"created":"2024-04-18 09:00:01","title":"Single-peak and multi-peak solutions for Hamiltonian elliptic systems in dimension two","abstract":"This paper is concerned with the Hamiltonian elliptic system in dimension two\\begin{equation*}\\aligned \\left\\{ \\begin{array}{lll} -\\epsilon^2\\Delta u+V(x)u=g(v)\\ & \\text{in}\\quad \\mathbb{R}^2,\\\\ -\\epsilon^2\\Delta v+V(x)v=f(u)\\ & \\text{in}\\quad \\mathbb{R}^2, \\end{array}\\right.\\endaligned \\end{equation*} where $V\\in C(\\mathbb{R}^2)$ has local minimum points, and $f,g\\in C^1(\\mathbb{R})$ are assumed to be of exponential growth in the sense of Trudinger-Moser inequality. When $V$ admits one or several local strict minimum points, we show the existence and concentration of single-peak and multi-peak semiclassical states respectively, as well as strong convergence and exponential decay. In addition, positivity of solutions and uniqueness of local maximum points of solutions are also studied. Our theorems extend the results of Ramos and Tavares [Calc. Var. 31 (2008) 1-25], where $f$ and $g$ have polynomial growth. It seems that it is the first attempt to obtain multi-peak semiclassical states for Hamiltonian elliptic system with exponential growth.","sentences":["This paper is concerned with the Hamiltonian elliptic system in dimension two\\begin{equation*}\\aligned \\left\\{ \\begin{array}{lll} -\\epsilon^2\\Delta u+V(x)u=g(v)\\ & \\text{in}\\quad \\mathbb{R}^2,\\\\ -\\epsilon^2\\Delta v+V(x)v=f(u)\\ & \\text{in}\\quad \\mathbb{R}^2, \\end{array}\\right.\\endaligned \\end{equation*} where $V\\in C(\\mathbb{R}^2)$ has local minimum points, and $f,g\\in C^1(\\mathbb{R})$ are assumed to be of exponential growth in the sense of Trudinger-Moser inequality.","When $V$ admits one or several local strict minimum points, we show the existence and concentration of single-peak and multi-peak semiclassical states respectively, as well as strong convergence and exponential decay.","In addition, positivity of solutions and uniqueness of local maximum points of solutions are also studied.","Our theorems extend the results of Ramos and Tavares","[Calc.","Var.","31 (2008) 1-25], where $f$ and $g$ have polynomial growth.","It seems that it is the first attempt to obtain multi-peak semiclassical states for Hamiltonian elliptic system with exponential growth."],"url":"http://arxiv.org/abs/2404.12009v1","category":"math.AP"}
{"created":"2024-04-18 08:51:26","title":"Internet sentiment exacerbates intraday overtrading, evidence from A-Share market","abstract":"Market fluctuations caused by overtrading are important components of systemic market risk. This study examines the effect of investor sentiment on intraday overtrading activities in the Chinese A-share market. Employing high-frequency sentiment indices inferred from social media posts on the Eastmoney forum Guba, the research focuses on constituents of the CSI 300 and CSI 500 indices over a period from 01/01/2018, to 12/30/2022. The empirical analysis indicates that investor sentiment exerts a significantly positive impact on intraday overtrading, with the influence being more pronounced among institutional investors relative to individual traders. Moreover, sentiment-driven overtrading is found to be more prevalent during bull markets as opposed to bear markets. Additionally, the effect of sentiment on overtrading is observed to be more pronounced among individual investors in large-cap stocks compared to small- and mid-cap stocks.","sentences":["Market fluctuations caused by overtrading are important components of systemic market risk.","This study examines the effect of investor sentiment on intraday overtrading activities in the Chinese A-share market.","Employing high-frequency sentiment indices inferred from social media posts on the Eastmoney forum Guba, the research focuses on constituents of the CSI 300 and CSI 500 indices over a period from 01/01/2018, to 12/30/2022.","The empirical analysis indicates that investor sentiment exerts a significantly positive impact on intraday overtrading, with the influence being more pronounced among institutional investors relative to individual traders.","Moreover, sentiment-driven overtrading is found to be more prevalent during bull markets as opposed to bear markets.","Additionally, the effect of sentiment on overtrading is observed to be more pronounced among individual investors in large-cap stocks compared to small- and mid-cap stocks."],"url":"http://arxiv.org/abs/2404.12001v1","category":"q-fin.CP"}
{"created":"2024-04-18 08:46:12","title":"Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation","abstract":"Referring image segmentation (RIS) aims to precisely segment referents in images through corresponding natural language expressions, yet relying on cost-intensive mask annotations. Weakly supervised RIS thus learns from image-text pairs to pixel-level semantics, which is challenging for segmenting fine-grained masks. A natural approach to enhancing segmentation precision is to empower weakly supervised RIS with the image segmentation foundation model SAM. Nevertheless, we observe that simply integrating SAM yields limited benefits and can even lead to performance regression due to the inevitable noise issues and challenges in excessive focus on object parts. In this paper, we present an innovative framework, Point PrompTing (PPT), incorporated with the proposed multi-source curriculum learning strategy to address these challenges. Specifically, the core of PPT is a point generator that not only harnesses CLIP's text-image alignment capability and SAM's powerful mask generation ability but also generates negative point prompts to address the noisy and excessive focus issues inherently and effectively. In addition, we introduce a curriculum learning strategy with object-centric images to help PPT gradually learn from simpler yet precise semantic alignment to more complex RIS. Experiments demonstrate that our PPT significantly and consistently outperforms prior weakly supervised techniques on mIoU by 11.34%, 14.14%, and 6.97% across RefCOCO, RefCOCO+, and G-Ref, respectively.","sentences":["Referring image segmentation (RIS) aims to precisely segment referents in images through corresponding natural language expressions, yet relying on cost-intensive mask annotations.","Weakly supervised RIS thus learns from image-text pairs to pixel-level semantics, which is challenging for segmenting fine-grained masks.","A natural approach to enhancing segmentation precision is to empower weakly supervised RIS with the image segmentation foundation model SAM.","Nevertheless, we observe that simply integrating SAM yields limited benefits and can even lead to performance regression due to the inevitable noise issues and challenges in excessive focus on object parts.","In this paper, we present an innovative framework, Point PrompTing (PPT), incorporated with the proposed multi-source curriculum learning strategy to address these challenges.","Specifically, the core of PPT is a point generator that not only harnesses CLIP's text-image alignment capability and SAM's powerful mask generation ability but also generates negative point prompts to address the noisy and excessive focus issues inherently and effectively.","In addition, we introduce a curriculum learning strategy with object-centric images to help PPT gradually learn from simpler yet precise semantic alignment to more complex RIS.","Experiments demonstrate that our PPT significantly and consistently outperforms prior weakly supervised techniques on mIoU by 11.34%, 14.14%, and 6.97% across RefCOCO, RefCOCO+, and G-Ref, respectively."],"url":"http://arxiv.org/abs/2404.11998v1","category":"cs.CV"}
{"created":"2024-04-18 08:45:16","title":"Geodesic extensions of mechanical systems with nonholonomic constraints","abstract":"For a Lagrangian system with nonholonomic constraints, we construct extensions of the equations of motion to sets of second-order ordinary differential equations. In the case of a purely kinetic Lagrangian, we investigate the conditions under which the nonholonomic trajectories are geodesics of a Riemannian metric, while preserving the constrained Lagrangian. We interpret the algebraic and PDE conditions of this problem as infinitesimal versions of the relation between the nonholonomic exponential map and the Riemannian metric. We discuss the special case of a Chaplygin system with symmetries and we end the paper with a worked-out example.","sentences":["For a Lagrangian system with nonholonomic constraints, we construct extensions of the equations of motion to sets of second-order ordinary differential equations.","In the case of a purely kinetic Lagrangian, we investigate the conditions under which the nonholonomic trajectories are geodesics of a Riemannian metric, while preserving the constrained Lagrangian.","We interpret the algebraic and PDE conditions of this problem as infinitesimal versions of the relation between the nonholonomic exponential map and the Riemannian metric.","We discuss the special case of a Chaplygin system with symmetries and we end the paper with a worked-out example."],"url":"http://arxiv.org/abs/2404.11997v1","category":"math.DG"}
{"created":"2024-04-18 08:42:02","title":"Cost and CO2 emissions co-optimisation of green hydrogen production in a grid-connected renewable energy system","abstract":"Green hydrogen is essential for producing renewable fuels that are needed in sectors that are hard to electrify directly. Hydrogen production in a grid-connected hybrid renewable energy plant necessitates smart planning to meet long-term hydrogen trading agreements while minimising costs and emissions. Previous research analysed economic and environmental impact of hydrogen production based on full foresight of renewable energy availabilty, electricity price, and CO2 intensity in the electricity grid. However, the full foresight assumption is impractical in day-to-day operation, often leading to underestimations of both the cost and CO2 emissions associated with hydrogen production. Therefore, this research introduces a novel long-term planner that uses historical data and short-term forecasts to plan hydrogen production in the day-to-day operation of a grid-connected hybrid renewable energy plant. The long-term planner co-minimises cost and CO2 emissions to determine the hydrogen production for the next day taking into account the remaining hydrogen production and the time remaining until the end of the delivery period, which can be a week, a month, or a year. Extended delivery periods provide operation flexibility, enabling cost and CO2 emissions reductions. Significant reductions in CO2 emissions can be achieved with relatively small increases in the levelised cost. Under day-to-day operation, the levelised cost of hydrogen is marginally higher than that of the full foresight; the CO2 emissions can be up to 60% higher. Despite a significant portion of the produced hydrogen not meeting the criteria for green hydrogen designation under current rules, CO2 emissions are lower than those from existing alternative hydrogen production methods. These results underscore the importance of balancing cost considerations with environmental impacts in operational decision-making.","sentences":["Green hydrogen is essential for producing renewable fuels that are needed in sectors that are hard to electrify directly.","Hydrogen production in a grid-connected hybrid renewable energy plant necessitates smart planning to meet long-term hydrogen trading agreements while minimising costs and emissions.","Previous research analysed economic and environmental impact of hydrogen production based on full foresight of renewable energy availabilty, electricity price, and CO2 intensity in the electricity grid.","However, the full foresight assumption is impractical in day-to-day operation, often leading to underestimations of both the cost and CO2 emissions associated with hydrogen production.","Therefore, this research introduces a novel long-term planner that uses historical data and short-term forecasts to plan hydrogen production in the day-to-day operation of a grid-connected hybrid renewable energy plant.","The long-term planner co-minimises cost and CO2 emissions to determine the hydrogen production for the next day taking into account the remaining hydrogen production and the time remaining until the end of the delivery period, which can be a week, a month, or a year.","Extended delivery periods provide operation flexibility, enabling cost and CO2 emissions reductions.","Significant reductions in CO2 emissions can be achieved with relatively small increases in the levelised cost.","Under day-to-day operation, the levelised cost of hydrogen is marginally higher than that of the full foresight; the CO2 emissions can be up to 60% higher.","Despite a significant portion of the produced hydrogen not meeting the criteria for green hydrogen designation under current rules, CO2 emissions are lower than those from existing alternative hydrogen production methods.","These results underscore the importance of balancing cost considerations with environmental impacts in operational decision-making."],"url":"http://arxiv.org/abs/2404.11995v1","category":"eess.SY"}
{"created":"2024-04-18 08:39:58","title":"Image Compression and Reconstruction Based on Quantum Network","abstract":"Quantum network is an emerging type of network structure that leverages the principles of quantum mechanics to transmit and process information. Compared with classical data reconstruction algorithms, quantum networks make image reconstruction more efficient and accurate. They can also process more complex image information using fewer bits and faster parallel computing capabilities. Therefore, this paper will discuss image reconstruction methods based on our quantum network and explore their potential applications in image processing. We will introduce the basic structure of the quantum network, the process of image compression and reconstruction, and the specific parameter training method. Through this study, we can achieve a classical image reconstruction accuracy of 97.57\\%. Our quantum network design will introduce novel ideas and methods for image reconstruction in the future.","sentences":["Quantum network is an emerging type of network structure that leverages the principles of quantum mechanics to transmit and process information.","Compared with classical data reconstruction algorithms, quantum networks make image reconstruction more efficient and accurate.","They can also process more complex image information using fewer bits and faster parallel computing capabilities.","Therefore, this paper will discuss image reconstruction methods based on our quantum network and explore their potential applications in image processing.","We will introduce the basic structure of the quantum network, the process of image compression and reconstruction, and the specific parameter training method.","Through this study, we can achieve a classical image reconstruction accuracy of 97.57\\%.","Our quantum network design will introduce novel ideas and methods for image reconstruction in the future."],"url":"http://arxiv.org/abs/2404.11994v1","category":"quant-ph"}
{"created":"2024-04-18 08:29:27","title":"New Analysis of Overlapping Schwarz Methods for Vector Field Problems in Three Dimensions with Generally Shaped Domains","abstract":"This paper introduces a novel approach to analyzing overlapping Schwarz methods for N\\'{e}d\\'{e}lec and Raviart--Thomas vector field problems. The theory is based on new regular stable decompositions for vector fields that are robust to the topology of the domain. Enhanced estimates for the condition numbers of the preconditioned linear systems are derived, dependent linearly on the relative overlap between the overlapping subdomains. Furthermore, we present the numerical experiments which support our theoretical results.","sentences":["This paper introduces a novel approach to analyzing overlapping Schwarz methods for N\\'{e}d\\'{e}lec and Raviart--Thomas vector field problems.","The theory is based on new regular stable decompositions for vector fields that are robust to the topology of the domain.","Enhanced estimates for the condition numbers of the preconditioned linear systems are derived, dependent linearly on the relative overlap between the overlapping subdomains.","Furthermore, we present the numerical experiments which support our theoretical results."],"url":"http://arxiv.org/abs/2404.11986v1","category":"math.NA"}
{"created":"2024-04-18 08:27:04","title":"On the generic increase of observational entropy in isolated systems","abstract":"Observational entropy - a quantity that unifies Boltzmann's entropy, Gibbs' entropy, von Neumann's macroscopic entropy, and the diagonal entropy - has recently been argued to play a key role in a modern formulation of statistical mechanics. Here, relying on algebraic techniques taken from Petz's theory of statistical sufficiency and on a Levy-type concentration bound, we prove rigorous theorems showing how the observational entropy of a system undergoing a unitary evolution chosen at random tends to increase with overwhelming probability and to reach its maximum very quickly. More precisely, we show that for any observation that is sufficiently coarse with respect to the size of the system, regardless of the initial state of the system (be it pure or mixed), random evolution renders its state practically indistinguishable from the microcanonical distribution with a probability approaching one as the size of the system grows. The same conclusion holds not only for random evolutions sampled according to the unitarily invariant Haar distribution, but also for approximate 2-designs, which are thought to provide a more physically reasonable way to model random evolutions.","sentences":["Observational entropy - a quantity that unifies Boltzmann's entropy, Gibbs' entropy, von Neumann's macroscopic entropy, and the diagonal entropy - has recently been argued to play a key role in a modern formulation of statistical mechanics.","Here, relying on algebraic techniques taken from Petz's theory of statistical sufficiency and on a Levy-type concentration bound, we prove rigorous theorems showing how the observational entropy of a system undergoing a unitary evolution chosen at random tends to increase with overwhelming probability and to reach its maximum very quickly.","More precisely, we show that for any observation that is sufficiently coarse with respect to the size of the system, regardless of the initial state of the system (be it pure or mixed), random evolution renders its state practically indistinguishable from the microcanonical distribution with a probability approaching one as the size of the system grows.","The same conclusion holds not only for random evolutions sampled according to the unitarily invariant Haar distribution, but also for approximate 2-designs, which are thought to provide a more physically reasonable way to model random evolutions."],"url":"http://arxiv.org/abs/2404.11985v1","category":"quant-ph"}
{"created":"2024-04-18 08:26:22","title":"Monte Carlo method and the random isentropic Euler system","abstract":"We show several results on convergence of the Monte Carlo method applied to consistent approximations of the isentropic Euler system of gas dynamics with uncertain initial data. Our method is based on combination of several new concepts. We work with the dissipative weak solutions that can be seen as a universal closure of consistent approximations. Further, we apply the set-valued version of the Strong law of large numbers for general multivalued mapping with closed range and the Koml\\'os theorem on strong converge of empirical averages of integrable functions. Theoretical results are illustrated by a series of numerical simulations obtained by an unconditionally convergent viscosity finite volume method combined with the Monte Carlo method.","sentences":["We show several results on convergence of the Monte Carlo method applied to consistent approximations of the isentropic Euler system of gas dynamics with uncertain initial data.","Our method is based on combination of several new concepts.","We work with the dissipative weak solutions that can be seen as a universal closure of consistent approximations.","Further, we apply the set-valued version of the Strong law of large numbers for general multivalued mapping with closed range and the Koml\\'os theorem on strong converge of empirical averages of integrable functions.","Theoretical results are illustrated by a series of numerical simulations obtained by an unconditionally convergent viscosity finite volume method combined with the Monte Carlo method."],"url":"http://arxiv.org/abs/2404.11983v1","category":"math.NA"}
{"created":"2024-04-18 08:26:04","title":"SIGformer: Sign-aware Graph Transformer for Recommendation","abstract":"In recommender systems, most graph-based methods focus on positive user feedback, while overlooking the valuable negative feedback. Integrating both positive and negative feedback to form a signed graph can lead to a more comprehensive understanding of user preferences. However, the existing efforts to incorporate both types of feedback are sparse and face two main limitations: 1) They process positive and negative feedback separately, which fails to holistically leverage the collaborative information within the signed graph; 2) They rely on MLPs or GNNs for information extraction from negative feedback, which may not be effective.   To overcome these limitations, we introduce SIGformer, a new method that employs the transformer architecture to sign-aware graph-based recommendation. SIGformer incorporates two innovative positional encodings that capture the spectral properties and path patterns of the signed graph, enabling the full exploitation of the entire graph. Our extensive experiments across five real-world datasets demonstrate the superiority of SIGformer over state-of-the-art methods. The code is available at https://github.com/StupidThree/SIGformer.","sentences":["In recommender systems, most graph-based methods focus on positive user feedback, while overlooking the valuable negative feedback.","Integrating both positive and negative feedback to form a signed graph can lead to a more comprehensive understanding of user preferences.","However, the existing efforts to incorporate both types of feedback are sparse and face two main limitations: 1) They process positive and negative feedback separately, which fails to holistically leverage the collaborative information within the signed graph; 2) They rely on MLPs or GNNs for information extraction from negative feedback, which may not be effective.   ","To overcome these limitations, we introduce SIGformer, a new method that employs the transformer architecture to sign-aware graph-based recommendation.","SIGformer incorporates two innovative positional encodings that capture the spectral properties and path patterns of the signed graph, enabling the full exploitation of the entire graph.","Our extensive experiments across five real-world datasets demonstrate the superiority of SIGformer over state-of-the-art methods.","The code is available at https://github.com/StupidThree/SIGformer."],"url":"http://arxiv.org/abs/2404.11982v1","category":"cs.IR"}
{"created":"2024-04-18 07:55:35","title":"Lewis and Brouwer meet Strong L\u00f6b","abstract":"We study the principle phi implies box phi, known as `Strength' or `the Completeness Principle', over the constructive version of L\\\"ob's Logic. We consider this principle both for the modal language with the necessity operator and for the modal language with the Lewis arrow, where L\\\"ob's Logic is suitably adapted.   Central insights of provability logic, like the de Jongh-Sambin Theorem and the de Jongh-Sambin-Bernardi Theorem, take a simple form in the presence of Strength. We present these simple versions. We discuss the semantics of two salient systems and prove uniform interpolation for both. In addition, we sketch arithmetical interpretations of our systems. Finally, we describe the various connections of our subject with Computer Science.","sentences":["We study the principle phi implies box phi, known as `Strength' or `the Completeness Principle', over the constructive version of L\\\"ob's Logic.","We consider this principle both for the modal language with the necessity operator and for the modal language with the Lewis arrow, where L\\\"ob's Logic is suitably adapted.   ","Central insights of provability logic, like the de Jongh-Sambin Theorem and the de Jongh-Sambin-Bernardi Theorem, take a simple form in the presence of Strength.","We present these simple versions.","We discuss the semantics of two salient systems and prove uniform interpolation for both.","In addition, we sketch arithmetical interpretations of our systems.","Finally, we describe the various connections of our subject with Computer Science."],"url":"http://arxiv.org/abs/2404.11969v1","category":"math.LO"}
{"created":"2024-04-18 07:53:27","title":"Multi-Agent Relative Investment Games in a Jump Diffusion Market with Deep Reinforcement Learning Algorithm","abstract":"This paper focuses on multi-agent stochastic differential games for jump-diffusion systems. On one hand, we study the multi-agent game for optimal investment in a jump-diffusion market. We derive constant Nash equilibria and provide sufficient conditions for their existence and uniqueness for exponential, power, and logarithmic utilities, respectively. On the other hand, we introduce a computational framework based on the actor-critic method in deep reinforcement learning to solve the stochastic control problem with jumps. We extend this algorithm to address the multi-agent game with jumps and utilize parallel computing to enhance computational efficiency. We present numerical examples of the Merton problem with jumps, linear quadratic regulators, and the optimal investment game under various settings to demonstrate the accuracy, efficiency, and robustness of the proposed method. In particular, neural network solutions numerically converge to the derived constant Nash equilibrium for the multi-agent game.","sentences":["This paper focuses on multi-agent stochastic differential games for jump-diffusion systems.","On one hand, we study the multi-agent game for optimal investment in a jump-diffusion market.","We derive constant Nash equilibria and provide sufficient conditions for their existence and uniqueness for exponential, power, and logarithmic utilities, respectively.","On the other hand, we introduce a computational framework based on the actor-critic method in deep reinforcement learning to solve the stochastic control problem with jumps.","We extend this algorithm to address the multi-agent game with jumps and utilize parallel computing to enhance computational efficiency.","We present numerical examples of the Merton problem with jumps, linear quadratic regulators, and the optimal investment game under various settings to demonstrate the accuracy, efficiency, and robustness of the proposed method.","In particular, neural network solutions numerically converge to the derived constant Nash equilibrium for the multi-agent game."],"url":"http://arxiv.org/abs/2404.11967v1","category":"math.OC"}
{"created":"2024-04-18 07:49:41","title":"Extinction and survival in inherited sterility","abstract":"We introduce an interacting particle system which models the inherited sterility method. Individuals evolve on $\\mathbb{Z}^d$ according to a contact process with parameter $\\lambda>0$. With probability $p \\in [0,1]$ an offspring is fertile and can give birth to other individuals at rate $\\lambda$. With probability $1-p$, an offspring is sterile and blocks the site it sits on until it dies. The goal is to prove that at fixed $\\lambda$, the system survives for large enough $p$ and dies out for small enough $p$. The model is not attractive, since an increase of fertile individuals potentially causes that of sterile ones. However, thanks to a comparison argument with attractive models, we are able to answer our question.","sentences":["We introduce an interacting particle system which models the inherited sterility method.","Individuals evolve on $\\mathbb{Z}^d$ according to a contact process with parameter $\\lambda>0$. With probability $p \\in","[0,1]$ an offspring is fertile and can give birth to other individuals at rate $\\lambda$. With probability $1-p$, an offspring is sterile and blocks the site it sits on until it dies.","The goal is to prove that at fixed $\\lambda$, the system survives for large enough $p$ and dies out for small enough $p$. The model is not attractive, since an increase of fertile individuals potentially causes that of sterile ones.","However, thanks to a comparison argument with attractive models, we are able to answer our question."],"url":"http://arxiv.org/abs/2404.11963v1","category":"math.PR"}
{"created":"2024-04-18 07:37:46","title":"Segmented Model-Based Hydrogen Delivery Control for PEM Fuel Cells: a Port-Hamiltonian Approach","abstract":"This paper proposes an extended interconnection and damping assignment passivity-based control technique (IDA-PBC) to control the pressure dynamics in the fuel delivery subsystem (FDS) of proton exchange membrane fuel cells. The fuel cell stack is a distributed parameter model which can be modeled by partial differential equations PDEs). In this paper, the segmentation concept is used to approximate the PDEs model by ordinary differential equations (ODEs) model. Therefore, each segments are having multiple ODEs to obtain the lump-sum model of the segments. Subsequently, a generalized multi-input multi-output lumped parameters model is developed in port-Hamiltonian framework based on mass balance to minimize the modeling error. The modeling errors arises due to the difference between spatially distributed pressures in FDS segments, and also due to the difference between the actual stack pressure and the measured output pressure of the anode. The segments interconnection feasibilities are ensured by maintaining passivity of each segment. With consideration of re-circulation and bleeding of the anode in the modeling, an extended energy-shaping and output tracking IDA-PBC based state-feedback controller is proposed to control the spatially distributed pressure dynamics in the anode. Furthermore, a sliding mode observer of high order is designed to estimate the unmeasurable pressures in FDS with known disturbances. Performance recovery of output feedback control is accomplished with explicit stability analysis. The effectiveness of the proposed IDA-PBC approach is validated by the simulation results.","sentences":["This paper proposes an extended interconnection and damping assignment passivity-based control technique (IDA-PBC) to control the pressure dynamics in the fuel delivery subsystem (FDS) of proton exchange membrane fuel cells.","The fuel cell stack is a distributed parameter model which can be modeled by partial differential equations PDEs).","In this paper, the segmentation concept is used to approximate the PDEs model by ordinary differential equations (ODEs) model.","Therefore, each segments are having multiple ODEs to obtain the lump-sum model of the segments.","Subsequently, a generalized multi-input multi-output lumped parameters model is developed in port-Hamiltonian framework based on mass balance to minimize the modeling error.","The modeling errors arises due to the difference between spatially distributed pressures in FDS segments, and also due to the difference between the actual stack pressure and the measured output pressure of the anode.","The segments interconnection feasibilities are ensured by maintaining passivity of each segment.","With consideration of re-circulation and bleeding of the anode in the modeling, an extended energy-shaping and output tracking IDA-PBC based state-feedback controller is proposed to control the spatially distributed pressure dynamics in the anode.","Furthermore, a sliding mode observer of high order is designed to estimate the unmeasurable pressures in FDS with known disturbances.","Performance recovery of output feedback control is accomplished with explicit stability analysis.","The effectiveness of the proposed IDA-PBC approach is validated by the simulation results."],"url":"http://arxiv.org/abs/2404.11959v1","category":"eess.SY"}
{"created":"2024-04-18 07:22:38","title":"The devil is in the object boundary: towards annotation-free instance segmentation using Foundation Models","abstract":"Foundation models, pre-trained on a large amount of data have demonstrated impressive zero-shot capabilities in various downstream tasks. However, in object detection and instance segmentation, two fundamental computer vision tasks heavily reliant on extensive human annotations, foundation models such as SAM and DINO struggle to achieve satisfactory performance. In this study, we reveal that the devil is in the object boundary, \\textit{i.e.}, these foundation models fail to discern boundaries between individual objects. For the first time, we probe that CLIP, which has never accessed any instance-level annotations, can provide a highly beneficial and strong instance-level boundary prior in the clustering results of its particular intermediate layer. Following this surprising observation, we propose $\\textbf{Zip}$ which $\\textbf{Z}$ips up CL$\\textbf{ip}$ and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free, complex-scene-capable, open-vocabulary object detection and instance segmentation. Our Zip significantly boosts SAM's mask AP on COCO dataset by 12.5% and establishes state-of-the-art performance in various settings, including training-free, self-training, and label-efficient finetuning. Furthermore, annotation-free Zip even achieves comparable performance to the best-performing open-vocabulary object detecters using base annotations. Code is released at https://github.com/ChengShiest/Zip-Your-CLIP","sentences":["Foundation models, pre-trained on a large amount of data have demonstrated impressive zero-shot capabilities in various downstream tasks.","However, in object detection and instance segmentation, two fundamental computer vision tasks heavily reliant on extensive human annotations, foundation models such as SAM and DINO struggle to achieve satisfactory performance.","In this study, we reveal that the devil is in the object boundary, \\textit{i.e.}, these foundation models fail to discern boundaries between individual objects.","For the first time, we probe that CLIP, which has never accessed any instance-level annotations, can provide a highly beneficial and strong instance-level boundary prior in the clustering results of its particular intermediate layer.","Following this surprising observation, we propose $\\textbf{Zip}$ which $\\textbf{Z}$ips up CL$\\textbf{ip}$ and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free, complex-scene-capable, open-vocabulary object detection and instance segmentation.","Our Zip significantly boosts SAM's mask AP on COCO dataset by 12.5% and establishes state-of-the-art performance in various settings, including training-free, self-training, and label-efficient finetuning.","Furthermore, annotation-free Zip even achieves comparable performance to the best-performing open-vocabulary object detecters using base annotations.","Code is released at https://github.com/ChengShiest/Zip-Your-CLIP"],"url":"http://arxiv.org/abs/2404.11957v1","category":"cs.CV"}
{"created":"2024-04-18 06:45:18","title":"AgentCoord: Visually Exploring Coordination Strategy for LLM-based Multi-Agent Collaboration","abstract":"The potential of automatic task-solving through Large Language Model (LLM)-based multi-agent collaboration has recently garnered widespread attention from both the research community and industry. While utilizing natural language to coordinate multiple agents presents a promising avenue for democratizing agent technology for general users, designing coordination strategies remains challenging with existing coordination frameworks. This difficulty stems from the inherent ambiguity of natural language for specifying the collaboration process and the significant cognitive effort required to extract crucial information (e.g. agent relationship, task dependency, result correspondence) from a vast amount of text-form content during exploration. In this work, we present a visual exploration framework to facilitate the design of coordination strategies in multi-agent collaboration. We first establish a structured representation for LLM-based multi-agent coordination strategy to regularize the ambiguity of natural language. Based on this structure, we devise a three-stage generation method that leverages LLMs to convert a user's general goal into an executable initial coordination strategy. Users can further intervene at any stage of the generation process, utilizing LLMs and a set of interactions to explore alternative strategies. Whenever a satisfactory strategy is identified, users can commence the collaboration and examine the visually enhanced execution result. We develop AgentCoord, a prototype interactive system, and conduct a formal user study to demonstrate the feasibility and effectiveness of our approach.","sentences":["The potential of automatic task-solving through Large Language Model (LLM)-based multi-agent collaboration has recently garnered widespread attention from both the research community and industry.","While utilizing natural language to coordinate multiple agents presents a promising avenue for democratizing agent technology for general users, designing coordination strategies remains challenging with existing coordination frameworks.","This difficulty stems from the inherent ambiguity of natural language for specifying the collaboration process and the significant cognitive effort required to extract crucial information (e.g. agent relationship, task dependency, result correspondence) from a vast amount of text-form content during exploration.","In this work, we present a visual exploration framework to facilitate the design of coordination strategies in multi-agent collaboration.","We first establish a structured representation for LLM-based multi-agent coordination strategy to regularize the ambiguity of natural language.","Based on this structure, we devise a three-stage generation method that leverages LLMs to convert a user's general goal into an executable initial coordination strategy.","Users can further intervene at any stage of the generation process, utilizing LLMs and a set of interactions to explore alternative strategies.","Whenever a satisfactory strategy is identified, users can commence the collaboration and examine the visually enhanced execution result.","We develop AgentCoord, a prototype interactive system, and conduct a formal user study to demonstrate the feasibility and effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.11943v1","category":"cs.HC"}
{"created":"2024-04-18 06:39:34","title":"Semantic Satellite Communications Based on Generative Foundation Model","abstract":"Satellite communications can provide massive connections and seamless coverage, but they also face several challenges, such as rain attenuation, long propagation delays, and co-channel interference. To improve transmission efficiency and address severe scenarios, semantic communication has become a popular choice, particularly when equipped with foundation models (FMs). In this study, we introduce an FM-based semantic satellite communication framework, termed FMSAT. This framework leverages FM-based segmentation and reconstruction to significantly reduce bandwidth requirements and accurately recover semantic features under high noise and interference. Considering the high speed of satellites, an adaptive encoder-decoder is proposed to protect important features and avoid frequent retransmissions. Meanwhile, a well-received image can provide a reference for repairing damaged images under sudden attenuation. Since acknowledgment feedback is subject to long propagation delays when retransmission is unavoidable, a novel error detection method is proposed to roughly detect semantic errors at the regenerative satellite. With the proposed detectors at both the satellite and the gateway, the quality of the received images can be ensured. The simulation results demonstrate that the proposed method can significantly reduce bandwidth requirements, adapt to complex satellite scenarios, and protect semantic information with an acceptable transmission delay.","sentences":["Satellite communications can provide massive connections and seamless coverage, but they also face several challenges, such as rain attenuation, long propagation delays, and co-channel interference.","To improve transmission efficiency and address severe scenarios, semantic communication has become a popular choice, particularly when equipped with foundation models (FMs).","In this study, we introduce an FM-based semantic satellite communication framework, termed FMSAT.","This framework leverages FM-based segmentation and reconstruction to significantly reduce bandwidth requirements and accurately recover semantic features under high noise and interference.","Considering the high speed of satellites, an adaptive encoder-decoder is proposed to protect important features and avoid frequent retransmissions.","Meanwhile, a well-received image can provide a reference for repairing damaged images under sudden attenuation.","Since acknowledgment feedback is subject to long propagation delays when retransmission is unavoidable, a novel error detection method is proposed to roughly detect semantic errors at the regenerative satellite.","With the proposed detectors at both the satellite and the gateway, the quality of the received images can be ensured.","The simulation results demonstrate that the proposed method can significantly reduce bandwidth requirements, adapt to complex satellite scenarios, and protect semantic information with an acceptable transmission delay."],"url":"http://arxiv.org/abs/2404.11941v1","category":"eess.SP"}
{"created":"2024-04-18 06:35:01","title":"A variational discretization method for mean curvature flows by the Onsager principle","abstract":"The mean curvature flow describes the evolution of a surface(a curve) with normal velocity proportional to the local mean curvature. It has many applications in mathematics, science and engineering. In this paper, we develop a numerical method for mean curvature flows by using the Onsager principle as an approximation tool. We first show that the mean curvature flow can be derived naturally from the Onsager variational principle. Then we consider a piecewisely linear approximation of the curve and derive a discrete geometric flow. The discrete flow is described by a system of ordinary differential equations for the nodes of the discrete curve. We prove that the discrete system preserve the energy dissipation structure in the framework of the Onsager principle and this implies the energy decreasing property. The ODE system can be solved by an improved Euler scheme and this leads to an efficient fully discrete scheme. We first consider the method for a simple mean curvature flow and then extend it to volume preserving mean curvature flow and also a wetting problem on substrates. Numerical examples show that the method has optimal convergence rate and works well for all the three problems.","sentences":["The mean curvature flow describes the evolution of a surface(a curve) with normal velocity proportional to the local mean curvature.","It has many applications in mathematics, science and engineering.","In this paper, we develop a numerical method for mean curvature flows by using the Onsager principle as an approximation tool.","We first show that the mean curvature flow can be derived naturally from the Onsager variational principle.","Then we consider a piecewisely linear approximation of the curve and derive a discrete geometric flow.","The discrete flow is described by a system of ordinary differential equations for the nodes of the discrete curve.","We prove that the discrete system preserve the energy dissipation structure in the framework of the Onsager principle and this implies the energy decreasing property.","The ODE system can be solved by an improved Euler scheme and this leads to an efficient fully discrete scheme.","We first consider the method for a simple mean curvature flow and then extend it to volume preserving mean curvature flow and also a wetting problem on substrates.","Numerical examples show that the method has optimal convergence rate and works well for all the three problems."],"url":"http://arxiv.org/abs/2404.11935v1","category":"math.NA"}
{"created":"2024-04-18 06:20:07","title":"Entanglement generation between two comoving Unruh-DeWitt detectors in the cosmological de Sitter spacetime","abstract":"We investigate the entanglement generation or harvesting between two identical Unruh-DeWitt detectors in the cosmological de Sitter spacetime. We consider two comoving two-level detectors at a coincident spatial position. The detectors are assumed to be unentangled initially. The detectors are individually coupled to a scalar field, which eventually leads to coupling between the two detectors. We consider two kinds of scalar fields -- conformally symmetric and massless minimally coupled, for both real and complex cases. By tracing out the degrees of freedom corresponding to the scalar field, we construct the reduced density matrix for the two detectors, whose eigenvalues characterise transitions between the energy levels of the detectors. By using the existing results for the detector response functions per unit proper time for these fields, we next compute the logarithmic negativity, quantifying the degree of entanglement generated at late times between the two detectors. The similarities and differences of these results for different kind of scalar fields have been discussed.","sentences":["We investigate the entanglement generation or harvesting between two identical Unruh-DeWitt detectors in the cosmological de Sitter spacetime.","We consider two comoving two-level detectors at a coincident spatial position.","The detectors are assumed to be unentangled initially.","The detectors are individually coupled to a scalar field, which eventually leads to coupling between the two detectors.","We consider two kinds of scalar fields -- conformally symmetric and massless minimally coupled, for both real and complex cases.","By tracing out the degrees of freedom corresponding to the scalar field, we construct the reduced density matrix for the two detectors, whose eigenvalues characterise transitions between the energy levels of the detectors.","By using the existing results for the detector response functions per unit proper time for these fields, we next compute the logarithmic negativity, quantifying the degree of entanglement generated at late times between the two detectors.","The similarities and differences of these results for different kind of scalar fields have been discussed."],"url":"http://arxiv.org/abs/2404.11931v1","category":"gr-qc"}
{"created":"2024-04-18 06:05:19","title":"A Self-Consistent Treatment of the Line-Driving Radiation Force for Active Galactic Nuclei Outflows: New Prescriptions for Simulations","abstract":"Flows driven by photons have been studied for almost a century, and a quantitative description of the radiative forces on atoms and ions is important for understanding a wide variety of systems with outflows and accretion disks, such as active galactic nuclei. Quantifying the associated forces is crucial to determining how these outflows enable interactive mechanisms within these environments, such as AGN feedback. The total number of spectral lines in any given ion of the outflow material must be tabulated in order to give a complete characterization of this force. Here we provide calculations of the dimensionless line force multiplier for AGN environments. For a wide array of representative AGN sources, we explicitly calculate the photoionization balance at the proposed wind-launching region above the accretion disk, compute the strength of the line-driving force on the gas, and revisit and formalize the role of the commonly-used ionization parameter $\\xi$ in ultimately determining the line-driving force. We perform these computations and analyses for a variety of AGN central source properties, such as black hole mass, initial wind velocity, and number density. We find that, while useful, the ionization parameter provides an incomplete description of the overall ionization state of the outflow material. We use these findings to provide an updated method for calculating the strength of the radiative line-driving using both the X-ray spectral index $\\Gamma_X$ and the ionization parameter.","sentences":["Flows driven by photons have been studied for almost a century, and a quantitative description of the radiative forces on atoms and ions is important for understanding a wide variety of systems with outflows and accretion disks, such as active galactic nuclei.","Quantifying the associated forces is crucial to determining how these outflows enable interactive mechanisms within these environments, such as AGN feedback.","The total number of spectral lines in any given ion of the outflow material must be tabulated in order to give a complete characterization of this force.","Here we provide calculations of the dimensionless line force multiplier for AGN environments.","For a wide array of representative AGN sources, we explicitly calculate the photoionization balance at the proposed wind-launching region above the accretion disk, compute the strength of the line-driving force on the gas, and revisit and formalize the role of the commonly-used ionization parameter $\\xi$ in ultimately determining the line-driving force.","We perform these computations and analyses for a variety of AGN central source properties, such as black hole mass, initial wind velocity, and number density.","We find that, while useful, the ionization parameter provides an incomplete description of the overall ionization state of the outflow material.","We use these findings to provide an updated method for calculating the strength of the radiative line-driving using both the X-ray spectral index $\\Gamma_X$ and the ionization parameter."],"url":"http://arxiv.org/abs/2404.11926v1","category":"astro-ph.GA"}
{"created":"2024-04-18 05:50:21","title":"TeachNow: Enabling Teachers to Provide Spontaneous, Realtime 1:1 Help in Massive Online Courses","abstract":"One-on-one help from a teacher is highly impactful for students, yet extremely challenging to support in massive online courses (MOOCs). In this work, we present TeachNow: a novel system that lets volunteer teachers from anywhere in the world instantly provide 1:1 help sessions to students in MOOCs, without any scheduling or coordination overhead. TeachNow works by quickly finding an online student to help and putting them in a collaborative working session with the teacher. The spontaneous, on-demand nature of TeachNow gives teachers the flexibility to help whenever their schedule allows.   We share our experiences deploying TeachNow as an experimental feature in a six week online CS1 course with 9,000 students and 600 volunteer teachers. Even as an optional activity, TeachNow was used by teachers to provide over 12,300 minutes of 1:1 help to 375 unique students. Through a carefully designed randomised control trial, we show that TeachNow sessions increased student course retention rate by almost 15%. Moreover, the flexibility of our system captured valuable volunteer time that would otherwise go to waste. Lastly, TeachNow was rated by teachers as one of the most enjoyable and impactful aspects of their involvement in the course. We believe TeachNow is an important step towards providing more human-centered support in massive online courses.","sentences":["One-on-one help from a teacher is highly impactful for students, yet extremely challenging to support in massive online courses (MOOCs).","In this work, we present TeachNow: a novel system that lets volunteer teachers from anywhere in the world instantly provide 1:1 help sessions to students in MOOCs, without any scheduling or coordination overhead.","TeachNow works by quickly finding an online student to help and putting them in a collaborative working session with the teacher.","The spontaneous, on-demand nature of TeachNow gives teachers the flexibility to help whenever their schedule allows.   ","We share our experiences deploying TeachNow as an experimental feature in a six week online CS1 course with 9,000 students and 600 volunteer teachers.","Even as an optional activity, TeachNow was used by teachers to provide over 12,300 minutes of 1:1 help to 375 unique students.","Through a carefully designed randomised control trial, we show that TeachNow sessions increased student course retention rate by almost 15%.","Moreover, the flexibility of our system captured valuable volunteer time that would otherwise go to waste.","Lastly, TeachNow was rated by teachers as one of the most enjoyable and impactful aspects of their involvement in the course.","We believe TeachNow is an important step towards providing more human-centered support in massive online courses."],"url":"http://arxiv.org/abs/2404.11918v1","category":"cs.CY"}
{"created":"2024-04-18 05:25:54","title":"TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding","abstract":"With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable to long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.","sentences":["With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support.","However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length.","Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency.","While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality.","We introduce TriForce, a hierarchical speculative decoding system that is scalable to long sequence generation.","This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency.","TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in handling even longer contexts.","For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\\times$ on our optimized offloading system.","Additionally, TriForce performs 4.86$\\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU.","TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures.","The code is available at https://github.com/Infini-AI-Lab/TriForce."],"url":"http://arxiv.org/abs/2404.11912v1","category":"cs.CL"}
{"created":"2024-04-18 05:22:24","title":"Calibration of hydrogen atoms measurement by femtosecond two-photon laser induced fluorescence","abstract":"A new calibration method for H-fs-TALIF is proposed, and the ratio of two-photon absorption cross-sections $\\sigma^{(2)}$ for atomic hydrogen (H) and krypton (Kr) is determined for the broadband emission of a femtosecond laser system. The obtained estimate of the ratio of two-photon absorption cross-sections $\\sigma^{(2)}$ for H and Kr $\\sigma^{(2)}$(Kr)/$\\sigma^{(2)}$(H) = $0.034\\pm 0.006$, which is almost twenty times lower than the values previously obtained using narrowband nanosecond lasers. This difference is explained by the significantly different spectral width of the excitation line, and demonstrates the need for independent calibration of the TALIF measurements in the femtosecond range.","sentences":["A new calibration method for H-fs-TALIF is proposed, and the ratio of two-photon absorption cross-sections $\\sigma^{(2)}$ for atomic hydrogen (H) and krypton (Kr) is determined for the broadband emission of a femtosecond laser system.","The obtained estimate of the ratio of two-photon absorption cross-sections $\\sigma^{(2)}$ for H and Kr $\\sigma^{(2)}$(Kr)/$\\sigma^{(2)}$(H) = $0.034\\pm 0.006$, which is almost twenty times lower than the values previously obtained using narrowband nanosecond lasers.","This difference is explained by the significantly different spectral width of the excitation line, and demonstrates the need for independent calibration of the TALIF measurements in the femtosecond range."],"url":"http://arxiv.org/abs/2404.11909v1","category":"physics.atom-ph"}
{"created":"2024-04-18 05:17:23","title":"Asymptotics of Spectral Functions of Lower Energy Forms on Weakly 1-Complete Manifolds","abstract":"In this paper, we show that the optimal fundamental estimate holds true on a weakly $1$-complete manifold with mild conditions, then we establish the weak Morse inequalities for lower energy forms on the manifold. We also study the case for $q$-convex manifolds.","sentences":["In this paper, we show that the optimal fundamental estimate holds true on a weakly $1$-complete manifold with mild conditions, then we establish the weak Morse inequalities for lower energy forms on the manifold.","We also study the case for $q$-convex manifolds."],"url":"http://arxiv.org/abs/2404.11908v1","category":"math.CV"}
{"created":"2024-04-18 05:04:00","title":"Deep and Dynamic Metabolic and Structural Imaging in Living Tissues","abstract":"Label-free imaging through two-photon autofluorescence (2PAF) of NAD(P)H allows for non-destructive and high-resolution visualization of cellular activities in living systems. However, its application to thick tissues and organoids has been restricted by its limited penetration depth within 300 $\\mu$m, largely due to tissue scattering at the typical excitation wavelength (~750 nm) required for NAD(P)H. Here, we demonstrate that the imaging depth for NAD(P)H can be extended to over 700 $\\mu$m in living engineered human multicellular microtissues by adopting multimode fiber (MMF)-based low-repetition-rate high-peak-power three-photon (3P) excitation of NAD(P)H at 1100 nm. This is achieved by having over 0.5 MW peak power at the band of 1100$\\pm$25 nm through adaptively modulating multimodal nonlinear pulse propagation with a compact fiber shaper. Moreover, the 8-fold increase in pulse energy at 1100 nm enables faster imaging of monocyte behaviors in the living multicellular models. These results represent a significant advance for deep and dynamic metabolic and structural imaging of intact living biosystems. The modular design (MMF with a slip-on fiber shaper) is anticipated to allow wide adoption of this methodology for demanding in vivo and in vitro imaging applications, including cancer research, autoimmune diseases, and tissue engineering.","sentences":["Label-free imaging through two-photon autofluorescence (2PAF) of NAD(P)H allows for non-destructive and high-resolution visualization of cellular activities in living systems.","However, its application to thick tissues and organoids has been restricted by its limited penetration depth within 300 $\\mu$m, largely due to tissue scattering at the typical excitation wavelength (~750 nm) required for NAD(P)H.","Here, we demonstrate that the imaging depth for NAD(P)H can be extended to over 700 $\\mu$m in living engineered human multicellular microtissues by adopting multimode fiber (MMF)-based low-repetition-rate high-peak-power three-photon (3P) excitation of NAD(P)H at 1100 nm.","This is achieved by having over 0.5 MW peak power at the band of 1100$\\pm$25 nm through adaptively modulating multimodal nonlinear pulse propagation with a compact fiber shaper.","Moreover, the 8-fold increase in pulse energy at 1100 nm enables faster imaging of monocyte behaviors in the living multicellular models.","These results represent a significant advance for deep and dynamic metabolic and structural imaging of intact living biosystems.","The modular design (MMF with a slip-on fiber shaper) is anticipated to allow wide adoption of this methodology for demanding in vivo and in vitro imaging applications, including cancer research, autoimmune diseases, and tissue engineering."],"url":"http://arxiv.org/abs/2404.11901v1","category":"physics.optics"}
{"created":"2024-04-18 05:02:27","title":"A New Hybrid Automaton Framework with Partial Differential Equation Dynamics","abstract":"This paper presents the syntax and semantics of a novel type of hybrid automaton (HA) with partial differential equation (PDE) dynamic, partial differential hybrid automata (PDHA). In PDHA, we add a spatial domain $X$ and harness a mathematic conception, partition, to help us formally define the spatial relations. While classically the dynamics of HA are described by ordinary differential equations (ODEs) and differential inclusions, PDHA is capable of describing the behavior of cyber-physical systems (CPS) with continuous dynamics that cannot be modelled using the canonical hybrid systems' framework. For the purposes of analyzing PDHA, we propose another model called the discrete space partial differential hybrid automata (DSPDHA) which handles discrete spatial domains using finite difference methods (FDM) and this simple and intuitive approach reduces the PDHA into HA with ODE systems. We conclude with two illustrative examples in order to exhibit the nature of PDHA and DSPDHA.","sentences":["This paper presents the syntax and semantics of a novel type of hybrid automaton (HA) with partial differential equation (PDE) dynamic, partial differential hybrid automata (PDHA).","In PDHA, we add a spatial domain $X$ and harness a mathematic conception, partition, to help us formally define the spatial relations.","While classically the dynamics of HA are described by ordinary differential equations (ODEs) and differential inclusions, PDHA is capable of describing the behavior of cyber-physical systems (CPS) with continuous dynamics that cannot be modelled using the canonical hybrid systems' framework.","For the purposes of analyzing PDHA, we propose another model called the discrete space partial differential hybrid automata (DSPDHA) which handles discrete spatial domains using finite difference methods (FDM) and this simple and intuitive approach reduces the PDHA into HA with ODE systems.","We conclude with two illustrative examples in order to exhibit the nature of PDHA and DSPDHA."],"url":"http://arxiv.org/abs/2404.11900v1","category":"eess.SY"}
{"created":"2024-04-18 05:01:27","title":"Investigating the Molecular Design Mechanism Behind the Hydrophobicity of Biological Surface Nanostructures: Insights from Butterfly and Mosquito Systems","abstract":"Wettability is a fundamental physicochemical property of solid surfaces, with unique wettability patterns playing pivotal roles across diverse domains. Inspired by nature's ingenious designs, bio-inspired materials have emerged as a frontier of scientific inquiry. They showcase remarkable hydrophobic properties observed in phenomena such as mosquitoes preventing fog condensation, and lotus leaves exhibiting self-cleaning attributes. This groundbreaking research delves into the hydrophobic characteristics of biomimetic surfaces using coarse-grained molecular simulation and the free energy barrier evaluation system. By analyzing the butterfly wings and mosquito eyes model, we aim to pioneer a comprehensive framework that factors in the influence of surface parameters on the free energy barrier. Through meticulous simulation and analysis, we strive to validate and enhance the reliability of the free energy barrier assessment method, deepening our understanding of hydrophobicity across diverse biomaterials and paving the way for optimizing their properties for a myriad of applications. During our investigation, we shed light on the elusive intermediate state, a departure from the typical Cassie or Wenzel state, enriching our theoretical framework for surfaces with distinctive properties. This research is a catalyst for developing biomimetic materials with superior hydrophobic characteristics and innovative fabrication processes, transcending academic boundaries and promising significant strides in environmental conservation, medicine, and beyond, offering hope for a greener, healthier, and more sustainable future.","sentences":["Wettability is a fundamental physicochemical property of solid surfaces, with unique wettability patterns playing pivotal roles across diverse domains.","Inspired by nature's ingenious designs, bio-inspired materials have emerged as a frontier of scientific inquiry.","They showcase remarkable hydrophobic properties observed in phenomena such as mosquitoes preventing fog condensation, and lotus leaves exhibiting self-cleaning attributes.","This groundbreaking research delves into the hydrophobic characteristics of biomimetic surfaces using coarse-grained molecular simulation and the free energy barrier evaluation system.","By analyzing the butterfly wings and mosquito eyes model, we aim to pioneer a comprehensive framework that factors in the influence of surface parameters on the free energy barrier.","Through meticulous simulation and analysis, we strive to validate and enhance the reliability of the free energy barrier assessment method, deepening our understanding of hydrophobicity across diverse biomaterials and paving the way for optimizing their properties for a myriad of applications.","During our investigation, we shed light on the elusive intermediate state, a departure from the typical Cassie or Wenzel state, enriching our theoretical framework for surfaces with distinctive properties.","This research is a catalyst for developing biomimetic materials with superior hydrophobic characteristics and innovative fabrication processes, transcending academic boundaries and promising significant strides in environmental conservation, medicine, and beyond, offering hope for a greener, healthier, and more sustainable future."],"url":"http://arxiv.org/abs/2404.11899v1","category":"physics.bio-ph"}
{"created":"2024-04-18 04:47:28","title":"FreeDiff: Progressive Frequency Truncation for Image Editing with Diffusion Models","abstract":"Precise image editing with text-to-image models has attracted increasing interest due to their remarkable generative capabilities and user-friendly nature. However, such attempts face the pivotal challenge of misalignment between the intended precise editing target regions and the broader area impacted by the guidance in practice. Despite excellent methods leveraging attention mechanisms that have been developed to refine the editing guidance, these approaches necessitate modifications through complex network architecture and are limited to specific editing tasks. In this work, we re-examine the diffusion process and misalignment problem from a frequency perspective, revealing that, due to the power law of natural images and the decaying noise schedule, the denoising network primarily recovers low-frequency image components during the earlier timesteps and thus brings excessive low-frequency signals for editing. Leveraging this insight, we introduce a novel fine-tuning free approach that employs progressive $\\textbf{Fre}$qu$\\textbf{e}$ncy truncation to refine the guidance of $\\textbf{Diff}$usion models for universal editing tasks ($\\textbf{FreeDiff}$). Our method achieves comparable results with state-of-the-art methods across a variety of editing tasks and on a diverse set of images, highlighting its potential as a versatile tool in image editing applications.","sentences":["Precise image editing with text-to-image models has attracted increasing interest due to their remarkable generative capabilities and user-friendly nature.","However, such attempts face the pivotal challenge of misalignment between the intended precise editing target regions and the broader area impacted by the guidance in practice.","Despite excellent methods leveraging attention mechanisms that have been developed to refine the editing guidance, these approaches necessitate modifications through complex network architecture and are limited to specific editing tasks.","In this work, we re-examine the diffusion process and misalignment problem from a frequency perspective, revealing that, due to the power law of natural images and the decaying noise schedule, the denoising network primarily recovers low-frequency image components during the earlier timesteps and thus brings excessive low-frequency signals for editing.","Leveraging this insight, we introduce a novel fine-tuning free approach that employs progressive $\\textbf{Fre}$qu$\\textbf{e}$ncy truncation to refine the guidance of $\\textbf{Diff}$usion models for universal editing tasks ($\\textbf{FreeDiff}$).","Our method achieves comparable results with state-of-the-art methods across a variety of editing tasks and on a diverse set of images, highlighting its potential as a versatile tool in image editing applications."],"url":"http://arxiv.org/abs/2404.11895v1","category":"cs.CV"}
{"created":"2024-04-18 04:44:51","title":"Derivative-Free Optimization via Adaptive Sampling Strategies","abstract":"In this paper, we present a novel derivative-free optimization framework for solving unconstrained stochastic optimization problems. Many problems in fields ranging from simulation optimization to reinforcement learning involve settings where only stochastic function values are obtained via an oracle with no available gradient information, necessitating the usage of derivative-free optimization methodologies. Our approach includes estimating gradients using stochastic function evaluations and integrating adaptive sampling techniques to control the accuracy in these stochastic approximations. We consider various gradient estimation techniques including standard finite difference, Gaussian smoothing, sphere smoothing, randomized coordinate finite difference, and randomized subspace finite difference methods. We provide theoretical convergence guarantees for our framework and analyze the worst-case iteration and sample complexities associated with each gradient estimation method. Finally, we demonstrate the empirical performance of the methods on logistic regression and nonlinear least squares problems.","sentences":["In this paper, we present a novel derivative-free optimization framework for solving unconstrained stochastic optimization problems.","Many problems in fields ranging from simulation optimization to reinforcement learning involve settings where only stochastic function values are obtained via an oracle with no available gradient information, necessitating the usage of derivative-free optimization methodologies.","Our approach includes estimating gradients using stochastic function evaluations and integrating adaptive sampling techniques to control the accuracy in these stochastic approximations.","We consider various gradient estimation techniques including standard finite difference, Gaussian smoothing, sphere smoothing, randomized coordinate finite difference, and randomized subspace finite difference methods.","We provide theoretical convergence guarantees for our framework and analyze the worst-case iteration and sample complexities associated with each gradient estimation method.","Finally, we demonstrate the empirical performance of the methods on logistic regression and nonlinear least squares problems."],"url":"http://arxiv.org/abs/2404.11893v1","category":"math.OC"}
{"created":"2024-04-18 04:24:02","title":"EN-TensorCore: Advancing TensorCores Performance through Encoder-Based Methodology","abstract":"Tensor computations, with matrix multiplication being the primary operation, serve as the fundamental basis for data analysis, physics, machine learning, and deep learning. As the scale and complexity of data continue to grow rapidly, the demand for tensor computations has also increased significantly. To meet this demand, several research institutions have started developing dedicated hardware for tensor computations. To further improve the computational performance of tensor process units, we have reexamined the issue of computation reuse that was previously overlooked in existing architectures. As a result, we propose a novel EN-TensorCore architecture that can significantly reduce chip area and power consumption. Furthermore, our method is compatible with existing tensor processing architectures. We evaluated our method on prevalent microarchitectures, the results demonstrate an average improvement in area efficiency of 8.7\\%, 12.2\\%, and 11.0\\% for tensor computing units at computational scales of 256 GOPS, 1 TOPS, and 4 TOPS, respectively. Similarly, there were energy efficiency enhancements of 13.0\\%, 17.5\\%, and 15.5\\%.","sentences":["Tensor computations, with matrix multiplication being the primary operation, serve as the fundamental basis for data analysis, physics, machine learning, and deep learning.","As the scale and complexity of data continue to grow rapidly, the demand for tensor computations has also increased significantly.","To meet this demand, several research institutions have started developing dedicated hardware for tensor computations.","To further improve the computational performance of tensor process units, we have reexamined the issue of computation reuse that was previously overlooked in existing architectures.","As a result, we propose a novel EN-TensorCore architecture that can significantly reduce chip area and power consumption.","Furthermore, our method is compatible with existing tensor processing architectures.","We evaluated our method on prevalent microarchitectures, the results demonstrate an average improvement in area efficiency of 8.7\\%, 12.2\\%, and 11.0\\% for tensor computing units at computational scales of 256 GOPS, 1 TOPS, and 4 TOPS, respectively.","Similarly, there were energy efficiency enhancements of 13.0\\%, 17.5\\%, and 15.5\\%."],"url":"http://arxiv.org/abs/2404.11887v1","category":"cs.AR"}
{"created":"2024-04-18 04:07:53","title":"Quantitative bordism over acyclic groups and Cheeger-Gromov $\u03c1$-invariants","abstract":"We prove a bordism version of Gromov's linearity conjecture over a large family of acyclic groups. Since all groups embed into these acyclic groups, it follows that the linear bordism conjecture is true if one allows to enlarge a given group. Our result holds in both PL and smooth categories, and for both oriented and unoriented cases. The method of the proof hinges on quantitative algebraic and geometric techniques over infinite complexes with unbounded local (combinatorial) geometry, which seem interesting on their own. As an application, we prove that there is a universal linear bound for the Cheeger-Gromov $L^2$ $\\rho$-invariants of PL $(4k-1)$-manifolds associated with arbitrary regular covers.","sentences":["We prove a bordism version of Gromov's linearity conjecture over a large family of acyclic groups.","Since all groups embed into these acyclic groups, it follows that the linear bordism conjecture is true if one allows to enlarge a given group.","Our result holds in both PL and smooth categories, and for both oriented and unoriented cases.","The method of the proof hinges on quantitative algebraic and geometric techniques over infinite complexes with unbounded local (combinatorial) geometry, which seem interesting on their own.","As an application, we prove that there is a universal linear bound for the Cheeger-Gromov $L^2$ $\\rho$-invariants of PL $(4k-1)$-manifolds associated with arbitrary regular covers."],"url":"http://arxiv.org/abs/2404.11885v1","category":"math.GT"}
{"created":"2024-04-18 03:58:27","title":"Seeing Motion at Nighttime with an Event Camera","abstract":"We focus on a very challenging task: imaging at nighttime dynamic scenes. Most previous methods rely on the low-light enhancement of a conventional RGB camera. However, they would inevitably face a dilemma between the long exposure time of nighttime and the motion blur of dynamic scenes. Event cameras react to dynamic changes with higher temporal resolution (microsecond) and higher dynamic range (120dB), offering an alternative solution. In this work, we present a novel nighttime dynamic imaging method with an event camera. Specifically, we discover that the event at nighttime exhibits temporal trailing characteristics and spatial non-stationary distribution. Consequently, we propose a nighttime event reconstruction network (NER-Net) which mainly includes a learnable event timestamps calibration module (LETC) to align the temporal trailing events and a non-uniform illumination aware module (NIAM) to stabilize the spatiotemporal distribution of events. Moreover, we construct a paired real low-light event dataset (RLED) through a co-axial imaging system, including 64,200 spatially and temporally aligned image GTs and low-light events. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art methods in terms of visual quality and generalization ability on real-world nighttime datasets. The project are available at: https://github.com/Liu-haoyue/NER-Net.","sentences":["We focus on a very challenging task: imaging at nighttime dynamic scenes.","Most previous methods rely on the low-light enhancement of a conventional RGB camera.","However, they would inevitably face a dilemma between the long exposure time of nighttime and the motion blur of dynamic scenes.","Event cameras react to dynamic changes with higher temporal resolution (microsecond) and higher dynamic range (120dB), offering an alternative solution.","In this work, we present a novel nighttime dynamic imaging method with an event camera.","Specifically, we discover that the event at nighttime exhibits temporal trailing characteristics and spatial non-stationary distribution.","Consequently, we propose a nighttime event reconstruction network (NER-Net) which mainly includes a learnable event timestamps calibration module (LETC) to align the temporal trailing events and a non-uniform illumination aware module (NIAM) to stabilize the spatiotemporal distribution of events.","Moreover, we construct a paired real low-light event dataset (RLED) through a co-axial imaging system, including 64,200 spatially and temporally aligned image GTs and low-light events.","Extensive experiments demonstrate that the proposed method outperforms state-of-the-art methods in terms of visual quality and generalization ability on real-world nighttime datasets.","The project are available at: https://github.com/Liu-haoyue/NER-Net."],"url":"http://arxiv.org/abs/2404.11884v1","category":"cs.CV"}
{"created":"2024-04-18 03:29:22","title":"Finding the particularity of the active episode of SGR J1935+2154 during which FRB 20200428 occurred: Implication from a statistics of \\textit{Fermi}/GBM X-ray bursts","abstract":"By using the \\textit{Fermi}/GBM data of the X-ray bursts (XRBs) of SGR J1935+2154, we investigate the temporal clustering of the bursts and the cumulative distribution of the waiting time and fluence/flux. It is found that the bursts occurring in the episode hosting FRB 20200428 have obviously shorter waiting times than those in the other episodes. The general statistical properties of the XRBs further indicate they could belong to a self-organized critical (SOC) system (e.g., starquakes), making them very similar to the earthquake phenomena. Then, according to a unified scaling law between the waiting time and energy of the earthquakes as well as their aftershocks, we implement an analogy analysis on the XRBs and find that the FRB episode owns more dependent burst events than the other episodes. It is indicated that the FRB emission could be produced by the interaction between different burst events, which could correspond to a collision between different seismic/Alfven waves or different explosion outflows. Such a situation could appear when the magnetar enters into a global intensive activity period.","sentences":["By using the \\textit{Fermi}/GBM data of the X-ray bursts (XRBs) of SGR J1935+2154, we investigate the temporal clustering of the bursts and the cumulative distribution of the waiting time and fluence/flux.","It is found that the bursts occurring in the episode hosting FRB 20200428 have obviously shorter waiting times than those in the other episodes.","The general statistical properties of the XRBs further indicate they could belong to a self-organized critical (SOC) system (e.g., starquakes), making them very similar to the earthquake phenomena.","Then, according to a unified scaling law between the waiting time and energy of the earthquakes as well as their aftershocks, we implement an analogy analysis on the XRBs and find that the FRB episode owns more dependent burst events than the other episodes.","It is indicated that the FRB emission could be produced by the interaction between different burst events, which could correspond to a collision between different seismic/Alfven waves or different explosion outflows.","Such a situation could appear when the magnetar enters into a global intensive activity period."],"url":"http://arxiv.org/abs/2404.11877v1","category":"astro-ph.HE"}
{"created":"2024-04-18 03:25:05","title":"CelluloTactix: Towards Empowering Collaborative Online Learning through Tangible Haptic Interaction with Cellulo Robots","abstract":"Online learning has soared in popularity in the educational landscape of COVID-19 and carries the benefits of increased flexibility and access to far-away training resources. However, it also restricts communication between peers and teachers, limits physical interactions and confines learning to the computer screen and keyboard. In this project, we designed a novel way to engage students in collaborative online learning by using haptic-enabled tangible robots, Cellulo. We built a library which connects two robots remotely for a learning activity based around the structure of a biological cell. To discover how separate modes of haptic feedback might differentially affect collaboration, two modes of haptic force-feedback were implemented (haptic co-location and haptic consensus). With a case study, we found that the haptic co-location mode seemed to stimulate collectivist behaviour to a greater extent than the haptic consensus mode, which was associated with individualism and less interaction. While the haptic co-location mode seemed to encourage information pooling, participants using the haptic consensus mode tended to focus more on technical co-ordination. This work introduces a novel system that can provide interesting insights on how to integrate haptic feedback into collaborative remote learning activities in future.","sentences":["Online learning has soared in popularity in the educational landscape of COVID-19 and carries the benefits of increased flexibility and access to far-away training resources.","However, it also restricts communication between peers and teachers, limits physical interactions and confines learning to the computer screen and keyboard.","In this project, we designed a novel way to engage students in collaborative online learning by using haptic-enabled tangible robots, Cellulo.","We built a library which connects two robots remotely for a learning activity based around the structure of a biological cell.","To discover how separate modes of haptic feedback might differentially affect collaboration, two modes of haptic force-feedback were implemented (haptic co-location and haptic consensus).","With a case study, we found that the haptic co-location mode seemed to stimulate collectivist behaviour to a greater extent than the haptic consensus mode, which was associated with individualism and less interaction.","While the haptic co-location mode seemed to encourage information pooling, participants using the haptic consensus mode tended to focus more on technical co-ordination.","This work introduces a novel system that can provide interesting insights on how to integrate haptic feedback into collaborative remote learning activities in future."],"url":"http://arxiv.org/abs/2404.11876v1","category":"cs.HC"}
{"created":"2024-04-18 02:59:48","title":"OPTiML: Dense Semantic Invariance Using Optimal Transport for Self-Supervised Medical Image Representation","abstract":"Self-supervised learning (SSL) has emerged as a promising technique for medical image analysis due to its ability to learn without annotations. However, despite the promising potential, conventional SSL methods encounter limitations, including challenges in achieving semantic alignment and capturing subtle details. This leads to suboptimal representations, which fail to accurately capture the underlying anatomical structures and pathological details. In response to these constraints, we introduce a novel SSL framework OPTiML, employing optimal transport (OT), to capture the dense semantic invariance and fine-grained details, thereby enhancing the overall effectiveness of SSL in medical image representation learning. The core idea is to integrate OT with a cross-viewpoint semantics infusion module (CV-SIM), which effectively captures complex, fine-grained details inherent in medical images across different viewpoints. In addition to the CV-SIM module, OPTiML imposes the variance and covariance regularizations within OT framework to force the model focus on clinically relevant information while discarding less informative features. Through these, the proposed framework demonstrates its capacity to learn semantically rich representations that can be applied to various medical imaging tasks. To validate its effectiveness, we conduct experimental studies on three publicly available datasets from chest X-ray modality. Our empirical results reveal OPTiML's superiority over state-of-the-art methods across all evaluated tasks.","sentences":["Self-supervised learning (SSL) has emerged as a promising technique for medical image analysis due to its ability to learn without annotations.","However, despite the promising potential, conventional SSL methods encounter limitations, including challenges in achieving semantic alignment and capturing subtle details.","This leads to suboptimal representations, which fail to accurately capture the underlying anatomical structures and pathological details.","In response to these constraints, we introduce a novel SSL framework OPTiML, employing optimal transport (OT), to capture the dense semantic invariance and fine-grained details, thereby enhancing the overall effectiveness of SSL in medical image representation learning.","The core idea is to integrate OT with a cross-viewpoint semantics infusion module (CV-SIM), which effectively captures complex, fine-grained details inherent in medical images across different viewpoints.","In addition to the CV-SIM module, OPTiML imposes the variance and covariance regularizations within OT framework to force the model focus on clinically relevant information while discarding less informative features.","Through these, the proposed framework demonstrates its capacity to learn semantically rich representations that can be applied to various medical imaging tasks.","To validate its effectiveness, we conduct experimental studies on three publicly available datasets from chest X-ray modality.","Our empirical results reveal OPTiML's superiority over state-of-the-art methods across all evaluated tasks."],"url":"http://arxiv.org/abs/2404.11868v1","category":"cs.CV"}
{"created":"2024-04-18 02:43:37","title":"From Image to Video, what do we need in multimodal LLMs?","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated profound capabilities in understanding multimodal information, covering from Image LLMs to the more complex Video LLMs. Numerous studies have illustrated their exceptional cross-modal comprehension. Recently, integrating video foundation models with large language models to build a comprehensive video understanding system has been proposed to overcome the limitations of specific pre-defined vision tasks. However, the current advancements in Video LLMs tend to overlook the foundational contributions of Image LLMs, often opting for more complicated structures and a wide variety of multimodal data for pre-training. This approach significantly increases the costs associated with these methods.In response to these challenges, this work introduces an efficient method that strategically leverages the priors of Image LLMs, facilitating a resource-efficient transition from Image to Video LLMs. We propose RED-VILLM, a Resource-Efficient Development pipeline for Video LLMs from Image LLMs, which utilizes a temporal adaptation plug-and-play structure within the image fusion module of Image LLMs. This adaptation extends their understanding capabilities to include temporal information, enabling the development of Video LLMs that not only surpass baseline performances but also do so with minimal instructional data and training resources. Our approach highlights the potential for a more cost-effective and scalable advancement in multimodal models, effectively building upon the foundational work of Image LLMs.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated profound capabilities in understanding multimodal information, covering from Image LLMs to the more complex Video LLMs.","Numerous studies have illustrated their exceptional cross-modal comprehension.","Recently, integrating video foundation models with large language models to build a comprehensive video understanding system has been proposed to overcome the limitations of specific pre-defined vision tasks.","However, the current advancements in Video LLMs tend to overlook the foundational contributions of Image LLMs, often opting for more complicated structures and a wide variety of multimodal data for pre-training.","This approach significantly increases the costs associated with these methods.","In response to these challenges, this work introduces an efficient method that strategically leverages the priors of Image LLMs, facilitating a resource-efficient transition from Image to Video LLMs.","We propose RED-VILLM, a Resource-Efficient Development pipeline for Video LLMs from Image LLMs, which utilizes a temporal adaptation plug-and-play structure within the image fusion module of Image LLMs.","This adaptation extends their understanding capabilities to include temporal information, enabling the development of Video LLMs that not only surpass baseline performances but also do so with minimal instructional data and training resources.","Our approach highlights the potential for a more cost-effective and scalable advancement in multimodal models, effectively building upon the foundational work of Image LLMs."],"url":"http://arxiv.org/abs/2404.11865v1","category":"cs.CV"}
{"created":"2024-04-18 02:17:37","title":"On-chip Kerr parametric oscillation with integrated heating for enhanced frequency tuning and control","abstract":"Nonlinear microresonators can convert light from chip-integrated sources into new wavelengths within the visible and near-infrared spectrum. For most applications, such as the interrogation of quantum systems with specific transition wavelengths, tuning the frequency of converted light is critical. Nonetheless, demonstrations of wavelength conversion have mostly overlooked this metric. Here, we apply efficient integrated heaters to tune the idler frequency produced by Kerr optical parametric oscillation in a silicon-nitride microring across a continuous 1.5 terahertz range. Finally, we suppress idler frequency noise between DC and 5 kHz by several orders of magnitude using feedback to the heater drive.","sentences":["Nonlinear microresonators can convert light from chip-integrated sources into new wavelengths within the visible and near-infrared spectrum.","For most applications, such as the interrogation of quantum systems with specific transition wavelengths, tuning the frequency of converted light is critical.","Nonetheless, demonstrations of wavelength conversion have mostly overlooked this metric.","Here, we apply efficient integrated heaters to tune the idler frequency produced by Kerr optical parametric oscillation in a silicon-nitride microring across a continuous 1.5 terahertz range.","Finally, we suppress idler frequency noise between DC and 5 kHz by several orders of magnitude using feedback to the heater drive."],"url":"http://arxiv.org/abs/2404.11855v1","category":"physics.optics"}
{"created":"2024-04-18 02:08:57","title":"Cicero: Addressing Algorithmic and Architectural Bottlenecks in Neural Rendering by Radiance Warping and Memory Optimizations","abstract":"Neural Radiance Field (NeRF) is widely seen as an alternative to traditional physically-based rendering. However, NeRF has not yet seen its adoption in resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR), because it is simply extremely slow. On a mobile Volta GPU, even the state-of-the-art NeRF models generally execute only at 0.8 FPS. We show that the main performance bottlenecks are both algorithmic and architectural. We introduce, CICERO, to tame both forms of inefficiencies. We first introduce two algorithms, one fundamentally reduces the amount of work any NeRF model has to execute, and the other eliminates irregular DRAM accesses. We then describe an on-chip data layout strategy that eliminates SRAM bank conflicts. A pure software implementation of CICERO offers an 8.0x speed-up and 7.9x energy saving over a mobile Volta GPU. When compared to a baseline with a dedicated DNN accelerator, our speed-up and energy reduction increase to 28.2x and 37.8x, respectively - all with minimal quality loss (less than 1.0 dB peak signal-to-noise ratio reduction).","sentences":["Neural Radiance Field (NeRF) is widely seen as an alternative to traditional physically-based rendering.","However, NeRF has not yet seen its adoption in resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR), because it is simply extremely slow.","On a mobile Volta GPU, even the state-of-the-art NeRF models generally execute only at 0.8 FPS.","We show that the main performance bottlenecks are both algorithmic and architectural.","We introduce, CICERO, to tame both forms of inefficiencies.","We first introduce two algorithms, one fundamentally reduces the amount of work any NeRF model has to execute, and the other eliminates irregular DRAM accesses.","We then describe an on-chip data layout strategy that eliminates SRAM bank conflicts.","A pure software implementation of CICERO offers an 8.0x speed-up and 7.9x energy saving over a mobile Volta GPU.","When compared to a baseline with a dedicated DNN accelerator, our speed-up and energy reduction increase to 28.2x and 37.8x, respectively - all with minimal quality loss (less than 1.0 dB peak signal-to-noise ratio reduction)."],"url":"http://arxiv.org/abs/2404.11852v1","category":"cs.AR"}
{"created":"2024-04-18 01:57:38","title":"Experimental Hybrid Shadow Tomography and Distillation","abstract":"Characterization of quantum states is a fundamental requirement in quantum science and technology. As a promising framework, shadow tomography shows significant efficiency in estimating linear functions, however, for the challenging nonlinear ones, it requires measurements at an exponential cost. Here, we implement an advanced shadow protocol, so-called hybrid shadow~(HS) tomography, to reduce the measurement cost in the estimation of nonlinear functions in an optical system. We design and realize a deterministic quantum Fredkin gate with single photon, achieving high process fidelity of $0.935\\pm0.001$. Utilizing this novel Fredkin gate, we demonstrate HS in the estimations, like the higher-order moments up to 4, and reveal that the sample complexity of HS is significantly reduced compared with the original shadow protocol. Furthermore, we utilize these higher-degree functions to implement virtual distillation, which effectively extracts a high-purity quantum state from two noisy copies. The virtual distillation is also verified in a proof-of-principle demonstration of quantum metrology, further enhancing the accuracy of parameter estimation. Our results suggest that HS is efficient in state characterization and promising for quantum technologies.","sentences":["Characterization of quantum states is a fundamental requirement in quantum science and technology.","As a promising framework, shadow tomography shows significant efficiency in estimating linear functions, however, for the challenging nonlinear ones, it requires measurements at an exponential cost.","Here, we implement an advanced shadow protocol, so-called hybrid shadow~(HS) tomography, to reduce the measurement cost in the estimation of nonlinear functions in an optical system.","We design and realize a deterministic quantum Fredkin gate with single photon, achieving high process fidelity of $0.935\\pm0.001$. Utilizing this novel Fredkin gate, we demonstrate HS in the estimations, like the higher-order moments up to 4, and reveal that the sample complexity of HS is significantly reduced compared with the original shadow protocol.","Furthermore, we utilize these higher-degree functions to implement virtual distillation, which effectively extracts a high-purity quantum state from two noisy copies.","The virtual distillation is also verified in a proof-of-principle demonstration of quantum metrology, further enhancing the accuracy of parameter estimation.","Our results suggest that HS is efficient in state characterization and promising for quantum technologies."],"url":"http://arxiv.org/abs/2404.11850v1","category":"quant-ph"}
{"created":"2024-04-18 01:46:31","title":"Computer-Aided Diagnosis of Thoracic Diseases in Chest X-rays using hybrid CNN-Transformer Architecture","abstract":"Medical imaging has been used for diagnosis of various conditions, making it one of the most powerful resources for effective patient care. Due to widespread availability, low cost, and low radiation, chest X-ray is one of the most sought after radiology examination for the diagnosis of various thoracic diseases. Due to advancements in medical imaging technologies and increasing patient load, current radiology workflow faces various challenges including increasing backlogs, working long hours, and increase in diagnostic errors. An automated computer-aided diagnosis system that can interpret chest X-rays to augment radiologists by providing actionable insights has potential to provide second opinion to radiologists, highlight relevant regions in the image, in turn expediting clinical workflow, reducing diagnostic errors, and improving patient care. In this study, we applied a novel architecture augmenting the DenseNet121 Convolutional Neural Network (CNN) with multi-head self-attention mechanism using transformer, namely SA-DenseNet121, that can identify multiple thoracic diseases in chest X-rays. We conducted experiments on four of the largest chest X-ray datasets, namely, ChestX-ray14, CheXpert, MIMIC-CXR-JPG, and IU-CXR. Experimental results in terms of area under the receiver operating characteristics (AUC-ROC) shows that augmenting CNN with self-attention has potential in diagnosing different thoracic diseases from chest X-rays. The proposed methodology has the potential to support the reading workflow, improve efficiency, and reduce diagnostic errors.","sentences":["Medical imaging has been used for diagnosis of various conditions, making it one of the most powerful resources for effective patient care.","Due to widespread availability, low cost, and low radiation, chest X-ray is one of the most sought after radiology examination for the diagnosis of various thoracic diseases.","Due to advancements in medical imaging technologies and increasing patient load, current radiology workflow faces various challenges including increasing backlogs, working long hours, and increase in diagnostic errors.","An automated computer-aided diagnosis system that can interpret chest X-rays to augment radiologists by providing actionable insights has potential to provide second opinion to radiologists, highlight relevant regions in the image, in turn expediting clinical workflow, reducing diagnostic errors, and improving patient care.","In this study, we applied a novel architecture augmenting the DenseNet121 Convolutional Neural Network (CNN) with multi-head self-attention mechanism using transformer, namely SA-DenseNet121, that can identify multiple thoracic diseases in chest X-rays.","We conducted experiments on four of the largest chest X-ray datasets, namely, ChestX-ray14, CheXpert, MIMIC-CXR-JPG, and IU-CXR.","Experimental results in terms of area under the receiver operating characteristics (AUC-ROC) shows that augmenting CNN with self-attention has potential in diagnosing different thoracic diseases from chest X-rays.","The proposed methodology has the potential to support the reading workflow, improve efficiency, and reduce diagnostic errors."],"url":"http://arxiv.org/abs/2404.11843v1","category":"eess.IV"}
{"created":"2024-04-18 01:45:22","title":"On the Unprovability of Circuit Size Bounds in Intuitionistic $\\mathsf{S}^1_2$","abstract":"We show that there is a constant $k$ such that Buss's intuitionistic theory $\\mathsf{IS}^1_2$ does not prove that SAT requires co-nondeterministic circuits of size at least $n^k$. To our knowledge, this is the first unconditional unprovability result in bounded arithmetic in the context of worst-case fixed-polynomial size circuit lower bounds. We complement this result by showing that the upper bound $\\mathsf{NP} \\subseteq \\mathsf{coNSIZE}[n^k]$ is unprovable in $\\mathsf{IS}^1_2$.","sentences":["We show that there is a constant $k$ such that Buss's intuitionistic theory $\\mathsf{IS}^1_2$ does not prove that SAT requires co-nondeterministic circuits of size at least $n^k$. To our knowledge, this is the first unconditional unprovability result in bounded arithmetic in the context of worst-case fixed-polynomial size circuit lower bounds.","We complement this result by showing that the upper bound $\\mathsf{NP} \\subseteq \\mathsf{coNSIZE}[n^k]$ is unprovable in $\\mathsf{IS}^1_2$."],"url":"http://arxiv.org/abs/2404.11841v1","category":"cs.LO"}
{"created":"2024-04-18 01:27:31","title":"Actor-Critic Reinforcement Learning with Phased Actor","abstract":"Policy gradient methods in actor-critic reinforcement learning (RL) have become perhaps the most promising approaches to solving continuous optimal control problems. However, the trial-and-error nature of RL and the inherent randomness associated with solution approximations cause variations in the learned optimal values and policies. This has significantly hindered their successful deployment in real life applications where control responses need to meet dynamic performance criteria deterministically. Here we propose a novel phased actor in actor-critic (PAAC) method, aiming at improving policy gradient estimation and thus the quality of the control policy. Specifically, PAAC accounts for both $Q$ value and TD error in its actor update. We prove qualitative properties of PAAC for learning convergence of the value and policy, solution optimality, and stability of system dynamics. Additionally, we show variance reduction in policy gradient estimation. PAAC performance is systematically and quantitatively evaluated in this study using DeepMind Control Suite (DMC). Results show that PAAC leads to significant performance improvement measured by total cost, learning variance, robustness, learning speed and success rate. As PAAC can be piggybacked onto general policy gradient learning frameworks, we select well-known methods such as direct heuristic dynamic programming (dHDP), deep deterministic policy gradient (DDPG) and their variants to demonstrate the effectiveness of PAAC. Consequently we provide a unified view on these related policy gradient algorithms.","sentences":["Policy gradient methods in actor-critic reinforcement learning (RL) have become perhaps the most promising approaches to solving continuous optimal control problems.","However, the trial-and-error nature of RL and the inherent randomness associated with solution approximations cause variations in the learned optimal values and policies.","This has significantly hindered their successful deployment in real life applications where control responses need to meet dynamic performance criteria deterministically.","Here we propose a novel phased actor in actor-critic (PAAC) method, aiming at improving policy gradient estimation and thus the quality of the control policy.","Specifically, PAAC accounts for both $Q$ value and TD error in its actor update.","We prove qualitative properties of PAAC for learning convergence of the value and policy, solution optimality, and stability of system dynamics.","Additionally, we show variance reduction in policy gradient estimation.","PAAC performance is systematically and quantitatively evaluated in this study using DeepMind Control Suite (DMC).","Results show that PAAC leads to significant performance improvement measured by total cost, learning variance, robustness, learning speed and success rate.","As PAAC can be piggybacked onto general policy gradient learning frameworks, we select well-known methods such as direct heuristic dynamic programming (dHDP), deep deterministic policy gradient (DDPG) and their variants to demonstrate the effectiveness of PAAC.","Consequently we provide a unified view on these related policy gradient algorithms."],"url":"http://arxiv.org/abs/2404.11834v1","category":"cs.LG"}
{"created":"2024-04-18 01:27:02","title":"JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Agent Reinforcement Learning","abstract":"While Centralized Training with Decentralized Execution (CTDE) has become the prevailing paradigm in Multi-Agent Reinforcement Learning (MARL), it may not be suitable for scenarios in which agents can fully communicate and share observations with each other. Fully centralized methods, also know as Centralized Training with Centralized Execution (CTCE) methods, can fully utilize observations of all the agents by treating the entire system as a single agent. However, traditional CTCE methods suffer from scalability issues due to the exponential growth of the joint action space. To address these challenges, in this paper we propose JointPPO, a CTCE method that uses Proximal Policy Optimization (PPO) to directly optimize the joint policy of the multi-agent system. JointPPO decomposes the joint policy into conditional probabilities, transforming the decision-making process into a sequence generation task. A Transformer-based joint policy network is constructed, trained with a PPO loss tailored for the joint policy. JointPPO effectively handles a large joint action space and extends PPO to multi-agent setting with theoretical clarity and conciseness. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) testbed demonstrate the superiority of JointPPO over the strong baselines. Ablation experiments and analyses are conducted to explores the factors influencing JointPPO's performance.","sentences":["While Centralized Training with Decentralized Execution (CTDE) has become the prevailing paradigm in Multi-Agent Reinforcement Learning (MARL), it may not be suitable for scenarios in which agents can fully communicate and share observations with each other.","Fully centralized methods, also know as Centralized Training with Centralized Execution (CTCE) methods, can fully utilize observations of all the agents by treating the entire system as a single agent.","However, traditional CTCE methods suffer from scalability issues due to the exponential growth of the joint action space.","To address these challenges, in this paper we propose JointPPO, a CTCE method that uses Proximal Policy Optimization (PPO) to directly optimize the joint policy of the multi-agent system.","JointPPO decomposes the joint policy into conditional probabilities, transforming the decision-making process into a sequence generation task.","A Transformer-based joint policy network is constructed, trained with a PPO loss tailored for the joint policy.","JointPPO effectively handles a large joint action space and extends PPO to multi-agent setting with theoretical clarity and conciseness.","Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) testbed demonstrate the superiority of JointPPO over the strong baselines.","Ablation experiments and analyses are conducted to explores the factors influencing JointPPO's performance."],"url":"http://arxiv.org/abs/2404.11831v1","category":"cs.MA"}
{"created":"2024-04-18 01:24:51","title":"Alignment-induced depression and shear thinning of anisotropic granular media","abstract":"When granular materials of shape-anisotropic grains are sheared in a split-bottom shear cell, a localized shear band is formed with a depression at its center. This effect is closely related to the alignment of the particles with aspect ratio (AR), which, in turn, influences the local packing density, the stress distribution, and the system's overall bulk rheology. Particles with large AR tend to align with the shear direction, which increases the packing density in the shear band and affects rheological properties like stress, macroscopic friction coefficient, and effective viscosity. A scaling law correlates particle AR to macroscopic friction and effective viscosity, revealing shear-thinning behavior in bulk and near the surface.","sentences":["When granular materials of shape-anisotropic grains are sheared in a split-bottom shear cell, a localized shear band is formed with a depression at its center.","This effect is closely related to the alignment of the particles with aspect ratio (AR), which, in turn, influences the local packing density, the stress distribution, and the system's overall bulk rheology.","Particles with large AR tend to align with the shear direction, which increases the packing density in the shear band and affects rheological properties like stress, macroscopic friction coefficient, and effective viscosity.","A scaling law correlates particle AR to macroscopic friction and effective viscosity, revealing shear-thinning behavior in bulk and near the surface."],"url":"http://arxiv.org/abs/2404.11830v1","category":"cond-mat.soft"}
{"created":"2024-04-18 01:17:49","title":"Aerodynamic Design and Performance Evaluation of Pipe Diffuser for Centrifugal Compressor of Micro Gas Turbine","abstract":"This study introduces a new approach to optimize the geometrical parameters of pipe diffusers in centrifugal compressors for Micro Gas Turbines, tailored for a 100 kW unit. The methodology draws insights from optimized airfoil-type diffusers and addresses the unique topological challenges of pipe diffusers, using diffuser maps to enhance design precision. The effectiveness of this method is validated through 3D-RANS based steady CFD simulations, using the ANSYS CFX solver. Comparative performance assessments at 100 percent rotation speed show that the best-performing pipe diffuser slightly trails its airfoil counterpart in efficiency, achieving 82.2 percent total-to-total isentropic efficiency compared to 84.4 percent. However, it offers a reduced frontal area, enhancing compactness. The analysis also reveals a dualistic impact from the leading-edge geometry of the pipe diffuser, which generates two counter-rotating vortices. These vortices have beneficial effects in pseudo and semi-vaneless spaces while introducing destabilizing factors in channel spaces. This investigation highlights potential trade-offs and outlines conditions under which adverse effects dominate, leading to significant flow separation. These insights pave the way for refining diffuser designs to better balance performance with spatial efficiency, marking a critical step forward in compressor technology of micro gas turbine for decentralized power systems.","sentences":["This study introduces a new approach to optimize the geometrical parameters of pipe diffusers in centrifugal compressors for Micro Gas Turbines, tailored for a 100 kW unit.","The methodology draws insights from optimized airfoil-type diffusers and addresses the unique topological challenges of pipe diffusers, using diffuser maps to enhance design precision.","The effectiveness of this method is validated through 3D-RANS based steady CFD simulations, using the ANSYS CFX solver.","Comparative performance assessments at 100 percent rotation speed show that the best-performing pipe diffuser slightly trails its airfoil counterpart in efficiency, achieving 82.2 percent total-to-total isentropic efficiency compared to 84.4 percent.","However, it offers a reduced frontal area, enhancing compactness.","The analysis also reveals a dualistic impact from the leading-edge geometry of the pipe diffuser, which generates two counter-rotating vortices.","These vortices have beneficial effects in pseudo and semi-vaneless spaces while introducing destabilizing factors in channel spaces.","This investigation highlights potential trade-offs and outlines conditions under which adverse effects dominate, leading to significant flow separation.","These insights pave the way for refining diffuser designs to better balance performance with spatial efficiency, marking a critical step forward in compressor technology of micro gas turbine for decentralized power systems."],"url":"http://arxiv.org/abs/2404.11828v1","category":"math.NA"}
{"created":"2024-04-18 17:59:48","title":"Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models","abstract":"We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka. Reka models are able to process and reason with text, images, video, and audio inputs. This technical report discusses details of training some of these models and provides comprehensive evaluation results. We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class. Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models on both automatic evaluations and blind human evaluations. On image question answering benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V. Meanwhile, on multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup, outperforming other models such as Claude 3 Opus. On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g. MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped in production at http://chat.reka.ai . A showcase of non cherry picked qualitative examples can also be found at http://showcase.reka.ai .","sentences":["We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka.","Reka models are able to process and reason with text, images, video, and audio inputs.","This technical report discusses details of training some of these models and provides comprehensive evaluation results.","We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class.","Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models on both automatic evaluations and blind human evaluations.","On image question answering benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.","Meanwhile, on multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup, outperforming other models such as Claude 3 Opus.","On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g. MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation.","On video question answering (Perception-Test), Core outperforms Gemini Ultra.","Models are shipped in production at http://chat.reka.ai .","A showcase of non cherry picked qualitative examples can also be found at http://showcase.reka.ai ."],"url":"http://arxiv.org/abs/2404.12387v1","category":"cs.CL"}
{"created":"2024-04-18 17:57:09","title":"On the spin interface distribution for non-integrable variants of the two-dimensional Ising model","abstract":"We construct a martingale observable related to the spin interface for a class of non-integrable variants of the two-dimensional Ising model and express it in terms of Grassmann integrals. Under a conjecture about the scaling limit of this object, which is similar to some results recently obtained using constructive renormalization group methods, this would imply that the distribution of the interface has the same scaling limit as in the integrable model, that is, the Schramm Loewner evolution curve SLE(3).","sentences":["We construct a martingale observable related to the spin interface for a class of non-integrable variants of the two-dimensional Ising model and express it in terms of Grassmann integrals.","Under a conjecture about the scaling limit of this object, which is similar to some results recently obtained using constructive renormalization group methods, this would imply that the distribution of the interface has the same scaling limit as in the integrable model, that is, the Schramm Loewner evolution curve SLE(3)."],"url":"http://arxiv.org/abs/2404.12375v1","category":"math-ph"}
{"created":"2024-04-18 17:50:23","title":"Gradient-Regularized Out-of-Distribution Detection","abstract":"One of the challenges for neural networks in real-life applications is the overconfident errors these models make when the data is not from the original training distribution.   Addressing this issue is known as Out-of-Distribution (OOD) detection.   Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate for OOD data during training to achieve improved performance.   However, these methods fail to fully exploit the local information embedded in the auxiliary dataset.   In this work, we propose the idea of leveraging the information embedded in the gradient of the loss function during training to enable the network to not only learn a desired OOD score for each sample but also to exhibit similar behavior in a local neighborhood around each sample.   We also develop a novel energy-based sampling method to allow the network to be exposed to more informative OOD samples during the training phase. This is especially important when the auxiliary dataset is large. We demonstrate the effectiveness of our method through extensive experiments on several OOD benchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet experiment.   We further provide a theoretical analysis through the lens of certified robustness and Lipschitz analysis to showcase the theoretical foundation of our work. We will publicly release our code after the review process.","sentences":["One of the challenges for neural networks in real-life applications is the overconfident errors these models make when the data is not from the original training distribution.   ","Addressing this issue is known as Out-of-Distribution (OOD) detection.   ","Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate for OOD data during training to achieve improved performance.   ","However, these methods fail to fully exploit the local information embedded in the auxiliary dataset.   ","In this work, we propose the idea of leveraging the information embedded in the gradient of the loss function during training to enable the network to not only learn a desired OOD score for each sample but also to exhibit similar behavior in a local neighborhood around each sample.   ","We also develop a novel energy-based sampling method to allow the network to be exposed to more informative OOD samples during the training phase.","This is especially important when the auxiliary dataset is large.","We demonstrate the effectiveness of our method through extensive experiments on several OOD benchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet experiment.   ","We further provide a theoretical analysis through the lens of certified robustness and Lipschitz analysis to showcase the theoretical foundation of our work.","We will publicly release our code after the review process."],"url":"http://arxiv.org/abs/2404.12368v1","category":"cs.CV"}
{"created":"2024-04-18 17:34:47","title":"Improving the interpretability of GNN predictions through conformal-based graph sparsification","abstract":"Graph Neural Networks (GNNs) have achieved state-of-the-art performance in solving graph classification tasks. However, most GNN architectures aggregate information from all nodes and edges in a graph, regardless of their relevance to the task at hand, thus hindering the interpretability of their predictions. In contrast to prior work, in this paper we propose a GNN \\emph{training} approach that jointly i) finds the most predictive subgraph by removing edges and/or nodes -- -\\emph{without making assumptions about the subgraph structure} -- while ii) optimizing the performance of the graph classification task. To that end, we rely on reinforcement learning to solve the resulting bi-level optimization with a reward function based on conformal predictions to account for the current in-training uncertainty of the classifier. Our empirical results on nine different graph classification datasets show that our method competes in performance with baselines while relying on significantly sparser subgraphs, leading to more interpretable GNN-based predictions.","sentences":["Graph Neural Networks (GNNs) have achieved state-of-the-art performance in solving graph classification tasks.","However, most GNN architectures aggregate information from all nodes and edges in a graph, regardless of their relevance to the task at hand, thus hindering the interpretability of their predictions.","In contrast to prior work, in this paper we propose a GNN \\emph{training} approach that jointly i) finds the most predictive subgraph by removing edges and/or nodes -- -\\emph{without making assumptions about the subgraph structure} -- while ii) optimizing the performance of the graph classification task.","To that end, we rely on reinforcement learning to solve the resulting bi-level optimization with a reward function based on conformal predictions to account for the current in-training uncertainty of the classifier.","Our empirical results on nine different graph classification datasets show that our method competes in performance with baselines while relying on significantly sparser subgraphs, leading to more interpretable GNN-based predictions."],"url":"http://arxiv.org/abs/2404.12356v1","category":"stat.ML"}
{"created":"2024-04-18 17:09:10","title":"SPOT: Point Cloud Based Stereo Visual Place Recognition for Similar and Opposing Viewpoints","abstract":"Recognizing places from an opposing viewpoint during a return trip is a common experience for human drivers. However, the analogous robotics capability, visual place recognition (VPR) with limited field of view cameras under 180 degree rotations, has proven to be challenging to achieve. To address this problem, this paper presents Same Place Opposing Trajectory (SPOT), a technique for opposing viewpoint VPR that relies exclusively on structure estimated through stereo visual odometry (VO). The method extends recent advances in lidar descriptors and utilizes a novel double (similar and opposing) distance matrix sequence matching method. We evaluate SPOT on a publicly available dataset with 6.7-7.6 km routes driven in similar and opposing directions under various lighting conditions. The proposed algorithm demonstrates remarkable improvement over the state-of-the-art, achieving up to 91.7% recall at 100% precision in opposing viewpoint cases, while requiring less storage than all baselines tested and running faster than all but one. Moreover, the proposed method assumes no a priori knowledge of whether the viewpoint is similar or opposing, and also demonstrates competitive performance in similar viewpoint cases.","sentences":["Recognizing places from an opposing viewpoint during a return trip is a common experience for human drivers.","However, the analogous robotics capability, visual place recognition (VPR) with limited field of view cameras under 180 degree rotations, has proven to be challenging to achieve.","To address this problem, this paper presents Same Place Opposing Trajectory (SPOT), a technique for opposing viewpoint VPR that relies exclusively on structure estimated through stereo visual odometry (VO).","The method extends recent advances in lidar descriptors and utilizes a novel double (similar and opposing) distance matrix sequence matching method.","We evaluate SPOT on a publicly available dataset with 6.7-7.6 km routes driven in similar and opposing directions under various lighting conditions.","The proposed algorithm demonstrates remarkable improvement over the state-of-the-art, achieving up to 91.7% recall at 100% precision in opposing viewpoint cases, while requiring less storage than all baselines tested and running faster than all but one.","Moreover, the proposed method assumes no a priori knowledge of whether the viewpoint is similar or opposing, and also demonstrates competitive performance in similar viewpoint cases."],"url":"http://arxiv.org/abs/2404.12339v1","category":"cs.RO"}
{"created":"2024-04-18 16:12:38","title":"Reducing Bias in Pre-trained Models by Tuning while Penalizing Change","abstract":"Deep models trained on large amounts of data often incorporate implicit biases present during training time. If later such a bias is discovered during inference or deployment, it is often necessary to acquire new data and retrain the model. This behavior is especially problematic in critical areas such as autonomous driving or medical decision-making. In these scenarios, new data is often expensive and hard to come by. In this work, we present a method based on change penalization that takes a pre-trained model and adapts the weights to mitigate a previously detected bias. We achieve this by tuning a zero-initialized copy of a frozen pre-trained network. Our method needs very few, in extreme cases only a single, examples that contradict the bias to increase performance. Additionally, we propose an early stopping criterion to modify baselines and reduce overfitting. We evaluate our approach on a well-known bias in skin lesion classification and three other datasets from the domain shift literature. We find that our approach works especially well with very few images. Simple fine-tuning combined with our early stopping also leads to performance benefits for a larger number of tuning samples.","sentences":["Deep models trained on large amounts of data often incorporate implicit biases present during training time.","If later such a bias is discovered during inference or deployment, it is often necessary to acquire new data and retrain the model.","This behavior is especially problematic in critical areas such as autonomous driving or medical decision-making.","In these scenarios, new data is often expensive and hard to come by.","In this work, we present a method based on change penalization that takes a pre-trained model and adapts the weights to mitigate a previously detected bias.","We achieve this by tuning a zero-initialized copy of a frozen pre-trained network.","Our method needs very few, in extreme cases only a single, examples that contradict the bias to increase performance.","Additionally, we propose an early stopping criterion to modify baselines and reduce overfitting.","We evaluate our approach on a well-known bias in skin lesion classification and three other datasets from the domain shift literature.","We find that our approach works especially well with very few images.","Simple fine-tuning combined with our early stopping also leads to performance benefits for a larger number of tuning samples."],"url":"http://arxiv.org/abs/2404.12292v1","category":"cs.CV"}
{"created":"2024-04-18 15:52:43","title":"Push-forward of geometric distributions under Collatz iteration: Part 1","abstract":"Two conjectures are presented. The first, Conjecture 1, is that the pushforward of a geometric distribution on the integers under $n$ Collatz iterates, modulo $2^p$, is usefully close to uniform distribution on the integers modulo $2^p$, if $p/n$ is small enough. Conjecture 2 is that the density is bounded from zero for the incidence of both $0$ and $1$ for the coefficients in the dyadic expansions of $-3^{-\\ell }$ on all but an exponentially small set of paths of a geometrically distributed random walk on the two-dimensional array of these coefficients. It is shown that Conjecture 2 implies Conjecture 1. At present, Conjecture 2 is unresolved.","sentences":["Two conjectures are presented.","The first, Conjecture 1, is that the pushforward of a geometric distribution on the integers under $n$ Collatz iterates, modulo $2^p$, is usefully close to uniform distribution on the integers modulo $2^p$, if $p/n$ is small enough.","Conjecture 2 is that the density is bounded from zero for the incidence of both $0$ and $1$ for the coefficients in the dyadic expansions of $-3^{-\\ell }$ on all but an exponentially small set of paths of a geometrically distributed random walk on the two-dimensional array of these coefficients.","It is shown that Conjecture 2 implies Conjecture 1.","At present, Conjecture 2 is unresolved."],"url":"http://arxiv.org/abs/2404.12279v1","category":"math.PR"}
{"created":"2024-04-18 14:20:19","title":"Quantifying Aleatoric and Epistemic Uncertainty with Proper Scoring Rules","abstract":"Uncertainty representation and quantification are paramount in machine learning and constitute an important prerequisite for safety-critical applications. In this paper, we propose novel measures for the quantification of aleatoric and epistemic uncertainty based on proper scoring rules, which are loss functions with the meaningful property that they incentivize the learner to predict ground-truth (conditional) probabilities. We assume two common representations of (epistemic) uncertainty, namely, in terms of a credal set, i.e. a set of probability distributions, or a second-order distribution, i.e., a distribution over probability distributions. Our framework establishes a natural bridge between these representations. We provide a formal justification of our approach and introduce new measures of epistemic and aleatoric uncertainty as concrete instantiations.","sentences":["Uncertainty representation and quantification are paramount in machine learning and constitute an important prerequisite for safety-critical applications.","In this paper, we propose novel measures for the quantification of aleatoric and epistemic uncertainty based on proper scoring rules, which are loss functions with the meaningful property that they incentivize the learner to predict ground-truth (conditional) probabilities.","We assume two common representations of (epistemic) uncertainty, namely, in terms of a credal set, i.e. a set of probability distributions, or a second-order distribution, i.e., a distribution over probability distributions.","Our framework establishes a natural bridge between these representations.","We provide a formal justification of our approach and introduce new measures of epistemic and aleatoric uncertainty as concrete instantiations."],"url":"http://arxiv.org/abs/2404.12215v1","category":"cs.LG"}
{"created":"2024-04-18 14:14:44","title":"Observation, Analysis, and Solution: Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-Training","abstract":"Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) in computer vision has enabled promising downstream performance on top of the learned self-supervised ViT features. In this paper, we question if the extremely simple ViTs' fine-tuning performance with a small-scale architecture can also benefit from this pre-training paradigm, which is considerably less studied yet in contrast to the well-established lightweight architecture design methodology with sophisticated components introduced. By carefully adapting various typical MIM pre-training methods to this lightweight regime and comparing them with the contrastive learning (CL) pre-training on various downstream image classification and dense prediction tasks, we systematically observe different behaviors between MIM and CL with respect to the downstream fine-tuning data scales. Furthermore, we analyze the frozen features under linear probing evaluation and also the layer representation similarities and attention maps across the obtained models, which clearly show the inferior learning of MIM pre-training on higher layers, leading to unsatisfactory fine-tuning performance on data-insufficient downstream tasks. This finding is naturally a guide to choosing appropriate distillation strategies during pre-training to solve the above deterioration problem. Extensive experiments on various vision tasks demonstrate the effectiveness of our observation-analysis-solution flow. In particular, our pre-training with distillation on pure lightweight ViTs with vanilla/hierarchical design (5.7M/6.5M) can achieve 79.4%/78.9% top-1 accuracy on ImageNet-1K. It also enables SOTA performance on the ADE20K semantic segmentation task (42.8% mIoU) and LaSOT visual tracking task (66.1% AUC) in the lightweight regime. The latter even surpasses all the current SOTA lightweight CPU-realtime trackers.","sentences":["Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) in computer vision has enabled promising downstream performance on top of the learned self-supervised ViT features.","In this paper, we question if the extremely simple ViTs' fine-tuning performance with a small-scale architecture can also benefit from this pre-training paradigm, which is considerably less studied yet in contrast to the well-established lightweight architecture design methodology with sophisticated components introduced.","By carefully adapting various typical MIM pre-training methods to this lightweight regime and comparing them with the contrastive learning (CL) pre-training on various downstream image classification and dense prediction tasks, we systematically observe different behaviors between MIM and CL with respect to the downstream fine-tuning data scales.","Furthermore, we analyze the frozen features under linear probing evaluation and also the layer representation similarities and attention maps across the obtained models, which clearly show the inferior learning of MIM pre-training on higher layers, leading to unsatisfactory fine-tuning performance on data-insufficient downstream tasks.","This finding is naturally a guide to choosing appropriate distillation strategies during pre-training to solve the above deterioration problem.","Extensive experiments on various vision tasks demonstrate the effectiveness of our observation-analysis-solution flow.","In particular, our pre-training with distillation on pure lightweight ViTs with vanilla/hierarchical design (5.7M/6.5M) can achieve 79.4%/78.9% top-1 accuracy on ImageNet-1K. It also enables SOTA performance on the ADE20K semantic segmentation task (42.8% mIoU) and LaSOT visual tracking task (66.1% AUC) in the lightweight regime.","The latter even surpasses all the current SOTA lightweight CPU-realtime trackers."],"url":"http://arxiv.org/abs/2404.12210v1","category":"cs.CV"}
{"created":"2024-04-18 14:04:11","title":"Metamaterial-induced-transparency engineering through quasi-bound states in the continuum by using dielectric cross-shaped trimers","abstract":"This study presents a novel approach to activate a narrowband transparency line within a reflecting broadband window in all-dielectric metasurfaces, in analogy to the electromagnetically-induced transparency effect, by means of a quasi-bound state in the continuum (qBIC). We demonstrate that the resonance overlapping of a bright mode and a qBIC-based nearly-dark mode with distinct Q-factor can be fully governed by a silicon trimer-based unit cell with broken-inversion-symmetry cross shape, thus providing the required response under normal incidence of a linearly-polarized light. Our analysis that is derived from the far-field multipolar decomposition and near-field electromagnetic distributions uncovers the main contributions of different multipoles on the qBIC resonance, with governing magnetic dipole and electric quadrupole terms supplied by distinct parts of the dielectric ``molecule.'' The findings extracted from this research open up new avenues for the development of polarization-dependent technologies, with particular interest in its capabilities for sensing and biosensing.","sentences":["This study presents a novel approach to activate a narrowband transparency line within a reflecting broadband window in all-dielectric metasurfaces, in analogy to the electromagnetically-induced transparency effect, by means of a quasi-bound state in the continuum (qBIC).","We demonstrate that the resonance overlapping of a bright mode and a qBIC-based nearly-dark mode with distinct Q-factor can be fully governed by a silicon trimer-based unit cell with broken-inversion-symmetry cross shape, thus providing the required response under normal incidence of a linearly-polarized light.","Our analysis that is derived from the far-field multipolar decomposition and near-field electromagnetic distributions uncovers the main contributions of different multipoles on the qBIC resonance, with governing magnetic dipole and electric quadrupole terms supplied by distinct parts of the dielectric ``molecule.''","The findings extracted from this research open up new avenues for the development of polarization-dependent technologies, with particular interest in its capabilities for sensing and biosensing."],"url":"http://arxiv.org/abs/2404.12200v1","category":"physics.optics"}
{"created":"2024-04-18 13:41:00","title":"Estimation of the invariant measure of a multidimensional diffusion from noisy observations","abstract":"We introduce a new approach for estimating the invariant density of a multidimensional diffusion when dealing with high-frequency observations blurred by independent noises. We consider the intermediate regime, where observations occur at discrete time instances $k\\Delta_n$ for $k=0,\\dots,n$, under the conditions $\\Delta_n\\to 0$ and $n\\Delta_n\\to\\infty$. Our methodology involves the construction of a kernel density estimator that uses a pre-averaging technique to proficiently remove noise from the data while preserving the analytical characteristics of the underlying signal and its asymptotic properties. The rate of convergence of our estimator depends on both the anisotropic regularity of the density and the intensity of the noise. We establish conditions on the intensity of the noise that ensure the recovery of convergence rates similar to those achievable without any noise. Furthermore, we prove a Bernstein concentration inequality for our estimator, from which we derive an adaptive procedure for the kernel bandwidth selection.","sentences":["We introduce a new approach for estimating the invariant density of a multidimensional diffusion when dealing with high-frequency observations blurred by independent noises.","We consider the intermediate regime, where observations occur at discrete time instances $k\\Delta_n$ for $k=0,\\dots,n$, under the conditions $\\Delta_n\\to 0$ and $n\\Delta_n\\to\\infty$. Our methodology involves the construction of a kernel density estimator that uses a pre-averaging technique to proficiently remove noise from the data while preserving the analytical characteristics of the underlying signal and its asymptotic properties.","The rate of convergence of our estimator depends on both the anisotropic regularity of the density and the intensity of the noise.","We establish conditions on the intensity of the noise that ensure the recovery of convergence rates similar to those achievable without any noise.","Furthermore, we prove a Bernstein concentration inequality for our estimator, from which we derive an adaptive procedure for the kernel bandwidth selection."],"url":"http://arxiv.org/abs/2404.12181v1","category":"math.ST"}
{"created":"2024-04-18 13:31:57","title":"EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque","abstract":"The widespread availability of Question Answering (QA) datasets in English has greatly facilitated the advancement of the Natural Language Processing (NLP) field. However, the scarcity of such resources for minority languages, such as Basque, poses a substantial challenge for these communities. In this context, the translation and alignment of existing QA datasets plays a crucial role in narrowing this technological gap. This work presents EuSQuAD, the first initiative dedicated to automatically translating and aligning SQuAD2.0 into Basque, resulting in more than 142k QA examples. We demonstrate EuSQuAD's value through extensive qualitative analysis and QA experiments supported with EuSQuAD as training data. These experiments are evaluated with a new human-annotated dataset.","sentences":["The widespread availability of Question Answering (QA) datasets in English has greatly facilitated the advancement of the Natural Language Processing (NLP) field.","However, the scarcity of such resources for minority languages, such as Basque, poses a substantial challenge for these communities.","In this context, the translation and alignment of existing QA datasets plays a crucial role in narrowing this technological gap.","This work presents EuSQuAD, the first initiative dedicated to automatically translating and aligning SQuAD2.0 into Basque, resulting in more than 142k QA examples.","We demonstrate EuSQuAD's value through extensive qualitative analysis and QA experiments supported with EuSQuAD as training data.","These experiments are evaluated with a new human-annotated dataset."],"url":"http://arxiv.org/abs/2404.12177v1","category":"cs.CL"}
{"created":"2024-04-18 12:53:57","title":"Unknown Interference Modeling for Rate Adaptation in Cell-Free Massive MIMO Networks","abstract":"Co-channel interference poses a challenge in any wireless communication network where the time-frequency resources are reused over different geographical areas. The interference is particularly diverse in cell-free massive multiple-input multiple-output (MIMO) networks, where a large number of user equipments (UEs) are multiplexed by a multitude of access points (APs) on the same time-frequency resources. For realistic and scalable network operation, only the interference from UEs belonging to the same serving cluster of APs can be estimated in real-time and suppressed by precoding/combining. As a result, the unknown interference arising from scheduling variations in neighboring clusters makes the rate adaptation hard and can lead to outages. This paper aims to model the unknown interference power in the uplink of a cell-free massive MIMO network. The results show that the proposed method effectively describes the distribution of the unknown interference power and provides a tool for rate adaptation with guaranteed target outage.","sentences":["Co-channel interference poses a challenge in any wireless communication network where the time-frequency resources are reused over different geographical areas.","The interference is particularly diverse in cell-free massive multiple-input multiple-output (MIMO) networks, where a large number of user equipments (UEs) are multiplexed by a multitude of access points (APs) on the same time-frequency resources.","For realistic and scalable network operation, only the interference from UEs belonging to the same serving cluster of APs can be estimated in real-time and suppressed by precoding/combining.","As a result, the unknown interference arising from scheduling variations in neighboring clusters makes the rate adaptation hard and can lead to outages.","This paper aims to model the unknown interference power in the uplink of a cell-free massive MIMO network.","The results show that the proposed method effectively describes the distribution of the unknown interference power and provides a tool for rate adaptation with guaranteed target outage."],"url":"http://arxiv.org/abs/2404.12148v1","category":"eess.SP"}
{"created":"2024-04-18 12:51:32","title":"Faster Optimization Through Genetic Drift","abstract":"The compact Genetic Algorithm (cGA), parameterized by its hypothetical population size $K$, offers a low-memory alternative to evolving a large offspring population of solutions. It evolves a probability distribution, biasing it towards promising samples. For the classical benchmark OneMax, the cGA has to two different modes of operation: a conservative one with small step sizes $\\Theta(1/(\\sqrt{n}\\log n))$, which is slow but prevents genetic drift, and an aggressive one with large step sizes $\\Theta(1/\\log n)$, in which genetic drift leads to wrong decisions, but those are corrected efficiently. On OneMax, an easy hill-climbing problem, both modes lead to optimization times of $\\Theta(n\\log n)$ and are thus equally efficient.   In this paper we study how both regimes change when we replace OneMax by the harder hill-climbing problem DynamicBinVal. It turns out that the aggressive mode is not affected and still yields quasi-linear runtime $O(n\\cdot polylog (n))$. However, the conservative mode becomes substantially slower, yielding a runtime of $\\Omega(n^2)$, since genetic drift can only be avoided with smaller step sizes of $O(1/n)$. We complement our theoretical results with simulations.","sentences":["The compact Genetic Algorithm (cGA), parameterized by its hypothetical population size $K$, offers a low-memory alternative to evolving a large offspring population of solutions.","It evolves a probability distribution, biasing it towards promising samples.","For the classical benchmark OneMax, the cGA has to two different modes of operation: a conservative one with small step sizes $\\Theta(1/(\\sqrt{n}\\log n))$, which is slow but prevents genetic drift, and an aggressive one with large step sizes $\\Theta(1/\\log n)$, in which genetic drift leads to wrong decisions, but those are corrected efficiently.","On OneMax, an easy hill-climbing problem, both modes lead to optimization times of $\\Theta(n\\log n)$ and are thus equally efficient.   ","In this paper we study how both regimes change when we replace OneMax by the harder hill-climbing problem DynamicBinVal.","It turns out that the aggressive mode is not affected and still yields quasi-linear runtime $O(n\\cdot polylog (n))$. However, the conservative mode becomes substantially slower, yielding a runtime of $\\Omega(n^2)$, since genetic drift can only be avoided with smaller step sizes of $O(1/n)$. We complement our theoretical results with simulations."],"url":"http://arxiv.org/abs/2404.12147v1","category":"cs.NE"}
{"created":"2024-04-18 11:27:25","title":"Convex Sequence and Convex Polygon","abstract":"In this paper, we deal with the question; under what conditions the points $P_i(xi,yi)$ $(i = 1,\\cdots, n)$ form a convex polygon provided $x_1 < \\cdots < x_n$ holds. One of the main findings of the paper can be stated as follows: \"Let $P_1(x_1,y_1),\\cdots ,P_n(x_n,y_n)$ are $n$ distinct points ($n\\geq3$) with $x_1<\\cdots<x_n$. Then $\\overline{P_1P_2},\\cdots \\overline{P_nP_1}$ form a convex $n$-gon that lies in the half-space \\begin{equation*}{ \\underline{\\mathbb{H}}=\\bigg\\{(x,y)\\big|\\quad x\\in\\mathbb{R} \\quad \\mbox{and} \\quad y\\leq y_1+\\bigg(\\dfrac{x-x_1}{x_n-x_1}\\bigg)(y_n-y_1)\\bigg\\}\\subseteq{\\mathbb{R}^{2}} } \\end{equation*} if and only if the following inequality holds \\begin{equation} \\dfrac{y_i-y_{i-1}}{x_i-x_{i-1}} \\leq \\dfrac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\quad \\quad \\mbox{for all} \\quad \\quad i\\in\\{2,\\cdots,n-1\\} .\" \\end{equation} Based on this result, we establish a linkage between the property of sequential convexity and convex polygon. We show that in a plane if any $n$ points are scattered in such a way that their horizontal and vertical distances preserve some specific monotonic properties; then those points form a $2$-dimensional convex polytope.","sentences":["In this paper, we deal with the question; under what conditions the points $P_i(xi,yi)$ $(i = 1,\\cdots, n)$ form a convex polygon provided $x_1 <","\\cdots < x_n$ holds.","One of the main findings of the paper can be stated as follows: \"Let $P_1(x_1,y_1),\\cdots ,P_n(x_n,y_n)$ are $n$ distinct points ($n\\geq3$) with $x_1<\\cdots<x_n$.","Then $\\overline{P_1P_2},\\cdots \\overline{P_nP_1}$ form a convex $n$-gon that lies in the half-space \\begin{equation*}{ \\underline{\\mathbb{H}}=\\bigg\\{(x,y)\\big|\\quad x\\in\\mathbb{R} \\quad \\mbox{and} \\quad y\\leq y_1+\\bigg(\\dfrac{x-x_1}{x_n-x_1}\\bigg)(y_n-y_1)\\bigg\\}\\subseteq{\\mathbb{R}^{2}} } \\end{equation*} if and only if the following inequality holds \\begin{equation} \\dfrac{y_i-y_{i-1}}{x_i-x_{i-1}} \\leq \\dfrac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\quad \\quad \\mbox{for all} \\quad \\quad i\\in\\{2,\\cdots,n-1\\} .\"","\\end{equation} Based on this result, we establish a linkage between the property of sequential convexity and convex polygon.","We show that in a plane if any $n$ points are scattered in such a way that their horizontal and vertical distances preserve some specific monotonic properties; then those points form a $2$-dimensional convex polytope."],"url":"http://arxiv.org/abs/2404.12095v1","category":"math.MG"}
{"created":"2024-04-18 11:25:27","title":"Strange quark matter as dark matter: 40 years later, a reappraisal","abstract":"Forty years ago, Witten suggested that dark matter could be composed of macroscopic clusters of strange quark matter. This idea was very popular for several years, but it dropped out of fashion once lattice QCD calculations indicated that the confinement/deconfinement transition, at small baryonic chemical potential, is not first order, which seemed to be a crucial requirement in order to produce large clusters of quarks. Here we revisit both the conditions under which strangelets can be produced in the Early Universe and the many phenomenological implications of their existence. Most of the paper discusses the limits on their mass distribution and a possible and simple scheme for their production. Finally, we discuss the most promising techniques to detect this type of objects.","sentences":["Forty years ago, Witten suggested that dark matter could be composed of macroscopic clusters of strange quark matter.","This idea was very popular for several years, but it dropped out of fashion once lattice QCD calculations indicated that the confinement/deconfinement transition, at small baryonic chemical potential, is not first order, which seemed to be a crucial requirement in order to produce large clusters of quarks.","Here we revisit both the conditions under which strangelets can be produced in the Early Universe and the many phenomenological implications of their existence.","Most of the paper discusses the limits on their mass distribution and a possible and simple scheme for their production.","Finally, we discuss the most promising techniques to detect this type of objects."],"url":"http://arxiv.org/abs/2404.12094v1","category":"hep-ph"}
{"created":"2024-04-18 11:16:48","title":"Well-posedness for fractional Hardy-H\u00e9non parabolic equations with fractional Brownian noise","abstract":"We investigate the fractional Hardy-H\\'enon equation with fractional Brownian noise $$ \\partial_t u(t)+(-\\Delta)^{\\theta/2} u(t)=|x|^{-\\gamma} |u(t)|^{p-1}u(t)+\\mu \\partial_t B^H(t), $$ where $\\theta>0$, $p>1$, $\\gamma\\geq 0$, $\\mu \\in\\mathbb{R}$, and the random forcing $B^H$ is the fractional Brownian motion defined on some complete probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ with Hurst parameter $H\\in (0,1)$. We obtain the local existence and uniqueness of mild solutions under tailored conditions on the parameters of the equation.","sentences":["We investigate the fractional Hardy-H\\'enon equation with fractional Brownian noise $$ \\partial_t u(t)+(-\\Delta)^{\\theta/2} u(t)=|x|^{-\\gamma} |u(t)|^{p-1}u(t)+\\mu \\partial_t B^H(t), $$ where $\\theta>0$, $p>1$, $\\gamma\\geq 0$, $\\mu \\in\\mathbb{R}$, and the random forcing $B^H$ is the fractional Brownian motion defined on some complete probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ with Hurst parameter $H\\in (0,1)$. We obtain the local existence and uniqueness of mild solutions under tailored conditions on the parameters of the equation."],"url":"http://arxiv.org/abs/2404.12088v1","category":"math.AP"}
{"created":"2024-04-18 10:41:33","title":"Developing Application Profiles for Enhancing Data and Workflows in Cultural Heritage Digitisation Processes","abstract":"As a result of the proliferation of 3D digitisation in the context of cultural heritage projects, digital assets and digitisation processes - being considered as proper research objects - must prioritise adherence to FAIR principles. Existing standards and ontologies, such as CIDOC CRM, play a crucial role in this regard, but they are often over-engineered for the need of a particular application context, thus making their understanding and adoption difficult. Application profiles of a given standard - defined as sets of ontological entities drawn from one or more semantic artefacts for a particular context or application - are usually proposed as tools for promoting interoperability and reuse while being tied entirely to the particular application context they refer to. In this paper, we present an adaptation and application of an ontology development methodology, i.e. SAMOD, to guide the creation of robust, semantically sound application profiles of large standard models. Using an existing pilot study we have developed in a project dedicated to leveraging virtual technologies to preserve and valorise cultural heritage, we introduce an application profile named CHAD-AP, that we have developed following our customised version of SAMOD. We reflect on the use of SAMOD and similar ontology development methodologies for this purpose, highlighting its strengths and current limitations, future developments, and possible adoption in other similar projects.","sentences":["As a result of the proliferation of 3D digitisation in the context of cultural heritage projects, digital assets and digitisation processes - being considered as proper research objects - must prioritise adherence to FAIR principles.","Existing standards and ontologies, such as CIDOC CRM, play a crucial role in this regard, but they are often over-engineered for the need of a particular application context, thus making their understanding and adoption difficult.","Application profiles of a given standard - defined as sets of ontological entities drawn from one or more semantic artefacts for a particular context or application - are usually proposed as tools for promoting interoperability and reuse while being tied entirely to the particular application context they refer to.","In this paper, we present an adaptation and application of an ontology development methodology, i.e. SAMOD, to guide the creation of robust, semantically sound application profiles of large standard models.","Using an existing pilot study we have developed in a project dedicated to leveraging virtual technologies to preserve and valorise cultural heritage, we introduce an application profile named CHAD-AP, that we have developed following our customised version of SAMOD.","We reflect on the use of SAMOD and similar ontology development methodologies for this purpose, highlighting its strengths and current limitations, future developments, and possible adoption in other similar projects."],"url":"http://arxiv.org/abs/2404.12069v1","category":"cs.DL"}
{"created":"2024-04-18 10:23:10","title":"PureForest: A Large-scale Aerial Lidar and Aerial Imagery Dataset for Tree Species Classification in Monospecific Forests","abstract":"Knowledge of tree species distribution is fundamental to managing forests. New deep learning approaches promise significant accuracy gains for forest mapping, and are becoming a critical tool for mapping multiple tree species at scale. To advance the field, deep learning researchers need large benchmark datasets with high-quality annotations. To this end, we present the PureForest dataset: a large-scale, open, multimodal dataset designed for tree species classification from both Aerial Lidar Scanning (ALS) point clouds and Very High Resolution (VHR) aerial images. Most current public Lidar datasets for tree species classification have low diversity as they only span a small area of a few dozen annotated hectares at most. In contrast, PureForest has 18 tree species grouped into 13 semantic classes, and spans 339 km$^2$ across 449 distinct monospecific forests, and is to date the largest and most comprehensive Lidar dataset for the identification of tree species. By making PureForest publicly available, we hope to provide a challenging benchmark dataset to support the development of deep learning approaches for tree species identification from Lidar and/or aerial imagery. In this data paper, we describe the annotation workflow, the dataset, the recommended evaluation methodology, and establish a baseline performance from both 3D and 2D modalities.","sentences":["Knowledge of tree species distribution is fundamental to managing forests.","New deep learning approaches promise significant accuracy gains for forest mapping, and are becoming a critical tool for mapping multiple tree species at scale.","To advance the field, deep learning researchers need large benchmark datasets with high-quality annotations.","To this end, we present the PureForest dataset: a large-scale, open, multimodal dataset designed for tree species classification from both Aerial Lidar Scanning (ALS) point clouds and Very High Resolution (VHR) aerial images.","Most current public Lidar datasets for tree species classification have low diversity as they only span a small area of a few dozen annotated hectares at most.","In contrast, PureForest has 18 tree species grouped into 13 semantic classes, and spans 339 km$^2$ across 449 distinct monospecific forests, and is to date the largest and most comprehensive Lidar dataset for the identification of tree species.","By making PureForest publicly available, we hope to provide a challenging benchmark dataset to support the development of deep learning approaches for tree species identification from Lidar and/or aerial imagery.","In this data paper, we describe the annotation workflow, the dataset, the recommended evaluation methodology, and establish a baseline performance from both 3D and 2D modalities."],"url":"http://arxiv.org/abs/2404.12064v1","category":"cs.CV"}
{"created":"2024-04-18 10:08:15","title":"Regularity results for H\u00f6lder minimizers to functionals with non-standard growth","abstract":"We study the regularity properties of H\\\"older continuous minimizers to non-autonomous functionals satisfying $(p,q)$-growth conditions, under Besov assumptions on the coefficients. In particular, we are able to prove higher integrability and higher differentiability results for solutions to our minimum problem.","sentences":["We study the regularity properties of H\\\"older continuous minimizers to non-autonomous functionals satisfying $(p,q)$-growth conditions, under Besov assumptions on the coefficients.","In particular, we are able to prove higher integrability and higher differentiability results for solutions to our minimum problem."],"url":"http://arxiv.org/abs/2404.12053v1","category":"math.AP"}
{"created":"2024-04-18 08:39:52","title":"Knowledge-Aware Multi-Intent Contrastive Learning for Multi-Behavior Recommendation","abstract":"Multi-behavioral recommendation optimizes user experiences by providing users with more accurate choices based on their diverse behaviors, such as view, add to cart, and purchase. Current studies on multi-behavioral recommendation mainly explore the connections and differences between multi-behaviors from an implicit perspective. Specifically, they directly model those relations using black-box neural networks. In fact, users' interactions with items under different behaviors are driven by distinct intents. For instance, when users view products, they tend to pay greater attention to information such as ratings and brands. However, when it comes to the purchasing phase, users become more price-conscious. To tackle this challenge and data sparsity problem in the multi-behavioral recommendation, we propose a novel model: Knowledge-Aware Multi-Intent Contrastive Learning (KAMCL) model. This model uses relationships in the knowledge graph to construct intents, aiming to mine the connections between users' multi-behaviors from the perspective of intents to achieve more accurate recommendations. KAMCL is equipped with two contrastive learning schemes to alleviate the data scarcity problem and further enhance user representations. Extensive experiments on three real datasets demonstrate the superiority of our model.","sentences":["Multi-behavioral recommendation optimizes user experiences by providing users with more accurate choices based on their diverse behaviors, such as view, add to cart, and purchase.","Current studies on multi-behavioral recommendation mainly explore the connections and differences between multi-behaviors from an implicit perspective.","Specifically, they directly model those relations using black-box neural networks.","In fact, users' interactions with items under different behaviors are driven by distinct intents.","For instance, when users view products, they tend to pay greater attention to information such as ratings and brands.","However, when it comes to the purchasing phase, users become more price-conscious.","To tackle this challenge and data sparsity problem in the multi-behavioral recommendation, we propose a novel model: Knowledge-Aware Multi-Intent Contrastive Learning (KAMCL) model.","This model uses relationships in the knowledge graph to construct intents, aiming to mine the connections between users' multi-behaviors from the perspective of intents to achieve more accurate recommendations.","KAMCL is equipped with two contrastive learning schemes to alleviate the data scarcity problem and further enhance user representations.","Extensive experiments on three real datasets demonstrate the superiority of our model."],"url":"http://arxiv.org/abs/2404.11993v1","category":"cs.IR"}
{"created":"2024-04-18 08:23:24","title":"Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation","abstract":"Weakly Incremental Learning for Semantic Segmentation (WILSS) leverages a pre-trained segmentation model to segment new classes using cost-effective and readily available image-level labels. A prevailing way to solve WILSS is the generation of seed areas for each new class, serving as a form of pixel-level supervision. However, a scenario usually arises where a pixel is concurrently predicted as an old class by the pre-trained segmentation model and a new class by the seed areas. Such a scenario becomes particularly problematic in WILSS, as the lack of pixel-level annotations on new classes makes it intractable to ascertain whether the pixel pertains to the new class or not. To surmount this issue, we propose an innovative, tendency-driven relationship of mutual exclusivity, meticulously tailored to govern the behavior of the seed areas and the predictions generated by the pre-trained segmentation model. This relationship stipulates that predictions for the new and old classes must not conflict whilst prioritizing the preservation of predictions for the old classes, which not only addresses the conflicting prediction issue but also effectively mitigates the inherent challenge of incremental learning - catastrophic forgetting. Furthermore, under the auspices of this tendency-driven mutual exclusivity relationship, we generate pseudo masks for the new classes, allowing for concurrent execution with model parameter updating via the resolution of a bi-level optimization problem. Extensive experiments substantiate the effectiveness of our framework, resulting in the establishment of new benchmarks and paving the way for further research in this field.","sentences":["Weakly Incremental Learning for Semantic Segmentation (WILSS) leverages a pre-trained segmentation model to segment new classes using cost-effective and readily available image-level labels.","A prevailing way to solve WILSS is the generation of seed areas for each new class, serving as a form of pixel-level supervision.","However, a scenario usually arises where a pixel is concurrently predicted as an old class by the pre-trained segmentation model and a new class by the seed areas.","Such a scenario becomes particularly problematic in WILSS, as the lack of pixel-level annotations on new classes makes it intractable to ascertain whether the pixel pertains to the new class or not.","To surmount this issue, we propose an innovative, tendency-driven relationship of mutual exclusivity, meticulously tailored to govern the behavior of the seed areas and the predictions generated by the pre-trained segmentation model.","This relationship stipulates that predictions for the new and old classes must not conflict whilst prioritizing the preservation of predictions for the old classes, which not only addresses the conflicting prediction issue but also effectively mitigates the inherent challenge of incremental learning - catastrophic forgetting.","Furthermore, under the auspices of this tendency-driven mutual exclusivity relationship, we generate pseudo masks for the new classes, allowing for concurrent execution with model parameter updating via the resolution of a bi-level optimization problem.","Extensive experiments substantiate the effectiveness of our framework, resulting in the establishment of new benchmarks and paving the way for further research in this field."],"url":"http://arxiv.org/abs/2404.11981v1","category":"cs.CV"}
{"created":"2024-04-18 08:14:53","title":"EVIT: Event-Oriented Instruction Tuning for Event Reasoning","abstract":"Events refer to specific occurrences, incidents, or happenings that take place under a particular background. Event reasoning aims to infer events according to certain relations and predict future events. The cutting-edge techniques for event reasoning play a crucial role in various natural language processing applications. Large language models (LLMs) have made significant advancements in event reasoning owing to their wealth of knowledge and reasoning capabilities. However, smaller instruction-tuned models currently in use do not consistently demonstrate exceptional proficiency in managing these tasks. This discrepancy arises from the absence of explicit modeling of events and the interconnections of them within their instruction data. Consequently, these models face challenges in comprehending event structures and semantics while struggling to bridge the gap between their interpretations and human understanding of events. Additionally, their limitations in grasping event relations lead to constrained event reasoning abilities to effectively deduce and incorporate pertinent event knowledge. In this paper, we propose Event-Oriented Instruction Tuning (EvIT) to train our LLM. Specifically, we first propose a novel structure named event quadruple which contains the structure and semantics of events and is complete in the event representation. We then design event-relation learning based on the structures. We encapsulate the learning into the instruction-tuning formulation to better stimulate the event reasoning capacity of our model. We design a heuristic unsupervised method to mine event quadruple from a large-scale corpus. At last, we finetune a Llama model on our Event-Oriented Instruction Tuning. We conduct extensive experiments on event reasoning tasks on several datasets. Automatic and human evaluations demonstrate EvIT achieves competitive performances on event reasoning.","sentences":["Events refer to specific occurrences, incidents, or happenings that take place under a particular background.","Event reasoning aims to infer events according to certain relations and predict future events.","The cutting-edge techniques for event reasoning play a crucial role in various natural language processing applications.","Large language models (LLMs) have made significant advancements in event reasoning owing to their wealth of knowledge and reasoning capabilities.","However, smaller instruction-tuned models currently in use do not consistently demonstrate exceptional proficiency in managing these tasks.","This discrepancy arises from the absence of explicit modeling of events and the interconnections of them within their instruction data.","Consequently, these models face challenges in comprehending event structures and semantics while struggling to bridge the gap between their interpretations and human understanding of events.","Additionally, their limitations in grasping event relations lead to constrained event reasoning abilities to effectively deduce and incorporate pertinent event knowledge.","In this paper, we propose Event-Oriented Instruction Tuning (EvIT) to train our LLM.","Specifically, we first propose a novel structure named event quadruple which contains the structure and semantics of events and is complete in the event representation.","We then design event-relation learning based on the structures.","We encapsulate the learning into the instruction-tuning formulation to better stimulate the event reasoning capacity of our model.","We design a heuristic unsupervised method to mine event quadruple from a large-scale corpus.","At last, we finetune a Llama model on our Event-Oriented Instruction Tuning.","We conduct extensive experiments on event reasoning tasks on several datasets.","Automatic and human evaluations demonstrate EvIT achieves competitive performances on event reasoning."],"url":"http://arxiv.org/abs/2404.11978v1","category":"cs.CL"}
{"created":"2024-04-18 08:07:42","title":"Large Language Models: From Notes to Musical Form","abstract":"While many topics of the learning-based approach to automated music generation are under active research, musical form is under-researched. In particular, recent methods based on deep learning models generate music that, at the largest time scale, lacks any structure. In practice, music longer than one minute generated by such models is either unpleasantly repetitive or directionless. Adapting a recent music generation model, this paper proposes a novel method to generate music with form. The experimental results show that the proposed method can generate 2.5-minute-long music that is considered as pleasant as the music used to train the model. The paper first reviews a recent music generation method based on language models (transformer architecture). We discuss why learning musical form by such models is infeasible. Then we discuss our proposed method and the experiments.","sentences":["While many topics of the learning-based approach to automated music generation are under active research, musical form is under-researched.","In particular, recent methods based on deep learning models generate music that, at the largest time scale, lacks any structure.","In practice, music longer than one minute generated by such models is either unpleasantly repetitive or directionless.","Adapting a recent music generation model, this paper proposes a novel method to generate music with form.","The experimental results show that the proposed method can generate 2.5-minute-long music that is considered as pleasant as the music used to train the model.","The paper first reviews a recent music generation method based on language models (transformer architecture).","We discuss why learning musical form by such models is infeasible.","Then we discuss our proposed method and the experiments."],"url":"http://arxiv.org/abs/2404.11976v1","category":"cs.SD"}
{"created":"2024-04-18 07:56:09","title":"More on the (Uniform) Mazur Intersection Property","abstract":"In this paper, we introduce two moduli of w*-semidenting points and characterise the Mazur Intersection Property (MIP) and the Uniform MIP (UMIP) in terms of these moduli. We show that a property slightly stronger than UMIP already implies uniform convexity of the dual. This may lead to a possible approach towards answering the long standing open question whether the UMIP implies the existence of an equivalent uniformly convex renorming. We also obtain the condition for stability of the UMIP under $\\ell_p$-sums.","sentences":["In this paper, we introduce two moduli of w*-semidenting points and characterise the Mazur Intersection Property (MIP) and the Uniform MIP (UMIP) in terms of these moduli.","We show that a property slightly stronger than UMIP already implies uniform convexity of the dual.","This may lead to a possible approach towards answering the long standing open question whether the UMIP implies the existence of an equivalent uniformly convex renorming.","We also obtain the condition for stability of the UMIP under $\\ell_p$-sums."],"url":"http://arxiv.org/abs/2404.11970v1","category":"math.FA"}
{"created":"2024-04-18 07:25:59","title":"Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with Self-Distillation","abstract":"Semantic scene completion, also known as semantic occupancy prediction, can provide dense geometric and semantic information for autonomous vehicles, which attracts the increasing attention of both academia and industry. Unfortunately, existing methods usually formulate this task as a voxel-wise classification problem and treat each voxel equally in 3D space during training. As the hard voxels have not been paid enough attention, the performance in some challenging regions is limited. The 3D dense space typically contains a large number of empty voxels, which are easy to learn but require amounts of computation due to handling all the voxels uniformly for the existing models. Furthermore, the voxels in the boundary region are more challenging to differentiate than those in the interior. In this paper, we propose HASSC approach to train the semantic scene completion model with hardness-aware design. The global hardness from the network optimization process is defined for dynamical hard voxel selection. Then, the local hardness with geometric anisotropy is adopted for voxel-wise refinement. Besides, self-distillation strategy is introduced to make training process stable and consistent. Extensive experiments show that our HASSC scheme can effectively promote the accuracy of the baseline model without incurring the extra inference cost. Source code is available at: https://github.com/songw-zju/HASSC.","sentences":["Semantic scene completion, also known as semantic occupancy prediction, can provide dense geometric and semantic information for autonomous vehicles, which attracts the increasing attention of both academia and industry.","Unfortunately, existing methods usually formulate this task as a voxel-wise classification problem and treat each voxel equally in 3D space during training.","As the hard voxels have not been paid enough attention, the performance in some challenging regions is limited.","The 3D dense space typically contains a large number of empty voxels, which are easy to learn but require amounts of computation due to handling all the voxels uniformly for the existing models.","Furthermore, the voxels in the boundary region are more challenging to differentiate than those in the interior.","In this paper, we propose HASSC approach to train the semantic scene completion model with hardness-aware design.","The global hardness from the network optimization process is defined for dynamical hard voxel selection.","Then, the local hardness with geometric anisotropy is adopted for voxel-wise refinement.","Besides, self-distillation strategy is introduced to make training process stable and consistent.","Extensive experiments show that our HASSC scheme can effectively promote the accuracy of the baseline model without incurring the extra inference cost.","Source code is available at: https://github.com/songw-zju/HASSC."],"url":"http://arxiv.org/abs/2404.11958v1","category":"cs.CV"}
{"created":"2024-04-18 07:16:30","title":"Electrical control of a Kondo spin screening cloud","abstract":"In metals and semiconductors, an impurity spin is quantum entangled with and thereby screened by surrounding conduction electrons at low temperatures, called the Kondo screening cloud. Quantum confinement of the Kondo screening cloud in a region, called a Kondo box, with a length smaller than the original cloud extension length strongly deforms the screening cloud and provides a way of controlling the entanglement. Here we realize such a Kondo box and develop an approach to controlling and monitoring the entanglement. It is based on a spin localized in a semiconductor quantum dot, which is screened by conduction electrons along a quasi-one-dimensional channel. The box is formed between the dot and a quantum point contact placed on a channel. As the quantum point contact is tuned to make the confinement stronger, electron conductance through the dot as a function of temperature starts to deviate from the known universal function of the single energy scale, the Kondo temperature. Nevertheless, the entanglement is monitored by the measured conductance according to our theoretical development. The dependence of the monitored entanglement on the confinement strength and temperature implies that the Kondo screening is controlled by tuning the quantum point contact. Namely, the Kondo cloud is deformed by the Kondo box in the region across the original cloud length. Our findings offer a way of manipulating and detecting spatially extended quantum many-body entanglement in solids by electrical means.","sentences":["In metals and semiconductors, an impurity spin is quantum entangled with and thereby screened by surrounding conduction electrons at low temperatures, called the Kondo screening cloud.","Quantum confinement of the Kondo screening cloud in a region, called a Kondo box, with a length smaller than the original cloud extension length strongly deforms the screening cloud and provides a way of controlling the entanglement.","Here we realize such a Kondo box and develop an approach to controlling and monitoring the entanglement.","It is based on a spin localized in a semiconductor quantum dot, which is screened by conduction electrons along a quasi-one-dimensional channel.","The box is formed between the dot and a quantum point contact placed on a channel.","As the quantum point contact is tuned to make the confinement stronger, electron conductance through the dot as a function of temperature starts to deviate from the known universal function of the single energy scale, the Kondo temperature.","Nevertheless, the entanglement is monitored by the measured conductance according to our theoretical development.","The dependence of the monitored entanglement on the confinement strength and temperature implies that the Kondo screening is controlled by tuning the quantum point contact.","Namely, the Kondo cloud is deformed by the Kondo box in the region across the original cloud length.","Our findings offer a way of manipulating and detecting spatially extended quantum many-body entanglement in solids by electrical means."],"url":"http://arxiv.org/abs/2404.11955v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-18 07:15:20","title":"Odd-Derivative Couplings in Heterotic Theory","abstract":"In this paper, our focus is on exploring the gauge invariant basis for bosonic couplings within the framework of heterotic string theories, specifically examining 3-, 5-, and 7-derivative terms. We thoroughly analyze the invariance of these couplings under T-duality transformations and make a notable observation: the T-duality constraint enforces the vanishing of these couplings. We speculate that this result likely holds true for all higher odd-derivative couplings as well. This is unlike the result in type I superstring theory, where, for example, the couplings of 5 Yang-Mills field strength are non-zero.","sentences":["In this paper, our focus is on exploring the gauge invariant basis for bosonic couplings within the framework of heterotic string theories, specifically examining 3-, 5-, and 7-derivative terms.","We thoroughly analyze the invariance of these couplings under T-duality transformations and make a notable observation: the T-duality constraint enforces the vanishing of these couplings.","We speculate that this result likely holds true for all higher odd-derivative couplings as well.","This is unlike the result in type I superstring theory, where, for example, the couplings of 5 Yang-Mills field strength are non-zero."],"url":"http://arxiv.org/abs/2404.11954v1","category":"hep-th"}
{"created":"2024-04-18 06:59:40","title":"VCC-INFUSE: Towards Accurate and Efficient Selection of Unlabeled Examples in Semi-supervised Learning","abstract":"Despite the progress of Semi-supervised Learning (SSL), existing methods fail to utilize unlabeled data effectively and efficiently. Many pseudo-label-based methods select unlabeled examples based on inaccurate confidence scores from the classifier. Most prior work also uses all available unlabeled data without pruning, making it difficult to handle large amounts of unlabeled data. To address these issues, we propose two methods: Variational Confidence Calibration (VCC) and Influence-Function-based Unlabeled Sample Elimination (INFUSE). VCC is an universal plugin for SSL confidence calibration, using a variational autoencoder to select more accurate pseudo labels based on three types of consistency scores. INFUSE is a data pruning method that constructs a core dataset of unlabeled examples under SSL. Our methods are effective in multiple datasets and settings, reducing classification errors rates and saving training time. Together, VCC-INFUSE reduces the error rate of FlexMatch on the CIFAR-100 dataset by 1.08% while saving nearly half of the training time.","sentences":["Despite the progress of Semi-supervised Learning (SSL), existing methods fail to utilize unlabeled data effectively and efficiently.","Many pseudo-label-based methods select unlabeled examples based on inaccurate confidence scores from the classifier.","Most prior work also uses all available unlabeled data without pruning, making it difficult to handle large amounts of unlabeled data.","To address these issues, we propose two methods: Variational Confidence Calibration (VCC) and Influence-Function-based Unlabeled Sample Elimination (INFUSE).","VCC is an universal plugin for SSL confidence calibration, using a variational autoencoder to select more accurate pseudo labels based on three types of consistency scores.","INFUSE is a data pruning method that constructs a core dataset of unlabeled examples under SSL.","Our methods are effective in multiple datasets and settings, reducing classification errors rates and saving training time.","Together, VCC-INFUSE reduces the error rate of FlexMatch on the CIFAR-100 dataset by 1.08% while saving nearly half of the training time."],"url":"http://arxiv.org/abs/2404.11947v1","category":"cs.LG"}
{"created":"2024-04-18 06:52:26","title":"Terrain-Aware Stride-Level Trajectory Forecasting for a Powered Hip Exoskeleton via Vision and Kinematics Fusion","abstract":"Powered hip exoskeletons have shown the ability for locomotion assistance during treadmill walking. However, providing suitable assistance in real-world walking scenarios which involve changing terrain remains challenging. Recent research suggests that forecasting the lower limb joint's angles could provide target trajectories for exoskeletons and prostheses, and the performance could be improved with visual information. In this letter, We share a real-world dataset of 10 healthy subjects walking through five common types of terrain with stride-level label. We design a network called Sandwich Fusion Transformer for Image and Kinematics (SFTIK), which predicts the thigh angle of the ensuing stride given the terrain images at the beginning of the preceding and the ensuing stride and the IMU time series during the preceding stride. We introduce width-level patchify, tailored for egocentric terrain images, to reduce the computational demands. We demonstrate the proposed sandwich input and fusion mechanism could significantly improve the forecasting performance. Overall, the SFTIK outperforms baseline methods, achieving a computational efficiency of 3.31 G Flops, and root mean square error (RMSE) of 3.445 \\textpm \\ 0.804\\textdegree \\ and Pearson's correlation coefficient (PCC) of 0.971 \\textpm\\ 0.025. The results demonstrate that SFTIK could forecast the thigh's angle accurately with low computational cost, which could serve as a terrain adaptive trajectory planning method for hip exoskeletons. Codes and data are available at https://github.com/RuoqiZhao116/SFTIK.","sentences":["Powered hip exoskeletons have shown the ability for locomotion assistance during treadmill walking.","However, providing suitable assistance in real-world walking scenarios which involve changing terrain remains challenging.","Recent research suggests that forecasting the lower limb joint's angles could provide target trajectories for exoskeletons and prostheses, and the performance could be improved with visual information.","In this letter, We share a real-world dataset of 10 healthy subjects walking through five common types of terrain with stride-level label.","We design a network called Sandwich Fusion Transformer for Image and Kinematics (SFTIK), which predicts the thigh angle of the ensuing stride given the terrain images at the beginning of the preceding and the ensuing stride and the IMU time series during the preceding stride.","We introduce width-level patchify, tailored for egocentric terrain images, to reduce the computational demands.","We demonstrate the proposed sandwich input and fusion mechanism could significantly improve the forecasting performance.","Overall, the SFTIK outperforms baseline methods, achieving a computational efficiency of 3.31 G Flops, and root mean square error (RMSE) of 3.445 \\textpm \\ 0.804\\textdegree \\ and Pearson's correlation coefficient (PCC) of 0.971 \\textpm\\ 0.025.","The results demonstrate that SFTIK could forecast the thigh's angle accurately with low computational cost, which could serve as a terrain adaptive trajectory planning method for hip exoskeletons.","Codes and data are available at https://github.com/RuoqiZhao116/SFTIK."],"url":"http://arxiv.org/abs/2404.11945v1","category":"cs.RO"}
{"created":"2024-04-18 06:47:30","title":"Trusted Multi-view Learning with Label Noise","abstract":"Multi-view learning methods often focus on improving decision accuracy while neglecting the decision uncertainty, which significantly restricts their applications in safety-critical applications. To address this issue, researchers propose trusted multi-view methods that learn the class distribution for each instance, enabling the estimation of classification probabilities and uncertainty. However, these methods heavily rely on high-quality ground-truth labels. This motivates us to delve into a new generalized trusted multi-view learning problem: how to develop a reliable multi-view learning model under the guidance of noisy labels? We propose a trusted multi-view noise refining method to solve this problem. We first construct view-opinions using evidential deep neural networks, which consist of belief mass vectors and uncertainty estimates. Subsequently, we design view-specific noise correlation matrices that transform the original opinions into noisy opinions aligned with the noisy labels. Considering label noises originating from low-quality data features and easily-confused classes, we ensure that the diagonal elements of these matrices are inversely proportional to the uncertainty, while incorporating class relations into the off-diagonal elements. Finally, we aggregate the noisy opinions and employ a generalized maximum likelihood loss on the aggregated opinion for model training, guided by the noisy labels. We empirically compare TMNR with state-of-the-art trusted multi-view learning and label noise learning baselines on 5 publicly available datasets. Experiment results show that TMNR outperforms baseline methods on accuracy, reliability and robustness. We promise to release the code and all datasets on Github and show the link here.","sentences":["Multi-view learning methods often focus on improving decision accuracy while neglecting the decision uncertainty, which significantly restricts their applications in safety-critical applications.","To address this issue, researchers propose trusted multi-view methods that learn the class distribution for each instance, enabling the estimation of classification probabilities and uncertainty.","However, these methods heavily rely on high-quality ground-truth labels.","This motivates us to delve into a new generalized trusted multi-view learning problem: how to develop a reliable multi-view learning model under the guidance of noisy labels?","We propose a trusted multi-view noise refining method to solve this problem.","We first construct view-opinions using evidential deep neural networks, which consist of belief mass vectors and uncertainty estimates.","Subsequently, we design view-specific noise correlation matrices that transform the original opinions into noisy opinions aligned with the noisy labels.","Considering label noises originating from low-quality data features and easily-confused classes, we ensure that the diagonal elements of these matrices are inversely proportional to the uncertainty, while incorporating class relations into the off-diagonal elements.","Finally, we aggregate the noisy opinions and employ a generalized maximum likelihood loss on the aggregated opinion for model training, guided by the noisy labels.","We empirically compare TMNR with state-of-the-art trusted multi-view learning and label noise learning baselines on 5 publicly available datasets.","Experiment results show that TMNR outperforms baseline methods on accuracy, reliability and robustness.","We promise to release the code and all datasets on Github and show the link here."],"url":"http://arxiv.org/abs/2404.11944v1","category":"cs.LG"}
{"created":"2024-04-18 06:38:15","title":"Regional impacts poorly constrained by climate sensitivity","abstract":"Climate risk assessments must account for a wide range of possible futures, so scientists often use simulations made by numerous global climate models to explore potential changes in regional climates and their impacts. Some of the latest-generation models have high effective climate sensitivities or EffCS. It has been argued these so-called hot models are unrealistic and should therefore be excluded from analyses of climate change impacts. Whether this would improve regional impact assessments, or make them worse, is unclear. Here we show there is no universal relationship between EffCS and projected changes in a number of important climatic drivers of regional impacts. Analysing heavy rainfall events, meteorological drought, and fire weather in different regions, we find little or no significant correlation with EffCS for most regions and climatic drivers. Even when a correlation is found, internal variability and processes unrelated to EffCS have similar effects on projected changes in the climatic drivers as EffCS. Model selection based solely on EffCS appears to be unjustified and may neglect realistic impacts, leading to an underestimation of climate risks.","sentences":["Climate risk assessments must account for a wide range of possible futures, so scientists often use simulations made by numerous global climate models to explore potential changes in regional climates and their impacts.","Some of the latest-generation models have high effective climate sensitivities or EffCS.","It has been argued these so-called hot models are unrealistic and should therefore be excluded from analyses of climate change impacts.","Whether this would improve regional impact assessments, or make them worse, is unclear.","Here we show there is no universal relationship between EffCS and projected changes in a number of important climatic drivers of regional impacts.","Analysing heavy rainfall events, meteorological drought, and fire weather in different regions, we find little or no significant correlation with EffCS for most regions and climatic drivers.","Even when a correlation is found, internal variability and processes unrelated to EffCS have similar effects on projected changes in the climatic drivers as EffCS.","Model selection based solely on EffCS appears to be unjustified and may neglect realistic impacts, leading to an underestimation of climate risks."],"url":"http://arxiv.org/abs/2404.11939v1","category":"physics.ao-ph"}
{"created":"2024-04-18 06:16:34","title":"Oceanic influence on Large-Scale Atmospheric Convection during co-occurring La Nina and IOD events","abstract":"The ISMR profoundly impacts over a billion people across the region. ISMR extremes have been linked to the ENSO and modulated by the Indian Ocean Dipole (IOD). ISMR of 2022 displayed intriguing spatial patterns: above-normal precipitation over the south peninsula and central India, normal over Northwest India, and below-normal over East and NE India. In 2022, La Nina conditions associated with a negative IOD (nIOD), were prevalent over the equatorial Pacific Ocean (PO) and IO. This study investigates the often-overlooked Oceanic subsurface contribution to large-scale atmospheric convection over the tropical IO. By undertaking a comprehensive analysis, we find that strong equatorial westerly wind anomalies prevailed over the equatorial IO during co-occurring years generating eastward propagating downwelling Kelvin waves which deepens the D20 in the eastern equatorial IO. The Kelvin waves then propagates into BoB and gets reflected from southern tip of India as westward propagating Rossby waves deepening the D20 and causing low level wind convergence in the Northern Arabian Sea(NAS). We identify two centres of low-level wind convergence, one over NAS and another over Maritime Continent during co-occurrence years. These wind convergence centres play a pivotal role in channeling moisture towards their core, also, significantly deepen D20, increase the Ocean heat content and thus maintaining warmer SSTs over area of conversions. Together, these interrelated factors create an environment that is conducive for enhanced convective activity over the zone of convergence, enhancing and sustaining the convective conditions. This study underscores the significant contribution of ocean dynamics in shaping large-scale atmospheric convection during the co-occurrence years, highlighting the significance of considering broader Oceanic variables in comprehending and forecasting monsoonal variability.","sentences":["The ISMR profoundly impacts over a billion people across the region.","ISMR extremes have been linked to the ENSO and modulated by the Indian Ocean Dipole (IOD).","ISMR of 2022 displayed intriguing spatial patterns: above-normal precipitation over the south peninsula and central India, normal over Northwest India, and below-normal over East and NE India.","In 2022, La Nina conditions associated with a negative IOD (nIOD), were prevalent over the equatorial Pacific Ocean (PO) and IO.","This study investigates the often-overlooked Oceanic subsurface contribution to large-scale atmospheric convection over the tropical IO.","By undertaking a comprehensive analysis, we find that strong equatorial westerly wind anomalies prevailed over the equatorial IO during co-occurring years generating eastward propagating downwelling Kelvin waves which deepens the D20 in the eastern equatorial IO.","The Kelvin waves then propagates into BoB and gets reflected from southern tip of India as westward propagating Rossby waves deepening the D20 and causing low level wind convergence in the Northern Arabian Sea(NAS).","We identify two centres of low-level wind convergence, one over NAS and another over Maritime Continent during co-occurrence years.","These wind convergence centres play a pivotal role in channeling moisture towards their core, also, significantly deepen D20, increase the Ocean heat content and thus maintaining warmer SSTs over area of conversions.","Together, these interrelated factors create an environment that is conducive for enhanced convective activity over the zone of convergence, enhancing and sustaining the convective conditions.","This study underscores the significant contribution of ocean dynamics in shaping large-scale atmospheric convection during the co-occurrence years, highlighting the significance of considering broader Oceanic variables in comprehending and forecasting monsoonal variability."],"url":"http://arxiv.org/abs/2404.11928v1","category":"physics.ao-ph"}
{"created":"2024-04-18 05:59:28","title":"Redefining the Shortest Path Problem Formulation of the Linear Non-Gaussian Acyclic Model: Pairwise Likelihood Ratios, Prior Knowledge, and Path Enumeration","abstract":"Effective causal discovery is essential for learning the causal graph from observational data. The linear non-Gaussian acyclic model (LiNGAM) operates under the assumption of a linear data generating process with non-Gaussian noise in determining the causal graph. Its assumption of unmeasured confounders being absent, however, poses practical limitations. In response, empirical research has shown that the reformulation of LiNGAM as a shortest path problem (LiNGAM-SPP) addresses this limitation. Within LiNGAM-SPP, mutual information is chosen to serve as the measure of independence. A challenge is introduced - parameter tuning is now needed due to its reliance on kNN mutual information estimators. The paper proposes a threefold enhancement to the LiNGAM-SPP framework.   First, the need for parameter tuning is eliminated by using the pairwise likelihood ratio in lieu of kNN-based mutual information. This substitution is validated on a general data generating process and benchmark real-world data sets, outperforming existing methods especially when given a larger set of features. The incorporation of prior knowledge is then enabled by a node-skipping strategy implemented on the graph representation of all causal orderings to eliminate violations based on the provided input of relative orderings. Flexibility relative to existing approaches is achieved. Last among the three enhancements is the utilization of the distribution of paths in the graph representation of all causal orderings. From this, crucial properties of the true causal graph such as the presence of unmeasured confounders and sparsity may be inferred. To some extent, the expected performance of the causal discovery algorithm may be predicted. The refinements above advance the practicality and performance of LiNGAM-SPP, showcasing the potential of graph-search-based methodologies in advancing causal discovery.","sentences":["Effective causal discovery is essential for learning the causal graph from observational data.","The linear non-Gaussian acyclic model (LiNGAM) operates under the assumption of a linear data generating process with non-Gaussian noise in determining the causal graph.","Its assumption of unmeasured confounders being absent, however, poses practical limitations.","In response, empirical research has shown that the reformulation of LiNGAM as a shortest path problem (LiNGAM-SPP) addresses this limitation.","Within LiNGAM-SPP, mutual information is chosen to serve as the measure of independence.","A challenge is introduced - parameter tuning is now needed due to its reliance on kNN mutual information estimators.","The paper proposes a threefold enhancement to the LiNGAM-SPP framework.   ","First, the need for parameter tuning is eliminated by using the pairwise likelihood ratio in lieu of kNN-based mutual information.","This substitution is validated on a general data generating process and benchmark real-world data sets, outperforming existing methods especially when given a larger set of features.","The incorporation of prior knowledge is then enabled by a node-skipping strategy implemented on the graph representation of all causal orderings to eliminate violations based on the provided input of relative orderings.","Flexibility relative to existing approaches is achieved.","Last among the three enhancements is the utilization of the distribution of paths in the graph representation of all causal orderings.","From this, crucial properties of the true causal graph such as the presence of unmeasured confounders and sparsity may be inferred.","To some extent, the expected performance of the causal discovery algorithm may be predicted.","The refinements above advance the practicality and performance of LiNGAM-SPP, showcasing the potential of graph-search-based methodologies in advancing causal discovery."],"url":"http://arxiv.org/abs/2404.11922v1","category":"cs.LG"}
{"created":"2024-04-18 05:22:40","title":"Even-parity stability of hairy black holes in $U(1)$ gauge-invariant scalar-vector-tensor theories","abstract":"The $U(1)$ gauge-invariant scalar-vector-tensor theories, which catches five degrees of freedom, are valuable for its implications to inflation problems, generation of primordial magnetic fields, new black hole (BH) and neutron star solutions, etc. In this paper, we derive conditions for the absence of ghosts and Laplacian instabilities of nontrivial BH solutions dressed with scalar hair against both odd- and even-parity perturbations on top of the static and spherically symmetric background in the most general $U(1)$ gauge-invariant scalar-vector-tensor theories with second-order equations of motion. In addition to some general discussions, several typical concrete models are investigated. Specially, we show that the stability against even-parity perturbations is ensured outside the event horizon under certain constraints to these models. This is a crucial step to check the self-consistency of the theories and to shed light on the physically accessible models of such theories for future studies.","sentences":["The $U(1)$ gauge-invariant scalar-vector-tensor theories, which catches five degrees of freedom, are valuable for its implications to inflation problems, generation of primordial magnetic fields, new black hole (BH) and neutron star solutions, etc.","In this paper, we derive conditions for the absence of ghosts and Laplacian instabilities of nontrivial BH solutions dressed with scalar hair against both odd- and even-parity perturbations on top of the static and spherically symmetric background in the most general $U(1)$ gauge-invariant scalar-vector-tensor theories with second-order equations of motion.","In addition to some general discussions, several typical concrete models are investigated.","Specially, we show that the stability against even-parity perturbations is ensured outside the event horizon under certain constraints to these models.","This is a crucial step to check the self-consistency of the theories and to shed light on the physically accessible models of such theories for future studies."],"url":"http://arxiv.org/abs/2404.11910v1","category":"gr-qc"}
{"created":"2024-04-18 05:10:05","title":"FedMID: A Data-Free Method for Using Intermediate Outputs as a Defense Mechanism Against Poisoning Attacks in Federated Learning","abstract":"Federated learning combines local updates from clients to produce a global model, which is susceptible to poisoning attacks. Most previous defense strategies relied on vectors derived from projections of local updates on a Euclidean space; however, these methods fail to accurately represent the functionality and structure of local models, resulting in inconsistent performance. Here, we present a new paradigm to defend against poisoning attacks in federated learning using functional mappings of local models based on intermediate outputs. Experiments show that our mechanism is robust under a broad range of computing conditions and advanced attack scenarios, enabling safer collaboration among data-sensitive participants via federated learning.","sentences":["Federated learning combines local updates from clients to produce a global model, which is susceptible to poisoning attacks.","Most previous defense strategies relied on vectors derived from projections of local updates on a Euclidean space; however, these methods fail to accurately represent the functionality and structure of local models, resulting in inconsistent performance.","Here, we present a new paradigm to defend against poisoning attacks in federated learning using functional mappings of local models based on intermediate outputs.","Experiments show that our mechanism is robust under a broad range of computing conditions and advanced attack scenarios, enabling safer collaboration among data-sensitive participants via federated learning."],"url":"http://arxiv.org/abs/2404.11905v1","category":"cs.LG"}
{"created":"2024-04-18 03:10:04","title":"Group-On: Boosting One-Shot Segmentation with Supportive Query","abstract":"One-shot semantic segmentation aims to segment query images given only ONE annotated support image of the same class. This task is challenging because target objects in the support and query images can be largely different in appearance and pose (i.e., intra-class variation). Prior works suggested that incorporating more annotated support images in few-shot settings boosts performances but increases costs due to additional manual labeling. In this paper, we propose a novel approach for ONE-shot semantic segmentation, called Group-On, which packs multiple query images in batches for the benefit of mutual knowledge support within the same category. Specifically, after coarse segmentation masks of the batch of queries are predicted, query-mask pairs act as pseudo support data to enhance mask predictions mutually, under the guidance of a simple Group-On Voting module. Comprehensive experiments on three standard benchmarks show that, in the ONE-shot setting, our Group-On approach significantly outperforms previous works by considerable margins. For example, on the COCO-20i dataset, we increase mIoU scores by 8.21% and 7.46% on ASNet and HSNet baselines, respectively. With only one support image, Group-On can be even competitive with the counterparts using 5 annotated support images.","sentences":["One-shot semantic segmentation aims to segment query images given only ONE annotated support image of the same class.","This task is challenging because target objects in the support and query images can be largely different in appearance and pose (i.e., intra-class variation).","Prior works suggested that incorporating more annotated support images in few-shot settings boosts performances but increases costs due to additional manual labeling.","In this paper, we propose a novel approach for ONE-shot semantic segmentation, called Group-On, which packs multiple query images in batches for the benefit of mutual knowledge support within the same category.","Specifically, after coarse segmentation masks of the batch of queries are predicted, query-mask pairs act as pseudo support data to enhance mask predictions mutually, under the guidance of a simple Group-On Voting module.","Comprehensive experiments on three standard benchmarks show that, in the ONE-shot setting, our Group-On approach significantly outperforms previous works by considerable margins.","For example, on the COCO-20i dataset, we increase mIoU scores by 8.21% and 7.46% on ASNet and HSNet baselines, respectively.","With only one support image, Group-On can be even competitive with the counterparts using 5 annotated support images."],"url":"http://arxiv.org/abs/2404.11871v1","category":"cs.CV"}
{"created":"2024-04-18 02:35:29","title":"A Fast Maximum Clique Algorithm Based on Network Decomposition for Large Sparse Networks","abstract":"Finding maximum cliques in large networks is a challenging combinatorial problem with many real-world applications. We present a fast algorithm to achieve the exact solution for the maximum clique problem in large sparse networks based on efficient graph decomposition. A bunch of effective techniques is being used to greatly prune the graph and a novel concept called Complete-Upper-Bound-Induced Subgraph (CUBIS) is proposed to ensure that the structures with the potential to form the maximum clique are retained in the process of graph decomposition. Our algorithm first pre-prunes peripheral nodes, subsequently, one or two small-scale CUBISs are constructed guided by the core number and current maximum clique size. Bron-Kerbosch search is performed on each CUBIS to find the maximum clique. Experiments on 50 empirical networks with a scale of up to 20 million show the CUBIS scales are largely independent of the original network scale. This enables an approximately linear runtime, making our algorithm amenable for large networks. Our work provides a new framework for effectively solving maximum clique problems on massive sparse graphs, which not only makes the graph scale no longer the bottleneck but also shows some light on solving other clique-related problems.","sentences":["Finding maximum cliques in large networks is a challenging combinatorial problem with many real-world applications.","We present a fast algorithm to achieve the exact solution for the maximum clique problem in large sparse networks based on efficient graph decomposition.","A bunch of effective techniques is being used to greatly prune the graph and a novel concept called Complete-Upper-Bound-Induced Subgraph (CUBIS) is proposed to ensure that the structures with the potential to form the maximum clique are retained in the process of graph decomposition.","Our algorithm first pre-prunes peripheral nodes, subsequently, one or two small-scale CUBISs are constructed guided by the core number and current maximum clique size.","Bron-Kerbosch search is performed on each CUBIS to find the maximum clique.","Experiments on 50 empirical networks with a scale of up to 20 million show the CUBIS scales are largely independent of the original network scale.","This enables an approximately linear runtime, making our algorithm amenable for large networks.","Our work provides a new framework for effectively solving maximum clique problems on massive sparse graphs, which not only makes the graph scale no longer the bottleneck but also shows some light on solving other clique-related problems."],"url":"http://arxiv.org/abs/2404.11862v1","category":"cs.SI"}
{"created":"2024-04-18 02:20:29","title":"Solutions to discrete nonlinear Kirchhoff-Choquard equations","abstract":"In this paper, we study the discrete Kirchhoff-Choquard equation $$ -\\left(a+b \\int_{\\mathbb{Z}^3}|\\nabla u|^{2} d \\mu\\right) \\Delta u+V(x) u=\\left(R_{\\alpha} *F(u)\\right)f(u),\\quad x\\in \\mathbb{Z}^3, $$ where $a,\\,b>0$ are constants, $R_{\\alpha}$ is the Green's function of the discrete fractional Laplacian with $\\alpha \\in(0,3)$, which has no singularity but has same asymptotics as the Riesz potential. Under some suitable assumptions on $V$ and $f$, we prove the existence of nontrivial solutions and ground state solutions by variational methods.","sentences":["In this paper, we study the discrete Kirchhoff-Choquard equation $$ -\\left(a+b \\int_{\\mathbb{Z}^3}|\\nabla u|^{2} d \\mu\\right) \\Delta u+V(x) u=\\left(R_{\\alpha} *F(u)\\right)f(u),\\quad x\\in \\mathbb{Z}^3, $$ where $a,\\,b>0$ are constants, $R_{\\alpha}$ is the Green's function of the discrete fractional Laplacian with $\\alpha \\in(0,3)$, which has no singularity but has same asymptotics as the Riesz potential.","Under some suitable assumptions on $V$ and $f$, we prove the existence of nontrivial solutions and ground state solutions by variational methods."],"url":"http://arxiv.org/abs/2404.11856v1","category":"math.AP"}
{"created":"2024-04-18 00:41:32","title":"Utilizing Adversarial Examples for Bias Mitigation and Accuracy Enhancement","abstract":"We propose a novel approach to mitigate biases in computer vision models by utilizing counterfactual generation and fine-tuning. While counterfactuals have been used to analyze and address biases in DNN models, the counterfactuals themselves are often generated from biased generative models, which can introduce additional biases or spurious correlations. To address this issue, we propose using adversarial images, that is images that deceive a deep neural network but not humans, as counterfactuals for fair model training.   Our approach leverages a curriculum learning framework combined with a fine-grained adversarial loss to fine-tune the model using adversarial examples. By incorporating adversarial images into the training data, we aim to prevent biases from propagating through the pipeline. We validate our approach through both qualitative and quantitative assessments, demonstrating improved bias mitigation and accuracy compared to existing methods. Qualitatively, our results indicate that post-training, the decisions made by the model are less dependent on the sensitive attribute and our model better disentangles the relationship between sensitive attributes and classification variables.","sentences":["We propose a novel approach to mitigate biases in computer vision models by utilizing counterfactual generation and fine-tuning.","While counterfactuals have been used to analyze and address biases in DNN models, the counterfactuals themselves are often generated from biased generative models, which can introduce additional biases or spurious correlations.","To address this issue, we propose using adversarial images, that is images that deceive a deep neural network but not humans, as counterfactuals for fair model training.   ","Our approach leverages a curriculum learning framework combined with a fine-grained adversarial loss to fine-tune the model using adversarial examples.","By incorporating adversarial images into the training data, we aim to prevent biases from propagating through the pipeline.","We validate our approach through both qualitative and quantitative assessments, demonstrating improved bias mitigation and accuracy compared to existing methods.","Qualitatively, our results indicate that post-training, the decisions made by the model are less dependent on the sensitive attribute and our model better disentangles the relationship between sensitive attributes and classification variables."],"url":"http://arxiv.org/abs/2404.11819v1","category":"cs.CV"}
{"created":"2024-04-18 00:26:29","title":"AquaSonic: Acoustic Manipulation of Underwater Data Center Operations and Resource Management","abstract":"Underwater datacenters (UDCs) hold promise as next-generation data storage due to their energy efficiency and environmental sustainability benefits. While the natural cooling properties of water save power, the isolated aquatic environment and long-range sound propagation in water create unique vulnerabilities which differ from those of on-land data centers. Our research discovers the unique vulnerabilities of fault-tolerant storage devices, resource allocation software, and distributed file systems to acoustic injection attacks in UDCs. With a realistic testbed approximating UDC server operations, we empirically characterize the capabilities of acoustic injection underwater and find that an attacker can reduce fault-tolerant RAID 5 storage system throughput by 17% up to 100%. Our closed-water analyses reveal that attackers can (i) cause unresponsiveness and automatic node removal in a distributed filesystem with only 2.4 minutes of sustained acoustic injection, (ii) induce a distributed database's latency to increase by up to 92.7% to reduce system reliability, and (iii) induce load-balance managers to redirect up to 74% of resources to a target server to cause overload or force resource colocation. Furthermore, we perform open-water experiments in a lake and find that an attacker can cause controlled throughput degradation at a maximum allowable distance of 6.35 m using a commercial speaker. We also investigate and discuss the effectiveness of standard defenses against acoustic injection attacks. Finally, we formulate a novel machine learning-based detection system that reaches 0% False Positive Rate and 98.2% True Positive Rate trained on our dataset of profiled hard disk drives under 30-second FIO benchmark execution. With this work, we aim to help manufacturers proactively protect UDCs against acoustic injection attacks and ensure the security of subsea computing infrastructures.","sentences":["Underwater datacenters (UDCs) hold promise as next-generation data storage due to their energy efficiency and environmental sustainability benefits.","While the natural cooling properties of water save power, the isolated aquatic environment and long-range sound propagation in water create unique vulnerabilities which differ from those of on-land data centers.","Our research discovers the unique vulnerabilities of fault-tolerant storage devices, resource allocation software, and distributed file systems to acoustic injection attacks in UDCs.","With a realistic testbed approximating UDC server operations, we empirically characterize the capabilities of acoustic injection underwater and find that an attacker can reduce fault-tolerant RAID 5 storage system throughput by 17% up to 100%.","Our closed-water analyses reveal that attackers can (i) cause unresponsiveness and automatic node removal in a distributed filesystem with only 2.4 minutes of sustained acoustic injection, (ii) induce a distributed database's latency to increase by up to 92.7% to reduce system reliability, and (iii) induce load-balance managers to redirect up to 74% of resources to a target server to cause overload or force resource colocation.","Furthermore, we perform open-water experiments in a lake and find that an attacker can cause controlled throughput degradation at a maximum allowable distance of 6.35 m using a commercial speaker.","We also investigate and discuss the effectiveness of standard defenses against acoustic injection attacks.","Finally, we formulate a novel machine learning-based detection system that reaches 0% False Positive Rate and 98.2% True Positive Rate trained on our dataset of profiled hard disk drives under 30-second FIO benchmark execution.","With this work, we aim to help manufacturers proactively protect UDCs against acoustic injection attacks and ensure the security of subsea computing infrastructures."],"url":"http://arxiv.org/abs/2404.11815v1","category":"cs.CR"}
{"created":"2024-04-18 00:25:54","title":"Laser Irradiation of Carbonaceous Chondrite Simulants: Space Weathering Implications for C-complex Asteroids","abstract":"Surfaces of carbonaceous asteroids (C-complex) have shown diverse contrasting spectral variations, which may be related to space weathering. We performed laser irradiation experiments on CI and CM simulant material under vacuum to mimic the spectral alteration induced by micrometeorite impacts. We used in situ ultraviolet-visible and near-infrared reflectance spectroscopy to analyze spectral alterations in response to pulsed laser irradiation, as well as scanning electron microscopy and x-ray photoelectron spectroscopy to search for microstructural and compositional changes. Laser irradiation causes an increase in spectral slope (reddening) and a decrease in the albedo (darkening), and these changes are stronger in the ultraviolet-visible region. These spectral changes are likely driven by the excess iron found in the altered surface region, although other factors, such as the observed structural changes, may also contribute. Additionally, while the 0.27~${\\mu}m$ band appears relatively stable under laser irradiation, a broad feature at 0.6~${\\mu}m$ rapidly disappears with laser irradiation, suggesting that space weathering may inhibit the detection of any feature in this spectral region, including the 0.7~${\\mu}m$ band, which has typically been used an indicator of hydration. Comparing our laboratory results with optical spectrophotometry observations of C-complex asteroids, we find that the majority of objects are spectrally red and possess colors that are similar to our irradiated material rather than our fresh samples. Furthermore, we also find that ``younger'' and ``older'' C-complex families have similar colors, suggesting that the space weathering process is near-equal or faster than the time it takes to refresh the surfaces of these airless bodies.","sentences":["Surfaces of carbonaceous asteroids (C-complex) have shown diverse contrasting spectral variations, which may be related to space weathering.","We performed laser irradiation experiments on CI and CM simulant material under vacuum to mimic the spectral alteration induced by micrometeorite impacts.","We used in situ ultraviolet-visible and near-infrared reflectance spectroscopy to analyze spectral alterations in response to pulsed laser irradiation, as well as scanning electron microscopy and x-ray photoelectron spectroscopy to search for microstructural and compositional changes.","Laser irradiation causes an increase in spectral slope (reddening) and a decrease in the albedo (darkening), and these changes are stronger in the ultraviolet-visible region.","These spectral changes are likely driven by the excess iron found in the altered surface region, although other factors, such as the observed structural changes, may also contribute.","Additionally, while the 0.27~${\\mu}m$ band appears relatively stable under laser irradiation, a broad feature at 0.6~${\\mu}m$ rapidly disappears with laser irradiation, suggesting that space weathering may inhibit the detection of any feature in this spectral region, including the 0.7~${\\mu}m$ band, which has typically been used an indicator of hydration.","Comparing our laboratory results with optical spectrophotometry observations of C-complex asteroids, we find that the majority of objects are spectrally red and possess colors that are similar to our irradiated material rather than our fresh samples.","Furthermore, we also find that ``younger'' and ``older'' C-complex families have similar colors, suggesting that the space weathering process is near-equal or faster than the time it takes to refresh the surfaces of these airless bodies."],"url":"http://arxiv.org/abs/2404.11814v1","category":"astro-ph.EP"}
{"created":"2024-04-18 00:11:51","title":"Holographic Parallax Improves 3D Perceptual Realism","abstract":"Holographic near-eye displays are a promising technology to solve long-standing challenges in virtual and augmented reality display systems. Over the last few years, many different computer-generated holography (CGH) algorithms have been proposed that are supervised by different types of target content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It is unclear, however, what the perceptual implications are of the choice of algorithm and target content type. In this work, we build a perceptual testbed of a full-color, high-quality holographic near-eye display. Under natural viewing conditions, we examine the effects of various CGH supervision formats and conduct user studies to assess their perceptual impacts on 3D realism. Our results indicate that CGH algorithms designed for specific viewpoints exhibit noticeable deficiencies in achieving 3D realism. In contrast, holograms incorporating parallax cues consistently outperform other formats across different viewing conditions, including the center of the eyebox. This finding is particularly interesting and suggests that the inclusion of parallax cues in CGH rendering plays a crucial role in enhancing the overall quality of the holographic experience. This work represents an initial stride towards delivering a perceptually realistic 3D experience with holographic near-eye displays.","sentences":["Holographic near-eye displays are a promising technology to solve long-standing challenges in virtual and augmented reality display systems.","Over the last few years, many different computer-generated holography (CGH) algorithms have been proposed that are supervised by different types of target content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields.","It is unclear, however, what the perceptual implications are of the choice of algorithm and target content type.","In this work, we build a perceptual testbed of a full-color, high-quality holographic near-eye display.","Under natural viewing conditions, we examine the effects of various CGH supervision formats and conduct user studies to assess their perceptual impacts on 3D realism.","Our results indicate that CGH algorithms designed for specific viewpoints exhibit noticeable deficiencies in achieving 3D realism.","In contrast, holograms incorporating parallax cues consistently outperform other formats across different viewing conditions, including the center of the eyebox.","This finding is particularly interesting and suggests that the inclusion of parallax cues in CGH rendering plays a crucial role in enhancing the overall quality of the holographic experience.","This work represents an initial stride towards delivering a perceptually realistic 3D experience with holographic near-eye displays."],"url":"http://arxiv.org/abs/2404.11810v1","category":"cs.GR"}
{"created":"2024-04-17 22:39:09","title":"A Sequential Benders-based Mixed-Integer Quadratic Programming Algorithm","abstract":"For continuous decision spaces, nonlinear programs (NLPs) can be efficiently solved via sequential quadratic programming (SQP) and, more generally, sequential convex programming (SCP). These algorithms linearize only the nonlinear equality constraints and keep the outer convex structure of the problem intact. The aim of the presented sequential mixed-integer quadratic programming (MIQP) algorithm for mixed-integer nonlinear problems (MINLPs) is to extend the SQP/SCP methodology to MINLPs and leverage the availability of efficient MIQP solvers. The algorithm employs a three-step method in each iterate: First, the MINLP is linearized at a given iterate. Second, an MIQP with its feasible set restricted to a specific region around the current linearization point is formulated and solved. Third, the integer variables obtained from the MIQP solution are fixed, and only an NLP in the continuous variables is solved. The outcome of the third step is compared to previous iterates, and the best iterate so far is used as a linearization point in the next iterate. Crucially, the objective values and derivatives from all previous iterates are used to formulate the polyhedral region in the second step. The linear inequalities that define the region build on concepts from generalized Benders' decomposition for MINLPs. Although the presented MINLP algorithm is a heuristic method without any global optimality guarantee, it converges to the exact integer solution when applied to convex MINLP with a linear outer structure. The conducted numerical experiments demonstrate that the proposed algorithm is competitive with other open-source solvers for MINLP. Finally, we solve two mixed-integer optimal control problems (MIOCPs) transcribed into MINLPs via direct methods, showing that the presented algorithm can effectively deal with nonlinear equality constraints, a major hurdle for generic MINLP solvers.","sentences":["For continuous decision spaces, nonlinear programs (NLPs) can be efficiently solved via sequential quadratic programming (SQP) and, more generally, sequential convex programming (SCP).","These algorithms linearize only the nonlinear equality constraints and keep the outer convex structure of the problem intact.","The aim of the presented sequential mixed-integer quadratic programming (MIQP) algorithm for mixed-integer nonlinear problems (MINLPs) is to extend the SQP/SCP methodology to MINLPs and leverage the availability of efficient MIQP solvers.","The algorithm employs a three-step method in each iterate: First, the MINLP is linearized at a given iterate.","Second, an MIQP with its feasible set restricted to a specific region around the current linearization point is formulated and solved.","Third, the integer variables obtained from the MIQP solution are fixed, and only an NLP in the continuous variables is solved.","The outcome of the third step is compared to previous iterates, and the best iterate so far is used as a linearization point in the next iterate.","Crucially, the objective values and derivatives from all previous iterates are used to formulate the polyhedral region in the second step.","The linear inequalities that define the region build on concepts from generalized Benders' decomposition for MINLPs.","Although the presented MINLP algorithm is a heuristic method without any global optimality guarantee, it converges to the exact integer solution when applied to convex MINLP with a linear outer structure.","The conducted numerical experiments demonstrate that the proposed algorithm is competitive with other open-source solvers for MINLP.","Finally, we solve two mixed-integer optimal control problems (MIOCPs) transcribed into MINLPs via direct methods, showing that the presented algorithm can effectively deal with nonlinear equality constraints, a major hurdle for generic MINLP solvers."],"url":"http://arxiv.org/abs/2404.11786v1","category":"math.OC"}
{"created":"2024-04-17 22:07:41","title":"Dyakonov-Perel-like Orbital and Spin Relaxations in Centrosymmetric Systems","abstract":"The Dyakonov-Perel (DP) mechanism of spin relaxation has long been considered irrelevant in centrosymmetric systems since it was developed originally for non-centrosymmetric ones. We investigate whether this conventional understanding extends to the realm of orbital relaxation, which has recently attracted significant attention. Surprisingly, we find that orbital relaxation in centrosymmetric systems exhibits the DP-like behavior in the weak scattering regime. Moreover, the DP-like orbital relaxation can make the spin relaxation in centrosymmetric systems DP-like through the spin-orbit coupling. We also find that the DP-like orbital and spin relaxations are anisotropic even in materials with high crystal symmetry (such as face-centered cubic structure) and may depend on the orbital and spin nature of electron wavefunctions.","sentences":["The Dyakonov-Perel (DP) mechanism of spin relaxation has long been considered irrelevant in centrosymmetric systems since it was developed originally for non-centrosymmetric ones.","We investigate whether this conventional understanding extends to the realm of orbital relaxation, which has recently attracted significant attention.","Surprisingly, we find that orbital relaxation in centrosymmetric systems exhibits the DP-like behavior in the weak scattering regime.","Moreover, the DP-like orbital relaxation can make the spin relaxation in centrosymmetric systems DP-like through the spin-orbit coupling.","We also find that the DP-like orbital and spin relaxations are anisotropic even in materials with high crystal symmetry (such as face-centered cubic structure) and may depend on the orbital and spin nature of electron wavefunctions."],"url":"http://arxiv.org/abs/2404.11780v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 21:57:29","title":"A Nonlinear, Conservative, Entropic Fokker-Planck Model for Multi-Species Collisions","abstract":"A multi-species Fokker-Planck model for simulating particle collisions in a plasma is presented. The model includes various parameters that must be tuned. Under reasonable assumptions on these parameters, the model satisfies appropriate conservation laws, dissipates an entropy, and satisfies an $\\mathcal{H}$-Theorem. In addition, the model parameters provide the additional flexibility that is used to match simultaneously momentum and temperature relaxation formulas derived from the Boltzmann collision operator for a binary mixture with Coulomb potentials. A numerical method for solving the resulting space-homogeneous kinetic equation is presented and two examples are provided to demonstrate the relaxation of species bulk velocities and temperatures to their equilibrium values.","sentences":["A multi-species Fokker-Planck model for simulating particle collisions in a plasma is presented.","The model includes various parameters that must be tuned.","Under reasonable assumptions on these parameters, the model satisfies appropriate conservation laws, dissipates an entropy, and satisfies an $\\mathcal{H}$-Theorem.","In addition, the model parameters provide the additional flexibility that is used to match simultaneously momentum and temperature relaxation formulas derived from the Boltzmann collision operator for a binary mixture with Coulomb potentials.","A numerical method for solving the resulting space-homogeneous kinetic equation is presented and two examples are provided to demonstrate the relaxation of species bulk velocities and temperatures to their equilibrium values."],"url":"http://arxiv.org/abs/2404.11775v1","category":"math-ph"}
{"created":"2024-04-17 21:50:12","title":"Regret Analysis in Threshold Policy Design","abstract":"Threshold policies are targeting mechanisms that assign treatments based on whether an observable characteristic exceeds a certain threshold. They are widespread across multiple domains, such as welfare programs, taxation, and clinical medicine. This paper addresses the problem of designing threshold policies using experimental data, when the goal is to maximize the population welfare. First, I characterize the regret (a measure of policy optimality) of the Empirical Welfare Maximizer (EWM) policy, popular in the literature. Next, I introduce the Smoothed Welfare Maximizer (SWM) policy, which improves the EWM's regret convergence rate under an additional smoothness condition. The two policies are compared studying how differently their regrets depend on the population distribution, and investigating their finite sample performances through Monte Carlo simulations. In many contexts, the welfare guaranteed by the novel SWM policy is larger than with the EWM. An empirical illustration demonstrates how the treatment recommendation of the two policies may in practice notably differ.","sentences":["Threshold policies are targeting mechanisms that assign treatments based on whether an observable characteristic exceeds a certain threshold.","They are widespread across multiple domains, such as welfare programs, taxation, and clinical medicine.","This paper addresses the problem of designing threshold policies using experimental data, when the goal is to maximize the population welfare.","First, I characterize the regret (a measure of policy optimality) of the Empirical Welfare Maximizer (EWM) policy, popular in the literature.","Next, I introduce the Smoothed Welfare Maximizer (SWM) policy, which improves the EWM's regret convergence rate under an additional smoothness condition.","The two policies are compared studying how differently their regrets depend on the population distribution, and investigating their finite sample performances through Monte Carlo simulations.","In many contexts, the welfare guaranteed by the novel SWM policy is larger than with the EWM.","An empirical illustration demonstrates how the treatment recommendation of the two policies may in practice notably differ."],"url":"http://arxiv.org/abs/2404.11767v1","category":"econ.EM"}
{"created":"2024-04-17 21:47:53","title":"2D capsid formation within an oscillatory energy landscape: orderly self-assembly depends on the interplay between a dynamic potential and intrinsic relaxation times","abstract":"Multiple dissipative self-assembly protocols designed to create novel structures or to reduce kinetic traps have recently emerged. Specifically, temporal oscillations of particle interactions have been shown effective at both aims, but investigations thus far have focused on systems of simple colloids or their binary mixtures. In this work, we expand our understanding of the effect of temporally oscillating interactions to a two-dimensional coarse-grained viral capsid-like model that undergoes a self-limited assembly. This model includes multiple intrinsic relaxation times due to the internal structure of the capsid subunits and, under certain interaction regimes, proceeds via a two-step nucleation mechanism. We find that oscillations much faster than the local intrinsic relaxation times can be described via a time averaged inter-particle potential across a wide range of interaction strengths, while oscillations much slower than these relaxation times result in structures that adapt to the attraction strength of the current half-cycle. Interestingly, oscillations periods similar to these relaxation times shift the interaction window over which orderly assembly occurs by enabling error correction during the half-cycles with weaker attractions. Our results provide fundamental insights to non-equilibrium self-assembly on temporally variant energy landscapes.","sentences":["Multiple dissipative self-assembly protocols designed to create novel structures or to reduce kinetic traps have recently emerged.","Specifically, temporal oscillations of particle interactions have been shown effective at both aims, but investigations thus far have focused on systems of simple colloids or their binary mixtures.","In this work, we expand our understanding of the effect of temporally oscillating interactions to a two-dimensional coarse-grained viral capsid-like model that undergoes a self-limited assembly.","This model includes multiple intrinsic relaxation times due to the internal structure of the capsid subunits and, under certain interaction regimes, proceeds via a two-step nucleation mechanism.","We find that oscillations much faster than the local intrinsic relaxation times can be described via a time averaged inter-particle potential across a wide range of interaction strengths, while oscillations much slower than these relaxation times result in structures that adapt to the attraction strength of the current half-cycle.","Interestingly, oscillations periods similar to these relaxation times shift the interaction window over which orderly assembly occurs by enabling error correction during the half-cycles with weaker attractions.","Our results provide fundamental insights to non-equilibrium self-assembly on temporally variant energy landscapes."],"url":"http://arxiv.org/abs/2404.11765v1","category":"cond-mat.soft"}
{"created":"2024-04-17 21:47:45","title":"Multimodal 3D Object Detection on Unseen Domains","abstract":"LiDAR datasets for autonomous driving exhibit biases in properties such as point cloud density, range, and object dimensions. As a result, object detection networks trained and evaluated in different environments often experience performance degradation. Domain adaptation approaches assume access to unannotated samples from the test distribution to address this problem. However, in the real world, the exact conditions of deployment and access to samples representative of the test dataset may be unavailable while training. We argue that the more realistic and challenging formulation is to require robustness in performance to unseen target domains. We propose to address this problem in a two-pronged manner. First, we leverage paired LiDAR-image data present in most autonomous driving datasets to perform multimodal object detection. We suggest that working with multimodal features by leveraging both images and LiDAR point clouds for scene understanding tasks results in object detectors more robust to unseen domain shifts. Second, we train a 3D object detector to learn multimodal object features across different distributions and promote feature invariance across these source domains to improve generalizability to unseen target domains. To this end, we propose CLIX$^\\text{3D}$, a multimodal fusion and supervised contrastive learning framework for 3D object detection that performs alignment of object features from same-class samples of different domains while pushing the features from different classes apart. We show that CLIX$^\\text{3D}$ yields state-of-the-art domain generalization performance under multiple dataset shifts.","sentences":["LiDAR datasets for autonomous driving exhibit biases in properties such as point cloud density, range, and object dimensions.","As a result, object detection networks trained and evaluated in different environments often experience performance degradation.","Domain adaptation approaches assume access to unannotated samples from the test distribution to address this problem.","However, in the real world, the exact conditions of deployment and access to samples representative of the test dataset may be unavailable while training.","We argue that the more realistic and challenging formulation is to require robustness in performance to unseen target domains.","We propose to address this problem in a two-pronged manner.","First, we leverage paired LiDAR-image data present in most autonomous driving datasets to perform multimodal object detection.","We suggest that working with multimodal features by leveraging both images and LiDAR point clouds for scene understanding tasks results in object detectors more robust to unseen domain shifts.","Second, we train a 3D object detector to learn multimodal object features across different distributions and promote feature invariance across these source domains to improve generalizability to unseen target domains.","To this end, we propose CLIX$^\\text{3D}$, a multimodal fusion and supervised contrastive learning framework for 3D object detection that performs alignment of object features from same-class samples of different domains while pushing the features from different classes apart.","We show that CLIX$^\\text{3D}$","yields state-of-the-art domain generalization performance under multiple dataset shifts."],"url":"http://arxiv.org/abs/2404.11764v1","category":"cs.CV"}
{"created":"2024-04-17 21:27:33","title":"Language Models Still Struggle to Zero-shot Reason about Time Series","abstract":"Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains unknown whether non-trivial forecasting implies that language models can reason about time series. To address this gap, we generate a first-of-its-kind evaluation framework for time series reasoning, including formal tasks and a corresponding dataset of multi-scale time series paired with text captions across ten domains. Using these data, we probe whether language models achieve three forms of reasoning: (1) Etiological Reasoning - given an input time series, can the language model identify the scenario that most likely created it? (2) Question Answering - can a language model answer factual questions about time series? (3) Context-Aided Forecasting - does highly relevant textual context improve a language model's time series forecasts?   We find that otherwise highly-capable language models demonstrate surprisingly limited time series reasoning: they score marginally above random on etiological and question answering tasks (up to 30 percentage points worse than humans) and show modest success in using context to improve forecasting. These weakness showcase that time series reasoning is an impactful, yet deeply underdeveloped direction for language model research. We also make our datasets and code public at to support further research in this direction at https://github.com/behavioral-data/TSandLanguage","sentences":["Time series are critical for decision-making in fields like finance and healthcare.","Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets.","But it remains unknown whether non-trivial forecasting implies that language models can reason about time series.","To address this gap, we generate a first-of-its-kind evaluation framework for time series reasoning, including formal tasks and a corresponding dataset of multi-scale time series paired with text captions across ten domains.","Using these data, we probe whether language models achieve three forms of reasoning: (1) Etiological Reasoning - given an input time series, can the language model identify the scenario that most likely created it?","(2) Question Answering - can a language model answer factual questions about time series?","(3) Context-Aided Forecasting - does highly relevant textual context improve a language model's time series forecasts?   ","We find that otherwise highly-capable language models demonstrate surprisingly limited time series reasoning: they score marginally above random on etiological and question answering tasks (up to 30 percentage points worse than humans) and show modest success in using context to improve forecasting.","These weakness showcase that time series reasoning is an impactful, yet deeply underdeveloped direction for language model research.","We also make our datasets and code public at to support further research in this direction at https://github.com/behavioral-data/TSandLanguage"],"url":"http://arxiv.org/abs/2404.11757v1","category":"cs.CL"}
{"created":"2024-04-17 20:57:19","title":"Spatio-temporal patterns of diurnal temperature: a random matrix approach I-case of India","abstract":"We consider the spatio-temporal gridded daily diurnal temperature range (DTR) data across India during the 72-year period 1951--2022. We augment this data with information on the El Nino-Southern Oscillation (ENSO) and on the climatic regions (Stamp's and Koeppen's classification) and four seasons of India.   We use various matrix theory approaches to trim out strong but routine signals, random matrix theory to remove noise, and novel empirical generalised singular-value distributions to establish retention of essential signals in the trimmed data. We make use of the spatial Bergsma statistics to measure spatial association and identify temporal change points in the spatial-association.   In particular, our investigation captures a yet unknown change-point over the 72 years under study with drastic changes in spatial-association of DTR in India. It also brings out changes in spatial association with regard to ENSO.   We conclude that while studying/modelling Indian DTR data, due consideration should be granted to the strong spatial association that is being persistently exhibited over decades, and provision should be kept for potential change points in the temporal behaviour, which in turn can bring moderate to dramatic changes in the spatial association pattern.   Some of our analysis also reaffirms the conclusions made by other authors, regarding spatial and temporal behavior of DTR, adding our own insights. We consider the data from the yearly, seasonal and climatic zones points of view, and discover several new and interesting statistical structures which should be of interest, especially to climatologists and statisticians. Our methods are not country specific and could be used profitably for DTR data from other geographical areas.","sentences":["We consider the spatio-temporal gridded daily diurnal temperature range (DTR) data across India during the 72-year period 1951--2022.","We augment this data with information on the El Nino-Southern Oscillation (ENSO) and on the climatic regions (Stamp's and Koeppen's classification) and four seasons of India.   ","We use various matrix theory approaches to trim out strong but routine signals, random matrix theory to remove noise, and novel empirical generalised singular-value distributions to establish retention of essential signals in the trimmed data.","We make use of the spatial Bergsma statistics to measure spatial association and identify temporal change points in the spatial-association.   ","In particular, our investigation captures a yet unknown change-point over the 72 years under study with drastic changes in spatial-association of DTR in India.","It also brings out changes in spatial association with regard to ENSO.   ","We conclude that while studying/modelling Indian DTR data, due consideration should be granted to the strong spatial association that is being persistently exhibited over decades, and provision should be kept for potential change points in the temporal behaviour, which in turn can bring moderate to dramatic changes in the spatial association pattern.   ","Some of our analysis also reaffirms the conclusions made by other authors, regarding spatial and temporal behavior of DTR, adding our own insights.","We consider the data from the yearly, seasonal and climatic zones points of view, and discover several new and interesting statistical structures which should be of interest, especially to climatologists and statisticians.","Our methods are not country specific and could be used profitably for DTR data from other geographical areas."],"url":"http://arxiv.org/abs/2404.11747v1","category":"stat.ME"}
{"created":"2024-04-17 20:51:54","title":"On the Representation of Block Languages","abstract":"In this paper we consider block languages, namely sets of words having the same length, and we propose a new representation for these languages. In particular, given an alphabet of size $k$ and a length $\\ell$, these languages can be represented by bitmaps of size $k^\\ell$, in which each bit indicates whether the correspondent word, according to the lexicographical order, belongs to the language (bit equal to 1) or not (bit equal to 0). This representation turns out to be a good tool for the investigation of several properties of block languages, making proofs simpler and reasoning clearer. After showing how to convert bitmaps into minimal deterministic and nondeterministic finite automata, we use this representation as a tool to study the deterministic and nondeterministic state complexity of block languages, as well as the costs of basic operations on block languages, in terms of the sizes of the equivalent finite automata.","sentences":["In this paper we consider block languages, namely sets of words having the same length, and we propose a new representation for these languages.","In particular, given an alphabet of size $k$ and a length $\\ell$, these languages can be represented by bitmaps of size $k^\\ell$, in which each bit indicates whether the correspondent word, according to the lexicographical order, belongs to the language (bit equal to 1) or not (bit equal to 0).","This representation turns out to be a good tool for the investigation of several properties of block languages, making proofs simpler and reasoning clearer.","After showing how to convert bitmaps into minimal deterministic and nondeterministic finite automata, we use this representation as a tool to study the deterministic and nondeterministic state complexity of block languages, as well as the costs of basic operations on block languages, in terms of the sizes of the equivalent finite automata."],"url":"http://arxiv.org/abs/2404.11746v1","category":"cs.FL"}
{"created":"2024-04-17 20:51:01","title":"Soft Photon Heating: A Semi-Analytic Framework and Applications to $21$cm Cosmology","abstract":"The presence of an abundant population of low frequency photons at high redshifts (such as a radio background) can source leading order effects on the evolution of the matter and spin temperatures through rapid free-free absorptions. This effect, known as soft photon heating, can have a dramatic impact on the differential brightness temperature, $\\Delta T_{\\rm b}$, a central observable in $21$cm cosmology. Here, we introduce a semi-analytic framework to describe the dynamics of soft photon heating, providing a simplified set of evolution equations and a useful numerical scheme which can be used to study this generic effect. We also perform quasi-instantaneous and continuous soft photon injections to elucidate the different regimes in which soft photon heating is expected to impart a significant contribution to the global $21$cm signal and its fluctuations. We find that soft photon backgrounds produced after recombination with spectral index $\\gamma > 3.0$ undergo significant free-free absorption, and therefore this heating effect cannot be neglected. The effect becomes stronger with steeper spectral index, and in some cases the injection of a synchrotron-like spectrum ($\\gamma = 3.6$) can suppress the amplitude of $\\Delta T_{\\rm b}$ relative to the standard model prediction, making the global $21$cm signal even more difficult to detect in these scenarios.","sentences":["The presence of an abundant population of low frequency photons at high redshifts (such as a radio background) can source leading order effects on the evolution of the matter and spin temperatures through rapid free-free absorptions.","This effect, known as soft photon heating, can have a dramatic impact on the differential brightness temperature, $\\Delta T_{\\rm b}$, a central observable in $21$cm cosmology.","Here, we introduce a semi-analytic framework to describe the dynamics of soft photon heating, providing a simplified set of evolution equations and a useful numerical scheme which can be used to study this generic effect.","We also perform quasi-instantaneous and continuous soft photon injections to elucidate the different regimes in which soft photon heating is expected to impart a significant contribution to the global $21$cm signal and its fluctuations.","We find that soft photon backgrounds produced after recombination with spectral index $\\gamma > 3.0$ undergo significant free-free absorption, and therefore this heating effect cannot be neglected.","The effect becomes stronger with steeper spectral index, and in some cases the injection of a synchrotron-like spectrum ($\\gamma = 3.6$) can suppress the amplitude of $\\Delta T_{\\rm b}$ relative to the standard model prediction, making the global $21$cm signal even more difficult to detect in these scenarios."],"url":"http://arxiv.org/abs/2404.11743v1","category":"astro-ph.CO"}
{"created":"2024-04-17 20:41:49","title":"Equivariant Spatio-Temporal Self-Supervision for LiDAR Object Detection","abstract":"Popular representation learning methods encourage feature invariance under transformations applied at the input. However, in 3D perception tasks like object localization and segmentation, outputs are naturally equivariant to some transformations, such as rotation. Using pre-training loss functions that encourage equivariance of features under certain transformations provides a strong self-supervision signal while also retaining information of geometric relationships between transformed feature representations. This can enable improved performance in downstream tasks that are equivariant to such transformations. In this paper, we propose a spatio-temporal equivariant learning framework by considering both spatial and temporal augmentations jointly. Our experiments show that the best performance arises with a pre-training approach that encourages equivariance to translation, scaling, and flip, rotation and scene flow. For spatial augmentations, we find that depending on the transformation, either a contrastive objective or an equivariance-by-classification objective yields best results. To leverage real-world object deformations and motion, we consider sequential LiDAR scene pairs and develop a novel 3D scene flow-based equivariance objective that leads to improved performance overall. We show our pre-training method for 3D object detection which outperforms existing equivariant and invariant approaches in many settings.","sentences":["Popular representation learning methods encourage feature invariance under transformations applied at the input.","However, in 3D perception tasks like object localization and segmentation, outputs are naturally equivariant to some transformations, such as rotation.","Using pre-training loss functions that encourage equivariance of features under certain transformations provides a strong self-supervision signal while also retaining information of geometric relationships between transformed feature representations.","This can enable improved performance in downstream tasks that are equivariant to such transformations.","In this paper, we propose a spatio-temporal equivariant learning framework by considering both spatial and temporal augmentations jointly.","Our experiments show that the best performance arises with a pre-training approach that encourages equivariance to translation, scaling, and flip, rotation and scene flow.","For spatial augmentations, we find that depending on the transformation, either a contrastive objective or an equivariance-by-classification objective yields best results.","To leverage real-world object deformations and motion, we consider sequential LiDAR scene pairs and develop a novel 3D scene flow-based equivariance objective that leads to improved performance overall.","We show our pre-training method for 3D object detection which outperforms existing equivariant and invariant approaches in many settings."],"url":"http://arxiv.org/abs/2404.11737v1","category":"cs.CV"}
{"created":"2024-04-17 20:38:48","title":"Measuring the refractive index and thickness of multilayer samples by Fourier domain optical coherence tomography","abstract":"Non-contact measurement of the refractive index and thickness of multilayer biological tissues is of great significance for biomedical applications and can greatly improve medical diagnosis and treatment. In this work, we introduce a theoretical method to simultaneously extract the above information using a Fourier domain optical coherence tomography (FD-OCT) system, in which no additional arrangement and prior information about the object is required other than the OCT interference spectrum. The single reflection components can be extracted from the observed spectrum by isolating the primary spikes in the sample reflectance profile, and then the refractive index and thickness can be obtained by fitting the actual and modeled values of the single reflection spectrum. In a two-layer sample example, the simulation results show that our method can reconstruct the results with high accuracy. The relative error is within 0.01%. The complexity of our approach grows linearly with the number of sample layers, making it well-adapted to multilayer situations. Our method takes into account both single and multiple reflections in multilayer samples and is therefore equally applicable to samples with high refractive index contrast.","sentences":["Non-contact measurement of the refractive index and thickness of multilayer biological tissues is of great significance for biomedical applications and can greatly improve medical diagnosis and treatment.","In this work, we introduce a theoretical method to simultaneously extract the above information using a Fourier domain optical coherence tomography (FD-OCT) system, in which no additional arrangement and prior information about the object is required other than the OCT interference spectrum.","The single reflection components can be extracted from the observed spectrum by isolating the primary spikes in the sample reflectance profile, and then the refractive index and thickness can be obtained by fitting the actual and modeled values of the single reflection spectrum.","In a two-layer sample example, the simulation results show that our method can reconstruct the results with high accuracy.","The relative error is within 0.01%.","The complexity of our approach grows linearly with the number of sample layers, making it well-adapted to multilayer situations.","Our method takes into account both single and multiple reflections in multilayer samples and is therefore equally applicable to samples with high refractive index contrast."],"url":"http://arxiv.org/abs/2404.11736v1","category":"physics.optics"}
{"created":"2024-04-17 20:35:00","title":"Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach","abstract":"The emergence of attention-based transformer models has led to their extensive use in various tasks, due to their superior generalization and transfer properties. Recent research has demonstrated that such models, when prompted appropriately, are excellent for few-shot inference. However, such techniques are under-explored for dense prediction tasks like semantic segmentation. In this work, we examine the effectiveness of prompting a transformer-decoder with learned visual prompts for the generalized few-shot segmentation (GFSS) task. Our goal is to achieve strong performance not only on novel categories with limited examples, but also to retain performance on base categories. We propose an approach to learn visual prompts with limited examples. These learned visual prompts are used to prompt a multiscale transformer decoder to facilitate accurate dense predictions. Additionally, we introduce a unidirectional causal attention mechanism between the novel prompts, learned with limited examples, and the base prompts, learned with abundant data. This mechanism enriches the novel prompts without deteriorating the base class performance. Overall, this form of prompting helps us achieve state-of-the-art performance for GFSS on two different benchmark datasets: COCO-$20^i$ and Pascal-$5^i$, without the need for test-time optimization (or transduction). Furthermore, test-time optimization leveraging unlabelled test data can be used to improve the prompts, which we refer to as transductive prompt tuning.","sentences":["The emergence of attention-based transformer models has led to their extensive use in various tasks, due to their superior generalization and transfer properties.","Recent research has demonstrated that such models, when prompted appropriately, are excellent for few-shot inference.","However, such techniques are under-explored for dense prediction tasks like semantic segmentation.","In this work, we examine the effectiveness of prompting a transformer-decoder with learned visual prompts for the generalized few-shot segmentation (GFSS) task.","Our goal is to achieve strong performance not only on novel categories with limited examples, but also to retain performance on base categories.","We propose an approach to learn visual prompts with limited examples.","These learned visual prompts are used to prompt a multiscale transformer decoder to facilitate accurate dense predictions.","Additionally, we introduce a unidirectional causal attention mechanism between the novel prompts, learned with limited examples, and the base prompts, learned with abundant data.","This mechanism enriches the novel prompts without deteriorating the base class performance.","Overall, this form of prompting helps us achieve state-of-the-art performance for GFSS on two different benchmark datasets: COCO-$20^i$ and Pascal-$5^i$, without the need for test-time optimization (or transduction).","Furthermore, test-time optimization leveraging unlabelled test data can be used to improve the prompts, which we refer to as transductive prompt tuning."],"url":"http://arxiv.org/abs/2404.11732v1","category":"cs.CV"}
{"created":"2024-04-17 20:34:41","title":"A Learning-to-Rank Formulation of Clustering-Based Approximate Nearest Neighbor Search","abstract":"A critical piece of the modern information retrieval puzzle is approximate nearest neighbor search. Its objective is to return a set of $k$ data points that are closest to a query point, with its accuracy measured by the proportion of exact nearest neighbors captured in the returned set. One popular approach to this question is clustering: The indexing algorithm partitions data points into non-overlapping subsets and represents each partition by a point such as its centroid. The query processing algorithm first identifies the nearest clusters -- a process known as routing -- then performs a nearest neighbor search over those clusters only. In this work, we make a simple observation: The routing function solves a ranking problem. Its quality can therefore be assessed with a ranking metric, making the function amenable to learning-to-rank. Interestingly, ground-truth is often freely available: Given a query distribution in a top-$k$ configuration, the ground-truth is the set of clusters that contain the exact top-$k$ vectors. We develop this insight and apply it to Maximum Inner Product Search (MIPS). As we demonstrate empirically on various datasets, learning a simple linear function consistently improves the accuracy of clustering-based MIPS.","sentences":["A critical piece of the modern information retrieval puzzle is approximate nearest neighbor search.","Its objective is to return a set of $k$ data points that are closest to a query point, with its accuracy measured by the proportion of exact nearest neighbors captured in the returned set.","One popular approach to this question is clustering: The indexing algorithm partitions data points into non-overlapping subsets and represents each partition by a point such as its centroid.","The query processing algorithm first identifies the nearest clusters -- a process known as routing -- then performs a nearest neighbor search over those clusters only.","In this work, we make a simple observation: The routing function solves a ranking problem.","Its quality can therefore be assessed with a ranking metric, making the function amenable to learning-to-rank.","Interestingly, ground-truth is often freely available: Given a query distribution in a top-$k$ configuration, the ground-truth is the set of clusters that contain the exact top-$k$ vectors.","We develop this insight and apply it to Maximum Inner Product Search (MIPS).","As we demonstrate empirically on various datasets, learning a simple linear function consistently improves the accuracy of clustering-based MIPS."],"url":"http://arxiv.org/abs/2404.11731v1","category":"cs.IR"}
{"created":"2024-04-17 20:24:41","title":"Investigating Gender Bias in Turkish Language Models","abstract":"Language models are trained mostly on Web data, which often contains social stereotypes and biases that the models can inherit. This has potentially negative consequences, as models can amplify these biases in downstream tasks or applications. However, prior research has primarily focused on the English language, especially in the context of gender bias. In particular, grammatically gender-neutral languages such as Turkish are underexplored despite representing different linguistic properties to language models with possibly different effects on biases. In this paper, we fill this research gap and investigate the significance of gender bias in Turkish language models. We build upon existing bias evaluation frameworks and extend them to the Turkish language by translating existing English tests and creating new ones designed to measure gender bias in the context of T\\\"urkiye. Specifically, we also evaluate Turkish language models for their embedded ethnic bias toward Kurdish people. Based on the experimental results, we attribute possible biases to different model characteristics such as the model size, their multilingualism, and the training corpora. We make the Turkish gender bias dataset publicly available.","sentences":["Language models are trained mostly on Web data, which often contains social stereotypes and biases that the models can inherit.","This has potentially negative consequences, as models can amplify these biases in downstream tasks or applications.","However, prior research has primarily focused on the English language, especially in the context of gender bias.","In particular, grammatically gender-neutral languages such as Turkish are underexplored despite representing different linguistic properties to language models with possibly different effects on biases.","In this paper, we fill this research gap and investigate the significance of gender bias in Turkish language models.","We build upon existing bias evaluation frameworks and extend them to the Turkish language by translating existing English tests and creating new ones designed to measure gender bias in the context of T\\\"urkiye.","Specifically, we also evaluate Turkish language models for their embedded ethnic bias toward Kurdish people.","Based on the experimental results, we attribute possible biases to different model characteristics such as the model size, their multilingualism, and the training corpora.","We make the Turkish gender bias dataset publicly available."],"url":"http://arxiv.org/abs/2404.11726v1","category":"cs.CL"}
{"created":"2024-04-17 20:19:48","title":"Beyond the Bid-Ask: Strategic Insights into Spread Prediction and the Global Mid-Price Phenomenon","abstract":"This study introduces novel concepts in the analysis of limit order books (LOBs) with a focus on unveiling strategic insights into spread prediction and understanding the global mid-price (GMP) phenomenon. We define and analyze the total market order book bid--ask spread (TMOBBAS) and GMP, showcasing their significance in providing a deeper understanding of market dynamics beyond traditional LOB models. Employing high-frequency data, we comprehensively examine these concepts through various methodological lenses, including tail behavior analysis, dynamics of log-returns, and risk--return performance evaluation. Our findings reveal the intricate behavior of TMOBBAS and GMP under different market conditions, offering new perspectives on the liquidity, volatility, and efficiency of markets. This paper not only contributes to the academic discourse on financial markets but also presents practical implications for traders, risk managers, and policymakers seeking to navigate the complexities of modern financial systems.","sentences":["This study introduces novel concepts in the analysis of limit order books (LOBs) with a focus on unveiling strategic insights into spread prediction and understanding the global mid-price (GMP) phenomenon.","We define and analyze the total market order book bid--ask spread (TMOBBAS) and GMP, showcasing their significance in providing a deeper understanding of market dynamics beyond traditional LOB models.","Employing high-frequency data, we comprehensively examine these concepts through various methodological lenses, including tail behavior analysis, dynamics of log-returns, and risk--return performance evaluation.","Our findings reveal the intricate behavior of TMOBBAS and GMP under different market conditions, offering new perspectives on the liquidity, volatility, and efficiency of markets.","This paper not only contributes to the academic discourse on financial markets but also presents practical implications for traders, risk managers, and policymakers seeking to navigate the complexities of modern financial systems."],"url":"http://arxiv.org/abs/2404.11722v1","category":"q-fin.TR"}
{"created":"2024-04-17 20:12:21","title":"Linear and nonlinear filtering for a two-layer quasi-geostrophic ocean model","abstract":"Although the two-layer quasi-geostrophic equations (2QGE) are a simplified model for the dynamics of a stratified, wind-driven ocean, their numerical simulation is still plagued by the need for high resolution to capture the full spectrum of turbulent scales. Since such high resolution would lead to unreasonable computational times, it is typical to resort to coarse low-resolution meshes combined with the so-called eddy viscosity parameterization to account for the diffusion mechanisms that are not captured due to mesh under-resolution. We propose to enable the use of further coarsened meshes by adding a (linear or nonlinear) differential low-pass to the 2QGE, without changing the eddy viscosity coefficient. While the linear filter introduces constant (additional) artificial viscosity everywhere in the domain, the nonlinear filter relies on an indicator function to determine where and how much artificial viscosity is needed. Through several numerical results for a double-gyre wind forcing benchmark, we show that with the nonlinear filter we obtain accurate results with very coarse meshes, thereby drastically reducing the computational time (speed up ranging from 30 to 300).","sentences":["Although the two-layer quasi-geostrophic equations (2QGE) are a simplified model for the dynamics of a stratified, wind-driven ocean, their numerical simulation is still plagued by the need for high resolution to capture the full spectrum of turbulent scales.","Since such high resolution would lead to unreasonable computational times, it is typical to resort to coarse low-resolution meshes combined with the so-called eddy viscosity parameterization to account for the diffusion mechanisms that are not captured due to mesh under-resolution.","We propose to enable the use of further coarsened meshes by adding a (linear or nonlinear) differential low-pass to the 2QGE, without changing the eddy viscosity coefficient.","While the linear filter introduces constant (additional) artificial viscosity everywhere in the domain, the nonlinear filter relies on an indicator function to determine where and how much artificial viscosity is needed.","Through several numerical results for a double-gyre wind forcing benchmark, we show that with the nonlinear filter we obtain accurate results with very coarse meshes, thereby drastically reducing the computational time (speed up ranging from 30 to 300)."],"url":"http://arxiv.org/abs/2404.11718v1","category":"math.NA"}
{"created":"2024-04-17 19:04:56","title":"Nonequilibrium finite frequency resonances in differential quantum noise driven by Majorana interference","abstract":"Nonequilibrium quantum noise $S^>(\\omega,V)$ measured at finite frequencies $\\omega$ and bias voltages $V$ probes Majorana bound states in a host nanostructure via fluctuation fingerprints unavailable in average currents or static shot noise. When Majorana interference is brought into play, it enriches nonequilibrium states and makes their nature even more unique. Here we demonstrate that an interference of two Majorana modes via a nonequilibrium quantum dot gives rise to a remarkable finite frequency response of the differential quantum noise $\\partial S^>(\\omega,V,\\Delta\\phi)/\\partial V$ driven by the Majorana phase difference $\\Delta\\phi$. Specifically, at low bias voltages there develops a narrow resonance of width $\\hbar\\Delta\\omega\\sim\\sin^2\\Delta\\phi$ at a finite frequency determined by $V$, whereas for high bias voltages there arise two antiresonances at two finite frequencies controlled by both $V$ and $\\Delta\\phi$. We show that the maximum and minimum of these resonance and antiresonances have universal fractional values, $3e^3/4h$ and $-e^3/4h$. Moreover, detecting the frequencies of the antiresonances provides a potential tool to measure $\\Delta\\phi$ in nonequilibrium experiments on Majorana finite frequency quantum noise.","sentences":["Nonequilibrium quantum noise $S^>(\\omega,V)$ measured at finite frequencies $\\omega$ and bias voltages $V$ probes Majorana bound states in a host nanostructure via fluctuation fingerprints unavailable in average currents or static shot noise.","When Majorana interference is brought into play, it enriches nonequilibrium states and makes their nature even more unique.","Here we demonstrate that an interference of two Majorana modes via a nonequilibrium quantum dot gives rise to a remarkable finite frequency response of the differential quantum noise $\\partial S^>(\\omega,V,\\Delta\\phi)/\\partial V$ driven by the Majorana phase difference $\\Delta\\phi$. Specifically, at low bias voltages there develops a narrow resonance of width $\\hbar\\Delta\\omega\\sim\\sin^2\\Delta\\phi$ at a finite frequency determined by $V$, whereas for high bias voltages there arise two antiresonances at two finite frequencies controlled by both $V$ and $\\Delta\\phi$. We show that the maximum and minimum of these resonance and antiresonances have universal fractional values, $3e^3/4h$ and $-e^3/4h$. Moreover, detecting the frequencies of the antiresonances provides a potential tool to measure $\\Delta\\phi$ in nonequilibrium experiments on Majorana finite frequency quantum noise."],"url":"http://arxiv.org/abs/2404.11702v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 19:04:54","title":"Axial, Planar-Diagonal, Body-Diagonal Fields on the Cubic-Spin Spin Glass in d=3: A Plethora of Ordered Phases under Finite Fields","abstract":"A nematic phase, previously seen in the d=3 classical Heisenberg spin-glass system, occurs in the n-component cubic-spin spin-glass system, between the low-temperature spin-glass phase and the high-temperature disordered phase, for number of spin components n>=3, in spatial dimension d=3, thus constituting a liquid-crystal phase in a dirty (quenched-disordered) magnet. Furthermore, under application of a variety of uniform magnetic fields, a veritable plethora of phases are found. Under uniform magnetic fields, 15 different phases and two spin-glass phase diagram topologies, qualitatively different from the conventional spin-glass phase diagram topology, are seen. The chaotic rescaling behaviors and their Lyapunov exponents are calculated in each of these spin-glass phase diagram topologies. These results are obtained from renormalization-group calculations that are exact on the hierarchical lattice and, equivalently, approximate on the hypercubic spatial lattice. Axial, planar-diagonal, or body-diagonal finite-strength uniform fields are applied to n=2 and 3 component cubic-spin spin-glass systems in d=3.","sentences":["A nematic phase, previously seen in the d=3 classical Heisenberg spin-glass system, occurs in the n-component cubic-spin spin-glass system, between the low-temperature spin-glass phase and the high-temperature disordered phase, for number of spin components n>=3, in spatial dimension d=3, thus constituting a liquid-crystal phase in a dirty (quenched-disordered) magnet.","Furthermore, under application of a variety of uniform magnetic fields, a veritable plethora of phases are found.","Under uniform magnetic fields, 15 different phases and two spin-glass phase diagram topologies, qualitatively different from the conventional spin-glass phase diagram topology, are seen.","The chaotic rescaling behaviors and their Lyapunov exponents are calculated in each of these spin-glass phase diagram topologies.","These results are obtained from renormalization-group calculations that are exact on the hierarchical lattice and, equivalently, approximate on the hypercubic spatial lattice.","Axial, planar-diagonal, or body-diagonal finite-strength uniform fields are applied to n=2 and 3 component cubic-spin spin-glass systems in d=3."],"url":"http://arxiv.org/abs/2404.11701v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-17 18:36:13","title":"Probing the intergalactic medium during the Epoch of Reionization using 21-cm signal power spectra","abstract":"The redshifted 21-cm signal from the epoch of reionization (EoR) directly probes the ionization and thermal states of the intergalactic medium during that period. In particular, the distribution of the ionized regions around the radiating sources during EoR introduces scale-dependent features in the spherically-averaged EoR 21-cm signal power spectrum. The goal is to study these scale-dependent features at different stages of reionization using numerical simulations and build a source model-independent framework to probe the properties of the intergalactic medium using EoR 21-cm signal power spectrum measurements. Under the assumption of high spin temperature, we modelled the redshift evolution of the ratio of EoR 21-cm brightness temperature power spectrum and the corresponding density power spectrum using an ansatz consisting of a set of redshift and scale-independent parameters. This set of eight parameters probes the redshift evolution of the average ionization fraction and the quantities related to the morphology of the ionized regions. We have tested this ansatz on different reionization scenarios generated using different simulation algorithms and found that it is able to recover the redshift evolution of the average neutral fraction within an absolute deviation $\\lesssim 0.1$. Our framework allows us to interpret 21-cm signal power spectra in terms of parameters related to the state of the IGM. This source model-independent framework can efficiently constrain reionization scenarios using multi-redshift power spectrum measurements with ongoing and future radio telescopes such as LOFAR, MWA, HERA, and SKA. This will add independent information regarding the EoR IGM properties.","sentences":["The redshifted 21-cm signal from the epoch of reionization (EoR) directly probes the ionization and thermal states of the intergalactic medium during that period.","In particular, the distribution of the ionized regions around the radiating sources during EoR introduces scale-dependent features in the spherically-averaged EoR 21-cm signal power spectrum.","The goal is to study these scale-dependent features at different stages of reionization using numerical simulations and build a source model-independent framework to probe the properties of the intergalactic medium using EoR 21-cm signal power spectrum measurements.","Under the assumption of high spin temperature, we modelled the redshift evolution of the ratio of EoR 21-cm brightness temperature power spectrum and the corresponding density power spectrum using an ansatz consisting of a set of redshift and scale-independent parameters.","This set of eight parameters probes the redshift evolution of the average ionization fraction and the quantities related to the morphology of the ionized regions.","We have tested this ansatz on different reionization scenarios generated using different simulation algorithms and found that it is able to recover the redshift evolution of the average neutral fraction within an absolute deviation $\\lesssim 0.1$.","Our framework allows us to interpret 21-cm signal power spectra in terms of parameters related to the state of the IGM.","This source model-independent framework can efficiently constrain reionization scenarios using multi-redshift power spectrum measurements with ongoing and future radio telescopes such as LOFAR, MWA, HERA, and SKA.","This will add independent information regarding the EoR IGM properties."],"url":"http://arxiv.org/abs/2404.11686v1","category":"astro-ph.CO"}
{"created":"2024-04-17 18:35:20","title":"Photon blockade in non-Hermitian optomechanical systems with nonreciprocal couplings","abstract":"We study the photon blockade at exceptional points for a non-Hermitian optomechanical system coupled to the driven whispering-gallery-mode microresonator with two nanoparticles under the weak optomechanical coupling approximation, where exceptional points emerge periodically by controlling the relative angle of the nanoparticles. We find that conventional photon blockade occurs at exceptional points for the eigenenergy resonance of the single-excitation subspace driven by a laser field, and discuss the physical origin of conventional photon blockade. Under the weak driving condition, we analyze the influences of the different parameters on conventional photon blockade. We investigate conventional photon blockade at non-exceptional points, which exists at two optimal detunings due to the eigenstates in the single-excitation subspace splitting from one (coalescence) at exceptional points to two at non-exceptional points. \\textbf{Unconventional photon blockade can occur at non-exceptional points, while it does not exist at exceptional points since the destructive quantum interference cannot occur due to the two different quantum pathways to the two-photon state being not formed.} The realization of photon blockade in our proposal provides a viable and flexible way for the preparation of single-photon sources in the non-Hermitian optomechanical system.","sentences":["We study the photon blockade at exceptional points for a non-Hermitian optomechanical system coupled to the driven whispering-gallery-mode microresonator with two nanoparticles under the weak optomechanical coupling approximation, where exceptional points emerge periodically by controlling the relative angle of the nanoparticles.","We find that conventional photon blockade occurs at exceptional points for the eigenenergy resonance of the single-excitation subspace driven by a laser field, and discuss the physical origin of conventional photon blockade.","Under the weak driving condition, we analyze the influences of the different parameters on conventional photon blockade.","We investigate conventional photon blockade at non-exceptional points, which exists at two optimal detunings due to the eigenstates in the single-excitation subspace splitting from one (coalescence) at exceptional points to two at non-exceptional points.","\\textbf{Unconventional photon blockade can occur at non-exceptional points, while it does not exist at exceptional points since the destructive quantum interference cannot occur due to the two different quantum pathways to the two-photon state being not formed.}","The realization of photon blockade in our proposal provides a viable and flexible way for the preparation of single-photon sources in the non-Hermitian optomechanical system."],"url":"http://arxiv.org/abs/2404.11685v1","category":"quant-ph"}
{"created":"2024-04-17 18:29:32","title":"Unifying Scene Representation and Hand-Eye Calibration with 3D Foundation Models","abstract":"Representing the environment is a central challenge in robotics, and is essential for effective decision-making. Traditionally, before capturing images with a manipulator-mounted camera, users need to calibrate the camera using a specific external marker, such as a checkerboard or AprilTag. However, recent advances in computer vision have led to the development of \\emph{3D foundation models}. These are large, pre-trained neural networks that can establish fast and accurate multi-view correspondences with very few images, even in the absence of rich visual features. This paper advocates for the integration of 3D foundation models into scene representation approaches for robotic systems equipped with manipulator-mounted RGB cameras. Specifically, we propose the Joint Calibration and Representation (JCR) method. JCR uses RGB images, captured by a manipulator-mounted camera, to simultaneously construct an environmental representation and calibrate the camera relative to the robot's end-effector, in the absence of specific calibration markers. The resulting 3D environment representation is aligned with the robot's coordinate frame and maintains physically accurate scales. We demonstrate that JCR can build effective scene representations using a low-cost RGB camera attached to a manipulator, without prior calibration.","sentences":["Representing the environment is a central challenge in robotics, and is essential for effective decision-making.","Traditionally, before capturing images with a manipulator-mounted camera, users need to calibrate the camera using a specific external marker, such as a checkerboard or AprilTag.","However, recent advances in computer vision have led to the development of \\emph{3D foundation models}.","These are large, pre-trained neural networks that can establish fast and accurate multi-view correspondences with very few images, even in the absence of rich visual features.","This paper advocates for the integration of 3D foundation models into scene representation approaches for robotic systems equipped with manipulator-mounted RGB cameras.","Specifically, we propose the Joint Calibration and Representation (JCR) method.","JCR uses RGB images, captured by a manipulator-mounted camera, to simultaneously construct an environmental representation and calibrate the camera relative to the robot's end-effector, in the absence of specific calibration markers.","The resulting 3D environment representation is aligned with the robot's coordinate frame and maintains physically accurate scales.","We demonstrate that JCR can build effective scene representations using a low-cost RGB camera attached to a manipulator, without prior calibration."],"url":"http://arxiv.org/abs/2404.11683v1","category":"cs.RO"}
{"created":"2024-04-17 18:27:59","title":"How Well Can You Articulate that Idea? Insights from Automated Formative Assessment","abstract":"Automated methods are becoming increasingly integrated into studies of formative feedback on students' science explanation writing. Most of this work, however, addresses students' responses to short answer questions. We investigate automated feedback on students' science explanation essays, where students must articulate multiple ideas. Feedback is based on a rubric that identifies the main ideas students are prompted to include in explanatory essays about the physics of energy and mass, given their experiments with a simulated roller coaster. We have found that students generally improve on revised versions of their essays. Here, however, we focus on two factors that affect the accuracy of the automated feedback. First, we find that the main ideas in the rubric differ with respect to how much freedom they afford in explanations of the idea, thus explanation of a natural law is relatively constrained. Students have more freedom in how they explain complex relations they observe in their roller coasters, such as transfer of different forms of energy. Second, by tracing the automated decision process, we can diagnose when a student's statement lacks sufficient clarity for the automated tool to associate it more strongly with one of the main ideas above all others. This in turn provides an opportunity for teachers and peers to help students reflect on how to state their ideas more clearly.","sentences":["Automated methods are becoming increasingly integrated into studies of formative feedback on students' science explanation writing.","Most of this work, however, addresses students' responses to short answer questions.","We investigate automated feedback on students' science explanation essays, where students must articulate multiple ideas.","Feedback is based on a rubric that identifies the main ideas students are prompted to include in explanatory essays about the physics of energy and mass, given their experiments with a simulated roller coaster.","We have found that students generally improve on revised versions of their essays.","Here, however, we focus on two factors that affect the accuracy of the automated feedback.","First, we find that the main ideas in the rubric differ with respect to how much freedom they afford in explanations of the idea, thus explanation of a natural law is relatively constrained.","Students have more freedom in how they explain complex relations they observe in their roller coasters, such as transfer of different forms of energy.","Second, by tracing the automated decision process, we can diagnose when a student's statement lacks sufficient clarity for the automated tool to associate it more strongly with one of the main ideas above all others.","This in turn provides an opportunity for teachers and peers to help students reflect on how to state their ideas more clearly."],"url":"http://arxiv.org/abs/2404.11682v1","category":"cs.CL"}
{"created":"2024-04-17 18:20:24","title":"Quantitative metric density and connectivity for sets of positive measure","abstract":"We show that in doubling, geodesic metric measure spaces (including, for example, Euclidean space), sets of positive measure have a certain large-scale metric density property. As an application, we prove that a set of positive measure in the unit cube of $\\mathbb{R}^d$ can be decomposed into a controlled number of subsets that are \"well-connected\" within the original set, along with a \"garbage set\" of arbitrarily small measure. Our results are quantitative, i.e., they provide bounds independent of the particular set under consideration.","sentences":["We show that in doubling, geodesic metric measure spaces (including, for example, Euclidean space), sets of positive measure have a certain large-scale metric density property.","As an application, we prove that a set of positive measure in the unit cube of $\\mathbb{R}^d$ can be decomposed into a controlled number of subsets that are \"well-connected\" within the original set, along with a \"garbage set\" of arbitrarily small measure.","Our results are quantitative, i.e., they provide bounds independent of the particular set under consideration."],"url":"http://arxiv.org/abs/2404.11679v1","category":"math.CA"}
{"created":"2024-04-17 18:19:26","title":"Corrected Correlation Estimates for Meta-Analysis","abstract":"Meta-analysis allows rigorous aggregation of estimates and uncertainty across multiple studies. When a given study reports multiple estimates, such as log odds ratios (ORs) or log relative risks (RRs) across exposure groups, accounting for within-study correlations improves accuracy and efficiency of meta-analytic results. Canonical approaches of Greenland-Longnecker and Hamling estimate pseudo cases and non-cases for exposure groups to obtain within-study correlations. However, currently available implementations for both methods fail on simple examples.   We review both GL and Hamling methods through the lens of optimization. For ORs, we provide modifications of each approach that ensure convergence for any feasible inputs. For GL, this is achieved through a new connection to entropic minimization. For Hamling, a modification leads to a provably solvable equivalent set of equations given a specific initialization. For each, we provide implementations a guaranteed to work for any feasible input.   For RRs, we show the new GL approach is always guaranteed to succeed, but any Hamling approach may fail: we give counter-examples where no solutions exist. We derive a sufficient condition on reported RRs that guarantees success when reported variances are all equal.","sentences":["Meta-analysis allows rigorous aggregation of estimates and uncertainty across multiple studies.","When a given study reports multiple estimates, such as log odds ratios (ORs) or log relative risks (RRs) across exposure groups, accounting for within-study correlations improves accuracy and efficiency of meta-analytic results.","Canonical approaches of Greenland-Longnecker and Hamling estimate pseudo cases and non-cases for exposure groups to obtain within-study correlations.","However, currently available implementations for both methods fail on simple examples.   ","We review both GL and Hamling methods through the lens of optimization.","For ORs, we provide modifications of each approach that ensure convergence for any feasible inputs.","For GL, this is achieved through a new connection to entropic minimization.","For Hamling, a modification leads to a provably solvable equivalent set of equations given a specific initialization.","For each, we provide implementations a guaranteed to work for any feasible input.   ","For RRs, we show the new GL approach is always guaranteed to succeed, but any Hamling approach may fail: we give counter-examples where no solutions exist.","We derive a sufficient condition on reported RRs that guarantees success when reported variances are all equal."],"url":"http://arxiv.org/abs/2404.11678v1","category":"stat.ME"}
{"created":"2024-04-17 18:16:02","title":"Hairpin Completion Distance Lower Bound","abstract":"Hairpin completion, derived from the hairpin formation observed in DNA biochemistry, is an operation applied to strings, particularly useful in DNA computing. Conceptually, a right hairpin completion operation transforms a string $S$ into $S\\cdot S'$ where $S'$ is the reverse complement of a prefix of $S$. Similarly, a left hairpin completion operation transforms a string $S$ into $S'\\cdot S$ where $S'$ is the reverse complement of a suffix of $S$. The hairpin completion distance from $S$ to $T$ is the minimum number of hairpin completion operations needed to transform $S$ into $T$. Recently Boneh et al. showed an $O(n^2)$ time algorithm for finding the hairpin completion distance between two strings of length at most $n$. In this paper we show that for any $\\varepsilon>0$ there is no $O(n^{2-\\varepsilon})$-time algorithm for the hairpin completion distance problem unless the Strong Exponential Time Hypothesis (SETH) is false. Thus, under SETH, the time complexity of the hairpin completion distance problem is quadratic, up to sub-polynomial factors.","sentences":["Hairpin completion, derived from the hairpin formation observed in DNA biochemistry, is an operation applied to strings, particularly useful in DNA computing.","Conceptually, a right hairpin completion operation transforms a string $S$ into $S\\cdot S'$ where $S'$ is the reverse complement of a prefix of $S$. Similarly, a left hairpin completion operation transforms a string $S$ into $S'\\cdot S$ where $S'$ is the reverse complement of a suffix of $S$. The hairpin completion distance from $S$ to $T$ is the minimum number of hairpin completion operations needed to transform $S$ into $T$. Recently Boneh et al. showed an $O(n^2)$ time algorithm for finding the hairpin completion distance between two strings of length at most $n$. In this paper we show that for any $\\varepsilon>0$ there is no $O(n^{2-\\varepsilon})$-time algorithm for the hairpin completion distance problem unless the Strong Exponential Time Hypothesis (SETH) is false.","Thus, under SETH, the time complexity of the hairpin completion distance problem is quadratic, up to sub-polynomial factors."],"url":"http://arxiv.org/abs/2404.11673v1","category":"cs.DS"}
{"created":"2024-04-17 18:13:16","title":"MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory","abstract":"While current large language models (LLMs) demonstrate some capabilities in knowledge-intensive tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with infrequent knowledge and temporal degradation. In addition, the uninterpretable nature of parametric memorization makes it challenging to understand and prevent hallucination. Parametric memory pools and model editing are only partial solutions. Retrieval Augmented Generation (RAG) $\\unicode{x2013}$ though non-parametric $\\unicode{x2013}$ has its own limitations: it lacks structure, complicates interpretability and makes it hard to effectively manage stored knowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation.","sentences":["While current large language models (LLMs) demonstrate some capabilities in knowledge-intensive tasks, they are limited by relying on their parameters as an implicit storage mechanism.","As a result, they struggle with infrequent knowledge and temporal degradation.","In addition, the uninterpretable nature of parametric memorization makes it challenging to understand and prevent hallucination.","Parametric memory pools and model editing are only partial solutions.","Retrieval Augmented Generation (RAG) $\\unicode{x2013}$ though non-parametric $\\unicode{x2013}$ has its own limitations: it lacks structure, complicates interpretability and makes it hard to effectively manage stored knowledge.","In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module.","MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge.","Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular.","We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation."],"url":"http://arxiv.org/abs/2404.11672v1","category":"cs.CL"}
{"created":"2024-04-17 18:08:00","title":"Factorized Motion Fields for Fast Sparse Input Dynamic View Synthesis","abstract":"Designing a 3D representation of a dynamic scene for fast optimization and rendering is a challenging task. While recent explicit representations enable fast learning and rendering of dynamic radiance fields, they require a dense set of input viewpoints. In this work, we focus on learning a fast representation for dynamic radiance fields with sparse input viewpoints. However, the optimization with sparse input is under-constrained and necessitates the use of motion priors to constrain the learning. Existing fast dynamic scene models do not explicitly model the motion, making them difficult to be constrained with motion priors. We design an explicit motion model as a factorized 4D representation that is fast and can exploit the spatio-temporal correlation of the motion field. We then introduce reliable flow priors including a combination of sparse flow priors across cameras and dense flow priors within cameras to regularize our motion model. Our model is fast, compact and achieves very good performance on popular multi-view dynamic scene datasets with sparse input viewpoints. The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html.","sentences":["Designing a 3D representation of a dynamic scene for fast optimization and rendering is a challenging task.","While recent explicit representations enable fast learning and rendering of dynamic radiance fields, they require a dense set of input viewpoints.","In this work, we focus on learning a fast representation for dynamic radiance fields with sparse input viewpoints.","However, the optimization with sparse input is under-constrained and necessitates the use of motion priors to constrain the learning.","Existing fast dynamic scene models do not explicitly model the motion, making them difficult to be constrained with motion priors.","We design an explicit motion model as a factorized 4D representation that is fast and can exploit the spatio-temporal correlation of the motion field.","We then introduce reliable flow priors including a combination of sparse flow priors across cameras and dense flow priors within cameras to regularize our motion model.","Our model is fast, compact and achieves very good performance on popular multi-view dynamic scene datasets with sparse input viewpoints.","The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html."],"url":"http://arxiv.org/abs/2404.11669v1","category":"cs.CV"}
{"created":"2024-04-17 18:05:25","title":"Joint r-Process Enrichment by Supernovae with a Metallicity Threshold and Neutron Star Mergers","abstract":"The enrichment history of $r$-process elements has been imprinted on the stellar abundances that change in accordance with increasing metallicity in galaxies. Close examination of the [Eu/Fe] feature caused by stars in nearby galaxies, including the Large Magellanic Cloud (LMC), shows its perplexity. The decreasing trend of the [Eu/Fe] feature is followed by a nearly constant value; this trend is generally attributed to an onset of the delayed Fe release from type Ia supernovae (SNe Ia), which is the same interpretation of the [$\\alpha$/Fe] feature. However, this feature appears in the LMC at [Fe/H] of approximately -0.7, which is significantly higher than that for the [alpha/Fe] case ($\\approx$ -2). This result potentially indicates the presence of an overlooked property of the $r$-process site that remains unseen in the study of the Milky Way. Here, we propose that this [Eu/Fe]-knee feature is created by a fade-out of core-collapse SNe producing $r$-process elements; these elements along with neutron star mergers (NSMs) promote the $r$-process enrichment under the condition for this specific SNe such that their occurrence is limited to a low-metallicity environment. This metallicity threshold for the occurrence rate of $r$-process SNe at a subsolar is nearly identical to that for long gamma-ray bursts whose origin may be connected to fast-rotating massive stars. Moreover, we reason that the contribution of Eu from NSMs is crucial to maintain a high [Eu/Fe] at an early stage in dwarf galaxies by a balance with Fe from SNe Ia; both enrichments via NSMs and SNe Ia proceed with similar delay time distributions.","sentences":["The enrichment history of $r$-process elements has been imprinted on the stellar abundances that change in accordance with increasing metallicity in galaxies.","Close examination of the [Eu/Fe] feature caused by stars in nearby galaxies, including the Large Magellanic Cloud (LMC), shows its perplexity.","The decreasing trend of the [Eu/Fe] feature is followed by a nearly constant value; this trend is generally attributed to an onset of the delayed Fe release from type Ia supernovae (SNe Ia), which is the same interpretation of the [$\\alpha$/Fe] feature.","However, this feature appears in the LMC at [Fe/H] of approximately -0.7, which is significantly higher than that for the [alpha/Fe] case ($\\approx$ -2).","This result potentially indicates the presence of an overlooked property of the $r$-process site that remains unseen in the study of the Milky Way.","Here, we propose that this [Eu/Fe]-knee feature is created by a fade-out of core-collapse SNe producing $r$-process elements; these elements along with neutron star mergers (NSMs) promote the $r$-process enrichment under the condition for this specific SNe such that their occurrence is limited to a low-metallicity environment.","This metallicity threshold for the occurrence rate of $r$-process SNe at a subsolar is nearly identical to that for long gamma-ray bursts whose origin may be connected to fast-rotating massive stars.","Moreover, we reason that the contribution of Eu from NSMs is crucial to maintain a high [Eu/Fe] at an early stage in dwarf galaxies by a balance with Fe from SNe Ia; both enrichments via NSMs and SNe Ia proceed with similar delay time distributions."],"url":"http://arxiv.org/abs/2404.11668v1","category":"astro-ph.GA"}
{"created":"2024-04-17 18:04:11","title":"Searching for Free-Floating Planets with TESS: I. Discovery of a First Terrestrial-Mass Candidate","abstract":"Though free-floating planets (FFPs) that have been ejected from their natal star systems may outpopulate their bound counterparts in the terrestrial-mass range, they remain one of the least explored exoplanet demographics. Due to their negligible electromagnetic emission at all wavelengths, the only observational technique able to detect these worlds is gravitational microlensing. Microlensing by terrestrial-mass FFPs induces rare, short-duration magnifications of background stars, requiring high-cadence, wide-field surveys to detect these events. The Transiting Exoplanet Survey Satellite (TESS), though designed to detect close-bound exoplanets via the transit technique, boasts a cadence as short as 200 seconds and has monitored hundreds of millions of stars, making it well-suited to search for short-duration microlensing events as well. We have used existing data products from the TESS Quick-Look Pipeline (QLP) to perform a preliminary search for FFP microlensing candidates in 1.3 million light curves from TESS Sector 61. We find one compelling candidate associated with TIC-107150013, a source star at $d_s = 3.194$ kpc. The event has a duration $t_E = 0.074^{+0.002}_{-0.002}$ days and shows prominent finite-source features ($\\rho = 4.55^{+0.08}_{-0.07}$), making it consistent with an FFP in the terrestrial-mass range. This exciting result indicates that our ongoing search through all TESS sectors has the opportunity to shed new light on this enigmatic population of worlds.","sentences":["Though free-floating planets (FFPs) that have been ejected from their natal star systems may outpopulate their bound counterparts in the terrestrial-mass range, they remain one of the least explored exoplanet demographics.","Due to their negligible electromagnetic emission at all wavelengths, the only observational technique able to detect these worlds is gravitational microlensing.","Microlensing by terrestrial-mass FFPs induces rare, short-duration magnifications of background stars, requiring high-cadence, wide-field surveys to detect these events.","The Transiting Exoplanet Survey Satellite (TESS), though designed to detect close-bound exoplanets via the transit technique, boasts a cadence as short as 200 seconds and has monitored hundreds of millions of stars, making it well-suited to search for short-duration microlensing events as well.","We have used existing data products from the TESS Quick-Look Pipeline (QLP) to perform a preliminary search for FFP microlensing candidates in 1.3 million light curves from TESS Sector 61.","We find one compelling candidate associated with TIC-107150013, a source star at $d_s = 3.194$ kpc.","The event has a duration $t_E = 0.074^{+0.002}_{-0.002}$ days and shows prominent finite-source features ($\\rho = 4.55^{+0.08}_{-0.07}$), making it consistent with an FFP in the terrestrial-mass range.","This exciting result indicates that our ongoing search through all TESS sectors has the opportunity to shed new light on this enigmatic population of worlds."],"url":"http://arxiv.org/abs/2404.11666v1","category":"astro-ph.EP"}
{"created":"2024-04-17 18:02:13","title":"Tunneling away the relic neutrino asymmetry","abstract":"The Earth acts as a matter potential for relic neutrinos which modifies their index of refraction from vacuum by $\\delta\\sim10^{-8}$. It has been argued that the refractive effects from this potential should lead to a large $\\mathcal O(\\sqrt\\delta)$ neutrino-antineutrino asymmetry at the surface of the Earth. This result was computed by treating the Earth as flat. In this work, we revisit this calculation in the context of a perfectly spherical Earth. We demonstrate, both numerically and through analytic arguments, that the flat-Earth result is only recovered under the condition $\\delta^{3/2}kR\\gg1$, where $k$ is the typical momentum of the relic neutrinos and $R$ is the radius of the Earth. This condition is required to prevent antineutrinos from tunneling into classically inaccessible trajectories below the Earth's surface and washing away the large asymmetry. As the physical parameters of the Earth do not satisfy this condition, we find that the asymmetry at the surface should only be $\\mathcal O(\\delta)$. While the asphericity of the Earth may serve as a loophole to our conclusions, we argue that it is still difficult to generate a large asymmetry even in the presence of local terrain.","sentences":["The Earth acts as a matter potential for relic neutrinos which modifies their index of refraction from vacuum by $\\delta\\sim10^{-8}$. It has been argued that the refractive effects from this potential should lead to a large $\\mathcal O(\\sqrt\\delta)$ neutrino-antineutrino asymmetry at the surface of the Earth.","This result was computed by treating the Earth as flat.","In this work, we revisit this calculation in the context of a perfectly spherical Earth.","We demonstrate, both numerically and through analytic arguments, that the flat-Earth result is only recovered under the condition $\\delta^{3/2}kR\\gg1$, where $k$ is the typical momentum of the relic neutrinos and $R$ is the radius of the Earth.","This condition is required to prevent antineutrinos from tunneling into classically inaccessible trajectories below the Earth's surface and washing away the large asymmetry.","As the physical parameters of the Earth do not satisfy this condition, we find that the asymmetry at the surface should only be $\\mathcal O(\\delta)$.","While the asphericity of the Earth may serve as a loophole to our conclusions, we argue that it is still difficult to generate a large asymmetry even in the presence of local terrain."],"url":"http://arxiv.org/abs/2404.11664v1","category":"hep-ph"}
{"created":"2024-04-17 18:01:03","title":"Modelling stochastic and quasi-periodic behaviour in stellar time-series: Gaussian process regression versus power-spectrum fitting","abstract":"As the hunt for an Earth-like exoplanets has intensified in recent years, so has the effort to characterise and model the stellar signals that can hide or mimic small planetary signals. Stellar variability arises from a number of sources, including granulation, supergranulation, oscillations and activity, all of which result in quasi-periodic or stochastic behaviour in photometric and/or radial velocity observations. Traditionally, the characterisation of these signals has mostly been done in the frequency domain. However, the recent development of scalable Gaussian process regression methods makes direct time-domain modelling of stochastic processes a feasible and arguably preferable alternative, obviating the need to estimate the power spectral density of the data before modelling it. In this paper, we compare the two approaches using a series of experiments on simulated data. We show that frequency domain modelling can lead to inaccurate results, especially when the time sampling is irregular. By contrast, Gaussian process regression results are often more precise, and systematically more accurate, in both the regular and irregular time sampling regimes. While this work was motivated by the analysis of radial velocity and photometry observations of main sequence stars in the context of planet searches, we note that our results may also have applications for the study of other types of astrophysical variability such as quasi-periodic oscillations in X-ray binaries and active galactic nuclei variability.","sentences":["As the hunt for an Earth-like exoplanets has intensified in recent years, so has the effort to characterise and model the stellar signals that can hide or mimic small planetary signals.","Stellar variability arises from a number of sources, including granulation, supergranulation, oscillations and activity, all of which result in quasi-periodic or stochastic behaviour in photometric and/or radial velocity observations.","Traditionally, the characterisation of these signals has mostly been done in the frequency domain.","However, the recent development of scalable Gaussian process regression methods makes direct time-domain modelling of stochastic processes a feasible and arguably preferable alternative, obviating the need to estimate the power spectral density of the data before modelling it.","In this paper, we compare the two approaches using a series of experiments on simulated data.","We show that frequency domain modelling can lead to inaccurate results, especially when the time sampling is irregular.","By contrast, Gaussian process regression results are often more precise, and systematically more accurate, in both the regular and irregular time sampling regimes.","While this work was motivated by the analysis of radial velocity and photometry observations of main sequence stars in the context of planet searches, we note that our results may also have applications for the study of other types of astrophysical variability such as quasi-periodic oscillations in X-ray binaries and active galactic nuclei variability."],"url":"http://arxiv.org/abs/2404.11662v1","category":"astro-ph.SR"}
{"created":"2024-04-17 18:00:34","title":"Constraints on Symmetry Preserving Gapped Phases from Coupling Constant Anomalies","abstract":"In this note, we will characterize constraints on the possible IR phases of a given QFT by anomalies in the space of coupling constants. We will give conditions under which a coupling constant anomaly cannot be matched by a continuous family of symmetry preserving gapped phases, in which case the theory is either gapless, or exhibits spontaneous symmetry breaking or a phase transition. We additionally demonstrate examples of theories with coupling constant anomalies which can be matched by a family of symmetry preserving gapped phases without a phase transition and comment on the interpretation of our results for the spontaneous breaking of \"$(-1)$-form global symmetries.\"","sentences":["In this note, we will characterize constraints on the possible IR phases of a given QFT by anomalies in the space of coupling constants.","We will give conditions under which a coupling constant anomaly cannot be matched by a continuous family of symmetry preserving gapped phases, in which case the theory is either gapless, or exhibits spontaneous symmetry breaking or a phase transition.","We additionally demonstrate examples of theories with coupling constant anomalies which can be matched by a family of symmetry preserving gapped phases without a phase transition and comment on the interpretation of our results for the spontaneous breaking of \"$(-1)$-form global symmetries.\""],"url":"http://arxiv.org/abs/2404.11660v1","category":"hep-th"}
{"created":"2024-04-17 18:00:06","title":"Phase sensitive information from a planar Josephson junction","abstract":"We analyze both the general symmetry-related and more microscopic considerations that govern the Josephson tunneling across a finite planar junction between a known $s$-wave superconductor and a candidate unconventional superconductor (e.g., $d_{x^2-y^2}$-wave). Due to the finite size of the probe, the Josephson current possesses an edge contribution, which is shown to be the dominant contribution under certain conditions. Thus, the dependence of the edge contribution on the geometry of the junction can serve as a direct probe of the symmetry of the order parameter in the unconventional superconductor.","sentences":["We analyze both the general symmetry-related and more microscopic considerations that govern the Josephson tunneling across a finite planar junction between a known $s$-wave superconductor and a candidate unconventional superconductor (e.g., $d_{x^2-y^2}$-wave).","Due to the finite size of the probe, the Josephson current possesses an edge contribution, which is shown to be the dominant contribution under certain conditions.","Thus, the dependence of the edge contribution on the geometry of the junction can serve as a direct probe of the symmetry of the order parameter in the unconventional superconductor."],"url":"http://arxiv.org/abs/2404.11657v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 18:00:01","title":"Pions from higher-dimensional gluons: general realizations and stringy models","abstract":"In this paper we revisit the general phenomenon that scattering amplitudes of pions can be obtained from \"dimensional reduction\" of gluons in higher dimensions in a more general context. We show that such \"dimensional reduction\" operations universally turn gluons into pions regardless of details of interactions: under such operations any amplitude that is gauge invariant and contains only local simple poles becomes one that satisfies Adler zero in the soft limit. As two such examples, we show that starting from gluon amplitudes in both superstring and bosonic string theories, the operations produce \"stringy\" completion of pion scattering amplitudes to all orders in $\\alpha'$, with leading order given by non-linear sigma model amplitudes. Via Kawai-Lewellen-Tye relations, they give closed-stringy completion for Born-Infeld theory and the special Galileon theory, which are directly related to gravity amplitudes in closed-string theories. We also discuss how they naturally produce stringy models for mixed amplitudes of pions and colored scalars.","sentences":["In this paper we revisit the general phenomenon that scattering amplitudes of pions can be obtained from \"dimensional reduction\" of gluons in higher dimensions in a more general context.","We show that such \"dimensional reduction\" operations universally turn gluons into pions regardless of details of interactions: under such operations any amplitude that is gauge invariant and contains only local simple poles becomes one that satisfies Adler zero in the soft limit.","As two such examples, we show that starting from gluon amplitudes in both superstring and bosonic string theories, the operations produce \"stringy\" completion of pion scattering amplitudes to all orders in $\\alpha'$, with leading order given by non-linear sigma model amplitudes.","Via Kawai-Lewellen-Tye relations, they give closed-stringy completion for Born-Infeld theory and the special Galileon theory, which are directly related to gravity amplitudes in closed-string theories.","We also discuss how they naturally produce stringy models for mixed amplitudes of pions and colored scalars."],"url":"http://arxiv.org/abs/2404.11648v1","category":"hep-th"}
{"created":"2024-04-17 18:00:01","title":"Stark Many-Body Localisation Under Periodic Driving","abstract":"We study stability of localisation under periodic driving in many-body Stark systems. We find that localisation is stable except near special resonant frequencies, where resonances cause delocalisation. We provide approximate analytical arguments and numerical evidence in support of these results. This shows that disorder-free broken ergodicity is stable to driving, opening up the way to studying nonequilibrium driven physics in a novel setting.","sentences":["We study stability of localisation under periodic driving in many-body Stark systems.","We find that localisation is stable except near special resonant frequencies, where resonances cause delocalisation.","We provide approximate analytical arguments and numerical evidence in support of these results.","This shows that disorder-free broken ergodicity is stable to driving, opening up the way to studying nonequilibrium driven physics in a novel setting."],"url":"http://arxiv.org/abs/2404.11649v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-17 18:00:00","title":"Quantized Acoustoelectric Floquet Effect in Quantum Nanowires","abstract":"External coherent fields can drive quantum materials into non-equilibrium states, revealing exotic properties that are unattainable under equilibrium conditions -- an approach known as ``Floquet engineering.'' While optical lasers have commonly been used as the driving fields, recent advancements have introduced nontraditional sources, such as coherent phonon drives. Building on this progress, we demonstrate that driving a metallic quantum nanowire with a coherent wave of terahertz phonons can induce an electronic steady state characterized by a persistent quantized current along the wire. The quantization of the current is achieved due to the coupling of electrons to the nanowire's vibrational modes, providing the low-temperature heat bath and energy relaxation mechanisms. Our findings underscore the potential of using non-optical drives, such as coherent phonon sources, to induce non-equilibrium phenomena in materials. Furthermore, our approach suggests a new method for the high-precision detection of coherent phonon oscillations via transport measurements.","sentences":["External coherent fields can drive quantum materials into non-equilibrium states, revealing exotic properties that are unattainable under equilibrium conditions -- an approach known as ``Floquet engineering.''","While optical lasers have commonly been used as the driving fields, recent advancements have introduced nontraditional sources, such as coherent phonon drives.","Building on this progress, we demonstrate that driving a metallic quantum nanowire with a coherent wave of terahertz phonons can induce an electronic steady state characterized by a persistent quantized current along the wire.","The quantization of the current is achieved due to the coupling of electrons to the nanowire's vibrational modes, providing the low-temperature heat bath and energy relaxation mechanisms.","Our findings underscore the potential of using non-optical drives, such as coherent phonon sources, to induce non-equilibrium phenomena in materials.","Furthermore, our approach suggests a new method for the high-precision detection of coherent phonon oscillations via transport measurements."],"url":"http://arxiv.org/abs/2404.11647v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 17:59:53","title":"InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior","abstract":"3D Gaussians have recently emerged as an efficient representation for novel view synthesis. This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering. Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions. To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image. Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior. Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios. We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion.","sentences":["3D Gaussians have recently emerged as an efficient representation for novel view synthesis.","This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering.","Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions.","To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image.","Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior.","Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios.","We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion."],"url":"http://arxiv.org/abs/2404.11613v1","category":"cs.CV"}
{"created":"2024-04-17 17:59:38","title":"Sublinear transport in Kagome metals: Interplay of Dirac cones and Van Hove singularities","abstract":"Kagome metals are known to host Dirac fermions and saddle point Van Hove singularities near Fermi level. With the minimal two-pocket model (Dirac cone + Van Hove singularity), we propose a semiclassical theory to explain the experimentally observed sublinear resistivity in Ni$_3$In and other Kagome metals. We derive the full semiclassical description of kinetic phenomena using Boltzmann equation, and demonstrate that internode electron-electron interaction leads to sublinear in $T$ scaling for both electrical and thermal transport at low temperatures. At higher temperatures above the Dirac node chemical potential, thermal and electric current dissipate through distinct scattering channels, making a ground for Wiedemann-Franz law violation.","sentences":["Kagome metals are known to host Dirac fermions and saddle point Van Hove singularities near Fermi level.","With the minimal two-pocket model (Dirac cone + Van Hove singularity), we propose a semiclassical theory to explain the experimentally observed sublinear resistivity in Ni$_3$In and other Kagome metals.","We derive the full semiclassical description of kinetic phenomena using Boltzmann equation, and demonstrate that internode electron-electron interaction leads to sublinear in $T$ scaling for both electrical and thermal transport at low temperatures.","At higher temperatures above the Dirac node chemical potential, thermal and electric current dissipate through distinct scattering channels, making a ground for Wiedemann-Franz law violation."],"url":"http://arxiv.org/abs/2404.11612v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 17:51:15","title":"Interaction Techniques for Exploratory Data Visualization on Mobile Devices","abstract":"The ubiquity and on-the-go availability of mobile devices makes them central to many tasks such as interpersonal communication and media consumption. However, despite the potential of mobile devices for on-demand exploratory data visualization, existing mobile interactions are difficult, often using highly custom interactions, complex gestures, or multi-modal input. We synthesize limitations from the literature and outline four motivating principles for improved mobile interaction: leverage ubiquitous modalities, prioritize discoverability, enable rapid in-context data exploration, and promote graceful recovery. We then contribute thirteen interaction candidates and conduct a formative study with twelve participants who experienced our interactions in a testbed prototype. Based on these interviews, we discuss design considerations and tradeoffs from four main themes: precise and rapid inspection, focused navigation, single-touch and fixed orientation interaction, and judicious use of motion.","sentences":["The ubiquity and on-the-go availability of mobile devices makes them central to many tasks such as interpersonal communication and media consumption.","However, despite the potential of mobile devices for on-demand exploratory data visualization, existing mobile interactions are difficult, often using highly custom interactions, complex gestures, or multi-modal input.","We synthesize limitations from the literature and outline four motivating principles for improved mobile interaction: leverage ubiquitous modalities, prioritize discoverability, enable rapid in-context data exploration, and promote graceful recovery.","We then contribute thirteen interaction candidates and conduct a formative study with twelve participants who experienced our interactions in a testbed prototype.","Based on these interviews, we discuss design considerations and tradeoffs from four main themes: precise and rapid inspection, focused navigation, single-touch and fixed orientation interaction, and judicious use of motion."],"url":"http://arxiv.org/abs/2404.11602v1","category":"cs.HC"}
{"created":"2024-04-17 17:50:24","title":"Variational Bayesian Last Layers","abstract":"We introduce a deterministic variational formulation for training Bayesian last layer neural networks. This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation. Our variational Bayesian last layer (VBLL) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures. We experimentally investigate VBLLs, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification. Finally, we investigate combining VBLL layers with variational Bayesian feature learning, yielding a lower variance collapsed variational inference method for Bayesian neural networks.","sentences":["We introduce a deterministic variational formulation for training Bayesian last layer neural networks.","This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation.","Our variational Bayesian last layer (VBLL) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures.","We experimentally investigate VBLLs, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification.","Finally, we investigate combining VBLL layers with variational Bayesian feature learning, yielding a lower variance collapsed variational inference method for Bayesian neural networks."],"url":"http://arxiv.org/abs/2404.11599v1","category":"cs.LG"}
{"created":"2024-04-17 17:43:23","title":"Real Time Evolvable Hardware for Optimal Reconfiguration of Cusp-Like Pulse Shapers","abstract":"The design of a cusp-like digital pulse shaper for particle energy measurements requires the definition of four parameters whose values are defined based on the nature of the shaper input signal (timing, noise, ...) provided by a sensor. However, after high doses of radiation, sensors degenerate and their output signals do not meet the original characteristics, which may lead to erroneous measurements of the particle energies. We present in this paper an evolvable cusp-like digital shaper, which is able to auto-recalibrate the original hardware implementation into a new design that match the original specifications under the new sensor features.","sentences":["The design of a cusp-like digital pulse shaper for particle energy measurements requires the definition of four parameters whose values are defined based on the nature of the shaper input signal (timing, noise, ...) provided by a sensor.","However, after high doses of radiation, sensors degenerate and their output signals do not meet the original characteristics, which may lead to erroneous measurements of the particle energies.","We present in this paper an evolvable cusp-like digital shaper, which is able to auto-recalibrate the original hardware implementation into a new design that match the original specifications under the new sensor features."],"url":"http://arxiv.org/abs/2404.11592v1","category":"cs.AR"}
{"created":"2024-04-17 17:39:59","title":"A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion","abstract":"We present the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers. STE is a fusion of the Tyler's M-estimator (TME) and a variant of the fast median subspace. Our theoretical analysis suggests that, under a common inlier-outlier model, STE can effectively recover the underlying subspace, even when it contains a smaller fraction of inliers relative to other methods in the field of robust subspace recovery. We apply STE in the context of Structure from Motion (SfM) in two ways: for robust estimation of the fundamental matrix and for the removal of outlying cameras, enhancing the robustness of the SfM pipeline. Numerical experiments confirm the state-of-the-art performance of our method in these applications. This research makes significant contributions to the field of robust subspace recovery, particularly in the context of computer vision and 3D reconstruction.","sentences":["We present the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers.","STE is a fusion of the Tyler's M-estimator (TME) and a variant of the fast median subspace.","Our theoretical analysis suggests that, under a common inlier-outlier model, STE can effectively recover the underlying subspace, even when it contains a smaller fraction of inliers relative to other methods in the field of robust subspace recovery.","We apply STE in the context of Structure from Motion (SfM) in two ways: for robust estimation of the fundamental matrix and for the removal of outlying cameras, enhancing the robustness of the SfM pipeline.","Numerical experiments confirm the state-of-the-art performance of our method in these applications.","This research makes significant contributions to the field of robust subspace recovery, particularly in the context of computer vision and 3D reconstruction."],"url":"http://arxiv.org/abs/2404.11590v1","category":"cs.CV"}
{"created":"2024-04-17 17:28:30","title":"Maximin Shares in Hereditary Set Systems","abstract":"We consider the problem of fairly allocating a set of indivisible items under the criteria of the maximin share guarantee. Specifically, we study approximation of maximin share allocations under hereditary set system valuations, in which each valuation function is based on the independent sets of an underlying hereditary set systems. Using a lone divider approach, we show the existence of $1/2$-approximate MMS allocations, improving on the $11/30$ guarantee of Li and Vetta. Moreover, we prove that ($2/3 + \\epsilon$)-approximate MMS allocations do not always exist in this model for every $\\epsilon > 0$, an improvement from the recent $3/4 + \\epsilon$ result of Li and Deng. Our existence proof is constructive, but does not directly yield a polynomial-time approximation algorithm. However, we show that a $2/5$-approximate MMS allocation can be found in polynomial time, given valuation oracles. Finally, we show that our existence and approximation results transfer to a variety of problems within constrained fair allocation, improving on existing results in some of these settings.","sentences":["We consider the problem of fairly allocating a set of indivisible items under the criteria of the maximin share guarantee.","Specifically, we study approximation of maximin share allocations under hereditary set system valuations, in which each valuation function is based on the independent sets of an underlying hereditary set systems.","Using a lone divider approach, we show the existence of $1/2$-approximate MMS allocations, improving on the $11/30$ guarantee of Li and Vetta.","Moreover, we prove that ($2/3 + \\epsilon$)-approximate MMS allocations do not always exist in this model for every $\\epsilon > 0$, an improvement from the recent $3/4 + \\epsilon$ result of Li and Deng.","Our existence proof is constructive, but does not directly yield a polynomial-time approximation algorithm.","However, we show that a $2/5$-approximate MMS allocation can be found in polynomial time, given valuation oracles.","Finally, we show that our existence and approximation results transfer to a variety of problems within constrained fair allocation, improving on existing results in some of these settings."],"url":"http://arxiv.org/abs/2404.11582v1","category":"cs.GT"}
{"created":"2024-04-17 17:25:25","title":"Spatial Heterogeneous Additive Partial Linear Model: A Joint Approach of Bivariate Spline and Forest Lasso","abstract":"Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields. In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging. In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model. Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect. On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain. On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern. Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data. Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches. To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States.","sentences":["Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields.","In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging.","In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model.","Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect.","On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain.","On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern.","Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data.","Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches.","To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States."],"url":"http://arxiv.org/abs/2404.11579v1","category":"stat.ME"}
{"created":"2024-04-18 16:11:16","title":"Debiased Distribution Compression","abstract":"Modern compression methods can summarize a target distribution $\\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein Kernel Thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with $\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb {P}$. For larger-scale compression tasks, Low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein Recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\\operatorname{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.","sentences":["Modern compression methods can summarize a target distribution $\\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences.","Given $n$ points targeting the wrong distribution and quadratic time, Stein Kernel Thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with $\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb {P}$. For larger-scale compression tasks, Low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest.","For downstream tasks that support simplex or constant-preserving weights, Stein Recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\\operatorname{poly-log}(n)$ weighted points.","Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces.","In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering."],"url":"http://arxiv.org/abs/2404.12290v1","category":"stat.ML"}
{"created":"2024-04-18 13:18:51","title":"No pattern formation in a quasilinear chemotaxis model with local sensing","abstract":"Convergence to spatially homogeneous steady states is shown for a chemotaxis model with local sensing and possibly nonlinear diffusion when the intrinsic diffusion rate $\\phi$ dominates the inverse of the chemotactic motility function $\\gamma$, in the sense that $(\\phi\\gamma)'\\ge 0$. This result encompasses and complies with the analysis and numerical simulations performed in Choi \\& Kim (2023). The proof involves two steps: first, a Liapunov functional is constructed when $\\phi\\gamma$ is non-decreasing. The convergence proof relies on a detailed study of the dissipation of the Liapunov functional and requires additional technical assumptions on $\\phi$ and $\\gamma$.","sentences":["Convergence to spatially homogeneous steady states is shown for a chemotaxis model with local sensing and possibly nonlinear diffusion when the intrinsic diffusion rate $\\phi$ dominates the inverse of the chemotactic motility function $\\gamma$, in the sense that $(\\phi\\gamma)'\\ge 0$.","This result encompasses and complies with the analysis and numerical simulations performed in Choi \\& Kim (2023).","The proof involves two steps: first, a Liapunov functional is constructed when $\\phi\\gamma$ is non-decreasing.","The convergence proof relies on a detailed study of the dissipation of the Liapunov functional and requires additional technical assumptions on $\\phi$ and $\\gamma$."],"url":"http://arxiv.org/abs/2404.12166v1","category":"math.AP"}
{"created":"2024-04-18 10:10:56","title":"Improving the perception of visual fiducial markers in the field using Adaptive Active Exposure Control","abstract":"Accurate localization is fundamental for autonomous underwater vehicles (AUVs) to carry out precise tasks, such as manipulation and construction. Vision-based solutions using fiducial marker are promising, but extremely challenging underwater because of harsh lighting condition underwater. This paper introduces a gradient-based active camera exposure control method to tackle sharp lighting variations during image acquisition, which can establish better foundation for subsequent image enhancement procedures. Considering a typical scenario for underwater operations where visual tags are used, we proposed several experiments comparing our method with other state-of-the-art exposure control method including Active Exposure Control (AEC) and Gradient-based Exposure Control (GEC). Results show a significant improvement in the accuracy of robot localization. This method is an important component that can be used in visual-based state estimation pipeline to improve the overall localization accuracy.","sentences":["Accurate localization is fundamental for autonomous underwater vehicles (AUVs) to carry out precise tasks, such as manipulation and construction.","Vision-based solutions using fiducial marker are promising, but extremely challenging underwater because of harsh lighting condition underwater.","This paper introduces a gradient-based active camera exposure control method to tackle sharp lighting variations during image acquisition, which can establish better foundation for subsequent image enhancement procedures.","Considering a typical scenario for underwater operations where visual tags are used, we proposed several experiments comparing our method with other state-of-the-art exposure control method including Active Exposure Control (AEC) and Gradient-based Exposure Control (GEC).","Results show a significant improvement in the accuracy of robot localization.","This method is an important component that can be used in visual-based state estimation pipeline to improve the overall localization accuracy."],"url":"http://arxiv.org/abs/2404.12055v1","category":"cs.CV"}
{"created":"2024-04-18 07:12:56","title":"Neuropsychological Effects of Rock Steady Boxing in Patients with Parkinson's Disease: A Comprehensive Analysis","abstract":"This study investigates the efficacy of adapted boxing, specifically Rock Steady Boxing, in mitigating dopamine decline in individuals with Parkinson disease. The research involved 40 participants with confirmed diagnosis of Parkinson disease who underwent biweekly RSB sessions over an 8 week period. Training regimen included activation, core exercise, and a cooldown phase. The findings revealed a significant amelioration in depressive symptoms through the sessions. Assessment using the Beck Depression Inventory demonstrated a progressive decrease in scores associated with depressive symptoms, particularly affective, cognitive, and somatic symptoms. The reduction in more severe symptoms was accompanied by an increase in milder symptoms. Statistical analysis confirmed the significance of the reduction in depressive symptoms over time, suggesting that physical activity, particularly RSB, may contribute to enhancing the quality of life for individuals with Parkinson disease. The positive impact was observed in both motor and depressive symptoms, suggesting an overall beneficial effect of exercise training. It is important to note that six participants withdrew from the study due to organizational reasons, resulting in a reduction in the participant count from 40 to 34. Nonetheless, the overall results suggest that RSB could be an effective approach to addressing depression in Parkinson patients, providing a complementary treatment option to conventional pharmacological therapy.","sentences":["This study investigates the efficacy of adapted boxing, specifically Rock Steady Boxing, in mitigating dopamine decline in individuals with Parkinson disease.","The research involved 40 participants with confirmed diagnosis of Parkinson disease who underwent biweekly RSB sessions over an 8 week period.","Training regimen included activation, core exercise, and a cooldown phase.","The findings revealed a significant amelioration in depressive symptoms through the sessions.","Assessment using the Beck Depression Inventory demonstrated a progressive decrease in scores associated with depressive symptoms, particularly affective, cognitive, and somatic symptoms.","The reduction in more severe symptoms was accompanied by an increase in milder symptoms.","Statistical analysis confirmed the significance of the reduction in depressive symptoms over time, suggesting that physical activity, particularly RSB, may contribute to enhancing the quality of life for individuals with Parkinson disease.","The positive impact was observed in both motor and depressive symptoms, suggesting an overall beneficial effect of exercise training.","It is important to note that six participants withdrew from the study due to organizational reasons, resulting in a reduction in the participant count from 40 to 34.","Nonetheless, the overall results suggest that RSB could be an effective approach to addressing depression in Parkinson patients, providing a complementary treatment option to conventional pharmacological therapy."],"url":"http://arxiv.org/abs/2404.11951v1","category":"q-bio.NC"}
{"created":"2024-04-18 05:41:47","title":"Functional renormalization group for p=2 like glassy matrices in the planar approximation: III. Equilibrium dynamics and beyond","abstract":"This paper is the last of the series investigating renormalization group aspects of stochastic random matrices, including a Wigner-like disorder. We consider the equilibrium dynamics formalism that can be merged with the Ward identities arising from the large N effective kinetics. We construct a regulator that does not break time-reversal symmetry and show that the resulting flow equations reduce to the equilibrium flow built in our previous works. Finally, we investigate the flow equations beyond the equilibrium dynamics assumption and study the stability of the perturbation around the fluctuation-dissipation theorem.","sentences":["This paper is the last of the series investigating renormalization group aspects of stochastic random matrices, including a Wigner-like disorder.","We consider the equilibrium dynamics formalism that can be merged with the Ward identities arising from the large N effective kinetics.","We construct a regulator that does not break time-reversal symmetry and show that the resulting flow equations reduce to the equilibrium flow built in our previous works.","Finally, we investigate the flow equations beyond the equilibrium dynamics assumption and study the stability of the perturbation around the fluctuation-dissipation theorem."],"url":"http://arxiv.org/abs/2404.11915v1","category":"hep-th"}
{"created":"2024-04-18 03:31:40","title":"Transition threshold for the 2-D Couette flow in whole space via Green's function","abstract":"In this paper, we investigate the transition threshold problem concerning the 2-D Navier-Stokes equations in the context of Couette flow $(y,0)$ at high Reynolds number $Re$ in whole space. By utilizing Green's function estimates for the linearized equations around Couette flow, we initially establish refined dissipation estimates for the linearized Navier-Stokes equations with a precise decay rate $(1+t)^{-1}.$ As an application, we prove that if the initial perturbation of vorticity satisfies$$\\|\\omega_{0}\\|_{H^{1}\\cap L^1}\\leq c_0\\nu^{\\frac{3}{4}}$$ for some small constant $c_0$ independent of the viscosity $\\nu$, then we can reach the conclusion that the solution remains within $O\\left( \\nu ^{\\frac{3}{4}}\\right) $ of the Couette flow.","sentences":["In this paper, we investigate the transition threshold problem concerning the 2-D Navier-Stokes equations in the context of Couette flow $(y,0)$ at high Reynolds number $Re$ in whole space.","By utilizing Green's function estimates for the linearized equations around Couette flow, we initially establish refined dissipation estimates for the linearized Navier-Stokes equations with a precise decay rate $(1+t)^{-1}.$ As an application, we prove that if the initial perturbation of vorticity satisfies$$\\|\\omega_{0}\\|_{H^{1}\\cap L^1}\\leq c_0\\nu^{\\frac{3}{4}}$$ for some small constant $c_0$ independent of the viscosity $\\nu$, then we can reach the conclusion that the solution remains within $O\\left( \\nu ^{\\frac{3}{4}}\\right) $ of the Couette flow."],"url":"http://arxiv.org/abs/2404.11878v1","category":"math.AP"}
{"created":"2024-04-18 02:30:59","title":"Graph Neural Networks for Wireless Networks: Graph Representation, Architecture and Evaluation","abstract":"Graph neural networks (GNNs) have been regarded as the basic model to facilitate deep learning (DL) to revolutionize resource allocation in wireless networks. GNN-based models are shown to be able to learn the structural information about graphs representing the wireless networks to adapt to the time-varying channel state information and dynamics of network topology. This article aims to provide a comprehensive overview of applying GNNs to optimize wireless networks via answering three fundamental questions, i.e., how to input the wireless network data into GNNs, how to improve the performance of GNNs, and how to evaluate GNNs. Particularly, two graph representations are given to transform wireless network parameters into graph-structured data. Then, we focus on the architecture design of the GNN-based models via introducing the basic message passing as well as model improvement methods including multi-head attention mechanism and residual structure. At last, we give task-oriented evaluation metrics for DL-enabled wireless resource allocation. We also highlight certain challenges and potential research directions for the application of GNNs in wireless networks.","sentences":["Graph neural networks (GNNs) have been regarded as the basic model to facilitate deep learning (DL) to revolutionize resource allocation in wireless networks.","GNN-based models are shown to be able to learn the structural information about graphs representing the wireless networks to adapt to the time-varying channel state information and dynamics of network topology.","This article aims to provide a comprehensive overview of applying GNNs to optimize wireless networks via answering three fundamental questions, i.e., how to input the wireless network data into GNNs, how to improve the performance of GNNs, and how to evaluate GNNs.","Particularly, two graph representations are given to transform wireless network parameters into graph-structured data.","Then, we focus on the architecture design of the GNN-based models via introducing the basic message passing as well as model improvement methods including multi-head attention mechanism and residual structure.","At last, we give task-oriented evaluation metrics for DL-enabled wireless resource allocation.","We also highlight certain challenges and potential research directions for the application of GNNs in wireless networks."],"url":"http://arxiv.org/abs/2404.11858v1","category":"eess.SP"}
{"created":"2024-04-18 01:56:37","title":"Multiphoton super-resolution imaging via virtual structured illumination","abstract":"Fluorescence imaging in thick biological tissues is challenging due to sample-induced aberration and scattering, which leads to severe degradation of image quality and resolution. Fluorescence imaging in reflection geometry further exacerbates this issue since the point spread function is distorted in both excitation and emission pathways. Here, we propose a novel approach termed adaptive optics virtual structured illumination microscopy (AO V-SIM) that enables super-resolution multiphoton imaging through a scattering medium in reflection geometry. Our approach exploits the incoherent reflection matrix obtained using a conventional point-scanning fluorescence microscope with an array detector. We introduce V-SIM super-resolution reconstruction algorithm based on the incoherent reflection matrix. Furthermore, we introduce a software adaptive optics correction algorithm, AO V-SIM, which recovers unattenuated and phase-corrected optical transfer function for both excitation and emission pathways. The effectiveness of our proposed method is experimentally validated through sub-diffraction-limited two-photon fluorescence imaging of various samples in the presence of strong aberration.","sentences":["Fluorescence imaging in thick biological tissues is challenging due to sample-induced aberration and scattering, which leads to severe degradation of image quality and resolution.","Fluorescence imaging in reflection geometry further exacerbates this issue since the point spread function is distorted in both excitation and emission pathways.","Here, we propose a novel approach termed adaptive optics virtual structured illumination microscopy (AO V-SIM) that enables super-resolution multiphoton imaging through a scattering medium in reflection geometry.","Our approach exploits the incoherent reflection matrix obtained using a conventional point-scanning fluorescence microscope with an array detector.","We introduce V-SIM super-resolution reconstruction algorithm based on the incoherent reflection matrix.","Furthermore, we introduce a software adaptive optics correction algorithm, AO V-SIM, which recovers unattenuated and phase-corrected optical transfer function for both excitation and emission pathways.","The effectiveness of our proposed method is experimentally validated through sub-diffraction-limited two-photon fluorescence imaging of various samples in the presence of strong aberration."],"url":"http://arxiv.org/abs/2404.11849v1","category":"physics.optics"}
{"created":"2024-04-18 01:55:44","title":"Partial Large Kernel CNNs for Efficient Super-Resolution","abstract":"Recently, in the super-resolution (SR) domain, transformers have outperformed CNNs with fewer FLOPs and fewer parameters since they can deal with long-range dependency and adaptively adjust weights based on instance. In this paper, we demonstrate that CNNs, although less focused on in the current SR domain, surpass Transformers in direct efficiency measures. By incorporating the advantages of Transformers into CNNs, we aim to achieve both computational efficiency and enhanced performance. However, using a large kernel in the SR domain, which mainly processes large images, incurs a large computational overhead. To overcome this, we propose novel approaches to employing the large kernel, which can reduce latency by 86\\% compared to the naive large kernel, and leverage an Element-wise Attention module to imitate instance-dependent weights. As a result, we introduce Partial Large Kernel CNNs for Efficient Super-Resolution (PLKSR), which achieves state-of-the-art performance on four datasets at a scale of $\\times$4, with reductions of 68.1\\% in latency and 80.2\\% in maximum GPU memory occupancy compared to SRFormer-light.","sentences":["Recently, in the super-resolution (SR) domain, transformers have outperformed CNNs with fewer FLOPs and fewer parameters since they can deal with long-range dependency and adaptively adjust weights based on instance.","In this paper, we demonstrate that CNNs, although less focused on in the current SR domain, surpass Transformers in direct efficiency measures.","By incorporating the advantages of Transformers into CNNs, we aim to achieve both computational efficiency and enhanced performance.","However, using a large kernel in the SR domain, which mainly processes large images, incurs a large computational overhead.","To overcome this, we propose novel approaches to employing the large kernel, which can reduce latency by 86\\% compared to the naive large kernel, and leverage an Element-wise Attention module to imitate instance-dependent weights.","As a result, we introduce Partial Large Kernel CNNs for Efficient Super-Resolution (PLKSR), which achieves state-of-the-art performance on four datasets at a scale of $\\times$4, with reductions of 68.1\\% in latency and 80.2\\% in maximum GPU memory occupancy compared to SRFormer-light."],"url":"http://arxiv.org/abs/2404.11848v1","category":"cs.CV"}
{"created":"2024-04-18 01:17:36","title":"Transitory tidal heating and its impact on cluster isochrones","abstract":"The kinetic energy in tidal flows, when converted into heat, can affect the internal structure of a star and shift its location on a color-magnitude diagram from that of standard models. In this paper we explore the impact of injecting heat into stars with masses near the main sequence turnoff mass (1.26 $M_\\odot$) of the open cluster M67. The heating rate is obtained from the tidal shear energy dissipation rate which is calculated from first principles by simultaneously solving the equations that describe orbital motion and the response of a star's layers to the gravitational, Coriolis, centrifugal, gas pressure and viscous forces. The stellar structure models are computed with MESA. We focus on the effects of injecting heat in pulses lasting 0.01 Gyr, a timeframe consistent with the synchonization timescale in binary systems. We find that the location of the tidally perturbed stars in the M67 color-magnitude diagram is shifted to significantly higher luminosities and effective temperatures than predicted by the standard model isochrone and include locations corresponding to some of the Blue Straggler Stars. Because tidal heating takes energy from the orbit causing it to shrink, Blue Straggler Stars could be merger or mass-transfer progenitors as well as products of these processes.","sentences":["The kinetic energy in tidal flows, when converted into heat, can affect the internal structure of a star and shift its location on a color-magnitude diagram from that of standard models.","In this paper we explore the impact of injecting heat into stars with masses near the main sequence turnoff mass (1.26 $M_\\odot$) of the open cluster M67.","The heating rate is obtained from the tidal shear energy dissipation rate which is calculated from first principles by simultaneously solving the equations that describe orbital motion and the response of a star's layers to the gravitational, Coriolis, centrifugal, gas pressure and viscous forces.","The stellar structure models are computed with MESA.","We focus on the effects of injecting heat in pulses lasting 0.01 Gyr, a timeframe consistent with the synchonization timescale in binary systems.","We find that the location of the tidally perturbed stars in the M67 color-magnitude diagram is shifted to significantly higher luminosities and effective temperatures than predicted by the standard model isochrone and include locations corresponding to some of the Blue Straggler Stars.","Because tidal heating takes energy from the orbit causing it to shrink, Blue Straggler Stars could be merger or mass-transfer progenitors as well as products of these processes."],"url":"http://arxiv.org/abs/2404.11827v1","category":"astro-ph.GA"}
{"created":"2024-04-18 00:02:18","title":"Continuous Dynamic Bipedal Jumping via Adaptive-model Optimization","abstract":"Dynamic and continuous jumping remains an open yet challenging problem in bipedal robot control. The choice of dynamic models in trajectory optimization (TO) problems plays a huge role in trajectory accuracy and computation efficiency, which normally cannot be ensured simultaneously. In this letter, we propose a novel adaptive-model optimization approach, a unified framework of Adaptive-model TO and Adaptive-frequency Model Predictive Control (MPC), to effectively realize continuous and robust jumping on HECTOR bipedal robot. The proposed Adaptive-model TO fuses adaptive-fidelity dynamics modeling of bipedal jumping motion for model fidelity necessities in different jumping phases to ensure trajectory accuracy and computation efficiency. In addition, conventional approaches have unsynchronized sampling frequencies in TO and real-time control, causing the framework to have mismatched modeling resolutions. We adapt MPC sampling frequency based on TO trajectory resolution in different phases for effective trajectory tracking. In hardware experiments, we have demonstrated robust and dynamic jumps covering a distance of up to 40 cm (57% of robot height). To verify the repeatability of this experiment, we run 53 jumping experiments and achieve 90% success rate. In continuous jumps, we demonstrate continuous bipedal jumping with terrain height perturbations (up to 5 cm) and discontinuities (up to 20 cm gap).","sentences":["Dynamic and continuous jumping remains an open yet challenging problem in bipedal robot control.","The choice of dynamic models in trajectory optimization (TO) problems plays a huge role in trajectory accuracy and computation efficiency, which normally cannot be ensured simultaneously.","In this letter, we propose a novel adaptive-model optimization approach, a unified framework of Adaptive-model TO and Adaptive-frequency Model Predictive Control (MPC), to effectively realize continuous and robust jumping on HECTOR bipedal robot.","The proposed Adaptive-model TO fuses adaptive-fidelity dynamics modeling of bipedal jumping motion for model fidelity necessities in different jumping phases to ensure trajectory accuracy and computation efficiency.","In addition, conventional approaches have unsynchronized sampling frequencies in TO and real-time control, causing the framework to have mismatched modeling resolutions.","We adapt MPC sampling frequency based on TO trajectory resolution in different phases for effective trajectory tracking.","In hardware experiments, we have demonstrated robust and dynamic jumps covering a distance of up to 40 cm (57% of robot height).","To verify the repeatability of this experiment, we run 53 jumping experiments and achieve 90% success rate.","In continuous jumps, we demonstrate continuous bipedal jumping with terrain height perturbations (up to 5 cm) and discontinuities (up to 20 cm gap)."],"url":"http://arxiv.org/abs/2404.11807v1","category":"cs.RO"}
{"created":"2024-04-17 20:01:51","title":"Entanglement Renormalization for Quantum Field Theories with Discrete Wavelet Transforms","abstract":"We propose an adaptation of Entanglement Renormalization for quantum field theories that, through the use of discrete wavelet transforms, strongly parallels the tensor network architecture of the \\emph{Multiscale Entanglement Renormalization Ansatz} (a.k.a. MERA). Our approach, called wMERA, has several advantages of over previous attempts to adapt MERA to continuum systems. In particular, (i) wMERA is formulated directly in position space, hence preserving the quasi-locality and sparsity of entanglers; and (ii) it enables a built-in RG flow in the implementation of real-time evolution and in computations of correlation functions, which is key for efficient numerical implementations. As examples, we describe in detail two concrete implementations of our wMERA algorithm for free scalar and fermionic theories in (1+1) spacetime dimensions. Possible avenues for constructing wMERAs for interacting field theories are also discussed.","sentences":["We propose an adaptation of Entanglement Renormalization for quantum field theories that, through the use of discrete wavelet transforms, strongly parallels the tensor network architecture of the \\emph{Multiscale Entanglement Renormalization Ansatz} (a.k.a. MERA).","Our approach, called wMERA, has several advantages of over previous attempts to adapt MERA to continuum systems.","In particular, (i) wMERA is formulated directly in position space, hence preserving the quasi-locality and sparsity of entanglers; and (ii) it enables a built-in RG flow in the implementation of real-time evolution and in computations of correlation functions, which is key for efficient numerical implementations.","As examples, we describe in detail two concrete implementations of our wMERA algorithm for free scalar and fermionic theories in (1+1) spacetime dimensions.","Possible avenues for constructing wMERAs for interacting field theories are also discussed."],"url":"http://arxiv.org/abs/2404.11715v1","category":"hep-th"}
{"created":"2024-04-17 19:40:15","title":"Numerical Analysis of Locally Adaptive Penalty Methods For The Navier-Stokes Equations","abstract":"Penalty methods relax the incompressibility condition and uncouple velocity and pressure. Experience with them indicates that the velocity error is sensitive to the choice of penalty parameter $\\epsilon$. So far, there is no effective \\'a prior formula for $\\epsilon$. Recently, Xie developed an adaptive penalty scheme for the Stokes problem that picks the penalty parameter $\\epsilon$ self-adaptively element by element small where $\\nabla \\cdot u^h$ is large. Her numerical tests gave accurate fluid predictions. The next natural step, developed here, is to extend the algorithm with supporting analysis to the non-linear, time-dependent incompressible Navier-Stokes equations. In this report, we prove its unconditional stability, control of $\\|\\nabla \\cdot u^h\\|$, and provide error estimates. We confirm the predicted convergence rates with numerical tests.","sentences":["Penalty methods relax the incompressibility condition and uncouple velocity and pressure.","Experience with them indicates that the velocity error is sensitive to the choice of penalty parameter $\\epsilon$. So far, there is no effective \\'a prior formula for $\\epsilon$. Recently, Xie developed an adaptive penalty scheme for the Stokes problem that picks the penalty parameter $\\epsilon$ self-adaptively element by element small where $\\nabla \\cdot u^h$ is large.","Her numerical tests gave accurate fluid predictions.","The next natural step, developed here, is to extend the algorithm with supporting analysis to the non-linear, time-dependent incompressible Navier-Stokes equations.","In this report, we prove its unconditional stability, control of $\\|\\nabla \\cdot u^h\\|$, and provide error estimates.","We confirm the predicted convergence rates with numerical tests."],"url":"http://arxiv.org/abs/2404.11712v1","category":"math.NA"}
{"created":"2024-04-17 18:48:13","title":"Asymptotic Nash Equilibria of Finite-State Ergodic Markovian Mean Field Games","abstract":"Mean field games (MFGs) model equilibria in games with a continuum of weakly interacting players as limiting systems of symmetric $n$-player games. We consider the finite-state, infinite-horizon problem with ergodic cost. Assuming Markovian strategies, we first prove that any solution to the MFG system gives rise to a $(C/\\sqrt{n})$-Nash equilibrium in the $n$-player game. We follow this result by proving the same is true for the strategy profile derived from the master equation. We conclude the main theoretical portion of the paper by establishing a large deviation principle for empirical measures associated with the asymptotic Nash equilibria. Then, we contrast the asymptotic Nash equilibria using an example. We solve the MFG system directly and numerically solve the ergodic master equation by adapting the deep Galerkin method of Sirignano and Spiliopoulos. We use these results to derive the strategies of the asymptotic Nash equilibria and compare them. Finally, we derive an explicit form for the rate functions in dimension two.","sentences":["Mean field games (MFGs) model equilibria in games with a continuum of weakly interacting players as limiting systems of symmetric $n$-player games.","We consider the finite-state, infinite-horizon problem with ergodic cost.","Assuming Markovian strategies, we first prove that any solution to the MFG system gives rise to a $(C/\\sqrt{n})$-Nash equilibrium in the $n$-player game.","We follow this result by proving the same is true for the strategy profile derived from the master equation.","We conclude the main theoretical portion of the paper by establishing a large deviation principle for empirical measures associated with the asymptotic Nash equilibria.","Then, we contrast the asymptotic Nash equilibria using an example.","We solve the MFG system directly and numerically solve the ergodic master equation by adapting the deep Galerkin method of Sirignano and Spiliopoulos.","We use these results to derive the strategies of the asymptotic Nash equilibria and compare them.","Finally, we derive an explicit form for the rate functions in dimension two."],"url":"http://arxiv.org/abs/2404.11695v1","category":"math.OC"}
{"created":"2024-04-17 17:38:54","title":"Unsupervised Microscopy Video Denoising","abstract":"In this paper, we introduce a novel unsupervised network to denoise microscopy videos featured by image sequences captured by a fixed location microscopy camera. Specifically, we propose a DeepTemporal Interpolation method, leveraging a temporal signal filter integrated into the bottom CNN layers, to restore microscopy videos corrupted by unknown noise types. Our unsupervised denoising architecture is distinguished by its ability to adapt to multiple noise conditions without the need for pre-existing noise distribution knowledge, addressing a significant challenge in real-world medical applications. Furthermore, we evaluate our denoising framework using both real microscopy recordings and simulated data, validating our outperforming video denoising performance across a broad spectrum of noise scenarios. Extensive experiments demonstrate that our unsupervised model consistently outperforms state-of-the-art supervised and unsupervised video denoising techniques, proving especially effective for microscopy videos.","sentences":["In this paper, we introduce a novel unsupervised network to denoise microscopy videos featured by image sequences captured by a fixed location microscopy camera.","Specifically, we propose a DeepTemporal Interpolation method, leveraging a temporal signal filter integrated into the bottom CNN layers, to restore microscopy videos corrupted by unknown noise types.","Our unsupervised denoising architecture is distinguished by its ability to adapt to multiple noise conditions without the need for pre-existing noise distribution knowledge, addressing a significant challenge in real-world medical applications.","Furthermore, we evaluate our denoising framework using both real microscopy recordings and simulated data, validating our outperforming video denoising performance across a broad spectrum of noise scenarios.","Extensive experiments demonstrate that our unsupervised model consistently outperforms state-of-the-art supervised and unsupervised video denoising techniques, proving especially effective for microscopy videos."],"url":"http://arxiv.org/abs/2404.12163v1","category":"eess.IV"}
{"created":"2024-04-17 17:00:12","title":"Hierarchical storage management in user space for neuroimaging applications","abstract":"Neuroimaging open-data initiatives have led to increased availability of large scientific datasets. While these datasets are shifting the processing bottleneck from compute-intensive to data-intensive, current standardized analysis tools have yet to adopt strategies that mitigate the costs associated with large data transfers. A major challenge in adapting neuroimaging applications for data-intensive processing is that they must be entirely rewritten. To facilitate data management for standardized neuroimaging tools, we developed Sea, a library that intercepts and redirects application read and write calls to minimize data transfer time. In this paper, we investigate the performance of Sea on three preprocessing pipelines implemented using standard toolboxes (FSL, SPM and AFNI), using three neuroimaging datasets of different sizes (OpenNeuro's ds001545, PREVENT-AD and the HCP dataset) on two high-performance computing clusters. Our results demonstrate that Sea provides large speedups (up to 32X) when the shared file system's (e.g. Lustre) performance is deteriorated. When the shared file system is not overburdened by other users, performance is unaffected by Sea, suggesting that Sea's overhead is minimal even in cases where its benefits are limited. Overall, Sea is beneficial, even when performance gain is minimal, as it can be used to limit the number of files created on parallel file systems.","sentences":["Neuroimaging open-data initiatives have led to increased availability of large scientific datasets.","While these datasets are shifting the processing bottleneck from compute-intensive to data-intensive, current standardized analysis tools have yet to adopt strategies that mitigate the costs associated with large data transfers.","A major challenge in adapting neuroimaging applications for data-intensive processing is that they must be entirely rewritten.","To facilitate data management for standardized neuroimaging tools, we developed Sea, a library that intercepts and redirects application read and write calls to minimize data transfer time.","In this paper, we investigate the performance of Sea on three preprocessing pipelines implemented using standard toolboxes (FSL, SPM and AFNI), using three neuroimaging datasets of different sizes (OpenNeuro's ds001545, PREVENT-AD and the HCP dataset) on two high-performance computing clusters.","Our results demonstrate that Sea provides large speedups (up to 32X) when the shared file system's (e.g. Lustre) performance is deteriorated.","When the shared file system is not overburdened by other users, performance is unaffected by Sea, suggesting that Sea's overhead is minimal even in cases where its benefits are limited.","Overall, Sea is beneficial, even when performance gain is minimal, as it can be used to limit the number of files created on parallel file systems."],"url":"http://arxiv.org/abs/2404.11556v1","category":"cs.DC"}
{"created":"2024-04-17 16:28:16","title":"Constructing heat kernels on infinite graphs","abstract":"Let $G$ be an infinite, edge- and vertex-weighted graph with certain reasonable restrictions. We construct the heat kernel of the associated Laplacian using an adaptation of the parametrix approach due to Minakshisundaram-Pleijel in the setting of Riemannian geometry. This is partly motivated by the wish to relate the heat kernels of a graph and a subgraph, or of a domain and a discretization of it. As an application, assuming that the graph is locally finite, we express the heat kernel $H_G(x,y;t)$ as a Taylor series with the lead term being $a(x,y)t^r$, where $r$ is the combinatorial distance between $x$ and $y$ and $a(x,y)$ depends (explicitly) upon edge and vertex weights. In the case $G$ is the regular $(q+1)$-tree with $q\\geq 1$, our construction reproves different explicit formulas due to Chung-Yau and to Chinta-Jorgenson-Karlsson. Assuming uniform boundedness of the combinatorial vertex degree, we show that a dilated Gaussian depending on any distance metric on $G$, which is uniformly bounded from below can be taken as a parametrix in our construction. Our work extends in part the recent articles [LNY21, CJKS23] in that the graphs are infinite and weighted.","sentences":["Let $G$ be an infinite, edge- and vertex-weighted graph with certain reasonable restrictions.","We construct the heat kernel of the associated Laplacian using an adaptation of the parametrix approach due to Minakshisundaram-Pleijel in the setting of Riemannian geometry.","This is partly motivated by the wish to relate the heat kernels of a graph and a subgraph, or of a domain and a discretization of it.","As an application, assuming that the graph is locally finite, we express the heat kernel $H_G(x,y;t)$ as a Taylor series with the lead term being $a(x,y)t^r$, where $r$ is the combinatorial distance between $x$ and $y$ and $a(x,y)$ depends (explicitly) upon edge and vertex weights.","In the case $G$ is the regular $(q+1)$-tree with $q\\geq 1$, our construction reproves different explicit formulas due to Chung-Yau and to Chinta-Jorgenson-Karlsson.","Assuming uniform boundedness of the combinatorial vertex degree, we show that a dilated Gaussian depending on any distance metric on $G$, which is uniformly bounded from below can be taken as a parametrix in our construction.","Our work extends in part the recent articles","[LNY21, CJKS23] in that the graphs are infinite and weighted."],"url":"http://arxiv.org/abs/2404.11535v1","category":"math.AP"}
{"created":"2024-04-17 16:07:25","title":"Frameworking for a Community-led Feminist Ethics","abstract":"This paper introduces a relational perspective on ethics within the context of Feminist Digital Civics and community-led design. Ethics work in HCI has primarily focused on prescriptive machine ethics and bioethics principles rather than people. In response, we advocate for a community-led, processual approach to ethics, acknowledging power dynamics and local contexts. We thus propose a multidimensional adaptive model for ethics in HCI design, integrating an intersectional feminist ethical lens. This framework embraces feminist epistemologies, methods, and methodologies, fostering a reflexive practice. By weaving together situated knowledges, standpoint theory, intersectionality, participatory methods, and care ethics, our approach offers a holistic foundation for ethics in HCI, aiming to advance community-led practices and enrich the discourse surrounding ethics within this field.","sentences":["This paper introduces a relational perspective on ethics within the context of Feminist Digital Civics and community-led design.","Ethics work in HCI has primarily focused on prescriptive machine ethics and bioethics principles rather than people.","In response, we advocate for a community-led, processual approach to ethics, acknowledging power dynamics and local contexts.","We thus propose a multidimensional adaptive model for ethics in HCI design, integrating an intersectional feminist ethical lens.","This framework embraces feminist epistemologies, methods, and methodologies, fostering a reflexive practice.","By weaving together situated knowledges, standpoint theory, intersectionality, participatory methods, and care ethics, our approach offers a holistic foundation for ethics in HCI, aiming to advance community-led practices and enrich the discourse surrounding ethics within this field."],"url":"http://arxiv.org/abs/2404.11514v1","category":"cs.CY"}
{"created":"2024-04-17 15:59:25","title":"Testing Intersectingness of Uniform Families","abstract":"A set family ${\\cal F}$ is called intersecting if every two members of ${\\cal F}$ intersect, and it is called uniform if all members of ${\\cal F}$ share a common size. A uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\\varepsilon$-far from intersecting if one has to remove more than $\\varepsilon \\cdot \\binom{n}{k}$ of the sets of ${\\cal F}$ to make it intersecting. We study the property testing problem that given query access to a uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$, asks to distinguish between the case that ${\\cal F}$ is intersecting and the case that it is $\\varepsilon$-far from intersecting. We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\\frac{\\ln n}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\\frac{\\ln k}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms. For $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\\frac{1}{\\varepsilon})$. Our findings show that the query complexity of the problem differs substantially from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024).","sentences":["A set family ${\\cal F}$ is called intersecting if every two members of ${\\cal F}$ intersect, and it is called uniform if all members of ${\\cal F}$ share a common size.","A uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\\varepsilon$-far from intersecting if one has to remove more than $\\varepsilon \\cdot \\binom{n}{k}$ of the sets of ${\\cal F}$ to make it intersecting.","We study the property testing problem that given query access to a uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$, asks to distinguish between the case that ${\\cal F}$ is intersecting and the case that it is $\\varepsilon$-far from intersecting.","We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\\frac{\\ln n}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\\frac{\\ln k}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms.","For $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\\frac{1}{\\varepsilon})$. Our findings show that the query complexity of the problem differs substantially from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024)."],"url":"http://arxiv.org/abs/2404.11504v1","category":"cs.DS"}
{"created":"2024-04-17 15:52:29","title":"Runtime Verification and Field Testing for ROS-Based Robotic Systems","abstract":"Robotic systems are becoming pervasive and adopted in increasingly many domains, such as manufacturing, healthcare, and space exploration. To this end, engineering software has emerged as a crucial discipline for building maintainable and reusable robotic systems. Robotics software engineering research has received increasing attention, fostering autonomy as a fundamental goal. However, robotics developers are still challenged trying to achieve this goal given that simulation is not able to deliver solutions to realistically emulate real-world phenomena. Robots also need to operate in unpredictable and uncontrollable environments, which require safe and trustworthy self-adaptation capabilities implemented in software. Typical techniques to address the challenges are runtime verification, field-based testing, and mitigation techniques that enable fail-safe solutions. However, there is no clear guidance to architect ROS-based systems to enable and facilitate runtime verification and field-based testing. This paper aims to fill in this gap by providing guidelines that can help developers and QA teams when developing, verifying or testing their robots in the field. These guidelines are carefully tailored to address the challenges and requirements of testing robotics systems in real-world scenarios. We conducted a literature review on studies addressing runtime verification and field-based testing for robotic systems, mined ROS-based application repositories, and validated the applicability, clarity, and usefulness via two questionnaires with 55 answers. We contribute 20 guidelines formulated for researchers and practitioners in robotic software engineering. Finally, we map our guidelines to open challenges thus far in runtime verification and field-based testing for ROS-based systems and, we outline promising research directions in the field.","sentences":["Robotic systems are becoming pervasive and adopted in increasingly many domains, such as manufacturing, healthcare, and space exploration.","To this end, engineering software has emerged as a crucial discipline for building maintainable and reusable robotic systems.","Robotics software engineering research has received increasing attention, fostering autonomy as a fundamental goal.","However, robotics developers are still challenged trying to achieve this goal given that simulation is not able to deliver solutions to realistically emulate real-world phenomena.","Robots also need to operate in unpredictable and uncontrollable environments, which require safe and trustworthy self-adaptation capabilities implemented in software.","Typical techniques to address the challenges are runtime verification, field-based testing, and mitigation techniques that enable fail-safe solutions.","However, there is no clear guidance to architect ROS-based systems to enable and facilitate runtime verification and field-based testing.","This paper aims to fill in this gap by providing guidelines that can help developers and QA teams when developing, verifying or testing their robots in the field.","These guidelines are carefully tailored to address the challenges and requirements of testing robotics systems in real-world scenarios.","We conducted a literature review on studies addressing runtime verification and field-based testing for robotic systems, mined ROS-based application repositories, and validated the applicability, clarity, and usefulness via two questionnaires with 55 answers.","We contribute 20 guidelines formulated for researchers and practitioners in robotic software engineering.","Finally, we map our guidelines to open challenges thus far in runtime verification and field-based testing for ROS-based systems and, we outline promising research directions in the field."],"url":"http://arxiv.org/abs/2404.11498v1","category":"cs.SE"}
{"created":"2024-04-17 15:34:55","title":"IoTSim-Osmosis-RES: Towards autonomic renewable energy-aware osmotic computing","abstract":"Internet of Things systems exists in various areas of our everyday life. For example, sensors installed in smart cities and homes are processed in edge and cloud computing centres providing several benefits that improve our lives. The place of data processing is related to the required system response times -- processing data closer to its source results in a shorter system response time. The Osmotic Computing concept enables flexible deployment of data processing services and their possible movement, just like particles in the osmosis phenomenon move between regions of different densities. At the same time, the impact of complex computer architecture on the environment is increasingly being compensated by the use of renewable and low-carbon energy sources. However, the uncertainty of supplying green energy makes the management of Osmotic Computing demanding, and therefore their autonomy is desirable. In the paper, we present a framework enabling osmotic computing simulation based on renewable energy sources and autonomic osmotic agents, allowing the analysis of distributed management algorithms. We discuss the challenges posed to the framework and analyze various management algorithms for cooperating osmotic agents. In the evaluation we show that changing the adaptation logic of the osmotic agents, it is possible to increase the self-consumption of renewable energy sources or increase the usage of low emission ones.","sentences":["Internet of Things systems exists in various areas of our everyday life.","For example, sensors installed in smart cities and homes are processed in edge and cloud computing centres providing several benefits that improve our lives.","The place of data processing is related to the required system response times -- processing data closer to its source results in a shorter system response time.","The Osmotic Computing concept enables flexible deployment of data processing services and their possible movement, just like particles in the osmosis phenomenon move between regions of different densities.","At the same time, the impact of complex computer architecture on the environment is increasingly being compensated by the use of renewable and low-carbon energy sources.","However, the uncertainty of supplying green energy makes the management of Osmotic Computing demanding, and therefore their autonomy is desirable.","In the paper, we present a framework enabling osmotic computing simulation based on renewable energy sources and autonomic osmotic agents, allowing the analysis of distributed management algorithms.","We discuss the challenges posed to the framework and analyze various management algorithms for cooperating osmotic agents.","In the evaluation we show that changing the adaptation logic of the osmotic agents, it is possible to increase the self-consumption of renewable energy sources or increase the usage of low emission ones."],"url":"http://arxiv.org/abs/2404.11481v1","category":"cs.DC"}
{"created":"2024-04-17 15:26:55","title":"Assessing The Effectiveness Of Current Cybersecurity Regulations And Policies In The US","abstract":"This article assesses the effectiveness of current cybersecurity regulations and policies in the United States amidst the escalating frequency and sophistication of cyber threats. The focus is on the comprehensive framework established by the U.S. government, with a spotlight on the National Institute of Standards and Technology (NIST) Cybersecurity Framework and key regulations such as HIPAA, GLBA, FISMA, CISA, CCPA, and the DOD Cybersecurity Maturity Model Certification. The study evaluates the impact of these regulations on different sectors and analyzes trends in cybercrime data from 2000 to 2022. The findings highlight the challenges, successes, and the need for continuous adaptation in the face of evolving cyber threats","sentences":["This article assesses the effectiveness of current cybersecurity regulations and policies in the United States amidst the escalating frequency and sophistication of cyber threats.","The focus is on the comprehensive framework established by the U.S. government, with a spotlight on the National Institute of Standards and Technology (NIST) Cybersecurity Framework and key regulations such as HIPAA, GLBA, FISMA, CISA, CCPA, and the DOD Cybersecurity Maturity Model Certification.","The study evaluates the impact of these regulations on different sectors and analyzes trends in cybercrime data from 2000 to 2022.","The findings highlight the challenges, successes, and the need for continuous adaptation in the face of evolving cyber threats"],"url":"http://arxiv.org/abs/2404.11473v1","category":"cs.CR"}
{"created":"2024-04-17 15:15:54","title":"Quantum dynamics of dissipative Chern insulator","abstract":"For open quantum systems,a short-time evolution is usually well described by the effective non-Hermitian Hamiltonians,while long-time dynamics requires the Lindblad master equation,in which the Liouvillian superoperators characterize the time evolution. In this paper, we constructed an open system by adding suitable gain and loss operators to the Chen insulator to investigate the time evolution of quantum states at long times by numerical simulations.Finally,we also propose a topolectrical circuits to realize the dissipative system for experimental observation. It is found found that the opening and closing of the Liouvillian gap leads to different damping behaviours of the system and that the presence of non-Hermitian skin effects leads to a phenomenon of chiral damping with sharp wavefronts.Our study deepens the understanding of quantum dynamics of dissipative system.","sentences":["For open quantum systems,a short-time evolution is usually well described by the effective non-Hermitian Hamiltonians,while long-time dynamics requires the Lindblad master equation,in which the Liouvillian superoperators characterize the time evolution.","In this paper, we constructed an open system by adding suitable gain and loss operators to the Chen insulator to investigate the time evolution of quantum states at long times by numerical simulations.","Finally,we also propose a topolectrical circuits to realize the dissipative system for experimental observation.","It is found found that the opening and closing of the Liouvillian gap leads to different damping behaviours of the system and that the presence of non-Hermitian skin effects leads to a phenomenon of chiral damping with sharp wavefronts.","Our study deepens the understanding of quantum dynamics of dissipative system."],"url":"http://arxiv.org/abs/2404.11466v2","category":"quant-ph"}
{"created":"2024-04-17 15:14:45","title":"Low-Density Parity-Check Codes and Spatial Coupling for Quantitative Group Testing","abstract":"A non-adaptive quantitative group testing (GT) scheme based on sparse codes-on-graphs in combination with low-complexity peeling decoding was introduced and analyzed by Karimi et al.. In this work, we propose a variant of this scheme based on low-density parity-check codes where the BCH codes at the constraint nodes are replaced by simple single parity-check codes. Furthermore, we apply spatial coupling to both GT schemes, perform a density evolution analysis, and compare their performance with and without coupling. Our analysis shows that both schemes improve with increasing coupling memory, and for all considered cases, it is observed that the LDPC code-based scheme substantially outperforms the original scheme. Simulation results for finite block length confirm the asymptotic density evolution thresholds.","sentences":["A non-adaptive quantitative group testing (GT) scheme based on sparse codes-on-graphs in combination with low-complexity peeling decoding was introduced and analyzed by Karimi et al..","In this work, we propose a variant of this scheme based on low-density parity-check codes where the BCH codes at the constraint nodes are replaced by simple single parity-check codes.","Furthermore, we apply spatial coupling to both GT schemes, perform a density evolution analysis, and compare their performance with and without coupling.","Our analysis shows that both schemes improve with increasing coupling memory, and for all considered cases, it is observed that the LDPC code-based scheme substantially outperforms the original scheme.","Simulation results for finite block length confirm the asymptotic density evolution thresholds."],"url":"http://arxiv.org/abs/2404.11463v1","category":"cs.IT"}
{"created":"2024-04-17 14:43:05","title":"An adaptive linearized alternating direction multiplier method for solving convex optimization problems","abstract":"This thesis proposes an adaptive linearized alternating direction multiplier method to improve the convergence rate of the algorithm by using adaptive techniques to dynamically select the regular term coefficients. The innovation of this method is to utilize the information of the current iteration point to adaptively select the appropriate parameters, thus expanding the selection of the subproblem step size and improving the convergence rate of the algorithm while ensuring convergence.The advantage of this method is that it can improve the convergence rate of the algorithm as much as possible without compromising the convergence. This is very beneficial for the solution of optimization problems because the traditional linearized alternating direction multiplier method has a trade-off in the selection of the regular term coefficients: larger coefficients ensure convergence but tend to lead to small step sizes, while smaller coefficients allow for an increase in the iterative step size but tend to lead to the algorithm's non-convergence. This balance can be better handled by adaptively selecting the parameters, thus improving the efficiency of the algorithm.Overall, the method proposed in this thesis is of great importance in the field of matrix optimization and has a positive effect on improving the convergence speed and efficiency of the algorithm. It is hoped that this adaptive idea can bring new inspiration to the development of the field of matrix optimization and promote the research and application in related fields.","sentences":["This thesis proposes an adaptive linearized alternating direction multiplier method to improve the convergence rate of the algorithm by using adaptive techniques to dynamically select the regular term coefficients.","The innovation of this method is to utilize the information of the current iteration point to adaptively select the appropriate parameters, thus expanding the selection of the subproblem step size and improving the convergence rate of the algorithm while ensuring convergence.","The advantage of this method is that it can improve the convergence rate of the algorithm as much as possible without compromising the convergence.","This is very beneficial for the solution of optimization problems because the traditional linearized alternating direction multiplier method has a trade-off in the selection of the regular term coefficients: larger coefficients ensure convergence but tend to lead to small step sizes, while smaller coefficients allow for an increase in the iterative step size but tend to lead to the algorithm's non-convergence.","This balance can be better handled by adaptively selecting the parameters, thus improving the efficiency of the algorithm.","Overall, the method proposed in this thesis is of great importance in the field of matrix optimization and has a positive effect on improving the convergence speed and efficiency of the algorithm.","It is hoped that this adaptive idea can bring new inspiration to the development of the field of matrix optimization and promote the research and application in related fields."],"url":"http://arxiv.org/abs/2404.11435v1","category":"math.OC"}
{"created":"2024-04-17 14:20:59","title":"Directional Filter Design and Simulation for Superconducting On-chip Filter-banks","abstract":"Many superconducting on-chip filter-banks suffer from poor coupling to the detectors behind each filter. This is a problem intrinsic to the commonly used half wavelength filter, which has a maximum theoretical coupling of 50 %. In this paper we introduce a phase coherent filter, called a directional filter, which has a theoretical coupling of 100 %. In order to to study and compare different types of filter-banks, we first analyze the measured filter frequency scatter, losses, and spectral resolution of a DESHIMA 2.0 filter-bank chip. Based on measured fabrication tolerances and losses, we adapt the input parameters for our circuit simulations, quantitatively reproducing the measurements. We find that the frequency scatter is caused by nanometer-scale line-width variations and that variances in the spectral resolution is caused by losses in the dielectric only. Finally, we include these realistic parameters in a full filter-bank model and simulate a wide range of spectral resolutions and oversampling values. For all cases the directional filter-bank has significantly higher coupling to the detectors than the half-wave resonator filter-bank. The directional filter eliminates the need to use oversampling as a method to improve the total efficiency, instead capturing nearly all the power remaining after dielectric losses.","sentences":["Many superconducting on-chip filter-banks suffer from poor coupling to the detectors behind each filter.","This is a problem intrinsic to the commonly used half wavelength filter, which has a maximum theoretical coupling of 50 %.","In this paper we introduce a phase coherent filter, called a directional filter, which has a theoretical coupling of 100 %.","In order to to study and compare different types of filter-banks, we first analyze the measured filter frequency scatter, losses, and spectral resolution of a DESHIMA 2.0 filter-bank chip.","Based on measured fabrication tolerances and losses, we adapt the input parameters for our circuit simulations, quantitatively reproducing the measurements.","We find that the frequency scatter is caused by nanometer-scale line-width variations and that variances in the spectral resolution is caused by losses in the dielectric only.","Finally, we include these realistic parameters in a full filter-bank model and simulate a wide range of spectral resolutions and oversampling values.","For all cases the directional filter-bank has significantly higher coupling to the detectors than the half-wave resonator filter-bank.","The directional filter eliminates the need to use oversampling as a method to improve the total efficiency, instead capturing nearly all the power remaining after dielectric losses."],"url":"http://arxiv.org/abs/2404.11417v1","category":"astro-ph.IM"}
{"created":"2024-04-17 13:18:39","title":"Boosting Medical Image Segmentation Performance with Adaptive Convolution Layer","abstract":"Medical image segmentation plays a vital role in various clinical applications, enabling accurate delineation and analysis of anatomical structures or pathological regions. Traditional CNNs have achieved remarkable success in this field. However, they often rely on fixed kernel sizes, which can limit their performance and adaptability in medical images where features exhibit diverse scales and configurations due to variability in equipment, target sizes, and expert interpretations.   In this paper, we propose an adaptive layer placed ahead of leading deep-learning models such as UCTransNet, which dynamically adjusts the kernel size based on the local context of the input image.   By adaptively capturing and fusing features at multiple scales, our approach enhances the network's ability to handle diverse anatomical structures and subtle image details, even for recently performing architectures that internally implement intra-scale modules, such as UCTransnet.   Extensive experiments are conducted on   benchmark medical image datasets to evaluate the effectiveness of our proposal. It consistently outperforms traditional \\glspl{CNN} with fixed kernel sizes with a similar number of parameters, achieving superior segmentation Accuracy, Dice, and IoU in popular datasets such as SegPC2021 and ISIC2018. The model and data are published in the open-source repository, ensuring transparency and reproducibility of our promising results.","sentences":["Medical image segmentation plays a vital role in various clinical applications, enabling accurate delineation and analysis of anatomical structures or pathological regions.","Traditional CNNs have achieved remarkable success in this field.","However, they often rely on fixed kernel sizes, which can limit their performance and adaptability in medical images where features exhibit diverse scales and configurations due to variability in equipment, target sizes, and expert interpretations.   ","In this paper, we propose an adaptive layer placed ahead of leading deep-learning models such as UCTransNet, which dynamically adjusts the kernel size based on the local context of the input image.   ","By adaptively capturing and fusing features at multiple scales, our approach enhances the network's ability to handle diverse anatomical structures and subtle image details, even for recently performing architectures that internally implement intra-scale modules, such as UCTransnet.   ","Extensive experiments are conducted on   benchmark medical image datasets to evaluate the effectiveness of our proposal.","It consistently outperforms traditional \\glspl{CNN} with fixed kernel sizes with a similar number of parameters, achieving superior segmentation Accuracy, Dice, and IoU in popular datasets such as SegPC2021 and ISIC2018.","The model and data are published in the open-source repository, ensuring transparency and reproducibility of our promising results."],"url":"http://arxiv.org/abs/2404.11361v1","category":"eess.IV"}
{"created":"2024-04-17 13:12:14","title":"Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness","abstract":"Object detection tasks, crucial in safety-critical systems like autonomous driving, focus on pinpointing object locations. These detectors are known to be susceptible to backdoor attacks. However, existing backdoor techniques have primarily been adapted from classification tasks, overlooking deeper vulnerabilities specific to object detection. This paper is dedicated to bridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor attack paradigm tailored for object detection. DC is designed to instantly incapacitate detectors (i.e., severely impairing detector's performance and culminating in a denial-of-service). To this end, we develop two innovative attack schemes: Sponge for triggering widespread misidentifications and Blinding for rendering objects invisible. Remarkably, we introduce a novel poisoning strategy exploiting natural objects, enabling DC to act as a practical backdoor in real-world environments. Our experiments on different detectors across several benchmarks show a significant improvement ($\\sim$10\\%-60\\% absolute and $\\sim$2-7$\\times$ relative) in attack efficacy over state-of-the-art attacks.","sentences":["Object detection tasks, crucial in safety-critical systems like autonomous driving, focus on pinpointing object locations.","These detectors are known to be susceptible to backdoor attacks.","However, existing backdoor techniques have primarily been adapted from classification tasks, overlooking deeper vulnerabilities specific to object detection.","This paper is dedicated to bridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor attack paradigm tailored for object detection.","DC is designed to instantly incapacitate detectors (i.e., severely impairing detector's performance and culminating in a denial-of-service).","To this end, we develop two innovative attack schemes: Sponge for triggering widespread misidentifications and Blinding for rendering objects invisible.","Remarkably, we introduce a novel poisoning strategy exploiting natural objects, enabling DC to act as a practical backdoor in real-world environments.","Our experiments on different detectors across several benchmarks show a significant improvement ($\\sim$10\\%-60\\% absolute and $\\sim$2-7$\\times$ relative) in attack efficacy over state-of-the-art attacks."],"url":"http://arxiv.org/abs/2404.11357v1","category":"cs.CV"}
{"created":"2024-04-17 13:09:15","title":"Accelerating Geo-distributed Machine Learning with Network-Aware Adaptive Tree and Auxiliary Route","abstract":"Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions. This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks. Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures. However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements. This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers. First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology. Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions. Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions. Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies. Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET.","sentences":["Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions.","This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks.","Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures.","However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements.","This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers.","First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology.","Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions.","Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions.","Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies.","Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET."],"url":"http://arxiv.org/abs/2404.11352v1","category":"cs.DC"}
{"created":"2024-04-17 13:07:04","title":"Isotropic maps and moment map flow","abstract":"We consider the moduli space of isotropic maps from a closed surface $\\Sigma$ to a symplectic affine space and construct a K\\\"ahler moment map geometry, on a space of differential forms on $\\Sigma$, such that the isotropic maps correspond to certain zeroes of the moment map. The moment map geometry induces a modified moment map flow, whose fixed point set correspond to isotropic maps. This construction can be adapted to the polyhedral setting. In particular, we prove that the polyhedral modified moment map flow induces a strong deformation retraction from the space of polyhedral maps onto the space of polyhedral isotropic maps.","sentences":["We consider the moduli space of isotropic maps from a closed surface $\\Sigma$ to a symplectic affine space and construct a K\\\"ahler moment map geometry, on a space of differential forms on $\\Sigma$, such that the isotropic maps correspond to certain zeroes of the moment map.","The moment map geometry induces a modified moment map flow, whose fixed point set correspond to isotropic maps.","This construction can be adapted to the polyhedral setting.","In particular, we prove that the polyhedral modified moment map flow induces a strong deformation retraction from the space of polyhedral maps onto the space of polyhedral isotropic maps."],"url":"http://arxiv.org/abs/2404.11347v1","category":"math.DG"}
{"created":"2024-04-17 13:01:39","title":"A numerical approach to levitated superconductors and its application to a superconducting cylinder in a quadrupole field","abstract":"Magnetically levitated superconductors in the Meissner state can be utilized as micro-mechanical oscillators with large mass, high quality factors and long coherence times. In previous works analytical solutions for the magnetic field distribution around a superconducting sphere in a quadrupole field have been found and used to derive the trap parameters, while non-spherical geometries have only been investigated in a few idealized cases. However, superconductors of almost arbitrary shape can be used as levitators in a magnetic trap and, as the trap's properties depend strongly on the superconductors shape, allow for a wider parameter regime to be accessed. Finite element models are suitable to obtain the field distribution around arbitrarily shaped superconductors in arbitrary fields, but have not yet been used widely in the context of levitated superconductors. Here we present a simple numerical model for this purpose and use it to calculate the field distribution around cylindrical superconductors in a quadrupole field and to evaluate the trap parameters. We find that the cylindrical shape, compared to spherical levitators, allows for substantially higher trap frequencies and coupling strengths. This in turn reduces the demands on vibration isolation and significantly eases the requirements for feedback cooling to the ground state. The numerical model is provided as supplemental material and can easily be adapted to various geometries and trap fields.","sentences":["Magnetically levitated superconductors in the Meissner state can be utilized as micro-mechanical oscillators with large mass, high quality factors and long coherence times.","In previous works analytical solutions for the magnetic field distribution around a superconducting sphere in a quadrupole field have been found and used to derive the trap parameters, while non-spherical geometries have only been investigated in a few idealized cases.","However, superconductors of almost arbitrary shape can be used as levitators in a magnetic trap and, as the trap's properties depend strongly on the superconductors shape, allow for a wider parameter regime to be accessed.","Finite element models are suitable to obtain the field distribution around arbitrarily shaped superconductors in arbitrary fields, but have not yet been used widely in the context of levitated superconductors.","Here we present a simple numerical model for this purpose and use it to calculate the field distribution around cylindrical superconductors in a quadrupole field and to evaluate the trap parameters.","We find that the cylindrical shape, compared to spherical levitators, allows for substantially higher trap frequencies and coupling strengths.","This in turn reduces the demands on vibration isolation and significantly eases the requirements for feedback cooling to the ground state.","The numerical model is provided as supplemental material and can easily be adapted to various geometries and trap fields."],"url":"http://arxiv.org/abs/2404.11342v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 13:00:23","title":"When time delays and phase lags are not the same: higher-order phase reduction unravels delay-induced synchronization in oscillator networks","abstract":"Coupled oscillators with time-delayed network interactions are critical to understand synchronization phenomena in many physical systems. Phase reductions to finite-dimensional phase oscillator networks allow for their explicit analysis. However, first-order phase reductions - where delays correspond to phase lags - fail to capture the delay-dependence of synchronization. We develop a systematic approach to derive phase reductions for delay-coupled oscillators to arbitrary order. Already the second-order reduction can predict delay-dependent (bi-)stability of synchronized states as demonstrated for Stuart-Landau oscillators.","sentences":["Coupled oscillators with time-delayed network interactions are critical to understand synchronization phenomena in many physical systems.","Phase reductions to finite-dimensional phase oscillator networks allow for their explicit analysis.","However, first-order phase reductions - where delays correspond to phase lags - fail to capture the delay-dependence of synchronization.","We develop a systematic approach to derive phase reductions for delay-coupled oscillators to arbitrary order.","Already the second-order reduction can predict delay-dependent (bi-)stability of synchronized states as demonstrated for Stuart-Landau oscillators."],"url":"http://arxiv.org/abs/2404.11340v1","category":"math.DS"}
{"created":"2024-04-17 12:39:48","title":"Following the Human Thread in Social Navigation","abstract":"The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.   We propose the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time. Tested on the novel Habitat 3.0 platform, SDA sets a novel state of the art (SoA) performance in finding and following humans.","sentences":["The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion.","Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions.","Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.   ","We propose the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics.","We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action.","Here, the trajectories are fully visible, i.e., assumed as privileged information.","In the second stage, the trained policy operates without direct access to trajectories.","Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time.","Tested on the novel Habitat 3.0 platform, SDA sets a novel state of the art (SoA) performance in finding and following humans."],"url":"http://arxiv.org/abs/2404.11327v1","category":"cs.RO"}
{"created":"2024-04-17 12:12:48","title":"Learning from Unlabelled Data with Transformers: Domain Adaptation for Semantic Segmentation of High Resolution Aerial Images","abstract":"Data from satellites or aerial vehicles are most of the times unlabelled. Annotating such data accurately is difficult, requires expertise, and is costly in terms of time. Even if Earth Observation (EO) data were correctly labelled, labels might change over time. Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging. In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Observation Semantic Segmentation (NEOS) model. NEOS performs domain adaptation as the target domain does not have ground truth semantic segmentation masks. The distribution inconsistencies between the target and source domains are due to differences in acquisition scenes, environment conditions, sensors, and times. Our model aligns the learned representations of the different domains to make them coincide. The evaluation results show that NEOS is successful and outperforms other models for semantic segmentation of unlabelled data.","sentences":["Data from satellites or aerial vehicles are most of the times unlabelled.","Annotating such data accurately is difficult, requires expertise, and is costly in terms of time.","Even if Earth Observation (EO) data were correctly labelled, labels might change over time.","Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging.","In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Observation Semantic Segmentation (NEOS) model.","NEOS performs domain adaptation as the target domain does not have ground truth semantic segmentation masks.","The distribution inconsistencies between the target and source domains are due to differences in acquisition scenes, environment conditions, sensors, and times.","Our model aligns the learned representations of the different domains to make them coincide.","The evaluation results show that NEOS is successful and outperforms other models for semantic segmentation of unlabelled data."],"url":"http://arxiv.org/abs/2404.11299v1","category":"cs.CV"}
{"created":"2024-04-17 11:55:45","title":"Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption","abstract":"Existing multi-person human reconstruction approaches mainly focus on recovering accurate poses or avoiding penetration, but overlook the modeling of close interactions. In this work, we tackle the task of reconstructing closely interactive humans from a monocular video. The main challenge of this task comes from insufficient visual information caused by depth ambiguity and severe inter-person occlusion. In view of this, we propose to leverage knowledge from proxemic behavior and physics to compensate the lack of visual information. This is based on the observation that human interaction has specific patterns following the social proxemics. Specifically, we first design a latent representation based on Vector Quantised-Variational AutoEncoder (VQ-VAE) to model human interaction. A proxemics and physics guided diffusion model is then introduced to denoise the initial distribution. We design the diffusion model as dual branch with each branch representing one individual such that the interaction can be modeled via cross attention. With the learned priors of VQ-VAE and physical constraint as the additional information, our proposed approach is capable of estimating accurate poses that are also proxemics and physics plausible. Experimental results on Hi4D, 3DPW, and CHI3D demonstrate that our method outperforms existing approaches. The code is available at \\url{https://github.com/boycehbz/HumanInteraction}.","sentences":["Existing multi-person human reconstruction approaches mainly focus on recovering accurate poses or avoiding penetration, but overlook the modeling of close interactions.","In this work, we tackle the task of reconstructing closely interactive humans from a monocular video.","The main challenge of this task comes from insufficient visual information caused by depth ambiguity and severe inter-person occlusion.","In view of this, we propose to leverage knowledge from proxemic behavior and physics to compensate the lack of visual information.","This is based on the observation that human interaction has specific patterns following the social proxemics.","Specifically, we first design a latent representation based on Vector Quantised-Variational AutoEncoder (VQ-VAE) to model human interaction.","A proxemics and physics guided diffusion model is then introduced to denoise the initial distribution.","We design the diffusion model as dual branch with each branch representing one individual such that the interaction can be modeled via cross attention.","With the learned priors of VQ-VAE and physical constraint as the additional information, our proposed approach is capable of estimating accurate poses that are also proxemics and physics plausible.","Experimental results on Hi4D, 3DPW, and CHI3D demonstrate that our method outperforms existing approaches.","The code is available at \\url{https://github.com/boycehbz/HumanInteraction}."],"url":"http://arxiv.org/abs/2404.11291v1","category":"cs.CV"}
{"created":"2024-04-17 11:04:48","title":"Turbulence revealed by wavelet transform: power spectrum and intermittency for the velocity field of the cosmic baryonic fluid","abstract":"We use continuous wavelet transform techniques to construct the global and environment-dependent wavelet statistics, such as energy spectrum and kurtosis, to study the fluctuation and intermittency of the turbulent motion in the cosmic fluid velocity field with the IllustrisTNG simulation data. We find that the peak scales of the energy spectrum and the spectral ratio define two characteristic scales, which can be regarded as the integral scale and the dissipation scale of turbulence, respectively, so that the energy spectrum can be divided into the energy-containing range, the inertial range and the dissipation range of turbulence. The wavelet kurtosis is an increasing function of the wavenumber $k$, first grows rapidly then slowly with $k$, indicating that the cosmic fluid becomes increasingly intermittent with $k$. In the energy-containing range, the energy spectrum increases significantly from $z = 2$ to $1$, but remains almost unchanged from $z = 1$ to $0$. We find that both the environment-dependent spectrum and kurtosis are similar to the global ones, and the magnitude of the spectrum is smallest in the lowest-density and largest in the highest-density environment, suggesting that the cosmic fluid is more turbulent in a high-density than in a low-density environment. In the inertial range, the exponent of the energy spectrum is steeper than not only the Kolmogorov but also the Burgers exponent, suggesting that there may be more complex mechanisms for energy transfer than Kolmogorov and Burgers turbulence.","sentences":["We use continuous wavelet transform techniques to construct the global and environment-dependent wavelet statistics, such as energy spectrum and kurtosis, to study the fluctuation and intermittency of the turbulent motion in the cosmic fluid velocity field with the IllustrisTNG simulation data.","We find that the peak scales of the energy spectrum and the spectral ratio define two characteristic scales, which can be regarded as the integral scale and the dissipation scale of turbulence, respectively, so that the energy spectrum can be divided into the energy-containing range, the inertial range and the dissipation range of turbulence.","The wavelet kurtosis is an increasing function of the wavenumber $k$, first grows rapidly then slowly with $k$, indicating that the cosmic fluid becomes increasingly intermittent with $k$. In the energy-containing range, the energy spectrum increases significantly from $z = 2$ to $1$, but remains almost unchanged from $z = 1$ to $0$. We find that both the environment-dependent spectrum and kurtosis are similar to the global ones, and the magnitude of the spectrum is smallest in the lowest-density and largest in the highest-density environment, suggesting that the cosmic fluid is more turbulent in a high-density than in a low-density environment.","In the inertial range, the exponent of the energy spectrum is steeper than not only the Kolmogorov but also the Burgers exponent, suggesting that there may be more complex mechanisms for energy transfer than Kolmogorov and Burgers turbulence."],"url":"http://arxiv.org/abs/2404.11255v1","category":"astro-ph.CO"}
{"created":"2024-04-17 11:00:12","title":"Bayesian Parameterized Quantum Circuit Optimization (BPQCO): A task and hardware-dependent approach","abstract":"Variational quantum algorithms (VQA) have emerged as a promising quantum alternative for solving optimization and machine learning problems using parameterized quantum circuits (PQCs). The design of these circuits influences the ability of the algorithm to efficiently explore the solution space and converge to more optimal solutions. Choosing an appropriate circuit topology, gate set, and parameterization scheme is determinant to achieve good performance. In addition, it is not only problem-dependent, but the quantum hardware used also has a significant impact on the results. Therefore, we present BPQCO, a Bayesian Optimization-based strategy to search for optimal PQCs adapted to the problem to be solved and to the characteristics and limitations of the chosen quantum hardware. To this end, we experimentally demonstrate the influence of the circuit design on the performance obtained for two classification problems (a synthetic dataset and the well-known Iris dataset), focusing on the design of the circuit ansatz. In addition, we study the degradation of the obtained circuits in the presence of noise when simulating real quantum computers. To mitigate the effect of noise, two alternative optimization strategies based on the characteristics of the quantum system are proposed. The results obtained confirm the relevance of the presented approach and allow its adoption in further work based on the use of PQCs.","sentences":["Variational quantum algorithms (VQA) have emerged as a promising quantum alternative for solving optimization and machine learning problems using parameterized quantum circuits (PQCs).","The design of these circuits influences the ability of the algorithm to efficiently explore the solution space and converge to more optimal solutions.","Choosing an appropriate circuit topology, gate set, and parameterization scheme is determinant to achieve good performance.","In addition, it is not only problem-dependent, but the quantum hardware used also has a significant impact on the results.","Therefore, we present BPQCO, a Bayesian Optimization-based strategy to search for optimal PQCs adapted to the problem to be solved and to the characteristics and limitations of the chosen quantum hardware.","To this end, we experimentally demonstrate the influence of the circuit design on the performance obtained for two classification problems (a synthetic dataset and the well-known Iris dataset), focusing on the design of the circuit ansatz.","In addition, we study the degradation of the obtained circuits in the presence of noise when simulating real quantum computers.","To mitigate the effect of noise, two alternative optimization strategies based on the characteristics of the quantum system are proposed.","The results obtained confirm the relevance of the presented approach and allow its adoption in further work based on the use of PQCs."],"url":"http://arxiv.org/abs/2404.11253v1","category":"quant-ph"}
{"created":"2024-04-17 09:01:02","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document","abstract":"Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method metric Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems.","sentences":["Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems.","Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed.","But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability.","In this work, we propose highly effective and interpretable factual inconsistency detection method metric Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document for abstractive summarization systems that is based on fine-grained atomic facts decomposition.","Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion.","These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency.","Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems."],"url":"http://arxiv.org/abs/2404.11184v2","category":"cs.CL"}
{"created":"2024-04-17 08:01:15","title":"Automated, efficient and model-free inference for randomized clinical trials via data-driven covariate adjustment","abstract":"In May 2023, the U.S. Food and Drug Administration (FDA) released guidance for industry on \"Adjustment for Covariates in Randomized Clinical Trials for Drugs and Biological Products\". Covariate adjustment is a statistical analysis method for improving precision and power in clinical trials by adjusting for pre-specified, prognostic baseline variables. Though recommended by the FDA and the European Medicines Agency (EMA), many trials do not exploit the available information in baseline variables or make use only of the baseline measurement of the outcome. This is likely (partly) due to the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms. We will explore the potential of automated data-adaptive methods, such as machine learning algorithms, for covariate adjustment, addressing the challenge of pre-specification. Specifically, our approach allows the use of complex models or machine learning algorithms without compromising the interpretation or validity of the treatment effect estimate and its corresponding standard error, even in the presence of misspecified outcome working models. This contrasts the majority of competing works which assume correct model specification for the validity of standard errors. Our proposed estimators either necessitate ultra-sparsity in the outcome model (which can be relaxed by limiting the number of predictors in the model) or necessitate integration with sample splitting to enhance their performance. As such, we will arrive at simple estimators and standard errors for the marginal treatment effect in randomized clinical trials, which exploit data-adaptive outcome predictions based on prognostic baseline covariates, and have low (or no) bias in finite samples even when those predictions are themselves biased.","sentences":["In May 2023, the U.S. Food and Drug Administration (FDA) released guidance for industry on \"Adjustment for Covariates in Randomized Clinical Trials for Drugs and Biological Products\".","Covariate adjustment is a statistical analysis method for improving precision and power in clinical trials by adjusting for pre-specified, prognostic baseline variables.","Though recommended by the FDA and the European Medicines Agency (EMA), many trials do not exploit the available information in baseline variables or make use only of the baseline measurement of the outcome.","This is likely (partly) due to the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms.","We will explore the potential of automated data-adaptive methods, such as machine learning algorithms, for covariate adjustment, addressing the challenge of pre-specification.","Specifically, our approach allows the use of complex models or machine learning algorithms without compromising the interpretation or validity of the treatment effect estimate and its corresponding standard error, even in the presence of misspecified outcome working models.","This contrasts the majority of competing works which assume correct model specification for the validity of standard errors.","Our proposed estimators either necessitate ultra-sparsity in the outcome model (which can be relaxed by limiting the number of predictors in the model) or necessitate integration with sample splitting to enhance their performance.","As such, we will arrive at simple estimators and standard errors for the marginal treatment effect in randomized clinical trials, which exploit data-adaptive outcome predictions based on prognostic baseline covariates, and have low (or no) bias in finite samples even when those predictions are themselves biased."],"url":"http://arxiv.org/abs/2404.11150v1","category":"stat.ME"}
{"created":"2024-04-17 07:39:14","title":"[DC] bRight XR: How to train designers to keep on the bright side?","abstract":"This research project aims to promote ethical principles among designers engaged in adaptive-XR by providing tools for self-assessment. We introduce a Design-Based Research (DBR) methodology to build bRight-XR, a framework including a heuristic evaluation matrix and based on learning theory.","sentences":["This research project aims to promote ethical principles among designers engaged in adaptive-XR by providing tools for self-assessment.","We introduce a Design-Based Research (DBR) methodology to build bRight-XR, a framework including a heuristic evaluation matrix and based on learning theory."],"url":"http://arxiv.org/abs/2404.11142v1","category":"cs.ET"}
{"created":"2024-04-17 07:13:49","title":"Rational curves on complete intersections and the circle method","abstract":"We study the geometry of the space of rational curves on smooth complete intersections of low degree, which pass through a given set of points on the variety. The argument uses spreading out to a finite field, together with an adaptation to function fields of positive characteristic of work by Rydin Myerson on the circle method. Our work also allows us to handle weak approximation for such varieties.","sentences":["We study the geometry of the space of rational curves on smooth complete intersections of low degree, which pass through a given set of points on the variety.","The argument uses spreading out to a finite field, together with an adaptation to function fields of positive characteristic of work by Rydin Myerson on the circle method.","Our work also allows us to handle weak approximation for such varieties."],"url":"http://arxiv.org/abs/2404.11123v1","category":"math.AG"}
{"created":"2024-04-17 06:58:32","title":"An Adaptive Regularized Proximal Newton-Type Methods for Composite Optimization over the Stiefel Manifold","abstract":"Recently, the proximal Newton-type method and its variants have been generalized to solve composite optimization problems over the Stiefel manifold whose objective function is the summation of a smooth function and a nonsmooth function. In this paper, we propose an adaptive quadratically regularized proximal quasi-Newton method, named ARPQN, to solve this class of problems. Under some mild assumptions, the global convergence, the local linear convergence rate and the iteration complexity of ARPQN are established. Numerical experiments and comparisons with other state-of-the-art methods indicate that ARPQN is very promising. We also propose an adaptive quadratically regularized proximal Newton method, named ARPN. It is shown the ARPN method has a local superlinear convergence rate under certain reasonable assumptions, which demonstrates attractive convergence properties of regularized proximal Newton methods.","sentences":["Recently, the proximal Newton-type method and its variants have been generalized to solve composite optimization problems over the Stiefel manifold whose objective function is the summation of a smooth function and a nonsmooth function.","In this paper, we propose an adaptive quadratically regularized proximal quasi-Newton method, named ARPQN, to solve this class of problems.","Under some mild assumptions, the global convergence, the local linear convergence rate and the iteration complexity of ARPQN are established.","Numerical experiments and comparisons with other state-of-the-art methods indicate that ARPQN is very promising.","We also propose an adaptive quadratically regularized proximal Newton method, named ARPN.","It is shown the ARPN method has a local superlinear convergence rate under certain reasonable assumptions, which demonstrates attractive convergence properties of regularized proximal Newton methods."],"url":"http://arxiv.org/abs/2404.11112v1","category":"math.OC"}
{"created":"2024-04-17 06:57:57","title":"CorrNet+: Sign Language Recognition and Translation via Spatial-Temporal Correlation","abstract":"In sign language, the conveyance of human body trajectories predominantly relies upon the coordinated movements of hands and facial expressions across successive frames. Despite the recent advancements of sign language understanding methods, they often solely focus on individual frames, inevitably overlooking the inter-frame correlations that are essential for effectively modeling human body trajectories. To address this limitation, this paper introduces a spatial-temporal correlation network, denoted as CorrNet+, which explicitly identifies body trajectories across multiple frames. In specific, CorrNet+ employs a correlation module and an identification module to build human body trajectories. Afterwards, a temporal attention module is followed to adaptively evaluate the contributions of different frames. The resultant features offer a holistic perspective on human body movements, facilitating a deeper understanding of sign language. As a unified model, CorrNet+ achieves new state-of-the-art performance on two extensive sign language understanding tasks, including continuous sign language recognition (CSLR) and sign language translation (SLT). Especially, CorrNet+ surpasses previous methods equipped with resource-intensive pose-estimation networks or pre-extracted heatmaps for hand and facial feature extraction. Compared with CorrNet, CorrNet+ achieves a significant performance boost across all benchmarks while halving the computational overhead. A comprehensive comparison with previous spatial-temporal reasoning methods verifies the superiority of CorrNet+. Code is available at https://github.com/hulianyuyy/CorrNet_Plus.","sentences":["In sign language, the conveyance of human body trajectories predominantly relies upon the coordinated movements of hands and facial expressions across successive frames.","Despite the recent advancements of sign language understanding methods, they often solely focus on individual frames, inevitably overlooking the inter-frame correlations that are essential for effectively modeling human body trajectories.","To address this limitation, this paper introduces a spatial-temporal correlation network, denoted as CorrNet+, which explicitly identifies body trajectories across multiple frames.","In specific, CorrNet+ employs a correlation module and an identification module to build human body trajectories.","Afterwards, a temporal attention module is followed to adaptively evaluate the contributions of different frames.","The resultant features offer a holistic perspective on human body movements, facilitating a deeper understanding of sign language.","As a unified model, CorrNet+ achieves new state-of-the-art performance on two extensive sign language understanding tasks, including continuous sign language recognition (CSLR) and sign language translation (SLT).","Especially, CorrNet+ surpasses previous methods equipped with resource-intensive pose-estimation networks or pre-extracted heatmaps for hand and facial feature extraction.","Compared with CorrNet, CorrNet+ achieves a significant performance boost across all benchmarks while halving the computational overhead.","A comprehensive comparison with previous spatial-temporal reasoning methods verifies the superiority of CorrNet+.","Code is available at https://github.com/hulianyuyy/CorrNet_Plus."],"url":"http://arxiv.org/abs/2404.11111v1","category":"cs.CV"}
{"created":"2024-04-17 06:06:36","title":"High fidelity adaptive mirror simulations with reduced order models","abstract":"In the design process of large adaptive mirrors numerical simulations represent the first step to evaluate the system design compliance in terms of performance, stability and robustness. For the next generation of Extremely Large Telescopes increased system dimensions and bandwidths lead to the need of modeling not only the deformable mirror alone, but also all the system supporting structure or even the full telescope. The capability to perform the simulations with an acceptable amount of time and computational resources is highly dependent on finding appropriate methods to reduce the size of the resulting dynamic models. In this paper we present a framework developed together with the company Microgate to create a reduced order structural model of a large adaptive mirror as a preprocessing step to the control system simulations. The reduced dynamic model is then combined with the remaining system components allowing to simulate the full adaptive mirror in a computationally efficient way. We analyze the feasibility of our reduced models for Microgate's prototype of the adaptive mirror of the Giant Magellan Telescope.","sentences":["In the design process of large adaptive mirrors numerical simulations represent the first step to evaluate the system design compliance in terms of performance, stability and robustness.","For the next generation of Extremely Large Telescopes increased system dimensions and bandwidths lead to the need of modeling not only the deformable mirror alone, but also all the system supporting structure or even the full telescope.","The capability to perform the simulations with an acceptable amount of time and computational resources is highly dependent on finding appropriate methods to reduce the size of the resulting dynamic models.","In this paper we present a framework developed together with the company Microgate to create a reduced order structural model of a large adaptive mirror as a preprocessing step to the control system simulations.","The reduced dynamic model is then combined with the remaining system components allowing to simulate the full adaptive mirror in a computationally efficient way.","We analyze the feasibility of our reduced models for Microgate's prototype of the adaptive mirror of the Giant Magellan Telescope."],"url":"http://arxiv.org/abs/2404.11088v1","category":"astro-ph.IM"}
{"created":"2024-04-17 05:50:11","title":"Estimating conditional hazard functions and densities with the highly-adaptive lasso","abstract":"We consider estimation of conditional hazard functions and densities over the class of multivariate c\\`adl\\`ag functions with uniformly bounded sectional variation norm when data are either fully observed or subject to right-censoring. We demonstrate that the empirical risk minimizer is either not well-defined or not consistent for estimation of conditional hazard functions and densities. Under a smoothness assumption about the data-generating distribution, a highly-adaptive lasso estimator based on a particular data-adaptive sieve achieves the same convergence rate as has been shown to hold for the empirical risk minimizer in settings where the latter is well-defined. We use this result to study a highly-adaptive lasso estimator of a conditional hazard function based on right-censored data. We also propose a new conditional density estimator and derive its convergence rate. Finally, we show that the result is of interest also for settings where the empirical risk minimizer is well-defined, because the highly-adaptive lasso depends on a much smaller number of basis function than the empirical risk minimizer.","sentences":["We consider estimation of conditional hazard functions and densities over the class of multivariate c\\`adl\\`ag functions with uniformly bounded sectional variation norm when data are either fully observed or subject to right-censoring.","We demonstrate that the empirical risk minimizer is either not well-defined or not consistent for estimation of conditional hazard functions and densities.","Under a smoothness assumption about the data-generating distribution, a highly-adaptive lasso estimator based on a particular data-adaptive sieve achieves the same convergence rate as has been shown to hold for the empirical risk minimizer in settings where the latter is well-defined.","We use this result to study a highly-adaptive lasso estimator of a conditional hazard function based on right-censored data.","We also propose a new conditional density estimator and derive its convergence rate.","Finally, we show that the result is of interest also for settings where the empirical risk minimizer is well-defined, because the highly-adaptive lasso depends on a much smaller number of basis function than the empirical risk minimizer."],"url":"http://arxiv.org/abs/2404.11083v1","category":"math.ST"}
{"created":"2024-04-17 03:39:51","title":"Offset Unlearning for Large Language Models","abstract":"Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.","sentences":["Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns.","In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data.","However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction.","We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs.","Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models.","Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks.","$\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs."],"url":"http://arxiv.org/abs/2404.11045v1","category":"cs.CL"}
{"created":"2024-04-17 03:39:02","title":"Asynchronous Memory Access Unit: Exploiting Massive Parallelism for Far Memory Access","abstract":"The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions. However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM. For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency. While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity. The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time. The longer far memory latencies exacerbate this limitation.   This paper proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside a contemporary Out-of-Order Core. AMI separates memory request issuing from response handling to reduce resource occupation. Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage. Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies.   Evaluation with a cycle-accurate simulation shows AMI achieves 2.42x speedup on average for memory-bound benchmarks with 1us additional far memory latency. Over 130 outstanding requests are supported with 26.86x speedup for GUPS (random access) with 5 us latency. These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation.","sentences":["The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions.","However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM.","For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency.","While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity.","The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time.","The longer far memory latencies exacerbate this limitation.   ","This paper proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside a contemporary Out-of-Order Core.","AMI separates memory request issuing from response handling to reduce resource occupation.","Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage.","Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies.   ","Evaluation with a cycle-accurate simulation shows AMI achieves 2.42x speedup on average for memory-bound benchmarks with 1us additional far memory latency.","Over 130 outstanding requests are supported with 26.86x speedup for GUPS (random access) with 5 us latency.","These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation."],"url":"http://arxiv.org/abs/2404.11044v1","category":"cs.AR"}
{"created":"2024-04-17 03:25:54","title":"Cross-Platform Hate Speech Detection with Weakly Supervised Causal Disentanglement","abstract":"Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity. With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited. In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators. However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech. Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate. Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance. HATE WATCH advances scalable content moderation techniques towards developing safer online communities.","sentences":["Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity.","With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited.","In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators.","However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech.","Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate.","Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance.","HATE WATCH advances scalable content moderation techniques towards developing safer online communities."],"url":"http://arxiv.org/abs/2404.11036v1","category":"cs.LG"}
{"created":"2024-04-17 02:57:31","title":"The Radius Distribution of M dwarf-hosted Planets and its Evolution","abstract":"M dwarf stars are not only the most promising hosts for detection and characterization of small and potentially habitable planets, they provide leverage relative to solar-type stars to test models of planet formation and evolution. Using Gaia astrometry, adaptive optics imaging, and calibrated gyrochronologic relations to estimate stellar properties, filter binaries, and assign ages, we refined the radii of 179 transiting planets orbiting 119 single late K- and early M-type stars detected by the Kepler mission, and assigned stellar rotation-based ages ) to 115 of these. We constructed the radius distribution of <4R$_{\\oplus}$ planets and assessed its evolution with time. As for solar-type stars, the inferred distribution contains distinct populations of \"super-Earths\" (at ~1.3R$_{\\oplus}$) and \"sub-Neptunes\" (at ~2.2Rearth) separated by a gap or \"valley\" at $\\approx$1.7R$_{\\oplus}$ that has a period dependence that is significantly weaker (power law index of -0.026$^{+0.026}_{-0.017}$) than for solar-type stars. Sub-Neptunes are largely absent at short periods ($<$2 days) and high irradiance, a feature analogous to the \"Neptune desert\" observed around solar-type stars. The relative number of sub-Neptunes to super-Earths declines between the younger and older halves of the sample (median age 3.8 Gyr), although the formal significance is low ($p = 0.06$) because of the small sample size. The decline in sub-Neptunes appears to be more pronounced at long orbital periods vs. short periods; this is not due to detection bias and could indicate that these objects are inflated by a mechanism that operates at elevated irradiance, e.g. a runaway water greenhouse augmented by H/He.","sentences":["M dwarf stars are not only the most promising hosts for detection and characterization of small and potentially habitable planets, they provide leverage relative to solar-type stars to test models of planet formation and evolution.","Using Gaia astrometry, adaptive optics imaging, and calibrated gyrochronologic relations to estimate stellar properties, filter binaries, and assign ages, we refined the radii of 179 transiting planets orbiting 119 single late K- and early M-type stars detected by the Kepler mission, and assigned stellar rotation-based ages ) to 115 of these.","We constructed the radius distribution of <4R$_{\\oplus}$ planets and assessed its evolution with time.","As for solar-type stars, the inferred distribution contains distinct populations of \"super-Earths\" (at ~1.3R$_{\\oplus}$) and \"sub-Neptunes\" (at ~2.2Rearth) separated by a gap or \"valley\" at $\\approx$1.7R$_{\\oplus}$ that has a period dependence that is significantly weaker (power law index of -0.026$^{+0.026}_{-0.017}$) than for solar-type stars.","Sub-Neptunes are largely absent at short periods ($<$2 days) and high irradiance, a feature analogous to the \"Neptune desert\" observed around solar-type stars.","The relative number of sub-Neptunes to super-Earths declines between the younger and older halves of the sample (median age 3.8 Gyr), although the formal significance is low ($p = 0.06$) because of the small sample size.","The decline in sub-Neptunes appears to be more pronounced at long orbital periods vs. short periods; this is not due to detection bias and could indicate that these objects are inflated by a mechanism that operates at elevated irradiance, e.g. a runaway water greenhouse augmented by H/He."],"url":"http://arxiv.org/abs/2404.11022v1","category":"astro-ph.EP"}
{"created":"2024-04-17 02:44:25","title":"Control Theoretic Approach to Fine-Tuning and Transfer Learning","abstract":"Given a training set in the form of a paired $(\\mathcal{X},\\mathcal{Y})$, we say that the control system $\\dot{x} = f(x,u)$ has learned the paired set via the control $u^*$ if the system steers each point of $\\mathcal{X}$ to its corresponding target in $\\mathcal{Y}$. Most existing methods for finding a control function $u^*$ require learning of a new control function if the training set is updated. To overcome this limitation, we introduce the concept of $\\textit{tuning without forgetting}$. We develop $\\textit{an iterative algorithm}$ to tune the control function $u^*$ when the training set expands, whereby points already in the paired set are still matched, and new training samples are learned. More specifically, at each update of our method, the control $u^*$ is projected onto the kernel of the end-point mapping generated by the controlled dynamics at the learned samples. It ensures keeping the end points for the previously learned samples constant while iteratively learning additional samples. Our work contributes to the scalability of control methods, offering a novel approach to adaptively handle training set expansions.","sentences":["Given a training set in the form of a paired $(\\mathcal{X},\\mathcal{Y})$, we say that the control system $\\dot{x} = f(x,u)$ has learned the paired set via the control $u^*$ if the system steers each point of $\\mathcal{X}$ to its corresponding target in $\\mathcal{Y}$. Most existing methods for finding a control function $u^*$ require learning of a new control function if the training set is updated.","To overcome this limitation, we introduce the concept of $\\textit{tuning without forgetting}$. We develop $\\textit{an iterative algorithm}$ to tune the control function $u^*$ when the training set expands, whereby points already in the paired set are still matched, and new training samples are learned.","More specifically, at each update of our method, the control $u^*$ is projected onto the kernel of the end-point mapping generated by the controlled dynamics at the learned samples.","It ensures keeping the end points for the previously learned samples constant while iteratively learning additional samples.","Our work contributes to the scalability of control methods, offering a novel approach to adaptively handle training set expansions."],"url":"http://arxiv.org/abs/2404.11013v1","category":"cs.LG"}
{"created":"2024-04-17 01:31:00","title":"Graph Continual Learning with Debiased Lossless Memory Replay","abstract":"Real-life graph data often expands continually, rendering the learning of graph neural networks (GNNs) on static graph data impractical. Graph continual learning (GCL) tackles this problem by continually adapting GNNs to the expanded graph of the current task while maintaining the performance over the graph of previous tasks. Memory replay-based methods, which aim to replay data of previous tasks when learning new tasks, have been explored as one principled approach to mitigate the forgetting of the knowledge learned from the previous tasks. In this paper we extend this methodology with a novel framework, called Debiased Lossless Memory replay (DeLoMe). Unlike existing methods that sample nodes/edges of previous graphs to construct the memory, DeLoMe learns small lossless synthetic node representations as the memory. The learned memory can not only preserve the graph data privacy but also capture the holistic graph information, for which the sampling-based methods are not viable. Further, prior methods suffer from bias toward the current task due to the data imbalance between the classes in the memory data and the current data. A debiased GCL loss function is devised in DeLoMe to effectively alleviate this bias. Extensive experiments on four graph datasets show the effectiveness of DeLoMe under both class- and task-incremental learning settings.","sentences":["Real-life graph data often expands continually, rendering the learning of graph neural networks (GNNs) on static graph data impractical.","Graph continual learning (GCL) tackles this problem by continually adapting GNNs to the expanded graph of the current task while maintaining the performance over the graph of previous tasks.","Memory replay-based methods, which aim to replay data of previous tasks when learning new tasks, have been explored as one principled approach to mitigate the forgetting of the knowledge learned from the previous tasks.","In this paper we extend this methodology with a novel framework, called Debiased Lossless Memory replay (DeLoMe).","Unlike existing methods that sample nodes/edges of previous graphs to construct the memory, DeLoMe learns small lossless synthetic node representations as the memory.","The learned memory can not only preserve the graph data privacy but also capture the holistic graph information, for which the sampling-based methods are not viable.","Further, prior methods suffer from bias toward the current task due to the data imbalance between the classes in the memory data and the current data.","A debiased GCL loss function is devised in DeLoMe to effectively alleviate this bias.","Extensive experiments on four graph datasets show the effectiveness of DeLoMe under both class- and task-incremental learning settings."],"url":"http://arxiv.org/abs/2404.10984v1","category":"cs.LG"}
{"created":"2024-04-17 01:29:20","title":"Remote Cross-resonance Gate between Superconducting Fixed-frequency Qubits","abstract":"High-fidelity quantum state transfer and remote entanglement between superconducting fixed-frequency qubits have not yet been realized. In this study, we propose an alternative remote cross-resonance gate. Considering multiple modes of a superconducting coaxial cable connecting qubits, we must find conditions under which the cross-resonance gate operates with a certain accuracy even in the presence of qubit frequency shifts due to manufacturing errors. For 0.25- and 0.5-m cables, remote cross-resonance gates with a concurrence of $>99.9\\%$ in entanglement generation are obtained even with $\\pm$10-MHz frequency shifts. For a 1-m cable with a narrow mode spacing, a concurrence of 99.5\\% is achieved by reducing the coupling between the qubits and cable. The optimized echoed raised-cosine pulse duration is 150--400 ns, which is similar to the operation time of cross-resonance gates between neighboring qubits on a chip. The dissipation through the cable modes does not considerably affect the obtained results. Such high-precision quantum interconnects pave the way not only for scaling up quantum computer systems but also for nonlocal connections on a chip.","sentences":["High-fidelity quantum state transfer and remote entanglement between superconducting fixed-frequency qubits have not yet been realized.","In this study, we propose an alternative remote cross-resonance gate.","Considering multiple modes of a superconducting coaxial cable connecting qubits, we must find conditions under which the cross-resonance gate operates with a certain accuracy even in the presence of qubit frequency shifts due to manufacturing errors.","For 0.25- and 0.5-m cables, remote cross-resonance gates with a concurrence of $>99.9\\%$ in entanglement generation are obtained even with $\\pm$10-MHz frequency shifts.","For a 1-m cable with a narrow mode spacing, a concurrence of 99.5\\% is achieved by reducing the coupling between the qubits and cable.","The optimized echoed raised-cosine pulse duration is 150--400 ns, which is similar to the operation time of cross-resonance gates between neighboring qubits on a chip.","The dissipation through the cable modes does not considerably affect the obtained results.","Such high-precision quantum interconnects pave the way not only for scaling up quantum computer systems but also for nonlocal connections on a chip."],"url":"http://arxiv.org/abs/2404.10983v1","category":"quant-ph"}
{"created":"2024-04-17 00:21:36","title":"Domain-Specific Block Selection and Paired-View Pseudo-Labeling for Online Test-Time Adaptation","abstract":"Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment. Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data. Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed. In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images. Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization. After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts. By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation. Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively. Also, we provide an extensive analysis to demonstrate effectiveness of our framework. Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA.","sentences":["Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment.","Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data.","Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed.","In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images.","Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization.","After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts.","By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation.","Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively.","Also, we provide an extensive analysis to demonstrate effectiveness of our framework.","Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA."],"url":"http://arxiv.org/abs/2404.10966v1","category":"cs.CV"}
{"created":"2024-04-17 00:18:48","title":"IMIL: Interactive Medical Image Learning Framework","abstract":"Data augmentations are widely used in training medical image deep learning models to increase the diversity and size of sparse datasets. However, commonly used augmentation techniques can result in loss of clinically relevant information from medical images, leading to incorrect predictions at inference time. We propose the Interactive Medical Image Learning (IMIL) framework, a novel approach for improving the training of medical image analysis algorithms that enables clinician-guided intermediate training data augmentations on misprediction outliers, focusing the algorithm on relevant visual information. To prevent the model from using irrelevant features during training, IMIL will 'blackout' clinician-designated irrelevant regions and replace the original images with the augmented samples. This ensures that for originally mispredicted samples, the algorithm subsequently attends only to relevant regions and correctly correlates them with the respective diagnosis. We validate the efficacy of IMIL using radiology residents and compare its performance to state-of-the-art data augmentations. A 4.2% improvement in accuracy over ResNet-50 was observed when using IMIL on only 4% of the training set. Our study demonstrates the utility of clinician-guided interactive training to achieve meaningful data augmentations for medical image analysis algorithms.","sentences":["Data augmentations are widely used in training medical image deep learning models to increase the diversity and size of sparse datasets.","However, commonly used augmentation techniques can result in loss of clinically relevant information from medical images, leading to incorrect predictions at inference time.","We propose the Interactive Medical Image Learning (IMIL) framework, a novel approach for improving the training of medical image analysis algorithms that enables clinician-guided intermediate training data augmentations on misprediction outliers, focusing the algorithm on relevant visual information.","To prevent the model from using irrelevant features during training, IMIL will 'blackout' clinician-designated irrelevant regions and replace the original images with the augmented samples.","This ensures that for originally mispredicted samples, the algorithm subsequently attends only to relevant regions and correctly correlates them with the respective diagnosis.","We validate the efficacy of IMIL using radiology residents and compare its performance to state-of-the-art data augmentations.","A 4.2% improvement in accuracy over ResNet-50 was observed when using IMIL on only 4% of the training set.","Our study demonstrates the utility of clinician-guided interactive training to achieve meaningful data augmentations for medical image analysis algorithms."],"url":"http://arxiv.org/abs/2404.10965v1","category":"eess.IV"}
{"created":"2024-04-16 23:30:57","title":"A computational account of the development and evolution of psychotic symptoms","abstract":"The mechanisms of psychotic symptoms like hallucinations and delusions are often investigated in fully-formed illness, well after symptoms emerge. These investigations have yielded key insights, but are not well-positioned to reveal the dynamic forces underlying symptom formation itself. Understanding symptom development over time would allow us to identify steps in the pathophysiological process leading to psychosis, shifting the focus of psychiatric intervention from symptom alleviation to prevention. We propose a model for understanding the emergence of psychotic symptoms within the context of an adaptive, developing neural system. We will make the case for a pathophysiological process that begins with cortical hyperexcitability and bottom-up noise transmission, which engenders inappropriate belief formation via aberrant prediction error signaling. We will argue that this bottom-up noise drives learning about the (im)precision of new incoming sensory information because of diminished signal-to-noise ratio, causing an adaptive relative over-reliance on prior beliefs. This over-reliance on priors predisposes to hallucinations and covaries with hallucination severity. An over-reliance on priors may also lead to increased conviction in the beliefs generated by bottom-up noise and drive movement toward conversion to psychosis. We will identify predictions of our model at each stage, examine evidence to support or refute those predictions, and propose experiments that could falsify or help select between alternative elements of the overall model. Nesting computational abnormalities within longitudinal development allows us to account for hidden dynamics among the mechanisms driving symptom formation and to view established symptomatology as a point of equilibrium among competing biological forces.","sentences":["The mechanisms of psychotic symptoms like hallucinations and delusions are often investigated in fully-formed illness, well after symptoms emerge.","These investigations have yielded key insights, but are not well-positioned to reveal the dynamic forces underlying symptom formation itself.","Understanding symptom development over time would allow us to identify steps in the pathophysiological process leading to psychosis, shifting the focus of psychiatric intervention from symptom alleviation to prevention.","We propose a model for understanding the emergence of psychotic symptoms within the context of an adaptive, developing neural system.","We will make the case for a pathophysiological process that begins with cortical hyperexcitability and bottom-up noise transmission, which engenders inappropriate belief formation via aberrant prediction error signaling.","We will argue that this bottom-up noise drives learning about the (im)precision of new incoming sensory information because of diminished signal-to-noise ratio, causing an adaptive relative over-reliance on prior beliefs.","This over-reliance on priors predisposes to hallucinations and covaries with hallucination severity.","An over-reliance on priors may also lead to increased conviction in the beliefs generated by bottom-up noise and drive movement toward conversion to psychosis.","We will identify predictions of our model at each stage, examine evidence to support or refute those predictions, and propose experiments that could falsify or help select between alternative elements of the overall model.","Nesting computational abnormalities within longitudinal development allows us to account for hidden dynamics among the mechanisms driving symptom formation and to view established symptomatology as a point of equilibrium among competing biological forces."],"url":"http://arxiv.org/abs/2404.10954v1","category":"q-bio.NC"}
{"created":"2024-04-16 23:29:23","title":"Limit points of $A_\u03b1$-matrices of graphs","abstract":"We study limit points of the spectral radii of $A_{\\alpha}$-matrices of graphs. Adapting a method used by J. B. Shearer in 1989, we prove a density property of $A_{\\alpha}$-limit points of caterpillars for $\\alpha$ close to zero. Precisely, we show that for $\\alpha \\in [0, 1/2)$ there exists a positive number $\\tau_2(\\alpha)>2$ such that any value $\\lambda> \\tau_2(\\alpha)$ is an $A_{\\alpha}$-limit point. We also determine the existence of other intervals for which all its points are $A_{\\alpha}$-limit points.","sentences":["We study limit points of the spectral radii of $A_{\\alpha}$-matrices of graphs.","Adapting a method used by J. B. Shearer in 1989, we prove a density property of $A_{\\alpha}$-limit points of caterpillars for $\\alpha$ close to zero.","Precisely, we show that for $\\alpha \\in [0, 1/2)$ there exists a positive number $\\tau_2(\\alpha)>2$ such that any value $\\lambda> \\tau_2(\\alpha)$ is an $A_{\\alpha}$-limit point.","We also determine the existence of other intervals for which all its points are $A_{\\alpha}$-limit points."],"url":"http://arxiv.org/abs/2404.10953v1","category":"math.CO"}
{"created":"2024-04-16 21:20:15","title":"Adaptive Kalman Filtering Developed from Recursive Least Squares Forgetting Algorithms","abstract":"Recursive least squares (RLS) is derived as the recursive minimizer of the least-squares cost function. Moreover, it is well known that RLS is a special case of the Kalman filter. This work presents the Kalman filter least squares (KFLS) cost function, whose recursive minimizer gives the Kalman filter. KFLS is an extension of generalized forgetting recursive least squares (GF-RLS), a general framework which contains various extensions of RLS from the literature as special cases. This then implies that extensions of RLS are also special cases of the Kalman filter. Motivated by this connection, we propose an algorithm that combines extensions of RLS with the Kalman filter, resulting in a new class of adaptive Kalman filters. A numerical example shows that one such adaptive Kalman filter provides improved state estimation for a mass-spring-damper with intermittent, unmodeled collisions. This example suggests that such adaptive Kalman filtering may provide potential benefits for systems with non-classical disturbances.","sentences":["Recursive least squares (RLS) is derived as the recursive minimizer of the least-squares cost function.","Moreover, it is well known that RLS is a special case of the Kalman filter.","This work presents the Kalman filter least squares (KFLS) cost function, whose recursive minimizer gives the Kalman filter.","KFLS is an extension of generalized forgetting recursive least squares (GF-RLS), a general framework which contains various extensions of RLS from the literature as special cases.","This then implies that extensions of RLS are also special cases of the Kalman filter.","Motivated by this connection, we propose an algorithm that combines extensions of RLS with the Kalman filter, resulting in a new class of adaptive Kalman filters.","A numerical example shows that one such adaptive Kalman filter provides improved state estimation for a mass-spring-damper with intermittent, unmodeled collisions.","This example suggests that such adaptive Kalman filtering may provide potential benefits for systems with non-classical disturbances."],"url":"http://arxiv.org/abs/2404.10914v1","category":"eess.SY"}
{"created":"2024-04-16 21:17:59","title":"Constructing $\\mathrm{NP}^{\\mathord{\\#}\\mathrm P}$-complete problems and ${\\mathord{\\#}\\mathrm P}$-hardness of circuit extraction in phase-free ZH","abstract":"The ZH calculus is a graphical language for quantum computation reasoning. The phase-free variant offers a simple set of generators that guarantee universality. ZH calculus is effective in MBQC and analysis of quantum circuits constructed with the universal gate set Toffoli+H. While circuits naturally translate to ZH diagrams, finding an ancilla-free circuit equivalent to a given diagram is hard. Here, we show that circuit extraction for phase-free ZH calculus is ${\\mathord{\\#}\\mathrm P}$-hard, extending the existing result for ZX calculus. Another problem believed to be hard is comparing whether two diagrams represent the same process. We show that two closely related problems are $\\mathrm{NP}^{\\mathord{\\#}\\mathrm P}$-complete. The first problem is: given two processes represented as diagrams, determine the existence of a computational basis state on which they equalize. The second problem is checking whether the matrix representation of a given diagram contains an entry equal to a given number. Our proof adapts the proof of Cook-Levin theorem to a reduction from a non-deterministic Turing Machine with access to ${\\mathord{\\#}\\mathrm P}$ oracle.","sentences":["The ZH calculus is a graphical language for quantum computation reasoning.","The phase-free variant offers a simple set of generators that guarantee universality.","ZH calculus is effective in MBQC and analysis of quantum circuits constructed with the universal gate set Toffoli+H. While circuits naturally translate to ZH diagrams, finding an ancilla-free circuit equivalent to a given diagram is hard.","Here, we show that circuit extraction for phase-free ZH calculus is ${\\mathord{\\#}\\mathrm P}$-hard, extending the existing result for ZX calculus.","Another problem believed to be hard is comparing whether two diagrams represent the same process.","We show that two closely related problems are $\\mathrm{NP}^{\\mathord{\\#}\\mathrm P}$-complete.","The first problem is: given two processes represented as diagrams, determine the existence of a computational basis state on which they equalize.","The second problem is checking whether the matrix representation of a given diagram contains an entry equal to a given number.","Our proof adapts the proof of Cook-Levin theorem to a reduction from a non-deterministic Turing Machine with access to ${\\mathord{\\#}\\mathrm P}$ oracle."],"url":"http://arxiv.org/abs/2404.10913v1","category":"quant-ph"}
{"created":"2024-04-16 21:12:54","title":"Efficient Batch and Recursive Least Squares for Matrix Parameter Estimation with Application to Adaptive MPC","abstract":"Traditionally, batch least squares (BLS) and recursive least squares (RLS) are used for identification of a vector of parameters that form a linear model. In some situations, however, it is of interest to identify parameters in a matrix structure. In this case, a common approach is to transform the problem into standard vector form using the vectorization (vec) operator and the Kronecker product, known as vec-permutation. However, the use of the Kronecker product introduces extraneous zero terms in the regressor, resulting in unnecessary additional computational and space requirements. This work derives matrix BLS and RLS formulations which, under mild assumptions, minimize the same cost as the vec-permutation approach. This new approach requires less computational complexity and space complexity than vec-permutation in both BLS and RLS identification. It is also shown that persistent excitation guarantees convergence to the true matrix parameters. This method can used to improve computation time in the online identification of multiple-input, multiple-output systems for indirect adaptive model predictive control.","sentences":["Traditionally, batch least squares (BLS) and recursive least squares (RLS) are used for identification of a vector of parameters that form a linear model.","In some situations, however, it is of interest to identify parameters in a matrix structure.","In this case, a common approach is to transform the problem into standard vector form using the vectorization (vec) operator and the Kronecker product, known as vec-permutation.","However, the use of the Kronecker product introduces extraneous zero terms in the regressor, resulting in unnecessary additional computational and space requirements.","This work derives matrix BLS and RLS formulations which, under mild assumptions, minimize the same cost as the vec-permutation approach.","This new approach requires less computational complexity and space complexity than vec-permutation in both BLS and RLS identification.","It is also shown that persistent excitation guarantees convergence to the true matrix parameters.","This method can used to improve computation time in the online identification of multiple-input, multiple-output systems for indirect adaptive model predictive control."],"url":"http://arxiv.org/abs/2404.10911v1","category":"eess.SP"}
{"created":"2024-04-16 20:37:14","title":"Semantics-Aware Attention Guidance for Diagnosing Whole Slide Images","abstract":"Accurate cancer diagnosis remains a critical challenge in digital pathology, largely due to the gigapixel size and complex spatial relationships present in whole slide images. Traditional multiple instance learning (MIL) methods often struggle with these intricacies, especially in preserving the necessary context for accurate diagnosis. In response, we introduce a novel framework named Semantics-Aware Attention Guidance (SAG), which includes 1) a technique for converting diagnostically relevant entities into attention signals, and 2) a flexible attention loss that efficiently integrates various semantically significant information, such as tissue anatomy and cancerous regions. Our experiments on two distinct cancer datasets demonstrate consistent improvements in accuracy, precision, and recall with two state-of-the-art baseline models. Qualitative analysis further reveals that the incorporation of heuristic guidance enables the model to focus on regions critical for diagnosis. SAG is not only effective for the models discussed here, but its adaptability extends to any attention-based diagnostic model. This opens up exciting possibilities for further improving the accuracy and efficiency of cancer diagnostics.","sentences":["Accurate cancer diagnosis remains a critical challenge in digital pathology, largely due to the gigapixel size and complex spatial relationships present in whole slide images.","Traditional multiple instance learning (MIL) methods often struggle with these intricacies, especially in preserving the necessary context for accurate diagnosis.","In response, we introduce a novel framework named Semantics-Aware Attention Guidance (SAG), which includes 1) a technique for converting diagnostically relevant entities into attention signals, and 2) a flexible attention loss that efficiently integrates various semantically significant information, such as tissue anatomy and cancerous regions.","Our experiments on two distinct cancer datasets demonstrate consistent improvements in accuracy, precision, and recall with two state-of-the-art baseline models.","Qualitative analysis further reveals that the incorporation of heuristic guidance enables the model to focus on regions critical for diagnosis.","SAG is not only effective for the models discussed here, but its adaptability extends to any attention-based diagnostic model.","This opens up exciting possibilities for further improving the accuracy and efficiency of cancer diagnostics."],"url":"http://arxiv.org/abs/2404.10894v1","category":"cs.CV"}
{"created":"2024-04-16 19:24:37","title":"Numerical methods and improvements for simulating quasi-static elastoplastic materials","abstract":"Hypo-elastoplasticity is a framework suitable for modeling the mechanics of many hard materials that have small elastic deformation and large plastic deformation. In most laboratory tests for these materials the Cauchy stress is in quasi-static equilibrium. Rycroft et al. discovered a mathematical correspondence between this physical system and the incompressible Navier-Stokes equations, and developed a projection method similar to Chorin's projection method (1968) for incompressible Newtonian fluids. Here, we improve the original projection method to simulate quasi-static hypo-elastoplasticity, by making three improvements. First, drawing inspiration from the second-order projection method for incompressible Newtonian fluids, we formulate a second-order in time numerical scheme for quasi-static hypo-elastoplasticity. Second, we implement a finite element method for solving the elliptic equations in the projection step, which provides both numerical benefits and flexibility. Third, we develop an adaptive global time-stepping scheme, which can compute accurate solutions in fewer timesteps. Our numerical tests use an example physical model of a bulk metallic glass based on the shear transformation zone theory, but the numerical methods can be applied to any elastoplastic material.","sentences":["Hypo-elastoplasticity is a framework suitable for modeling the mechanics of many hard materials that have small elastic deformation and large plastic deformation.","In most laboratory tests for these materials the Cauchy stress is in quasi-static equilibrium.","Rycroft et al. discovered a mathematical correspondence between this physical system and the incompressible Navier-Stokes equations, and developed a projection method similar to Chorin's projection method (1968) for incompressible Newtonian fluids.","Here, we improve the original projection method to simulate quasi-static hypo-elastoplasticity, by making three improvements.","First, drawing inspiration from the second-order projection method for incompressible Newtonian fluids, we formulate a second-order in time numerical scheme for quasi-static hypo-elastoplasticity.","Second, we implement a finite element method for solving the elliptic equations in the projection step, which provides both numerical benefits and flexibility.","Third, we develop an adaptive global time-stepping scheme, which can compute accurate solutions in fewer timesteps.","Our numerical tests use an example physical model of a bulk metallic glass based on the shear transformation zone theory, but the numerical methods can be applied to any elastoplastic material."],"url":"http://arxiv.org/abs/2404.10863v1","category":"physics.comp-ph"}
{"created":"2024-04-16 19:24:14","title":"Trackable Agent-based Evolution Models at Wafer Scale","abstract":"Continuing improvements in computing hardware are poised to transform capabilities for in silico modeling of cross-scale phenomena underlying major open questions in evolutionary biology and artificial life, such as transitions in individuality, eco-evolutionary dynamics, and rare evolutionary events. Emerging ML/AI-oriented hardware accelerators, like the 850,000 processor Cerebras Wafer Scale Engine (WSE), hold particular promise. However, practical challenges remain in conducting informative evolution experiments that efficiently utilize these platforms' large processor counts. Here, we focus on the problem of extracting phylogenetic information from agent-based evolution on the WSE platform. This goal drove significant refinements to decentralized in silico phylogenetic tracking, reported here. These improvements yield order-of-magnitude performance improvements. We also present an asynchronous island-based genetic algorithm (GA) framework for WSE hardware. Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million agents. We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions. In particular, we demonstrate extraction, from wafer-scale simulation, of clear phylometric signals that differentiate runs with adaptive dynamics enabled versus disabled. Together, these benchmark and validation trials reflect strong potential for highly scalable agent-based evolution simulation that is both efficient and observable. Developed capabilities will bring entirely new classes of previously intractable research questions within reach, benefiting further explorations within the evolutionary biology and artificial life communities across a variety of emerging high-performance computing platforms.","sentences":["Continuing improvements in computing hardware are poised to transform capabilities for in silico modeling of cross-scale phenomena underlying major open questions in evolutionary biology and artificial life, such as transitions in individuality, eco-evolutionary dynamics, and rare evolutionary events.","Emerging ML/AI-oriented hardware accelerators, like the 850,000 processor Cerebras Wafer Scale Engine (WSE), hold particular promise.","However, practical challenges remain in conducting informative evolution experiments that efficiently utilize these platforms' large processor counts.","Here, we focus on the problem of extracting phylogenetic information from agent-based evolution on the WSE platform.","This goal drove significant refinements to decentralized in silico phylogenetic tracking, reported here.","These improvements yield order-of-magnitude performance improvements.","We also present an asynchronous island-based genetic algorithm (GA) framework for WSE hardware.","Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million agents.","We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions.","In particular, we demonstrate extraction, from wafer-scale simulation, of clear phylometric signals that differentiate runs with adaptive dynamics enabled versus disabled.","Together, these benchmark and validation trials reflect strong potential for highly scalable agent-based evolution simulation that is both efficient and observable.","Developed capabilities will bring entirely new classes of previously intractable research questions within reach, benefiting further explorations within the evolutionary biology and artificial life communities across a variety of emerging high-performance computing platforms."],"url":"http://arxiv.org/abs/2404.10861v1","category":"cs.NE"}
{"created":"2024-04-16 19:04:03","title":"Methods to Estimate Cryptic Sequence Complexity","abstract":"Complexity is a signature quality of interest in artificial life systems. Alongside other dimensions of assessment, it is common to quantify genome sites that contribute to fitness as a complexity measure. However, limitations to the sensitivity of fitness assays in models with implicit replication criteria involving rich biotic interactions introduce the possibility of difficult-to-detect ``cryptic'' adaptive sites, which contribute small fitness effects below the threshold of individual detectability or involve epistatic redundancies. Here, we propose three knockout-based assay procedures designed to quantify cryptic adaptive sites within digital genomes. We report initial tests of these methods on a simple genome model with explicitly configured site fitness effects. In these limited tests, estimation results reflect ground truth cryptic sequence complexities well. Presented work provides initial steps toward development of new methods and software tools that improve the resolution, rigor, and tractability of complexity analyses across alife systems, particularly those requiring expensive in situ assessments of organism fitness.","sentences":["Complexity is a signature quality of interest in artificial life systems.","Alongside other dimensions of assessment, it is common to quantify genome sites that contribute to fitness as a complexity measure.","However, limitations to the sensitivity of fitness assays in models with implicit replication criteria involving rich biotic interactions introduce the possibility of difficult-to-detect ``cryptic'' adaptive sites, which contribute small fitness effects below the threshold of individual detectability or involve epistatic redundancies.","Here, we propose three knockout-based assay procedures designed to quantify cryptic adaptive sites within digital genomes.","We report initial tests of these methods on a simple genome model with explicitly configured site fitness effects.","In these limited tests, estimation results reflect ground truth cryptic sequence complexities well.","Presented work provides initial steps toward development of new methods and software tools that improve the resolution, rigor, and tractability of complexity analyses across alife systems, particularly those requiring expensive in situ assessments of organism fitness."],"url":"http://arxiv.org/abs/2404.10854v1","category":"q-bio.PE"}
{"created":"2024-04-16 18:47:07","title":"Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of Micro-UAVs","abstract":"In communication-deprived disaster scenarios, this paper introduces a Micro-Unmanned Aerial Vehicle (UAV)- enhanced content management system. In the absence of cellular infrastructure, this system deploys a hybrid network of stationary and mobile UAVs to offer vital content access to isolated communities. Static anchor UAVs equipped with both vertical and lateral links cater to local users, while agile micro-ferrying UAVs, equipped with lateral links and greater mobility, reach users in various communities. The primary goal is to devise an adaptive content dissemination system that dynamically learns caching policies to maximize content accessibility. The paper proposes a decentralized Top-k Multi-Armed Bandit (Top-k MAB) learning approach for UAV caching decisions, accommodating geotemporal disparities in content popularity and diverse content demands. The proposed mechanism involves a Selective Caching Algorithm that algorithmically reduces redundant copies of the contents by leveraging the shared information between the UAVs. It is demonstrated that Top-k MAB learning, along with selective caching algorithm, can improve system performance while making the learning process adaptive. The paper does functional verification and performance evaluation of the proposed caching framework under a wide range of network size, swarm of micro-ferrying UAVs, and heterogeneous popularity distributions.","sentences":["In communication-deprived disaster scenarios, this paper introduces a Micro-Unmanned Aerial Vehicle (UAV)- enhanced content management system.","In the absence of cellular infrastructure, this system deploys a hybrid network of stationary and mobile UAVs to offer vital content access to isolated communities.","Static anchor UAVs equipped with both vertical and lateral links cater to local users, while agile micro-ferrying UAVs, equipped with lateral links and greater mobility, reach users in various communities.","The primary goal is to devise an adaptive content dissemination system that dynamically learns caching policies to maximize content accessibility.","The paper proposes a decentralized Top-k Multi-Armed Bandit (Top-k MAB) learning approach for UAV caching decisions, accommodating geotemporal disparities in content popularity and diverse content demands.","The proposed mechanism involves a Selective Caching Algorithm that algorithmically reduces redundant copies of the contents by leveraging the shared information between the UAVs.","It is demonstrated that Top-k MAB learning, along with selective caching algorithm, can improve system performance while making the learning process adaptive.","The paper does functional verification and performance evaluation of the proposed caching framework under a wide range of network size, swarm of micro-ferrying UAVs, and heterogeneous popularity distributions."],"url":"http://arxiv.org/abs/2404.10845v1","category":"cs.LG"}
{"created":"2024-04-16 18:30:40","title":"Uncertainty Quantification of Super-Resolution Flow Mapping in Liquid Metals using Ultrasound Localization Microscopy","abstract":"Convection of liquid metals drives large natural processes and is important in technical processes. Model experiments are conducted for research purposes where simulations are expensive and the clarification of open questions requires novel flow mapping methods with an increased spatial resolution. In this work, the method of Ultrasound Localization Microscopy (ULM) is investigated for this purpose. Known from microvasculature imaging, this method provides an increased spatial resolution beyond the diffraction limit. Its applicability in liquid metal flows is promising, however the realization and reliability is challenging, as artificial scattering particles or microbubbles cannot be utilized. To solve this issue an approach using nonlinear adaptive beamforming is proposed. This allowed the reliable tracking of particles of which super-resolved flow maps can be deduced. Furthermore, the application in fluid physics requires quantified results. Therefore, an uncertainty quantification model based on the spatial resolution, velocity gradient and measurement parameters is proposed, which allows to estimate the flow maps validity under experimental conditions. The proposed method is demonstrated in magnetohydrodynamic convection experiments. In some occasions, ULM was able to measure velocity vectors within the boundary layer of the flow, which will help for future in-depth flow studies. Furthermore, the proposed uncertainty model of ULM is of generic use in other applications.","sentences":["Convection of liquid metals drives large natural processes and is important in technical processes.","Model experiments are conducted for research purposes where simulations are expensive and the clarification of open questions requires novel flow mapping methods with an increased spatial resolution.","In this work, the method of Ultrasound Localization Microscopy (ULM) is investigated for this purpose.","Known from microvasculature imaging, this method provides an increased spatial resolution beyond the diffraction limit.","Its applicability in liquid metal flows is promising, however the realization and reliability is challenging, as artificial scattering particles or microbubbles cannot be utilized.","To solve this issue an approach using nonlinear adaptive beamforming is proposed.","This allowed the reliable tracking of particles of which super-resolved flow maps can be deduced.","Furthermore, the application in fluid physics requires quantified results.","Therefore, an uncertainty quantification model based on the spatial resolution, velocity gradient and measurement parameters is proposed, which allows to estimate the flow maps validity under experimental conditions.","The proposed method is demonstrated in magnetohydrodynamic convection experiments.","In some occasions, ULM was able to measure velocity vectors within the boundary layer of the flow, which will help for future in-depth flow studies.","Furthermore, the proposed uncertainty model of ULM is of generic use in other applications."],"url":"http://arxiv.org/abs/2404.10840v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 18:22:49","title":"Dynamic Self-adaptive Multiscale Distillation from Pre-trained Multimodal Large Model for Efficient Cross-modal Representation Learning","abstract":"In recent years, pre-trained multimodal large models have attracted widespread attention due to their outstanding performance in various multimodal applications. Nonetheless, the extensive computational resources and vast datasets required for their training present significant hurdles for deployment in environments with limited computational resources. To address this challenge, we propose a novel dynamic self-adaptive multiscale distillation from pre-trained multimodal large model for efficient cross-modal representation learning for the first time. Unlike existing distillation methods, our strategy employs a multiscale perspective, enabling the extraction structural knowledge across from the pre-trained multimodal large model. Ensuring that the student model inherits a comprehensive and nuanced understanding of the teacher knowledge. To optimize each distillation loss in a balanced and efficient manner, we propose a dynamic self-adaptive distillation loss balancer, a novel component eliminating the need for manual loss weight adjustments and dynamically balances each loss item during the distillation process. Our methodology streamlines pre-trained multimodal large models using only their output features and original image-level information, requiring minimal computational resources. This efficient approach is suited for various applications and allows the deployment of advanced multimodal technologies even in resource-limited settings. Extensive experiments has demonstrated that our method maintains high performance while significantly reducing model complexity and training costs. Moreover, our distilled student model utilizes only image-level information to achieve state-of-the-art performance on cross-modal retrieval tasks, surpassing previous methods that relied on region-level information.","sentences":["In recent years, pre-trained multimodal large models have attracted widespread attention due to their outstanding performance in various multimodal applications.","Nonetheless, the extensive computational resources and vast datasets required for their training present significant hurdles for deployment in environments with limited computational resources.","To address this challenge, we propose a novel dynamic self-adaptive multiscale distillation from pre-trained multimodal large model for efficient cross-modal representation learning for the first time.","Unlike existing distillation methods, our strategy employs a multiscale perspective, enabling the extraction structural knowledge across from the pre-trained multimodal large model.","Ensuring that the student model inherits a comprehensive and nuanced understanding of the teacher knowledge.","To optimize each distillation loss in a balanced and efficient manner, we propose a dynamic self-adaptive distillation loss balancer, a novel component eliminating the need for manual loss weight adjustments and dynamically balances each loss item during the distillation process.","Our methodology streamlines pre-trained multimodal large models using only their output features and original image-level information, requiring minimal computational resources.","This efficient approach is suited for various applications and allows the deployment of advanced multimodal technologies even in resource-limited settings.","Extensive experiments has demonstrated that our method maintains high performance while significantly reducing model complexity and training costs.","Moreover, our distilled student model utilizes only image-level information to achieve state-of-the-art performance on cross-modal retrieval tasks, surpassing previous methods that relied on region-level information."],"url":"http://arxiv.org/abs/2404.10838v1","category":"cs.CV"}
{"created":"2024-04-16 18:08:53","title":"Implantation of asteroids from the terrestrial planet region: The effect of the timing of the giant planet instability","abstract":"The dynamical architecture and compositional diversity of the asteroid belt strongly constrain planet formation models. Recent Solar System formation models have shown that the asteroid belt may have been born empty and later filled with objects from the inner ($<$2~au) and outer regions (>5 au) of the solar system. In this work, we focus on the implantation of inner solar system planetesimals into the asteroid belt - envisioned to represent S and/or E- type asteroids - during the late-stage accretion of the terrestrial planets. It is widely accepted that the solar system's giant planets formed in a more compact orbital configuration and evolved to their current dynamical state due to a planetary dynamical instability. In this work, we explore how the implantation efficiency of asteroids from the terrestrial region correlates with the timing of the giant planet instability, which has proven challenging to constrain. We carried out a suite of numerical simulations of the accretion of terrestrial planets considering different initial distributions of planetesimals in the terrestrial region and dynamical instability times. Our simulations show that a giant planet dynamical instability occurring at $t\\gtrapprox5$ Myr -- relative to the time of the sun's natal disk dispersal -- is broadly consistent with the current asteroid belt, allowing the total mass carried out by S-complex type asteroids to be implanted into the belt from the terrestrial region. Finally, we conclude that an instability that occurs coincident with the gas disk dispersal is either inconsistent with the empty asteroid belt scenario, or may require that the gas disk in the inner solar system have dissipated at least a few Myr earlier than the gas in the outer disk (beyond Jupiter's orbit).","sentences":["The dynamical architecture and compositional diversity of the asteroid belt strongly constrain planet formation models.","Recent Solar System formation models have shown that the asteroid belt may have been born empty and later filled with objects from the inner ($<$2~au) and outer regions (>5 au) of the solar system.","In this work, we focus on the implantation of inner solar system planetesimals into the asteroid belt - envisioned to represent S and/or E- type asteroids - during the late-stage accretion of the terrestrial planets.","It is widely accepted that the solar system's giant planets formed in a more compact orbital configuration and evolved to their current dynamical state due to a planetary dynamical instability.","In this work, we explore how the implantation efficiency of asteroids from the terrestrial region correlates with the timing of the giant planet instability, which has proven challenging to constrain.","We carried out a suite of numerical simulations of the accretion of terrestrial planets considering different initial distributions of planetesimals in the terrestrial region and dynamical instability times.","Our simulations show that a giant planet dynamical instability occurring at $t\\gtrapprox5$ Myr -- relative to the time of the sun's natal disk dispersal -- is broadly consistent with the current asteroid belt, allowing the total mass carried out by S-complex type asteroids to be implanted into the belt from the terrestrial region.","Finally, we conclude that an instability that occurs coincident with the gas disk dispersal is either inconsistent with the empty asteroid belt scenario, or may require that the gas disk in the inner solar system have dissipated at least a few Myr earlier than the gas in the outer disk (beyond Jupiter's orbit)."],"url":"http://arxiv.org/abs/2404.10831v1","category":"astro-ph.EP"}
{"created":"2024-04-16 17:57:19","title":"Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes","abstract":"Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time.","However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians.","In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes.","Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work.","We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry.","Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity.","Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis.","Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed."],"url":"http://arxiv.org/abs/2404.10772v1","category":"cs.CV"}
{"created":"2024-04-16 17:50:02","title":"RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting","abstract":"Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction.","sentences":["Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge.","In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content.","Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content.","A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control.","Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal.","Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view.","The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details.","Our framework achieves state-of-the-art results for object removal while maintaining high controllability.","We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction."],"url":"http://arxiv.org/abs/2404.10765v1","category":"cs.CV"}
{"created":"2024-04-16 17:13:08","title":"N-Agent Ad Hoc Teamwork","abstract":"Current approaches to learning cooperative behaviors in multi-agent settings assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls \\textit{all} agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards generalizing the class of scenarios that cooperative learning methods can address, we introduce $N$-agent ad hoc teamwork, in which a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates at evaluation time. This paper formalizes the problem, and proposes the $\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on StarCraft II tasks shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.","sentences":["Current approaches to learning cooperative behaviors in multi-agent settings assume relatively restrictive settings.","In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls \\textit{all} agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\\textit{single}$ agent in the scenario.","However, many cooperative settings in the real world are much less restrictive.","For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company.","Towards generalizing the class of scenarios that cooperative learning methods can address, we introduce $N$-agent ad hoc teamwork, in which a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates at evaluation time.","This paper formalizes the problem, and proposes the $\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm.","POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors.","Empirical evaluation on StarCraft II tasks shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates."],"url":"http://arxiv.org/abs/2404.10740v1","category":"cs.AI"}
{"created":"2024-04-16 17:05:43","title":"Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration","abstract":"Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions. Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets. Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior. In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment. However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration. We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models. For code, please see our project page https://sites.google.com/view/blr-hac.","sentences":["Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions.","Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets.","Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior.","In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment.","However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations.","We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration.","We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models.","For code, please see our project page https://sites.google.com/view/blr-hac."],"url":"http://arxiv.org/abs/2404.10733v1","category":"cs.AI"}
{"created":"2024-04-16 17:03:50","title":"What is Meant by AGI? On the Definition of Artificial General Intelligence","abstract":"This paper aims to establish a consensus on AGI's definition. General intelligence refers to the adaptation to open environments according to certain principles using limited resources. It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives.","sentences":["This paper aims to establish a consensus on AGI's definition.","General intelligence refers to the adaptation to open environments according to certain principles using limited resources.","It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives."],"url":"http://arxiv.org/abs/2404.10731v1","category":"cs.AI"}
{"created":"2024-04-16 16:51:27","title":"GazeHTA: End-to-end Gaze Target Detection with Head-Target Association","abstract":"We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets.","sentences":["We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at.","Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets.","In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image.","GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets.","Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets."],"url":"http://arxiv.org/abs/2404.10718v1","category":"cs.CV"}
{"created":"2024-04-16 16:23:10","title":"Question Difficulty Ranking for Multiple-Choice Reading Comprehension","abstract":"Multiple-choice (MC) tests are an efficient method to assess English learners. It is useful for test creators to rank candidate MC questions by difficulty during exam curation. Typically, the difficulty is determined by having human test takers trial the questions in a pretesting stage. However, this is expensive and not scalable. Therefore, we explore automated approaches to rank MC questions by difficulty. However, there is limited data for explicit training of a system for difficulty scores. Hence, we compare task transfer and zero-shot approaches: task transfer adapts level classification and reading comprehension systems for difficulty ranking while zero-shot prompting of instruction finetuned language models contrasts absolute assessment against comparative. It is found that level classification transfers better than reading comprehension. Additionally, zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment and even the task transfer approaches at question difficulty ranking with a Spearman's correlation of 40.4%. Combining the systems is observed to further boost the correlation.","sentences":["Multiple-choice (MC) tests are an efficient method to assess English learners.","It is useful for test creators to rank candidate MC questions by difficulty during exam curation.","Typically, the difficulty is determined by having human test takers trial the questions in a pretesting stage.","However, this is expensive and not scalable.","Therefore, we explore automated approaches to rank MC questions by difficulty.","However, there is limited data for explicit training of a system for difficulty scores.","Hence, we compare task transfer and zero-shot approaches: task transfer adapts level classification and reading comprehension systems for difficulty ranking while zero-shot prompting of instruction finetuned language models contrasts absolute assessment against comparative.","It is found that level classification transfers better than reading comprehension.","Additionally, zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment and even the task transfer approaches at question difficulty ranking with a Spearman's correlation of 40.4%.","Combining the systems is observed to further boost the correlation."],"url":"http://arxiv.org/abs/2404.10704v1","category":"cs.CL"}
{"created":"2024-04-16 16:17:48","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","abstract":"Modern smartphone camera quality heavily relies on the image signal processor (ISP) to enhance captured raw images, utilizing carefully designed modules to produce final output images encoded in a standard color space (e.g., sRGB). Neural-based end-to-end learnable ISPs offer promising advancements, potentially replacing traditional ISPs with their ability to adapt without requiring extensive tuning for each new camera model, as is often the case for nearly every module in traditional ISPs. However, the key challenge with the recent learning-based ISPs is the urge to collect large paired datasets for each distinct camera model due to the influence of intrinsic camera characteristics on the formation of input raw images. This paper tackles this challenge by introducing a novel method for unpaired learning of raw-to-raw translation across diverse cameras. Specifically, we propose Rawformer, an unsupervised Transformer-based encoder-decoder method for raw-to-raw translation. It accurately maps raw images captured by a certain camera to the target camera, facilitating the generalization of learnable ISPs to new unseen cameras. Our method demonstrates superior performance on real camera datasets, achieving higher accuracy compared to previous state-of-the-art techniques, and preserving a more robust correlation between the original and translated raw images.","sentences":["Modern smartphone camera quality heavily relies on the image signal processor (ISP) to enhance captured raw images, utilizing carefully designed modules to produce final output images encoded in a standard color space (e.g., sRGB).","Neural-based end-to-end learnable ISPs offer promising advancements, potentially replacing traditional ISPs with their ability to adapt without requiring extensive tuning for each new camera model, as is often the case for nearly every module in traditional ISPs.","However, the key challenge with the recent learning-based ISPs is the urge to collect large paired datasets for each distinct camera model due to the influence of intrinsic camera characteristics on the formation of input raw images.","This paper tackles this challenge by introducing a novel method for unpaired learning of raw-to-raw translation across diverse cameras.","Specifically, we propose Rawformer, an unsupervised Transformer-based encoder-decoder method for raw-to-raw translation.","It accurately maps raw images captured by a certain camera to the target camera, facilitating the generalization of learnable ISPs to new unseen cameras.","Our method demonstrates superior performance on real camera datasets, achieving higher accuracy compared to previous state-of-the-art techniques, and preserving a more robust correlation between the original and translated raw images."],"url":"http://arxiv.org/abs/2404.10700v1","category":"eess.IV"}
{"created":"2024-04-16 15:58:49","title":"StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text Reference via Progressive Optimization","abstract":"Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences.","sentences":["Creating large-scale virtual urban scenes with variant styles is inherently challenging.","To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity.","Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background.","To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally.","During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content.","We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views.","Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization.","Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process.","The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process.","Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences."],"url":"http://arxiv.org/abs/2404.10681v1","category":"cs.CV"}
{"created":"2024-04-16 15:35:08","title":"Partial Differential Equations on Low-Dimensional Structures","abstract":"This thesis pertains to the study of elliptic and parabolic partial differential equations on \"thin\" structures. The first main objective is to establish the strong and weak low-dimensional counterparts of the parabolic Neumann problem. The main technical result is proving the closedness of the low-dimensional second-order operator. To construct a semigroup, a variant of Magyar of the Hille-Yosida Theorem for non-invertible operators is adapted. An alternative direction of study is presented to extend the class of accessible initial data. Weak-type parabolic problems are defined, and the existence of solutions is obtained by the application of the Lions version of the Lax-Milgram Lemma. The second aspect of the thesis is to examine the higher regularity of weak solutions to low-dimensional elliptic problems. We prove that for any weak low-dimensional elliptic solution, some Cosserat vector field exists as a witness of the membership of the solution to the domain of the second-order operator.","sentences":["This thesis pertains to the study of elliptic and parabolic partial differential equations on \"thin\" structures.","The first main objective is to establish the strong and weak low-dimensional counterparts of the parabolic Neumann problem.","The main technical result is proving the closedness of the low-dimensional second-order operator.","To construct a semigroup, a variant of Magyar of the Hille-Yosida Theorem for non-invertible operators is adapted.","An alternative direction of study is presented to extend the class of accessible initial data.","Weak-type parabolic problems are defined, and the existence of solutions is obtained by the application of the Lions version of the Lax-Milgram Lemma.","The second aspect of the thesis is to examine the higher regularity of weak solutions to low-dimensional elliptic problems.","We prove that for any weak low-dimensional elliptic solution, some Cosserat vector field exists as a witness of the membership of the solution to the domain of the second-order operator."],"url":"http://arxiv.org/abs/2404.10657v1","category":"math.AP"}
{"created":"2024-04-16 15:31:25","title":"Quantum Simulation of Open Quantum Dynamics via Non-Markovian Quantum State Diffusion","abstract":"Quantum simulation of non-Markovian open quantum dynamics is essential but challenging for standard quantum computers due to their non-Hermitian nature, leading to non-unitary evolution, and the limitations of available quantum resources. Here we introduce a hybrid quantum-classical algorithm designed for simulating dissipative dynamics in system with non-Markovian environment. Our approach includes formulating a non-Markovian Stochastic Schr\\\"odinger equation with complex frequency modes (cNMSSE) where the non-Markovianity is characterized by the mode excitation. Following this, we utilize variational quantum simulation to capture the non-unitary evolution within the cNMSSE framework, leading to a substantial reduction in qubit requirements. To demonstrate our approach, we investigated the spin-boson model and dynamic quantum phase transitions (DQPT) within transverse field Ising model (TFIM). Significantly, our findings reveal the enhanced DQPT in TFIM due to non-Markovian behavior.","sentences":["Quantum simulation of non-Markovian open quantum dynamics is essential but challenging for standard quantum computers due to their non-Hermitian nature, leading to non-unitary evolution, and the limitations of available quantum resources.","Here we introduce a hybrid quantum-classical algorithm designed for simulating dissipative dynamics in system with non-Markovian environment.","Our approach includes formulating a non-Markovian Stochastic Schr\\\"odinger equation with complex frequency modes (cNMSSE) where the non-Markovianity is characterized by the mode excitation.","Following this, we utilize variational quantum simulation to capture the non-unitary evolution within the cNMSSE framework, leading to a substantial reduction in qubit requirements.","To demonstrate our approach, we investigated the spin-boson model and dynamic quantum phase transitions (DQPT) within transverse field Ising model (TFIM).","Significantly, our findings reveal the enhanced DQPT in TFIM due to non-Markovian behavior."],"url":"http://arxiv.org/abs/2404.10655v2","category":"quant-ph"}
{"created":"2024-04-16 15:22:29","title":"Navigating the Serious Game Design Landscape: A Comprehensive Reference Document","abstract":"Within the evolving field of digital intervention, serious games emerge as promising tools for evidence-based interventions. Research indicates that gamified therapy, whether employed independently or in conjunction with online psychoeducation or traditional programs, proves more efficacious in delivering care to patients. As we navigate the intricate realm of serious game design, bridging the gap between therapeutic approaches and creative design proves complex. Professionals in clinical and research roles demonstrate innovative thinking yet face challenges in executing engaging therapeutic serious games due to the lack of specialized design skills and knowledge. Thus, a larger question remains: How might we aid and educate professionals in clinical and research roles the importance of game design to support their innovative therapeutic approaches? This study examines potential solutions aimed at facilitating the integration of gamification design principles into clinical study protocols, a pivotal aspect for aligning therapeutic practices with captivating narratives in the pursuit of innovative interventions. We propose two solutions, a flow chart framework for serious games or a comprehensive reference document encompassing gamification design principles and guidelines for best design practices. Through an examination of literature reviews, it was observed that selected design decisions varied across studies. Thus, we propose that the second solution, a comprehensive reference design guide, is more versatile and adaptable.","sentences":["Within the evolving field of digital intervention, serious games emerge as promising tools for evidence-based interventions.","Research indicates that gamified therapy, whether employed independently or in conjunction with online psychoeducation or traditional programs, proves more efficacious in delivering care to patients.","As we navigate the intricate realm of serious game design, bridging the gap between therapeutic approaches and creative design proves complex.","Professionals in clinical and research roles demonstrate innovative thinking yet face challenges in executing engaging therapeutic serious games due to the lack of specialized design skills and knowledge.","Thus, a larger question remains: How might we aid and educate professionals in clinical and research roles the importance of game design to support their innovative therapeutic approaches?","This study examines potential solutions aimed at facilitating the integration of gamification design principles into clinical study protocols, a pivotal aspect for aligning therapeutic practices with captivating narratives in the pursuit of innovative interventions.","We propose two solutions, a flow chart framework for serious games or a comprehensive reference document encompassing gamification design principles and guidelines for best design practices.","Through an examination of literature reviews, it was observed that selected design decisions varied across studies.","Thus, we propose that the second solution, a comprehensive reference design guide, is more versatile and adaptable."],"url":"http://arxiv.org/abs/2404.10649v1","category":"cs.HC"}
{"created":"2024-04-16 15:14:45","title":"Adapting SAM for Surgical Instrument Tracking and Segmentation in Endoscopic Submucosal Dissection Videos","abstract":"The precise tracking and segmentation of surgical instruments have led to a remarkable enhancement in the efficiency of surgical procedures. However, the challenge lies in achieving accurate segmentation of surgical instruments while minimizing the need for manual annotation and reducing the time required for the segmentation process. To tackle this, we propose a novel framework for surgical instrument segmentation and tracking. Specifically, with a tiny subset of frames for segmentation, we ensure accurate segmentation across the entire surgical video. Our method adopts a two-stage approach to efficiently segment videos. Initially, we utilize the Segment-Anything (SAM) model, which has been fine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset. The fine-tuned SAM model is applied to segment the initial frames of the video accurately. Subsequently, we deploy the XMem++ tracking algorithm to follow the annotated frames, thereby facilitating the segmentation of the entire video sequence. This workflow enables us to precisely segment and track objects within the video. Through extensive evaluation of the in-distribution dataset (EndoVis17) and the out-of-distribution datasets (EndoVis18 \\& the endoscopic submucosal dissection surgery (ESD) dataset), our framework demonstrates exceptional accuracy and robustness, thus showcasing its potential to advance the automated robotic-assisted surgery.","sentences":["The precise tracking and segmentation of surgical instruments have led to a remarkable enhancement in the efficiency of surgical procedures.","However, the challenge lies in achieving accurate segmentation of surgical instruments while minimizing the need for manual annotation and reducing the time required for the segmentation process.","To tackle this, we propose a novel framework for surgical instrument segmentation and tracking.","Specifically, with a tiny subset of frames for segmentation, we ensure accurate segmentation across the entire surgical video.","Our method adopts a two-stage approach to efficiently segment videos.","Initially, we utilize the Segment-Anything (SAM) model, which has been fine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset.","The fine-tuned SAM model is applied to segment the initial frames of the video accurately.","Subsequently, we deploy the XMem++ tracking algorithm to follow the annotated frames, thereby facilitating the segmentation of the entire video sequence.","This workflow enables us to precisely segment and track objects within the video.","Through extensive evaluation of the in-distribution dataset (EndoVis17) and the out-of-distribution datasets (EndoVis18 \\& the endoscopic submucosal dissection surgery (ESD) dataset), our framework demonstrates exceptional accuracy and robustness, thus showcasing its potential to advance the automated robotic-assisted surgery."],"url":"http://arxiv.org/abs/2404.10640v1","category":"eess.IV"}
{"created":"2024-04-16 15:10:36","title":"On Homomorphism Indistinguishability and Hypertree Depth","abstract":"$GC^k$ is a logic introduced by Scheidt and Schweikardt (2023) to express properties of hypergraphs. It is similar to first-order logic with counting quantifiers ($C$) adapted to the hypergraph setting. It has distinct sets of variables for vertices and for hyperedges and requires vertex variables to be guarded by hyperedge variables on every quantification.   We prove that two hypergraphs $G$, $H$ satisfy the same sentences in the logic $GC^k$ with guard depth at most $k$ if, and only if, they are homomorphism indistinguishable over the class of hypergraphs of strict hypertree depth at most $k$. This lifts the analogous result for tree depth $\\leq k$ and sentences of first-order logic with counting quantifiers of quantifier rank at most $k$ due to Grohe (2020) from graphs to hypergraphs. The guard depth of a formula is the quantifier rank with respect to hyperedge variables, and strict hypertree depth is a restriction of hypertree depth as defined by Adler, Gaven\\v{c}iak and Klimo\\v{s}ov\\'a (2012). To justify this restriction, we show that for every $H$, the strict hypertree depth of $H$ is at most 1 larger than its hypertree depth, and we give additional evidence that strict hypertree depth can be viewed as a reasonable generalisation of tree depth for hypergraphs.","sentences":["$GC^k$ is a logic introduced by Scheidt and Schweikardt (2023) to express properties of hypergraphs.","It is similar to first-order logic with counting quantifiers ($C$) adapted to the hypergraph setting.","It has distinct sets of variables for vertices and for hyperedges and requires vertex variables to be guarded by hyperedge variables on every quantification.   ","We prove that two hypergraphs $G$, $H$ satisfy the same sentences in the logic $GC^k$ with guard depth at most $k$ if, and only if, they are homomorphism indistinguishable over the class of hypergraphs of strict hypertree depth at most $k$.","This lifts the analogous result for tree depth $\\leq k$ and sentences of first-order logic with counting quantifiers of quantifier rank at most $k$ due to Grohe (2020) from graphs to hypergraphs.","The guard depth of a formula is the quantifier rank with respect to hyperedge variables, and strict hypertree depth is a restriction of hypertree depth as defined by Adler, Gaven\\v{c}iak and Klimo\\v{s}ov\\'a (2012).","To justify this restriction, we show that for every $H$, the strict hypertree depth of $H$ is at most 1 larger than its hypertree depth, and we give additional evidence that strict hypertree depth can be viewed as a reasonable generalisation of tree depth for hypergraphs."],"url":"http://arxiv.org/abs/2404.10637v1","category":"cs.LO"}
{"created":"2024-04-16 14:52:15","title":"Exploring selective image matching methods for zero-shot and few-sample unsupervised domain adaptation of urban canopy prediction","abstract":"We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning. Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning. We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning. These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model. The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively.","sentences":["We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning.","Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning.","We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning.","These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model.","The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively."],"url":"http://arxiv.org/abs/2404.10626v1","category":"cs.CV"}
{"created":"2024-04-16 14:40:50","title":"Emergent intelligence of buckling-driven elasto-active structures","abstract":"Active systems of self-propelled agents, e.g., birds, fish, and bacteria, can organize their collective motion into myriad autonomous behaviors. Ubiquitous in nature and across length scales, such phenomena are also amenable to artificial settings, e.g., where brainless self-propelled robots orchestrate their movements into spatio-temportal patterns via the application of external cues or when confined within flexible boundaries. Very much like their natural counterparts, these approaches typically require many units to initiate collective motion such that controlling the ensuing dynamics is challenging. Here, we demonstrate a novel yet simple mechanism that leverages nonlinear elasticity to tame near-diffusive motile particles in forming structures capable of directed motion and other emergent intelligent behaviors. Our elasto-active system comprises two centimeter-sized self-propelled microbots connected with elastic beams. These microbots exert forces that suffice to buckle the beam and set the structure in motion. We first rationalize the physics of the interaction between the beam and the microbots. Then we use reduced order models to predict the interactions of our elasto-active structure with boundaries, e.g., walls and constrictions, and demonstrate how they can exhibit intelligent behaviors such as maze navigation. The findings are relevant to designing intelligent materials or soft robots capable of autonomous space exploration, adaptation, and interaction with the surrounding environment.","sentences":["Active systems of self-propelled agents, e.g., birds, fish, and bacteria, can organize their collective motion into myriad autonomous behaviors.","Ubiquitous in nature and across length scales, such phenomena are also amenable to artificial settings, e.g., where brainless self-propelled robots orchestrate their movements into spatio-temportal patterns via the application of external cues or when confined within flexible boundaries.","Very much like their natural counterparts, these approaches typically require many units to initiate collective motion such that controlling the ensuing dynamics is challenging.","Here, we demonstrate a novel yet simple mechanism that leverages nonlinear elasticity to tame near-diffusive motile particles in forming structures capable of directed motion and other emergent intelligent behaviors.","Our elasto-active system comprises two centimeter-sized self-propelled microbots connected with elastic beams.","These microbots exert forces that suffice to buckle the beam and set the structure in motion.","We first rationalize the physics of the interaction between the beam and the microbots.","Then we use reduced order models to predict the interactions of our elasto-active structure with boundaries, e.g., walls and constrictions, and demonstrate how they can exhibit intelligent behaviors such as maze navigation.","The findings are relevant to designing intelligent materials or soft robots capable of autonomous space exploration, adaptation, and interaction with the surrounding environment."],"url":"http://arxiv.org/abs/2404.10614v1","category":"cond-mat.soft"}
{"created":"2024-04-16 14:29:05","title":"Stability of planar rarefaction waves in the vanishing dissipation limit of the Navier-Stokes-Fourier system","abstract":"We consider the vanishing dissipation limit of the compressible Navier-Stokes-Fourier system, where the initial data approach a profile generating a planar rarefaction wave for the limit Euler system. We show that the associated weak solutions converge unconditionally to the planar rarefaction wave strongly in the energy norm.","sentences":["We consider the vanishing dissipation limit of the compressible Navier-Stokes-Fourier system, where the initial data approach a profile generating a planar rarefaction wave for the limit Euler system.","We show that the associated weak solutions converge unconditionally to the planar rarefaction wave strongly in the energy norm."],"url":"http://arxiv.org/abs/2404.10604v1","category":"math.AP"}
{"created":"2024-04-16 14:06:40","title":"Extended Automatic Repeat Request For Integrated Sensing And Communication Networks","abstract":"6G wireless networks will integrate communication, computing, localization, and sensing capabilities while meeting the needs of high reliability and trustworthiness. In this paper, we develop similar techniques as those used by communication modules of previous generations for the sensing functionality of 6G networks. Specifically, this paper introduces the concept of extended automatic repeat request (e-ARQ) for integrated sensing and communications (ISAC) networks. We focus on multi-static sensing schemes, in which the nodes receiving the reflected sensing signals provide the transmitting nodes with configurable levels of feedback about the sensing result. This technique improves the sensing quality via retransmissions using adaptive parameters. We show that our proposed e-ARQ boosts the sensing quality in terms of detection accuracy and provides a sense of adaptability for applications supported by ISAC networks.","sentences":["6G wireless networks will integrate communication, computing, localization, and sensing capabilities while meeting the needs of high reliability and trustworthiness.","In this paper, we develop similar techniques as those used by communication modules of previous generations for the sensing functionality of 6G networks.","Specifically, this paper introduces the concept of extended automatic repeat request (e-ARQ) for integrated sensing and communications (ISAC) networks.","We focus on multi-static sensing schemes, in which the nodes receiving the reflected sensing signals provide the transmitting nodes with configurable levels of feedback about the sensing result.","This technique improves the sensing quality via retransmissions using adaptive parameters.","We show that our proposed e-ARQ boosts the sensing quality in terms of detection accuracy and provides a sense of adaptability for applications supported by ISAC networks."],"url":"http://arxiv.org/abs/2404.10583v1","category":"eess.SP"}
{"created":"2024-04-16 13:53:58","title":"EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence","abstract":"A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data. These negative samples often follow a softmax distribution which are dynamically updated during the training process. However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function. In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$). We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization. We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost. Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms. We report the results for pre-training image encoders on STL-10 and Imagenet-100.","sentences":["A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data.","These negative samples often follow a softmax distribution which are dynamically updated during the training process.","However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function.","In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$).","We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization.","We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations.","Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost.","Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms.","We report the results for pre-training image encoders on STL-10 and Imagenet-100."],"url":"http://arxiv.org/abs/2404.10575v1","category":"cs.LG"}
{"created":"2024-04-16 13:52:00","title":"Uncertainty-guided Open-Set Source-Free Unsupervised Domain Adaptation with Target-private Class Segregation","abstract":"Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target but usually requires simultaneous access to both source and target data. Moreover, UDA approaches commonly assume that source and target domains share the same labels space. Yet, these two assumptions are hardly satisfied in real-world scenarios. This paper considers the more challenging Source-Free Open-set Domain Adaptation (SF-OSDA) setting, where both assumptions are dropped. We propose a novel approach for SF-OSDA that exploits the granularity of target-private categories by segregating their samples into multiple unknown classes. Starting from an initial clustering-based assignment, our method progressively improves the segregation of target-private samples by refining their pseudo-labels with the guide of an uncertainty-based sample selection module. Additionally, we propose a novel contrastive loss, named NL-InfoNCELoss, that, integrating negative learning into self-supervised contrastive learning, enhances the model robustness to noisy pseudo-labels. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed method over existing approaches, establishing new state-of-the-art performance. Notably, additional analyses show that our method is able to learn the underlying semantics of novel classes, opening the possibility to perform novel class discovery.","sentences":["Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target but usually requires simultaneous access to both source and target data.","Moreover, UDA approaches commonly assume that source and target domains share the same labels space.","Yet, these two assumptions are hardly satisfied in real-world scenarios.","This paper considers the more challenging Source-Free Open-set Domain Adaptation (SF-OSDA) setting, where both assumptions are dropped.","We propose a novel approach for SF-OSDA that exploits the granularity of target-private categories by segregating their samples into multiple unknown classes.","Starting from an initial clustering-based assignment, our method progressively improves the segregation of target-private samples by refining their pseudo-labels with the guide of an uncertainty-based sample selection module.","Additionally, we propose a novel contrastive loss, named NL-InfoNCELoss, that, integrating negative learning into self-supervised contrastive learning, enhances the model robustness to noisy pseudo-labels.","Extensive experiments on benchmark datasets demonstrate the superiority of the proposed method over existing approaches, establishing new state-of-the-art performance.","Notably, additional analyses show that our method is able to learn the underlying semantics of novel classes, opening the possibility to perform novel class discovery."],"url":"http://arxiv.org/abs/2404.10574v1","category":"cs.CV"}
{"created":"2024-04-16 13:11:02","title":"Characterizing Polkadot's Transactions Ecosystem: methodology, tools, and insights","abstract":"The growth potential of a crypto(currency) project can be measured by the use cases spurred by the underlying technology. However, these projects are usually distributed, with a weak feedback schemes. Hence, a metric that is widely used as a proxy for their healthiness is the number of transactions and related volumes. Nevertheless, such a metric can be subject to manipulation (the crypto market being an unregulated one magnifies such a risk). To address the cited gap we design a comprehensive methodology to process large cryptocurrency transaction graphs that, after clustering user addresses of interest, derives a compact representation of the network that highlights clusters interactions.   To show the viability of our solution, we bring forward a use case centered on Polkadot, which has gained significant attention in the digital currency landscape due to its pioneering approach to interoperability and scalability. However, little is known about how many and to what extent its wide range of enabled use cases have been adopted by end-users so far. The answer to this type of question means mapping Polkadot (or any analyzed crypto project) on a palette that ranges from a thriving ecosystem to a speculative coin without compelling use cases.   Our findings demonstrate that crypto exchanges exert considerable influence on the Polkadot network, owning nearly 40% of all addresses in the ledger and absorbing at least 80% of all transactions. In addition, the high volume of inter-exchange transactions (> 20%) underscores the strong interconnections among just a couple of prominent exchanges, prompting further investigations into the behavior of these actors to uncover potential unethical activities, such as wash trading. These results, while characterized by a high level of scalability and adaptability, are at the same time immune from the drawbacks of currently used metrics.","sentences":["The growth potential of a crypto(currency) project can be measured by the use cases spurred by the underlying technology.","However, these projects are usually distributed, with a weak feedback schemes.","Hence, a metric that is widely used as a proxy for their healthiness is the number of transactions and related volumes.","Nevertheless, such a metric can be subject to manipulation (the crypto market being an unregulated one magnifies such a risk).","To address the cited gap we design a comprehensive methodology to process large cryptocurrency transaction graphs that, after clustering user addresses of interest, derives a compact representation of the network that highlights clusters interactions.   ","To show the viability of our solution, we bring forward a use case centered on Polkadot, which has gained significant attention in the digital currency landscape due to its pioneering approach to interoperability and scalability.","However, little is known about how many and to what extent its wide range of enabled use cases have been adopted by end-users so far.","The answer to this type of question means mapping Polkadot (or any analyzed crypto project) on a palette that ranges from a thriving ecosystem to a speculative coin without compelling use cases.   ","Our findings demonstrate that crypto exchanges exert considerable influence on the Polkadot network, owning nearly 40% of all addresses in the ledger and absorbing at least 80% of all transactions.","In addition, the high volume of inter-exchange transactions (> 20%) underscores the strong interconnections among just a couple of prominent exchanges, prompting further investigations into the behavior of these actors to uncover potential unethical activities, such as wash trading.","These results, while characterized by a high level of scalability and adaptability, are at the same time immune from the drawbacks of currently used metrics."],"url":"http://arxiv.org/abs/2404.10543v1","category":"cs.CR"}
{"created":"2024-04-16 12:12:06","title":"LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Perception System","abstract":"Recent large vision models (e.g., SAM) enjoy great potential to facilitate intelligent perception with high accuracy. Yet, the resource constraints in the IoT environment tend to limit such large vision models to be locally deployed, incurring considerable inference latency thereby making it difficult to support real-time applications, such as autonomous driving and robotics. Edge-cloud collaboration with large-small model co-inference offers a promising approach to achieving high inference accuracy and low latency. However, existing edge-cloud collaboration methods are tightly coupled with the model architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT environments. To address the issues, we propose LAECIPS, a new edge-cloud collaboration framework. In LAECIPS, both the large vision model on the cloud and the lightweight model on the edge are plug-and-play. We design an edge-cloud collaboration strategy based on hard input mining, optimized for both high accuracy and low latency. We propose to update the edge model and its collaboration strategy with the cloud under the supervision of the large vision model, so as to adapt to the dynamic IoT data streams. Theoretical analysis of LAECIPS proves its feasibility. Experiments conducted in a robotic semantic segmentation system using real-world datasets show that LAECIPS outperforms its state-of-the-art competitors in accuracy, latency, and communication overhead while having better adaptability to dynamic environments.","sentences":["Recent large vision models (e.g., SAM) enjoy great potential to facilitate intelligent perception with high accuracy.","Yet, the resource constraints in the IoT environment tend to limit such large vision models to be locally deployed, incurring considerable inference latency thereby making it difficult to support real-time applications, such as autonomous driving and robotics.","Edge-cloud collaboration with large-small model co-inference offers a promising approach to achieving high inference accuracy and low latency.","However, existing edge-cloud collaboration methods are tightly coupled with the model architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT environments.","To address the issues, we propose LAECIPS, a new edge-cloud collaboration framework.","In LAECIPS, both the large vision model on the cloud and the lightweight model on the edge are plug-and-play.","We design an edge-cloud collaboration strategy based on hard input mining, optimized for both high accuracy and low latency.","We propose to update the edge model and its collaboration strategy with the cloud under the supervision of the large vision model, so as to adapt to the dynamic IoT data streams.","Theoretical analysis of LAECIPS proves its feasibility.","Experiments conducted in a robotic semantic segmentation system using real-world datasets show that LAECIPS outperforms its state-of-the-art competitors in accuracy, latency, and communication overhead while having better adaptability to dynamic environments."],"url":"http://arxiv.org/abs/2404.10498v1","category":"cs.AI"}
{"created":"2024-04-16 12:04:04","title":"Assumption-Lean Quantile Regression","abstract":"Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure. Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic. In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model. We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified. This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning. Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods. The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight.","sentences":["Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure.","Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic.","In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model.","We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified.","This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning.","Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods.","The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight."],"url":"http://arxiv.org/abs/2404.10495v2","category":"stat.ME"}
{"created":"2024-04-16 12:03:38","title":"BDAN: Mitigating Temporal Difference Across Electrodes in Cross-Subject Motor Imagery Classification via Generative Bridging Domain","abstract":"Because of \"the non-repeatability of the experiment settings and conditions\" and \"the variability of brain patterns among subjects\", the data distributions across sessions and electrodes are different in cross-subject motor imagery (MI) studies, eventually reducing the performance of the classification model. Systematically summarised based on the existing studies, a novel temporal-electrode data distribution problem is investigated under both intra-subject and inter-subject scenarios in this paper. Based on the presented issue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to minimise the data distribution difference across sessions in the aspect of the electrode, thus improving and enhancing model performance. In the proposed BDAN, deep features of all the EEG data are extracted via a specially designed spatial feature extractor. With the obtained spatio-temporal features, a special generative bridging domain is established, bridging the data from all the subjects across sessions. The difference across sessions and electrodes is then minimized using the customized bridging loss functions, and the known knowledge is automatically transferred through the constructed bridging domain. To show the effectiveness of the proposed BDAN, comparison experiments and ablation studies are conducted on a public EEG dataset. The overall comparison results demonstrate the superior performance of the proposed BDAN compared with the other advanced deep learning and domain adaptation methods.","sentences":["Because of \"the non-repeatability of the experiment settings and conditions\" and \"the variability of brain patterns among subjects\", the data distributions across sessions and electrodes are different in cross-subject motor imagery (MI) studies, eventually reducing the performance of the classification model.","Systematically summarised based on the existing studies, a novel temporal-electrode data distribution problem is investigated under both intra-subject and inter-subject scenarios in this paper.","Based on the presented issue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to minimise the data distribution difference across sessions in the aspect of the electrode, thus improving and enhancing model performance.","In the proposed BDAN, deep features of all the EEG data are extracted via a specially designed spatial feature extractor.","With the obtained spatio-temporal features, a special generative bridging domain is established, bridging the data from all the subjects across sessions.","The difference across sessions and electrodes is then minimized using the customized bridging loss functions, and the known knowledge is automatically transferred through the constructed bridging domain.","To show the effectiveness of the proposed BDAN, comparison experiments and ablation studies are conducted on a public EEG dataset.","The overall comparison results demonstrate the superior performance of the proposed BDAN compared with the other advanced deep learning and domain adaptation methods."],"url":"http://arxiv.org/abs/2404.10494v1","category":"cs.HC"}
{"created":"2024-04-16 11:44:12","title":"AbsGS: Recovering Fine Details for 3D Gaussian Splatting","abstract":"3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: https://ty424.github.io/AbsGS.github.io/","sentences":["3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance.","However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images.","The underlying reason for the flaw has still been under-explored.","In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting.","To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification.","Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting.","We evaluate our proposed method on various challenging datasets.","The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption.","Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods.","We will open source our codes upon formal publication.","Our project page is available at: https://ty424.github.io/AbsGS.github.io/"],"url":"http://arxiv.org/abs/2404.10484v1","category":"cs.CV"}
{"created":"2024-04-16 11:08:20","title":"Forward lateral photovoltage scanning problem: Perturbation approach and existence-uniqueness analysis","abstract":"In this paper, we present analytical results for the so-called forward lateral photovoltage scanning (LPS) problem. The (inverse) LPS model predicts doping variations in crystal by measuring the current leaving the crystal generated by a laser at various positions. The forward model consists of a set of nonlinear elliptic equations coupled with a measuring device modeled by a resistance. Standard methods to ensure the existence and uniqueness of the forward model cannot be used in a straightforward manner due to the presence of an additional generation term modeling the effect of the laser on the crystal. Hence, we scale the original forward LPS problem and employ a perturbation approach to derive the leading order system and the correction up to the second order in an appropriate small parameter. While these simplifications pose no issues from a physical standpoint, they enable us to demonstrate the analytic existence and uniqueness of solutions for the simplified system using standard arguments from elliptic theory adapted to the coupling with the measuring device.","sentences":["In this paper, we present analytical results for the so-called forward lateral photovoltage scanning (LPS) problem.","The (inverse) LPS model predicts doping variations in crystal by measuring the current leaving the crystal generated by a laser at various positions.","The forward model consists of a set of nonlinear elliptic equations coupled with a measuring device modeled by a resistance.","Standard methods to ensure the existence and uniqueness of the forward model cannot be used in a straightforward manner due to the presence of an additional generation term modeling the effect of the laser on the crystal.","Hence, we scale the original forward LPS problem and employ a perturbation approach to derive the leading order system and the correction up to the second order in an appropriate small parameter.","While these simplifications pose no issues from a physical standpoint, they enable us to demonstrate the analytic existence and uniqueness of solutions for the simplified system using standard arguments from elliptic theory adapted to the coupling with the measuring device."],"url":"http://arxiv.org/abs/2404.10466v1","category":"math-ph"}
{"created":"2024-04-16 10:10:19","title":"Language Proficiency and F0 Entrainment: A Study of L2 English Imitation in Italian, French, and Slovak Speakers","abstract":"This study explores F0 entrainment in second language (L2) English speech imitation during an Alternating Reading Task (ART). Participants with Italian, French, and Slovak native languages imitated English utterances, and their F0 entrainment was quantified using the Dynamic Time Warping (DTW) distance between the parameterized F0 contours of the imitated utterances and those of the model utterances. Results indicate a nuanced relationship between L2 English proficiency and entrainment: speakers with higher proficiency generally exhibit less entrainment in pitch variation and declination. However, within dyads, the more proficient speakers demonstrate a greater ability to mimic pitch range, leading to increased entrainment. This suggests that proficiency influences entrainment differently at individual and dyadic levels, highlighting the complex interplay between language skill and prosodic adaptation.","sentences":["This study explores F0 entrainment in second language (L2) English speech imitation during an Alternating Reading Task (ART).","Participants with Italian, French, and Slovak native languages imitated English utterances, and their F0 entrainment was quantified using the Dynamic Time Warping (DTW) distance between the parameterized F0 contours of the imitated utterances and those of the model utterances.","Results indicate a nuanced relationship between L2 English proficiency and entrainment: speakers with higher proficiency generally exhibit less entrainment in pitch variation and declination.","However, within dyads, the more proficient speakers demonstrate a greater ability to mimic pitch range, leading to increased entrainment.","This suggests that proficiency influences entrainment differently at individual and dyadic levels, highlighting the complex interplay between language skill and prosodic adaptation."],"url":"http://arxiv.org/abs/2404.10440v1","category":"cs.CL"}
{"created":"2024-04-16 09:37:41","title":"AudioProtoPNet: An interpretable deep learning model for bird sound classification","abstract":"Recently, scientists have proposed several deep learning models to monitor the diversity of bird species. These models can detect bird species with high accuracy by analyzing acoustic signals. However, traditional deep learning algorithms are black-box models that provide no insight into their decision-making process. For domain experts, such as ornithologists, it is crucial that these models are not only efficient, but also interpretable in order to be used as assistive tools. In this study, we present an adaption of the Prototypical Part Network (ProtoPNet) for audio classification that provides inherent interpretability through its model architecture. Our approach is based on a ConvNeXt backbone architecture for feature extraction and learns prototypical patterns for each bird species using spectrograms of the training data. Classification of new data is done by comparison with these prototypes in latent space, which simultaneously serve as easily understandable explanations for the model's decisions.","sentences":["Recently, scientists have proposed several deep learning models to monitor the diversity of bird species.","These models can detect bird species with high accuracy by analyzing acoustic signals.","However, traditional deep learning algorithms are black-box models that provide no insight into their decision-making process.","For domain experts, such as ornithologists, it is crucial that these models are not only efficient, but also interpretable in order to be used as assistive tools.","In this study, we present an adaption of the Prototypical Part Network (ProtoPNet) for audio classification that provides inherent interpretability through its model architecture.","Our approach is based on a ConvNeXt backbone architecture for feature extraction and learns prototypical patterns for each bird species using spectrograms of the training data.","Classification of new data is done by comparison with these prototypes in latent space, which simultaneously serve as easily understandable explanations for the model's decisions."],"url":"http://arxiv.org/abs/2404.10420v1","category":"cs.LG"}
{"created":"2024-04-18 17:47:33","title":"On the well-posedness of the KP-I equation","abstract":"We revisit the local well-posedness for the KP-I equation. We obtain unconditional local well-posedness in $H^{s,0}({\\mathbb R}^2)$ for $s>3/4$ and unconditional global well-posedness in the energy space. We also prove the global existence of perturbations with finite energy of non decaying smooth global solutions.","sentences":["We revisit the local well-posedness for the KP-I equation.","We obtain unconditional local well-posedness in $H^{s,0}({\\mathbb R}^2)$ for $s>3/4$ and unconditional global well-posedness in the energy space.","We also prove the global existence of perturbations with finite energy of non decaying smooth global solutions."],"url":"http://arxiv.org/abs/2404.12364v1","category":"math.AP"}
{"created":"2024-04-18 17:35:59","title":"The cosmological constant problem and the effective potential of a gravity-coupled scalar","abstract":"We consider a quantum scalar field in a classical (Euclidean) De Sitter background, whose radius is fixed dynamically by Einstein's equations. In the case of a free scalar, it has been shown by Becker and Reuter that if one regulates the quantum effective action by putting a cutoff $N$ on the modes of the quantum field, the radius is driven dynamically to infinity when $N$ tends to infinity. We show that this result holds also in the case of a self-interacting scalar, both in the symmetric and broken-symmetry phase. Furthermore, when the gravitational background is put on shell, the quantum corrections to the mass and quartic self-coupling are found to be finite.","sentences":["We consider a quantum scalar field in a classical (Euclidean) De Sitter background, whose radius is fixed dynamically by Einstein's equations.","In the case of a free scalar, it has been shown by Becker and Reuter that if one regulates the quantum effective action by putting a cutoff $N$ on the modes of the quantum field, the radius is driven dynamically to infinity when $N$ tends to infinity.","We show that this result holds also in the case of a self-interacting scalar, both in the symmetric and broken-symmetry phase.","Furthermore, when the gravitational background is put on shell, the quantum corrections to the mass and quartic self-coupling are found to be finite."],"url":"http://arxiv.org/abs/2404.12357v1","category":"hep-th"}
{"created":"2024-04-18 17:10:14","title":"Symmetries and conservation of spin angular momentum, helicity, and chirality in photonic time-varying media","abstract":"Polarization-dependent dynamical properties of light as the spin angular momentum (SAM), helicity, and chirality are conserved quantities in free-space. Despite their similarities on account of their relationship with a circular state of polarization, SAM, helicity, and chirality emerge from distinct symmetries, which endows them with different physical meanings, properties, and practical applications. In this work, we investigate the behavior of such quantities in time-varying media (TVM), i.e., how a temporal modulation impacts their symmetries and conservation laws. Our results demonstrate that the SAM is conserved for any time modulation, helicity is only preserved in impedance-matched time modulations, while chirality is not conserved. In addition, the continuity equations highlight the dependence of the chirality with the energy content of the fields. These results provide additional insights into the similarities and differences between SAM, helicity, and chirality, as well as their physical meaning. Furthermore, our theoretical framework provides with a new perspective to analyze polarization-dependent light-matter interactions in TVM.","sentences":["Polarization-dependent dynamical properties of light as the spin angular momentum (SAM), helicity, and chirality are conserved quantities in free-space.","Despite their similarities on account of their relationship with a circular state of polarization, SAM, helicity, and chirality emerge from distinct symmetries, which endows them with different physical meanings, properties, and practical applications.","In this work, we investigate the behavior of such quantities in time-varying media (TVM), i.e., how a temporal modulation impacts their symmetries and conservation laws.","Our results demonstrate that the SAM is conserved for any time modulation, helicity is only preserved in impedance-matched time modulations, while chirality is not conserved.","In addition, the continuity equations highlight the dependence of the chirality with the energy content of the fields.","These results provide additional insights into the similarities and differences between SAM, helicity, and chirality, as well as their physical meaning.","Furthermore, our theoretical framework provides with a new perspective to analyze polarization-dependent light-matter interactions in TVM."],"url":"http://arxiv.org/abs/2404.12340v1","category":"physics.optics"}
{"created":"2024-04-18 16:53:18","title":"The Sine-Gordon QFT in de Sitter spacetime","abstract":"We consider the massless Sine-Gordon model in de Sitter spacetime, in the regime $\\beta^2 < 4 \\pi$ and using the framework of perturbative algebraic quantum field theory. We show that a Fock space representation exists for the free massless field, but that the natural one-parameter family of vacuum-like states breaks the de Sitter boost symmetries. We prove convergence of the perturbative series for the S matrix in this representation, and construct the interacting Haag-Kastler net of local algebras from the relative S matrices. We show that the net fulfills isotony, locality and de Sitter covariance (in the algebraic adiabatic limit), even though the states that we consider are not invariant. We furthermore prove convergence of the perturbative series for the interacting field and the vertex operators, and verify that the interacting equation of motion holds.","sentences":["We consider the massless Sine-Gordon model in de Sitter spacetime, in the regime $\\beta^2 < 4 \\pi$ and using the framework of perturbative algebraic quantum field theory.","We show that a Fock space representation exists for the free massless field, but that the natural one-parameter family of vacuum-like states breaks the de Sitter boost symmetries.","We prove convergence of the perturbative series for the S matrix in this representation, and construct the interacting Haag-Kastler net of local algebras from the relative S matrices.","We show that the net fulfills isotony, locality and de Sitter covariance (in the algebraic adiabatic limit), even though the states that we consider are not invariant.","We furthermore prove convergence of the perturbative series for the interacting field and the vertex operators, and verify that the interacting equation of motion holds."],"url":"http://arxiv.org/abs/2404.12324v1","category":"math-ph"}
{"created":"2024-04-18 16:32:12","title":"New free boundary minimal annuli of revolution in the 3-sphere","abstract":"We rigorously establish the existence of many free boundary minimal annuli with boundary in a geodesic sphere of $\\mathbb{S}^3$. These arise as compact subdomains of a one-parameter family of complete minimal immersions of $\\mathbb{R} \\times \\mathbb{S}^1$ into $\\mathbb{S}^3$ described by do Carmo and Dajczer [1]. While the free boundary minimal annuli we exhibit may have self-intersections and may not be contained in a geodesic ball, we show that there is at least a one-parameter family of embedded examples that are contained in geodesic balls whose radius may be less than, equal to or greater than $\\pi$/2. After explaining the connection to Otsuki tori [13], we establish lower bounds on the number of free boundary minimal annuli contained in each Otsuki torus in terms of the corresponding rational number. Finally, we show that some of the recent work of Lee and Seo [7] on isoperimetric inequalities and of Lima and Menezes [10] on index bounds extends to geodesic balls equal to or larger than a hemisphere.","sentences":["We rigorously establish the existence of many free boundary minimal annuli with boundary in a geodesic sphere of $\\mathbb{S}^3$. These arise as compact subdomains of a one-parameter family of complete minimal immersions of $\\mathbb{R} \\times \\mathbb{S}^1$ into $\\mathbb{S}^3$ described by do Carmo and Dajczer [1].","While the free boundary minimal annuli we exhibit may have self-intersections and may not be contained in a geodesic ball, we show that there is at least a one-parameter family of embedded examples that are contained in geodesic balls whose radius may be less than, equal to or greater than $\\pi$/2.","After explaining the connection to Otsuki tori","[13], we establish lower bounds on the number of free boundary minimal annuli contained in each Otsuki torus in terms of the corresponding rational number.","Finally, we show that some of the recent work of Lee and Seo [7] on isoperimetric inequalities and of Lima and Menezes [10] on index bounds extends to geodesic balls equal to or larger than a hemisphere."],"url":"http://arxiv.org/abs/2404.12304v1","category":"math.DG"}
{"created":"2024-04-18 15:50:56","title":"A New Computational Method for Energetic Particle Acceleration and Transport with its Feedback","abstract":"We have developed a new computational method to explore astrophysical and heliophysical phenomena, especially those considerably influenced by non-thermal energetic particles. This novel approach considers the backreaction from these energetic particles by incorporating the non-thermal fluid pressure into Magnetohydrodynamics (MHD) equations. The pressure of the non-thermal fluid is evaluated from the energetic particle distribution evolved through Parker's transport equation, which is solved using stochastic differential equations. We implement this method in the HOW-MHD code (Seo \\& Ryu 2023), which achieves 5th-order accuracy. We find that without spatial diffusion, the method accurately reproduces the Riemann solution in the hydrodynamic shock tube test when including the non-thermal pressure. Solving Parker's transport equation allows the determination of pressure terms for both relativistic and non-relativistic non-thermal fluids with adiabatic indices $\\gamma_{\\rm{NT}}=4/3$ and $\\gamma_{\\rm{NT}}=5/3$, respectively. The method also successfully replicates the Magnetohydrodynamic shock tube test with non-thermal pressure, successfully resolving the discontinuities within a few cells. Introducing spatial diffusion of non-thermal particles leads to marginal changes in the shock but smooths the contact discontinuity. Importantly, this method successfully simulates the energy spectrum of the non-thermal particles accelerated through shock, which includes feedback from the non-thermal population. These results demonstrate that this method is very powerful for studying particle acceleration when a significant portion of the plasma energy is taken by energetic particles.","sentences":["We have developed a new computational method to explore astrophysical and heliophysical phenomena, especially those considerably influenced by non-thermal energetic particles.","This novel approach considers the backreaction from these energetic particles by incorporating the non-thermal fluid pressure into Magnetohydrodynamics (MHD) equations.","The pressure of the non-thermal fluid is evaluated from the energetic particle distribution evolved through Parker's transport equation, which is solved using stochastic differential equations.","We implement this method in the HOW-MHD code (Seo \\& Ryu 2023), which achieves 5th-order accuracy.","We find that without spatial diffusion, the method accurately reproduces the Riemann solution in the hydrodynamic shock tube test when including the non-thermal pressure.","Solving Parker's transport equation allows the determination of pressure terms for both relativistic and non-relativistic non-thermal fluids with adiabatic indices $\\gamma_{\\rm{NT}}=4/3$ and $\\gamma_{\\rm{NT}}=5/3$, respectively.","The method also successfully replicates the Magnetohydrodynamic shock tube test with non-thermal pressure, successfully resolving the discontinuities within a few cells.","Introducing spatial diffusion of non-thermal particles leads to marginal changes in the shock but smooths the contact discontinuity.","Importantly, this method successfully simulates the energy spectrum of the non-thermal particles accelerated through shock, which includes feedback from the non-thermal population.","These results demonstrate that this method is very powerful for studying particle acceleration when a significant portion of the plasma energy is taken by energetic particles."],"url":"http://arxiv.org/abs/2404.12276v1","category":"astro-ph.HE"}
{"created":"2024-04-18 15:42:17","title":"Upward L\u00f6wenheim-Skolem-Tarski Numbers for Abstract Logics","abstract":"Galeotti, Khomskii and V\\\"a\\\"an\\\"aanen recently introduced the notion of the upward L\\\"owenheim-Skolem-Tarski number for a logic, strengthening the classical notion of a Hanf number. A cardinal $\\kappa$ is the \\emph{upward L\\\"owenheim-Skolem-Tarski number} (ULST) of a logic $\\mathcal L$ if it is the least cardinal with the property that whenever $M$ is a model of size at least $\\kappa$ satisfying a sentence $\\varphi$ in $\\mathcal L$, then there are arbitrarily large models satisfying $\\varphi$ and having $M$ as a substructure. The substructure requirement is what differentiates the ULST number from the Hanf number and gives the notion large cardinal strength. While it is a theorem of ZFC that every logic has a Hanf number, Galeotti, Khomskii and V\\\"a\\\"an\\\"anen showed that the existence of the ULST number for second-order logic implies the existence of a partially extendible cardinal. We answer positively their conjecture that the ULST number for second-order logic is the least extendible cardinal.   We define the strong ULST number by strengthening the substructure requirement to elementary substructure. We investigate the ULST and strong ULST numbers for several classical strong logics: infinitary logics, the equicardinality logic, logic with the well-foundedness quantifier, second-order logic, and sort logics. We show that the ULST and the strong ULST numbers are characterized in some cases by classical large cardinals and in some cases by natural new large cardinal notions that they give rise to. We show that for some logics the notions of the ULST number, strong ULST number and least strong compactness cardinal coincide, while for others, it is consistent that they can be separated. Finally, we introduce a natural large cardinal notion characterizing strong compactness cardinals for the equicardinality logic.","sentences":["Galeotti, Khomskii and V\\\"a\\\"an\\\"aanen recently introduced the notion of the upward L\\\"owenheim-Skolem-Tarski number for a logic, strengthening the classical notion of a Hanf number.","A cardinal $\\kappa$ is the \\emph{upward L\\\"owenheim-Skolem-Tarski number} (ULST) of a logic $\\mathcal L$ if it is the least cardinal with the property that whenever $M$ is a model of size at least $\\kappa$ satisfying a sentence $\\varphi$ in $\\mathcal L$, then there are arbitrarily large models satisfying $\\varphi$ and having $M$ as a substructure.","The substructure requirement is what differentiates the ULST number from the Hanf number and gives the notion large cardinal strength.","While it is a theorem of ZFC that every logic has a Hanf number, Galeotti, Khomskii and V\\\"a\\\"an\\\"anen showed that the existence of the ULST number for second-order logic implies the existence of a partially extendible cardinal.","We answer positively their conjecture that the ULST number for second-order logic is the least extendible cardinal.   ","We define the strong ULST number by strengthening the substructure requirement to elementary substructure.","We investigate the ULST and strong ULST numbers for several classical strong logics: infinitary logics, the equicardinality logic, logic with the well-foundedness quantifier, second-order logic, and sort logics.","We show that the ULST and the strong ULST numbers are characterized in some cases by classical large cardinals and in some cases by natural new large cardinal notions that they give rise to.","We show that for some logics the notions of the ULST number, strong ULST number and least strong compactness cardinal coincide, while for others, it is consistent that they can be separated.","Finally, we introduce a natural large cardinal notion characterizing strong compactness cardinals for the equicardinality logic."],"url":"http://arxiv.org/abs/2404.12269v1","category":"math.LO"}
{"created":"2024-04-18 15:20:59","title":"Deep Gaussian mixture model for unsupervised image segmentation","abstract":"The recent emergence of deep learning has led to a great deal of work on designing supervised deep semantic segmentation algorithms. As in many tasks sufficient pixel-level labels are very difficult to obtain, we propose a method which combines a Gaussian mixture model (GMM) with unsupervised deep learning techniques. In the standard GMM the pixel values with each sub-region are modelled by a Gaussian distribution. In order to identify the different regions, the parameter vector that minimizes the negative log-likelihood (NLL) function regarding the GMM has to be approximated. For this task, usually iterative optimization methods such as the expectation-maximization (EM) algorithm are used. In this paper, we propose to estimate these parameters directly from the image using a convolutional neural network (CNN). We thus change the iterative procedure in the EM algorithm replacing the expectation-step by a gradient-step with regard to the networks parameters. This means that the network is trained to minimize the NLL function of the GMM which comes with at least two advantages. As once trained, the network is able to predict label probabilities very quickly compared with time consuming iterative optimization methods. Secondly, due to the deep image prior our method is able to partially overcome one of the main disadvantages of GMM, which is not taking into account correlation between neighboring pixels, as it assumes independence between them. We demonstrate the advantages of our method in various experiments on the example of myocardial infarct segmentation on multi-sequence MRI images.","sentences":["The recent emergence of deep learning has led to a great deal of work on designing supervised deep semantic segmentation algorithms.","As in many tasks sufficient pixel-level labels are very difficult to obtain, we propose a method which combines a Gaussian mixture model (GMM) with unsupervised deep learning techniques.","In the standard GMM the pixel values with each sub-region are modelled by a Gaussian distribution.","In order to identify the different regions, the parameter vector that minimizes the negative log-likelihood (NLL) function regarding the GMM has to be approximated.","For this task, usually iterative optimization methods such as the expectation-maximization (EM) algorithm are used.","In this paper, we propose to estimate these parameters directly from the image using a convolutional neural network (CNN).","We thus change the iterative procedure in the EM algorithm replacing the expectation-step by a gradient-step with regard to the networks parameters.","This means that the network is trained to minimize the NLL function of the GMM which comes with at least two advantages.","As once trained, the network is able to predict label probabilities very quickly compared with time consuming iterative optimization methods.","Secondly, due to the deep image prior our method is able to partially overcome one of the main disadvantages of GMM, which is not taking into account correlation between neighboring pixels, as it assumes independence between them.","We demonstrate the advantages of our method in various experiments on the example of myocardial infarct segmentation on multi-sequence MRI images."],"url":"http://arxiv.org/abs/2404.12252v1","category":"cs.CV"}
{"created":"2024-04-18 14:57:17","title":"Neural Networks with Causal Graph Constraints: A New Approach for Treatment Effects Estimation","abstract":"In recent years, there has been a growing interest in using machine learning techniques for the estimation of treatment effects. Most of the best-performing methods rely on representation learning strategies that encourage shared behavior among potential outcomes to increase the precision of treatment effect estimates. In this paper we discuss and classify these models in terms of their algorithmic inductive biases and present a new model, NN-CGC, that considers additional information from the causal graph. NN-CGC tackles bias resulting from spurious variable interactions by implementing novel constraints on models, and it can be integrated with other representation learning methods. We test the effectiveness of our method using three different base models on common benchmarks. Our results indicate that our model constraints lead to significant improvements, achieving new state-of-the-art results in treatment effects estimation. We also show that our method is robust to imperfect causal graphs and that using partial causal information is preferable to ignoring it.","sentences":["In recent years, there has been a growing interest in using machine learning techniques for the estimation of treatment effects.","Most of the best-performing methods rely on representation learning strategies that encourage shared behavior among potential outcomes to increase the precision of treatment effect estimates.","In this paper we discuss and classify these models in terms of their algorithmic inductive biases and present a new model, NN-CGC, that considers additional information from the causal graph.","NN-CGC tackles bias resulting from spurious variable interactions by implementing novel constraints on models, and it can be integrated with other representation learning methods.","We test the effectiveness of our method using three different base models on common benchmarks.","Our results indicate that our model constraints lead to significant improvements, achieving new state-of-the-art results in treatment effects estimation.","We also show that our method is robust to imperfect causal graphs and that using partial causal information is preferable to ignoring it."],"url":"http://arxiv.org/abs/2404.12238v1","category":"cs.LG"}
{"created":"2024-04-18 14:13:40","title":"The Explicit values of the UBCT, the LBCT and the DBCT of the inverse function","abstract":"Substitution boxes (S-boxes) play a significant role in ensuring the resistance of block ciphers against various attacks. The Upper Boomerang Connectivity Table (UBCT), the Lower Boomerang Connectivity Table (LBCT) and the Double Boomerang Connectivity Table (DBCT) of a given S-box are crucial tools to analyze its security concerning specific attacks. However, there are currently no related results for this research. The inverse function is crucial for constructing S-boxes of block ciphers with good cryptographic properties in symmetric cryptography. Therefore, extensive research has been conducted on the inverse function, exploring various properties related to standard attacks. Thanks to the recent advancements in boomerang cryptanalysis, particularly the introduction of concepts such as UBCT, LBCT, and DBCT, this paper aims to further investigate the properties of the inverse function $F(x)=x^{2^n-2}$ over $\\gf_{2^n}$ for arbitrary $n$. As a consequence, by carrying out certain finer manipulations of solving specific equations over $\\gf_{2^n}$, we give all entries of the UBCT, LBCT of $F(x)$ over $\\gf_{2^n}$ for arbitrary $n$. Besides, based on the results of the UBCT and LBCT for the inverse function, we determine that $F(x)$ is hard when $n$ is odd. Furthermore, we completely compute all entries of the DBCT of $F(x)$ over $\\gf_{2^n}$ for arbitrary $n$. Additionally, we provide the precise number of elements with a given entry by means of the values of some Kloosterman sums. Further, we determine the double boomerang uniformity of $F(x)$ over $\\gf_{2^n}$ for arbitrary $n$. Our in-depth analysis of the DBCT of $F(x)$ contributes to a better evaluation of the S-box's resistance against boomerang attacks.","sentences":["Substitution boxes (S-boxes) play a significant role in ensuring the resistance of block ciphers against various attacks.","The Upper Boomerang Connectivity Table (UBCT), the Lower Boomerang Connectivity Table (LBCT) and the Double Boomerang Connectivity Table (DBCT) of a given S-box are crucial tools to analyze its security concerning specific attacks.","However, there are currently no related results for this research.","The inverse function is crucial for constructing S-boxes of block ciphers with good cryptographic properties in symmetric cryptography.","Therefore, extensive research has been conducted on the inverse function, exploring various properties related to standard attacks.","Thanks to the recent advancements in boomerang cryptanalysis, particularly the introduction of concepts such as UBCT, LBCT, and DBCT, this paper aims to further investigate the properties of the inverse function $F(x)=x^{2^n-2}$ over $\\gf_{2^n}$ for arbitrary $n$. As a consequence, by carrying out certain finer manipulations of solving specific equations over $\\gf_{2^n}$, we give all entries of the UBCT, LBCT of $F(x)$ over $\\gf_{2^n}$ for arbitrary $n$. Besides, based on the results of the UBCT and LBCT for the inverse function, we determine that $F(x)$ is hard when $n$ is odd.","Furthermore, we completely compute all entries of the DBCT of $F(x)$ over $\\gf_{2^n}$ for arbitrary $n$. Additionally, we provide the precise number of elements with a given entry by means of the values of some Kloosterman sums.","Further, we determine the double boomerang uniformity of $F(x)$ over $\\gf_{2^n}$ for arbitrary $n$. Our in-depth analysis of the DBCT of $F(x)$ contributes to a better evaluation of the S-box's resistance against boomerang attacks."],"url":"http://arxiv.org/abs/2404.12208v1","category":"cs.CR"}
{"created":"2024-04-18 13:58:27","title":"A congruence theorem for compact embedded hypersurfaces in $\\mathbb{S}^{n+1}_+$","abstract":"We prove a codimension reduction and congruence theorem for compact $n$-dimensional submanifolds of $\\mathbb{S}^{n+p}$ that admit a mean convex isometric embedding into $\\mathbb{S}^{n+1}_+$ using a Reilly type formula for space forms.","sentences":["We prove a codimension reduction and congruence theorem for compact $n$-dimensional submanifolds of $\\mathbb{S}^{n+p}$ that admit a mean convex isometric embedding into $\\mathbb{S}^{n+1}_+$ using a Reilly type formula for space forms."],"url":"http://arxiv.org/abs/2404.12197v1","category":"math.DG"}
{"created":"2024-04-18 12:58:56","title":"Low-rank alternating direction doubling algorithm for solving large-scale continuous time algebraic Riccati equations","abstract":"This paper proposes an effective low-rank alternating direction doubling algorithm (R-ADDA) for computing numerical low-rank solutions to large-scale sparse continuous-time algebraic Riccati matrix equations. The method is based on the alternating direction doubling algorithm (ADDA), utilizing the low-rank property of matrices and employing Cholesky factorization for solving. The advantage of the new algorithm lies in computing only the $2^k$-th approximation during the iterative process, instead of every approximation. Its efficient low-rank formula saves storage space and is highly effective from a computational perspective. Finally, the effectiveness of the new algorithm is demonstrated through theoretical analysis and numerical experiments.","sentences":["This paper proposes an effective low-rank alternating direction doubling algorithm (R-ADDA) for computing numerical low-rank solutions to large-scale sparse continuous-time algebraic Riccati matrix equations.","The method is based on the alternating direction doubling algorithm (ADDA), utilizing the low-rank property of matrices and employing Cholesky factorization for solving.","The advantage of the new algorithm lies in computing only the $2^k$-th approximation during the iterative process, instead of every approximation.","Its efficient low-rank formula saves storage space and is highly effective from a computational perspective.","Finally, the effectiveness of the new algorithm is demonstrated through theoretical analysis and numerical experiments."],"url":"http://arxiv.org/abs/2404.12155v1","category":"math.NA"}
{"created":"2024-04-18 12:40:08","title":"Global regularity of integral 2-varifolds with square integrable mean curvature","abstract":"We provide sharp sufficient criteria for an integral $2$-varifold to be induced by a $W^{2,2}$-conformal immersion of a smooth surface. Our approach is based on a fine analysis of the Hausdorff density for $2$-varifolds with critical integrability of the mean curvature and a recent local regularity result by Bi-Zhou. In codimension one, there are only three possible density values below $2$, each of which can be attained with equality in the Li--Yau inequality for the Willmore functional by the unit sphere, the double bubble, and the triple bubble. We show that below an optimal threshold for the Willmore energy, a varifold induced by a current without boundary is in fact a curvature varifold with a uniform bound on its second fundamental form. Consequently, the minimization of the Willmore functional in the class of curvature varifolds with prescribed even Euler characteristic provides smooth solutions for the Willmore problem. In particular, the \"ambient\" varifold approach and the \"parametric\" approach are equivalent for minimizing the Willmore energy.","sentences":["We provide sharp sufficient criteria for an integral $2$-varifold to be induced by a $W^{2,2}$-conformal immersion of a smooth surface.","Our approach is based on a fine analysis of the Hausdorff density for $2$-varifolds with critical integrability of the mean curvature and a recent local regularity result by Bi-Zhou.","In codimension one, there are only three possible density values below $2$, each of which can be attained with equality in the Li--Yau inequality for the Willmore functional by the unit sphere, the double bubble, and the triple bubble.","We show that below an optimal threshold for the Willmore energy, a varifold induced by a current without boundary is in fact a curvature varifold with a uniform bound on its second fundamental form.","Consequently, the minimization of the Willmore functional in the class of curvature varifolds with prescribed even Euler characteristic provides smooth solutions for the Willmore problem.","In particular, the \"ambient\" varifold approach and the \"parametric\" approach are equivalent for minimizing the Willmore energy."],"url":"http://arxiv.org/abs/2404.12136v1","category":"math.DG"}
{"created":"2024-04-18 11:48:33","title":"Revisiting holographic model for thermal and dense QCD with a critical point","abstract":"To quantitatively provide reliable predictions for the hot and dense QCD matter, a holographic model should be adjusted to describe first-principles lattice results available at vanishing baryon chemical potential. The equation of state from two well-known lattice groups, the HotQCD collaboration and the Wuppertal-Budapest (WB) collaboration, shows visible differences at high temperatures. We revisit the Einstein-Maxwell-dilaton (EMD) holographic model for hot QCD with 2+1 flavors and physical quark masses by fitting lattice QCD data from the WB collaboration. Using the parameterization for the scalar potential and gauge coupling proposed in our work [Phys.Rev.D 106 (2022) 12, L121902], the equation of state, the higher order baryon number susceptibilities, and the chiral condensates are in quantitative agreement with state-of-the-art lattice results. We find that the critical endpoint (CEP) obtained from fitting the WB collaboration data is nearly identical to the one from the HotQCD collaboration, suggesting the robustness of the location of the CEP. Moreover, our holographic prediction for the CEP location is in accord with more recent Bayesian analysis on a large number of holographic EMD models and an effective potential approach of QCD from gap equations.","sentences":["To quantitatively provide reliable predictions for the hot and dense QCD matter, a holographic model should be adjusted to describe first-principles lattice results available at vanishing baryon chemical potential.","The equation of state from two well-known lattice groups, the HotQCD collaboration and the Wuppertal-Budapest (WB) collaboration, shows visible differences at high temperatures.","We revisit the Einstein-Maxwell-dilaton (EMD) holographic model for hot QCD with 2+1 flavors and physical quark masses by fitting lattice QCD data from the WB collaboration.","Using the parameterization for the scalar potential and gauge coupling proposed in our work [Phys.Rev.D 106 (2022) 12, L121902], the equation of state, the higher order baryon number susceptibilities, and the chiral condensates are in quantitative agreement with state-of-the-art lattice results.","We find that the critical endpoint (CEP) obtained from fitting the WB collaboration data is nearly identical to the one from the HotQCD collaboration, suggesting the robustness of the location of the CEP.","Moreover, our holographic prediction for the CEP location is in accord with more recent Bayesian analysis on a large number of holographic EMD models and an effective potential approach of QCD from gap equations."],"url":"http://arxiv.org/abs/2404.12109v1","category":"hep-ph"}
{"created":"2024-04-18 11:36:37","title":"S3R-Net: A Single-Stage Approach to Self-Supervised Shadow Removal","abstract":"In this paper we present S3R-Net, the Self-Supervised Shadow Removal Network. The two-branch WGAN model achieves self-supervision relying on the unify-and-adaptphenomenon - it unifies the style of the output data and infers its characteristics from a database of unaligned shadow-free reference images. This approach stands in contrast to the large body of supervised frameworks. S3R-Net also differentiates itself from the few existing self-supervised models operating in a cycle-consistent manner, as it is a non-cyclic, unidirectional solution. The proposed framework achieves comparable numerical scores to recent selfsupervised shadow removal models while exhibiting superior qualitative performance and keeping the computational cost low.","sentences":["In this paper we present S3R-Net, the Self-Supervised Shadow Removal Network.","The two-branch WGAN model achieves self-supervision relying on the unify-and-adaptphenomenon - it unifies the style of the output data and infers its characteristics from a database of unaligned shadow-free reference images.","This approach stands in contrast to the large body of supervised frameworks.","S3R-Net also differentiates itself from the few existing self-supervised models operating in a cycle-consistent manner, as it is a non-cyclic, unidirectional solution.","The proposed framework achieves comparable numerical scores to recent selfsupervised shadow removal models while exhibiting superior qualitative performance and keeping the computational cost low."],"url":"http://arxiv.org/abs/2404.12103v1","category":"cs.CV"}
{"created":"2024-04-18 10:13:35","title":"Nonexistence of solutions to parabolic problems with a potential on weighted graphs","abstract":"We investigate nonexistence of nontrivial nonnegative solutions to a class of semilinear parabolic equations with a positive potential, posed on weighted graphs. Assuming an upper bound on the Laplacian of the distance and a suitable weighted space-time volume growth condition, we show that no global solutions exists. We also discuss the optimality of the hypotheses, thus recovering a critical exponent phenomenon of Fujita type.","sentences":["We investigate nonexistence of nontrivial nonnegative solutions to a class of semilinear parabolic equations with a positive potential, posed on weighted graphs.","Assuming an upper bound on the Laplacian of the distance and a suitable weighted space-time volume growth condition, we show that no global solutions exists.","We also discuss the optimality of the hypotheses, thus recovering a critical exponent phenomenon of Fujita type."],"url":"http://arxiv.org/abs/2404.12058v1","category":"math.AP"}
{"created":"2024-04-18 10:10:35","title":"On the asymptotic behavior of a diffraction problem with a thin layer","abstract":"We investigate the behavior of the solution to an elliptic diffraction problem in the union of a smooth set $\\Omega$ and a thin layer $\\Sigma$ locally described by $\\varepsilon h$, where $h$ is a positive function defined on the boundary $\\partial\\Omega$, and $\\varepsilon$ is the ellipticity constant of the differential operator in the thin layer $\\Sigma$. We study the problem in the limit for $\\varepsilon$ going to zero and prove a first-order asymptotic development by $\\Gamma$-convergence of the associated energy functional.","sentences":["We investigate the behavior of the solution to an elliptic diffraction problem in the union of a smooth set $\\Omega$ and a thin layer $\\Sigma$ locally described by $\\varepsilon h$, where $h$ is a positive function defined on the boundary $\\partial\\Omega$, and $\\varepsilon$ is the ellipticity constant of the differential operator in the thin layer $\\Sigma$. We study the problem in the limit for $\\varepsilon$ going to zero and prove a first-order asymptotic development by $\\Gamma$-convergence of the associated energy functional."],"url":"http://arxiv.org/abs/2404.12054v1","category":"math.AP"}
{"created":"2024-04-18 09:12:46","title":"The relic density and temperature evolution of light dark sector","abstract":"We have developed a set of four fully coupled Boltzmann equations to precisely determine the relic density and temperature of dark matter by including three distinct sectors: dark matter, light scalar, and standard model sectors. The intricacies of heat transfer between DM and the SM sector through a light scalar particle are explored, inspired by stringent experimental constraints on the scalar-Higgs mixing angle and the DM-scalar coupling. Three distinct sectors emerge prior to DM freeze-out, requiring fully coupled Boltzmann equations to accurately compute relic density. Investigation of forbidden, resonance, and secluded DM scenarios demonstrates significant deviations between established methods and the novel approach with fully coupled Boltzmann equations. Despite increased computational demands, this emphasizes the need for improved precision in relic density calculations, underlining the importance of incorporating these equations in comprehensive analyses.","sentences":["We have developed a set of four fully coupled Boltzmann equations to precisely determine the relic density and temperature of dark matter by including three distinct sectors: dark matter, light scalar, and standard model sectors.","The intricacies of heat transfer between DM and the SM sector through a light scalar particle are explored, inspired by stringent experimental constraints on the scalar-Higgs mixing angle and the DM-scalar coupling.","Three distinct sectors emerge prior to DM freeze-out, requiring fully coupled Boltzmann equations to accurately compute relic density.","Investigation of forbidden, resonance, and secluded DM scenarios demonstrates significant deviations between established methods and the novel approach with fully coupled Boltzmann equations.","Despite increased computational demands, this emphasizes the need for improved precision in relic density calculations, underlining the importance of incorporating these equations in comprehensive analyses."],"url":"http://arxiv.org/abs/2404.12019v1","category":"hep-ph"}
{"created":"2024-04-18 08:38:32","title":"Spectral determinant for the wave equation on an interval with Dirac damping","abstract":"A closed formula for the spectral determinant for the wave equation on a bounded interval, subject to Dirichlet boundary conditions and an $\\alpha$-multiple of the Dirac $\\delta$-type damping, is derived. Depending on the choice of the branch cut of the logarithm used in its definition, the spectral determinant diverges either for $\\alpha =2$ or $\\alpha=-2$.","sentences":["A closed formula for the spectral determinant for the wave equation on a bounded interval, subject to Dirichlet boundary conditions and an $\\alpha$-multiple of the Dirac $\\delta$-type damping, is derived.","Depending on the choice of the branch cut of the logarithm used in its definition, the spectral determinant diverges either for $\\alpha =2$ or $\\alpha=-2$."],"url":"http://arxiv.org/abs/2404.11992v1","category":"math.SP"}
{"created":"2024-04-18 08:37:39","title":"Pohozaev identities and Kelvin transformation of semilinear Grushin equation","abstract":"In this paper, we study Pohozaev identities, Kelvin transformation and their applications of semilinear Grushin equation. First, we establish two Pohozaev identities generated from translations and determine the location of the concentration point for solution of a kind of Grushin equation by such identities. Next, we establish Pohozaev identity generated from scaling and prove the nonexistence of nontrivial solutions of another kind of Grushin equation by such identity. Finally, we provide the change of Grushin operator by Kelvin transformation and obtain the decay rate of solution at infinity for a critical Grushin equation by Kelvin transformation.","sentences":["In this paper, we study Pohozaev identities, Kelvin transformation and their applications of semilinear Grushin equation.","First, we establish two Pohozaev identities generated from translations and determine the location of the concentration point for solution of a kind of Grushin equation by such identities.","Next, we establish Pohozaev identity generated from scaling and prove the nonexistence of nontrivial solutions of another kind of Grushin equation by such identity.","Finally, we provide the change of Grushin operator by Kelvin transformation and obtain the decay rate of solution at infinity for a critical Grushin equation by Kelvin transformation."],"url":"http://arxiv.org/abs/2404.11991v1","category":"math.AP"}
{"created":"2024-04-18 08:07:06","title":"Analytical calculation of Kerr and Kerr-Ads black holes in $f(R)$ theory","abstract":"In this paper, we extend Chandrasekhar's method of calculating rotating black holes into $f(R)$ theory. We consider the Ricci scalar is a constant and derive the Kerr and Kerr-Ads metric by using the analytical mathematical method. Suppose that the spacetime is a 4-dimensional Riemannian manifold with a general stationary axisymmetric metric, we calculate Cartan's equation of structure and derive the Einstein tensor. In order to reduce the solving difficulty, we fix the gauge freedom to transform the metric into a more symmetric form. We solve the field equations in the two cases of the Ricci scalar $R=0$ and $R\\neq 0$. In the case of $R=0$, the Ernst's equations are derived. We give the elementary solution of Ernst's equations and show the way to obtain more solutions including Kerr metric. In the case of $R\\neq 0$, we reasonably assume that the solution to the equations consists of two parts: the first is Kerr part and the second is introduced by the Ricci scalar. Giving solution to the second part and combining the two parts, we obtain the Kerr-Ads metric. The calculations are carried out in a general $f(R)$ theory, indicating the Kerr and Kerr-Ads black holes exist widely in general $f(R)$ models. Furthermore, the whole solving process can be treated as a standard calculation procedure to obtain rotating black holes, which can be applied to other modified gravities.","sentences":["In this paper, we extend Chandrasekhar's method of calculating rotating black holes into $f(R)$ theory.","We consider the Ricci scalar is a constant and derive the Kerr and Kerr-Ads metric by using the analytical mathematical method.","Suppose that the spacetime is a 4-dimensional Riemannian manifold with a general stationary axisymmetric metric, we calculate Cartan's equation of structure and derive the Einstein tensor.","In order to reduce the solving difficulty, we fix the gauge freedom to transform the metric into a more symmetric form.","We solve the field equations in the two cases of the Ricci scalar $R=0$ and $R\\neq 0$.","In the case of $R=0$, the Ernst's equations are derived.","We give the elementary solution of Ernst's equations and show the way to obtain more solutions including Kerr metric.","In the case of $R\\neq 0$, we reasonably assume that the solution to the equations consists of two parts: the first is Kerr part and the second is introduced by the Ricci scalar.","Giving solution to the second part and combining the two parts, we obtain the Kerr-Ads metric.","The calculations are carried out in a general $f(R)$ theory, indicating the Kerr and Kerr-Ads black holes exist widely in general $f(R)$ models.","Furthermore, the whole solving process can be treated as a standard calculation procedure to obtain rotating black holes, which can be applied to other modified gravities."],"url":"http://arxiv.org/abs/2404.11975v1","category":"gr-qc"}
{"created":"2024-04-18 07:53:20","title":"A $\u03b4$-first Whitehead Lemma for Jordan algebras","abstract":"We compute $\\delta$-derivations of simple Jordan algebras with values in irreducible bimodules. They turn out to be either ordinary derivations ($\\delta = 1$), or scalar multiples of the identity map ($\\delta = \\frac 12$). This can be considered as a generalization of the \"First Whitehead Lemma\" for Jordan algebras which claims that all such ordinary derivations are inner. The proof amounts to simple calculations in matrix algebras, or, in the case of Jordan algebras of a symmetric bilinear form, to more elaborated calculations in Clifford algebras.","sentences":["We compute $\\delta$-derivations of simple Jordan algebras with values in irreducible bimodules.","They turn out to be either ordinary derivations ($\\delta = 1$), or scalar multiples of the identity map ($\\delta = \\frac 12$).","This can be considered as a generalization of the \"First Whitehead Lemma\" for Jordan algebras which claims that all such ordinary derivations are inner.","The proof amounts to simple calculations in matrix algebras, or, in the case of Jordan algebras of a symmetric bilinear form, to more elaborated calculations in Clifford algebras."],"url":"http://arxiv.org/abs/2404.11966v1","category":"math.RA"}
{"created":"2024-04-18 05:08:31","title":"Inflationary dynamics in modified gravity models","abstract":"Higher-order theories of gravity are a branch of modified gravity wherein the geometrodynamics of the four-dimensional Riemannian manifold is determined by field equations involving derivatives of the metric tensor of order higher than two. This paper considers a general action built with the Einstein-Hilbert term plus additional curvature-based invariants, viz. the Starobinsky $R^{2}$-type term, a term scaling with $R^{3}$, and a correction of the type $R\\square R$. The focus is on the background inflationary regime accommodated by these three models. For that, the higher-order field equations are built and specified for the FLRW line element. The dynanical analysis in the phase space is carried in each case. This analysis shows that the Starobinsky-plus-$R^{3}$ model keeps the good features exhibited by the pure Starobinsky inflationary model, although the set of initial conditions for the inflaton field $\\chi$ leading to a graceful exit scenario is more contrived; the coupling constant $\\alpha_{0}$ of the $R^{3}$ invariant is also constrained by the dynamical analysis. The Starobinsky-plus-$R\\square R$ model turns out being a double-field inflation model; it consistently enables an almost-exponential primordial acceleration followed by a radiation dominated universe if its coupling $\\beta_{0}$ takes values in the interval $0\\leq\\beta_{0}\\leq3/4$. The models introducing higher-order correction to Starobinsky inflation are interesting due to the possibility of a running spectral index $n_{s}$, something that is allowed by current CMB observations.","sentences":["Higher-order theories of gravity are a branch of modified gravity wherein the geometrodynamics of the four-dimensional Riemannian manifold is determined by field equations involving derivatives of the metric tensor of order higher than two.","This paper considers a general action built with the Einstein-Hilbert term plus additional curvature-based invariants, viz.","the Starobinsky $R^{2}$-type term, a term scaling with $R^{3}$, and a correction of the type $R\\square R$.","The focus is on the background inflationary regime accommodated by these three models.","For that, the higher-order field equations are built and specified for the FLRW line element.","The dynanical analysis in the phase space is carried in each case.","This analysis shows that the Starobinsky-plus-$R^{3}$ model keeps the good features exhibited by the pure Starobinsky inflationary model, although the set of initial conditions for the inflaton field $\\chi$ leading to a graceful exit scenario is more contrived; the coupling constant $\\alpha_{0}$ of the $R^{3}$ invariant is also constrained by the dynamical analysis.","The Starobinsky-plus-$R\\square R$ model turns out being a double-field inflation model; it consistently enables an almost-exponential primordial acceleration followed by a radiation dominated universe if its coupling $\\beta_{0}$ takes values in the interval $0\\leq\\beta_{0}\\leq3/4$.","The models introducing higher-order correction to Starobinsky inflation are interesting due to the possibility of a running spectral index $n_{s}$, something that is allowed by current CMB observations."],"url":"http://arxiv.org/abs/2404.11904v1","category":"gr-qc"}
{"created":"2024-04-18 05:06:05","title":"On the spectrum of a differential operator on a Hilbert-P\u00f3lya space","abstract":"In this paper we study the spectrum of a fundamental differential operator on a Hilbert-P\\'olya space. A number is an eigenvalue of this differential operator if and only if it is a nontrivial zero of the Riemann zeta function. An explicit formula is given for the eigenfunction associated with each nontrivial zero of the zeta function. Every eigenfunction is characterized via the Poisson summation formula by a sequence of mysterious functions whose explicit formulas are given.","sentences":["In this paper we study the spectrum of a fundamental differential operator on a Hilbert-P\\'olya space.","A number is an eigenvalue of this differential operator if and only if it is a nontrivial zero of the Riemann zeta function.","An explicit formula is given for the eigenfunction associated with each nontrivial zero of the zeta function.","Every eigenfunction is characterized via the Poisson summation formula by a sequence of mysterious functions whose explicit formulas are given."],"url":"http://arxiv.org/abs/2404.11902v1","category":"math.CA"}
{"created":"2024-04-18 04:54:28","title":"AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height Large-scale Outdoor Scene Rendering","abstract":"Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF.","sentences":["Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude.","Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes.","In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes.","Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering.","Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF."],"url":"http://arxiv.org/abs/2404.11897v1","category":"cs.CV"}
{"created":"2024-04-18 03:16:05","title":"On length-preserving and area-preserving inverse curvature flow of planar curves with singularities","abstract":"This paper aims to investigate the evolution problem for planar curves with singularities. Motivated by the inverse curvature flow introduced by Li and Wang (Calc. Var. Partial Differ. Equ. 62 (2023), No. 135), we intend to consider the area-preserving and length-preserving inverse curvature flow with nonlocal term for $\\ell$-convex Legendre curves. For the area-preserving flow, an $\\ell$-convex Legendre curve %of with initial algebraic area $A_0>0$ evolves to a circle of radius $\\sqrt{\\frac{A_0}{\\pi}}$. For the length-preserving flow, an $\\ell$-convex Legendre curve %of with initial algebraic length $L_0$ evolves to a circle of radius $\\frac{L_0}{2\\pi}$. As the by-product, we obtain some geometric inequalities for $\\ell$-convex Legendre curves through the length-preserving flow.","sentences":["This paper aims to investigate the evolution problem for planar curves with singularities.","Motivated by the inverse curvature flow introduced by Li and Wang (Calc.","Var.","Partial Differ.","Equ. 62 (2023), No. 135), we intend to consider the area-preserving and length-preserving inverse curvature flow with nonlocal term for $\\ell$-convex Legendre curves.","For the area-preserving flow, an $\\ell$-convex Legendre curve %of with initial algebraic area $A_0>0$ evolves to a circle of radius $\\sqrt{\\frac{A_0}{\\pi}}$. For the length-preserving flow, an $\\ell$-convex Legendre curve %of with initial algebraic length $L_0$ evolves to a circle of radius $\\frac{L_0}{2\\pi}$. As the by-product, we obtain some geometric inequalities for $\\ell$-convex Legendre curves through the length-preserving flow."],"url":"http://arxiv.org/abs/2404.11872v1","category":"math.DG"}
{"created":"2024-04-18 03:03:46","title":"Enhancing Length Extrapolation in Sequential Models with Pointer-Augmented Neural Memory","abstract":"We propose Pointer-Augmented Neural Memory (PANM) to help neural networks understand and apply symbol processing to new, longer sequences of data. PANM integrates an external neural memory that uses novel physical addresses and pointer manipulation techniques to mimic human and computer symbol processing abilities. PANM facilitates pointer assignment, dereference, and arithmetic by explicitly using physical pointers to access memory content. Remarkably, it can learn to perform these operations through end-to-end training on sequence data, powering various sequential models. Our experiments demonstrate PANM's exceptional length extrapolating capabilities and improved performance in tasks that require symbol processing, such as algorithmic reasoning and Dyck language recognition. PANM helps Transformer achieve up to 100% generalization accuracy in compositional learning tasks and significantly better results in mathematical reasoning, question answering and machine translation tasks.","sentences":["We propose Pointer-Augmented Neural Memory (PANM) to help neural networks understand and apply symbol processing to new, longer sequences of data.","PANM integrates an external neural memory that uses novel physical addresses and pointer manipulation techniques to mimic human and computer symbol processing abilities.","PANM facilitates pointer assignment, dereference, and arithmetic by explicitly using physical pointers to access memory content.","Remarkably, it can learn to perform these operations through end-to-end training on sequence data, powering various sequential models.","Our experiments demonstrate PANM's exceptional length extrapolating capabilities and improved performance in tasks that require symbol processing, such as algorithmic reasoning and Dyck language recognition.","PANM helps Transformer achieve up to 100% generalization accuracy in compositional learning tasks and significantly better results in mathematical reasoning, question answering and machine translation tasks."],"url":"http://arxiv.org/abs/2404.11870v1","category":"cs.LG"}
{"created":"2024-04-18 02:38:28","title":"Refined behavior and structural universality of the blow-up profile for the semilinear heat equation with non scale invariant nonlinearity","abstract":"We consider the semilinear heat equation $$u_t-\\Delta u=f(u) $$   for a large class of non scale invariant nonlinearities of the form   $f(u)=u^pL(u)$, where $p>1$ is Sobolev subcritical and $L$ is a slowly varying function (which includes for instance logarithms and their powers and iterates, as well as some strongly oscillating functions).   For any positive radial decreasing blow-up solution, we obtain the sharp, global blow-up profile in the scale of the original variables $(x, t)$, which takes the form:   $$u(x,t)=(1+o(1))\\,G^{-1}\\bigg(T-t+\\frac{p-1}{8p}\\frac{|x|^2}{|\\log |x||}\\bigg),   \\ \\hbox{as $(x,t)\\to (0,T)$, \\quad where } G(X)=\\int_{X}^{\\infty}\\frac{ ds}{f(s)}.$$ This estimate in particular provides the sharp final space profile and the refined space-time profile.   As a remarkable fact and completely new observation, our results reveal a {\\it structural universality} of the global blow-up profile, being given by the \"resolvent\" $G^{-1}$ of the ODE, composed with a universal, time-space building block, which is the same as in the pure power case.","sentences":["We consider the semilinear heat equation $$u_t-\\Delta u=f(u) $$   for a large class of non scale invariant nonlinearities of the form   $f(u)=u^pL(u)$, where $p>1$ is Sobolev subcritical and $L$ is a slowly varying function (which includes for instance logarithms and their powers and iterates, as well as some strongly oscillating functions).   ","For any positive radial decreasing blow-up solution, we obtain the sharp, global blow-up profile in the scale of the original variables $(x, t)$, which takes the form:   $$u(x,t)=(1+o(1))\\,G^{-1}\\bigg(T-t+\\frac{p-1}{8p}\\frac{|x|^2}{|\\log |x||}\\bigg),   \\ \\hbox{as","$(x,t)\\to (0,T)$, \\quad where } G(X)=\\int_{X}^{\\infty}\\frac{ ds}{f(s)}.$$","This estimate in particular provides the sharp final space profile and the refined space-time profile.   ","As a remarkable fact and completely new observation, our results reveal a {\\it structural universality} of the global blow-up profile, being given by the \"resolvent\" $G^{-1}$ of the ODE, composed with a universal, time-space building block, which is the same as in the pure power case."],"url":"http://arxiv.org/abs/2404.11863v1","category":"math.AP"}
{"created":"2024-04-18 02:08:28","title":"General non-linear fragmentation with discontinuous Galerkin methods","abstract":"Dust grains play a significant role in several astrophysical processes, including gas/dust dynamics, chemical reactions, and radiative transfer. Replenishment of small-grain populations is mainly governed by fragmentation during pair-wise collisions between grains. The wide spectrum of fragmentation outcomes, from complete disruption to erosion and/or mass transfer, can be modelled by the general non-linear fragmentation equation. Efficiently solving this equation is crucial for an accurate treatment of the dust fragmentation in numerical modelling. However, similar to dust coagulation, numerical errors in current fragmentation algorithms employed in astrophysics are dominated by the numerical over-diffusion problem -- particularly in 3D hydrodynamic simulations where the discrete resolution of the mass density distribution tends to be highly limited. With this in mind, we have derived the first conservative form of the general non-linear fragmentation with a mass flux highlighting the mass transfer phenomenon. Then, to address cases of limited mass density resolution, we applied a high-order discontinuous Galerkin scheme to efficiently solve the conservative fragmentation equation with a reduced number of dust bins. An accuracy of 0.1 -1% is reached with 20 dust bins spanning a mass range of 9 orders of magnitude.","sentences":["Dust grains play a significant role in several astrophysical processes, including gas/dust dynamics, chemical reactions, and radiative transfer.","Replenishment of small-grain populations is mainly governed by fragmentation during pair-wise collisions between grains.","The wide spectrum of fragmentation outcomes, from complete disruption to erosion and/or mass transfer, can be modelled by the general non-linear fragmentation equation.","Efficiently solving this equation is crucial for an accurate treatment of the dust fragmentation in numerical modelling.","However, similar to dust coagulation, numerical errors in current fragmentation algorithms employed in astrophysics are dominated by the numerical over-diffusion problem -- particularly in 3D hydrodynamic simulations where the discrete resolution of the mass density distribution tends to be highly limited.","With this in mind, we have derived the first conservative form of the general non-linear fragmentation with a mass flux highlighting the mass transfer phenomenon.","Then, to address cases of limited mass density resolution, we applied a high-order discontinuous Galerkin scheme to efficiently solve the conservative fragmentation equation with a reduced number of dust bins.","An accuracy of 0.1 -1% is reached with 20 dust bins spanning a mass range of 9 orders of magnitude."],"url":"http://arxiv.org/abs/2404.11851v1","category":"astro-ph.SR"}
{"created":"2024-04-18 01:51:11","title":"Directional intense terahertz radiation driven by abruptly autofocusing lasers in air","abstract":"Two-color laser induced plasma filamentation in air could serve as tabletop sources of broadband terahertz (THz) pulses. Ubiquitous air in the earth facilitates its widespread utilities, particularly, in wireless communication and remote sensing, exploiting the unique advantage of the air-based scheme that the THz sources can be delivered over standoff distances via the pump laser propagation in air. However, the THz emission pattern inevitably has a conical angular profile with a dip in the propagation axis, therefore, THz energy concentration, propagation directionality, and accuracy of signal demodulation are significantly impaired, greatly limiting its direct applications. Here, we successfully eliminate the unfavorable conical profile by experiments and meanwhile enhance THz directionality and intensity by 17 folds by use of abruptly-autofocusing laser beams. Our theory and simulations show that these observations are attributed to efficient suppression of the dephasing effect appearing in previous investigations with ordinary Gaussian laser beams. This scheme is easily accessible since the abruptly-autofocusing beam can be achieved by imposing a spatial light modulator on the input Gaussian beam. This study solves a long-standing problem in the two-color laser scheme and clarifies the underlying physics of the conical angular profile formation.","sentences":["Two-color laser induced plasma filamentation in air could serve as tabletop sources of broadband terahertz (THz) pulses.","Ubiquitous air in the earth facilitates its widespread utilities, particularly, in wireless communication and remote sensing, exploiting the unique advantage of the air-based scheme that the THz sources can be delivered over standoff distances via the pump laser propagation in air.","However, the THz emission pattern inevitably has a conical angular profile with a dip in the propagation axis, therefore, THz energy concentration, propagation directionality, and accuracy of signal demodulation are significantly impaired, greatly limiting its direct applications.","Here, we successfully eliminate the unfavorable conical profile by experiments and meanwhile enhance THz directionality and intensity by 17 folds by use of abruptly-autofocusing laser beams.","Our theory and simulations show that these observations are attributed to efficient suppression of the dephasing effect appearing in previous investigations with ordinary Gaussian laser beams.","This scheme is easily accessible since the abruptly-autofocusing beam can be achieved by imposing a spatial light modulator on the input Gaussian beam.","This study solves a long-standing problem in the two-color laser scheme and clarifies the underlying physics of the conical angular profile formation."],"url":"http://arxiv.org/abs/2404.11846v1","category":"physics.optics"}
{"created":"2024-04-18 01:43:54","title":"Gromov-Hausdorff continuity of non-K\u00e4hler Calabi-Yau conifold transitions","abstract":"We study the geometry of Calabi-Yau conifold transitions. This deformation process is known to possibly connect a K\\\"ahler threefold to a non-K\\\"ahler threefold. We use balanced and Hermitian-Yang-Mills metrics to geometrize the conifold transition and show that the whole operation is continuous in the Gromov-Hausdorff topology.","sentences":["We study the geometry of Calabi-Yau conifold transitions.","This deformation process is known to possibly connect a K\\\"ahler threefold to a non-K\\\"ahler threefold.","We use balanced and Hermitian-Yang-Mills metrics to geometrize the conifold transition and show that the whole operation is continuous in the Gromov-Hausdorff topology."],"url":"http://arxiv.org/abs/2404.11840v1","category":"math.DG"}
{"created":"2024-04-18 01:05:14","title":"A class of maximum-based iteration methods for the generalized absolute value equation","abstract":"In this paper, by using $|x|=2\\max\\{0,x\\}-x$, a class of maximum-based iteration methods is established to solve the generalized absolute value equation $Ax-B|x|=b$. Some convergence conditions of the proposed method are presented. By some numerical experiments, the effectiveness and feasibility of the proposed method are confirmed.","sentences":["In this paper, by using $|x|=2\\max\\{0,x\\}-x$, a class of maximum-based iteration methods is established to solve the generalized absolute value equation $Ax-B|x|=b$. Some convergence conditions of the proposed method are presented.","By some numerical experiments, the effectiveness and feasibility of the proposed method are confirmed."],"url":"http://arxiv.org/abs/2404.11822v1","category":"math.NA"}
{"created":"2024-04-17 22:20:02","title":"Analysis of Evolutionary Diversity Optimisation for the Maximum Matching Problem","abstract":"This paper explores the enhancement of solution diversity in evolutionary algorithms (EAs) for the maximum matching problem, concentrating on complete bipartite graphs and paths. We adopt binary string encoding for matchings and use Hamming distance to measure diversity, aiming for its maximization. Our study centers on the $(\\mu+1)$-EA and $2P-EA_D$, which are applied to optimize diversity. We provide a rigorous theoretical and empirical analysis of these algorithms.   For complete bipartite graphs, our runtime analysis shows that, with a reasonably small $\\mu$, the $(\\mu+1)$-EA achieves maximal diversity with an expected runtime of $O(\\mu^2 m^4 \\log(m))$ for the small gap case (where the population size $\\mu$ is less than the difference in the sizes of the bipartite partitions) and $O(\\mu^2 m^2 \\log(m))$ otherwise. For paths, we establish an upper runtime bound of $O(\\mu^3 m^3)$. The $2P-EA_D$ displays stronger performance, with bounds of $O(\\mu^2 m^2 \\log(m))$ for the small gap case, $O(\\mu^2 n^2 \\log(n))$ otherwise, and $O(\\mu^3 m^2)$ for paths. Here, $n$ represents the total number of vertices and $m$ the number of edges. Our empirical studies, which examine the scaling behavior with respect to $m$ and $\\mu$, complement these theoretical insights and suggest potential for further refinement of the runtime bounds.","sentences":["This paper explores the enhancement of solution diversity in evolutionary algorithms (EAs) for the maximum matching problem, concentrating on complete bipartite graphs and paths.","We adopt binary string encoding for matchings and use Hamming distance to measure diversity, aiming for its maximization.","Our study centers on the $(\\mu+1)$-EA and $2P-EA_D$, which are applied to optimize diversity.","We provide a rigorous theoretical and empirical analysis of these algorithms.   ","For complete bipartite graphs, our runtime analysis shows that, with a reasonably small $\\mu$, the $(\\mu+1)$-EA achieves maximal diversity with an expected runtime of $O(\\mu^2 m^4 \\log(m))$ for the small gap case (where the population size $\\mu$ is less than the difference in the sizes of the bipartite partitions) and $O(\\mu^2 m^2 \\log(m))$ otherwise.","For paths, we establish an upper runtime bound of $O(\\mu^3 m^3)$. The $2P-EA_D$ displays stronger performance, with bounds of $O(\\mu^2 m^2 \\log(m))$ for the small gap case, $O(\\mu^2 n^2 \\log(n))$ otherwise, and $O(\\mu^3 m^2)$ for paths.","Here, $n$ represents the total number of vertices and $m$ the number of edges.","Our empirical studies, which examine the scaling behavior with respect to $m$ and $\\mu$, complement these theoretical insights and suggest potential for further refinement of the runtime bounds."],"url":"http://arxiv.org/abs/2404.11784v1","category":"cs.NE"}
{"created":"2024-04-17 21:56:13","title":"Periodic traveling waves for nonlinear Schr\u00f6dinger equations with non-zero conditions at infinity in $ \\R ^2 $","abstract":"We consider the nonlinear Schr\\\"odinger equation with nonzero conditions at infinity in $\\R^2$. We investigate the existence of traveling waves that are periodic in the direction transverse to the direction of propagation and minimize the energy when the momentum is kept fixed. We show that for any given value of the momentum, there is a critical value of the period such that traveling waves with period smaller than the critical value are one-dimensional, and those with larger periods depend on two variables.","sentences":["We consider the nonlinear Schr\\\"odinger equation with nonzero conditions at infinity in $\\R^2$. We investigate the existence of traveling waves that are periodic in the direction transverse to the direction of propagation and minimize the energy when the momentum is kept fixed.","We show that for any given value of the momentum, there is a critical value of the period such that traveling waves with period smaller than the critical value are one-dimensional, and those with larger periods depend on two variables."],"url":"http://arxiv.org/abs/2404.11772v1","category":"math.AP"}
{"created":"2024-04-17 21:52:21","title":"QGen: On the Ability to Generalize in Quantization Aware Training","abstract":"Quantization lowers memory usage, computational requirements, and latency by utilizing fewer bits to represent model weights and activations. In this work, we investigate the generalization properties of quantized neural networks, a characteristic that has received little attention despite its implications on model performance. In particular, first, we develop a theoretical model for quantization in neural networks and demonstrate how quantization functions as a form of regularization. Second, motivated by recent work connecting the sharpness of the loss landscape and generalization, we derive an approximate bound for the generalization of quantized models conditioned on the amount of quantization noise. We then validate our hypothesis by experimenting with over 2000 models trained on CIFAR-10, CIFAR-100, and ImageNet datasets on convolutional and transformer-based models.","sentences":["Quantization lowers memory usage, computational requirements, and latency by utilizing fewer bits to represent model weights and activations.","In this work, we investigate the generalization properties of quantized neural networks, a characteristic that has received little attention despite its implications on model performance.","In particular, first, we develop a theoretical model for quantization in neural networks and demonstrate how quantization functions as a form of regularization.","Second, motivated by recent work connecting the sharpness of the loss landscape and generalization, we derive an approximate bound for the generalization of quantized models conditioned on the amount of quantization noise.","We then validate our hypothesis by experimenting with over 2000 models trained on CIFAR-10, CIFAR-100, and ImageNet datasets on convolutional and transformer-based models."],"url":"http://arxiv.org/abs/2404.11769v1","category":"cs.LG"}
{"created":"2024-04-17 21:49:45","title":"End-to-End Mesh Optimization of a Hybrid Deep Learning Black-Box PDE Solver","abstract":"Deep learning has been widely applied to solve partial differential equations (PDEs) in computational fluid dynamics. Recent research proposed a PDE correction framework that leverages deep learning to correct the solution obtained by a PDE solver on a coarse mesh. However, end-to-end training of such a PDE correction model over both solver-dependent parameters such as mesh parameters and neural network parameters requires the PDE solver to support automatic differentiation through the iterative numerical process. Such a feature is not readily available in many existing solvers. In this study, we explore the feasibility of end-to-end training of a hybrid model with a black-box PDE solver and a deep learning model for fluid flow prediction. Specifically, we investigate a hybrid model that integrates a black-box PDE solver into a differentiable deep graph neural network. To train this model, we use a zeroth-order gradient estimator to differentiate the PDE solver via forward propagation. Although experiments show that the proposed approach based on zeroth-order gradient estimation underperforms the baseline that computes exact derivatives using automatic differentiation, our proposed method outperforms the baseline trained with a frozen input mesh to the solver. Moreover, with a simple warm-start on the neural network parameters, we show that models trained by these zeroth-order algorithms achieve an accelerated convergence and improved generalization performance.","sentences":["Deep learning has been widely applied to solve partial differential equations (PDEs) in computational fluid dynamics.","Recent research proposed a PDE correction framework that leverages deep learning to correct the solution obtained by a PDE solver on a coarse mesh.","However, end-to-end training of such a PDE correction model over both solver-dependent parameters such as mesh parameters and neural network parameters requires the PDE solver to support automatic differentiation through the iterative numerical process.","Such a feature is not readily available in many existing solvers.","In this study, we explore the feasibility of end-to-end training of a hybrid model with a black-box PDE solver and a deep learning model for fluid flow prediction.","Specifically, we investigate a hybrid model that integrates a black-box PDE solver into a differentiable deep graph neural network.","To train this model, we use a zeroth-order gradient estimator to differentiate the PDE solver via forward propagation.","Although experiments show that the proposed approach based on zeroth-order gradient estimation underperforms the baseline that computes exact derivatives using automatic differentiation, our proposed method outperforms the baseline trained with a frozen input mesh to the solver.","Moreover, with a simple warm-start on the neural network parameters, we show that models trained by these zeroth-order algorithms achieve an accelerated convergence and improved generalization performance."],"url":"http://arxiv.org/abs/2404.11766v1","category":"cs.LG"}
{"created":"2024-04-17 21:20:09","title":"The Ramshaw-Mesina Hybrid Algorithm applied to the Navier Stokes Equations","abstract":"In 1991, Ramshaw and Mesina proposed a novel synthesis of penalty methods and artificial compression methods. When the two were balanced they found the combination was 3-4 orders more accurate than either alone. This report begins the study of their interesting method applied to the Navier-Stokes equations. We perform stability analysis, semi-discrete error analysis, and tests of the algorithm. Although most of the results for implicit time discretizations of our numerical tests comply with theirs for explicit time discretizations, the behavior in damping pressure oscillations and violations of incompressibility are different from their findings and our heuristic analysis.","sentences":["In 1991, Ramshaw and Mesina proposed a novel synthesis of penalty methods and artificial compression methods.","When the two were balanced they found the combination was 3-4 orders more accurate than either alone.","This report begins the study of their interesting method applied to the Navier-Stokes equations.","We perform stability analysis, semi-discrete error analysis, and tests of the algorithm.","Although most of the results for implicit time discretizations of our numerical tests comply with theirs for explicit time discretizations, the behavior in damping pressure oscillations and violations of incompressibility are different from their findings and our heuristic analysis."],"url":"http://arxiv.org/abs/2404.11755v1","category":"math.NA"}
{"created":"2024-04-17 20:23:07","title":"Postoperative glioblastoma segmentation: Development of a fully automated pipeline using deep convolutional neural networks and comparison with currently available models","abstract":"Accurately assessing tumor removal is paramount in the management of glioblastoma. We developed a pipeline using MRI scans and neural networks to segment tumor subregions and the surgical cavity in postoperative images. Our model excels in accurately classifying the extent of resection, offering a valuable tool for clinicians in assessing treatment effectiveness.","sentences":["Accurately assessing tumor removal is paramount in the management of glioblastoma.","We developed a pipeline using MRI scans and neural networks to segment tumor subregions and the surgical cavity in postoperative images.","Our model excels in accurately classifying the extent of resection, offering a valuable tool for clinicians in assessing treatment effectiveness."],"url":"http://arxiv.org/abs/2404.11725v1","category":"eess.IV"}
{"created":"2024-04-17 20:20:40","title":"Finslerian extension of an anisotropic strange star in the domain of modified gravity","abstract":"In this article, we apply the Finsler spacetime to develop the Einstein field equations in the extension of modified geometry. Following Finsler geometry, which is focused on the tangent bundle with a scalar function, a scalar equation should be the field equation that defines this structure. This spacetime maintains the required causality properties on the generalized Lorentzian metric manifold. The matter field is coupled with the Finsler geometry to produce the complete action. In this work, we use modified gravity to develop the Einstein field equations from the variational principle. Developed Einstein field equations are employed on the strange stellar system to improve the study. The interior of the system is made of a strange quark, maintained by the MIT Bag equation of state. In addition, the modified Tolman-Oppenheimer-Volkov (TOV) equation is formulated. In particular, the anisotropic stress attains the maximum at the surface. The mass-central density variation justifies the stability of the system.","sentences":["In this article, we apply the Finsler spacetime to develop the Einstein field equations in the extension of modified geometry.","Following Finsler geometry, which is focused on the tangent bundle with a scalar function, a scalar equation should be the field equation that defines this structure.","This spacetime maintains the required causality properties on the generalized Lorentzian metric manifold.","The matter field is coupled with the Finsler geometry to produce the complete action.","In this work, we use modified gravity to develop the Einstein field equations from the variational principle.","Developed Einstein field equations are employed on the strange stellar system to improve the study.","The interior of the system is made of a strange quark, maintained by the MIT Bag equation of state.","In addition, the modified Tolman-Oppenheimer-Volkov (TOV) equation is formulated.","In particular, the anisotropic stress attains the maximum at the surface.","The mass-central density variation justifies the stability of the system."],"url":"http://arxiv.org/abs/2404.11723v1","category":"gr-qc"}
{"created":"2024-04-17 19:39:10","title":"The topological complexity of the ordered configuration space of disks in a strip","abstract":"How hard is it to program $n$ robots to move about a long narrow aisle such that only $w$ of them can fit across the width of the aisle? In this paper, we answer that question by calculating the topological complexity of $\\text{conf}(n,w)$, the ordered configuration space of open unit-diameter disks in the infinite strip of width $w$. By studying its cohomology ring, we prove that, as long as $n$ is greater than $w$, the topological complexity of $\\text{conf}(n,w)$ is $2n-2\\big\\lceil\\frac{n}{w}\\big\\rceil+1$, providing a lower bound for the minimum number of cases such a program must consider.","sentences":["How hard is it to program $n$ robots to move about a long narrow aisle such that only $w$ of them can fit across the width of the aisle?","In this paper, we answer that question by calculating the topological complexity of $\\text{conf}(n,w)$, the ordered configuration space of open unit-diameter disks in the infinite strip of width $w$. By studying its cohomology ring, we prove that, as long as $n$ is greater than $w$, the topological complexity of $\\text{conf}(n,w)$ is $2n-2\\big\\lceil\\frac{n}{w}\\big\\rceil+1$, providing a lower bound for the minimum number of cases such a program must consider."],"url":"http://arxiv.org/abs/2404.11711v1","category":"math.AT"}
{"created":"2024-04-17 19:26:50","title":"Satisfiability of commutative vs. non-commutative CSPs","abstract":"The Mermin-Peres magic square is a celebrated example of a system of Boolean linear equations that is not (classically) satisfiable but is satisfiable via linear operators on a Hilbert space of dimension four. A natural question is then, for what kind of problems such a phenomenon occurs? Atserias, Kolaitis, and Severini answered this question for all Boolean Constraint Satisfaction Problems (CSPs): For 2-SAT, Horn-SAT, and Dual Horn-SAT, classical satisfiability and operator satisfiability is the same and thus there is no gap; for all other Boolean CSPs, the two notions differ as there is a gap, i.e., there are unsatisfiable instances that are satisfied via operators on a finite-dimensional Hilbert space. We generalize their result to CSPs on arbitrary finite domains: CSPs of so-called bounded-width have no satisfiability gap, whereas all other CSPs have a satisfiability gap.","sentences":["The Mermin-Peres magic square is a celebrated example of a system of Boolean linear equations that is not (classically) satisfiable but is satisfiable via linear operators on a Hilbert space of dimension four.","A natural question is then, for what kind of problems such a phenomenon occurs?","Atserias, Kolaitis, and Severini answered this question for all Boolean Constraint Satisfaction Problems (CSPs): For 2-SAT, Horn-SAT, and Dual Horn-SAT, classical satisfiability and operator satisfiability is the same and thus there is no gap; for all other Boolean CSPs, the two notions differ as there is a gap, i.e., there are unsatisfiable instances that are satisfied via operators on a finite-dimensional Hilbert space.","We generalize their result to CSPs on arbitrary finite domains: CSPs of so-called bounded-width have no satisfiability gap, whereas all other CSPs have a satisfiability gap."],"url":"http://arxiv.org/abs/2404.11709v1","category":"cs.CC"}
{"created":"2024-04-17 19:10:32","title":"Cosmological Solutions in Polynomial Affine Gravity with Torsion","abstract":"The Polynomial Affine Gravity is an alternative gravitational model, where the interactions are mediated solely by the affine connection, instead of the metric tensor. In this paper, we explore the space of solutions to the field equations when the torsion fields are turned on, in a homogeneous and isotropic (cosmological) scenario. We explore various metric structures that emerge in the space of solutions.","sentences":["The Polynomial Affine Gravity is an alternative gravitational model, where the interactions are mediated solely by the affine connection, instead of the metric tensor.","In this paper, we explore the space of solutions to the field equations when the torsion fields are turned on, in a homogeneous and isotropic (cosmological) scenario.","We explore various metric structures that emerge in the space of solutions."],"url":"http://arxiv.org/abs/2404.11703v1","category":"gr-qc"}
{"created":"2024-04-17 19:00:31","title":"The central limit theorem and rate of mixing for simple random walks on the circle","abstract":"We prove the Central Limit Theorem and superpolynomial mixing for environment viewed for the particle process in quasi periodic Diophantine random environment. The main ingredients are smoothness estimates for the solution of the Poisson equation and local limit asymptotics for certain accelerated walks.","sentences":["We prove the Central Limit Theorem and superpolynomial mixing for environment viewed for the particle process in quasi periodic Diophantine random environment.","The main ingredients are smoothness estimates for the solution of the Poisson equation and local limit asymptotics for certain accelerated walks."],"url":"http://arxiv.org/abs/2404.11700v1","category":"math.DS"}
{"created":"2024-04-17 18:50:34","title":"Saddle solutions for Allen-Cahn type equations involving the prescribed mean curvature operator","abstract":"The goal of this paper is to investigate the existence of saddle solutions for some classes of elliptic partial differential equations of the Allen-Cahn type, formulated as follows:   \\begin{equation*}   -div\\left(\\frac{\\nabla u}{\\sqrt{1+|\\nabla u|^2}}\\right) + A(x,y)V'(u)=0~~\\text{ in }~~\\mathbb{R}^2.   \\end{equation*}   Here, the function $A:\\mathbb{R}^2\\to\\mathbb{R}$ exhibits periodicity in all its arguments, while $V:\\mathbb{R}\\to\\mathbb{R}$ characterizes a double-well symmetric potential with minima at $t=\\pm\\alpha$.","sentences":["The goal of this paper is to investigate the existence of saddle solutions for some classes of elliptic partial differential equations of the Allen-Cahn type, formulated as follows:   \\begin{equation*}   -div\\left(\\frac{\\nabla u}{\\sqrt{1+|\\nabla u|^2}}\\right) + A(x,y)V'(u)=0~~\\text{ in }~~\\mathbb{R}^2.   \\end{equation*}   Here, the function $A:\\mathbb{R}^2\\to\\mathbb{R}$ exhibits periodicity in all its arguments, while $V:\\mathbb{R}\\to\\mathbb{R}$ characterizes a double-well symmetric potential with minima at $t=\\pm\\alpha$."],"url":"http://arxiv.org/abs/2404.11697v1","category":"math.AP"}
{"created":"2024-04-17 18:47:18","title":"Uniqueness of Heteroclinic Solutions in a Class of Autonomous Quasilinear ODE Problems","abstract":"In this paper, we prove the existence, uniqueness and qualitative properties of heteroclinic solution for a class of autonomous quasilinear ordinary differential equations of the Allen-Cahn type given by   $$   -\\left(\\phi(|u'|)u'\\right)'+V'(u)=0~~\\text{ in }~~\\mathbb{R},   $$   where $V$ is a double-well potential with minima at $t=\\pm\\alpha$ and $\\phi:(0,+\\infty)\\to(0,+\\infty)$ is a $C^1$ function satisfying some technical assumptions. Our results include the classic case $\\phi(t)=t^{p-2}$, which is related to the celebrated $p$-Laplacian operator, presenting the explicit solution in this specific scenario. Moreover, we also study the case $\\phi(t)=\\frac{1}{\\sqrt{1+t^2}}$, which is directly associated with the prescribed mean curvature operator.","sentences":["In this paper, we prove the existence, uniqueness and qualitative properties of heteroclinic solution for a class of autonomous quasilinear ordinary differential equations of the Allen-Cahn type given by   $$   -\\left(\\phi(|u'|)u'\\right)'+V'(u)=0~~\\text{ in }~~\\mathbb{R},   $$   where $V$ is a double-well potential with minima at $t=\\pm\\alpha$ and $\\phi:(0,+\\infty)\\to(0,+\\infty)$ is a $C^1$ function satisfying some technical assumptions.","Our results include the classic case $\\phi(t)=t^{p-2}$, which is related to the celebrated $p$-Laplacian operator, presenting the explicit solution in this specific scenario.","Moreover, we also study the case $\\phi(t)=\\frac{1}{\\sqrt{1+t^2}}$, which is directly associated with the prescribed mean curvature operator."],"url":"http://arxiv.org/abs/2404.11693v1","category":"math.AP"}
{"created":"2024-04-17 18:40:50","title":"Heteroclinic solutions for some classes of prescribed mean curvature equations in whole $\\mathbb{R}^2$","abstract":"The purpose of this paper consists in using variational methods to establish the existence of heteroclinic solutions for some classes of prescribed mean curvature equations of the type   $$   -div\\left(\\frac{\\nabla u}{\\sqrt{1+|\\nabla u|^2}}\\right) + A(\\epsilon x,y)V'(u)=0~~\\text{ in }~~\\mathbb{R}^2,   $$   where $\\epsilon>0$ and $V$ is a double-well potential with minima at $t=\\alpha$ and $t=\\beta$ with $\\alpha<\\beta$. Here, we consider some class of functions $A(x,y)$ that are oscillatory in the variable $y$ and satisfy different geometric conditions such as periodicity in all variables or asymptotically periodic at infinity.","sentences":["The purpose of this paper consists in using variational methods to establish the existence of heteroclinic solutions for some classes of prescribed mean curvature equations of the type   $$   -div\\left(\\frac{\\nabla u}{\\sqrt{1+|\\nabla u|^2}}\\right) + A(\\epsilon x,y)V'(u)=0~~\\text{ in }~~\\mathbb{R}^2,   $$   where $\\epsilon>0$ and $V$ is a double-well potential with minima at $t=\\alpha$ and $t=\\beta$ with $\\alpha<\\beta$. Here, we consider some class of functions $A(x,y)$ that are oscillatory in the variable $y$ and satisfy different geometric conditions such as periodicity in all variables or asymptotically periodic at infinity."],"url":"http://arxiv.org/abs/2404.11689v1","category":"math.AP"}
{"created":"2024-04-17 18:39:24","title":"On the electromagnetic interaction and the anomalous term in the Duffin-Kemmer-Petiau theory","abstract":"The problem of vectorial mesons embedded in an electromagnetic field via Duffin-Kemmer-Petiau (DKP) formalism is reinvestigated. Considering the electromagnetic interaction as a minimal coupling, an incorrect value $(g=1)$ is identified for the gyromagnetic factor ($g$-factor). Furthermore, it is shown that is cumbersome to find analytical solutions due to the presence of the so-called anomalous term for the spin-1 sector of the DKP theory. Suspecting that the anomalous term results from an incomplete version of the DKP equation to describe the electromagnetic interaction, we consider the addition of a non-minimal coupling. This leads to the correct $g$-factor $(g=2)$, and as a consequence, the anomalous term becomes proportional to an external four current. As an application, the DKP equation with a static uniform magnetic field is considered, yielding the corresponding Landau levels.","sentences":["The problem of vectorial mesons embedded in an electromagnetic field via Duffin-Kemmer-Petiau (DKP) formalism is reinvestigated.","Considering the electromagnetic interaction as a minimal coupling, an incorrect value $(g=1)$ is identified for the gyromagnetic factor ($g$-factor).","Furthermore, it is shown that is cumbersome to find analytical solutions due to the presence of the so-called anomalous term for the spin-1 sector of the DKP theory.","Suspecting that the anomalous term results from an incomplete version of the DKP equation to describe the electromagnetic interaction, we consider the addition of a non-minimal coupling.","This leads to the correct $g$-factor $(g=2)$, and as a consequence, the anomalous term becomes proportional to an external four current.","As an application, the DKP equation with a static uniform magnetic field is considered, yielding the corresponding Landau levels."],"url":"http://arxiv.org/abs/2404.11687v1","category":"hep-th"}
{"created":"2024-04-17 18:20:26","title":"What if string theory has a de Sitter excited state?","abstract":"We propose precise effective field theory criteria to obtain a four-dimensional de Sitter space within M-theory. To this effect, starting with the state space described by the action of metric perturbations, fluxes etc over the supersymmetric Minkowski vacuum in eleven-dimensions, we discuss the most general low energy effective action in terms of the eleven-dimensional fields including non-perturbative and non-local terms. Given this, our criteria to obtain a valid four-dimensional de Sitter solution at far IR involve satisfying the Schwinger-Dyson equations of the associated path integral, as well as obeying positivity constraints on the dual IIA string coupling and its time derivative. For excited states, the Schwinger-Dyson equations imply an effective emergent potential different from the original potential. We show that while vacuum solutions and arbitrary coherent states fail to satisfy these criteria, a specific class of excited states called the Glauber-Sudarshan states obey them. Using the resurgent structure of observables computed using the path integral over the Glauber-Sudarshan states, four-dimensional de Sitter in the flat slicing can be constructed using a Glauber-Sudarshan state in M-theory.   Among other novel results, we discuss the smallness of the positive cosmological constant, including the curious case where the cosmological constant is very slowly varying with time. We also discuss the resolution of identity with the Glauber-Sudarshan states, generation and the convergence properties of the non-perturbative and the non-local effects, the problems with the static patch and other related topics. We analyze briefly the issues related to the compatibility of the Wilsonian effective action with Borel resummations and discuss how they influence the effective field theory description in a four-dimensional de Sitter space.","sentences":["We propose precise effective field theory criteria to obtain a four-dimensional de Sitter space within M-theory.","To this effect, starting with the state space described by the action of metric perturbations, fluxes etc over the supersymmetric Minkowski vacuum in eleven-dimensions, we discuss the most general low energy effective action in terms of the eleven-dimensional fields including non-perturbative and non-local terms.","Given this, our criteria to obtain a valid four-dimensional de Sitter solution at far IR involve satisfying the Schwinger-Dyson equations of the associated path integral, as well as obeying positivity constraints on the dual IIA string coupling and its time derivative.","For excited states, the Schwinger-Dyson equations imply an effective emergent potential different from the original potential.","We show that while vacuum solutions and arbitrary coherent states fail to satisfy these criteria, a specific class of excited states called the Glauber-Sudarshan states obey them.","Using the resurgent structure of observables computed using the path integral over the Glauber-Sudarshan states, four-dimensional de Sitter in the flat slicing can be constructed using a Glauber-Sudarshan state in M-theory.   ","Among other novel results, we discuss the smallness of the positive cosmological constant, including the curious case where the cosmological constant is very slowly varying with time.","We also discuss the resolution of identity with the Glauber-Sudarshan states, generation and the convergence properties of the non-perturbative and the non-local effects, the problems with the static patch and other related topics.","We analyze briefly the issues related to the compatibility of the Wilsonian effective action with Borel resummations and discuss how they influence the effective field theory description in a four-dimensional de Sitter space."],"url":"http://arxiv.org/abs/2404.11680v1","category":"hep-th"}
{"created":"2024-04-17 18:03:12","title":"Exploring DNN Robustness Against Adversarial Attacks Using Approximate Multipliers","abstract":"Deep Neural Networks (DNNs) have advanced in many real-world applications, such as healthcare and autonomous driving. However, their high computational complexity and vulnerability to adversarial attacks are ongoing challenges. In this letter, approximate multipliers are used to explore DNN robustness improvement against adversarial attacks. By uniformly replacing accurate multipliers for state-of-the-art approximate ones in DNN layer models, we explore the DNNs robustness against various adversarial attacks in a feasible time. Results show up to 7% accuracy drop due to approximations when no attack is present while improving robust accuracy up to 10% when attacks applied.","sentences":["Deep Neural Networks (DNNs) have advanced in many real-world applications, such as healthcare and autonomous driving.","However, their high computational complexity and vulnerability to adversarial attacks are ongoing challenges.","In this letter, approximate multipliers are used to explore DNN robustness improvement against adversarial attacks.","By uniformly replacing accurate multipliers for state-of-the-art approximate ones in DNN layer models, we explore the DNNs robustness against various adversarial attacks in a feasible time.","Results show up to 7% accuracy drop due to approximations when no attack is present while improving robust accuracy up to 10% when attacks applied."],"url":"http://arxiv.org/abs/2404.11665v1","category":"cs.LG"}
{"created":"2024-04-17 18:00:03","title":"On the Geometry of N=2 Minkowski Vacua of Gauged N=2 Supergravity Theories in Four Dimensions","abstract":"Gauging isometries of four-dimensional N=2 supergravity theories yields an N=2 supersymmetric theory with a scalar potential. In this note, we study the well-known constraints for four-dimensional N=2 Minkowski vacua of such theories. We propose that classically a projective special K\\\"ahler submanifold of the projective K\\\"ahler target space of the ungauged theory describes the moduli space of the complex scalar fields of massless vector multiplets for N=2 Minkowski vacua configurations, which then receives quantum corrections from integrating out massive fields. Subloci of projective special K\\\"ahler manifolds appear as supersymmetric flux vacua in the context of type IIB Calabi-Yau threefold compactifications with background fluxes as well. While these flux vacua equations arise from the critical locus of an N=1 superpotential, we show that these equations can also be obtained from the N=2 supersymmetric Minkowski vacuum equations of gauged N=2 supergravity theories upon gauging suitable isometries in the semi-classical universal hypermultiplet sector of type IIB string Calabi-Yau threefold compactifications. Thus, we give an intrinsic N=2 supersymmetric interpretation to the flux vacua equations.","sentences":["Gauging isometries of four-dimensional N=2 supergravity theories yields an N=2 supersymmetric theory with a scalar potential.","In this note, we study the well-known constraints for four-dimensional N=2 Minkowski vacua of such theories.","We propose that classically a projective special K\\\"ahler submanifold of the projective K\\\"ahler target space of the ungauged theory describes the moduli space of the complex scalar fields of massless vector multiplets for N=2 Minkowski vacua configurations, which then receives quantum corrections from integrating out massive fields.","Subloci of projective special K\\\"ahler manifolds appear as supersymmetric flux vacua in the context of type IIB Calabi-Yau threefold compactifications with background fluxes as well.","While these flux vacua equations arise from the critical locus of an N=1 superpotential, we show that these equations can also be obtained from the N=2 supersymmetric Minkowski vacuum equations of gauged N=2 supergravity theories upon gauging suitable isometries in the semi-classical universal hypermultiplet sector of type IIB string Calabi-Yau threefold compactifications.","Thus, we give an intrinsic N=2 supersymmetric interpretation to the flux vacua equations."],"url":"http://arxiv.org/abs/2404.11655v1","category":"hep-th"}
{"created":"2024-04-17 18:00:00","title":"A Partially Fixed Background Field Gauge","abstract":"We examine the incorporation of gauge symmetries in the modern effective field theory (EFT) matching paradigm with a particular focus on spontaneously broken symmetries. The presence of gauge symmetries entails the introduction of gauge-fixing terms in matching calculations, which may prevent (partial) cancellation between loops from the underlying theory and those of the EFT, thereby preventing the establishment of a hard-region matching formula. While this is not an issue when using the ordinary background field (BF) gauge for unbroken gauge theories, we find ourselves unable to demonstrate the cancellation with the ordinary BF gauge in spontaneously broken gauge theories. As a convenient alternative, we construct a partially fixed BF gauge that, in addition to being simpler than the ordinary BF gauge, allows us to prove a hard-region matching formula.","sentences":["We examine the incorporation of gauge symmetries in the modern effective field theory (EFT) matching paradigm with a particular focus on spontaneously broken symmetries.","The presence of gauge symmetries entails the introduction of gauge-fixing terms in matching calculations, which may prevent (partial) cancellation between loops from the underlying theory and those of the EFT, thereby preventing the establishment of a hard-region matching formula.","While this is not an issue when using the ordinary background field (BF) gauge for unbroken gauge theories, we find ourselves unable to demonstrate the cancellation with the ordinary BF gauge in spontaneously broken gauge theories.","As a convenient alternative, we construct a partially fixed BF gauge that, in addition to being simpler than the ordinary BF gauge, allows us to prove a hard-region matching formula."],"url":"http://arxiv.org/abs/2404.11640v1","category":"hep-ph"}
{"created":"2024-04-17 18:00:00","title":"PDS 70 unveiled by star-hopping: total intensity, polarimetry and mm-imaging modeled in concert","abstract":"Context. Most ground-based planet search direct imaging campaigns use angular differential imaging, which distorts the signal from extended sources like protoplanetary disks. In the case PDS 70, a young system with two planets found within the cavity of a protoplanetary disk, obtaining a reliable image of both planets and disk is essential to understanding planet-disk interactions. Aims. Our goals are to reveal the true intensity of the planets and disk without self-subtraction effects for the first time, search for new giant planets beyond separations of 0.1\" and to study the morphology of the disk shaped by two massive planets. Methods. We present YJHK-band imaging, polarimetry, and spatially resolved spectroscopy of PDS 70 using near-simultaneous reference star differential imaging, also known as star-hopping. We created a radiative transfer model of the system to match the near-infrared imaging and polarimetric data, along with sub-millimeter imaging from ALMA. Furthermore, we extracted the spectra of the planets and the disk and compared them. Results. We find that the disk is quite flared with a scale height of ~15% at the outer edge of the disk at ~90 au, similar to some disks in the literature. The gap inside of ~50 au is estimated to have ~1% of the dust density of the outer disk. The Northeast outer disk arc seen in previous observations is likely the outer lip of the flared disk. Abundance ratios of grains estimated by the modeling indicate a shallow grain-size index > -2.7, instead of the canonical -3.5. There is both vertical and radial segregation of grains. Planet c is well separated from the disk and has a spectrum similar to planet b, clearly redder than the disk spectra. Planet c is possibly associated with the sudden flaring of the disk starting at ~50 au. No new planets > 5 Mj were found.","sentences":["Context.","Most ground-based planet search direct imaging campaigns use angular differential imaging, which distorts the signal from extended sources like protoplanetary disks.","In the case PDS 70, a young system with two planets found within the cavity of a protoplanetary disk, obtaining a reliable image of both planets and disk is essential to understanding planet-disk interactions.","Aims.","Our goals are to reveal the true intensity of the planets and disk without self-subtraction effects for the first time, search for new giant planets beyond separations of 0.1\" and to study the morphology of the disk shaped by two massive planets.","Methods.","We present YJHK-band imaging, polarimetry, and spatially resolved spectroscopy of PDS 70 using near-simultaneous reference star differential imaging, also known as star-hopping.","We created a radiative transfer model of the system to match the near-infrared imaging and polarimetric data, along with sub-millimeter imaging from ALMA.","Furthermore, we extracted the spectra of the planets and the disk and compared them.","Results.","We find that the disk is quite flared with a scale height of ~15% at the outer edge of the disk at ~90 au, similar to some disks in the literature.","The gap inside of ~50 au is estimated to have ~1% of the dust density of the outer disk.","The Northeast outer disk arc seen in previous observations is likely the outer lip of the flared disk.","Abundance ratios of grains estimated by the modeling indicate a shallow grain-size index > -2.7, instead of the canonical -3.5.","There is both vertical and radial segregation of grains.","Planet c is well separated from the disk and has a spectrum similar to planet b, clearly redder than the disk spectra.","Planet c is possibly associated with the sudden flaring of the disk starting at ~50 au.","No new planets >","5 Mj were found."],"url":"http://arxiv.org/abs/2404.11641v1","category":"astro-ph.EP"}
{"created":"2024-04-17 17:55:54","title":"Localized dopant motion across the 2D Ising phase transition","abstract":"I investigate the motion of a single hole in 2D spin lattices with square and triangular geometries. While the spins have nearest neighbor Ising spin couplings $J$, the hole is allowed to move only in 1D along a single line in the 2D lattice with nearest neighbor hopping amplitude $t$. The non-equilibrium hole dynamics is initialized by suddenly removing a single spin from the thermal Ising spin lattice. I find that for any nonzero spin coupling and temperature, the hole is localized. This is an extension of the thermally induced localization phenomenon [arXiv:2310.11193] to the case, where there is a phase transition to a long-range ordered ferromagnetic phase. The dynamics depends only on the ratio of the temperature to the spin coupling, $k_BT / |J|$, and on the ratio of the spin coupling to the hopping $J/t$. I characterize these dependencies in great detail. In particular, I find universal behavior at high temperatures, common features for the square and triangular lattices across the Curie temperatures for ferromagnetic interactions, and highly distinct behaviors for the two geometries in the presence of antiferromagnetic interactions due geometric frustration in the triangular lattice.","sentences":["I investigate the motion of a single hole in 2D spin lattices with square and triangular geometries.","While the spins have nearest neighbor Ising spin couplings $J$, the hole is allowed to move only in 1D along a single line in the 2D lattice with nearest neighbor hopping amplitude $t$. The non-equilibrium hole dynamics is initialized by suddenly removing a single spin from the thermal Ising spin lattice.","I find that for any nonzero spin coupling and temperature, the hole is localized.","This is an extension of the thermally induced localization phenomenon [arXiv:2310.11193] to the case, where there is a phase transition to a long-range ordered ferromagnetic phase.","The dynamics depends only on the ratio of the temperature to the spin coupling, $k_BT / |J|$, and on the ratio of the spin coupling to the hopping $J/t$. I characterize these dependencies in great detail.","In particular, I find universal behavior at high temperatures, common features for the square and triangular lattices across the Curie temperatures for ferromagnetic interactions, and highly distinct behaviors for the two geometries in the presence of antiferromagnetic interactions due geometric frustration in the triangular lattice."],"url":"http://arxiv.org/abs/2404.11608v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 17:55:27","title":"Private federated discovery of out-of-vocabulary words for Gboard","abstract":"The vocabulary of language models in Gboard, Google's keyboard application, plays a crucial role for improving user experience. One way to improve the vocabulary is to discover frequently typed out-of-vocabulary (OOV) words on user devices. This task requires strong privacy protection due to the sensitive nature of user input data. In this report, we present a private OOV discovery algorithm for Gboard, which builds on recent advances in private federated analytics. The system offers local differential privacy (LDP) guarantees for user contributed words. With anonymous aggregation, the final released result would satisfy central differential privacy guarantees with $\\varepsilon = 0.315, \\delta = 10^{-10}$ for OOV discovery in en-US (English in United States).","sentences":["The vocabulary of language models in Gboard, Google's keyboard application, plays a crucial role for improving user experience.","One way to improve the vocabulary is to discover frequently typed out-of-vocabulary (OOV) words on user devices.","This task requires strong privacy protection due to the sensitive nature of user input data.","In this report, we present a private OOV discovery algorithm for Gboard, which builds on recent advances in private federated analytics.","The system offers local differential privacy (LDP) guarantees for user contributed words.","With anonymous aggregation, the final released result would satisfy central differential privacy guarantees with $\\varepsilon = 0.315, \\delta = 10^{-10}$ for OOV discovery in en-US (English in United States)."],"url":"http://arxiv.org/abs/2404.11607v2","category":"cs.DS"}
{"created":"2024-04-17 17:11:47","title":"Simple Image Signal Processing using Global Context Guidance","abstract":"In modern smartphone cameras, the Image Signal Processor (ISP) is the core element that converts the RAW readings from the sensor into perceptually pleasant RGB images for the end users. The ISP is typically proprietary and handcrafted and consists of several blocks such as white balance, color correction, and tone mapping. Deep learning-based ISPs aim to transform RAW images into DSLR-like RGB images using deep neural networks. However, most learned ISPs are trained using patches (small regions) due to computational limitations. Such methods lack global context, which limits their efficacy on full-resolution images and harms their ability to capture global properties such as color constancy or illumination. First, we propose a novel module that can be integrated into any neural ISP to capture the global context information from the full RAW images. Second, we propose an efficient and simple neural ISP that utilizes our proposed module. Our model achieves state-of-the-art results on different benchmarks using diverse and real smartphone images.","sentences":["In modern smartphone cameras, the Image Signal Processor (ISP) is the core element that converts the RAW readings from the sensor into perceptually pleasant RGB images for the end users.","The ISP is typically proprietary and handcrafted and consists of several blocks such as white balance, color correction, and tone mapping.","Deep learning-based ISPs aim to transform RAW images into DSLR-like RGB images using deep neural networks.","However, most learned ISPs are trained using patches (small regions) due to computational limitations.","Such methods lack global context, which limits their efficacy on full-resolution images and harms their ability to capture global properties such as color constancy or illumination.","First, we propose a novel module that can be integrated into any neural ISP to capture the global context information from the full RAW images.","Second, we propose an efficient and simple neural ISP that utilizes our proposed module.","Our model achieves state-of-the-art results on different benchmarks using diverse and real smartphone images."],"url":"http://arxiv.org/abs/2404.11569v1","category":"cs.CV"}
{"created":"2024-04-17 16:50:14","title":"SDIP: Self-Reinforcement Deep Image Prior Framework for Image Processing","abstract":"Deep image prior (DIP) proposed in recent research has revealed the inherent trait of convolutional neural networks (CNN) for capturing substantial low-level image statistics priors. This framework efficiently addresses the inverse problems in image processing and has induced extensive applications in various domains. However, as the whole algorithm is initialized randomly, the DIP algorithm often lacks stability. Thus, this method still has space for further improvement. In this paper, we propose the self-reinforcement deep image prior (SDIP) as an improved version of the original DIP. We observed that the changes in the DIP networks' input and output are highly correlated during each iteration. SDIP efficiently utilizes this trait in a reinforcement learning manner, where the current iteration's output is utilized by a steering algorithm to update the network input for the next iteration, guiding the algorithm toward improved results. Experimental results across multiple applications demonstrate that our proposed SDIP framework offers improvement compared to the original DIP method and other state-of-the-art methods.","sentences":["Deep image prior (DIP) proposed in recent research has revealed the inherent trait of convolutional neural networks (CNN) for capturing substantial low-level image statistics priors.","This framework efficiently addresses the inverse problems in image processing and has induced extensive applications in various domains.","However, as the whole algorithm is initialized randomly, the DIP algorithm often lacks stability.","Thus, this method still has space for further improvement.","In this paper, we propose the self-reinforcement deep image prior (SDIP) as an improved version of the original DIP.","We observed that the changes in the DIP networks' input and output are highly correlated during each iteration.","SDIP efficiently utilizes this trait in a reinforcement learning manner, where the current iteration's output is utilized by a steering algorithm to update the network input for the next iteration, guiding the algorithm toward improved results.","Experimental results across multiple applications demonstrate that our proposed SDIP framework offers improvement compared to the original DIP method and other state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.12142v1","category":"cs.CV"}
{"created":"2024-04-17 16:49:02","title":"Euclid view of dusty star forming galaxies at z>~1.5 detected in wide area submillimetre surveys","abstract":"We investigate the constraints provided by the Euclid space observatory on the physical properties of dusty star forming galaxies (DSFGs) at z>~1.5 detected in wide area sub millimetre surveys with Herschel. We adopt a physical model for the high z progenitors of spheroidal galaxies, which form the bulk of the DSFGs at z>~1.5. We improve the model by combining the output of the equations of the model with a formalism for the spectral energy distribution(SED). After optimising the SED parameters to reproduce the measured infrared luminosity function and the number counts of DSFGs, we simulated a sample of DSFGs over 100 sq deg and then applied a 5 sigma detection limit of 37 mJy at 250 microns. We estimated the redshifts from the Euclid data and then fitted the Euclid and Herschel photometry with the code CIGALE to extract the physicsl parameters. We found that 100 % of the Herschel galaxies are detected in all 4 Euclid bands above 3 sigma. For 87% of the sources the accuracy on 1+z is better than 15%. The sample comprises mostly massive log(Mstar/Msun)~10.5-12.9, highly star forming, log(SFR/Msun/yr)~1.5-4, dusty, log(Mdust/Msun)~7.5-9.9, galaxies. The measured stellar mass have a dispersion of 0.19 dex around the true value, thus showing that Euclid will provide reliable stellar mass estimates for the majority of the bright DSFGs at z>~1.5 detected by Herschel. We also explored the effect of complementing the Euclid photometry with that from Vera C. Rubin Observatory/LSST.","sentences":["We investigate the constraints provided by the Euclid space observatory on the physical properties of dusty star forming galaxies (DSFGs) at z>~1.5 detected in wide area sub millimetre surveys with Herschel.","We adopt a physical model for the high z progenitors of spheroidal galaxies, which form the bulk of the DSFGs at z>~1.5.","We improve the model by combining the output of the equations of the model with a formalism for the spectral energy distribution(SED).","After optimising the SED parameters to reproduce the measured infrared luminosity function and the number counts of DSFGs, we simulated a sample of DSFGs over 100 sq deg and then applied a 5 sigma detection limit of 37 mJy at 250 microns.","We estimated the redshifts from the Euclid data and then fitted the Euclid and Herschel photometry with the code CIGALE to extract the physicsl parameters.","We found that 100 % of the Herschel galaxies are detected in all 4 Euclid bands above 3 sigma.","For 87% of the sources the accuracy on 1+z is better than 15%.","The sample comprises mostly massive log(Mstar/Msun)~10.5-12.9, highly star forming, log(SFR/Msun/yr)~1.5-4, dusty, log(Mdust/Msun)~7.5-9.9, galaxies.","The measured stellar mass have a dispersion of 0.19 dex around the true value, thus showing that Euclid will provide reliable stellar mass estimates for the majority of the bright DSFGs at z>~1.5 detected by Herschel.","We also explored the effect of complementing the Euclid photometry with that from Vera C. Rubin Observatory/LSST."],"url":"http://arxiv.org/abs/2404.11551v1","category":"astro-ph.GA"}
{"created":"2024-04-17 16:35:00","title":"Carbon- and Oxygen-rich stars in MaStar: identification and classification","abstract":"Carbon- and Oxygen-rich stars populating the Thermally-Pulsing Asymptotic Giant Branch (TP-AGB) phase of stellar evolution are relevant contributors to the spectra of ~1 Gyr old populations. Atmosphere models for these types are uncertain, due to complex molecules and mass-loss effects. Empirical spectra are then crucial, but samples are small due to the short (~3 Myr) TP-AGB lifetime. Here we exploit the vastness of the MaNGA Stellar library MaStar (~60,000 spectra) to identify C,O-rich type stars. We define an optical colour selection with cuts of (g-r)>2 and (g-i)<1.55(g-r)-0.07, calibrated with known C- and O- rich spectra. This identifies C-,O-rich stars along clean, separated sequences. An analogue selection is found in V,R,I bands. Our equation identifies C- and O-rich spectra with predictive performance metric F1-scores of 0.72 and 0.74 (over 1), respectively. We finally identify 41 C- and 87 O-rich type AGB stars in MaStar, 5 and 49 of which do not have a SIMBAD counterpart. We also detect a sample of non-AGB, dwarf C-stars. We further design a fitting procedure to classify the spectra into broad spectral types, by using as fitting templates empirical C and O-rich spectra. We find remarkably good fits for the majority of candidates and categorise them into C- and O-rich bins following existing classifications, which correlate to effective temperature. Our selection models can be applied to large photometric surveys (e.g. Euclid, Rubin). The classified spectra will facilitate future evolutionary population synthesis models.","sentences":["Carbon- and Oxygen-rich stars populating the Thermally-Pulsing Asymptotic Giant Branch (TP-AGB) phase of stellar evolution are relevant contributors to the spectra of ~1 Gyr old populations.","Atmosphere models for these types are uncertain, due to complex molecules and mass-loss effects.","Empirical spectra are then crucial, but samples are small due to the short (~3 Myr) TP-AGB lifetime.","Here we exploit the vastness of the MaNGA Stellar library MaStar (~60,000 spectra) to identify C,O-rich type stars.","We define an optical colour selection with cuts of (g-r)>2 and (g-i)<1.55(g-r)-0.07, calibrated with known C- and O- rich spectra.","This identifies C-,O-rich stars along clean, separated sequences.","An analogue selection is found in V,R,I bands.","Our equation identifies C- and O-rich spectra with predictive performance metric F1-scores of 0.72 and 0.74 (over 1), respectively.","We finally identify 41 C- and 87 O-rich type AGB stars in MaStar, 5 and 49 of which do not have a SIMBAD counterpart.","We also detect a sample of non-AGB, dwarf C-stars.","We further design a fitting procedure to classify the spectra into broad spectral types, by using as fitting templates empirical C and O-rich spectra.","We find remarkably good fits for the majority of candidates and categorise them into C- and O-rich bins following existing classifications, which correlate to effective temperature.","Our selection models can be applied to large photometric surveys (e.g. Euclid, Rubin).","The classified spectra will facilitate future evolutionary population synthesis models."],"url":"http://arxiv.org/abs/2404.11541v1","category":"astro-ph.GA"}
{"created":"2024-04-17 16:25:19","title":"Select and Reorder: A Novel Approach for Neural Sign Language Production","abstract":"Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.","sentences":["Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets.","This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR).","Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment.","Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds.","Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation.","This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings."],"url":"http://arxiv.org/abs/2404.11532v1","category":"cs.CL"}
{"created":"2024-04-17 16:19:30","title":"Efficient anisotropic Migdal-Eliashberg calculations with the Intermediate Representation basis and Wannier interpolation","abstract":"In this study, we combine the ab initio Migdal-Eliashberg approach with the intermediate representation for the Green's function, enabling accurate and efficient calculations of the momentum-dependent superconducting gap function while fully considering the effect of the Coulomb retardation. Unlike the conventional scheme that relies on a uniform sampling across Matsubara frequencies - demanding hundreds to thousands of points - the intermediate representation works with fewer than 100 sampled Matsubara Green's functions. The developed methodology is applied to investigate the superconducting properties of three representative low-temperature elemental metals: aluminum (Al), lead (Pb), and niobium (Nb). The results demonstrate the power and reliability of our computational technique to accurately solve the ab initio anisotropic Migdal-Eliashberg equations even at extremely low temperatures, below 1 Kelvin.","sentences":["In this study, we combine the ab initio Migdal-Eliashberg approach with the intermediate representation for the Green's function, enabling accurate and efficient calculations of the momentum-dependent superconducting gap function while fully considering the effect of the Coulomb retardation.","Unlike the conventional scheme that relies on a uniform sampling across Matsubara frequencies - demanding hundreds to thousands of points - the intermediate representation works with fewer than 100 sampled Matsubara Green's functions.","The developed methodology is applied to investigate the superconducting properties of three representative low-temperature elemental metals: aluminum (Al), lead (Pb), and niobium (Nb).","The results demonstrate the power and reliability of our computational technique to accurately solve the ab initio anisotropic Migdal-Eliashberg equations even at extremely low temperatures, below 1 Kelvin."],"url":"http://arxiv.org/abs/2404.11528v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 16:10:25","title":"High-order meshless global stability analysis of Taylor-Couette flows in complex domains","abstract":"Recently, meshless methods have become popular in numerically solving partial differential equations and have been employed to solve equations governing fluid flows, heat transfer, and species transport. In the present study, a numerical solver is developed employing the meshless framework to efficiently compute the hydrodynamic stability of fluid flows in complex geometries. The developed method is tested on two cases of Taylor-Couette flows. The concentric case represents the parallel flow assumption incorporated in the Orr-Sommerfeld model and the eccentric Taylor-Couette flow incorporates a non-parallel base flow with separation bubbles. The method was validated against earlier works by Marcus [1], Oikawa et al. [2], Leclercq et al. [3], and Mittal et al. [4]. The results for the two cases and the effectiveness of the method are discussed in detail. The method is then applied to Taylor-Couette flow in an elliptical enclosure and the stability of the flow is investigated.","sentences":["Recently, meshless methods have become popular in numerically solving partial differential equations and have been employed to solve equations governing fluid flows, heat transfer, and species transport.","In the present study, a numerical solver is developed employing the meshless framework to efficiently compute the hydrodynamic stability of fluid flows in complex geometries.","The developed method is tested on two cases of Taylor-Couette flows.","The concentric case represents the parallel flow assumption incorporated in the Orr-Sommerfeld model and the eccentric Taylor-Couette flow incorporates a non-parallel base flow with separation bubbles.","The method was validated against earlier works by Marcus","[1], Oikawa et al.","[2], Leclercq et al.","[3], and Mittal et al.","[4].","The results for the two cases and the effectiveness of the method are discussed in detail.","The method is then applied to Taylor-Couette flow in an elliptical enclosure and the stability of the flow is investigated."],"url":"http://arxiv.org/abs/2404.11517v1","category":"physics.flu-dyn"}
{"created":"2024-04-17 15:45:30","title":"On the non-local problem for Boussinesq type fractional equation","abstract":"In recent years, the Boussinesq type fractional partial differential equation has attracted much attentions of researchers for its practical importance. In this paper we study a non-local problem for the Boussinesq type equation $D_t^\\alpha u(t)+A D_t^\\alpha u(t)+\\nu^2A u(t)=0,\\,\\, 0< t< T,\\,\\, 1<\\alpha<2,$ where $D_t^\\alpha$ is the Caputo fractional derivative and $A$ is abstract operator. In the classical case, i.e. at $\\alpha=2$, this problem was studied earlier and an interesting effect was discovered: the well-posedness of the problem significantly depends on the length of the time interval and the parameter $\\nu$. This note shows that for the case of a fractional equation there is no such effect: the problem is well-posed for any $T$ and $\\nu$.","sentences":["In recent years, the Boussinesq type fractional partial differential equation has attracted much attentions of researchers for its practical importance.","In this paper we study a non-local problem for the Boussinesq type equation $D_t^\\alpha u(t)+A D_t^\\alpha u(t)+\\nu^2A u(t)=0,\\,\\, 0< t<","T,\\,\\, 1<\\alpha<2,$ where $D_t^\\alpha$ is the Caputo fractional derivative and $A$ is abstract operator.","In the classical case, i.e. at $\\alpha=2$, this problem was studied earlier and an interesting effect was discovered: the well-posedness of the problem significantly depends on the length of the time interval and the parameter $\\nu$. This note shows that for the case of a fractional equation there is no such effect: the problem is well-posed for any $T$ and $\\nu$."],"url":"http://arxiv.org/abs/2404.11486v1","category":"math.AP"}
{"created":"2024-04-17 15:33:45","title":"Impact of lensing of gravitational waves on the observed distribution of neutron star masses","abstract":"The distribution of masses of neutron stars, particularly the maximum mass value, is considered a probe of their formation, evolution and internal physics (i.e., equation of state). This mass distribution could in principle be inferred from the detection of gravitational waves from binary neutron star mergers. Using mock catalogues of $10^5$ dark sirens events, expected to be detected by Einstein Telescope over an operational period of $\\sim1\\, \\rm year$ , we show how the biased luminosity distance measurement induced by gravitational lensing affects the inferred redshift and mass of the merger. This results in higher observed masses than expected. Up to $2\\%$ of the events are predicted to fall above the maximum allowed neutron star mass depending on the intrinsic mass distribution and signal-to-noise ratio threshold adopted. The underlying true mass distribution and maximum mass could still be approximately recovered in the case of bright standard sirens.","sentences":["The distribution of masses of neutron stars, particularly the maximum mass value, is considered a probe of their formation, evolution and internal physics (i.e., equation of state).","This mass distribution could in principle be inferred from the detection of gravitational waves from binary neutron star mergers.","Using mock catalogues of $10^5$ dark sirens events, expected to be detected by Einstein Telescope over an operational period of $\\sim1\\, \\rm year$ , we show how the biased luminosity distance measurement induced by gravitational lensing affects the inferred redshift and mass of the merger.","This results in higher observed masses than expected.","Up to $2\\%$ of the events are predicted to fall above the maximum allowed neutron star mass depending on the intrinsic mass distribution and signal-to-noise ratio threshold adopted.","The underlying true mass distribution and maximum mass could still be approximately recovered in the case of bright standard sirens."],"url":"http://arxiv.org/abs/2404.11480v1","category":"astro-ph.CO"}
{"created":"2024-04-17 15:01:54","title":"Solution to the iterative differential equation $-\u03b3g' = g^{-1}$","abstract":"Using a Picard-like operator $T$, we prove that the iterative differential equation $-\\gamma g' = g^{-1}$ with parameter $\\gamma>0$ has a solution $g=h\\colon[0,1]\\to[0,1]$ for only one value $\\gamma=\\kappa\\approx0.278877$, and that this solution $h$ is unique. As an even stronger result, we exhibit $h$ as the global limit of the operator $T$.","sentences":["Using a Picard-like operator $T$, we prove that the iterative differential equation $-\\gamma g' = g^{-1}$ with parameter $\\gamma>0$ has a solution $g=h\\colon[0,1]\\to[0,1]$ for only one value $\\gamma=\\kappa\\approx0.278877$, and that this solution $h$ is unique.","As an even stronger result, we exhibit $h$ as the global limit of the operator $T$."],"url":"http://arxiv.org/abs/2404.11455v1","category":"math.CA"}
{"created":"2024-04-17 14:55:11","title":"An accelerated Levin-Clenshaw-Curtis method for the evaluation of highly oscillatory integrals","abstract":"The efficient approximation of highly oscillatory integrals plays an important role in a wide range of applications. Whilst traditional quadrature becomes prohibitively expensive in the high-frequency regime, Levin methods provide a way to approximate these integrals in many settings at uniform cost. In this work, we present an accelerated version of Levin methods that can be applied to a wide range of physically important oscillatory integrals, by exploiting the banded action of certain differential operators on a Chebyshev polynomial basis. Our proposed version of the Levin method can be computed essentially in just $\\mathcal{O}(\\nu\\log\\nu)$ operations, where $\\nu$ is the number of quadrature points and the dependence of the cost on a number of additional parameters is made explicit in the manuscript. This presents a significant speed-up over the direct computation of the Levin method in current state-of-the-art. We outline the construction of this accelerated method for a fairly broad class of integrals and support our theoretical description with a number of illustrative numerical examples.","sentences":["The efficient approximation of highly oscillatory integrals plays an important role in a wide range of applications.","Whilst traditional quadrature becomes prohibitively expensive in the high-frequency regime, Levin methods provide a way to approximate these integrals in many settings at uniform cost.","In this work, we present an accelerated version of Levin methods that can be applied to a wide range of physically important oscillatory integrals, by exploiting the banded action of certain differential operators on a Chebyshev polynomial basis.","Our proposed version of the Levin method can be computed essentially in just $\\mathcal{O}(\\nu\\log\\nu)$ operations, where $\\nu$ is the number of quadrature points and the dependence of the cost on a number of additional parameters is made explicit in the manuscript.","This presents a significant speed-up over the direct computation of the Levin method in current state-of-the-art.","We outline the construction of this accelerated method for a fairly broad class of integrals and support our theoretical description with a number of illustrative numerical examples."],"url":"http://arxiv.org/abs/2404.11448v1","category":"math.NA"}
{"created":"2024-04-17 14:53:26","title":"Fidelity decay and error accumulation in quantum volume circuits","abstract":"We provide a comprehensive analysis of fidelity decay and error accumulation in faulty quantum circuit models. We devise an analytical bound to the average fidelity between the desired and faulty output states, accounting for errors that may arise during the implementation of two-qubit gates and multi-qubit permutations. We demonstrate that fidelity decays exponentially with both the number of qubits and circuit depth, and determine the decay rates as a function of the parameterized probabilities of the two types of errors. These decay constants are intricately linked to the connectivity and dimensionality of the processor's architecture. Furthermore, we establish a robust linear relationship between fidelity and the heavy output frequency used in Quantum Volume tests to benchmark quantum processors, under the considered errors protocol. These findings pave the way for predicting fidelity trends in the presence of specific errors and offer insights into the best strategies for increasing Quantum Volume.","sentences":["We provide a comprehensive analysis of fidelity decay and error accumulation in faulty quantum circuit models.","We devise an analytical bound to the average fidelity between the desired and faulty output states, accounting for errors that may arise during the implementation of two-qubit gates and multi-qubit permutations.","We demonstrate that fidelity decays exponentially with both the number of qubits and circuit depth, and determine the decay rates as a function of the parameterized probabilities of the two types of errors.","These decay constants are intricately linked to the connectivity and dimensionality of the processor's architecture.","Furthermore, we establish a robust linear relationship between fidelity and the heavy output frequency used in Quantum Volume tests to benchmark quantum processors, under the considered errors protocol.","These findings pave the way for predicting fidelity trends in the presence of specific errors and offer insights into the best strategies for increasing Quantum Volume."],"url":"http://arxiv.org/abs/2404.11444v1","category":"quant-ph"}
{"created":"2024-04-17 14:51:46","title":"Structural properties of amorphous Na$_3$OCl electrolyte by first-principles and machine learning molecular dynamics","abstract":"Solid-state electrolytes mark a significant leap forward in the field of electrochemical energy storage, offering improved safety and efficiency compared to conventional liquid electrolytes. Among these, antiperovskite electrolytes, particularly those based on Li and Na, have emerged as promising candidates due to their superior ionic conductivity and straightforward synthesis processes. This study focuses on the amorphous phase of antiperovskite Na$_3$OCl, assessing its structural properties through a combination of first-principles molecular dynamics (FPMD) and machine learning interatomic potential (MLIP) simulations. Our comprehensive analysis spans models ranging from 135 to 3645 atoms, allowing for a detailed examination of X-ray and neutron structure factors, total and partial pair correlation functions, coordination numbers, and structural unit distributions. We demonstrate the minimal, albeit partially present, size effects on these structural features and validate the accuracy of the MLIP model in reproducing the intricate details of the amorphous Na$_3$OCl structure described at the FPMD level.","sentences":["Solid-state electrolytes mark a significant leap forward in the field of electrochemical energy storage, offering improved safety and efficiency compared to conventional liquid electrolytes.","Among these, antiperovskite electrolytes, particularly those based on Li and Na, have emerged as promising candidates due to their superior ionic conductivity and straightforward synthesis processes.","This study focuses on the amorphous phase of antiperovskite Na$_3$OCl, assessing its structural properties through a combination of first-principles molecular dynamics (FPMD) and machine learning interatomic potential (MLIP) simulations.","Our comprehensive analysis spans models ranging from 135 to 3645 atoms, allowing for a detailed examination of X-ray and neutron structure factors, total and partial pair correlation functions, coordination numbers, and structural unit distributions.","We demonstrate the minimal, albeit partially present, size effects on these structural features and validate the accuracy of the MLIP model in reproducing the intricate details of the amorphous Na$_3$OCl structure described at the FPMD level."],"url":"http://arxiv.org/abs/2404.11442v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 14:39:14","title":"Runtime Analyses of NSGA-III on Many-Objective Problems","abstract":"NSGA-II and NSGA-III are two of the most popular evolutionary multi-objective algorithms used in practice. While NSGA-II is used for few objectives such as 2 and 3, NSGA-III is designed to deal with a larger number of objectives. In a recent breakthrough, Wietheger and Doerr (IJCAI 2023) gave the first runtime analysis for NSGA-III on the 3-objective OneMinMax problem, showing that this state-of-the-art algorithm can be analyzed rigorously. We advance this new line of research by presenting the first runtime analyses of NSGA-III on the popular many-objective benchmark problems mLOTZ, mOMM, and mCOCZ, for an arbitrary constant number $m$ of objectives. Our analysis provides ways to set the important parameters of the algorithm: the number of reference points and the population size, so that a good performance can be guaranteed. We show how these parameters should be scaled with the problem dimension, the number of objectives and the fitness range. To our knowledge, these are the first runtime analyses for NSGA-III for more than 3 objectives.","sentences":["NSGA-II and NSGA-III are two of the most popular evolutionary multi-objective algorithms used in practice.","While NSGA-II is used for few objectives such as 2 and 3, NSGA-III is designed to deal with a larger number of objectives.","In a recent breakthrough, Wietheger and Doerr (IJCAI 2023) gave the first runtime analysis for NSGA-III on the 3-objective OneMinMax problem, showing that this state-of-the-art algorithm can be analyzed rigorously.","We advance this new line of research by presenting the first runtime analyses of NSGA-III on the popular many-objective benchmark problems mLOTZ, mOMM, and mCOCZ, for an arbitrary constant number $m$ of objectives.","Our analysis provides ways to set the important parameters of the algorithm: the number of reference points and the population size, so that a good performance can be guaranteed.","We show how these parameters should be scaled with the problem dimension, the number of objectives and the fitness range.","To our knowledge, these are the first runtime analyses for NSGA-III for more than 3 objectives."],"url":"http://arxiv.org/abs/2404.11433v2","category":"cs.NE"}
{"created":"2024-04-17 14:28:57","title":"Correlation Function Of Thin-Shell Operators","abstract":"In this study, we explore the correlation functions of thin-shell operators, represented semiclassically by a homogeneous, thin interface of dust particles. Employing the monodromy method, we successfully compute the contribution from the Virasoro vacuum block and present the monodromy equation in a closed form without assuming the probe limit. Although an analytical solution to the monodromy equation remains difficult, we demonstrate that it is perturbatively solvable within specific limits, including the probe, the heavy-shell, and the early-time limits. Moreover, we compare our results with gravitational calculations and find precise agreement. We strengthen our findings by proving that the thermal correlation functions in gravity, after an inverse Laplace transformation, satisfy the field theory's monodromy equation. Additionally, we identify an infinite series of unphysical solutions to the monodromy equation and discuss their potential geometrical duals.","sentences":["In this study, we explore the correlation functions of thin-shell operators, represented semiclassically by a homogeneous, thin interface of dust particles.","Employing the monodromy method, we successfully compute the contribution from the Virasoro vacuum block and present the monodromy equation in a closed form without assuming the probe limit.","Although an analytical solution to the monodromy equation remains difficult, we demonstrate that it is perturbatively solvable within specific limits, including the probe, the heavy-shell, and the early-time limits.","Moreover, we compare our results with gravitational calculations and find precise agreement.","We strengthen our findings by proving that the thermal correlation functions in gravity, after an inverse Laplace transformation, satisfy the field theory's monodromy equation.","Additionally, we identify an infinite series of unphysical solutions to the monodromy equation and discuss their potential geometrical duals."],"url":"http://arxiv.org/abs/2404.11423v1","category":"hep-th"}
{"created":"2024-04-17 14:26:24","title":"Excitonic circular dichroism in boron-nitrogen clusters decorated graphene","abstract":"Within the first principle calculations, we propose a boron and nitrogen cluster incorporated graphene system for efficient valley polarization. The broken spatial inversion symmetry results in high Berry curvature at K and K' valleys of the hexagonal Brillouin zone in this semiconducting system. The consideration of excitonic quasiparticles within GW approximation along with their scattering processes within many-body Bethe-Salpeter equation gives rise to an optical gap of 1.72 eV with an excitonic binding energy of 0.65 eV. Owing to the negligible intervalley scattering, the electrons in opposite valleys are selectively excited by left- and right-handed circular polarized lights, as evident from the oscillator strength calculations. Therefore, this system can exhibit circular-dichroism valley Hall effect in the presence of the in-plane electric field. Moreover, such excitonic qubits can be exploited for information processing.","sentences":["Within the first principle calculations, we propose a boron and nitrogen cluster incorporated graphene system for efficient valley polarization.","The broken spatial inversion symmetry results in high Berry curvature at K and K' valleys of the hexagonal Brillouin zone in this semiconducting system.","The consideration of excitonic quasiparticles within GW approximation along with their scattering processes within many-body Bethe-Salpeter equation gives rise to an optical gap of 1.72 eV with an excitonic binding energy of 0.65 eV. Owing to the negligible intervalley scattering, the electrons in opposite valleys are selectively excited by left- and right-handed circular polarized lights, as evident from the oscillator strength calculations.","Therefore, this system can exhibit circular-dichroism valley Hall effect in the presence of the in-plane electric field.","Moreover, such excitonic qubits can be exploited for information processing."],"url":"http://arxiv.org/abs/2404.11421v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 14:23:28","title":"SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping","abstract":"We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy.","sentences":["We present SLAIM - Simultaneous Localization and Implicit Mapping.","We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance.","Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms.","NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment.","Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences.","We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy.","Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views.","While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry.","In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces.","Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution.","We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy."],"url":"http://arxiv.org/abs/2404.11419v1","category":"cs.CV"}
{"created":"2024-04-17 14:22:57","title":"On an inhomogeneous coagulation model with a differential sedimentation kernel","abstract":"We study an inhomogeneous coagulation equation that contains a transport term in the spatial variable modeling the sedimentation of clusters. We prove local existence of mass conserving solutions for a class of coagulation kernels for which in the space homogeneous case instantaneous gelation (i.e., instantaneous loss of mass) occurs. Our result holds true in particular for sum-type kernels of homogeneity greater than one, for which solutions do not exist at all in the spatially homogeneous case. Moreover, our result covers kernels that in addition vanish on the diagonal, which have been used to describe the onset of rain and the behavior of air bubbles in water.","sentences":["We study an inhomogeneous coagulation equation that contains a transport term in the spatial variable modeling the sedimentation of clusters.","We prove local existence of mass conserving solutions for a class of coagulation kernels for which in the space homogeneous case instantaneous gelation (i.e., instantaneous loss of mass) occurs.","Our result holds true in particular for sum-type kernels of homogeneity greater than one, for which solutions do not exist at all in the spatially homogeneous case.","Moreover, our result covers kernels that in addition vanish on the diagonal, which have been used to describe the onset of rain and the behavior of air bubbles in water."],"url":"http://arxiv.org/abs/2404.11418v1","category":"math.AP"}
{"created":"2024-04-17 14:17:05","title":"Neural Shr\u00f6dinger Bridge Matching for Pansharpening","abstract":"Recent diffusion probabilistic models (DPM) in the field of pansharpening have been gradually gaining attention and have achieved state-of-the-art (SOTA) performance. In this paper, we identify shortcomings in directly applying DPMs to the task of pansharpening as an inverse problem: 1) initiating sampling directly from Gaussian noise neglects the low-resolution multispectral image (LRMS) as a prior; 2) low sampling efficiency often necessitates a higher number of sampling steps. We first reformulate pansharpening into the stochastic differential equation (SDE) form of an inverse problem. Building upon this, we propose a Schr\\\"odinger bridge matching method that addresses both issues.   We design an efficient deep neural network architecture tailored for the proposed SB matching.   In comparison to the well-established DL-regressive-based framework and the recent DPM framework, our method demonstrates SOTA performance with fewer sampling steps. Moreover, we discuss the relationship between SB matching and other methods based on SDEs and ordinary differential equations (ODEs), as well as its connection with optimal transport.   Code will be available.","sentences":["Recent diffusion probabilistic models (DPM) in the field of pansharpening have been gradually gaining attention and have achieved state-of-the-art (SOTA) performance.","In this paper, we identify shortcomings in directly applying DPMs to the task of pansharpening as an inverse problem: 1) initiating sampling directly from Gaussian noise neglects the low-resolution multispectral image (LRMS) as a prior; 2) low sampling efficiency often necessitates a higher number of sampling steps.","We first reformulate pansharpening into the stochastic differential equation (SDE) form of an inverse problem.","Building upon this, we propose a Schr\\\"odinger bridge matching method that addresses both issues.   ","We design an efficient deep neural network architecture tailored for the proposed SB matching.   ","In comparison to the well-established DL-regressive-based framework and the recent DPM framework, our method demonstrates SOTA performance with fewer sampling steps.","Moreover, we discuss the relationship between SB matching and other methods based on SDEs and ordinary differential equations (ODEs), as well as its connection with optimal transport.   ","Code will be available."],"url":"http://arxiv.org/abs/2404.11416v1","category":"cs.CV"}
{"created":"2024-04-17 14:08:51","title":"Wave-front tracking for a quasi-linear scalar conservation law with hysteresis","abstract":"In this article we deal with the Cauchy problem for the quasi-linear scalar conservation law \\[u_t+ {\\cal F}(u)_t+u_x=0,\\] where ${\\cal F}$ is a specific hysteresis operator, namely the Play operator. Hysteresis models a rate-independent memory relationship between the input $u$ and its output. Its presence in the partial differential equation gives a particular non-local feature to the latter allowing us to capture phenomena that may arise in some application fields. Riemann problems and the interactions between shock lines are studied and via the so-called Wave-Front Tracking method a solution to the Cauchy problem with bounded variation initial data is constructed. The solution found satisfies an entropy-like condition, making it the unique solution in the class of entropy admissible ones.","sentences":["In this article we deal with the Cauchy problem for the quasi-linear scalar conservation law \\[u_t+ {\\cal F}(u)_t+u_x=0,\\] where ${\\cal F}$ is a specific hysteresis operator, namely the Play operator.","Hysteresis models a rate-independent memory relationship between the input $u$ and its output.","Its presence in the partial differential equation gives a particular non-local feature to the latter allowing us to capture phenomena that may arise in some application fields.","Riemann problems and the interactions between shock lines are studied and via the so-called Wave-Front Tracking method a solution to the Cauchy problem with bounded variation initial data is constructed.","The solution found satisfies an entropy-like condition, making it the unique solution in the class of entropy admissible ones."],"url":"http://arxiv.org/abs/2404.11405v1","category":"math.AP"}
{"created":"2024-04-18 17:51:02","title":"KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated Learning","abstract":"Vertical Federated Learning (VFL) is a category of Federated Learning in which models are trained collaboratively among parties with vertically partitioned data. Typically, in a VFL scenario, the labels of the samples are kept private from all the parties except for the aggregating server, that is the label owner. Nevertheless, recent works discovered that by exploiting gradient information returned by the server to bottom models, with the knowledge of only a small set of auxiliary labels on a very limited subset of training data points, an adversary can infer the private labels. These attacks are known as label inference attacks in VFL. In our work, we propose a novel framework called KDk, that combines Knowledge Distillation and k-anonymity to provide a defense mechanism against potential label inference attacks in a VFL scenario. Through an exhaustive experimental campaign we demonstrate that by applying our approach, the performance of the analyzed label inference attacks decreases consistently, even by more than 60%, maintaining the accuracy of the whole VFL almost unaltered.","sentences":["Vertical Federated Learning (VFL) is a category of Federated Learning in which models are trained collaboratively among parties with vertically partitioned data.","Typically, in a VFL scenario, the labels of the samples are kept private from all the parties except for the aggregating server, that is the label owner.","Nevertheless, recent works discovered that by exploiting gradient information returned by the server to bottom models, with the knowledge of only a small set of auxiliary labels on a very limited subset of training data points, an adversary can infer the private labels.","These attacks are known as label inference attacks in VFL.","In our work, we propose a novel framework called KDk, that combines Knowledge Distillation and k-anonymity to provide a defense mechanism against potential label inference attacks in a VFL scenario.","Through an exhaustive experimental campaign we demonstrate that by applying our approach, the performance of the analyzed label inference attacks decreases consistently, even by more than 60%, maintaining the accuracy of the whole VFL almost unaltered."],"url":"http://arxiv.org/abs/2404.12369v1","category":"cs.LG"}
{"created":"2024-04-18 15:42:52","title":"Efficient Identification of Broad Absorption Line Quasars using Dimensionality Reduction and Machine Learning","abstract":"Broad Absorption Line Quasars (BALQSOs) displaying distinct blue-shifted broad absorption lines. These serve as invaluable probes for unraveling the intricate structure and evolution of quasars, shedding light on the profound influence exerted by supermassive black holes on galaxy formation. The proliferation of large-scale spectroscopic surveys such as LAMOST, SDSS, and DESI has exponentially expanded the repository of quasar spectra at our disposal. In this study, we present an innovative approach to streamline the identification of BALQSOs, leveraging the power of dimensionality reduction and machine learning algorithms. Our dataset is curated from the SDSS DR16, amalgamating quasar spectra with classification labels sourced from the DR16Q quasar catalog. We employ a diverse array of dimensionality reduction techniques, including Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Locally Linear Embedding (LLE), and Isometric Mapping (ISOMAP), to distill the essence of the original spectral data. The resultant low-dimensional representations serve as inputs for a suite of machine learning classifiers, including XGBoost and Random Forest models. Through experimentation, we unveil PCA as the most effective dimensionality reduction methodology, adeptly navigating the intricate balance between dimensionality reduction and preservation of vital spectral information. Notably, the synergistic fusion of PCA with the XGBoost classifier emerges as the pinnacle of efficacy in the BALQSO classification endeavor, boasting impressive accuracy rates of 97.60% by 10-cross validation and 96.92% on the outer test sample. This study not only introduces a novel machine learning-based paradigm for quasar classification but also offers invaluable insights transferrable to a myriad of spectral classification challenges pervasive in the realm of astronomy.","sentences":["Broad Absorption Line Quasars (BALQSOs) displaying distinct blue-shifted broad absorption lines.","These serve as invaluable probes for unraveling the intricate structure and evolution of quasars, shedding light on the profound influence exerted by supermassive black holes on galaxy formation.","The proliferation of large-scale spectroscopic surveys such as LAMOST, SDSS, and DESI has exponentially expanded the repository of quasar spectra at our disposal.","In this study, we present an innovative approach to streamline the identification of BALQSOs, leveraging the power of dimensionality reduction and machine learning algorithms.","Our dataset is curated from the SDSS DR16, amalgamating quasar spectra with classification labels sourced from the DR16Q quasar catalog.","We employ a diverse array of dimensionality reduction techniques, including Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Locally Linear Embedding (LLE), and Isometric Mapping (ISOMAP), to distill the essence of the original spectral data.","The resultant low-dimensional representations serve as inputs for a suite of machine learning classifiers, including XGBoost and Random Forest models.","Through experimentation, we unveil PCA as the most effective dimensionality reduction methodology, adeptly navigating the intricate balance between dimensionality reduction and preservation of vital spectral information.","Notably, the synergistic fusion of PCA with the XGBoost classifier emerges as the pinnacle of efficacy in the BALQSO classification endeavor, boasting impressive accuracy rates of 97.60% by 10-cross validation and 96.92% on the outer test sample.","This study not only introduces a novel machine learning-based paradigm for quasar classification but also offers invaluable insights transferrable to a myriad of spectral classification challenges pervasive in the realm of astronomy."],"url":"http://arxiv.org/abs/2404.12270v1","category":"astro-ph.GA"}
{"created":"2024-04-18 13:46:16","title":"Gait Recognition from Highly Compressed Videos","abstract":"Surveillance footage represents a valuable resource and opportunities for conducting gait analysis. However, the typical low quality and high noise levels in such footage can severely impact the accuracy of pose estimation algorithms, which are foundational for reliable gait analysis. Existing literature suggests a direct correlation between the efficacy of pose estimation and the subsequent gait analysis results. A common mitigation strategy involves fine-tuning pose estimation models on noisy data to improve robustness. However, this approach may degrade the downstream model's performance on the original high-quality data, leading to a trade-off that is undesirable in practice. We propose a processing pipeline that incorporates a task-targeted artifact correction model specifically designed to pre-process and enhance surveillance footage before pose estimation. Our artifact correction model is optimized to work alongside a state-of-the-art pose estimation network, HRNet, without requiring repeated fine-tuning of the pose estimation model. Furthermore, we propose a simple and robust method for obtaining low quality videos that are annotated with poses in an automatic manner with the purpose of training the artifact correction model. We systematically evaluate the performance of our artifact correction model against a range of noisy surveillance data and demonstrate that our approach not only achieves improved pose estimation on low-quality surveillance footage, but also preserves the integrity of the pose estimation on high resolution footage. Our experiments show a clear enhancement in gait analysis performance, supporting the viability of the proposed method as a superior alternative to direct fine-tuning strategies. Our contributions pave the way for more reliable gait analysis using surveillance data in real-world applications, regardless of data quality.","sentences":["Surveillance footage represents a valuable resource and opportunities for conducting gait analysis.","However, the typical low quality and high noise levels in such footage can severely impact the accuracy of pose estimation algorithms, which are foundational for reliable gait analysis.","Existing literature suggests a direct correlation between the efficacy of pose estimation and the subsequent gait analysis results.","A common mitigation strategy involves fine-tuning pose estimation models on noisy data to improve robustness.","However, this approach may degrade the downstream model's performance on the original high-quality data, leading to a trade-off that is undesirable in practice.","We propose a processing pipeline that incorporates a task-targeted artifact correction model specifically designed to pre-process and enhance surveillance footage before pose estimation.","Our artifact correction model is optimized to work alongside a state-of-the-art pose estimation network, HRNet, without requiring repeated fine-tuning of the pose estimation model.","Furthermore, we propose a simple and robust method for obtaining low quality videos that are annotated with poses in an automatic manner with the purpose of training the artifact correction model.","We systematically evaluate the performance of our artifact correction model against a range of noisy surveillance data and demonstrate that our approach not only achieves improved pose estimation on low-quality surveillance footage, but also preserves the integrity of the pose estimation on high resolution footage.","Our experiments show a clear enhancement in gait analysis performance, supporting the viability of the proposed method as a superior alternative to direct fine-tuning strategies.","Our contributions pave the way for more reliable gait analysis using surveillance data in real-world applications, regardless of data quality."],"url":"http://arxiv.org/abs/2404.12183v1","category":"cs.CV"}
{"created":"2024-04-18 13:25:29","title":"Stance Detection on Social Media with Fine-Tuned Large Language Models","abstract":"Stance detection, a key task in natural language processing, determines an author's viewpoint based on textual analysis. This study evaluates the evolution of stance detection methods, transitioning from early machine learning approaches to the groundbreaking BERT model, and eventually to modern Large Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. While ChatGPT's closed-source nature and associated costs present challenges, the open-source models like LLaMa-2 and Mistral-7B offers an encouraging alternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2, and Mistral-7B using several publicly available datasets. Subsequently, to provide a comprehensive comparison, we assess the performance of these models in zero-shot and few-shot learning scenarios. The results underscore the exceptional ability of LLMs in accurately detecting stance, with all tested models surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7B demonstrate remarkable efficiency and potential for stance detection, despite their smaller sizes compared to ChatGPT. This study emphasizes the potential of LLMs in stance detection and calls for more extensive research in this field.","sentences":["Stance detection, a key task in natural language processing, determines an author's viewpoint based on textual analysis.","This study evaluates the evolution of stance detection methods, transitioning from early machine learning approaches to the groundbreaking BERT model, and eventually to modern Large Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B.","While ChatGPT's closed-source nature and associated costs present challenges, the open-source models like LLaMa-2 and Mistral-7B offers an encouraging alternative.","Initially, our research focused on fine-tuning ChatGPT, LLaMa-2, and Mistral-7B using several publicly available datasets.","Subsequently, to provide a comprehensive comparison, we assess the performance of these models in zero-shot and few-shot learning scenarios.","The results underscore the exceptional ability of LLMs in accurately detecting stance, with all tested models surpassing existing benchmarks.","Notably, LLaMa-2 and Mistral-7B demonstrate remarkable efficiency and potential for stance detection, despite their smaller sizes compared to ChatGPT.","This study emphasizes the potential of LLMs in stance detection and calls for more extensive research in this field."],"url":"http://arxiv.org/abs/2404.12171v1","category":"cs.CL"}
{"created":"2024-04-18 11:29:23","title":"LongEmbed: Extending Embedding Models for Long Context Retrieval","abstract":"Embedding models play a pivot role in modern NLP applications such as IR and RAG. While the context limit of LLMs has been pushed beyond 1 million tokens, embedding models are still confined to a narrow context window not exceeding 8k tokens, refrained from application scenarios requiring long inputs such as legal contracts. This paper explores context window extension of existing embedding models, pushing the limit to 32k without requiring additional training. First, we examine the performance of current embedding models for long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed comprises two synthetic tasks and four carefully chosen real-world tasks, featuring documents of varying length and dispersed target information. Benchmarking results underscore huge room for improvement in these models. Based on this, comprehensive experiments show that training-free context window extension strategies like position interpolation can effectively extend the context window of existing embedding models by several folds, regardless of their original context being 512 or beyond 4k. Furthermore, for models employing absolute position encoding (APE), we show the possibility of further fine-tuning to harvest notable performance gains while strictly preserving original behavior for short inputs. For models using rotary position embedding (RoPE), significant enhancements are observed when employing RoPE-specific methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for context window extension. To facilitate future research, we release E5-Base-4k and E5-RoPE-Base, along with the LongEmbed benchmark.","sentences":["Embedding models play a pivot role in modern NLP applications such as IR and RAG.","While the context limit of LLMs has been pushed beyond 1 million tokens, embedding models are still confined to a narrow context window not exceeding 8k tokens, refrained from application scenarios requiring long inputs such as legal contracts.","This paper explores context window extension of existing embedding models, pushing the limit to 32k without requiring additional training.","First, we examine the performance of current embedding models for long context retrieval on our newly constructed LongEmbed benchmark.","LongEmbed comprises two synthetic tasks and four carefully chosen real-world tasks, featuring documents of varying length and dispersed target information.","Benchmarking results underscore huge room for improvement in these models.","Based on this, comprehensive experiments show that training-free context window extension strategies like position interpolation can effectively extend the context window of existing embedding models by several folds, regardless of their original context being 512 or beyond 4k.","Furthermore, for models employing absolute position encoding (APE), we show the possibility of further fine-tuning to harvest notable performance gains while strictly preserving original behavior for short inputs.","For models using rotary position embedding (RoPE), significant enhancements are observed when employing RoPE-specific methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for context window extension.","To facilitate future research, we release E5-Base-4k and E5-RoPE-Base, along with the LongEmbed benchmark."],"url":"http://arxiv.org/abs/2404.12096v1","category":"cs.CL"}
{"created":"2024-04-18 11:20:53","title":"Harnessing Joint Rain-/Detail-aware Representations to Eliminate Intricate Rains","abstract":"Recent advances in image deraining have focused on training powerful models on mixed multiple datasets comprising diverse rain types and backgrounds. However, this approach tends to overlook the inherent differences among rainy images, leading to suboptimal results. To overcome this limitation, we focus on addressing various rainy images by delving into meaningful representations that encapsulate both the rain and background components. Leveraging these representations as instructive guidance, we put forth a Context-based Instance-level Modulation (CoI-M) mechanism adept at efficiently modulating CNN- or Transformer-based models. Furthermore, we devise a rain-/detail-aware contrastive learning strategy to help extract joint rain-/detail-aware representations. By integrating CoI-M with the rain-/detail-aware Contrastive learning, we develop CoIC, an innovative and potent algorithm tailored for training models on mixed datasets. Moreover, CoIC offers insight into modeling relationships of datasets, quantitatively assessing the impact of rain and details on restoration, and unveiling distinct behaviors of models given diverse inputs. Extensive experiments validate the efficacy of CoIC in boosting the deraining ability of CNN and Transformer models. CoIC also enhances the deraining prowess remarkably when real-world dataset is included.","sentences":["Recent advances in image deraining have focused on training powerful models on mixed multiple datasets comprising diverse rain types and backgrounds.","However, this approach tends to overlook the inherent differences among rainy images, leading to suboptimal results.","To overcome this limitation, we focus on addressing various rainy images by delving into meaningful representations that encapsulate both the rain and background components.","Leveraging these representations as instructive guidance, we put forth a Context-based Instance-level Modulation (CoI-M) mechanism adept at efficiently modulating CNN- or Transformer-based models.","Furthermore, we devise a rain-/detail-aware contrastive learning strategy to help extract joint rain-/detail-aware representations.","By integrating CoI-M with the rain-/detail-aware Contrastive learning, we develop CoIC, an innovative and potent algorithm tailored for training models on mixed datasets.","Moreover, CoIC offers insight into modeling relationships of datasets, quantitatively assessing the impact of rain and details on restoration, and unveiling distinct behaviors of models given diverse inputs.","Extensive experiments validate the efficacy of CoIC in boosting the deraining ability of CNN and Transformer models.","CoIC also enhances the deraining prowess remarkably when real-world dataset is included."],"url":"http://arxiv.org/abs/2404.12091v1","category":"cs.CV"}
{"created":"2024-04-18 08:56:47","title":"Variational Multi-Modal Hypergraph Attention Network for Multi-Modal Relation Extraction","abstract":"Multi-modal relation extraction (MMRE) is a challenging task that aims to identify relations between entities in text leveraging image information. Existing methods are limited by their neglect of the multiple entity pairs in one sentence sharing very similar contextual information (ie, the same text and image), resulting in increased difficulty in the MMRE task. To address this limitation, we propose the Variational Multi-Modal Hypergraph Attention Network (VM-HAN) for multi-modal relation extraction. Specifically, we first construct a multi-modal hypergraph for each sentence with the corresponding image, to establish different high-order intra-/inter-modal correlations for different entity pairs in each sentence. We further design the Variational Hypergraph Attention Networks (V-HAN) to obtain representational diversity among different entity pairs using Gaussian distribution and learn a better hypergraph structure via variational attention. VM-HAN achieves state-of-the-art performance on the multi-modal relation extraction task, outperforming existing methods in terms of accuracy and efficiency.","sentences":["Multi-modal relation extraction (MMRE) is a challenging task that aims to identify relations between entities in text leveraging image information.","Existing methods are limited by their neglect of the multiple entity pairs in one sentence sharing very similar contextual information (ie, the same text and image), resulting in increased difficulty in the MMRE task.","To address this limitation, we propose the Variational Multi-Modal Hypergraph Attention Network (VM-HAN) for multi-modal relation extraction.","Specifically, we first construct a multi-modal hypergraph for each sentence with the corresponding image, to establish different high-order intra-/inter-modal correlations for different entity pairs in each sentence.","We further design the Variational Hypergraph Attention Networks (V-HAN) to obtain representational diversity among different entity pairs using Gaussian distribution and learn a better hypergraph structure via variational attention.","VM-HAN achieves state-of-the-art performance on the multi-modal relation extraction task, outperforming existing methods in terms of accuracy and efficiency."],"url":"http://arxiv.org/abs/2404.12006v1","category":"cs.CL"}
{"created":"2024-04-18 08:16:56","title":"MTGA: Multi-view Temporal Granularity aligned Aggregation for Event-based Lip-reading","abstract":"Lip-reading is to utilize the visual information of the speaker's lip movements to recognize words and sentences. Existing event-based lip-reading solutions integrate different frame rate branches to learn spatio-temporal features of varying granularities. However, aggregating events into event frames inevitably leads to the loss of fine-grained temporal information within frames. To remedy this drawback, we propose a novel framework termed Multi-view Temporal Granularity aligned Aggregation (MTGA). Specifically, we first present a novel event representation method, namely time-segmented voxel graph list, where the most significant local voxels are temporally connected into a graph list. Then we design a spatio-temporal fusion module based on temporal granularity alignment, where the global spatial features extracted from event frames, together with the local relative spatial and temporal features contained in voxel graph list are effectively aligned and integrated. Finally, we design a temporal aggregation module that incorporates positional encoding, which enables the capture of local absolute spatial and global temporal information. Experiments demonstrate that our method outperforms both the event-based and video-based lip-reading counterparts. Our code will be publicly available.","sentences":["Lip-reading is to utilize the visual information of the speaker's lip movements to recognize words and sentences.","Existing event-based lip-reading solutions integrate different frame rate branches to learn spatio-temporal features of varying granularities.","However, aggregating events into event frames inevitably leads to the loss of fine-grained temporal information within frames.","To remedy this drawback, we propose a novel framework termed Multi-view Temporal Granularity aligned Aggregation (MTGA).","Specifically, we first present a novel event representation method, namely time-segmented voxel graph list, where the most significant local voxels are temporally connected into a graph list.","Then we design a spatio-temporal fusion module based on temporal granularity alignment, where the global spatial features extracted from event frames, together with the local relative spatial and temporal features contained in voxel graph list are effectively aligned and integrated.","Finally, we design a temporal aggregation module that incorporates positional encoding, which enables the capture of local absolute spatial and global temporal information.","Experiments demonstrate that our method outperforms both the event-based and video-based lip-reading counterparts.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2404.11979v1","category":"cs.CV"}
{"created":"2024-04-18 07:55:02","title":"P-NAL: an Effective and Interpretable Entity Alignment Method","abstract":"Entity alignment (EA) aims to find equivalent entities between two Knowledge Graphs. Existing embedding-based EA methods usually encode entities as embeddings, triples as embeddings' constraint and learn to align the embeddings. The structural and side information are usually utilized via embedding propagation, aggregation or interaction. However, the details of the underlying logical inference steps among the alignment process are usually omitted, resulting in inadequate inference process. In this paper, we introduce P-NAL, an entity alignment method that captures two types of logical inference paths with Non-Axiomatic Logic (NAL). Type 1 is the bridge-like inference path between to-be-aligned entity pairs, consisting of two relation/attribute triples and a similarity sentence between the other two entities. Type 2 links the entity pair by their embeddings. P-NAL iteratively aligns entities and relations by integrating the conclusions of the inference paths. Moreover, our method is logically interpretable and extensible due to the expressiveness of NAL. Our proposed method is suitable for various EA settings. Experimental results show that our method outperforms state-of-the-art methods in terms of Hits@1, achieving 0.98+ on all three datasets of DBP15K with both supervised and unsupervised settings. To our knowledge, we present the first in-depth analysis of entity alignment's basic principles from a unified logical perspective.","sentences":["Entity alignment (EA) aims to find equivalent entities between two Knowledge Graphs.","Existing embedding-based EA methods usually encode entities as embeddings, triples as embeddings' constraint and learn to align the embeddings.","The structural and side information are usually utilized via embedding propagation, aggregation or interaction.","However, the details of the underlying logical inference steps among the alignment process are usually omitted, resulting in inadequate inference process.","In this paper, we introduce P-NAL, an entity alignment method that captures two types of logical inference paths with Non-Axiomatic Logic (NAL).","Type 1 is the bridge-like inference path between to-be-aligned entity pairs, consisting of two relation/attribute triples and a similarity sentence between the other two entities.","Type 2 links the entity pair by their embeddings.","P-NAL iteratively aligns entities and relations by integrating the conclusions of the inference paths.","Moreover, our method is logically interpretable and extensible due to the expressiveness of NAL.","Our proposed method is suitable for various EA settings.","Experimental results show that our method outperforms state-of-the-art methods in terms of Hits@1, achieving 0.98+ on all three datasets of DBP15K with both supervised and unsupervised settings.","To our knowledge, we present the first in-depth analysis of entity alignment's basic principles from a unified logical perspective."],"url":"http://arxiv.org/abs/2404.11968v1","category":"cs.CL"}
{"created":"2024-04-18 07:52:12","title":"Multi-fidelity Gaussian process surrogate modeling for regression problems in physics","abstract":"One of the main challenges in surrogate modeling is the limited availability of data due to resource constraints associated with computationally expensive simulations. Multi-fidelity methods provide a solution by chaining models in a hierarchy with increasing fidelity, associated with lower error, but increasing cost. In this paper, we compare different multi-fidelity methods employed in constructing Gaussian process surrogates for regression. Non-linear autoregressive methods in the existing literature are primarily confined to two-fidelity models, and we extend these methods to handle more than two levels of fidelity. Additionally, we propose enhancements for an existing method incorporating delay terms by introducing a structured kernel. We demonstrate the performance of these methods across various academic and real-world scenarios. Our findings reveal that multi-fidelity methods generally have a smaller prediction error for the same computational cost as compared to the single-fidelity method, although their effectiveness varies across different scenarios.","sentences":["One of the main challenges in surrogate modeling is the limited availability of data due to resource constraints associated with computationally expensive simulations.","Multi-fidelity methods provide a solution by chaining models in a hierarchy with increasing fidelity, associated with lower error, but increasing cost.","In this paper, we compare different multi-fidelity methods employed in constructing Gaussian process surrogates for regression.","Non-linear autoregressive methods in the existing literature are primarily confined to two-fidelity models, and we extend these methods to handle more than two levels of fidelity.","Additionally, we propose enhancements for an existing method incorporating delay terms by introducing a structured kernel.","We demonstrate the performance of these methods across various academic and real-world scenarios.","Our findings reveal that multi-fidelity methods generally have a smaller prediction error for the same computational cost as compared to the single-fidelity method, although their effectiveness varies across different scenarios."],"url":"http://arxiv.org/abs/2404.11965v1","category":"stat.ML"}
{"created":"2024-04-18 06:38:02","title":"HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy Preservation in Multimodal Sentiment Analysis","abstract":"Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment tendencies in multimodal video content, raising serious concerns about privacy risks associated with multimodal data, such as voiceprints and facial images. Recent distributed collaborative learning has been verified as an effective paradigm for privacy preservation in multimodal tasks. However, they often overlook the privacy distinctions among different modalities, struggling to strike a balance between performance and privacy preservation. Consequently, it poses an intriguing question of maximizing multimodal utilization to improve performance while simultaneously protecting necessary modalities. This paper forms the first attempt at modality-specified (i.e., audio and visual) privacy preservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality cGAN framework (HyDiscGAN), which learns multimodality alignment to generate fake audio and visual features conditioned on shareable de-identified textual data. The objective is to leverage the fake features to approximate real audio and visual content to guarantee privacy preservation while effectively enhancing performance. Extensive experiments show that compared with the state-of-the-art MSA model, HyDiscGAN can achieve superior or competitive performance while preserving privacy.","sentences":["Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment tendencies in multimodal video content, raising serious concerns about privacy risks associated with multimodal data, such as voiceprints and facial images.","Recent distributed collaborative learning has been verified as an effective paradigm for privacy preservation in multimodal tasks.","However, they often overlook the privacy distinctions among different modalities, struggling to strike a balance between performance and privacy preservation.","Consequently, it poses an intriguing question of maximizing multimodal utilization to improve performance while simultaneously protecting necessary modalities.","This paper forms the first attempt at modality-specified (i.e., audio and visual) privacy preservation in MSA tasks.","We propose a novel Hybrid Distributed cross-modality cGAN framework (HyDiscGAN), which learns multimodality alignment to generate fake audio and visual features conditioned on shareable de-identified textual data.","The objective is to leverage the fake features to approximate real audio and visual content to guarantee privacy preservation while effectively enhancing performance.","Extensive experiments show that compared with the state-of-the-art MSA model, HyDiscGAN can achieve superior or competitive performance while preserving privacy."],"url":"http://arxiv.org/abs/2404.11938v1","category":"cs.MM"}
{"created":"2024-04-18 05:06:12","title":"Simultaneous Detection and Interaction Reasoning for Object-Centric Action Recognition","abstract":"The interactions between human and objects are important for recognizing object-centric actions. Existing methods usually adopt a two-stage pipeline, where object proposals are first detected using a pretrained detector, and then are fed to an action recognition model for extracting video features and learning the object relations for action recognition. However, since the action prior is unknown in the object detection stage, important objects could be easily overlooked, leading to inferior action recognition performance. In this paper, we propose an end-to-end object-centric action recognition framework that simultaneously performs Detection And Interaction Reasoning in one stage. Particularly, after extracting video features with a base network, we create three modules for concurrent object detection and interaction reasoning. First, a Patch-based Object Decoder generates proposals from video patch tokens. Then, an Interactive Object Refining and Aggregation identifies important objects for action recognition, adjusts proposal scores based on position and appearance, and aggregates object-level info into a global video representation. Lastly, an Object Relation Modeling module encodes object relations. These three modules together with the video feature extractor can be trained jointly in an end-to-end fashion, thus avoiding the heavy reliance on an off-the-shelf object detector, and reducing the multi-stage training burden. We conduct experiments on two datasets, Something-Else and Ikea-Assembly, to evaluate the performance of our proposed approach on conventional, compositional, and few-shot action recognition tasks. Through in-depth experimental analysis, we show the crucial role of interactive objects in learning for action recognition, and we can outperform state-of-the-art methods on both datasets.","sentences":["The interactions between human and objects are important for recognizing object-centric actions.","Existing methods usually adopt a two-stage pipeline, where object proposals are first detected using a pretrained detector, and then are fed to an action recognition model for extracting video features and learning the object relations for action recognition.","However, since the action prior is unknown in the object detection stage, important objects could be easily overlooked, leading to inferior action recognition performance.","In this paper, we propose an end-to-end object-centric action recognition framework that simultaneously performs Detection And Interaction Reasoning in one stage.","Particularly, after extracting video features with a base network, we create three modules for concurrent object detection and interaction reasoning.","First, a Patch-based Object Decoder generates proposals from video patch tokens.","Then, an Interactive Object Refining and Aggregation identifies important objects for action recognition, adjusts proposal scores based on position and appearance, and aggregates object-level info into a global video representation.","Lastly, an Object Relation Modeling module encodes object relations.","These three modules together with the video feature extractor can be trained jointly in an end-to-end fashion, thus avoiding the heavy reliance on an off-the-shelf object detector, and reducing the multi-stage training burden.","We conduct experiments on two datasets, Something-Else and Ikea-Assembly, to evaluate the performance of our proposed approach on conventional, compositional, and few-shot action recognition tasks.","Through in-depth experimental analysis, we show the crucial role of interactive objects in learning for action recognition, and we can outperform state-of-the-art methods on both datasets."],"url":"http://arxiv.org/abs/2404.11903v1","category":"cs.CV"}
{"created":"2024-04-18 04:25:56","title":"Multi-view X-ray Image Synthesis with Multiple Domain Disentanglement from CT Scans","abstract":"X-ray images play a vital role in the intraoperative processes due to their high resolution and fast imaging speed and greatly promote the subsequent segmentation, registration and reconstruction. However, over-dosed X-rays superimpose potential risks to human health to some extent. Data-driven algorithms from volume scans to X-ray images are restricted by the scarcity of paired X-ray and volume data. Existing methods are mainly realized by modelling the whole X-ray imaging procedure. In this study, we propose a learning-based approach termed CT2X-GAN to synthesize the X-ray images in an end-to-end manner using the content and style disentanglement from three different image domains. Our method decouples the anatomical structure information from CT scans and style information from unpaired real X-ray images/ digital reconstructed radiography (DRR) images via a series of decoupling encoders. Additionally, we introduce a novel consistency regularization term to improve the stylistic resemblance between synthesized X-ray images and real X-ray images. Meanwhile, we also impose a supervised process by computing the similarity of computed real DRR and synthesized DRR images. We further develop a pose attention module to fully strengthen the comprehensive information in the decoupled content code from CT scans, facilitating high-quality multi-view image synthesis in the lower 2D space. Extensive experiments were conducted on the publicly available CTSpine1K dataset and achieved 97.8350, 0.0842 and 3.0938 in terms of FID, KID and defined user-scored X-ray similarity, respectively. In comparison with 3D-aware methods ($\\pi$-GAN, EG3D), CT2X-GAN is superior in improving the synthesis quality and realistic to the real X-ray images.","sentences":["X-ray images play a vital role in the intraoperative processes due to their high resolution and fast imaging speed and greatly promote the subsequent segmentation, registration and reconstruction.","However, over-dosed X-rays superimpose potential risks to human health to some extent.","Data-driven algorithms from volume scans to X-ray images are restricted by the scarcity of paired X-ray and volume data.","Existing methods are mainly realized by modelling the whole X-ray imaging procedure.","In this study, we propose a learning-based approach termed CT2X-GAN to synthesize the X-ray images in an end-to-end manner using the content and style disentanglement from three different image domains.","Our method decouples the anatomical structure information from CT scans and style information from unpaired real X-ray images/ digital reconstructed radiography (DRR) images via a series of decoupling encoders.","Additionally, we introduce a novel consistency regularization term to improve the stylistic resemblance between synthesized X-ray images and real X-ray images.","Meanwhile, we also impose a supervised process by computing the similarity of computed real DRR and synthesized DRR images.","We further develop a pose attention module to fully strengthen the comprehensive information in the decoupled content code from CT scans, facilitating high-quality multi-view image synthesis in the lower 2D space.","Extensive experiments were conducted on the publicly available CTSpine1K dataset and achieved 97.8350, 0.0842 and 3.0938 in terms of FID, KID and defined user-scored X-ray similarity, respectively.","In comparison with 3D-aware methods ($\\pi$-GAN, EG3D), CT2X-GAN is superior in improving the synthesis quality and realistic to the real X-ray images."],"url":"http://arxiv.org/abs/2404.11889v1","category":"eess.IV"}
{"created":"2024-04-18 03:03:37","title":"Multi-view Graph Structural Representation Learning via Graph Coarsening","abstract":"Graph Transformers (GTs) have made remarkable achievements in graph-level tasks. However, most existing works regard graph structures as a form of guidance or bias for enhancing node representations, which focuses on node-central perspectives and lacks explicit representations of edges and structures. One natural question is, can we treat graph structures node-like as a whole to learn high-level features? Through experimental analysis, we explore the feasibility of this assumption. Based on our findings, we propose a novel multi-view graph structural representation learning model via graph coarsening (MSLgo) on GT architecture for graph classification. Specifically, we build three unique views, original, coarsening, and conversion, to learn a thorough structural representation. We compress loops and cliques via hierarchical heuristic graph coarsening and restrict them with well-designed constraints, which builds the coarsening view to learn high-level interactions between structures. We also introduce line graphs for edge embeddings and switch to edge-central perspective to construct the conversion view. Experiments on six real-world datasets demonstrate the improvements of MSLgo over 14 baselines from various architectures.","sentences":["Graph Transformers (GTs) have made remarkable achievements in graph-level tasks.","However, most existing works regard graph structures as a form of guidance or bias for enhancing node representations, which focuses on node-central perspectives and lacks explicit representations of edges and structures.","One natural question is, can we treat graph structures node-like as a whole to learn high-level features?","Through experimental analysis, we explore the feasibility of this assumption.","Based on our findings, we propose a novel multi-view graph structural representation learning model via graph coarsening (MSLgo) on GT architecture for graph classification.","Specifically, we build three unique views, original, coarsening, and conversion, to learn a thorough structural representation.","We compress loops and cliques via hierarchical heuristic graph coarsening and restrict them with well-designed constraints, which builds the coarsening view to learn high-level interactions between structures.","We also introduce line graphs for edge embeddings and switch to edge-central perspective to construct the conversion view.","Experiments on six real-world datasets demonstrate the improvements of MSLgo over 14 baselines from various architectures."],"url":"http://arxiv.org/abs/2404.11869v1","category":"cs.LG"}
{"created":"2024-04-18 02:52:20","title":"Automated tomographic assessment of structural defects of freeze-dried pharmaceuticals","abstract":"The topology and surface characteristics of lyophilisates significantly impact the stability and reconstitutability of freeze-dried pharmaceuticals. Consequently, visual quality control of the product is imperative. However, this procedure is not only time-consuming and labor-intensive but also expensive and prone to errors. In this paper, we present an approach for fully automated, non-destructive inspection of freeze-dried pharmaceuticals, leveraging robotics, computed tomography, and machine learning.","sentences":["The topology and surface characteristics of lyophilisates significantly impact the stability and reconstitutability of freeze-dried pharmaceuticals.","Consequently, visual quality control of the product is imperative.","However, this procedure is not only time-consuming and labor-intensive but also expensive and prone to errors.","In this paper, we present an approach for fully automated, non-destructive inspection of freeze-dried pharmaceuticals, leveraging robotics, computed tomography, and machine learning."],"url":"http://arxiv.org/abs/2404.11867v1","category":"cond-mat.soft"}
{"created":"2024-04-18 02:35:14","title":"sEMG-based Fine-grained Gesture Recognition via Improved LightGBM Model","abstract":"Surface electromyogram (sEMG), as a bioelectrical signal reflecting the activity of human muscles, has a wide range of applications in the control of prosthetics, human-computer interaction and so on. However, the existing recognition methods are all discrete actions, that is, every time an action is executed, it is necessary to restore the resting state before the next action, and it is unable to effectively recognize the gestures of continuous actions. To solve this problem, this paper proposes an improved fine gesture recognition model based on LightGBM algorithm. A sliding window sample segmentation scheme is adopted to replace active segment detection, and a series of innovative schemes such as improved loss function, Optuna hyperparameter search and Bagging integration are adopted to optimize LightGBM model and realize gesture recognition of continuous active segment signals. In order to verify the effectiveness of the proposed algorithm, we used the NinaproDB7 dataset to design the normal data recognition experiment and the disabled data transfer experiment. The results showed that the recognition rate of the proposed model was 89.72% higher than that of the optimal model Bi-ConvGRU for 18 gesture recognition tasks in the open data set, it reached 90.28%. Compared with the scheme directly trained on small sample data, the recognition rate of transfer learning was significantly improved from 60.35% to 78.54%, effectively solving the problem of insufficient data, and proving the applicability and advantages of transfer learning in fine gesture recognition tasks for disabled people.","sentences":["Surface electromyogram (sEMG), as a bioelectrical signal reflecting the activity of human muscles, has a wide range of applications in the control of prosthetics, human-computer interaction and so on.","However, the existing recognition methods are all discrete actions, that is, every time an action is executed, it is necessary to restore the resting state before the next action, and it is unable to effectively recognize the gestures of continuous actions.","To solve this problem, this paper proposes an improved fine gesture recognition model based on LightGBM algorithm.","A sliding window sample segmentation scheme is adopted to replace active segment detection, and a series of innovative schemes such as improved loss function, Optuna hyperparameter search and Bagging integration are adopted to optimize LightGBM model and realize gesture recognition of continuous active segment signals.","In order to verify the effectiveness of the proposed algorithm, we used the NinaproDB7 dataset to design the normal data recognition experiment and the disabled data transfer experiment.","The results showed that the recognition rate of the proposed model was 89.72% higher than that of the optimal model Bi-ConvGRU for 18 gesture recognition tasks in the open data set, it reached 90.28%.","Compared with the scheme directly trained on small sample data, the recognition rate of transfer learning was significantly improved from 60.35% to 78.54%, effectively solving the problem of insufficient data, and proving the applicability and advantages of transfer learning in fine gesture recognition tasks for disabled people."],"url":"http://arxiv.org/abs/2404.11861v1","category":"eess.SP"}
{"created":"2024-04-18 01:47:31","title":"Finding A Taxi with Illegal Driver Substitution Activity via Behavior Modelings","abstract":"In our urban life, Illegal Driver Substitution (IDS) activity for a taxi is a grave unlawful activity in the taxi industry, possibly causing severe traffic accidents and painful social repercussions. Currently, the IDS activity is manually supervised by law enforcers, i.e., law enforcers empirically choose a taxi and inspect it. The pressing problem of this scheme is the dilemma between the limited number of law-enforcers and the large volume of taxis. In this paper, motivated by this problem, we propose a computational method that helps law enforcers efficiently find the taxis which tend to have the IDS activity. Firstly, our method converts the identification of the IDS activity to a supervised learning task. Secondly, two kinds of taxi driver behaviors, i.e., the Sleeping Time and Location (STL) behavior and the Pick-Up (PU) behavior are proposed. Thirdly, the multiple scale pooling on self-similarity is proposed to encode the individual behaviors into the universal features for all taxis. Finally, a Multiple Component- Multiple Instance Learning (MC-MIL) method is proposed to handle the deficiency of the behavior features and to align the behavior features simultaneously. Extensive experiments on a real-world data set shows that the proposed behavior features have a good generalization ability across different classifiers, and the proposed MC-MIL method suppresses the baseline methods.","sentences":["In our urban life, Illegal Driver Substitution (IDS) activity for a taxi is a grave unlawful activity in the taxi industry, possibly causing severe traffic accidents and painful social repercussions.","Currently, the IDS activity is manually supervised by law enforcers, i.e., law enforcers empirically choose a taxi and inspect it.","The pressing problem of this scheme is the dilemma between the limited number of law-enforcers and the large volume of taxis.","In this paper, motivated by this problem, we propose a computational method that helps law enforcers efficiently find the taxis which tend to have the IDS activity.","Firstly, our method converts the identification of the IDS activity to a supervised learning task.","Secondly, two kinds of taxi driver behaviors, i.e., the Sleeping Time and Location (STL) behavior and the Pick-Up (PU) behavior are proposed.","Thirdly, the multiple scale pooling on self-similarity is proposed to encode the individual behaviors into the universal features for all taxis.","Finally, a Multiple Component- Multiple Instance Learning (MC-MIL) method is proposed to handle the deficiency of the behavior features and to align the behavior features simultaneously.","Extensive experiments on a real-world data set shows that the proposed behavior features have a good generalization ability across different classifiers, and the proposed MC-MIL method suppresses the baseline methods."],"url":"http://arxiv.org/abs/2404.11844v1","category":"cs.CY"}
{"created":"2024-04-18 01:40:03","title":"(Empirical) Bayes Approaches to Parallel Trends","abstract":"We consider Bayes and Empirical Bayes (EB) approaches for dealing with violations of parallel trends. In the Bayes approach, the researcher specifies a prior over both the pre-treatment violations of parallel trends $\\delta_{pre}$ and the post-treatment violations $\\delta_{post}$. The researcher then updates their posterior about the post-treatment bias $\\delta_{post}$ given an estimate of the pre-trends $\\delta_{pre}$. This allows them to form posterior means and credible sets for the treatment effect of interest, $\\tau_{post}$. In the EB approach, the prior on the violations of parallel trends is learned from the pre-treatment observations. We illustrate these approaches in two empirical applications.","sentences":["We consider Bayes and Empirical Bayes (EB) approaches for dealing with violations of parallel trends.","In the Bayes approach, the researcher specifies a prior over both the pre-treatment violations of parallel trends $\\delta_{pre}$ and the post-treatment violations $\\delta_{post}$. The researcher then updates their posterior about the post-treatment bias $\\delta_{post}$ given an estimate of the pre-trends $\\delta_{pre}$. This allows them to form posterior means and credible sets for the treatment effect of interest, $\\tau_{post}$. In the EB approach, the prior on the violations of parallel trends is learned from the pre-treatment observations.","We illustrate these approaches in two empirical applications."],"url":"http://arxiv.org/abs/2404.11839v1","category":"econ.EM"}
{"created":"2024-04-18 01:14:50","title":"Hypergraph Self-supervised Learning with Sampling-efficient Signals","abstract":"Self-supervised learning (SSL) provides a promising alternative for representation learning on hypergraphs without costly labels. However, existing hypergraph SSL models are mostly based on contrastive methods with the instance-level discrimination strategy, suffering from two significant limitations: (1) They select negative samples arbitrarily, which is unreliable in deciding similar and dissimilar pairs, causing training bias. (2) They often require a large number of negative samples, resulting in expensive computational costs. To address the above issues, we propose SE-HSSL, a hypergraph SSL framework with three sampling-efficient self-supervised signals. Specifically, we introduce two sampling-free objectives leveraging the canonical correlation analysis as the node-level and group-level self-supervised signals. Additionally, we develop a novel hierarchical membership-level contrast objective motivated by the cascading overlap relationship in hypergraphs, which can further reduce membership sampling bias and improve the efficiency of sample utilization. Through comprehensive experiments on 7 real-world hypergraphs, we demonstrate the superiority of our approach over the state-of-the-art method in terms of both effectiveness and efficiency.","sentences":["Self-supervised learning (SSL) provides a promising alternative for representation learning on hypergraphs without costly labels.","However, existing hypergraph SSL models are mostly based on contrastive methods with the instance-level discrimination strategy, suffering from two significant limitations: (1) They select negative samples arbitrarily, which is unreliable in deciding similar and dissimilar pairs, causing training bias.","(2) They often require a large number of negative samples, resulting in expensive computational costs.","To address the above issues, we propose SE-HSSL, a hypergraph SSL framework with three sampling-efficient self-supervised signals.","Specifically, we introduce two sampling-free objectives leveraging the canonical correlation analysis as the node-level and group-level self-supervised signals.","Additionally, we develop a novel hierarchical membership-level contrast objective motivated by the cascading overlap relationship in hypergraphs, which can further reduce membership sampling bias and improve the efficiency of sample utilization.","Through comprehensive experiments on 7 real-world hypergraphs, we demonstrate the superiority of our approach over the state-of-the-art method in terms of both effectiveness and efficiency."],"url":"http://arxiv.org/abs/2404.11825v1","category":"cs.LG"}
{"created":"2024-04-18 00:33:07","title":"Reinforcement Learning of Multi-robot Task Allocation for Multi-object Transportation with Infeasible Tasks","abstract":"Multi-object transport using multi-robot systems has the potential for diverse practical applications such as delivery services owing to its efficient individual and scalable cooperative transport. However, allocating transportation tasks of objects with unknown weights remains challenging. Moreover, the presence of infeasible tasks (untransportable objects) can lead to robot stoppage (deadlock). This paper proposes a framework for dynamic task allocation that involves storing task experiences for each task in a scalable manner with respect to the number of robots. First, these experiences are broadcasted from the cloud server to the entire robot system. Subsequently, each robot learns the exclusion levels for each task based on those task experiences, enabling it to exclude infeasible tasks and reset its task priorities. Finally, individual transportation, cooperative transportation, and the temporary exclusion of tasks considered infeasible are achieved. The scalability and versatility of the proposed method were confirmed through numerical experiments with an increased number of robots and objects, including unlearned weight objects. The effectiveness of the temporary deadlock avoidance was also confirmed by introducing additional robots within an episode. The proposed method enables the implementation of task allocation strategies that are feasible for different numbers of robots and various transport tasks without prior consideration of feasibility.","sentences":["Multi-object transport using multi-robot systems has the potential for diverse practical applications such as delivery services owing to its efficient individual and scalable cooperative transport.","However, allocating transportation tasks of objects with unknown weights remains challenging.","Moreover, the presence of infeasible tasks (untransportable objects) can lead to robot stoppage (deadlock).","This paper proposes a framework for dynamic task allocation that involves storing task experiences for each task in a scalable manner with respect to the number of robots.","First, these experiences are broadcasted from the cloud server to the entire robot system.","Subsequently, each robot learns the exclusion levels for each task based on those task experiences, enabling it to exclude infeasible tasks and reset its task priorities.","Finally, individual transportation, cooperative transportation, and the temporary exclusion of tasks considered infeasible are achieved.","The scalability and versatility of the proposed method were confirmed through numerical experiments with an increased number of robots and objects, including unlearned weight objects.","The effectiveness of the temporary deadlock avoidance was also confirmed by introducing additional robots within an episode.","The proposed method enables the implementation of task allocation strategies that are feasible for different numbers of robots and various transport tasks without prior consideration of feasibility."],"url":"http://arxiv.org/abs/2404.11817v1","category":"cs.RO"}
{"created":"2024-04-18 00:26:43","title":"Tailoring Generative Adversarial Networks for Smooth Airfoil Design","abstract":"In the realm of aerospace design, achieving smooth curves is paramount, particularly when crafting objects such as airfoils. Generative Adversarial Network (GAN), a widely employed generative AI technique, has proven instrumental in synthesizing airfoil designs. However, a common limitation of GAN is the inherent lack of smoothness in the generated airfoil surfaces. To address this issue, we present a GAN model featuring a customized loss function built to produce seamlessly contoured airfoil designs. Additionally, our model demonstrates a substantial increase in design diversity compared to a conventional GAN augmented with a post-processing smoothing filter.","sentences":["In the realm of aerospace design, achieving smooth curves is paramount, particularly when crafting objects such as airfoils.","Generative Adversarial Network (GAN), a widely employed generative AI technique, has proven instrumental in synthesizing airfoil designs.","However, a common limitation of GAN is the inherent lack of smoothness in the generated airfoil surfaces.","To address this issue, we present a GAN model featuring a customized loss function built to produce seamlessly contoured airfoil designs.","Additionally, our model demonstrates a substantial increase in design diversity compared to a conventional GAN augmented with a post-processing smoothing filter."],"url":"http://arxiv.org/abs/2404.11816v1","category":"cs.LG"}
{"created":"2024-04-18 00:05:02","title":"Sharing Parameter by Conjugation for Knowledge Graph Embeddings in Complex Space","abstract":"A Knowledge Graph (KG) is the directed graphical representation of entities and relations in the real world. KG can be applied in diverse Natural Language Processing (NLP) tasks where knowledge is required. The need to scale up and complete KG automatically yields Knowledge Graph Embedding (KGE), a shallow machine learning model that is suffering from memory and training time consumption issues. To mitigate the computational load, we propose a parameter-sharing method, i.e., using conjugate parameters for complex numbers employed in KGE models. Our method improves memory efficiency by 2x in relation embedding while achieving comparable performance to the state-of-the-art non-conjugate models, with faster, or at least comparable, training time. We demonstrated the generalizability of our method on two best-performing KGE models $5^{\\bigstar}\\mathrm{E}$ and $\\mathrm{ComplEx}$ on five benchmark datasets.","sentences":["A Knowledge Graph (KG) is the directed graphical representation of entities and relations in the real world.","KG can be applied in diverse Natural Language Processing (NLP) tasks where knowledge is required.","The need to scale up and complete KG automatically yields Knowledge Graph Embedding (KGE), a shallow machine learning model that is suffering from memory and training time consumption issues.","To mitigate the computational load, we propose a parameter-sharing method, i.e., using conjugate parameters for complex numbers employed in KGE models.","Our method improves memory efficiency by 2x in relation embedding while achieving comparable performance to the state-of-the-art non-conjugate models, with faster, or at least comparable, training time.","We demonstrated the generalizability of our method on two best-performing KGE models $5^{\\bigstar}\\mathrm{E}$ and $\\mathrm{ComplEx}$ on five benchmark datasets."],"url":"http://arxiv.org/abs/2404.11809v1","category":"cs.CL"}
{"created":"2024-04-17 23:33:34","title":"Establishing a Baseline for Gaze-driven Authentication Performance in VR: A Breadth-First Investigation on a Very Large Dataset","abstract":"This paper performs the crucial work of establishing a baseline for gaze-driven authentication performance to begin answering fundamental research questions using a very large dataset of gaze recordings from 9202 people with a level of eye tracking (ET) signal quality equivalent to modern consumer-facing virtual reality (VR) platforms. The size of the employed dataset is at least an order-of-magnitude larger than any other dataset from previous related work. Binocular estimates of the optical and visual axes of the eyes and a minimum duration for enrollment and verification are required for our model to achieve a false rejection rate (FRR) of below 3% at a false acceptance rate (FAR) of 1 in 50,000. In terms of identification accuracy which decreases with gallery size, we estimate that our model would fall below chance-level accuracy for gallery sizes of 148,000 or more. Our major findings indicate that gaze authentication can be as accurate as required by the FIDO standard when driven by a state-of-the-art machine learning architecture and a sufficiently large training dataset.","sentences":["This paper performs the crucial work of establishing a baseline for gaze-driven authentication performance to begin answering fundamental research questions using a very large dataset of gaze recordings from 9202 people with a level of eye tracking (ET) signal quality equivalent to modern consumer-facing virtual reality (VR) platforms.","The size of the employed dataset is at least an order-of-magnitude larger than any other dataset from previous related work.","Binocular estimates of the optical and visual axes of the eyes and a minimum duration for enrollment and verification are required for our model to achieve a false rejection rate (FRR) of below 3% at a false acceptance rate (FAR) of 1 in 50,000.","In terms of identification accuracy which decreases with gallery size, we estimate that our model would fall below chance-level accuracy for gallery sizes of 148,000 or more.","Our major findings indicate that gaze authentication can be as accurate as required by the FIDO standard when driven by a state-of-the-art machine learning architecture and a sufficiently large training dataset."],"url":"http://arxiv.org/abs/2404.11798v1","category":"cs.CV"}
{"created":"2024-04-17 22:44:22","title":"NonGEMM Bench: Understanding the Performance Horizon of the Latest ML Workloads with NonGEMM Workloads","abstract":"Machine Learning (ML) operators are the building blocks to design ML models with various target applications. GEneral Matrix Multiplication (GEMM) operators are the backbone of ML models. They are notorious for being computationally expensive requiring billions of multiply-and-accumulate. Therefore, significant effort has been put to study and optimize the GEMM operators in order to speed up the execution of ML models. GPUs and accelerators are widely deployed to accelerate ML workloads by optimizing the execution of GEMM operators. Nonetheless, the performance of NonGEMM operators have not been studied as thoroughly as GEMMs. Therefore, this paper describes \\bench, a benchmark to study NonGEMM operators. We first construct \\bench using popular ML workloads from different domains, then perform case studies on various grade GPU platforms to analyze the behavior of NonGEMM operators in GPU accelerated systems. Finally, we present some key takeaways to bridge the gap between GEMM and NonGEMM operators and to offer the community with potential new optimization directions.","sentences":["Machine Learning (ML) operators are the building blocks to design ML models with various target applications.","GEneral Matrix Multiplication (GEMM) operators are the backbone of ML models.","They are notorious for being computationally expensive requiring billions of multiply-and-accumulate.","Therefore, significant effort has been put to study and optimize the GEMM operators in order to speed up the execution of ML models.","GPUs and accelerators are widely deployed to accelerate ML workloads by optimizing the execution of GEMM operators.","Nonetheless, the performance of NonGEMM operators have not been studied as thoroughly as GEMMs.","Therefore, this paper describes \\bench, a benchmark to study NonGEMM operators.","We first construct \\bench using popular ML workloads from different domains, then perform case studies on various grade GPU platforms to analyze the behavior of NonGEMM operators in GPU accelerated systems.","Finally, we present some key takeaways to bridge the gap between GEMM and NonGEMM operators and to offer the community with potential new optimization directions."],"url":"http://arxiv.org/abs/2404.11788v1","category":"cs.AR"}
{"created":"2024-04-17 22:02:22","title":"CU-Mamba: Selective State Space Models with Channel Learning for Image Restoration","abstract":"Reconstructing degraded images is a critical task in image processing. Although CNN and Transformer-based models are prevalent in this field, they exhibit inherent limitations, such as inadequate long-range dependency modeling and high computational costs. To overcome these issues, we introduce the Channel-Aware U-Shaped Mamba (CU-Mamba) model, which incorporates a dual State Space Model (SSM) framework into the U-Net architecture. CU-Mamba employs a Spatial SSM module for global context encoding and a Channel SSM component to preserve channel correlation features, both in linear computational complexity relative to the feature map size. Extensive experimental results validate CU-Mamba's superiority over existing state-of-the-art methods, underscoring the importance of integrating both spatial and channel contexts in image restoration.","sentences":["Reconstructing degraded images is a critical task in image processing.","Although CNN and Transformer-based models are prevalent in this field, they exhibit inherent limitations, such as inadequate long-range dependency modeling and high computational costs.","To overcome these issues, we introduce the Channel-Aware U-Shaped Mamba (CU-Mamba) model, which incorporates a dual State Space Model (SSM) framework into the U-Net architecture.","CU-Mamba employs a Spatial SSM module for global context encoding and a Channel SSM component to preserve channel correlation features, both in linear computational complexity relative to the feature map size.","Extensive experimental results validate CU-Mamba's superiority over existing state-of-the-art methods, underscoring the importance of integrating both spatial and channel contexts in image restoration."],"url":"http://arxiv.org/abs/2404.11778v1","category":"cs.CV"}
{"created":"2024-04-17 21:57:29","title":"3D object quality prediction for Metal Jet Printer with Multimodal thermal encoder","abstract":"With the advancements in 3D printing technologies, it is extremely important that the quality of 3D printed objects, and dimensional accuracies should meet the customer's specifications. Various factors during metal printing affect the printed parts' quality, including the power quality, the printing stage parameters, the print part's location inside the print bed, the curing stage parameters, and the metal sintering process. With the large data gathered from HP's MetJet printing process, AI techniques can be used to analyze, learn, and effectively infer the printed part quality metrics, as well as assist in improving the print yield. In-situ thermal sensing data captured by printer-installed thermal sensors contains the part thermal signature of fusing layers. Such part thermal signature contains a convoluted impact from various factors. In this paper, we use a multimodal thermal encoder network to fuse data of a different nature including the video data vectorized printer control data, and exact part thermal signatures with a trained encoder-decoder module. We explored the data fusing techniques and stages for data fusing, the optimized end-to-end model architecture indicates an improved part quality prediction accuracy.","sentences":["With the advancements in 3D printing technologies, it is extremely important that the quality of 3D printed objects, and dimensional accuracies should meet the customer's specifications.","Various factors during metal printing affect the printed parts' quality, including the power quality, the printing stage parameters, the print part's location inside the print bed, the curing stage parameters, and the metal sintering process.","With the large data gathered from HP's MetJet printing process, AI techniques can be used to analyze, learn, and effectively infer the printed part quality metrics, as well as assist in improving the print yield.","In-situ thermal sensing data captured by printer-installed thermal sensors contains the part thermal signature of fusing layers.","Such part thermal signature contains a convoluted impact from various factors.","In this paper, we use a multimodal thermal encoder network to fuse data of a different nature including the video data vectorized printer control data, and exact part thermal signatures with a trained encoder-decoder module.","We explored the data fusing techniques and stages for data fusing, the optimized end-to-end model architecture indicates an improved part quality prediction accuracy."],"url":"http://arxiv.org/abs/2404.11776v1","category":"cs.LG"}
{"created":"2024-04-17 21:51:03","title":"Tensor-Networks-based Learning of Probabilistic Cellular Automata Dynamics","abstract":"Algorithms developed to solve many-body quantum problems, like tensor networks, can turn into powerful quantum-inspired tools to tackle problems in the classical domain. In this work, we focus on matrix product operators, a prominent numerical technique to study many-body quantum systems, especially in one dimension. It has been previously shown that such a tool can be used for classification, learning of deterministic sequence-to-sequence processes and of generic quantum processes. We further develop a matrix product operator algorithm to learn probabilistic sequence-to-sequence processes and apply this algorithm to probabilistic cellular automata. This new approach can accurately learn probabilistic cellular automata processes in different conditions, even when the process is a probabilistic mixture of different chaotic rules. In addition, we find that the ability to learn these dynamics is a function of the bit-wise difference between the rules and whether one is much more likely than the other.","sentences":["Algorithms developed to solve many-body quantum problems, like tensor networks, can turn into powerful quantum-inspired tools to tackle problems in the classical domain.","In this work, we focus on matrix product operators, a prominent numerical technique to study many-body quantum systems, especially in one dimension.","It has been previously shown that such a tool can be used for classification, learning of deterministic sequence-to-sequence processes and of generic quantum processes.","We further develop a matrix product operator algorithm to learn probabilistic sequence-to-sequence processes and apply this algorithm to probabilistic cellular automata.","This new approach can accurately learn probabilistic cellular automata processes in different conditions, even when the process is a probabilistic mixture of different chaotic rules.","In addition, we find that the ability to learn these dynamics is a function of the bit-wise difference between the rules and whether one is much more likely than the other."],"url":"http://arxiv.org/abs/2404.11768v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-17 21:36:33","title":"Predictive Model Development to Identify Failed Healing in Patients after Non-Union Fracture Surgery","abstract":"Bone non-union is among the most severe complications associated with trauma surgery, occurring in 10-30% of cases after long bone fractures. Treating non-unions requires a high level of surgical expertise and often involves multiple revision surgeries, sometimes even leading to amputation. Thus, more accurate prognosis is crucial for patient well-being. Recent advances in machine learning (ML) hold promise for developing models to predict non-union healing, even when working with smaller datasets, a commonly encountered challenge in clinical domains. To demonstrate the effectiveness of ML in identifying candidates at risk of failed non-union healing, we applied three ML models (logistic regression, support vector machine, and XGBoost) to the clinical dataset TRUFFLE, which includes 797 patients with long bone non-union. The models provided prediction results with 70% sensitivity, and the specificities of 66% (XGBoost), 49% (support vector machine), and 43% (logistic regression). These findings offer valuable clinical insights because they enable early identification of patients at risk of failed non-union healing after the initial surgical revision treatment protocol.","sentences":["Bone non-union is among the most severe complications associated with trauma surgery, occurring in 10-30% of cases after long bone fractures.","Treating non-unions requires a high level of surgical expertise and often involves multiple revision surgeries, sometimes even leading to amputation.","Thus, more accurate prognosis is crucial for patient well-being.","Recent advances in machine learning (ML) hold promise for developing models to predict non-union healing, even when working with smaller datasets, a commonly encountered challenge in clinical domains.","To demonstrate the effectiveness of ML in identifying candidates at risk of failed non-union healing, we applied three ML models (logistic regression, support vector machine, and XGBoost) to the clinical dataset TRUFFLE, which includes 797 patients with long bone non-union.","The models provided prediction results with 70% sensitivity, and the specificities of 66% (XGBoost), 49% (support vector machine), and 43% (logistic regression).","These findings offer valuable clinical insights because they enable early identification of patients at risk of failed non-union healing after the initial surgical revision treatment protocol."],"url":"http://arxiv.org/abs/2404.11760v1","category":"cs.LG"}
{"created":"2024-04-17 21:11:12","title":"Virtual Foundry Graphnet for Metal Sintering Deformation Prediction","abstract":"Metal Sintering is a necessary step for Metal Injection Molded parts and binder jet such as HP's metal 3D printer. The metal sintering process introduces large deformation varying from 25 to 50% depending on the green part porosity. In this paper, we use a graph-based deep learning approach to predict the part deformation, which can speed up the deformation simulation substantially at the voxel level. Running a well-trained Metal Sintering inferencing engine only takes a range of seconds to obtain the final sintering deformation value. The tested accuracy on example complex geometry achieves 0.7um mean deviation for a 63mm testing part.","sentences":["Metal Sintering is a necessary step for Metal Injection Molded parts and binder jet such as HP's metal 3D printer.","The metal sintering process introduces large deformation varying from 25 to 50% depending on the green part porosity.","In this paper, we use a graph-based deep learning approach to predict the part deformation, which can speed up the deformation simulation substantially at the voxel level.","Running a well-trained Metal Sintering inferencing engine only takes a range of seconds to obtain the final sintering deformation value.","The tested accuracy on example complex geometry achieves 0.7um mean deviation for a 63mm testing part."],"url":"http://arxiv.org/abs/2404.11753v1","category":"cs.LG"}
{"created":"2024-04-17 21:05:22","title":"Magnetic structure and component-separated transitions of HoNiSi$_{3}$","abstract":"HoNiSi$_{3}$ is an intermetallic compound characterized by two successive antiferromagnetic transitions at $T_{N1} = 6.3$ K and $T_{N2} = 10.4$ K. Here, its zero-field microscopic magnetic structure is inferred from resonant x-ray magnetic diffraction experiments on a single crystalline sample that complement previous bulk magnetic susceptibility data. For $T < T_{N2}$, the primitive magnetic unit cell matches the chemical cell. The magnetic structure features ferromagnetic {\\it ac} planes stacked in an antiferromagnetic $\\uparrow \\downarrow \\uparrow \\downarrow$ pattern. For $T_{N1} < T < T_{N2}$, the ordered magnetic moment points along $\\vec{a}$, and for $T < T_{N1}$ a component along $\\vec{c}$ also orders. A symmetry analysis indicates that the magnetic structure for $T<T_{N1}$ is not compatible with the presumed orthorhombic $Cmmm$ space group of the chemical structure, and therefore a slight lattice distortion is implied. Mean-field calculations using a simplified magnetic Hamiltonian, including a reduced set of three independent exchange coupling parameters determined by density functional theory calculations and two crystal electric field terms taken as free-fitting parameters, are able to reproduce the main experimental observations. An alternative approach using a more complete model including seven exchange coupling and nine crystal electric field terms is also explored, where the search of the ground state magnetic structure compatible with the available anisotropic magnetic susceptibility and magnetization data is carried out with the help of an unsupervised machine learning algorithm. The possible magnetic configurations are grouped into five clusters, and the cluster that yields the best comparison with the experimental macroscopic data contains the parameters previously found with the simplified model and also predicts the correct ground-state magnetic structure.","sentences":["HoNiSi$_{3}$ is an intermetallic compound characterized by two successive antiferromagnetic transitions at $T_{N1} = 6.3$ K and $T_{N2} = 10.4$ K. Here, its zero-field microscopic magnetic structure is inferred from resonant x-ray magnetic diffraction experiments on a single crystalline sample that complement previous bulk magnetic susceptibility data.","For $T <","T_{N2}$, the primitive magnetic unit cell matches the chemical cell.","The magnetic structure features ferromagnetic {\\it ac} planes stacked in an antiferromagnetic $\\uparrow \\downarrow \\uparrow \\downarrow$ pattern.","For $T_{N1} < T <","T_{N2}$, the ordered magnetic moment points along $\\vec{a}$, and for $T <","T_{N1}$ a component along $\\vec{c}$ also orders.","A symmetry analysis indicates that the magnetic structure for $T<T_{N1}$ is not compatible with the presumed orthorhombic $Cmmm$ space group of the chemical structure, and therefore a slight lattice distortion is implied.","Mean-field calculations using a simplified magnetic Hamiltonian, including a reduced set of three independent exchange coupling parameters determined by density functional theory calculations and two crystal electric field terms taken as free-fitting parameters, are able to reproduce the main experimental observations.","An alternative approach using a more complete model including seven exchange coupling and nine crystal electric field terms is also explored, where the search of the ground state magnetic structure compatible with the available anisotropic magnetic susceptibility and magnetization data is carried out with the help of an unsupervised machine learning algorithm.","The possible magnetic configurations are grouped into five clusters, and the cluster that yields the best comparison with the experimental macroscopic data contains the parameters previously found with the simplified model and also predicts the correct ground-state magnetic structure."],"url":"http://arxiv.org/abs/2404.11751v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 20:48:19","title":"Diffusion Schr\u00f6dinger Bridge Models for High-Quality MR-to-CT Synthesis for Head and Neck Proton Treatment Planning","abstract":"In recent advancements in proton therapy, MR-based treatment planning is gaining momentum to minimize additional radiation exposure compared to traditional CT-based methods. This transition highlights the critical need for accurate MR-to-CT image synthesis, which is essential for precise proton dose calculations. Our research introduces the Diffusion Schr\\\"odinger Bridge Models (DSBM), an innovative approach for high-quality MR-to-CT synthesis. DSBM learns the nonlinear diffusion processes between MR and CT data distributions. This method improves upon traditional diffusion models by initiating synthesis from the prior distribution rather than the Gaussian distribution, enhancing both generation quality and efficiency. We validated the effectiveness of DSBM on a head and neck cancer dataset, demonstrating its superiority over traditional image synthesis methods through both image-level and dosimetric-level evaluations. The effectiveness of DSBM in MR-based proton treatment planning highlights its potential as a valuable tool in various clinical scenarios.","sentences":["In recent advancements in proton therapy, MR-based treatment planning is gaining momentum to minimize additional radiation exposure compared to traditional CT-based methods.","This transition highlights the critical need for accurate MR-to-CT image synthesis, which is essential for precise proton dose calculations.","Our research introduces the Diffusion Schr\\\"odinger Bridge Models (DSBM), an innovative approach for high-quality MR-to-CT synthesis.","DSBM learns the nonlinear diffusion processes between MR and CT data distributions.","This method improves upon traditional diffusion models by initiating synthesis from the prior distribution rather than the Gaussian distribution, enhancing both generation quality and efficiency.","We validated the effectiveness of DSBM on a head and neck cancer dataset, demonstrating its superiority over traditional image synthesis methods through both image-level and dosimetric-level evaluations.","The effectiveness of DSBM in MR-based proton treatment planning highlights its potential as a valuable tool in various clinical scenarios."],"url":"http://arxiv.org/abs/2404.11741v1","category":"physics.med-ph"}
{"created":"2024-04-17 20:45:23","title":"Data-driven computation of adjoint sensitivities without adjoint solvers: An application to thermoacoustics","abstract":"Adjoint methods have been the pillar of gradient-based optimization for decades. They enable the accurate computation of a gradient (sensitivity) of a quantity of interest with respect to all system's parameters in one calculation. When the gradient is embedded in an optimization routine, the quantity of interest can be optimized for the system to have the desired behaviour. Adjoint methods require the system's Jacobian, whose computation can be cumbersome, and is problem dependent. We propose a computational strategy to infer the adjoint sensitivities from data (observables), which bypasses the need of the Jacobian of the physical system. The key component of this strategy is an echo state network, which learns the dynamics of nonlinear regimes with varying parameters, and evolves dynamically via a hidden state. Although the framework is general, we focus on thermoacoustics governed by nonlinear and time-delayed systems. First, we show that a parameter-aware Echo State Network (ESN) infers the parameterized dynamics. Second, we derive the adjoint of the ESN to compute the sensitivity of time-averaged cost functionals. Third, we propose the Thermoacoustic Echo State Network (T-ESN), which hard constrains the physical knowledge in the network architecture. Fourth, we apply the framework to a variety of nonlinear thermoacoustic regimes of a prototypical system. We show that the T-ESN accurately infers the correct adjoint sensitivities of the time-averaged acoustic energy with respect to the flame parameters. The results are robust to noisy data, from periodic, through quasiperiodic, to chaotic regimes. A single network predicts the nonlinear bifurcations on unseen scenarios, and so the inferred adjoint sensitivities are employed to suppress an instability via steepest descent. This work opens new possibilities for gradient-based data-driven design optimization.","sentences":["Adjoint methods have been the pillar of gradient-based optimization for decades.","They enable the accurate computation of a gradient (sensitivity) of a quantity of interest with respect to all system's parameters in one calculation.","When the gradient is embedded in an optimization routine, the quantity of interest can be optimized for the system to have the desired behaviour.","Adjoint methods require the system's Jacobian, whose computation can be cumbersome, and is problem dependent.","We propose a computational strategy to infer the adjoint sensitivities from data (observables), which bypasses the need of the Jacobian of the physical system.","The key component of this strategy is an echo state network, which learns the dynamics of nonlinear regimes with varying parameters, and evolves dynamically via a hidden state.","Although the framework is general, we focus on thermoacoustics governed by nonlinear and time-delayed systems.","First, we show that a parameter-aware Echo State Network (ESN) infers the parameterized dynamics.","Second, we derive the adjoint of the ESN to compute the sensitivity of time-averaged cost functionals.","Third, we propose the Thermoacoustic Echo State Network (T-ESN), which hard constrains the physical knowledge in the network architecture.","Fourth, we apply the framework to a variety of nonlinear thermoacoustic regimes of a prototypical system.","We show that the T-ESN accurately infers the correct adjoint sensitivities of the time-averaged acoustic energy with respect to the flame parameters.","The results are robust to noisy data, from periodic, through quasiperiodic, to chaotic regimes.","A single network predicts the nonlinear bifurcations on unseen scenarios, and so the inferred adjoint sensitivities are employed to suppress an instability via steepest descent.","This work opens new possibilities for gradient-based data-driven design optimization."],"url":"http://arxiv.org/abs/2404.11738v1","category":"physics.flu-dyn"}
{"created":"2024-04-17 20:37:29","title":"Learning with 3D rotations, a hitchhiker's guide to SO(3)","abstract":"Many settings in machine learning require the selection of a rotation representation. However, choosing a suitable representation from the many available options is challenging. This paper acts as a survey and guide through rotation representations. We walk through their properties that harm or benefit deep learning with gradient-based optimization. By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations. We provide guidance on selecting representations based on whether rotations are in the model's input or output and whether the data primarily comprises small angles.","sentences":["Many settings in machine learning require the selection of a rotation representation.","However, choosing a suitable representation from the many available options is challenging.","This paper acts as a survey and guide through rotation representations.","We walk through their properties that harm or benefit deep learning with gradient-based optimization.","By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations.","We provide guidance on selecting representations based on whether rotations are in the model's input or output and whether the data primarily comprises small angles."],"url":"http://arxiv.org/abs/2404.11735v1","category":"cs.LG"}
{"created":"2024-04-17 20:37:00","title":"Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To Program Comprehension Questions","abstract":"Recent research has explored the creation of questions from code submitted by students. These Questions about Learners' Code (QLCs) are created through program analysis, exploring execution paths, and then creating code comprehension questions from these paths and the broader code structure. Responding to the questions requires reading and tracing the code, which is known to support students' learning. At the same time, computing education researchers have witnessed the emergence of Large Language Models (LLMs) that have taken the community by storm. Researchers have demonstrated the applicability of these models especially in the introductory programming context, outlining their performance in solving introductory programming problems and their utility in creating new learning resources. In this work, we explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in answering QLCs that are generated from code that the LLMs have created. Our results show that although the state-of-the-art LLMs can create programs and trace program execution when prompted, they easily succumb to similar errors that have previously been recorded for novice programmers. These results demonstrate the fallibility of these models and perhaps dampen the expectations fueled by the recent LLM hype. At the same time, we also highlight future research possibilities such as using LLMs to mimic students as their behavior can indeed be similar for some specific tasks.","sentences":["Recent research has explored the creation of questions from code submitted by students.","These Questions about Learners' Code (QLCs) are created through program analysis, exploring execution paths, and then creating code comprehension questions from these paths and the broader code structure.","Responding to the questions requires reading and tracing the code, which is known to support students' learning.","At the same time, computing education researchers have witnessed the emergence of Large Language Models (LLMs) that have taken the community by storm.","Researchers have demonstrated the applicability of these models especially in the introductory programming context, outlining their performance in solving introductory programming problems and their utility in creating new learning resources.","In this work, we explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in answering QLCs that are generated from code that the LLMs have created.","Our results show that although the state-of-the-art LLMs can create programs and trace program execution when prompted, they easily succumb to similar errors that have previously been recorded for novice programmers.","These results demonstrate the fallibility of these models and perhaps dampen the expectations fueled by the recent LLM hype.","At the same time, we also highlight future research possibilities such as using LLMs to mimic students as their behavior can indeed be similar for some specific tasks."],"url":"http://arxiv.org/abs/2404.11734v1","category":"cs.CY"}
{"created":"2024-04-17 19:31:57","title":"On Machine Learning Complete Intersection Calabi-Yau 3-folds","abstract":"Gaussian Process Regression, Kernel Support Vector Regression, the random forest, extreme gradient boosting and the generalized linear model algorithms are applied to data of Complete Intersection Calabi-Yau 3-folds. It is shown that Gaussian process regression is the most suitable for learning the Hodge number h^(2,1)in terms of h^(1,1). The performance of this regression algorithm is such that the Pearson correlation coefficient for the validation set is R^2 = 0.9999999995 with a Root Mean Square Error RMSE = 0.0002895011. As for the calibration set, these two parameters are as follows: R^2 = 0.9999999994 and RMSE = 0.0002854348. The training error and the cross-validation error of this regression are 10^(-9) and 1.28 * 10^(-7), respectively. Learning the Hodge number h^(1,1)in terms of h^(2,1) yields R^2 = 1.000000 and RMSE = 7.395731 * 10^(-5) for the validation set of the Gaussian Process regression.","sentences":["Gaussian Process Regression, Kernel Support Vector Regression, the random forest, extreme gradient boosting and the generalized linear model algorithms are applied to data of Complete Intersection Calabi-Yau 3-folds.","It is shown that Gaussian process regression is the most suitable for learning the Hodge number h^(2,1)in terms of h^(1,1).","The performance of this regression algorithm is such that the Pearson correlation coefficient for the validation set is R^2 = 0.9999999995 with a Root Mean Square Error RMSE = 0.0002895011.","As for the calibration set, these two parameters are as follows: R^2 = 0.9999999994 and RMSE = 0.0002854348.","The training error and the cross-validation error of this regression are 10^(-9) and 1.28 * 10^(-7), respectively.","Learning the Hodge number h^(1,1)in terms of h^(2,1) yields R^2 = 1.000000 and RMSE = 7.395731 * 10^(-5) for the validation set of the Gaussian Process regression."],"url":"http://arxiv.org/abs/2404.11710v1","category":"hep-th"}
{"created":"2024-04-17 19:16:40","title":"Perspectives on Contractivity in Control, Optimization, and Learning","abstract":"Contraction theory is a mathematical framework for studying the convergence, robustness, and modularity properties of dynamical systems and algorithms. In this opinion paper, we provide five main opinions on the virtues of contraction theory. These opinions are (i) contraction theory is a unifying framework emerging from classical and modern works, (ii) contractivity is computationally-friendly, robust, and modular stability, (iii) numerous dynamical systems are contracting, (iv) contraction theory is relevant to modern applications, and (v) contraction theory can be vastly extended in numerous directions. We survey recent theoretical and applied research in each of these five directions.","sentences":["Contraction theory is a mathematical framework for studying the convergence, robustness, and modularity properties of dynamical systems and algorithms.","In this opinion paper, we provide five main opinions on the virtues of contraction theory.","These opinions are (i) contraction theory is a unifying framework emerging from classical and modern works, (ii) contractivity is computationally-friendly, robust, and modular stability, (iii) numerous dynamical systems are contracting, (iv) contraction theory is relevant to modern applications, and (v) contraction theory can be vastly extended in numerous directions.","We survey recent theoretical and applied research in each of these five directions."],"url":"http://arxiv.org/abs/2404.11707v1","category":"eess.SY"}
{"created":"2024-04-17 18:57:48","title":"Retrieval-Augmented Embodied Agents","abstract":"Embodied agents operating in complex and uncertain environments face considerable challenges. While some advanced agents handle complex manipulation tasks with proficiency, their success often hinges on extensive training data to develop their capabilities. In contrast, humans typically rely on recalling past experiences and analogous situations to solve new problems. Aiming to emulate this human approach in robotics, we introduce the Retrieval-Augmented Embodied Agent (RAEA). This innovative system equips robots with a form of shared memory, significantly enhancing their performance. Our approach integrates a policy retriever, allowing robots to access relevant strategies from an external policy memory bank based on multi-modal inputs. Additionally, a policy generator is employed to assimilate these strategies into the learning process, enabling robots to formulate effective responses to tasks. Extensive testing of RAEA in both simulated and real-world scenarios demonstrates its superior performance over traditional methods, representing a major leap forward in robotic technology.","sentences":["Embodied agents operating in complex and uncertain environments face considerable challenges.","While some advanced agents handle complex manipulation tasks with proficiency, their success often hinges on extensive training data to develop their capabilities.","In contrast, humans typically rely on recalling past experiences and analogous situations to solve new problems.","Aiming to emulate this human approach in robotics, we introduce the Retrieval-Augmented Embodied Agent (RAEA).","This innovative system equips robots with a form of shared memory, significantly enhancing their performance.","Our approach integrates a policy retriever, allowing robots to access relevant strategies from an external policy memory bank based on multi-modal inputs.","Additionally, a policy generator is employed to assimilate these strategies into the learning process, enabling robots to formulate effective responses to tasks.","Extensive testing of RAEA in both simulated and real-world scenarios demonstrates its superior performance over traditional methods, representing a major leap forward in robotic technology."],"url":"http://arxiv.org/abs/2404.11699v1","category":"cs.RO"}
{"created":"2024-04-17 18:42:36","title":"Improvement in Semantic Address Matching using Natural Language Processing","abstract":"Address matching is an important task for many businesses especially delivery and take out companies which help them to take out a certain address from their data warehouse. Existing solution uses similarity of strings, and edit distance algorithms to find out the similar addresses from the address database, but these algorithms could not work effectively with redundant, unstructured, or incomplete address data. This paper discuss semantic Address matching technique, by which we can find out a particular address from a list of possible addresses. We have also reviewed existing practices and their shortcoming. Semantic address matching is an essentially NLP task in the field of deep learning. Through this technique We have the ability to triumph the drawbacks of existing methods like redundant or abbreviated data problems. The solution uses the OCR on invoices to extract the address and create the data pool of addresses. Then this data is fed to the algorithm BM-25 for scoring the best matching entries. Then to observe the best result, this will pass through BERT for giving the best possible result from the similar queries. Our investigation exhibits that our methodology enormously improves both accuracy and review of cutting-edge technology existing techniques.","sentences":["Address matching is an important task for many businesses especially delivery and take out companies which help them to take out a certain address from their data warehouse.","Existing solution uses similarity of strings, and edit distance algorithms to find out the similar addresses from the address database, but these algorithms could not work effectively with redundant, unstructured, or incomplete address data.","This paper discuss semantic Address matching technique, by which we can find out a particular address from a list of possible addresses.","We have also reviewed existing practices and their shortcoming.","Semantic address matching is an essentially NLP task in the field of deep learning.","Through this technique We have the ability to triumph the drawbacks of existing methods like redundant or abbreviated data problems.","The solution uses the OCR on invoices to extract the address and create the data pool of addresses.","Then this data is fed to the algorithm BM-25 for scoring the best matching entries.","Then to observe the best result, this will pass through BERT for giving the best possible result from the similar queries.","Our investigation exhibits that our methodology enormously improves both accuracy and review of cutting-edge technology existing techniques."],"url":"http://arxiv.org/abs/2404.11691v1","category":"cs.CL"}
{"created":"2024-04-17 18:20:31","title":"Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant Forums","abstract":"Tenant-landlord relationships exhibit a power asymmetry where landlords' power to evict the tenants at a low-cost results in their dominating status in such relationships. Tenant concerns are thus often unspoken, unresolved, or ignored and this could lead to blatant conflicts as suppressed tenant concerns accumulate. Modern machine learning methods and Large Language Models (LLM) have demonstrated immense abilities to perform language tasks. In this study, we incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit post data scraped from the subreddit r/Tenant, aiming to unveil trends in tenant concerns while exploring the adoption of LLMs and machine learning methods in social science research. We find that tenant concerns in topics like fee dispute and utility issues are consistently dominant in all four states analyzed while each state has other common tenant concerns special to itself. Moreover, we discover temporal trends in tenant concerns that provide important implications regarding the impact of the pandemic and the Eviction Moratorium.","sentences":["Tenant-landlord relationships exhibit a power asymmetry where landlords' power to evict the tenants at a low-cost results in their dominating status in such relationships.","Tenant concerns are thus often unspoken, unresolved, or ignored and this could lead to blatant conflicts as suppressed tenant concerns accumulate.","Modern machine learning methods and Large Language Models (LLM) have demonstrated immense abilities to perform language tasks.","In this study, we incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit post data scraped from the subreddit r/Tenant, aiming to unveil trends in tenant concerns while exploring the adoption of LLMs and machine learning methods in social science research.","We find that tenant concerns in topics like fee dispute and utility issues are consistently dominant in all four states analyzed while each state has other common tenant concerns special to itself.","Moreover, we discover temporal trends in tenant concerns that provide important implications regarding the impact of the pandemic and the Eviction Moratorium."],"url":"http://arxiv.org/abs/2404.11681v1","category":"cs.HC"}
{"created":"2024-04-17 18:17:14","title":"Practical applications of machine-learned flows on gauge fields","abstract":"Normalizing flows are machine-learned maps between different lattice theories which can be used as components in exact sampling and inference schemes. Ongoing work yields increasingly expressive flows on gauge fields, but it remains an open question how flows can improve lattice QCD at state-of-the-art scales. We discuss and demonstrate two applications of flows in replica exchange (parallel tempering) sampling, aimed at improving topological mixing, which are viable with iterative improvements upon presently available flows.","sentences":["Normalizing flows are machine-learned maps between different lattice theories which can be used as components in exact sampling and inference schemes.","Ongoing work yields increasingly expressive flows on gauge fields, but it remains an open question how flows can improve lattice QCD at state-of-the-art scales.","We discuss and demonstrate two applications of flows in replica exchange (parallel tempering) sampling, aimed at improving topological mixing, which are viable with iterative improvements upon presently available flows."],"url":"http://arxiv.org/abs/2404.11674v1","category":"hep-lat"}
{"created":"2024-04-18 17:58:16","title":"Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos","abstract":"Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines.","sentences":["Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations.","However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations.","The problem becomes even more challenging for dynamic scenes and objects.","To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video.","Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video.","Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects.","We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians.","By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames.","During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines."],"url":"http://arxiv.org/abs/2404.12379v1","category":"cs.CV"}
{"created":"2024-04-18 14:11:06","title":"Tracing Pareto-optimal points for multi-objective shape optimization applied to electric machines","abstract":"In the context of the optimization of rotating electric machines, many different objective functions are of interest and considering this during the optimization is of crucial importance. While evolutionary algorithms can provide a Pareto front straightforwardly and are widely used in this context, derivative-based optimization algorithms can be computationally more efficient. In this case, a Pareto front can be obtained by performing several optimization runs with different weights. In this work, we focus on a free-form shape optimization approach allowing for arbitrary motor geometries. In particular, we propose a way to efficiently obtain Pareto-optimal points by moving along to the Pareto front exploiting a homotopy method based on second order shape derivatives.","sentences":["In the context of the optimization of rotating electric machines, many different objective functions are of interest and considering this during the optimization is of crucial importance.","While evolutionary algorithms can provide a Pareto front straightforwardly and are widely used in this context, derivative-based optimization algorithms can be computationally more efficient.","In this case, a Pareto front can be obtained by performing several optimization runs with different weights.","In this work, we focus on a free-form shape optimization approach allowing for arbitrary motor geometries.","In particular, we propose a way to efficiently obtain Pareto-optimal points by moving along to the Pareto front exploiting a homotopy method based on second order shape derivatives."],"url":"http://arxiv.org/abs/2404.12205v1","category":"math.OC"}
{"created":"2024-04-18 10:18:16","title":"Maximal weak Orlicz types and the strong maximal on von Neumann algebras","abstract":"Let $\\mathbf{E}_n: \\mathcal{M} \\to \\mathcal{M}_n$ and $\\mathbf{E}_m: \\mathcal{N} \\to \\mathcal{N}_m$ be two sequences of conditional expectations on finite von Neumann algebras. The optimal weak Orlicz type of the associated strong maximal operator $\\mathcal{E} = (\\mathbf{E}_n\\otimes \\mathbf{E}_m)_{n,m}$ is not yet known. In a recent work of Jose Conde and the two first-named authors, it was show that $\\mathcal{E}$ has weak type $(\\Phi, \\Phi)$ for a family of functions including $\\Phi(t) = t \\, \\log^{2+\\varepsilon} t$, for every $\\varepsilon > 0$. In this article, we prove that the weak Orlicz type of $\\mathcal{E}$ cannot be lowered below $L \\log^2 L$, meaning that if $\\mathcal{E}$ is of weak type $(\\Phi, \\Phi)$, then $\\Phi(s) \\not\\in o(s \\, \\log^2 s)$. Our proof is based on interpolation. Namely, we use recent techniques of Cadilhac/Ricard to formulate a Marcinkiewicz type theorem for maximal weak Orlicz types. Then, we show that a weak Orlicz type lower than $L \\log^2 L$ would imply a $p$-operator constant for $\\mathcal{E}$ smaller than the known optimum as $p \\to 1^{+}$.","sentences":["Let $\\mathbf{E}_n: \\mathcal{M} \\to \\mathcal{M}_n$ and $\\mathbf{E}_m: \\mathcal{N} \\to \\mathcal{N}_m$ be two sequences of conditional expectations on finite von Neumann algebras.","The optimal weak Orlicz type of the associated strong maximal operator $\\mathcal{E} = (\\mathbf{E}_n\\otimes \\mathbf{E}_m)_{n,m}$ is not yet known.","In a recent work of Jose Conde and the two first-named authors, it was show that $\\mathcal{E}$ has weak type $(\\Phi, \\Phi)$ for a family of functions including $\\Phi(t) = t \\, \\log^{2+\\varepsilon} t$, for every $\\varepsilon > 0$.","In this article, we prove that the weak Orlicz type of $\\mathcal{E}$ cannot be lowered below $L \\log^2 L$, meaning that if $\\mathcal{E}$ is of weak type $(\\Phi, \\Phi)$, then $\\Phi(s) \\not\\in o(s \\, \\log^2 s)$.","Our proof is based on interpolation.","Namely, we use recent techniques of Cadilhac/Ricard to formulate a Marcinkiewicz type theorem for maximal weak Orlicz types.","Then, we show that a weak Orlicz type lower than $L \\log^2 L$ would imply a $p$-operator constant for $\\mathcal{E}$ smaller than the known optimum as $p \\to 1^{+}$."],"url":"http://arxiv.org/abs/2404.12061v1","category":"math.OA"}
{"created":"2024-04-18 08:27:03","title":"Particle motion around traversable wormholes: Possibility of closed timelike geodesics","abstract":"The present work investigates the general wormhole solution in Einstein gravity with an exponential shape function around an ultrastatic and a finite redshift geometry. The geodesic motion around the wormholes is studied in which the deflection angle of the orbiting photon sphere is found to be negative after a certain region, indicating the presence of repulsive effect of gravity in both the ultrastatic and finite redshift wormholes. Various unbounded and bounded timelike trajectories are presented on the wormhole embedding diagrams, in which some of the bound orbits involve intersection points that may lead to causality violating geodesics. Another class of closed timelike geodesics are obtained in the unstable circular trajectory that appeared at the wormhole throat. Finally, the trajectories are classified in terms of the family of CTG orbits.","sentences":["The present work investigates the general wormhole solution in Einstein gravity with an exponential shape function around an ultrastatic and a finite redshift geometry.","The geodesic motion around the wormholes is studied in which the deflection angle of the orbiting photon sphere is found to be negative after a certain region, indicating the presence of repulsive effect of gravity in both the ultrastatic and finite redshift wormholes.","Various unbounded and bounded timelike trajectories are presented on the wormhole embedding diagrams, in which some of the bound orbits involve intersection points that may lead to causality violating geodesics.","Another class of closed timelike geodesics are obtained in the unstable circular trajectory that appeared at the wormhole throat.","Finally, the trajectories are classified in terms of the family of CTG orbits."],"url":"http://arxiv.org/abs/2404.11984v1","category":"gr-qc"}
{"created":"2024-04-18 07:47:44","title":"Diamond surfaces with lateral gradients for systematic optimization of surface chemistry for relaxometry -- A low pressure plasma-based approach","abstract":"Diamond is increasingly popular because of its unique material properties. Diamond defects called nitrogen vacancy (NV) centers allow measurements with unprecedented sensitivity. However, to achieve ideal sensing performance NV centers need to be within nanometers from the surface and are thus strongly dependent on the local surface chemistry. Several attempts have been made to compare diamond surfaces. However, due to the high price of diamond crystals with shallow NV centers, a limited number of chemical modifications have been studied. Here, we developed a systematic method to investigate a continuity of different local environments with a varying density and nature of surface groups in a single experiment on a single diamond plate. To achieve this goal, we used diamonds with a shallow ensemble of NV centers and introduced a chemical gradient across the surface. More specifically we used air and hydrogen plasma. The gradients were formed by low pressure plasma treatment after masking with a right-angled triangular prism shield. As a result, the surface contained gradually more oxygen/hydrogen towards the open end of the shield. We then performed widefield relaxometry to determine the effect of surface chemistry on the sensing performance. As expected, relaxation times and thus sensing performance indeed varies along the gradient.","sentences":["Diamond is increasingly popular because of its unique material properties.","Diamond defects called nitrogen vacancy (NV) centers allow measurements with unprecedented sensitivity.","However, to achieve ideal sensing performance NV centers need to be within nanometers from the surface and are thus strongly dependent on the local surface chemistry.","Several attempts have been made to compare diamond surfaces.","However, due to the high price of diamond crystals with shallow NV centers, a limited number of chemical modifications have been studied.","Here, we developed a systematic method to investigate a continuity of different local environments with a varying density and nature of surface groups in a single experiment on a single diamond plate.","To achieve this goal, we used diamonds with a shallow ensemble of NV centers and introduced a chemical gradient across the surface.","More specifically we used air and hydrogen plasma.","The gradients were formed by low pressure plasma treatment after masking with a right-angled triangular prism shield.","As a result, the surface contained gradually more oxygen/hydrogen towards the open end of the shield.","We then performed widefield relaxometry to determine the effect of surface chemistry on the sensing performance.","As expected, relaxation times and thus sensing performance indeed varies along the gradient."],"url":"http://arxiv.org/abs/2404.11961v1","category":"physics.chem-ph"}
{"created":"2024-04-18 07:15:15","title":"Tailoring Fault-Tolerance to Quantum Algorithms","abstract":"The standard approach to universal fault-tolerant quantum computing is to develop a general purpose quantum error correction mechanism that can implement a universal set of logical gates fault-tolerantly. Given such a scheme, any quantum algorithm can be realized fault-tolerantly by composing the relevant logical gates from this set. However, we know that quantum computers provide a significant quantum advantage only for specific quantum algorithms. Hence, a universal quantum computer can likely gain from compiling such specific algorithms using tailored quantum error correction schemes. In this work, we take the first steps towards such algorithm-tailored quantum fault-tolerance. We consider Trotter circuits in quantum simulation, which is an important application of quantum computing. We develop a solve-and-stitch algorithm to systematically synthesize physical realizations of Clifford Trotter circuits on the well-known $[\\![ n,n-2,2 ]\\!]$ error-detecting code family. Our analysis shows that this family implements Trotter circuits with optimal depth, thereby serving as an illuminating example of tailored quantum error correction. We achieve fault-tolerance for these circuits using flag gadgets, which add minimal overhead. The solve-and-stitch algorithm has the potential to scale beyond this specific example and hence provide a principled approach to tailored fault-tolerance in quantum computing.","sentences":["The standard approach to universal fault-tolerant quantum computing is to develop a general purpose quantum error correction mechanism that can implement a universal set of logical gates fault-tolerantly.","Given such a scheme, any quantum algorithm can be realized fault-tolerantly by composing the relevant logical gates from this set.","However, we know that quantum computers provide a significant quantum advantage only for specific quantum algorithms.","Hence, a universal quantum computer can likely gain from compiling such specific algorithms using tailored quantum error correction schemes.","In this work, we take the first steps towards such algorithm-tailored quantum fault-tolerance.","We consider Trotter circuits in quantum simulation, which is an important application of quantum computing.","We develop a solve-and-stitch algorithm to systematically synthesize physical realizations of Clifford Trotter circuits on the well-known $[\\!","[ n,n-2,2 ]\\!]$ error-detecting code family.","Our analysis shows that this family implements Trotter circuits with optimal depth, thereby serving as an illuminating example of tailored quantum error correction.","We achieve fault-tolerance for these circuits using flag gadgets, which add minimal overhead.","The solve-and-stitch algorithm has the potential to scale beyond this specific example and hence provide a principled approach to tailored fault-tolerance in quantum computing."],"url":"http://arxiv.org/abs/2404.11953v1","category":"quant-ph"}
{"created":"2024-04-18 06:38:35","title":"Solid and hollow plasmonic nanoresonators for carrier envelope phase read-out","abstract":"The geometry of various plasmonic nanoantennae was numerically optimized to maximize their sensitivity to the carrier envelope phase (CEP) of the exciting ultra-short laser pulses. To verify the CEP sensitivity, the near-field response of the investigated nanoantennae was analyzed by combining frequency and time-domain numerical computations. The simulation methodology, capable of accurately calculating the time-dependent photocurrent stem from optical field emission, enabled the determination and optimization of all key parameters characterizing and governing the CEP dependence of the near-field responses. Three-types of structures were inspected, including individual triangular and teardrop-shaped nanoantennae and plasmonic lenses consisting of three hemispheres with gradually decreasing diameters. All structure types were optimized in solid and hollow compositions of gold nanoresonators as well. It was shown that hollow /solid singlets produce the largest /intermediate CEP dependent - to - CEP independent integrated current components' ratio, while their absolute CEP dependent integrated currents were the smallest /intermediate among the optimized structures. The highest /intermediate CEP sensitivity was achieved via solid plasmonic lenses due to their very large absolute CEP dependent integrated photocurrent originating from huge near-field enhancement in the nanogaps, while the integrated current components' ratio was smaller than for counterpart singlets.","sentences":["The geometry of various plasmonic nanoantennae was numerically optimized to maximize their sensitivity to the carrier envelope phase (CEP) of the exciting ultra-short laser pulses.","To verify the CEP sensitivity, the near-field response of the investigated nanoantennae was analyzed by combining frequency and time-domain numerical computations.","The simulation methodology, capable of accurately calculating the time-dependent photocurrent stem from optical field emission, enabled the determination and optimization of all key parameters characterizing and governing the CEP dependence of the near-field responses.","Three-types of structures were inspected, including individual triangular and teardrop-shaped nanoantennae and plasmonic lenses consisting of three hemispheres with gradually decreasing diameters.","All structure types were optimized in solid and hollow compositions of gold nanoresonators as well.","It was shown that hollow /solid singlets produce the largest /intermediate CEP dependent - to - CEP independent integrated current components' ratio, while their absolute CEP dependent integrated currents were the smallest /intermediate among the optimized structures.","The highest /intermediate CEP sensitivity was achieved via solid plasmonic lenses due to their very large absolute CEP dependent integrated photocurrent originating from huge near-field enhancement in the nanogaps, while the integrated current components' ratio was smaller than for counterpart singlets."],"url":"http://arxiv.org/abs/2404.11940v1","category":"physics.optics"}
{"created":"2024-04-18 04:12:32","title":"A Distributions-based Approach for Data-Consistent Inversion","abstract":"We formulate a novel approach to solve a class of stochastic problems, referred to as data-consistent inverse (DCI) problems, which involve the characterization of a probability measure on the parameters of a computational model whose subsequent push-forward matches an observed probability measure on specified quantities of interest (QoI) typically associated with the outputs from the computational model. Whereas prior DCI solution methodologies focused on either constructing non-parametric estimates of the densities or the probabilities of events associated with the pre-image of the QoI map, we develop and analyze a constrained quadratic optimization approach based on estimating push-forward measures using weighted empirical distribution functions. The method proposed here is more suitable for low-data regimes or high-dimensional problems than the density-based method, as well as for problems where the probability measure does not admit a density. Numerical examples are included to demonstrate the performance of the method and to compare with the density-based approach where applicable.","sentences":["We formulate a novel approach to solve a class of stochastic problems, referred to as data-consistent inverse (DCI) problems, which involve the characterization of a probability measure on the parameters of a computational model whose subsequent push-forward matches an observed probability measure on specified quantities of interest (QoI) typically associated with the outputs from the computational model.","Whereas prior DCI solution methodologies focused on either constructing non-parametric estimates of the densities or the probabilities of events associated with the pre-image of the QoI map, we develop and analyze a constrained quadratic optimization approach based on estimating push-forward measures using weighted empirical distribution functions.","The method proposed here is more suitable for low-data regimes or high-dimensional problems than the density-based method, as well as for problems where the probability measure does not admit a density.","Numerical examples are included to demonstrate the performance of the method and to compare with the density-based approach where applicable."],"url":"http://arxiv.org/abs/2404.11886v1","category":"math.NA"}
{"created":"2024-04-18 03:46:51","title":"Joint Transmitter and Receiver Design for Movable Antenna Enhanced Multicast Communications","abstract":"Movable antenna (MA) is an emerging technology that utilizes localized antenna movement to pursue better channel conditions for enhancing communication performance. In this paper, we study the MA-enhanced multicast transmission from a base station equipped with multiple MAs to multiple groups of single-MA users. Our goal is to maximize the minimum weighted signal-to-interference-plus-noise ratio (SINR) among all the users by jointly optimizing the position of each transmit/receive MA and the transmit beamforming. To tackle this challenging problem, we first consider the single-group scenario and propose an efficient algorithm based on the techniques of alternating optimization and successive convex approximation. Particularly, when optimizing transmit or receive MA positions, we construct a concave lower bound for the signal-to-noise ratio (SNR) of each user by applying only the second-order Taylor expansion, which is more effective than existing works utilizing two-step approximations. The proposed design is then extended to the general multi-group scenario. Simulation results demonstrate that significant performance gains in terms of achievable max-min SNR/SINR can be obtained by our proposed algorithm over benchmark schemes. Additionally, the proposed algorithm can notably reduce the required amount of transmit power or antennas for achieving a target level of max-min SNR/SINR performance compared to benchmark schemes.","sentences":["Movable antenna (MA) is an emerging technology that utilizes localized antenna movement to pursue better channel conditions for enhancing communication performance.","In this paper, we study the MA-enhanced multicast transmission from a base station equipped with multiple MAs to multiple groups of single-MA users.","Our goal is to maximize the minimum weighted signal-to-interference-plus-noise ratio (SINR) among all the users by jointly optimizing the position of each transmit/receive MA and the transmit beamforming.","To tackle this challenging problem, we first consider the single-group scenario and propose an efficient algorithm based on the techniques of alternating optimization and successive convex approximation.","Particularly, when optimizing transmit or receive MA positions, we construct a concave lower bound for the signal-to-noise ratio (SNR) of each user by applying only the second-order Taylor expansion, which is more effective than existing works utilizing two-step approximations.","The proposed design is then extended to the general multi-group scenario.","Simulation results demonstrate that significant performance gains in terms of achievable max-min SNR/SINR can be obtained by our proposed algorithm over benchmark schemes.","Additionally, the proposed algorithm can notably reduce the required amount of transmit power or antennas for achieving a target level of max-min SNR/SINR performance compared to benchmark schemes."],"url":"http://arxiv.org/abs/2404.11881v1","category":"cs.IT"}
{"created":"2024-04-18 02:40:31","title":"Progressive Multi-modal Conditional Prompt Tuning","abstract":"Pre-trained vision-language models (VLMs) have shown remarkable generalization capabilities via prompting, which leverages VLMs as knowledge bases to extract information beneficial for downstream tasks. However, existing methods primarily employ uni-modal prompting, which only engages a uni-modal branch, failing to simultaneously adjust vision-language (V-L) features. Additionally, the one-pass forward pipeline in VLM encoding struggles to align V-L features that have a huge gap. Confronting these challenges, we propose a novel method, Progressive Multi-modal conditional Prompt Tuning (ProMPT). ProMPT exploits a recurrent structure, optimizing and aligning V-L features by iteratively utilizing image and current encoding information. It comprises an initialization and a multi-modal iterative evolution (MIE) module. Initialization is responsible for encoding image and text using a VLM, followed by a feature filter that selects text features similar to image. MIE then facilitates multi-modal prompting through class-conditional vision prompting, instance-conditional text prompting, and feature filtering. In each MIE iteration, vision prompts are obtained from the filtered text features via a vision generator, promoting image features to focus more on target object during vision prompting. The encoded image features are fed into a text generator to produce text prompts that are more robust to class shift. Thus, V-L features are progressively aligned, enabling advance from coarse to exact classifications. Extensive experiments are conducted in three settings to evaluate the efficacy of ProMPT. The results indicate that ProMPT outperforms existing methods on average across all settings, demonstrating its superior generalization.","sentences":["Pre-trained vision-language models (VLMs) have shown remarkable generalization capabilities via prompting, which leverages VLMs as knowledge bases to extract information beneficial for downstream tasks.","However, existing methods primarily employ uni-modal prompting, which only engages a uni-modal branch, failing to simultaneously adjust vision-language (V-L) features.","Additionally, the one-pass forward pipeline in VLM encoding struggles to align V-L features that have a huge gap.","Confronting these challenges, we propose a novel method, Progressive Multi-modal conditional Prompt Tuning (ProMPT).","ProMPT exploits a recurrent structure, optimizing and aligning V-L features by iteratively utilizing image and current encoding information.","It comprises an initialization and a multi-modal iterative evolution (MIE) module.","Initialization is responsible for encoding image and text using a VLM, followed by a feature filter that selects text features similar to image.","MIE then facilitates multi-modal prompting through class-conditional vision prompting, instance-conditional text prompting, and feature filtering.","In each MIE iteration, vision prompts are obtained from the filtered text features via a vision generator, promoting image features to focus more on target object during vision prompting.","The encoded image features are fed into a text generator to produce text prompts that are more robust to class shift.","Thus, V-L features are progressively aligned, enabling advance from coarse to exact classifications.","Extensive experiments are conducted in three settings to evaluate the efficacy of ProMPT.","The results indicate that ProMPT outperforms existing methods on average across all settings, demonstrating its superior generalization."],"url":"http://arxiv.org/abs/2404.11864v1","category":"cs.CV"}
{"created":"2024-04-18 02:32:00","title":"Active robustness against the detuning-error for Rydberg quantum gates","abstract":"Error suppression to the experimental imperfections is a central challenge for useful quantum computing. Recent studies have shown the advantages of using single-modulated pulses based on optimal control which can realize high-fidelity two-qubit gates in neutral-atom arrays. However, typical optimization only minimizes the ideal gate error in the absence of any decay, which allows the gate to be passively influenced by all error sources leading to an exponential increase of sensitivity when the error becomes larger. In the present work, we propose the realization of two-qubit CZ gates with active robustness against two-photon detuning errors. Our method depends on a modified cost function in numerical optimization for shaping gate pulses, which can minimize, not only the ideal gate error but also the fluctuations of gate infidelity over a wide error range. We introduce a family of Rydberg blockade gates with active robustness towards the impacts of versatile noise sources such as Doppler dephasing and ac Stark shifts. The resulting gates with robust pulses can significantly increase the insensitivity to any type of errors acting on the two-photon detuning, benefiting from a relaxed requirement of colder atomic temperatures or more stable lasers for current experimental technology.","sentences":["Error suppression to the experimental imperfections is a central challenge for useful quantum computing.","Recent studies have shown the advantages of using single-modulated pulses based on optimal control which can realize high-fidelity two-qubit gates in neutral-atom arrays.","However, typical optimization only minimizes the ideal gate error in the absence of any decay, which allows the gate to be passively influenced by all error sources leading to an exponential increase of sensitivity when the error becomes larger.","In the present work, we propose the realization of two-qubit CZ gates with active robustness against two-photon detuning errors.","Our method depends on a modified cost function in numerical optimization for shaping gate pulses, which can minimize, not only the ideal gate error but also the fluctuations of gate infidelity over a wide error range.","We introduce a family of Rydberg blockade gates with active robustness towards the impacts of versatile noise sources such as Doppler dephasing and ac Stark shifts.","The resulting gates with robust pulses can significantly increase the insensitivity to any type of errors acting on the two-photon detuning, benefiting from a relaxed requirement of colder atomic temperatures or more stable lasers for current experimental technology."],"url":"http://arxiv.org/abs/2404.11860v1","category":"quant-ph"}
{"created":"2024-04-18 02:15:24","title":"Oracle-Augmented Prophet Inequalities","abstract":"In the classical prophet inequality settings, a gambler is given a sequence of $n$ random variables $X_1, \\dots, X_n$, taken from known distributions, observes their values in this (potentially adversarial) order, and select one of them, immediately after it is being observed, so that its value is as high as possible. The classical \\emph{prophet inequality} shows a strategy that guarantees a value at least half of that an omniscience prophet that picks the maximum, and this ratio is optimal.   Here, we generalize the prophet inequality, allowing the gambler some additional information about the future that is otherwise privy only to the prophet. Specifically, at any point in the process, the gambler is allowed to query an oracle $\\mathcal{O}$. The oracle responds with a single bit answer: YES if the current realization is greater than the remaining realizations, and NO otherwise. We show that the oracle model with $m$ oracle calls is equivalent to the \\textsc{Top-$1$-of-$(m+1)$} model when the objective is maximizing the probability of selecting the maximum. This equivalence fails to hold when the objective is maximizing the competitive ratio, but we still show that any algorithm for the oracle model implies an equivalent competitive ratio for the \\textsc{Top-$1$-of-$(m+1)$} model.   We resolve the oracle model for any $m$, giving tight lower and upper bound on the best possible competitive ratio compared to an almighty adversary. As a consequence, we provide new results as well as improvements on known results for the \\textsc{Top-$1$-of-$m$} model.","sentences":["In the classical prophet inequality settings, a gambler is given a sequence of $n$ random variables $X_1, \\dots, X_n$, taken from known distributions, observes their values in this (potentially adversarial) order, and select one of them, immediately after it is being observed, so that its value is as high as possible.","The classical \\emph{prophet inequality} shows a strategy that guarantees a value at least half of that an omniscience prophet that picks the maximum, and this ratio is optimal.   ","Here, we generalize the prophet inequality, allowing the gambler some additional information about the future that is otherwise privy only to the prophet.","Specifically, at any point in the process, the gambler is allowed to query an oracle $\\mathcal{O}$. The oracle responds with a single bit answer: YES if the current realization is greater than the remaining realizations, and NO otherwise.","We show that the oracle model with $m$ oracle calls is equivalent to the \\textsc{Top-$1$-of-$(m+1)$} model when the objective is maximizing the probability of selecting the maximum.","This equivalence fails to hold when the objective is maximizing the competitive ratio, but we still show that any algorithm for the oracle model implies an equivalent competitive ratio for the \\textsc{Top-$1$-of-$(m+1)$} model.   ","We resolve the oracle model for any $m$, giving tight lower and upper bound on the best possible competitive ratio compared to an almighty adversary.","As a consequence, we provide new results as well as improvements on known results for the \\textsc{Top-$1$-of-$m$} model."],"url":"http://arxiv.org/abs/2404.11853v1","category":"cs.GT"}
{"created":"2024-04-18 00:35:40","title":"Automated Similarity Metric Generation for Recommendation","abstract":"The embedding-based architecture has become the dominant approach in modern recommender systems, mapping users and items into a compact vector space. It then employs predefined similarity metrics, such as the inner product, to calculate similarity scores between user and item embeddings, thereby guiding the recommendation of items that align closely with a user's preferences. Given the critical role of similarity metrics in recommender systems, existing methods mainly employ handcrafted similarity metrics to capture the complex characteristics of user-item interactions. Yet, handcrafted metrics may not fully capture the diverse range of similarity patterns that can significantly vary across different domains.   To address this issue, we propose an Automated Similarity Metric Generation method for recommendations, named AutoSMG, which can generate tailored similarity metrics for various domains and datasets. Specifically, we first construct a similarity metric space by sampling from a set of basic embedding operators, which are then integrated into computational graphs to represent metrics. We employ an evolutionary algorithm to search for the optimal metrics within this metric space iteratively. To improve search efficiency, we utilize an early stopping strategy and a surrogate model to approximate the performance of candidate metrics instead of fully training models. Notably, our proposed method is model-agnostic, which can seamlessly plugin into different recommendation model architectures. The proposed method is validated on three public recommendation datasets across various domains in the Top-K recommendation task, and experimental results demonstrate that AutoSMG outperforms both commonly used handcrafted metrics and those generated by other search strategies.","sentences":["The embedding-based architecture has become the dominant approach in modern recommender systems, mapping users and items into a compact vector space.","It then employs predefined similarity metrics, such as the inner product, to calculate similarity scores between user and item embeddings, thereby guiding the recommendation of items that align closely with a user's preferences.","Given the critical role of similarity metrics in recommender systems, existing methods mainly employ handcrafted similarity metrics to capture the complex characteristics of user-item interactions.","Yet, handcrafted metrics may not fully capture the diverse range of similarity patterns that can significantly vary across different domains.   ","To address this issue, we propose an Automated Similarity Metric Generation method for recommendations, named AutoSMG, which can generate tailored similarity metrics for various domains and datasets.","Specifically, we first construct a similarity metric space by sampling from a set of basic embedding operators, which are then integrated into computational graphs to represent metrics.","We employ an evolutionary algorithm to search for the optimal metrics within this metric space iteratively.","To improve search efficiency, we utilize an early stopping strategy and a surrogate model to approximate the performance of candidate metrics instead of fully training models.","Notably, our proposed method is model-agnostic, which can seamlessly plugin into different recommendation model architectures.","The proposed method is validated on three public recommendation datasets across various domains in the Top-K recommendation task, and experimental results demonstrate that AutoSMG outperforms both commonly used handcrafted metrics and those generated by other search strategies."],"url":"http://arxiv.org/abs/2404.11818v1","category":"cs.IR"}
{"created":"2024-04-17 22:57:30","title":"Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing","abstract":"The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as ``How relevant is document A to query Q?\", results in sub-optimal ranking. Instead, the pairwise ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., ``Is document A more relevant than document B to query Q?\". Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation. In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing.","sentences":["The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications.","Previous work has found that directly asking about relevancy, such as ``How relevant is document A to query Q?\", results in sub-optimal ranking.","Instead, the pairwise ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., ``Is document A more relevant than document B to query Q?\".","Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation.","In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities.","Our method takes both LLM generated relevance labels and pairwise preferences.","The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible.","Our experimental results indicate that our approach effectively balances label accuracy and ranking performance.","Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing."],"url":"http://arxiv.org/abs/2404.11791v1","category":"cs.IR"}
{"created":"2024-04-17 22:51:38","title":"Constrained Stochastic Recursive Momentum Successive Convex Approximation","abstract":"We consider stochastic optimization problems with functional constraints. If the objective and constraint functions are not convex, the classical stochastic approximation algorithms such as the proximal stochastic gradient descent do not lead to efficient algorithms. In this work, we put forth an accelerated SCA algorithm that utilizes the recursive momentum-based acceleration which is widely used in the unconstrained setting. Remarkably, the proposed algorithm also achieves the optimal SFO complexity, at par with that achieved by state-of-the-art (unconstrained) stochastic optimization algorithms and match the SFO-complexity lower bound for minimization of general smooth functions. At each iteration, the proposed algorithm entails constructing convex surrogates of the objective and the constraint functions, and solving the resulting convex optimization problem. A recursive update rule is employed to track the gradient of the objective function, and contributes to achieving faster convergence and improved SFO complexity. A key ingredient of the proof is a new parameterized version of the standard Mangasarian-Fromowitz Constraints Qualification, that allows us to bound the dual variables and hence establish that the iterates approach an $\\epsilon$-stationary point. We also detail a obstacle-avoiding trajectory optimization problem that can be solved using the proposed algorithm, and show that its performance is superior to that of the existing algorithms. The performance of the proposed algorithm is also compared against that of a specialized sparse classification algorithm on a binary classification problem.","sentences":["We consider stochastic optimization problems with functional constraints.","If the objective and constraint functions are not convex, the classical stochastic approximation algorithms such as the proximal stochastic gradient descent do not lead to efficient algorithms.","In this work, we put forth an accelerated SCA algorithm that utilizes the recursive momentum-based acceleration which is widely used in the unconstrained setting.","Remarkably, the proposed algorithm also achieves the optimal SFO complexity, at par with that achieved by state-of-the-art (unconstrained) stochastic optimization algorithms and match the SFO-complexity lower bound for minimization of general smooth functions.","At each iteration, the proposed algorithm entails constructing convex surrogates of the objective and the constraint functions, and solving the resulting convex optimization problem.","A recursive update rule is employed to track the gradient of the objective function, and contributes to achieving faster convergence and improved SFO complexity.","A key ingredient of the proof is a new parameterized version of the standard Mangasarian-Fromowitz Constraints Qualification, that allows us to bound the dual variables and hence establish that the iterates approach an $\\epsilon$-stationary point.","We also detail a obstacle-avoiding trajectory optimization problem that can be solved using the proposed algorithm, and show that its performance is superior to that of the existing algorithms.","The performance of the proposed algorithm is also compared against that of a specialized sparse classification algorithm on a binary classification problem."],"url":"http://arxiv.org/abs/2404.11790v1","category":"math.OC"}
{"created":"2024-04-17 18:01:57","title":"Optimized measurement-free and fault-tolerant quantum error correction for neutral atoms","abstract":"A major challenge in performing quantum error correction (QEC) is implementing reliable measurements and conditional feed-forward operations. In quantum computing platforms supporting unconditional qubit resets, or a constant supply of fresh qubits, alternative schemes which do not require measurements are possible. In such schemes, the error correction is realized via crafted coherent quantum feedback. We propose implementations of small measurement-free QEC schemes, which are fault-tolerant to circuit-level noise. These implementations are guided by several heuristics to achieve fault-tolerance: redundant syndrome information is extracted, and additional single-shot flag qubits are used. By carefully designing the circuit, the additional overhead of these measurement-free schemes is moderate compared to their conventional measurement-and-feed-forward counterparts. We highlight how this alternative approach paves the way towards implementing resource-efficient measurement-free QEC on neutral-atom arrays.","sentences":["A major challenge in performing quantum error correction (QEC) is implementing reliable measurements and conditional feed-forward operations.","In quantum computing platforms supporting unconditional qubit resets, or a constant supply of fresh qubits, alternative schemes which do not require measurements are possible.","In such schemes, the error correction is realized via crafted coherent quantum feedback.","We propose implementations of small measurement-free QEC schemes, which are fault-tolerant to circuit-level noise.","These implementations are guided by several heuristics to achieve fault-tolerance: redundant syndrome information is extracted, and additional single-shot flag qubits are used.","By carefully designing the circuit, the additional overhead of these measurement-free schemes is moderate compared to their conventional measurement-and-feed-forward counterparts.","We highlight how this alternative approach paves the way towards implementing resource-efficient measurement-free QEC on neutral-atom arrays."],"url":"http://arxiv.org/abs/2404.11663v1","category":"quant-ph"}
{"created":"2024-04-17 16:39:30","title":"Reduction of dust radial drift by turbulence in protoplanetary disks","abstract":"Dust particles in protoplanetary disks, lacking support from pressure, rotate at velocities exceeding those of the surrounding gas. Consequently, they experience a head-wind from the gas that drives them toward the central star. Radial drift occurs on timescales much shorter than those inferred from disk observations or those required for dust to aggregate and form planets. Additionally, turbulence is often assumed to amplify the radial drift of dust in planet-forming disks when modeled through an effective viscous transport. However, the local interactions between turbulent eddies and particles are known to be significantly more intricate than in a viscous fluid. Our objective is to elucidate and characterize the dynamic effects of Keplerian turbulence on the mean radial and azimuthal velocities of dust particles. We employ 2D shearing-box incompressible simulations of the gas, which is maintained in a developed turbulent state while rotating at a sub-Keplerian speed. Dust is modeled as Lagrangian particles set at a Keplerian velocity, therefore experiencing a radial force toward the star through drag. Turbulent eddies are found to reduce the radial drift, while simultaneously enhancing the azimuthal velocities of small particles. This dynamic behavior arises from the modification of dust trajectories due to turbulent eddies.","sentences":["Dust particles in protoplanetary disks, lacking support from pressure, rotate at velocities exceeding those of the surrounding gas.","Consequently, they experience a head-wind from the gas that drives them toward the central star.","Radial drift occurs on timescales much shorter than those inferred from disk observations or those required for dust to aggregate and form planets.","Additionally, turbulence is often assumed to amplify the radial drift of dust in planet-forming disks when modeled through an effective viscous transport.","However, the local interactions between turbulent eddies and particles are known to be significantly more intricate than in a viscous fluid.","Our objective is to elucidate and characterize the dynamic effects of Keplerian turbulence on the mean radial and azimuthal velocities of dust particles.","We employ 2D shearing-box incompressible simulations of the gas, which is maintained in a developed turbulent state while rotating at a sub-Keplerian speed.","Dust is modeled as Lagrangian particles set at a Keplerian velocity, therefore experiencing a radial force toward the star through drag.","Turbulent eddies are found to reduce the radial drift, while simultaneously enhancing the azimuthal velocities of small particles.","This dynamic behavior arises from the modification of dust trajectories due to turbulent eddies."],"url":"http://arxiv.org/abs/2404.11544v1","category":"astro-ph.EP"}
{"created":"2024-04-17 16:16:50","title":"A Comparison of Traditional and Deep Learning Methods for Parameter Estimation of the Ornstein-Uhlenbeck Process","abstract":"We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely used in finance, physics, and biology. Parameter estimation of the OU process is a challenging problem. Thus, we review traditional tracking methods and compare them with novel applications of deep learning to estimate the parameters of the OU process. We use a multi-layer perceptron to estimate the parameters of the OU process and compare its performance with traditional parameter estimation methods, such as the Kalman filter and maximum likelihood estimation. We find that the multi-layer perceptron can accurately estimate the parameters of the OU process given a large dataset of observed trajectories; however, traditional parameter estimation methods may be more suitable for smaller datasets.","sentences":["We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely used in finance, physics, and biology.","Parameter estimation of the OU process is a challenging problem.","Thus, we review traditional tracking methods and compare them with novel applications of deep learning to estimate the parameters of the OU process.","We use a multi-layer perceptron to estimate the parameters of the OU process and compare its performance with traditional parameter estimation methods, such as the Kalman filter and maximum likelihood estimation.","We find that the multi-layer perceptron can accurately estimate the parameters of the OU process given a large dataset of observed trajectories; however, traditional parameter estimation methods may be more suitable for smaller datasets."],"url":"http://arxiv.org/abs/2404.11526v1","category":"q-fin.CP"}
{"created":"2024-04-17 16:14:06","title":"Equitably allocating wildfire resilience investments for power grids: The curse of aggregation and vulnerability indices","abstract":"Wildfires ignited by power systems infrastructure are among the most destructive wildfires; hence some utility companies in wildfire-prone regions have pursued a proactive policy of emergency power shutoffs. These shutoffs, while mitigating the risk of disastrous ignition events, result in power outages that could negatively impacts vulnerable communities. In this paper, we consider how to equitably allocate funds to underground and effectively de-risk power lines in transmission networks. We explore the impact of the 2021 White House resource allocation policy called the Justice40 initiative, which states that 40% of the benefits of federally-funded climate-related investments should go to socially vulnerable communities. The definition of what constitutes a vulnerable community varies by organization, and we consider two major recently proposed vulnerability indices: the Justice40 index created under the 2021 White House and the Social Vulnerability Index (SVI) developed by the Center for Disease Control and Prevention (CDC). We show that allocating budget according to these two indices fails to reduce power outages for indigenous communities and those subject to high wildfire ignition risk using a high-fidelity synthetic power grid dataset that matches the key features of the Texas transmission system. We discuss how aggregation of communities and \"one size fits all\" vulnerability indices might be the reasons for the misalignment between the goals of vulnerability indices and their realized impact in this particular case study. We provide a method of achieving an equitable investment plan by adding group-level protections on percentage of load that is shed across each population group of interest.","sentences":["Wildfires ignited by power systems infrastructure are among the most destructive wildfires; hence some utility companies in wildfire-prone regions have pursued a proactive policy of emergency power shutoffs.","These shutoffs, while mitigating the risk of disastrous ignition events, result in power outages that could negatively impacts vulnerable communities.","In this paper, we consider how to equitably allocate funds to underground and effectively de-risk power lines in transmission networks.","We explore the impact of the 2021 White House resource allocation policy called the Justice40 initiative, which states that 40% of the benefits of federally-funded climate-related investments should go to socially vulnerable communities.","The definition of what constitutes a vulnerable community varies by organization, and we consider two major recently proposed vulnerability indices: the Justice40 index created under the 2021 White House and the Social Vulnerability Index (SVI) developed by the Center for Disease Control and Prevention (CDC).","We show that allocating budget according to these two indices fails to reduce power outages for indigenous communities and those subject to high wildfire ignition risk using a high-fidelity synthetic power grid dataset that matches the key features of the Texas transmission system.","We discuss how aggregation of communities and \"one size fits all\" vulnerability indices might be the reasons for the misalignment between the goals of vulnerability indices and their realized impact in this particular case study.","We provide a method of achieving an equitable investment plan by adding group-level protections on percentage of load that is shed across each population group of interest."],"url":"http://arxiv.org/abs/2404.11520v1","category":"math.OC"}
{"created":"2024-04-17 16:10:55","title":"Disentangled Cascaded Graph Convolution Networks for Multi-Behavior Recommendation","abstract":"Multi-behavioral recommender systems have emerged as a solution to address data sparsity and cold-start issues by incorporating auxiliary behaviors alongside target behaviors. However, existing models struggle to accurately capture varying user preferences across different behaviors and fail to account for diverse item preferences within behaviors. Various user preference factors (such as price or quality) entangled in the behavior may lead to sub-optimization problems. Furthermore, these models overlook the personalized nature of user behavioral preferences by employing uniform transformation networks for all users and items. To tackle these challenges, we propose the Disentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel multi-behavior recommendation model. Disen-CGCN employs disentangled representation techniques to effectively separate factors within user and item representations, ensuring their independence. In addition, it incorporates a multi-behavioral meta-network, enabling personalized feature transformation across user and item behaviors. Furthermore, an attention mechanism captures user preferences for different item factors within each behavior. By leveraging attention weights, we aggregate user and item embeddings separately for each behavior, computing preference scores that predict overall user preferences for items. Our evaluation on benchmark datasets demonstrates the superiority of Disen-CGCN over state-of-the-art models, showcasing an average performance improvement of 7.07% and 9.00% on respective datasets. These results highlight Disen-CGCN's ability to effectively leverage multi-behavioral data, leading to more accurate recommendations.","sentences":["Multi-behavioral recommender systems have emerged as a solution to address data sparsity and cold-start issues by incorporating auxiliary behaviors alongside target behaviors.","However, existing models struggle to accurately capture varying user preferences across different behaviors and fail to account for diverse item preferences within behaviors.","Various user preference factors (such as price or quality) entangled in the behavior may lead to sub-optimization problems.","Furthermore, these models overlook the personalized nature of user behavioral preferences by employing uniform transformation networks for all users and items.","To tackle these challenges, we propose the Disentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel multi-behavior recommendation model.","Disen-CGCN employs disentangled representation techniques to effectively separate factors within user and item representations, ensuring their independence.","In addition, it incorporates a multi-behavioral meta-network, enabling personalized feature transformation across user and item behaviors.","Furthermore, an attention mechanism captures user preferences for different item factors within each behavior.","By leveraging attention weights, we aggregate user and item embeddings separately for each behavior, computing preference scores that predict overall user preferences for items.","Our evaluation on benchmark datasets demonstrates the superiority of Disen-CGCN over state-of-the-art models, showcasing an average performance improvement of 7.07% and 9.00% on respective datasets.","These results highlight Disen-CGCN's ability to effectively leverage multi-behavioral data, leading to more accurate recommendations."],"url":"http://arxiv.org/abs/2404.11519v1","category":"cs.IR"}
{"created":"2024-04-17 16:05:03","title":"Improved bounds and inference on optimal regimes","abstract":"Point identification of causal effects requires strong assumptions that are unreasonable in many practical settings. However, informative bounds on these effects can often be derived under plausible assumptions. Even when these bounds are wide or cover null effects, they can guide practical decisions based on formal decision theoretic criteria. Here we derive new results on optimal treatment regimes in settings where the effect of interest is bounded. These results are driven by consideration of superoptimal regimes; we define regimes that leverage an individual's natural treatment value, which is typically ignored in the existing literature. We obtain (sharp) bounds for the value function of superoptimal regimes, and provide performance guarantees relative to conventional optimal regimes. As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the superoptimal regime can be identified when conventional optimal regimes are not. We similarly illustrate this property in an instrumental variable setting. Finally, we derive efficient estimators for upper and lower bounds on the superoptimal value in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds. These estimators are applied to study the effect of prompt ICU admission on survival.","sentences":["Point identification of causal effects requires strong assumptions that are unreasonable in many practical settings.","However, informative bounds on these effects can often be derived under plausible assumptions.","Even when these bounds are wide or cover null effects, they can guide practical decisions based on formal decision theoretic criteria.","Here we derive new results on optimal treatment regimes in settings where the effect of interest is bounded.","These results are driven by consideration of superoptimal regimes; we define regimes that leverage an individual's natural treatment value, which is typically ignored in the existing literature.","We obtain (sharp) bounds for the value function of superoptimal regimes, and provide performance guarantees relative to conventional optimal regimes.","As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the superoptimal regime can be identified when conventional optimal regimes are not.","We similarly illustrate this property in an instrumental variable setting.","Finally, we derive efficient estimators for upper and lower bounds on the superoptimal value in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds.","These estimators are applied to study the effect of prompt ICU admission on survival."],"url":"http://arxiv.org/abs/2404.11510v1","category":"stat.ME"}
{"created":"2024-04-17 15:59:56","title":"Sky location of Massive Black Hole Binaries in the foreground of Galactic white dwarf binaries","abstract":"Sky locating massive black hole binaries (MBHBs) from the foreground of double white dwarf (DWD) is essential for space-based gravitational wave (GW) detection. In an orbit period of the space crafts, there is an optimal orbital position of the GW detectors to observe GW sources, where the signal intensity is at its peak. From the model Q3-d, five MBHB sources are selected based on the optimal observation orbital positions of the GW detectors, which are associated with the orientation of the MBHB perpendicular to the detection arms. For two MBHB sources of lower intensity, luminosity distance uncertainties, $\\Delta D_L$/$D_L$ at the 95$\\%$ confidence level from the overlapping GW signals of MBHB and DWD sources, when employing wavelet decomposition and reconstruction methods, are improved by $\\sim$ 2 times and $\\sim$ 10 times. Besides, the angular resolutions $\\Delta \\Omega_s$ are also improved by a factor of $\\sim$ 35 and $\\sim$ 8. These results imply that we can obtain relatively high accuracy of quickly localizing MBHB from the overlapped GW signals with DWDs at the best observation orbit position. The luminosity distance uncertainties at the 95$\\%$ confidence level for MBHB sources with the higher sign-noise ratio, have constraints on the precision of the Hubble constant.","sentences":["Sky locating massive black hole binaries (MBHBs) from the foreground of double white dwarf (DWD) is essential for space-based gravitational wave (GW) detection.","In an orbit period of the space crafts, there is an optimal orbital position of the GW detectors to observe GW sources, where the signal intensity is at its peak.","From the model Q3-d, five MBHB sources are selected based on the optimal observation orbital positions of the GW detectors, which are associated with the orientation of the MBHB perpendicular to the detection arms.","For two MBHB sources of lower intensity, luminosity distance uncertainties, $\\Delta D_L$/$D_L$ at the 95$\\%$ confidence level from the overlapping GW signals of MBHB and DWD sources, when employing wavelet decomposition and reconstruction methods, are improved by $\\sim$ 2 times and $\\sim$ 10 times.","Besides, the angular resolutions $\\Delta \\Omega_s$ are also improved by a factor of $\\sim$ 35 and $\\sim$ 8.","These results imply that we can obtain relatively high accuracy of quickly localizing MBHB from the overlapped GW signals with DWDs at the best observation orbit position.","The luminosity distance uncertainties at the 95$\\%$ confidence level for MBHB sources with the higher sign-noise ratio, have constraints on the precision of the Hubble constant."],"url":"http://arxiv.org/abs/2404.11505v2","category":"astro-ph.HE"}
{"created":"2024-04-17 15:07:06","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent","abstract":"A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.","sentences":["A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions.","Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging.","In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications.","To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters.","Like GPT-4, our model can process both English and Chinese.","We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi."],"url":"http://arxiv.org/abs/2404.11459v2","category":"cs.CL"}
{"created":"2024-04-17 13:51:37","title":"A $\u03c4$-preconditioner for space fractional diffusion equation with non-separable variable coefficients","abstract":"In this paper, we study a $\\tau$-matrix approximation based preconditioner for the linear systems arising from discretization of unsteady state Riesz space fractional diffusion equation with non-separable variable coefficients. The structure of coefficient matrices of the linear systems is identity plus summation of diagonal-times-multilevel-Toeplitz matrices. In our preconditioning technique, the diagonal matrices are approximated by scalar identity matrices and the Toeplitz matrices are approximated by {\\tau}-matrices (a type of matrices diagonalizable by discrete sine transforms). The proposed preconditioner is fast invertible through the fast sine transform (FST) algorithm. Theoretically, we show that the GMRES solver for the preconditioned systems has an optimal convergence rate (a convergence rate independent of discretization stepsizes). To the best of our knowledge, this is the first preconditioning method with the optimal convergence rate for the variable-coefficients space fractional diffusion equation. Numerical results are reported to demonstrate the efficiency of the proposed method.","sentences":["In this paper, we study a $\\tau$-matrix approximation based preconditioner for the linear systems arising from discretization of unsteady state Riesz space fractional diffusion equation with non-separable variable coefficients.","The structure of coefficient matrices of the linear systems is identity plus summation of diagonal-times-multilevel-Toeplitz matrices.","In our preconditioning technique, the diagonal matrices are approximated by scalar identity matrices and the Toeplitz matrices are approximated by {\\tau}-matrices (a type of matrices diagonalizable by discrete sine transforms).","The proposed preconditioner is fast invertible through the fast sine transform (FST) algorithm.","Theoretically, we show that the GMRES solver for the preconditioned systems has an optimal convergence rate (a convergence rate independent of discretization stepsizes).","To the best of our knowledge, this is the first preconditioning method with the optimal convergence rate for the variable-coefficients space fractional diffusion equation.","Numerical results are reported to demonstrate the efficiency of the proposed method."],"url":"http://arxiv.org/abs/2404.11390v1","category":"math.NA"}
{"created":"2024-04-17 13:43:12","title":"Lower Limb Movements Recognition Based on Feature Recursive Elimination and Backpropagation Neural Network","abstract":"Surface electromyographic (sEMG) signal serve as a signal source commonly used for lower limb movement recognition, reflecting the intent of human movement. However, it has been a challenge to improve the movements recognition rate while using fewer features in this area of research area. In this paper, a method for lower limb movements recognition based on recursive feature elimination and backpropagation neural network of support vector machine is proposed. First, the sEMG signal of five subjects performing eight different lower limb movements was recorded using a BIOPAC collector. The optimal feature subset consists of 25 feature vectors, determined using a Recursive Feature Elimination based on Support Vector Machine (SVM-RFE). Finally, this study used five supervised classification algorithms to recognize these eight different lower limb movements. The results of the experimental study show that the combination of the BPNN classifier and the SVM-RFE feature selection algorithm is able to achieve an excellent action recognition accuracy of 95\\%, which provides sufficient support for the feasibility of this approach.","sentences":["Surface electromyographic (sEMG) signal serve as a signal source commonly used for lower limb movement recognition, reflecting the intent of human movement.","However, it has been a challenge to improve the movements recognition rate while using fewer features in this area of research area.","In this paper, a method for lower limb movements recognition based on recursive feature elimination and backpropagation neural network of support vector machine is proposed.","First, the sEMG signal of five subjects performing eight different lower limb movements was recorded using a BIOPAC collector.","The optimal feature subset consists of 25 feature vectors, determined using a Recursive Feature Elimination based on Support Vector Machine (SVM-RFE).","Finally, this study used five supervised classification algorithms to recognize these eight different lower limb movements.","The results of the experimental study show that the combination of the BPNN classifier and the SVM-RFE feature selection algorithm is able to achieve an excellent action recognition accuracy of 95\\%, which provides sufficient support for the feasibility of this approach."],"url":"http://arxiv.org/abs/2404.11383v1","category":"eess.SP"}
{"created":"2024-04-17 13:39:51","title":"Convergence of Policy Gradient for Stochastic Linear-Quadratic Control Problem in Infinite Horizon","abstract":"With the outstanding performance of policy gradient (PG) method in the reinforcement learning field, the convergence theory of it has aroused more and more interest recently. Meanwhile, the significant importance and abundant theoretical researches make the stochastic linear quadratic (SLQ) control problem a starting point for studying PG in model-based learning setting. In this paper, we study the PG method for the SLQ problem in infinite horizon and take a step towards providing rigorous guarantees for gradient methods. Although the cost functional of linear-quadratic problem is typically nonconvex, we still overcome the difficulty based on gradient domination condition and L-smoothness property, and prove exponential/linear convergence of gradient flow/descent algorithm.","sentences":["With the outstanding performance of policy gradient (PG) method in the reinforcement learning field, the convergence theory of it has aroused more and more interest recently.","Meanwhile, the significant importance and abundant theoretical researches make the stochastic linear quadratic (SLQ) control problem a starting point for studying PG in model-based learning setting.","In this paper, we study the PG method for the SLQ problem in infinite horizon and take a step towards providing rigorous guarantees for gradient methods.","Although the cost functional of linear-quadratic problem is typically nonconvex, we still overcome the difficulty based on gradient domination condition and L-smoothness property, and prove exponential/linear convergence of gradient flow/descent algorithm."],"url":"http://arxiv.org/abs/2404.11382v2","category":"math.OC"}
{"created":"2024-04-17 13:33:53","title":"A New Algorithm With Lower Complexity for Bilevel Optimization","abstract":"Many stochastic algorithms have been proposed to solve the bilevel optimization problem, where the lower level function is strongly convex and the upper level value function is nonconvex. In particular, exising Hessian inverse-free algorithms that utilize momentum recursion or variance reduction technqiues can reach an $\\epsilon$-stationary point with a complexity of $\\tilde{O}(\\epsilon^{-1.5})$ under usual smoothness conditions. However, $\\tilde{O}(\\epsilon^{-1.5})$ is a complexity higher than $O(\\epsilon^{-1.5})$. How to make a Hessian inverse-free algorithm achieve the complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions remains an unresolved problem. In this paper, we propose a new Hessian inverse-free algorithm based on the projected stochastic gradient descent method and variance reduction technique of SPIDER. This algorithm can achieve a complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions whether it runs in a fully single loop or double loop structure. Finally, we validate our theoretical results through synthetic experiments and demonstrate the efficiency of our algorithm in some machine learning applications.","sentences":["Many stochastic algorithms have been proposed to solve the bilevel optimization problem, where the lower level function is strongly convex and the upper level value function is nonconvex.","In particular, exising Hessian inverse-free algorithms that utilize momentum recursion or variance reduction technqiues can reach an $\\epsilon$-stationary point with a complexity of $\\tilde{O}(\\epsilon^{-1.5})$ under usual smoothness conditions.","However, $\\tilde{O}(\\epsilon^{-1.5})$ is a complexity higher than $O(\\epsilon^{-1.5})$. How to make a Hessian inverse-free algorithm achieve the complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions remains an unresolved problem.","In this paper, we propose a new Hessian inverse-free algorithm based on the projected stochastic gradient descent method and variance reduction technique of SPIDER.","This algorithm can achieve a complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions whether it runs in a fully single loop or double loop structure.","Finally, we validate our theoretical results through synthetic experiments and demonstrate the efficiency of our algorithm in some machine learning applications."],"url":"http://arxiv.org/abs/2404.11377v1","category":"math.OC"}
{"created":"2024-04-17 13:22:26","title":"Stress analysis of functionally graded hyperelastic variable thickness rotating annular thin disk: A semi-analytic approach","abstract":"Functionally graded materials (FGMs) represent a promising class of advanced materials designed with tailored microstructures to achieve optimized mechanical, thermal, and functional properties across varying gradients. The strategic integration of distinct materials within functionally graded materials offers engineers unprecedented control over properties such as strength, thermal conductivity, and corrosion resistance, enabling innovative solutions for demanding applications in aerospace, automotive, and biomedical industries. This study investigates a rotating annular thin disk with variable thickness composed of incompressible hyperelastic material, made up of functionally graded properties under large deformations. To elucidate these phenomena, a power relation is employed to delineate the changes in cross-sectional geometry m, the material property n, and the angular velocity w of hyperelastic material. Constants used for hyperelastic material are obtained from the experimental data. Equations are solved semi-analytically for different values of m, n, and w, and the values of radial stresses, tangential stresses, and elongation are calculated and compared for different conditions. Results show that thickness and FG properties have a significant impact on the behavior of disk, so that the expected behavior of the disk can be obtained by an optimal selection of the disks geometry and material properties. By selecting the optimum values for these variables, the location of maximum stress can be controlled in large deformations, thereby furnishing significance advantages in structural design and material selection.","sentences":["Functionally graded materials (FGMs) represent a promising class of advanced materials designed with tailored microstructures to achieve optimized mechanical, thermal, and functional properties across varying gradients.","The strategic integration of distinct materials within functionally graded materials offers engineers unprecedented control over properties such as strength, thermal conductivity, and corrosion resistance, enabling innovative solutions for demanding applications in aerospace, automotive, and biomedical industries.","This study investigates a rotating annular thin disk with variable thickness composed of incompressible hyperelastic material, made up of functionally graded properties under large deformations.","To elucidate these phenomena, a power relation is employed to delineate the changes in cross-sectional geometry m, the material property n, and the angular velocity w of hyperelastic material.","Constants used for hyperelastic material are obtained from the experimental data.","Equations are solved semi-analytically for different values of m, n, and w, and the values of radial stresses, tangential stresses, and elongation are calculated and compared for different conditions.","Results show that thickness and FG properties have a significant impact on the behavior of disk, so that the expected behavior of the disk can be obtained by an optimal selection of the disks geometry and material properties.","By selecting the optimum values for these variables, the location of maximum stress can be controlled in large deformations, thereby furnishing significance advantages in structural design and material selection."],"url":"http://arxiv.org/abs/2404.11365v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 13:20:19","title":"A methodology of quantifying membrane permeability based on returning probability theory and molecular dynamics simulation","abstract":"We propose a theoretical approach to estimate the permeability coefficient of substrates (permeants) for crossing membranes from donor (D) phase to acceptor (A) phase by means of molecular dynamics (MD) simulation. A fundamental aspect of our approach involves reformulating the returning probability (RP) theory, a rigorous bimolecular reaction theory, to describe permeation phenomena. This reformulation relies on the parallelism between permeation and bimolecular reaction processes. In the present method, the permeability coefficient is represented in terms of the thermodynamic and kinetic quantities for the reactive (R) phase that exists within the inner region of membranes. One can evaluate these quantities using multiple MD trajectories starting from phase R. We apply the RP theory to the permeation of ethanol and methylamine at different concentrations (infinitely dilute and 1 mol% conditions of permeants). Under the 1 mol% condition, the present method yields a larger permeability coefficient for ethanol ($0.12 \\pm 0.01 ~\\mathrm{cm~s^{-1}}$) than for methylamine ($0.069\\pm 0.006~\\mathrm{cm~s^{-1}}$), while the values of the permeability coefficient are satisfactorily close to those obtained from the brute-force MD simulations [$0.18\\pm 0.03 ~\\mathrm{cm~s^{-1}}$ and $0.052 \\pm 0.005 ~\\mathrm{cm~s^{-1}}$ for ethanol and methylamine, respectively]. Moreover, upon analyzing the thermodynamic and kinetic contributions to the permeability, we clarify that a higher concentration dependency of permeability for ethanol, as compared to methylamine, arises from the sensitive nature of ethanol's free-energy barrier within the inner region of the membrane against ethanol concentration.","sentences":["We propose a theoretical approach to estimate the permeability coefficient of substrates (permeants) for crossing membranes from donor (D) phase to acceptor (A) phase by means of molecular dynamics (MD) simulation.","A fundamental aspect of our approach involves reformulating the returning probability (RP) theory, a rigorous bimolecular reaction theory, to describe permeation phenomena.","This reformulation relies on the parallelism between permeation and bimolecular reaction processes.","In the present method, the permeability coefficient is represented in terms of the thermodynamic and kinetic quantities for the reactive (R) phase that exists within the inner region of membranes.","One can evaluate these quantities using multiple MD trajectories starting from phase R. We apply the RP theory to the permeation of ethanol and methylamine at different concentrations (infinitely dilute and 1 mol% conditions of permeants).","Under the 1 mol% condition, the present method yields a larger permeability coefficient for ethanol ($0.12 \\pm 0.01 ~\\mathrm{cm~s^{-1}}$) than for methylamine ($0.069\\pm 0.006~\\mathrm{cm~s^{-1}}$), while the values of the permeability coefficient are satisfactorily close to those obtained from the brute-force MD simulations [$0.18\\pm 0.03 ~\\mathrm{cm~s^{-1}}$ and $0.052 \\pm 0.005 ~\\mathrm{cm~s^{-1}}$ for ethanol and methylamine, respectively].","Moreover, upon analyzing the thermodynamic and kinetic contributions to the permeability, we clarify that a higher concentration dependency of permeability for ethanol, as compared to methylamine, arises from the sensitive nature of ethanol's free-energy barrier within the inner region of the membrane against ethanol concentration."],"url":"http://arxiv.org/abs/2404.11363v1","category":"cond-mat.soft"}
{"created":"2024-04-17 13:00:05","title":"Best Practices for a Handwritten Text Recognition System","abstract":"Handwritten text recognition has been developed rapidly in the recent years, following the rise of deep learning and its applications. Though deep learning methods provide notable boost in performance concerning text recognition, non-trivial deviation in performance can be detected even when small pre-processing or architectural/optimization elements are changed. This work follows a ``best practice'' rationale; highlight simple yet effective empirical practices that can further help training and provide well-performing handwritten text recognition systems. Specifically, we considered three basic aspects of a deep HTR system and we proposed simple yet effective solutions: 1) retain the aspect ratio of the images in the preprocessing step, 2) use max-pooling for converting the 3D feature map of CNN output into a sequence of features and 3) assist the training procedure via an additional CTC loss which acts as a shortcut on the max-pooled sequential features. Using these proposed simple modifications, one can attain close to state-of-the-art results, while considering a basic convolutional-recurrent (CNN+LSTM) architecture, for both IAM and RIMES datasets. Code is available at https://github.com/georgeretsi/HTR-best-practices/.","sentences":["Handwritten text recognition has been developed rapidly in the recent years, following the rise of deep learning and its applications.","Though deep learning methods provide notable boost in performance concerning text recognition, non-trivial deviation in performance can be detected even when small pre-processing or architectural/optimization elements are changed.","This work follows a ``best practice'' rationale; highlight simple yet effective empirical practices that can further help training and provide well-performing handwritten text recognition systems.","Specifically, we considered three basic aspects of a deep HTR system and we proposed simple yet effective solutions: 1) retain the aspect ratio of the images in the preprocessing step, 2) use max-pooling for converting the 3D feature map of CNN output into a sequence of features and 3) assist the training procedure via an additional CTC loss which acts as a shortcut on the max-pooled sequential features.","Using these proposed simple modifications, one can attain close to state-of-the-art results, while considering a basic convolutional-recurrent (CNN+LSTM) architecture, for both IAM and RIMES datasets.","Code is available at https://github.com/georgeretsi/HTR-best-practices/."],"url":"http://arxiv.org/abs/2404.11339v1","category":"cs.CV"}
{"created":"2024-04-17 12:00:09","title":"LogSD: Detecting Anomalies from System Logs through Self-supervised Learning and Frequency-based Masking","abstract":"Log analysis is one of the main techniques that engineers use for troubleshooting large-scale software systems. Over the years, many supervised, semi-supervised, and unsupervised log analysis methods have been proposed to detect system anomalies by analyzing system logs. Among these, semi-supervised methods have garnered increasing attention as they strike a balance between relaxed labeled data requirements and optimal detection performance, contrasting with their supervised and unsupervised counterparts. However, existing semi-supervised methods overlook the potential bias introduced by highly frequent log messages on the learned normal patterns, which leads to their less than satisfactory performance. In this study, we propose LogSD, a novel semi-supervised self-supervised learning approach. LogSD employs a dual-network architecture and incorporates a frequency-based masking scheme, a global-to-local reconstruction paradigm and three self-supervised learning tasks. These features enable LogSD to focus more on relatively infrequent log messages, thereby effectively learning less biased and more discriminative patterns from historical normal data. This emphasis ultimately leads to improved anomaly detection performance. Extensive experiments have been conducted on three commonly-used datasets and the results show that LogSD significantly outperforms eight state-of-the-art benchmark methods.","sentences":["Log analysis is one of the main techniques that engineers use for troubleshooting large-scale software systems.","Over the years, many supervised, semi-supervised, and unsupervised log analysis methods have been proposed to detect system anomalies by analyzing system logs.","Among these, semi-supervised methods have garnered increasing attention as they strike a balance between relaxed labeled data requirements and optimal detection performance, contrasting with their supervised and unsupervised counterparts.","However, existing semi-supervised methods overlook the potential bias introduced by highly frequent log messages on the learned normal patterns, which leads to their less than satisfactory performance.","In this study, we propose LogSD, a novel semi-supervised self-supervised learning approach.","LogSD employs a dual-network architecture and incorporates a frequency-based masking scheme, a global-to-local reconstruction paradigm and three self-supervised learning tasks.","These features enable LogSD to focus more on relatively infrequent log messages, thereby effectively learning less biased and more discriminative patterns from historical normal data.","This emphasis ultimately leads to improved anomaly detection performance.","Extensive experiments have been conducted on three commonly-used datasets and the results show that LogSD significantly outperforms eight state-of-the-art benchmark methods."],"url":"http://arxiv.org/abs/2404.11294v1","category":"cs.SE"}
{"created":"2024-04-17 11:22:28","title":"Towards low-temperature processing of efficient $\u03b3$-CsPbI$_3$ perovskite solar cells","abstract":"Inorganic cesium lead iodide (CsPbI$_3$) perovskite solar cells (PSCs) have attracted enormous attention due to their excellent thermal stability and optical bandgap (~1.73 eV), well-suited for tandem device applications. However, achieving high-performing photovoltaic devices processed at low temperatures is still challenging. Here we reported a new method to fabricate high-efficiency and stable $\\gamma$-CsPbI$_3$ PSCs at lower temperatures than was previously possible by introducing the long-chain organic cation salt ethane-1,2-diammonium iodide (EDAI2) and regulating the content of lead acetate (Pb(OAc)2) in the perovskite precursor solution. We find that EDAI2 acts as an intermediate that can promote the formation of $\\gamma$-CsPbI$_3$, while excess Pb(OAc)2 can further stabilize the $\\gamma$-phase of CsPbI$_3$ perovskite. Consequently, improved crystallinity and morphology and reduced carrier recombination are observed in the CsPbI$_3$ films fabricated by the new method. By optimizing the hole transport layer of CsPbI$_3$ inverted architecture solar cells, we demonstrate up to 16.6% efficiencies, surpassing previous reports examining $\\gamma$-CsPbI$_3$ in inverted PSCs. Notably, the encapsulated solar cells maintain 97% of their initial efficiency at room temperature and dim light for 25 days, demonstrating the synergistic effect of EDAI2 and Pb(OAc)2 on stabilizing $\\gamma$-CsPbI$_3$ PSCs.","sentences":["Inorganic cesium lead iodide (CsPbI$_3$) perovskite solar cells (PSCs) have attracted enormous attention due to their excellent thermal stability and optical bandgap (~1.73 eV), well-suited for tandem device applications.","However, achieving high-performing photovoltaic devices processed at low temperatures is still challenging.","Here we reported a new method to fabricate high-efficiency and stable $\\gamma$-CsPbI$_3$ PSCs at lower temperatures than was previously possible by introducing the long-chain organic cation salt ethane-1,2-diammonium iodide (EDAI2) and regulating the content of lead acetate (Pb(OAc)2) in the perovskite precursor solution.","We find that EDAI2 acts as an intermediate that can promote the formation of $\\gamma$-CsPbI$_3$, while excess Pb(OAc)2 can further stabilize the $\\gamma$-phase of CsPbI$_3$ perovskite.","Consequently, improved crystallinity and morphology and reduced carrier recombination are observed in the CsPbI$_3$ films fabricated by the new method.","By optimizing the hole transport layer of CsPbI$_3$ inverted architecture solar cells, we demonstrate up to 16.6% efficiencies, surpassing previous reports examining $\\gamma$-CsPbI$_3$ in inverted PSCs.","Notably, the encapsulated solar cells maintain 97% of their initial efficiency at room temperature and dim light for 25 days, demonstrating the synergistic effect of EDAI2 and Pb(OAc)2 on stabilizing $\\gamma$-CsPbI$_3$ PSCs."],"url":"http://arxiv.org/abs/2404.11636v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 09:30:41","title":"Forecasting with panel data: Estimation uncertainty versus parameter heterogeneity","abstract":"We provide a comprehensive examination of the predictive accuracy of panel forecasting methods based on individual, pooling, fixed effects, and Bayesian estimation, and propose optimal weights for forecast combination schemes. We consider linear panel data models, allowing for weakly exogenous regressors and correlated heterogeneity. We quantify the gains from exploiting panel data and demonstrate how forecasting performance depends on the degree of parameter heterogeneity, whether such heterogeneity is correlated with the regressors, the goodness of fit of the model, and the cross-sectional ($N$) and time ($T$) dimensions. Monte Carlo simulations and empirical applications to house prices and CPI inflation show that forecast combination and Bayesian forecasting methods perform best overall and rarely produce the least accurate forecasts for individual series.","sentences":["We provide a comprehensive examination of the predictive accuracy of panel forecasting methods based on individual, pooling, fixed effects, and Bayesian estimation, and propose optimal weights for forecast combination schemes.","We consider linear panel data models, allowing for weakly exogenous regressors and correlated heterogeneity.","We quantify the gains from exploiting panel data and demonstrate how forecasting performance depends on the degree of parameter heterogeneity, whether such heterogeneity is correlated with the regressors, the goodness of fit of the model, and the cross-sectional ($N$) and time ($T$) dimensions.","Monte Carlo simulations and empirical applications to house prices and CPI inflation show that forecast combination and Bayesian forecasting methods perform best overall and rarely produce the least accurate forecasts for individual series."],"url":"http://arxiv.org/abs/2404.11198v1","category":"econ.EM"}
{"created":"2024-04-17 09:25:05","title":"Simultaneous compensation of input delay and state/input quantization for linear systems via switched predictor feedback","abstract":"We develop a switched predictor-feedback law, which achieves global asymptotic stabilization of linear systems with input delay and with the plant and actuator states available only in (almost) quantized form. The control design relies on a quantized version of the nominal predictor-feedback law for linear systems, in which quantized measurements of the plant and actuator states enter the predictor state formula. A switching strategy is constructed to dynamically adjust the tunable parameter of the quantizer (in a piecewise constant manner), in order to initially increase the range and subsequently decrease the error of the quantizers. The key element in the proof of global asymptotic stability in the supremum norm of the actuator state is derivation of solutions' estimates combining a backstepping transformation with small-gain and input-to-state stability arguments, for addressing the error due to quantization. We extend this result to the input quantization case and illustrate our theory with a numerical example.","sentences":["We develop a switched predictor-feedback law, which achieves global asymptotic stabilization of linear systems with input delay and with the plant and actuator states available only in (almost) quantized form.","The control design relies on a quantized version of the nominal predictor-feedback law for linear systems, in which quantized measurements of the plant and actuator states enter the predictor state formula.","A switching strategy is constructed to dynamically adjust the tunable parameter of the quantizer (in a piecewise constant manner), in order to initially increase the range and subsequently decrease the error of the quantizers.","The key element in the proof of global asymptotic stability in the supremum norm of the actuator state is derivation of solutions' estimates combining a backstepping transformation with small-gain and input-to-state stability arguments, for addressing the error due to quantization.","We extend this result to the input quantization case and illustrate our theory with a numerical example."],"url":"http://arxiv.org/abs/2404.11194v2","category":"math.OC"}
{"created":"2024-04-17 09:04:42","title":"Approximability of the Containment Problem for Zonotopes and Ellipsotopes","abstract":"The zonotope containment problem, i.e., whether one zonotope is contained in another, is a central problem in control theory to compute invariant sets, obtain fixed points of reachable sets, detect faults, and robustify controllers. Despite the inherent co-NP-hardness of this problem, an approximation algorithm developed by S. Sadraddini and R. Tedrake has gained widespread recognition for its swift execution and consistent reliability in practical scenarios. In our study, we substantiate the precision of the algorithm with a definitive proof, elucidating the empirical accuracy observed in practice. Our proof hinges on establishing a connection between the containment problem and the computation of matrix norms, thereby enabling the extension of the approximation algorithm to encompass ellipsotopes, a broader class of sets derived from zonotopes. Moreover, we explore the computational complexity of the ellipsotope containment problem, focusing on approximability. Finally, we present new methods to calculate robust control invariant sets for linear dynamical systems, demonstrating the practical relevance of approximations to the ellipsotope containment problem.","sentences":["The zonotope containment problem, i.e., whether one zonotope is contained in another, is a central problem in control theory to compute invariant sets, obtain fixed points of reachable sets, detect faults, and robustify controllers.","Despite the inherent co-NP-hardness of this problem, an approximation algorithm developed by S. Sadraddini and R. Tedrake has gained widespread recognition for its swift execution and consistent reliability in practical scenarios.","In our study, we substantiate the precision of the algorithm with a definitive proof, elucidating the empirical accuracy observed in practice.","Our proof hinges on establishing a connection between the containment problem and the computation of matrix norms, thereby enabling the extension of the approximation algorithm to encompass ellipsotopes, a broader class of sets derived from zonotopes.","Moreover, we explore the computational complexity of the ellipsotope containment problem, focusing on approximability.","Finally, we present new methods to calculate robust control invariant sets for linear dynamical systems, demonstrating the practical relevance of approximations to the ellipsotope containment problem."],"url":"http://arxiv.org/abs/2404.11185v1","category":"math.OC"}
{"created":"2024-04-17 08:38:57","title":"Relating bubble sort to birthday problem","abstract":"Birthday problem is a well-known classic problem in probability theory widely applied in cryptography. Although bubble sort is a popular algorithm leading to some interesting theoretical problems in computer science, its relation to birthday problem has not been found yet. This paper indicates how Rayleigh distribution naturally arises in bubble sort by relating it to birthday problem, which presents a novel direction for analysing bubble sort and birthday problem. Then asymptotic distributions and statistical characteristics of bubble sort and birthday problem with very small absolute errors are presented. Moreover, this paper proves that some common optimizations of bubble sort could lead to average performance degradation.","sentences":["Birthday problem is a well-known classic problem in probability theory widely applied in cryptography.","Although bubble sort is a popular algorithm leading to some interesting theoretical problems in computer science, its relation to birthday problem has not been found yet.","This paper indicates how Rayleigh distribution naturally arises in bubble sort by relating it to birthday problem, which presents a novel direction for analysing bubble sort and birthday problem.","Then asymptotic distributions and statistical characteristics of bubble sort and birthday problem with very small absolute errors are presented.","Moreover, this paper proves that some common optimizations of bubble sort could lead to average performance degradation."],"url":"http://arxiv.org/abs/2404.11170v1","category":"math.PR"}
{"created":"2024-04-17 08:21:02","title":"Pre-processing matters: A segment search method for WSI classification","abstract":"Pre-processing for whole slide images can affect classification performance both in the training and inference stages. Our study analyzes the impact of pre-processing parameters on inference and training across single- and multiple-domain datasets. However, searching for an optimal parameter set is time-consuming. To overcome this, we propose a novel Similarity-based Simulated Annealing approach for fast parameter tuning to enhance inference performance on single-domain data. Our method demonstrates significant performance improvements in accuracy, which raise accuracy from 0.512 to 0.847 in a single domain. We further extend our insight into training performance in multi-domain data by employing a novel Bayesian optimization to search optimal pre-processing parameters, resulting in a high AUC of 0.967. We highlight that better pre-processing for WSI can contribute to further accuracy improvement in the histology area.","sentences":["Pre-processing for whole slide images can affect classification performance both in the training and inference stages.","Our study analyzes the impact of pre-processing parameters on inference and training across single- and multiple-domain datasets.","However, searching for an optimal parameter set is time-consuming.","To overcome this, we propose a novel Similarity-based Simulated Annealing approach for fast parameter tuning to enhance inference performance on single-domain data.","Our method demonstrates significant performance improvements in accuracy, which raise accuracy from 0.512 to 0.847 in a single domain.","We further extend our insight into training performance in multi-domain data by employing a novel Bayesian optimization to search optimal pre-processing parameters, resulting in a high AUC of 0.967.","We highlight that better pre-processing for WSI can contribute to further accuracy improvement in the histology area."],"url":"http://arxiv.org/abs/2404.11161v1","category":"cs.CV"}
{"created":"2024-04-17 07:35:06","title":"Accuracy and repeatability of a parallel robot for personalised minimally invasive surgery","abstract":"The paper presents the methodology used for accuracy and repeatability measurements of the experimental model of a parallel robot developed for surgical applications. The experimental setup uses a motion tracking system (for accuracy) and a high precision measuring arm for position (for repeatability). The accuracy was obtained by comparing the trajectory data from the experimental measurement with a baseline trajectory defined with the kinematic models of the parallel robotic system. The repeatability was experi-mentally determined by moving (repeatedly) the robot platform in predefined points.","sentences":["The paper presents the methodology used for accuracy and repeatability measurements of the experimental model of a parallel robot developed for surgical applications.","The experimental setup uses a motion tracking system (for accuracy) and a high precision measuring arm for position (for repeatability).","The accuracy was obtained by comparing the trajectory data from the experimental measurement with a baseline trajectory defined with the kinematic models of the parallel robotic system.","The repeatability was experi-mentally determined by moving (repeatedly) the robot platform in predefined points."],"url":"http://arxiv.org/abs/2404.11140v1","category":"cs.RO"}
{"created":"2024-04-17 07:21:17","title":"Learning epidemic trajectories through Kernel Operator Learning: from modelling to optimal control","abstract":"Since infectious pathogens start spreading into a susceptible population, mathematical models can provide policy makers with reliable forecasts and scenario analyses, which can be concretely implemented or solely consulted. In these complex epidemiological scenarios, machine learning architectures can play an important role, since they directly reconstruct data-driven models circumventing the specific modelling choices and the parameter calibration, typical of classical compartmental models. In this work, we discuss the efficacy of Kernel Operator Learning (KOL) to reconstruct population dynamics during epidemic outbreaks, where the transmission rate is ruled by an input strategy. In particular, we introduce two surrogate models, named KOL-m and KOL-$\\partial$, which reconstruct in two different ways the evolution of the epidemics. Moreover, we evaluate the generalization performances of the two approaches with different kernels, including the Neural Tangent Kernels, and compare them with a classical neural network model learning method. Employing synthetic but semi-realistic data, we show how the two introduced approaches are suitable for realizing fast and robust forecasts and scenario analyses, and how these approaches are competitive for determining optimal intervention strategies with respect to specific performance measures.","sentences":["Since infectious pathogens start spreading into a susceptible population, mathematical models can provide policy makers with reliable forecasts and scenario analyses, which can be concretely implemented or solely consulted.","In these complex epidemiological scenarios, machine learning architectures can play an important role, since they directly reconstruct data-driven models circumventing the specific modelling choices and the parameter calibration, typical of classical compartmental models.","In this work, we discuss the efficacy of Kernel Operator Learning (KOL) to reconstruct population dynamics during epidemic outbreaks, where the transmission rate is ruled by an input strategy.","In particular, we introduce two surrogate models, named KOL-m and KOL-$\\partial$, which reconstruct in two different ways the evolution of the epidemics.","Moreover, we evaluate the generalization performances of the two approaches with different kernels, including the Neural Tangent Kernels, and compare them with a classical neural network model learning method.","Employing synthetic but semi-realistic data, we show how the two introduced approaches are suitable for realizing fast and robust forecasts and scenario analyses, and how these approaches are competitive for determining optimal intervention strategies with respect to specific performance measures."],"url":"http://arxiv.org/abs/2404.11130v1","category":"math.NA"}
