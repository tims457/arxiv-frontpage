{"created":"2024-01-24 18:59:06","title":"Artificial Precision Timing Array: bridging the decihertz gravitational-wave sensitivity gap with clock satellites","abstract":"Gravitational-wave astronomy has developed enormously over the last decade with the first detections across different frequency bands, but has yet to access $0.1-10$ $\\mathrm{Hz}$ gravitational waves. Gravitational waves in this band are emitted by some of the most enigmatic sources, including intermediate-mass binary black hole mergers, early inspiralling compact binaries, and possibly cosmic inflation. To tap this exciting band, we propose the construction of a detector based on pulsar timing principles, the Artificial Precision Timing Array (APTA). We envision APTA as a solar system array of artificial \"pulsars\"$-$precision-clock-carrying satellites that emit pulsing electromagnetic signals towards Earth or other centrum. In this fundamental study, we estimate the clock precision needed for APTA to successfully detect gravitational waves. Our results suggest that a clock relative uncertainty of $10^{-17}$, which is currently attainable, would be sufficient for APTA to surpass LISA's sensitivity in the decihertz band and observe $10^3-10^4$ $\\mathrm{M}_\\odot$ black hole mergers. Future atomic clock technology realistically expected in the next decade would enable the detection of an increasingly diverse set of astrophysical sources, including stellar-mass compact binaries that merge in the LIGO-Virgo-KAGRA band, extreme-mass-ratio inspirals, and Type Ia supernovae. This work opens up a new area of research into designing and constructing artificial gravitational-wave detectors relying on the successful principles of pulsar timing.","sentences":["Gravitational-wave astronomy has developed enormously over the last decade with the first detections across different frequency bands, but has yet to access $0.1-10$ $\\mathrm{Hz}$ gravitational waves.","Gravitational waves in this band are emitted by some of the most enigmatic sources, including intermediate-mass binary black hole mergers, early inspiralling compact binaries, and possibly cosmic inflation.","To tap this exciting band, we propose the construction of a detector based on pulsar timing principles, the Artificial Precision Timing Array (APTA).","We envision APTA as a solar system array of artificial \"pulsars\"$-$precision-clock-carrying satellites that emit pulsing electromagnetic signals towards Earth or other centrum.","In this fundamental study, we estimate the clock precision needed for APTA to successfully detect gravitational waves.","Our results suggest that a clock relative uncertainty of $10^{-17}$, which is currently attainable, would be sufficient for APTA to surpass LISA's sensitivity in the decihertz band and observe $10^3-10^4$ $\\mathrm{M}_\\odot$ black hole mergers.","Future atomic clock technology realistically expected in the next decade would enable the detection of an increasingly diverse set of astrophysical sources, including stellar-mass compact binaries that merge in the LIGO-Virgo-KAGRA band, extreme-mass-ratio inspirals, and Type Ia supernovae.","This work opens up a new area of research into designing and constructing artificial gravitational-wave detectors relying on the successful principles of pulsar timing."],"url":"http://arxiv.org/abs/2401.13668v1","category":"astro-ph.IM"}
{"created":"2024-01-24 18:58:35","title":"Predicting the Impact of Crashes Across Release Channels","abstract":"Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases. Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases. In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels. We also list the challenges that need to be considered when approaching this problem.","sentences":["Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases.","Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases.","In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels.","We also list the challenges that need to be considered when approaching this problem."],"url":"http://arxiv.org/abs/2401.13667v1","category":"cs.SE"}
{"created":"2024-01-24 18:58:21","title":"Algebraic methods for solving recognition problems with non-crossing classes","abstract":"In this paper, we propose to consider various models of pattern recognition. At the same time, it is proposed to consider models in the form of two operators: a recognizing operator and a decision rule. Algebraic operations are introduced on recognizing operators, and based on the application of these operators, a family of recognizing algorithms is created. An upper estimate is constructed for the model, which guarantees the completeness of the extension.","sentences":["In this paper, we propose to consider various models of pattern recognition.","At the same time, it is proposed to consider models in the form of two operators: a recognizing operator and a decision rule.","Algebraic operations are introduced on recognizing operators, and based on the application of these operators, a family of recognizing algorithms is created.","An upper estimate is constructed for the model, which guarantees the completeness of the extension."],"url":"http://arxiv.org/abs/2401.13666v1","category":"cs.CV"}
{"created":"2024-01-24 18:57:22","title":"Hamiltonian, Geometric Momentum and Force Operators for a Spin Zero Particle on a Curve: Physical Approach","abstract":"The Hamiltonian for a spin zero particle that is confined to a curve embedded in the 3D space is constructed by squeezing the coordinates spanning a tube normal to the curve onto the curve assuming strong normal forces. We follow the new approach that we applied to confine a particle to a surface, in that we start with an expression for the 3D momentum operators whose components along and normal to the curve directions are separately Hermitian. The kinetic energy operator expressed in terms of the momentum operator in the normal direction is then a Hermitian operator in this case. When this operator is dropped and the thickness of the tube surrounding the curve is set to zero, one automatically gets the Hermitian curve Hamiltonian that contains the geometric potential term as expected. It is demonstrated that the origin of this potential lies in the ordering or symmetrization of the original 3D momentum operators in order to render them Hermitian. The Hermitian momentum operator for the particle as it is confined to the curve is also constructed and is seen to be similar to what is known as the geometric momentum of a particle confined to a surface in that it has a term proportional to the curvature that is along the normal to the curve. The force operator of the particle on the curve is also derived, and is shown to reduce, for a curve with a constant curvature and torsion, to a -- apparently -- single component normal to the curve that is a symmetrization of the classical expression plus a quantum term. All the above quantities are then derived for the specific case of a particle confined to a cylindrical helix embedded in 3D space.","sentences":["The Hamiltonian for a spin zero particle that is confined to a curve embedded in the 3D space is constructed by squeezing the coordinates spanning a tube normal to the curve onto the curve assuming strong normal forces.","We follow the new approach that we applied to confine a particle to a surface, in that we start with an expression for the 3D momentum operators whose components along and normal to the curve directions are separately Hermitian.","The kinetic energy operator expressed in terms of the momentum operator in the normal direction is then a Hermitian operator in this case.","When this operator is dropped and the thickness of the tube surrounding the curve is set to zero, one automatically gets the Hermitian curve Hamiltonian that contains the geometric potential term as expected.","It is demonstrated that the origin of this potential lies in the ordering or symmetrization of the original 3D momentum operators in order to render them Hermitian.","The Hermitian momentum operator for the particle as it is confined to the curve is also constructed and is seen to be similar to what is known as the geometric momentum of a particle confined to a surface in that it has a term proportional to the curvature that is along the normal to the curve.","The force operator of the particle on the curve is also derived, and is shown to reduce, for a curve with a constant curvature and torsion, to a -- apparently -- single component normal to the curve that is a symmetrization of the classical expression plus a quantum term.","All the above quantities are then derived for the specific case of a particle confined to a cylindrical helix embedded in 3D space."],"url":"http://arxiv.org/abs/2401.13664v1","category":"quant-ph"}
{"created":"2024-01-24 18:56:53","title":"The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations","abstract":"In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.","sentences":["In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning.","While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms.","We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations.","In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms.","We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization.","All code is available at https://github.com/Matt00n/PolicyGradientsJax."],"url":"http://arxiv.org/abs/2401.13662v1","category":"cs.LG"}
{"created":"2024-01-24 18:53:53","title":"MambaByte: Token-free Selective State Space Model","abstract":"Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.","sentences":["Token-free language models learn directly from raw bytes and remove the bias of subword tokenization.","Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings.","We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences.","Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models.","We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers.","Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers.","Our findings establish the viability of MambaByte in enabling token-free language modeling."],"url":"http://arxiv.org/abs/2401.13660v1","category":"cs.CL"}
{"created":"2024-01-24 18:51:50","title":"Site-selective preparation and multi-state readout of molecules in optical tweezers","abstract":"Polar molecules are a quantum resource with rich internal structure that can be coherently controlled. The structure, however, also makes the state preparation and measurement (SPAM) of molecules challenging. We advance the SPAM of individual molecules assembled from constituent atoms trapped in optical tweezer arrays. Sites without NaCs molecules are eliminated using high-fidelity Cs atom detection, increasing the peak molecule filling fraction of the array threefold. We site-selectively initialize the array in a rotational qubit subspace that is insensitive to differential AC Stark shifts from the optical tweezer. Lastly, we detect multiple rotational states per experimental cycle by imaging atoms after sequential state-selective dissociations. These demonstrations extend the SPAM capabilities of molecules for quantum information, simulation, and metrology.","sentences":["Polar molecules are a quantum resource with rich internal structure that can be coherently controlled.","The structure, however, also makes the state preparation and measurement (SPAM) of molecules challenging.","We advance the SPAM of individual molecules assembled from constituent atoms trapped in optical tweezer arrays.","Sites without NaCs molecules are eliminated using high-fidelity Cs atom detection, increasing the peak molecule filling fraction of the array threefold.","We site-selectively initialize the array in a rotational qubit subspace that is insensitive to differential AC Stark shifts from the optical tweezer.","Lastly, we detect multiple rotational states per experimental cycle by imaging atoms after sequential state-selective dissociations.","These demonstrations extend the SPAM capabilities of molecules for quantum information, simulation, and metrology."],"url":"http://arxiv.org/abs/2401.13659v1","category":"physics.atom-ph"}
{"created":"2024-01-24 18:51:21","title":"Quantum sensing: Beyond the classical limits of precision","abstract":"Quantum sensors allow the estimation of parameters with precision higher than that obtained with classical strategies. Devices based on quantum physics have allowed the precise estimation of the gravitational field, the detailed imaging of the brain, the detection of gravitational-wave sources more than 400 million light years away, and an ever-increasing precision in the measurement of time. Quantum metrology, which is the conceptual framework that encompasses all these devices, is reviewed here, emphasizing recent results regarding noisy systems.","sentences":["Quantum sensors allow the estimation of parameters with precision higher than that obtained with classical strategies.","Devices based on quantum physics have allowed the precise estimation of the gravitational field, the detailed imaging of the brain, the detection of gravitational-wave sources more than 400 million light years away, and an ever-increasing precision in the measurement of time.","Quantum metrology, which is the conceptual framework that encompasses all these devices, is reviewed here, emphasizing recent results regarding noisy systems."],"url":"http://arxiv.org/abs/2401.13658v1","category":"quant-ph"}
{"created":"2024-01-24 18:49:30","title":"Inadequacy of common stochastic neural networks for reliable clinical decision support","abstract":"Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such as Bayesian neural network layers and model ensembles. Our models achieve state of the art performance in terms of discrimination performance (AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality prediction benchmark. However, epistemic uncertainty is critically underestimated by the selected stochastic deep learning methods. A heuristic proof for the responsible collapse of the posterior distribution is provided. Our findings reveal the inadequacy of commonly used stochastic deep learning approaches to reliably recognize OoD samples. In both methods, unsubstantiated model confidence is not prevented due to strongly biased functional posteriors, rendering them inappropriate for reliable clinical decision support. This highlights the need for approaches with more strictly enforced or inherent distance-awareness to known data points, e.g., using kernel-based techniques.","sentences":["Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns.","For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy.","Common deep learning approaches, however, have the tendency towards overconfidence under data shift.","Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences.","This highlights the importance of reliable estimation of local uncertainty and its communication to the end user.","While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications.","We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study.","For predictions on the EHR time series, Encoder-Only Transformer models were employed.","Stochasticity of model functions was achieved by incorporating common methods such as Bayesian neural network layers and model ensembles.","Our models achieve state of the art performance in terms of discrimination performance (AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality prediction benchmark.","However, epistemic uncertainty is critically underestimated by the selected stochastic deep learning methods.","A heuristic proof for the responsible collapse of the posterior distribution is provided.","Our findings reveal the inadequacy of commonly used stochastic deep learning approaches to reliably recognize OoD samples.","In both methods, unsubstantiated model confidence is not prevented due to strongly biased functional posteriors, rendering them inappropriate for reliable clinical decision support.","This highlights the need for approaches with more strictly enforced or inherent distance-awareness to known data points, e.g., using kernel-based techniques."],"url":"http://arxiv.org/abs/2401.13657v1","category":"cs.LG"}
{"created":"2024-01-24 18:49:19","title":"Navigating Multidimensional Ideologies with Reddit's Political Compass: Economic Conflict and Social Affinity","abstract":"The prevalent perspective in quantitative research on opinion dynamics flattens the landscape of the online political discourse into a traditional left--right dichotomy. While this approach helps simplify the analysis and modeling effort, it also neglects the intrinsic multidimensional richness of ideologies. In this study, we analyze social interactions on Reddit, under the lens of a multi-dimensional ideological framework: the political compass. We examine over 8 million comments posted on the subreddits /r/PoliticalCompass and /r/PoliticalCompassMemes during 2020--2022. By leveraging their self-declarations, we disentangle the ideological dimensions of users into economic (left--right) and social (libertarian--authoritarian) axes. In addition, we characterize users by their demographic attributes (age, gender, and affluence).   We find significant homophily for interactions along the social axis of the political compass and demographic attributes. Compared to a null model, interactions among individuals of similar ideology surpass expectations by 6%. In contrast, we uncover a significant heterophily along the economic axis: left/right interactions exceed expectations by 10%. Furthermore, heterophilic interactions are characterized by a higher language toxicity than homophilic interactions, which hints at a conflictual discourse between every opposite ideology. Our results help reconcile apparent contradictions in recent literature, which found a superposition of homophilic and heterophilic interactions in online political discussions. By disentangling such interactions into the economic and social axes we pave the way for a deeper understanding of opinion dynamics on social media.","sentences":["The prevalent perspective in quantitative research on opinion dynamics flattens the landscape of the online political discourse into a traditional left--right dichotomy.","While this approach helps simplify the analysis and modeling effort, it also neglects the intrinsic multidimensional richness of ideologies.","In this study, we analyze social interactions on Reddit, under the lens of a multi-dimensional ideological framework: the political compass.","We examine over 8 million comments posted on the subreddits /r/PoliticalCompass and /r/PoliticalCompassMemes during 2020--2022.","By leveraging their self-declarations, we disentangle the ideological dimensions of users into economic (left--right) and social (libertarian--authoritarian) axes.","In addition, we characterize users by their demographic attributes (age, gender, and affluence).   ","We find significant homophily for interactions along the social axis of the political compass and demographic attributes.","Compared to a null model, interactions among individuals of similar ideology surpass expectations by 6%.","In contrast, we uncover a significant heterophily along the economic axis: left/right interactions exceed expectations by 10%.","Furthermore, heterophilic interactions are characterized by a higher language toxicity than homophilic interactions, which hints at a conflictual discourse between every opposite ideology.","Our results help reconcile apparent contradictions in recent literature, which found a superposition of homophilic and heterophilic interactions in online political discussions.","By disentangling such interactions into the economic and social axes we pave the way for a deeper understanding of opinion dynamics on social media."],"url":"http://arxiv.org/abs/2401.13656v1","category":"cs.SI"}
{"created":"2024-01-24 18:48:45","title":"Bi-Hamiltonian in Semiflexible Polymer as Strongly Coupled System","abstract":"When many body system in Classical Mechanics is described using bi-Hamiltonian for its group of particles which forms a coarse grained bead, the damping energy from the heat bath perturbs the motion of a particle which is along the trajectory of multiple Hamiltonian system simultaneously. As managing the coarse grained modeling of the suspended semiflexible polymer like single walled carbon nanotubes(SWCNT) in vacuum, the authors discovered that such interconnection between Hamiltonian systems as sharing a heat bath builds a strongly coupled system. The consequence of this interconnection is revealed to form certain form of cross correlated momentum in coarsed grained system as well as in the original atomic scales. In this paper, we adapt the framework of Stochastic Thermodynamics to describe the cross correlation of bi-Hamiltonian system which replicates the macroscopic behaviour of SWCNT under the Smoluchowski picture for cross correlation between Hamiltonian systems. The numerical experiments using collision between tubes confirms the derivation and the justification of the usage of the heat diffusion to compensate the abnormaly from cross correlated momentum.","sentences":["When many body system in Classical Mechanics is described using bi-Hamiltonian for its group of particles which forms a coarse grained bead, the damping energy from the heat bath perturbs the motion of a particle which is along the trajectory of multiple Hamiltonian system simultaneously.","As managing the coarse grained modeling of the suspended semiflexible polymer like single walled carbon nanotubes(SWCNT) in vacuum, the authors discovered that such interconnection between Hamiltonian systems as sharing a heat bath builds a strongly coupled system.","The consequence of this interconnection is revealed to form certain form of cross correlated momentum in coarsed grained system as well as in the original atomic scales.","In this paper, we adapt the framework of Stochastic Thermodynamics to describe the cross correlation of bi-Hamiltonian system which replicates the macroscopic behaviour of SWCNT under the Smoluchowski picture for cross correlation between Hamiltonian systems.","The numerical experiments using collision between tubes confirms the derivation and the justification of the usage of the heat diffusion to compensate the abnormaly from cross correlated momentum."],"url":"http://arxiv.org/abs/2401.13655v1","category":"physics.comp-ph"}
{"created":"2024-01-24 18:48:28","title":"HetDAPAC: Distributed Attribute-Based Private Access Control with Heterogeneous Attributes","abstract":"Verifying user attributes to provide fine-grained access control to databases is fundamental to an attribute-based authentication system. In such systems, either a single (central) authority verifies all attributes, or multiple independent authorities verify individual attributes distributedly to allow a user to access records stored on the servers. While a \\emph{central} setup is more communication cost efficient, it causes privacy breach of \\emph{all} user attributes to a central authority. Recently, Jafarpisheh et al. studied an information theoretic formulation of the \\emph{distributed} multi-authority setup with $N$ non-colluding authorities, $N$ attributes and $K$ possible values for each attribute, called an $(N,K)$ distributed attribute-based private access control (DAPAC) system, where each server learns only one attribute value that it verifies, and remains oblivious to the remaining $N-1$ attributes. We show that off-loading a subset of attributes to a central server for verification improves the achievable rate from $\\frac{1}{2K}$ in Jafarpisheh et al. to $\\frac{1}{K+1}$ in this paper, thus \\emph{almost doubling the rate} for relatively large $K$, while sacrificing the privacy of a few possibly non-sensitive attributes.","sentences":["Verifying user attributes to provide fine-grained access control to databases is fundamental to an attribute-based authentication system.","In such systems, either a single (central) authority verifies all attributes, or multiple independent authorities verify individual attributes distributedly to allow a user to access records stored on the servers.","While a \\emph{central} setup is more communication cost efficient, it causes privacy breach of \\emph{all} user attributes to a central authority.","Recently, Jafarpisheh et al. studied an information theoretic formulation of the \\emph{distributed} multi-authority setup with $N$ non-colluding authorities, $N$ attributes and $K$ possible values for each attribute, called an $(N,K)$ distributed attribute-based private access control (DAPAC) system, where each server learns only one attribute value that it verifies, and remains oblivious to the remaining $N-1$ attributes.","We show that off-loading a subset of attributes to a central server for verification improves the achievable rate from $\\frac{1}{2K}$ in Jafarpisheh et al. to $\\frac{1}{K+1}$ in this paper, thus \\emph{almost doubling the rate} for relatively large $K$, while sacrificing the privacy of a few possibly non-sensitive attributes."],"url":"http://arxiv.org/abs/2401.13653v1","category":"cs.IT"}
{"created":"2024-01-24 18:44:14","title":"Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors","abstract":"In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.","sentences":["In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function.","This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances.","We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability.","Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces.","Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users."],"url":"http://arxiv.org/abs/2401.13652v1","category":"cs.LG"}
{"created":"2024-01-24 18:40:29","title":"Quantum Logics that are Symmetric-difference-closed","abstract":"In this note we contribute to the recently developing study of \"almost Boolean\" quantum logics (i.e. to the study of orthomodular partially ordered sets that are naturally endowed with a symmetric difference). We call them enriched quantum logics (EQLs). We first consider set-representable EQLs. We disprove a natural conjecture on compatibility in EQLs. Then we discuss the possibility of extending states and prove an extension result for $\\Ztwo$-states on EQLs. In the second part we pass to general orthoposets with a symmetric difference (GEQLs). We show that a simplex can be a state space of a GEQL that has an arbitrarily high degree of noncompatibility. Finally, we find an appropriate definition of a \"parametrization\" as a mapping between GEQLs that preserves the set-representation.","sentences":["In this note we contribute to the recently developing study of \"almost Boolean\" quantum logics (i.e. to the study of orthomodular partially ordered sets that are naturally endowed with a symmetric difference).","We call them enriched quantum logics (EQLs).","We first consider set-representable EQLs.","We disprove a natural conjecture on compatibility in EQLs.","Then we discuss the possibility of extending states and prove an extension result for $\\Ztwo$-states on EQLs.","In the second part we pass to general orthoposets with a symmetric difference (GEQLs).","We show that a simplex can be a state space of a GEQL that has an arbitrarily high degree of noncompatibility.","Finally, we find an appropriate definition of a \"parametrization\" as a mapping between GEQLs that preserves the set-representation."],"url":"http://arxiv.org/abs/2401.13651v1","category":"math-ph"}
{"created":"2024-01-24 18:35:21","title":"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks","abstract":"Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.","sentences":["Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks.","However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve.","Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively.","To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}.","VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents.","To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives.","We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models.","Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents.","VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.","Our code, baseline models, and data is publicly available at https://jykoh.com/vwa."],"url":"http://arxiv.org/abs/2401.13649v1","category":"cs.LG"}
{"created":"2024-01-24 18:33:15","title":"The FBSDE approach to sine-Gordon up to $6\u03c0$","abstract":"We develop a stochastic analysis of the sine-Gordon Euclidean quantum field $(\\cos (\\beta \\varphi))_2$ on the full space up to the second threshold, i.e. for $\\beta^2 < 6 \\pi$. The basis of our method is a forward-backward stochastic differential equation (FBSDE) for a decomposition $(X_t)_{t \\geqslant 0}$ of the interacting Euclidean field $X_{\\infty}$ along a scale parameter $t \\geqslant 0$. This FBSDE describes the optimiser of the stochastic control representation of the Euclidean QFT introduced by Barashkov and one of the authors. We show that the FBSDE provides a description of the interacting field without cut-offs and that it can be used effectively to study the sine-Gordon measure to obtain results about large deviations, integrability, decay of correlations for local observables, singularity with respect to the free field, Osterwalder-Schrader axioms and other properties.","sentences":["We develop a stochastic analysis of the sine-Gordon Euclidean quantum field $(\\cos (\\beta \\varphi))_2$ on the full space up to the second threshold, i.e. for $\\beta^2 < 6 \\pi$. The basis of our method is a forward-backward stochastic differential equation (FBSDE) for a decomposition $(X_t)_{t \\geqslant 0}$ of the interacting Euclidean field $X_{\\infty}$ along a scale parameter $t","\\geqslant 0$.","This FBSDE describes the optimiser of the stochastic control representation of the Euclidean QFT introduced by Barashkov and one of the authors.","We show that the FBSDE provides a description of the interacting field without cut-offs and that it can be used effectively to study the sine-Gordon measure to obtain results about large deviations, integrability, decay of correlations for local observables, singularity with respect to the free field, Osterwalder-Schrader axioms and other properties."],"url":"http://arxiv.org/abs/2401.13648v1","category":"math-ph"}
{"created":"2024-01-24 18:28:52","title":"Employing polyhedral methods to optimize stencils on FPGAs with stencil-specific caches, data reuse, and wide data bursts","abstract":"It is well known that to accelerate stencil codes on CPUs or GPUs and to exploit hardware caches and their lines optimizers must find spatial and temporal locality of array accesses to harvest data-reuse opportunities. On FPGAs there is the burden that there are no built-in caches (or only pre-built hardware descriptions for cache blocks that are inefficient for stencil codes). But this paper demonstrates that this lack is also a chance as polyhedral methods can be used to generate stencil-specific cache-structures of the right sizes on the FPGA and to fill and flush them efficiently with wide bursts during stencil execution. The paper shows how to derive the appropriate directives and code restructurings from stencil codes so that the FPGA compiler generates fast stencil hardware. Switching on our optimization improves the runtime of a set of 10 stencils by between 43x and 156x.","sentences":["It is well known that to accelerate stencil codes on CPUs or GPUs and to exploit hardware caches and their lines optimizers must find spatial and temporal locality of array accesses to harvest data-reuse opportunities.","On FPGAs there is the burden that there are no built-in caches (or only pre-built hardware descriptions for cache blocks that are inefficient for stencil codes).","But this paper demonstrates that this lack is also a chance as polyhedral methods can be used to generate stencil-specific cache-structures of the right sizes on the FPGA and to fill and flush them efficiently with wide bursts during stencil execution.","The paper shows how to derive the appropriate directives and code restructurings from stencil codes so that the FPGA compiler generates fast stencil hardware.","Switching on our optimization improves the runtime of a set of 10 stencils by between 43x and 156x."],"url":"http://arxiv.org/abs/2401.13645v1","category":"cs.PL"}
{"created":"2024-01-24 18:21:13","title":"Design, Development, and Deployment of Context-Adaptive AI Systems for Enhanced End-User Adoption","abstract":"My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences.","sentences":["My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods.","I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability.","My research unfolds in three key stages: design, development, and deployment.","In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations.","In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios.","The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms.","I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences."],"url":"http://arxiv.org/abs/2401.13643v1","category":"cs.HC"}
{"created":"2024-01-24 18:16:15","title":"Unveiling homophily beyond the pool of opportunities","abstract":"Unveiling individuals' preferences for connecting with similar others (choice homophily) beyond the structural factors determining the pool of opportunities, is a challenging task. Here, we introduce a robust methodology for quantifying and inferring choice homophily in a variety of social networks. Our approach employs statistical network ensembles to estimate and standardize homophily measurements. We control for group size imbalances and activity disparities by counting the number of possible network configurations with a given number of inter-group links using combinatorics. This method provides a principled measure of connection preferences and their confidence intervals. Our framework is versatile, suitable for undirected and directed networks, and applicable in scenarios involving multiple groups. To validate our inference method, we test it on synthetic networks and show that it outperforms traditional metrics. Our approach accurately captures the generative homophily used to build the networks, even when we include additional tie-formation mechanisms, such as preferential attachment and triadic closure. Results show that while triadic closure has some influence on the inference, its impact is small in homophilic networks. On the other hand, preferential attachment does not perturb the results of the inference method. We apply our method to real-world networks, demonstrating its effectiveness in unveiling underlying gender homophily. Our method aligns with traditional metrics in networks with balanced populations, but we obtain different results when the group sizes or degrees are imbalanced. This finding highlights the importance of considering structural factors when measuring choice homophily in social networks.","sentences":["Unveiling individuals' preferences for connecting with similar others (choice homophily) beyond the structural factors determining the pool of opportunities, is a challenging task.","Here, we introduce a robust methodology for quantifying and inferring choice homophily in a variety of social networks.","Our approach employs statistical network ensembles to estimate and standardize homophily measurements.","We control for group size imbalances and activity disparities by counting the number of possible network configurations with a given number of inter-group links using combinatorics.","This method provides a principled measure of connection preferences and their confidence intervals.","Our framework is versatile, suitable for undirected and directed networks, and applicable in scenarios involving multiple groups.","To validate our inference method, we test it on synthetic networks and show that it outperforms traditional metrics.","Our approach accurately captures the generative homophily used to build the networks, even when we include additional tie-formation mechanisms, such as preferential attachment and triadic closure.","Results show that while triadic closure has some influence on the inference, its impact is small in homophilic networks.","On the other hand, preferential attachment does not perturb the results of the inference method.","We apply our method to real-world networks, demonstrating its effectiveness in unveiling underlying gender homophily.","Our method aligns with traditional metrics in networks with balanced populations, but we obtain different results when the group sizes or degrees are imbalanced.","This finding highlights the importance of considering structural factors when measuring choice homophily in social networks."],"url":"http://arxiv.org/abs/2401.13642v1","category":"physics.soc-ph"}
{"created":"2024-01-24 18:10:39","title":"How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability","abstract":"Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).   The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field. The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability. For reproducibility reasons, we release all the code in GitHub.","sentences":["Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society.","This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed.","As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).   ","The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics.","In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results.","ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios.","Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field.","The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability.","For reproducibility reasons, we release all the code in GitHub."],"url":"http://arxiv.org/abs/2401.13641v1","category":"cs.CV"}
{"created":"2024-01-24 18:09:16","title":"Winding Clearness for Differentiable Point Cloud Optimization","abstract":"We propose to explore the properties of raw point clouds through the \\emph{winding clearness}, a concept we first introduce for assessing the clarity of the interior/exterior relationships represented by the winding number field of the point cloud. In geometric modeling, the winding number is a powerful tool for distinguishing the interior and exterior of a given surface $\\partial \\Omega$, and it has been previously used for point normal orientation and surface reconstruction. In this work, we introduce a novel approach to assess and optimize the quality of point clouds based on the winding clearness. We observe that point clouds with reduced noise tend to exhibit improved winding clearness. Accordingly, we propose an objective function that quantifies the error in winding clearness, solely utilizing the positions of the point clouds. Moreover, we demonstrate that the winding clearness error is differentiable and can serve as a loss function in optimization-based and learning-based point cloud processing. In the optimization-based method, the loss function is directly back-propagated to update the point positions, resulting in an overall improvement of the point cloud. In the learning-based method, we incorporate the winding clearness as a geometric constraint in the diffusion-based 3D generative model. Experimental results demonstrate the effectiveness of optimizing the winding clearness in enhancing the quality of the point clouds. Our method exhibits superior performance in handling noisy point clouds with thin structures, highlighting the benefits of the global perspective enabled by the winding number.","sentences":["We propose to explore the properties of raw point clouds through the \\emph{winding clearness}, a concept we first introduce for assessing the clarity of the interior/exterior relationships represented by the winding number field of the point cloud.","In geometric modeling, the winding number is a powerful tool for distinguishing the interior and exterior of a given surface $\\partial \\Omega$, and it has been previously used for point normal orientation and surface reconstruction.","In this work, we introduce a novel approach to assess and optimize the quality of point clouds based on the winding clearness.","We observe that point clouds with reduced noise tend to exhibit improved winding clearness.","Accordingly, we propose an objective function that quantifies the error in winding clearness, solely utilizing the positions of the point clouds.","Moreover, we demonstrate that the winding clearness error is differentiable and can serve as a loss function in optimization-based and learning-based point cloud processing.","In the optimization-based method, the loss function is directly back-propagated to update the point positions, resulting in an overall improvement of the point cloud.","In the learning-based method, we incorporate the winding clearness as a geometric constraint in the diffusion-based 3D generative model.","Experimental results demonstrate the effectiveness of optimizing the winding clearness in enhancing the quality of the point clouds.","Our method exhibits superior performance in handling noisy point clouds with thin structures, highlighting the benefits of the global perspective enabled by the winding number."],"url":"http://arxiv.org/abs/2401.13639v1","category":"cs.GR"}
{"created":"2024-01-24 18:05:04","title":"Stability of Wilson Loops and Other Observables in Various Type IIB Backgrounds","abstract":"This work involves the stability study of various observables (Wilson loop, 't Hooft loop and Entanglement Entropy) under linear fluctuations of the coordinates for certain ten-dimensional solutions of type IIB Supergravity that have appeared in the literature recently. These backgrounds are defined using intersecting and wrapped D5 branes, their holographically dual field theories are speculated to confine and they have a non-local UV completion that is governed by Little String Theory. The present study confirms previous claims on the stability of the solutions made using the concavity condition for the energy of the probe strings, by studying the eigenvalue problem for each case and also providing a numerical analysis.","sentences":["This work involves the stability study of various observables (Wilson loop, 't Hooft loop and Entanglement Entropy) under linear fluctuations of the coordinates for certain ten-dimensional solutions of type IIB Supergravity that have appeared in the literature recently.","These backgrounds are defined using intersecting and wrapped D5 branes, their holographically dual field theories are speculated to confine and they have a non-local UV completion that is governed by Little String Theory.","The present study confirms previous claims on the stability of the solutions made using the concavity condition for the energy of the probe strings, by studying the eigenvalue problem for each case and also providing a numerical analysis."],"url":"http://arxiv.org/abs/2401.13637v1","category":"hep-th"}
{"created":"2024-01-24 18:01:49","title":"A measure of chaos from eigenstate thermalization hypothesis","abstract":"Eigenstate thermalization hypothesis is a detailed statement of the matrix elements of few-body operators in energy eigenbasis of a chaotic Hamiltonian. Part of the statement is that the off-diagonal elements fall exponential for large energy difference. We propose that the exponent ($\\gamma>0$) is a measure of quantum chaos. Smaller $\\gamma$ implies more chaotic dynamics. The chaos bound is given by $\\gamma=\\beta/4$ where $\\beta$ is the inverse temperature. We give analytical argument in support of this proposal. The slower exponential fall also means that the action of the operator on a state leads to higher delocalization in energy eigenbasis. Numerically we compare two chaotic Hamiltonians - SYK model and chaotic XXZ spin chain. Using the new measure, we find that the SYK model becomes maximally chaotic at low temperature which has been shown rigorously in previous works. The new measure is more readily accessible compare to other measures using numerical methods.","sentences":["Eigenstate thermalization hypothesis is a detailed statement of the matrix elements of few-body operators in energy eigenbasis of a chaotic Hamiltonian.","Part of the statement is that the off-diagonal elements fall exponential for large energy difference.","We propose that the exponent ($\\gamma>0$) is a measure of quantum chaos.","Smaller $\\gamma$ implies more chaotic dynamics.","The chaos bound is given by $\\gamma=\\beta/4$ where $\\beta$ is the inverse temperature.","We give analytical argument in support of this proposal.","The slower exponential fall also means that the action of the operator on a state leads to higher delocalization in energy eigenbasis.","Numerically we compare two chaotic Hamiltonians - SYK model and chaotic XXZ spin chain.","Using the new measure, we find that the SYK model becomes maximally chaotic at low temperature which has been shown rigorously in previous works.","The new measure is more readily accessible compare to other measures using numerical methods."],"url":"http://arxiv.org/abs/2401.13633v1","category":"quant-ph"}
{"created":"2024-01-24 18:00:11","title":"Quantifying the Impact of Frame Preemption on Combined TSN Shapers","abstract":"Different scheduling mechanisms in Time Sensitive Networking (TSN) can be integrated together to design and support complex architectures with enhanced capabilities for mixed critical networks. Integrating Frame Preemption (FP) with Credit-Based Shaper (CBS) and Gate Control List (GCL) opens up different modes and configuration choices resulting in a complex evaluation of several possibilities and their impact on the Quality of Service (QoS). In this paper, we implement and quantify the integration of preemptive CBS with GCL by incorporating FP into the architecture. Our experiments show that the end-to-end delay of Audio Video Bridging (AVB) flows shaped by CBS reduces significantly (up to 40\\%) when AVB flows are set to preemptable class. We further show that the jitter of Time Triggered (TT) traffic remains unaffected in \"with Hold/Release\" mode. Furthermore, we propose to introduce Guardband (GB) in the \"without Hold/Release\" to reduce the jitter of the TT flow. We compare all the different integration modes, starting with CBS with GCL, extending it further to FP. We evaluate all feasible combinations in both synthetic and realistic scenarios and offer recommendations for practical configuration methods.","sentences":["Different scheduling mechanisms in Time Sensitive Networking (TSN) can be integrated together to design and support complex architectures with enhanced capabilities for mixed critical networks.","Integrating Frame Preemption (FP) with Credit-Based Shaper (CBS) and Gate Control List (GCL) opens up different modes and configuration choices resulting in a complex evaluation of several possibilities and their impact on the Quality of Service (QoS).","In this paper, we implement and quantify the integration of preemptive CBS with GCL by incorporating FP into the architecture.","Our experiments show that the end-to-end delay of Audio Video Bridging (AVB) flows shaped by CBS reduces significantly (up to 40\\%) when AVB flows are set to preemptable class.","We further show that the jitter of Time Triggered (TT) traffic remains unaffected in \"with Hold/Release\" mode.","Furthermore, we propose to introduce Guardband (GB) in the \"without Hold/Release\" to reduce the jitter of the TT flow.","We compare all the different integration modes, starting with CBS with GCL, extending it further to FP.","We evaluate all feasible combinations in both synthetic and realistic scenarios and offer recommendations for practical configuration methods."],"url":"http://arxiv.org/abs/2401.13631v1","category":"cs.NI"}
{"created":"2024-01-24 17:59:00","title":"Enabling Seamless Data Security, Consensus, and Trading in Vehicular Networks","abstract":"Cooperative driving is an emerging paradigm to enhance the safety and efficiency of autonomous vehicles. To ensure successful cooperation, road users must reach a consensus for making collective decisions, while recording vehicular data to analyze and address failures related to such agreements. This data has the potential to provide valuable insights into various vehicular events, while also potentially improving accountability measures. Furthermore, vehicles may benefit from the ability to negotiate and trade services among themselves, adding value to the cooperative driving framework. However, the majority of proposed systems aiming to ensure data security, consensus, or service trading, lack efficient and thoroughly validated mechanisms that consider the distinctive characteristics of vehicular networks. These limitations are amplified by a dependency on the centralized support provided by the infrastructure. Furthermore, corresponding mechanisms must diligently address security concerns, especially regarding potential malicious or misbehaving nodes, while also considering inherent constraints of the wireless medium. We introduce the Verifiable Event Extension (VEE), an applicational extension designed for Intelligent Transportation System (ITS) messages. The VEE operates seamlessly with any existing standardized vehicular communications protocol, addressing crucial aspects of data security, consensus, and trading with minimal overhead. To achieve this, we employ blockchain techniques, Byzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based mechanics. To assess our proposal's feasibility and lightweight nature, we employed a hardware-in-the-loop setup for analysis. Experimental results demonstrate the viability and efficiency of the VEE extension in overcoming the challenges posed by the distributed and opportunistic nature of wireless vehicular communications.","sentences":["Cooperative driving is an emerging paradigm to enhance the safety and efficiency of autonomous vehicles.","To ensure successful cooperation, road users must reach a consensus for making collective decisions, while recording vehicular data to analyze and address failures related to such agreements.","This data has the potential to provide valuable insights into various vehicular events, while also potentially improving accountability measures.","Furthermore, vehicles may benefit from the ability to negotiate and trade services among themselves, adding value to the cooperative driving framework.","However, the majority of proposed systems aiming to ensure data security, consensus, or service trading, lack efficient and thoroughly validated mechanisms that consider the distinctive characteristics of vehicular networks.","These limitations are amplified by a dependency on the centralized support provided by the infrastructure.","Furthermore, corresponding mechanisms must diligently address security concerns, especially regarding potential malicious or misbehaving nodes, while also considering inherent constraints of the wireless medium.","We introduce the Verifiable Event Extension (VEE), an applicational extension designed for Intelligent Transportation System (ITS) messages.","The VEE operates seamlessly with any existing standardized vehicular communications protocol, addressing crucial aspects of data security, consensus, and trading with minimal overhead.","To achieve this, we employ blockchain techniques, Byzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based mechanics.","To assess our proposal's feasibility and lightweight nature, we employed a hardware-in-the-loop setup for analysis.","Experimental results demonstrate the viability and efficiency of the VEE extension in overcoming the challenges posed by the distributed and opportunistic nature of wireless vehicular communications."],"url":"http://arxiv.org/abs/2401.13630v1","category":"cs.DC"}
{"created":"2024-01-24 17:58:07","title":"Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild","abstract":"We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.","sentences":["We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up.","Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration.","As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration.","We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations.","SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential.","Moreover, we introduce negative-quality prompts to further improve perceptual quality.","We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration.","Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts."],"url":"http://arxiv.org/abs/2401.13627v1","category":"cs.CV"}
{"created":"2024-01-24 17:57:27","title":"Kibble-Zurek mechanism and errors of gapped quantum phases","abstract":"Kibble-Zurek mechanism relates the domain of non-equilibrium dynamics with the critical properties at equilibrium. It establishes a power law connection between non-equilibrium defects quenched through a continuous phase transition and the quench rate via the scaling exponent. We present a novel numerical scheme to estimate the scaling exponent wherein the notion of defects is mapped to errors, previously introduced to quantify a variety of gapped quantum phases. To demonstrate the versatility of our method we conduct numerical experiments across a broad spectrum of spin-half models hosting local and symmetry protected topological order. Furthermore, an implementation of the quench dynamics featuring a topological phase transition on a digital quantum computer is proposed to quantify the associated criticality.","sentences":["Kibble-Zurek mechanism relates the domain of non-equilibrium dynamics with the critical properties at equilibrium.","It establishes a power law connection between non-equilibrium defects quenched through a continuous phase transition and the quench rate via the scaling exponent.","We present a novel numerical scheme to estimate the scaling exponent wherein the notion of defects is mapped to errors, previously introduced to quantify a variety of gapped quantum phases.","To demonstrate the versatility of our method we conduct numerical experiments across a broad spectrum of spin-half models hosting local and symmetry protected topological order.","Furthermore, an implementation of the quench dynamics featuring a topological phase transition on a digital quantum computer is proposed to quantify the associated criticality."],"url":"http://arxiv.org/abs/2401.13625v1","category":"quant-ph"}
{"created":"2024-01-24 17:54:55","title":"Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint","abstract":"Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations. However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \\textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint. Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) Linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough. iii) For regression, our results demonstrate that there also exist infinitely many overfitted DNNs with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error. Overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable. We hope our analysis will give a better understanding of the mathematical foundations of robustness in DNNs from an approximation view.","sentences":["Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations.","However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \\textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising.","In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint.","Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level.","ii) Linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough.","iii)","For regression, our results demonstrate that there also exist infinitely many overfitted DNNs with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error.","Overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable.","We hope our analysis will give a better understanding of the mathematical foundations of robustness in DNNs from an approximation view."],"url":"http://arxiv.org/abs/2401.13624v1","category":"stat.ML"}
{"created":"2024-01-24 17:52:24","title":"What Makes a Great Software Quality Assurance Engineer?","abstract":"Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities. In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end. Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked. As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great? We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world. We use the data collected from these activities to derive a comprehensive set of attributes that are considered important. As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes. Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills. This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice.","sentences":["Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities.","In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end.","Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked.","As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great?","We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world.","We use the data collected from these activities to derive a comprehensive set of attributes that are considered important.","As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes.","Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills.","This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice."],"url":"http://arxiv.org/abs/2401.13623v1","category":"cs.SE"}
{"created":"2024-01-24 17:51:06","title":"Cooperative Periodic Coverage With Collision Avoidance","abstract":"In this paper we propose a periodic solution to the problem of persistently covering a finite set of interest points with a group of autonomous mobile agents. These agents visit periodically the points and spend some time carrying out the coverage task, which we call coverage time. Since this periodic persistent coverage problem is NP-hard, we split it into three subproblems to counteract its complexity. In the first place, we plan individual closed paths for the agents to cover all the points. Second, we formulate a quadratically constrained linear program to find the optimal coverage times and actions that satisfy the coverage objective. Finally, we join together the individual plans of the agents in a periodic team plan by obtaining a schedule that guarantees collision avoidance. To this end, we solve a mixed integer linear program that minimizes the time in which two or more agents move at the same time. Eventually, we apply the proposed solution to an induction hob with mobile inductors for a domestic heating application and show its performance with experiments on a real prototype.","sentences":["In this paper we propose a periodic solution to the problem of persistently covering a finite set of interest points with a group of autonomous mobile agents.","These agents visit periodically the points and spend some time carrying out the coverage task, which we call coverage time.","Since this periodic persistent coverage problem is NP-hard, we split it into three subproblems to counteract its complexity.","In the first place, we plan individual closed paths for the agents to cover all the points.","Second, we formulate a quadratically constrained linear program to find the optimal coverage times and actions that satisfy the coverage objective.","Finally, we join together the individual plans of the agents in a periodic team plan by obtaining a schedule that guarantees collision avoidance.","To this end, we solve a mixed integer linear program that minimizes the time in which two or more agents move at the same time.","Eventually, we apply the proposed solution to an induction hob with mobile inductors for a domestic heating application and show its performance with experiments on a real prototype."],"url":"http://arxiv.org/abs/2401.13622v1","category":"cs.RO"}
{"created":"2024-01-24 17:48:45","title":"DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning","abstract":"Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in comparison to contrastive-learning-based methods. Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance. Our code is available at https://github.com/xinghaow99/DenoSent.","sentences":["Contrastive-learning-based methods have dominated sentence representation learning.","These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks.","However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples.","In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective.","By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form.","Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in comparison to contrastive-learning-based methods.","Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance.","Our code is available at https://github.com/xinghaow99/DenoSent."],"url":"http://arxiv.org/abs/2401.13621v1","category":"cs.CL"}
{"created":"2024-01-24 17:46:46","title":"A call for frugal modelling: two case studies involving molecular spin dynamics","abstract":"As scientists living through a climate emergency, we have a responsibility to lead by example, or to at least be consistent with our understanding of the problem, which in the case of theoreticians involves a frugal approach to modelling. Here we present and critically illustrate this principle. First, we compare two models of very different level of sophistication which nevertheless yield the same qualitative agreement with an experiment involving electric manipulation of molecular spin qubits while presenting a difference in cost of $>4$ orders of magnitude. As a second stage, an already minimalistic model involving the use of single-ion magnets to implement a network of probabilistic p-bits, programmed in two different programming languages, is shown to present a difference in cost of a factor of $\\simeq 50$. In both examples, the computationally expensive version of the model was the one that was published. As a community, we still have a lot of room for improvement in this direction.","sentences":["As scientists living through a climate emergency, we have a responsibility to lead by example, or to at least be consistent with our understanding of the problem, which in the case of theoreticians involves a frugal approach to modelling.","Here we present and critically illustrate this principle.","First, we compare two models of very different level of sophistication which nevertheless yield the same qualitative agreement with an experiment involving electric manipulation of molecular spin qubits while presenting a difference in cost of $>4$ orders of magnitude.","As a second stage, an already minimalistic model involving the use of single-ion magnets to implement a network of probabilistic p-bits, programmed in two different programming languages, is shown to present a difference in cost of a factor of $\\simeq 50$.","In both examples, the computationally expensive version of the model was the one that was published.","As a community, we still have a lot of room for improvement in this direction."],"url":"http://arxiv.org/abs/2401.13618v1","category":"physics.soc-ph"}
{"created":"2024-01-24 17:41:00","title":"Equitable Persistent Coverage of Non-Convex Environments with Graph-Based Planning","abstract":"In this paper we tackle the problem of persistently covering a complex non-convex environment with a team of robots. We consider scenarios where the coverage quality of the environment deteriorates with time, requiring to constantly revisit every point. As a first step, our solution finds a partition of the environment where the amount of work for each robot, weighted by the importance of each point, is equal. This is achieved using a power diagram and finding an equitable partition through a provably correct distributed control law on the power weights. Compared to other existing partitioning methods, our solution considers a continuous environment formulation with non-convex obstacles. In the second step, each robot computes a graph that gathers sweep-like paths and covers its entire partition. At each planning time, the coverage error at the graph vertices is assigned as weights of the corresponding edges. Then, our solution is capable of efficiently finding the optimal open coverage path through the graph with respect to the coverage error per distance traversed. Simulation and experimental results are presented to support our proposal.","sentences":["In this paper we tackle the problem of persistently covering a complex non-convex environment with a team of robots.","We consider scenarios where the coverage quality of the environment deteriorates with time, requiring to constantly revisit every point.","As a first step, our solution finds a partition of the environment where the amount of work for each robot, weighted by the importance of each point, is equal.","This is achieved using a power diagram and finding an equitable partition through a provably correct distributed control law on the power weights.","Compared to other existing partitioning methods, our solution considers a continuous environment formulation with non-convex obstacles.","In the second step, each robot computes a graph that gathers sweep-like paths and covers its entire partition.","At each planning time, the coverage error at the graph vertices is assigned as weights of the corresponding edges.","Then, our solution is capable of efficiently finding the optimal open coverage path through the graph with respect to the coverage error per distance traversed.","Simulation and experimental results are presented to support our proposal."],"url":"http://arxiv.org/abs/2401.13614v1","category":"cs.RO"}
{"created":"2024-01-24 17:35:38","title":"Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode","abstract":"Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications","sentences":["Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model.","CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding.","This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries.","By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification.","This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications"],"url":"http://arxiv.org/abs/2401.13613v1","category":"cs.CV"}
{"created":"2024-01-24 17:35:11","title":"Intermittent Connectivity Maintenance With Heterogeneous Robots","abstract":"We consider a scenario of cooperative task servicing, with a team of heterogeneous robots with different maximum speeds and communication radii, in charge of keeping the network intermittently connected. We abstract the task locations into a $1D$ cycle graph that is traversed by the communicating robots, and we discuss intermittent communication strategies so that each task location is periodically visited, with a worst--case revisiting time. Robots move forward and backward along the cycle graph, exchanging data with their previous and next neighbors when they meet, and updating their region boundaries. Asymptotically, each robot is in charge of a region of the cycle graph, depending on its capabilities. The method is distributed, and robots only exchange data when they meet.","sentences":["We consider a scenario of cooperative task servicing, with a team of heterogeneous robots with different maximum speeds and communication radii, in charge of keeping the network intermittently connected.","We abstract the task locations into a $1D$ cycle graph that is traversed by the communicating robots, and we discuss intermittent communication strategies so that each task location is periodically visited, with a worst--case revisiting time.","Robots move forward and backward along the cycle graph, exchanging data with their previous and next neighbors when they meet, and updating their region boundaries.","Asymptotically, each robot is in charge of a region of the cycle graph, depending on its capabilities.","The method is distributed, and robots only exchange data when they meet."],"url":"http://arxiv.org/abs/2401.13612v1","category":"cs.RO"}
{"created":"2024-01-24 17:31:07","title":"Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models","abstract":"Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.","sentences":["Neural networks have been successfully used for non-intrusive speech intelligibility prediction.","Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task.","This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users.","Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7."],"url":"http://arxiv.org/abs/2401.13611v1","category":"cs.SD"}
{"created":"2024-01-24 17:29:25","title":"Scale-free vision-based aerial control of a ground formation with hybrid topology","abstract":"We present a novel vision-based control method to make a group of ground mobile robots achieve a specified formation shape with unspecified size. Our approach uses multiple aerial control units equipped with downward-facing cameras, each observing a partial subset of the multirobot team. The units compute the control commands from the ground robots' image projections, using neither calibration nor scene scale information, and transmit them to the robots. The control strategy relies on the calculation of image similarity transformations, and we show it to be asymptotically stable if the overlaps between the subsets of controlled robots satisfy certain conditions. The presence of the supervisory units, which coordinate their motions to guarantee a correct control performance, gives rise to a hybrid system topology. All in all, the proposed system provides relevant practical advantages in simplicity and flexibility. Within the problem of controlling a team shape, our contribution lies in addressing several simultaneous challenges: the controller needs only partial information of the robotic group, does not use distance measurements or global reference frames, is designed for unicycle agents, and can accommodate topology changes. We present illustrative simulation results.","sentences":["We present a novel vision-based control method to make a group of ground mobile robots achieve a specified formation shape with unspecified size.","Our approach uses multiple aerial control units equipped with downward-facing cameras, each observing a partial subset of the multirobot team.","The units compute the control commands from the ground robots' image projections, using neither calibration nor scene scale information, and transmit them to the robots.","The control strategy relies on the calculation of image similarity transformations, and we show it to be asymptotically stable if the overlaps between the subsets of controlled robots satisfy certain conditions.","The presence of the supervisory units, which coordinate their motions to guarantee a correct control performance, gives rise to a hybrid system topology.","All in all, the proposed system provides relevant practical advantages in simplicity and flexibility.","Within the problem of controlling a team shape, our contribution lies in addressing several simultaneous challenges: the controller needs only partial information of the robotic group, does not use distance measurements or global reference frames, is designed for unicycle agents, and can accommodate topology changes.","We present illustrative simulation results."],"url":"http://arxiv.org/abs/2401.13610v1","category":"cs.RO"}
{"created":"2024-01-24 17:27:08","title":"Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion","abstract":"Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality.","sentences":["Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective.","While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms.","This leads to a foundation for personalized recommendations of learning paths.","In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated.","We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model.","We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones.","The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality."],"url":"http://arxiv.org/abs/2401.13609v1","category":"cs.IR"}
{"created":"2024-01-24 17:26:14","title":"A bialgebra theory of Gel'fand-Dorfman algebras with applications to Lie conformal bialgebras","abstract":"Gel'fand-Dorfman algebras (GD algebras) give a natural construction of Lie conformal algebras and are in turn characterized by this construction. In this paper, we define the Gel'fand-Dorfman bialgebra (GD bialgebras) and enrich the above construction to a construction of Lie conformal bialgebras by GD bialgebras. As a special case, Novikov bialgebras yield Lie conformal bialgebras. We further introduce the notion of the Gel'fand-Dorfman Yang-Baxter equation (GDYBE), whose skew-symmetric solutions produce GD bialgebras. Moreover, the notions of $\\mathcal{O}$-operators on GD algebras and pre-Gel'fand-Dorfman algebras (pre-GD algebras) are introduced to provide skew-symmetric solutions of the GDYBE. The relationships between these notions for GD algebras and the corresponding ones for Lie conformal algebras are given. In particular, there is a natural construction of Lie conformal bialgebras from pre-GD algebras. Finally, GD bialgebras are characterized by certain matched pairs and Manin triples of GD algebras.","sentences":["Gel'fand-Dorfman algebras (GD algebras) give a natural construction of Lie conformal algebras and are in turn characterized by this construction.","In this paper, we define the Gel'fand-Dorfman bialgebra (GD bialgebras) and enrich the above construction to a construction of Lie conformal bialgebras by GD bialgebras.","As a special case, Novikov bialgebras yield Lie conformal bialgebras.","We further introduce the notion of the Gel'fand-Dorfman Yang-Baxter equation (GDYBE), whose skew-symmetric solutions produce GD bialgebras.","Moreover, the notions of $\\mathcal{O}$-operators on GD algebras and pre-Gel'fand-Dorfman algebras (pre-GD algebras) are introduced to provide skew-symmetric solutions of the GDYBE.","The relationships between these notions for GD algebras and the corresponding ones for Lie conformal algebras are given.","In particular, there is a natural construction of Lie conformal bialgebras from pre-GD algebras.","Finally, GD bialgebras are characterized by certain matched pairs and Manin triples of GD algebras."],"url":"http://arxiv.org/abs/2401.13608v1","category":"math.QA"}
{"created":"2024-01-24 17:22:33","title":"Regulating AI-Based Remote Biometric Identification. Investigating the Public Demand for Bans, Audits, and Public Database Registrations","abstract":"AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but are criticised for inheriting biases and violating fundamental human rights. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI. As a possible counterweight, public opinion can have a decisive influence on policymakers to establish boundaries and conditions under which AI systems should be used -- if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems.","sentences":["AI is increasingly being used in the public sector, including public security.","In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology.","RBI systems are used to identify criminal activity in public spaces, but are criticised for inheriting biases and violating fundamental human rights.","It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised.","While there is a consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI.","As a possible counterweight, public opinion can have a decisive influence on policymakers to establish boundaries and conditions under which AI systems should be used -- if at all.","However, we know little about the conditions that lead to regulatory demand for AI systems.","In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology.","In addition, we explore the mediating effects of discrimination perceptions regarding RBI.","We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens.","We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation.","Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems."],"url":"http://arxiv.org/abs/2401.13605v1","category":"cs.CY"}
{"created":"2024-01-24 17:14:50","title":"Stream-based perception for cognitive agents in mobile ecosystems","abstract":"Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements. Experiments with real smartphone data demonstrate the benefits of stream-based agent perception.","sentences":["Cognitive agent abstractions can help to engineer intelligent systems across mobile devices.","On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation.","Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data.","Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation.","In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts.","In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams.","We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations.","We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements.","Experiments with real smartphone data demonstrate the benefits of stream-based agent perception."],"url":"http://arxiv.org/abs/2401.13604v1","category":"cs.AI"}
{"created":"2024-01-24 17:10:45","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models","abstract":"In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.","sentences":["In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies.","The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks.","In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs.","Specifically, we first outline general design formulations for model architecture and training pipeline.","Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations.","Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs.","Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field.","We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain."],"url":"http://arxiv.org/abs/2401.13601v1","category":"cs.CL"}
{"created":"2024-01-24 17:04:28","title":"Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction","abstract":"Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.","sentences":["Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document.","Existing methods heavily rely on a substantial amount of fully labeled data.","However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive.","Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations.","In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK.","Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step.","To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge.","Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.","We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets.","The experimental results illustrate that our GenRDK framework outperforms strong baselines."],"url":"http://arxiv.org/abs/2401.13598v1","category":"cs.CL"}
{"created":"2024-01-24 17:03:03","title":"Emergent Holographic Forces from Tensor Networks and Criticality","abstract":"The AdS/CFT correspondence stipulates a duality between conformal field theories and certain theories of quantum gravity in one higher spatial dimension. However, probing this conjecture on contemporary classical or quantum computers is challenging. We formulate an efficiently implementable multi-scale entanglement renormalization ansatz (MERA) model of AdS/CFT providing a mapping between a (1+1)-dimensional critical spin system and a (2+1)-dimensional bulk theory. Using a combination of numerics and analytics, we show that the bulk theory arising from this optimized tensor network furnishes excitations with attractive interactions. Remarkably, these excitations have one- and two-particle energies matching the predictions for matter coupled to AdS gravity at long distances, thus displaying key features of AdS physics. We show that these potentials arise as a direct consequence of entanglement renormalization and discuss how this approach can be used to efficiently simulate bulk dynamics using realistic quantum devices.","sentences":["The AdS/CFT correspondence stipulates a duality between conformal field theories and certain theories of quantum gravity in one higher spatial dimension.","However, probing this conjecture on contemporary classical or quantum computers is challenging.","We formulate an efficiently implementable multi-scale entanglement renormalization ansatz (MERA) model of AdS/CFT providing a mapping between a (1+1)-dimensional critical spin system and a (2+1)-dimensional bulk theory.","Using a combination of numerics and analytics, we show that the bulk theory arising from this optimized tensor network furnishes excitations with attractive interactions.","Remarkably, these excitations have one- and two-particle energies matching the predictions for matter coupled to AdS gravity at long distances, thus displaying key features of AdS physics.","We show that these potentials arise as a direct consequence of entanglement renormalization and discuss how this approach can be used to efficiently simulate bulk dynamics using realistic quantum devices."],"url":"http://arxiv.org/abs/2401.13595v1","category":"quant-ph"}
{"created":"2024-01-24 17:01:42","title":"Graph Guided Question Answer Generation for Procedural Question-Answering","abstract":"In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller. 2) semantic coverage is the key indicator for downstream QA performance. Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model. In contrast, the higher semantic coverage provided by our method is critical for QA performance.","sentences":["In this paper, we focus on task-specific question answering (QA).","To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants.","The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data.","While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model.","In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs.","We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner.","Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller.","2) semantic coverage is the key indicator for downstream QA performance.","Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model.","In contrast, the higher semantic coverage provided by our method is critical for QA performance."],"url":"http://arxiv.org/abs/2401.13594v1","category":"cs.CL"}
{"created":"2024-01-24 17:00:39","title":"Enhanced quantum control of individual ultracold molecules using optical tweezer arrays","abstract":"Control over the quantum states of individual molecules is crucial in the quest to harness their rich internal structure and dipolar interactions for applications in quantum science. In this paper, we develop a toolbox of techniques for the control and readout of individually trapped polar molecules in an array of optical tweezers. Starting with arrays of up to eight Rb and eight Cs atoms, we assemble arrays of RbCs molecules in their rovibrational and hyperfine ground state with an overall efficiency of 48(2)%. We demonstrate global microwave control of multiple rotational states of the molecules and use an auxiliary tweezer array to implement site-resolved addressing and state control. We show how the rotational state of the molecule can be mapped onto the position of Rb atoms and use this capability to readout multiple rotational states in a single experimental run. Further, using a scheme for the mid-sequence detection of molecule formation errors, we perform rearrangement of assembled molecules to prepare small defect-free arrays. Finally, we discuss a feasible route to scaling to larger arrays of molecules.","sentences":["Control over the quantum states of individual molecules is crucial in the quest to harness their rich internal structure and dipolar interactions for applications in quantum science.","In this paper, we develop a toolbox of techniques for the control and readout of individually trapped polar molecules in an array of optical tweezers.","Starting with arrays of up to eight Rb and eight Cs atoms, we assemble arrays of RbCs molecules in their rovibrational and hyperfine ground state with an overall efficiency of 48(2)%.","We demonstrate global microwave control of multiple rotational states of the molecules and use an auxiliary tweezer array to implement site-resolved addressing and state control.","We show how the rotational state of the molecule can be mapped onto the position of Rb atoms and use this capability to readout multiple rotational states in a single experimental run.","Further, using a scheme for the mid-sequence detection of molecule formation errors, we perform rearrangement of assembled molecules to prepare small defect-free arrays.","Finally, we discuss a feasible route to scaling to larger arrays of molecules."],"url":"http://arxiv.org/abs/2401.13593v1","category":"physics.atom-ph"}
{"created":"2024-01-24 16:52:37","title":"Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes","abstract":"The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an in-depth analysis. Results: GPT-4 showed overall superior performance compared to other LLMs. In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed. The GPT family models have demonstrated considerable efficiency, evidenced by their cost-effectiveness and time-saving capabilities. Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized. This framework goes beyond singular performance aspects. With expert annotations, this methodology not only validates LLMs' capabilities in processing complex medical data but also establishes a benchmark for future LLM evaluations across specialized domains.","sentences":["The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance.","However, their performance in actual clinical applications has been underexplored.","Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts.","This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings.","Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication.","Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes.","Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians.","Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an in-depth analysis.","Results:","GPT-4 showed overall superior performance compared to other LLMs.","In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed.","The GPT family models have demonstrated considerable efficiency, evidenced by their cost-effectiveness and time-saving capabilities.","Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized.","This framework goes beyond singular performance aspects.","With expert annotations, this methodology not only validates LLMs' capabilities in processing complex medical data but also establishes a benchmark for future LLM evaluations across specialized domains."],"url":"http://arxiv.org/abs/2401.13588v1","category":"cs.CL"}
{"created":"2024-01-24 16:51:42","title":"Deep Learning Based Adaptive Joint mmWave Beam Alignment","abstract":"The challenging propagation environment, combined with the hardware limitations of mmWave systems, gives rise to the need for accurate initial access beam alignment strategies with low latency and high achievable beamforming gain. Much of the recent work in this area either focuses on one-sided beam alignment, or, joint beam alignment methods where both sides of the link perform a sequence of fixed channel probing steps. Codebook-based non-adaptive beam alignment schemes have the potential to allow multiple user equipment (UE) to perform initial access beam alignment in parallel whereas adaptive schemes are favourable in achievable beamforming gain. This work introduces a novel deep learning based joint beam alignment scheme that aims to combine the benefits of adaptive, codebook-free beam alignment at the UE side with the advantages of a codebook-sweep based scheme at the base station. The proposed end-to-end trainable scheme is compatible with current cellular standard signaling and can be readily integrated into the standard without requiring significant changes to it. Extensive simulations demonstrate superior performance of the proposed approach over purely codebook-based ones.","sentences":["The challenging propagation environment, combined with the hardware limitations of mmWave systems, gives rise to the need for accurate initial access beam alignment strategies with low latency and high achievable beamforming gain.","Much of the recent work in this area either focuses on one-sided beam alignment, or, joint beam alignment methods where both sides of the link perform a sequence of fixed channel probing steps.","Codebook-based non-adaptive beam alignment schemes have the potential to allow multiple user equipment (UE) to perform initial access beam alignment in parallel whereas adaptive schemes are favourable in achievable beamforming gain.","This work introduces a novel deep learning based joint beam alignment scheme that aims to combine the benefits of adaptive, codebook-free beam alignment at the UE side with the advantages of a codebook-sweep based scheme at the base station.","The proposed end-to-end trainable scheme is compatible with current cellular standard signaling and can be readily integrated into the standard without requiring significant changes to it.","Extensive simulations demonstrate superior performance of the proposed approach over purely codebook-based ones."],"url":"http://arxiv.org/abs/2401.13587v1","category":"cs.IT"}
{"created":"2024-01-24 16:51:23","title":"Prompt Weight Experiments for LLM Instruction Fine-Tuning","abstract":"We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.","sentences":["We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks.","We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets.","We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW."],"url":"http://arxiv.org/abs/2401.13586v1","category":"cs.LG"}
{"created":"2024-01-24 16:50:54","title":"Securing the Invisible Thread: A Comprehensive Analysis of BLE Tracker Security in Apple AirTags and Samsung SmartTags","abstract":"This study presents an in-depth analysis of the security landscape in Bluetooth Low Energy (BLE) tracking systems, with a particular emphasis on Apple AirTags and Samsung SmartTags, including their cryptographic frameworks. Our investigation traverses a wide spectrum of attack vectors such as physical tampering, firmware exploitation, signal spoofing, eavesdropping, jamming, app security flaws, Bluetooth security weaknesses, location spoofing, threats to owner devices, and cloud-related vulnerabilities. Moreover, we delve into the security implications of the cryptographic methods utilized in these systems. Our findings reveal that while BLE trackers like AirTags and SmartTags offer substantial utility, they also pose significant security risks. Notably, Apple's approach, which prioritizes user privacy by removing intermediaries, inadvertently leads to device authentication challenges, evidenced by successful AirTag spoofing instances. Conversely, Samsung SmartTags, designed to thwart beacon spoofing, raise critical concerns about cloud security and user privacy. Our analysis also highlights the constraints faced by these devices due to their design focus on battery life conservation, particularly the absence of secure boot processes, which leaves them susceptible to OS modification and a range of potential attacks. The paper concludes with insights into the anticipated evolution of these tracking systems. We predict that future enhancements will likely focus on bolstering security features, especially as these devices become increasingly integrated into the broader IoT ecosystem and face evolving privacy regulations. This shift is imperative to address the intricate balance between functionality and security in next-generation BLE tracking systems.","sentences":["This study presents an in-depth analysis of the security landscape in Bluetooth Low Energy (BLE) tracking systems, with a particular emphasis on Apple AirTags and Samsung SmartTags, including their cryptographic frameworks.","Our investigation traverses a wide spectrum of attack vectors such as physical tampering, firmware exploitation, signal spoofing, eavesdropping, jamming, app security flaws, Bluetooth security weaknesses, location spoofing, threats to owner devices, and cloud-related vulnerabilities.","Moreover, we delve into the security implications of the cryptographic methods utilized in these systems.","Our findings reveal that while BLE trackers like AirTags and SmartTags offer substantial utility, they also pose significant security risks.","Notably, Apple's approach, which prioritizes user privacy by removing intermediaries, inadvertently leads to device authentication challenges, evidenced by successful AirTag spoofing instances.","Conversely, Samsung SmartTags, designed to thwart beacon spoofing, raise critical concerns about cloud security and user privacy.","Our analysis also highlights the constraints faced by these devices due to their design focus on battery life conservation, particularly the absence of secure boot processes, which leaves them susceptible to OS modification and a range of potential attacks.","The paper concludes with insights into the anticipated evolution of these tracking systems.","We predict that future enhancements will likely focus on bolstering security features, especially as these devices become increasingly integrated into the broader IoT ecosystem and face evolving privacy regulations.","This shift is imperative to address the intricate balance between functionality and security in next-generation BLE tracking systems."],"url":"http://arxiv.org/abs/2401.13584v1","category":"cs.CR"}
{"created":"2024-01-24 16:45:42","title":"Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation","abstract":"Previous contrastive deep clustering methods mostly focus on instance-level information while overlooking the member relationship within groups/clusters, which may significantly undermine their representation learning and clustering capability. Recently, some group-contrastive methods have been developed, which, however, typically rely on the samples of the entire dataset to obtain pseudo labels and lack the ability to efficiently update the group assignments in a batch-wise manner. To tackle these critical issues, we present a novel end-to-end deep clustering framework with dynamic grouping and prototype aggregation, termed as DigPro. Specifically, the proposed dynamic grouping extends contrastive learning from instance-level to group-level, which is effective and efficient for timely updating groups. Meanwhile, we perform contrastive learning on prototypes in a spherical feature space, termed as prototype aggregation, which aims to maximize the inter-cluster distance. Notably, with an expectation-maximization framework, DigPro simultaneously takes advantage of compact intra-cluster connections, well-separated clusters, and efficient group updating during the self-supervised training. Extensive experiments on six image benchmarks demonstrate the superior performance of our approach over the state-of-the-art. Code is available at https://github.com/Regan-Zhang/DigPro.","sentences":["Previous contrastive deep clustering methods mostly focus on instance-level information while overlooking the member relationship within groups/clusters, which may significantly undermine their representation learning and clustering capability.","Recently, some group-contrastive methods have been developed, which, however, typically rely on the samples of the entire dataset to obtain pseudo labels and lack the ability to efficiently update the group assignments in a batch-wise manner.","To tackle these critical issues, we present a novel end-to-end deep clustering framework with dynamic grouping and prototype aggregation, termed as DigPro.","Specifically, the proposed dynamic grouping extends contrastive learning from instance-level to group-level, which is effective and efficient for timely updating groups.","Meanwhile, we perform contrastive learning on prototypes in a spherical feature space, termed as prototype aggregation, which aims to maximize the inter-cluster distance.","Notably, with an expectation-maximization framework, DigPro simultaneously takes advantage of compact intra-cluster connections, well-separated clusters, and efficient group updating during the self-supervised training.","Extensive experiments on six image benchmarks demonstrate the superior performance of our approach over the state-of-the-art.","Code is available at https://github.com/Regan-Zhang/DigPro."],"url":"http://arxiv.org/abs/2401.13581v1","category":"cs.CV"}
{"created":"2024-01-24 16:43:35","title":"WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition","abstract":"This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works.","sentences":["This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack.","In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference.","Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods.","In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings.","We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions.","Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset.","Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods.","Besides, we also provide visualization analyses to explain why our method works."],"url":"http://arxiv.org/abs/2401.13578v1","category":"cs.CR"}
{"created":"2024-01-24 16:40:43","title":"Influence of initiators on the tipping point in the extended Watts model","abstract":"In this paper, we study how the influence of initiators (seeds) affects the tipping point of information cascades in networks. We consider an extended version of the Watts model, in which each node is either active (i.e., having adopted an innovation) or inactive. In this extended model, the adoption threshold, defined as the fraction of active neighbors required for an inactive node to become active, depends on whether the node is a seed neighbor (i.e., connected to one or more initiators) or an ordinary node (i.e., not connected to any initiators). Using the tree approximation on random graphs, we determine the tipping point, at which the fraction of active nodes in the final state increases discontinuously with an increasing seed fraction. The occurrence of a tipping point and the scale of cascades depend on two factors: whether a giant component of seed neighbors is formed when the seed fraction is large enough to trigger cascades among seed neighbors, and whether the giant component of ordinary nodes is maintained when newly activated nodes trigger further activations among ordinary nodes. The coexistence of two giant components suggests that a tipping point can appear twice. We present an example demonstrating the existence of two tipping points when there is a gap between the adoption thresholds of seed neighbors and ordinary nodes. Monte Carlo simulations clearly show that the first cascade, occurring at a small tipping point, occurs in the giant component of seed neighbors, while the second cascade, occurring at a larger tipping point, extends into the giant component of ordinary nodes.","sentences":["In this paper, we study how the influence of initiators (seeds) affects the tipping point of information cascades in networks.","We consider an extended version of the Watts model, in which each node is either active (i.e., having adopted an innovation) or inactive.","In this extended model, the adoption threshold, defined as the fraction of active neighbors required for an inactive node to become active, depends on whether the node is a seed neighbor (i.e., connected to one or more initiators) or an ordinary node (i.e., not connected to any initiators).","Using the tree approximation on random graphs, we determine the tipping point, at which the fraction of active nodes in the final state increases discontinuously with an increasing seed fraction.","The occurrence of a tipping point and the scale of cascades depend on two factors: whether a giant component of seed neighbors is formed when the seed fraction is large enough to trigger cascades among seed neighbors, and whether the giant component of ordinary nodes is maintained when newly activated nodes trigger further activations among ordinary nodes.","The coexistence of two giant components suggests that a tipping point can appear twice.","We present an example demonstrating the existence of two tipping points when there is a gap between the adoption thresholds of seed neighbors and ordinary nodes.","Monte Carlo simulations clearly show that the first cascade, occurring at a small tipping point, occurs in the giant component of seed neighbors, while the second cascade, occurring at a larger tipping point, extends into the giant component of ordinary nodes."],"url":"http://arxiv.org/abs/2401.13576v1","category":"physics.soc-ph"}
{"created":"2024-01-24 16:40:30","title":"CNN architecture extraction on edge GPU","abstract":"Neural networks have become popular due to their versatility and state-of-the-art results in many applications, such as image classification, natural language processing, speech recognition, forecasting, etc. These applications are also used in resource-constrained environments such as embedded devices. In this work, the susceptibility of neural network implementations to reverse engineering is explored on the NVIDIA Jetson Nano microcomputer via side-channel analysis. To this end, an architecture extraction attack is presented. In the attack, 15 popular convolutional neural network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is analyzed during the inference operation of the neural networks. The results of the analysis show that neural network architectures are easily distinguishable using deep learning-based side-channel analysis.","sentences":["Neural networks have become popular due to their versatility and state-of-the-art results in many applications, such as image classification, natural language processing, speech recognition, forecasting, etc.","These applications are also used in resource-constrained environments such as embedded devices.","In this work, the susceptibility of neural network implementations to reverse engineering is explored on the NVIDIA Jetson Nano microcomputer via side-channel analysis.","To this end, an architecture extraction attack is presented.","In the attack, 15 popular convolutional neural network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is analyzed during the inference operation of the neural networks.","The results of the analysis show that neural network architectures are easily distinguishable using deep learning-based side-channel analysis."],"url":"http://arxiv.org/abs/2401.13575v1","category":"cs.CR"}
{"created":"2024-01-24 16:34:27","title":"Distributed matrix multiplication with straggler tolerance using algebraic function fields","abstract":"The problem of straggler mitigation in distributed matrix multiplication (DMM) is considered for a large number of worker nodes and a fixed small finite field. Polynomial codes and matdot codes are generalized by making use of algebraic function fields (i.e., algebraic functions over an algebraic curve) over a finite field. The construction of optimal solutions is translated to a combinatorial problem on the Weierstrass semigroups of the corresponding algebraic curves. Optimal or almost optimal solutions are provided. These have the same computational complexity per worker as classical polynomial and matdot codes, and their recovery thresholds are almost optimal in the asymptotic regime (growing number of workers and a fixed finite field).","sentences":["The problem of straggler mitigation in distributed matrix multiplication (DMM) is considered for a large number of worker nodes and a fixed small finite field.","Polynomial codes and matdot codes are generalized by making use of algebraic function fields (i.e., algebraic functions over an algebraic curve) over a finite field.","The construction of optimal solutions is translated to a combinatorial problem on the Weierstrass semigroups of the corresponding algebraic curves.","Optimal or almost optimal solutions are provided.","These have the same computational complexity per worker as classical polynomial and matdot codes, and their recovery thresholds are almost optimal in the asymptotic regime (growing number of workers and a fixed finite field)."],"url":"http://arxiv.org/abs/2401.13573v1","category":"cs.IT"}
{"created":"2024-01-24 16:31:50","title":"Guided Diffusion for Fast Inverse Design of Density-based Mechanical Metamaterials","abstract":"Mechanical metamaterial is a synthetic material that can possess extraordinary physical characteristics, such as abnormal elasticity, stiffness, and stability, by carefully designing its internal structure. To make metamaterials contain delicate local structures with unique mechanical properties, it is a potential method to represent them through high-resolution voxels. However, it brings a substantial computational burden. To this end, this paper proposes a fast inverse design method, whose core is an advanced deep generative AI algorithm, to generate voxel-based mechanical metamaterials. Specifically, we use the self-conditioned diffusion model, capable of generating a microstructure with a resolution of $128^3$ to approach the specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid reverse design tool facilitates the exploration of extreme metamaterials, the sequence interpolation in metamaterials, and the generation of diverse microstructures for multi-scale design. This flexible and adaptive generative tool is of great value in structural engineering or other mechanical systems and can stimulate more subsequent research.","sentences":["Mechanical metamaterial is a synthetic material that can possess extraordinary physical characteristics, such as abnormal elasticity, stiffness, and stability, by carefully designing its internal structure.","To make metamaterials contain delicate local structures with unique mechanical properties, it is a potential method to represent them through high-resolution voxels.","However, it brings a substantial computational burden.","To this end, this paper proposes a fast inverse design method, whose core is an advanced deep generative AI algorithm, to generate voxel-based mechanical metamaterials.","Specifically, we use the self-conditioned diffusion model, capable of generating a microstructure with a resolution of $128^3$ to approach the specified homogenized tensor matrix in just 3 seconds.","Accordingly, this rapid reverse design tool facilitates the exploration of extreme metamaterials, the sequence interpolation in metamaterials, and the generation of diverse microstructures for multi-scale design.","This flexible and adaptive generative tool is of great value in structural engineering or other mechanical systems and can stimulate more subsequent research."],"url":"http://arxiv.org/abs/2401.13570v1","category":"cs.CE"}
{"created":"2024-01-24 16:30:21","title":"SPARC-LoRa: A Scalable, Power-efficient, Affordable, Reliable, and Cloud Service-enabled LoRa Networking System for Agriculture Applications","abstract":"With the rapid development of cloud and edge computing, Internet of Things (IoT) applications have been deployed in various aspects of human life. In this paper, we design and implement a holistic LoRa-based IoT system with LoRa communication capabilities, named SPARC-LoRa, which consists of field sensor nodes and a gateway connected to the Internet. SPARC-LoRa has the following important features. First, the proposed wireless network of SPARC-LoRa is even-driven and using off-the-shelf microcontroller and LoRa communication modules with a customized PCB design to integrate all the hardware. This enables SPARC-LoRa to achieve low power consumption, long range communication, and low cost. With a new connection-based upper layer protocol design, the scalability and communication reliability of SPARC-loRa can be achieved. Second, an open source software including sensor nodes and servers is designed based on Docker container with cloud storage, computing, and LTE functionalities. In order to achieve reliable wireless communication under extreme conditions, a relay module is designed and applied to SPARC-LoRa to forward the data from sensor nodes to the gateway node. The system design and implementation is completely open source and hosted on the DigitalOcean Droplet Cloud. Hence, the proposed system enables further research and applications in both academia and industry. The proposed system has been tested in real fields under different and extreme environmental conditions in Salt Lake City, Utah and the University of Nebraska-Lincoln. The experimental results validate the features of SPARC-LoRa including low power, reliability, and cloud services provided by SPARC-LoRa.","sentences":["With the rapid development of cloud and edge computing, Internet of Things (IoT) applications have been deployed in various aspects of human life.","In this paper, we design and implement a holistic LoRa-based IoT system with LoRa communication capabilities, named SPARC-LoRa, which consists of field sensor nodes and a gateway connected to the Internet.","SPARC-LoRa has the following important features.","First, the proposed wireless network of SPARC-LoRa is even-driven and using off-the-shelf microcontroller and LoRa communication modules with a customized PCB design to integrate all the hardware.","This enables SPARC-LoRa to achieve low power consumption, long range communication, and low cost.","With a new connection-based upper layer protocol design, the scalability and communication reliability of SPARC-loRa can be achieved.","Second, an open source software including sensor nodes and servers is designed based on Docker container with cloud storage, computing, and LTE functionalities.","In order to achieve reliable wireless communication under extreme conditions, a relay module is designed and applied to SPARC-LoRa to forward the data from sensor nodes to the gateway node.","The system design and implementation is completely open source and hosted on the DigitalOcean Droplet Cloud.","Hence, the proposed system enables further research and applications in both academia and industry.","The proposed system has been tested in real fields under different and extreme environmental conditions in Salt Lake City, Utah and the University of Nebraska-Lincoln.","The experimental results validate the features of SPARC-LoRa including low power, reliability, and cloud services provided by SPARC-LoRa."],"url":"http://arxiv.org/abs/2401.13569v1","category":"cs.NI"}
{"created":"2024-01-24 16:29:42","title":"Investigating the Performance of Soft Robotic Adaptive Feet with Longitudinal and Transverse Arches","abstract":"Biped robots usually adopt feet with a rigid structure that simplifies walking on flat grounds and yet hinders ground adaptation in unstructured environments, thus jeopardizing stability. We recently explored in the SoftFoot the idea of adapting a robotic foot to ground irregularities along the sagittal plane. Building on the previous results, we propose in this paper a novel robotic foot able to adapt both in the sagittal and frontal planes, similarly to the human foot. It features five parallel modules with intrinsic longitudinal adaptability that can be combined in many possible designs through optional rigid or elastic connections. By following a methodological design approach, we narrow down the design space to five candidate foot designs and implement them on a modular system. Prototypes are tested experimentally via controlled application of force, through a robotic arm, onto a sensorized plate endowed with different obstacles. Their performance is compared, using also a rigid foot and the previous SoftFoot as a baseline. Analysis of footprint stability shows that the introduction of the transverse arch, by elastically connecting the five parallel modules, is advantageous for obstacle negotiation, especially when obstacles are located under the forefoot. In addition to biped robots' locomotion, this finding might also benefit lower-limb prostheses design.","sentences":["Biped robots usually adopt feet with a rigid structure that simplifies walking on flat grounds and yet hinders ground adaptation in unstructured environments, thus jeopardizing stability.","We recently explored in the SoftFoot the idea of adapting a robotic foot to ground irregularities along the sagittal plane.","Building on the previous results, we propose in this paper a novel robotic foot able to adapt both in the sagittal and frontal planes, similarly to the human foot.","It features five parallel modules with intrinsic longitudinal adaptability that can be combined in many possible designs through optional rigid or elastic connections.","By following a methodological design approach, we narrow down the design space to five candidate foot designs and implement them on a modular system.","Prototypes are tested experimentally via controlled application of force, through a robotic arm, onto a sensorized plate endowed with different obstacles.","Their performance is compared, using also a rigid foot and the previous SoftFoot as a baseline.","Analysis of footprint stability shows that the introduction of the transverse arch, by elastically connecting the five parallel modules, is advantageous for obstacle negotiation, especially when obstacles are located under the forefoot.","In addition to biped robots' locomotion, this finding might also benefit lower-limb prostheses design."],"url":"http://arxiv.org/abs/2401.13568v1","category":"cs.RO"}
{"created":"2024-01-24 16:25:02","title":"Self-mirror Large Volume Scenario with de Sitter","abstract":"The large volume scenario has been an important issue for non-geometric flux compactifications. As one solution to this issue, we investigate in self-mirror Calabi-Yau flux compactification with large volume scenario visited. In particular, at the large volume limit, the non-perturbative terms contribute to the effective scalar potential in the order of $\\mathcal{O}\\left(\\frac{1}{\\mathcal{V}^2}\\right)$ with same order as from F-term $\\frac{D W. DW}{\\mathcal{V}^2}$, while the $\\alpha'$-corrections are trivialized due to the self-mirror Calabi-Yau construction. At the large volume limit, the $\\mathcal{O}\\left(\\frac{1}{\\mathcal{V}^2}\\right)$ order uplift term takes a dominant role of the non-perturbative contribution to effective scalar potential with possible de Sitter vacuum.","sentences":["The large volume scenario has been an important issue for non-geometric flux compactifications.","As one solution to this issue, we investigate in self-mirror Calabi-Yau flux compactification with large volume scenario visited.","In particular, at the large volume limit, the non-perturbative terms contribute to the effective scalar potential in the order of $\\mathcal{O}\\left(\\frac{1}{\\mathcal{V}^2}\\right)$ with same order as from F-term $\\frac{D W. DW}{\\mathcal{V}^2}$, while the $\\alpha'$-corrections are trivialized due to the self-mirror Calabi-Yau construction.","At the large volume limit, the $\\mathcal{O}\\left(\\frac{1}{\\mathcal{V}^2}\\right)$ order uplift term takes a dominant role of the non-perturbative contribution to effective scalar potential with possible de Sitter vacuum."],"url":"http://arxiv.org/abs/2401.13567v1","category":"hep-th"}
{"created":"2024-01-24 16:23:14","title":"A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in Recommendation","abstract":"When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure.","sentences":["When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups.","In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results.","Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists.","In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models.","This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity.","Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility.","Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure."],"url":"http://arxiv.org/abs/2401.13566v1","category":"cs.IR"}
{"created":"2024-01-24 16:21:28","title":"Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding","abstract":"In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.   Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.   Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (Malay grammar) test set, particularly when fine-tuned with instructions.   All models released at https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c","sentences":["In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens.","We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.   ","Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities.","Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.   ","Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2.","We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (Malay grammar) test set, particularly when fine-tuned with instructions.   ","All models released at https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c"],"url":"http://arxiv.org/abs/2401.13565v1","category":"cs.CL"}
{"created":"2024-01-24 16:21:14","title":"RIS Empowered Near-Field Covert Communications","abstract":"This paper studies an extremely large-scale reconfigurable intelligent surface (XL-RIS) empowered covert communication system in the near-field region. Alice covertly transmits messages to Bob with the assistance of the XL-RIS, while evading detection by Willie. To enhance the covert communication performance, we maximize the achievable covert rate by jointly optimizing the hybrid analog and digital beamformers at Alice, as well as the reflection coefficient matrix at the XL-RIS. An alternating optimization algorithm is proposed to solve the joint beamforming design problem. For the hybrid beamformer design, a semi-closed-form solution for fully digital beamformer is first obtained by a weighted minimum mean-square error based algorithm, then the baseband digital and analog beamformers at Alice are designed by approximating the fully digital beamformer via manifold optimization. For the XL-RIS's reflection coefficient matrix design, a low-complexity alternating direction method of multipliers based algorithm is proposed to address the challenge of large-scale variables and unit-modulus constraints. Numerical results unveil that i) the near-field communications can achieve a higher covert rate than the far-field covert communications in general, and still realize covert transmission even if Willie is located at the same direction as Bob and closer to the XL-RIS; ii) the proposed algorithm can enhance the covert rate significantly compared to the benchmark schemes; iii) the proposed algorithm leads to a beam diffraction pattern that can bypass Willie and achieve high-rate covert transmission to Bob.","sentences":["This paper studies an extremely large-scale reconfigurable intelligent surface (XL-RIS) empowered covert communication system in the near-field region.","Alice covertly transmits messages to Bob with the assistance of the XL-RIS, while evading detection by Willie.","To enhance the covert communication performance, we maximize the achievable covert rate by jointly optimizing the hybrid analog and digital beamformers at Alice, as well as the reflection coefficient matrix at the XL-RIS.","An alternating optimization algorithm is proposed to solve the joint beamforming design problem.","For the hybrid beamformer design, a semi-closed-form solution for fully digital beamformer is first obtained by a weighted minimum mean-square error based algorithm, then the baseband digital and analog beamformers at Alice are designed by approximating the fully digital beamformer via manifold optimization.","For the XL-RIS's reflection coefficient matrix design, a low-complexity alternating direction method of multipliers based algorithm is proposed to address the challenge of large-scale variables and unit-modulus constraints.","Numerical results unveil that i) the near-field communications can achieve a higher covert rate than the far-field covert communications in general, and still realize covert transmission even if Willie is located at the same direction as Bob and closer to the XL-RIS; ii) the proposed algorithm can enhance the covert rate significantly compared to the benchmark schemes; iii) the proposed algorithm leads to a beam diffraction pattern that can bypass Willie and achieve high-rate covert transmission to Bob."],"url":"http://arxiv.org/abs/2401.13564v1","category":"cs.IT"}
{"created":"2024-01-24 16:18:29","title":"Exploiting separation-dependent coherence to boost optical resolution","abstract":"The problem of resolving point-like light sources not only serves as a benchmark for optical resolution but also holds various practical applications ranging from microscopy to astronomy. In this research, we aim to resolve two thermal sources sharing arbitrary mutual coherence using the spatial mode demultiplexing technique. Our analytical study includes scenarios where the coherence and the emission rate depend on the separation between the sources, and is not limited to the faint sources limit. We consider the fluorescence of two interacting dipoles to demonstrate that the dependence of emission characteristics on the parameter of interest can boost the sensitivity of the estimation and noticeably prolong the duration of information decay.","sentences":["The problem of resolving point-like light sources not only serves as a benchmark for optical resolution but also holds various practical applications ranging from microscopy to astronomy.","In this research, we aim to resolve two thermal sources sharing arbitrary mutual coherence using the spatial mode demultiplexing technique.","Our analytical study includes scenarios where the coherence and the emission rate depend on the separation between the sources, and is not limited to the faint sources limit.","We consider the fluorescence of two interacting dipoles to demonstrate that the dependence of emission characteristics on the parameter of interest can boost the sensitivity of the estimation and noticeably prolong the duration of information decay."],"url":"http://arxiv.org/abs/2401.13562v1","category":"physics.optics"}
{"created":"2024-01-24 16:17:23","title":"SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation","abstract":"The Transformer architecture has shown a remarkable ability in modeling global relationships. However, it poses a significant computational challenge when processing high-dimensional medical images. This hinders its development and widespread adoption in this task. Mamba, as a State Space Model (SSM), recently emerged as a notable manner for long-range dependencies in sequential modeling, excelling in natural language processing filed with its remarkable memory efficiency and computational speed. Inspired by its success, we introduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation \\textbf{Mamba} model, designed to effectively capture long-range dependencies within whole volume features at every scale. Our SegMamba, in contrast to Transformer-based methods, excels in whole volume feature modeling from a state space model standpoint, maintaining superior processing speed, even with volume features at a resolution of {$64\\times 64\\times 64$}. Comprehensive experiments on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our SegMamba. The code for SegMamba is available at: https://github.com/ge-xing/SegMamba","sentences":["The Transformer architecture has shown a remarkable ability in modeling global relationships.","However, it poses a significant computational challenge when processing high-dimensional medical images.","This hinders its development and widespread adoption in this task.","Mamba, as a State Space Model (SSM), recently emerged as a notable manner for long-range dependencies in sequential modeling, excelling in natural language processing filed with its remarkable memory efficiency and computational speed.","Inspired by its success, we introduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation \\textbf{Mamba} model, designed to effectively capture long-range dependencies within whole volume features at every scale.","Our SegMamba, in contrast to Transformer-based methods, excels in whole volume feature modeling from a state space model standpoint, maintaining superior processing speed, even with volume features at a resolution of {$64\\times 64\\times 64$}.","Comprehensive experiments on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our SegMamba.","The code for SegMamba is available at: https://github.com/ge-xing/SegMamba"],"url":"http://arxiv.org/abs/2401.13560v1","category":"cs.CV"}
{"created":"2024-01-24 16:14:38","title":"Task structure and nonlinearity jointly determine learned representational geometry","abstract":"The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of ReLU, which leads feature neurons to specialize for different regions of input space. By contrast, feature neurons in Tanh networks tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks.","sentences":["The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks.","This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network.","By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs.","This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels.","We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of ReLU, which leads feature neurons to specialize for different regions of input space.","By contrast, feature neurons in Tanh networks tend to inherit the task label structure.","Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity.","Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks."],"url":"http://arxiv.org/abs/2401.13558v1","category":"cs.LG"}
{"created":"2024-01-24 16:14:04","title":"Scalarized Hybrid Neutron Stars in Scalar Tensor Gravity","abstract":"Hybrid neutron stars, the compact objects consisting hadronic matter and strange quark matter, can be considered as the probes for the scalar tensor gravity. In this work, we explore the scalarization of hybrid neutron stars in the scalar tensor gravity. For the hadronic phase, we apply a piecewise polytropic equation of state constrained by the observational data of GW170817 and the data of six low-mass X-ray binaries with thermonuclear burst or the symmetry energy of the nuclear interaction. In addition, to describe the strange quark matter inside the hybrid neutron star, different MIT bag models are employed. We study the effects of the value of bag constant, the mass of s quark, the perturbative quantum chromodynamics correction parameter, and the density jump at the surface of quark-hadronic phase transition on the scalarization of hybrid neutron stars. Our results confirm that the scalarization is more sensitive to the value of bag constant, the mass of s quark, and the density jump compared to the perturbative quantum chromodynamics correction parameter.","sentences":["Hybrid neutron stars, the compact objects consisting hadronic matter and strange quark matter, can be considered as the probes for the scalar tensor gravity.","In this work, we explore the scalarization of hybrid neutron stars in the scalar tensor gravity.","For the hadronic phase, we apply a piecewise polytropic equation of state constrained by the observational data of GW170817 and the data of six low-mass X-ray binaries with thermonuclear burst or the symmetry energy of the nuclear interaction.","In addition, to describe the strange quark matter inside the hybrid neutron star, different MIT bag models are employed.","We study the effects of the value of bag constant, the mass of s quark, the perturbative quantum chromodynamics correction parameter, and the density jump at the surface of quark-hadronic phase transition on the scalarization of hybrid neutron stars.","Our results confirm that the scalarization is more sensitive to the value of bag constant, the mass of s quark, and the density jump compared to the perturbative quantum chromodynamics correction parameter."],"url":"http://arxiv.org/abs/2401.13557v1","category":"astro-ph.HE"}
{"created":"2024-01-24 16:13:26","title":"Benchmarking the Fairness of Image Upsampling Methods","abstract":"Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\\unicode{x2013}$inspired by their supervised fairness counterparts$\\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results.","sentences":["Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos.","While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness.","In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models.","We develop a set of metrics$\\unicode{x2013}$inspired by their supervised fairness counterparts$\\unicode{x2013}$to evaluate the models on their fairness and diversity.","Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods.","As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets.","Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances.","Alarmingly, we find that none of the considered methods produces statistically fair and diverse results."],"url":"http://arxiv.org/abs/2401.13555v1","category":"cs.CV"}
{"created":"2024-01-24 16:13:24","title":"PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition","abstract":"We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 18 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts.","sentences":["We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment.","It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 18 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee.","The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition.","Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered.","We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts."],"url":"http://arxiv.org/abs/2401.13554v1","category":"cs.CV"}
{"created":"2024-01-24 16:11:42","title":"Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection","abstract":"Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.","sentences":["Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other.","In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly.","Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process.","To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD.","The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields.","For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data.","We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner.","Experiments demonstrate that the proposed UVAD method outperforms previous approaches."],"url":"http://arxiv.org/abs/2401.13551v1","category":"cs.CV"}
{"created":"2024-01-24 16:08:21","title":"A Phoneme-Scale Assessment of Multichannel Speech Enhancement Algorithms","abstract":"In the intricate acoustic landscapes where speech intelligibility is challenged by noise and reverberation, multichannel speech enhancement emerges as a promising solution for individuals with hearing loss. Such algorithms are commonly evaluated at the utterance level. However, this approach overlooks the granular acoustic nuances revealed by phoneme-specific analysis, potentially obscuring key insights into their performance. This paper presents an in-depth phoneme-scale evaluation of 3 state-of-the-art multichannel speech enhancement algorithms. These algorithms -- FasNet, MVDR, and Tango -- are extensively evaluated across different noise conditions and spatial setups, employing realistic acoustic simulations with measured room impulse responses, and leveraging diversity offered by multiple microphones in a binaural hearing setup. The study emphasizes the fine-grained phoneme-level analysis, revealing that while some phonemes like plosives are heavily impacted by environmental acoustics and challenging to deal with by the algorithms, others like nasals and sibilants see substantial improvements after enhancement. These investigations demonstrate important improvements in phoneme clarity in noisy conditions, with insights that could drive the development of more personalized and phoneme-aware hearing aid technologies.","sentences":["In the intricate acoustic landscapes where speech intelligibility is challenged by noise and reverberation, multichannel speech enhancement emerges as a promising solution for individuals with hearing loss.","Such algorithms are commonly evaluated at the utterance level.","However, this approach overlooks the granular acoustic nuances revealed by phoneme-specific analysis, potentially obscuring key insights into their performance.","This paper presents an in-depth phoneme-scale evaluation of 3 state-of-the-art multichannel speech enhancement algorithms.","These algorithms -- FasNet, MVDR, and Tango -- are extensively evaluated across different noise conditions and spatial setups, employing realistic acoustic simulations with measured room impulse responses, and leveraging diversity offered by multiple microphones in a binaural hearing setup.","The study emphasizes the fine-grained phoneme-level analysis, revealing that while some phonemes like plosives are heavily impacted by environmental acoustics and challenging to deal with by the algorithms, others like nasals and sibilants see substantial improvements after enhancement.","These investigations demonstrate important improvements in phoneme clarity in noisy conditions, with insights that could drive the development of more personalized and phoneme-aware hearing aid technologies."],"url":"http://arxiv.org/abs/2401.13548v1","category":"cs.SD"}
{"created":"2024-01-24 16:05:03","title":"Fine-grained Contract NER using instruction based model","abstract":"Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language. We harness the strength of LLMs by integrating supervised learning within them. The goal of this combined strategy is to boost the performance of LLMs in extraction tasks like NER while simultaneously addressing hallucination issues often observed in LLM-generated content. A novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models. Our models and dataset are available to the community for future research * .","sentences":["Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios.","They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks.","Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines.","The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs.","NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence.","In contrast, LLMs are designed as a text generation task.","This distinction between semantic labeling and text generation leads to subpar performance.","In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs.","This involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language.","We harness the strength of LLMs by integrating supervised learning within them.","The goal of this combined strategy is to boost the performance of LLMs in extraction tasks like NER while simultaneously addressing hallucination issues often observed in LLM-generated content.","A novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models.","Our models and dataset are available to the community for future research * ."],"url":"http://arxiv.org/abs/2401.13545v1","category":"cs.IR"}
{"created":"2024-01-24 16:02:14","title":"Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?","abstract":"Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of the proposed techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes can be as intervenable and more performant than CBMs.","sentences":["Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts.","A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output.","In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set.","Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models.","Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks.","We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions.","To showcase the practical utility of the proposed techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes can be as intervenable and more performant than CBMs."],"url":"http://arxiv.org/abs/2401.13544v1","category":"cs.LG"}
{"created":"2024-01-24 15:56:13","title":"State Estimation for Continuum Multi-Robot Systems on SE(3)","abstract":"In contrast to conventional robots, accurately modeling the kinematics and statics of continuum robots is challenging due to partially unknown material properties, parasitic effects, or unknown forces acting on the continuous body. Consequentially, state estimation approaches that utilize additional sensor information to predict the shape of continuum robots have garnered significant interest. This paper presents a novel approach to state estimation for systems with multiple coupled continuum robots, which allows estimating the shape and strain variables of multiple continuum robots in an arbitrary coupled topology. Simulations and experiments demonstrate the capabilities and versatility of the proposed method, while achieving accurate and continuous estimates for the state of such systems, resulting in average end-effector errors of 3.3 mm and 5.02{\\deg} depending on the sensor setup. It is further shown, that the approach offers fast computation times of below 10 ms, enabling its utilization in quasi-static real-time scenarios with average update rates of 100-200 Hz. An open-source C++ implementation of the proposed state estimation method is made publicly available to the community.","sentences":["In contrast to conventional robots, accurately modeling the kinematics and statics of continuum robots is challenging due to partially unknown material properties, parasitic effects, or unknown forces acting on the continuous body.","Consequentially, state estimation approaches that utilize additional sensor information to predict the shape of continuum robots have garnered significant interest.","This paper presents a novel approach to state estimation for systems with multiple coupled continuum robots, which allows estimating the shape and strain variables of multiple continuum robots in an arbitrary coupled topology.","Simulations and experiments demonstrate the capabilities and versatility of the proposed method, while achieving accurate and continuous estimates for the state of such systems, resulting in average end-effector errors of 3.3 mm and 5.02{\\deg} depending on the sensor setup.","It is further shown, that the approach offers fast computation times of below 10 ms, enabling its utilization in quasi-static real-time scenarios with average update rates of 100-200 Hz.","An open-source C++ implementation of the proposed state estimation method is made publicly available to the community."],"url":"http://arxiv.org/abs/2401.13540v1","category":"cs.RO"}
{"created":"2024-01-24 15:50:32","title":"Dynamic Risk Management in Cyber Physical Systems","abstract":"Cyber Physical Systems (CPS) enable new kinds of applications as well as significant improvements of existing ones in numerous different application domains. A major trait of upcoming CPS is an increasing degree of automation up to the point of autonomy, as there is a huge potential for economic success as well as for ecologic and societal improvements. However, to unlock the full potential of such (cooperative and automated) CPS, we first need to overcome several significant engineering challenges, where safety assurance is a particularly important one. Unfortunately, established safety assurance methods and standards do not live up to this task, as they have been designed with closed and less complex systems in mind. This paper structures safety assurance challenges of cooperative automated CPS, provides an overview on our vision of dynamic risk management and describes already existing building blocks.","sentences":["Cyber Physical Systems (CPS) enable new kinds of applications as well as significant improvements of existing ones in numerous different application domains.","A major trait of upcoming CPS is an increasing degree of automation up to the point of autonomy, as there is a huge potential for economic success as well as for ecologic and societal improvements.","However, to unlock the full potential of such (cooperative and automated) CPS, we first need to overcome several significant engineering challenges, where safety assurance is a particularly important one.","Unfortunately, established safety assurance methods and standards do not live up to this task, as they have been designed with closed and less complex systems in mind.","This paper structures safety assurance challenges of cooperative automated CPS, provides an overview on our vision of dynamic risk management and describes already existing building blocks."],"url":"http://arxiv.org/abs/2401.13539v1","category":"cs.SE"}
{"created":"2024-01-24 15:46:32","title":"Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models","abstract":"We propose \\textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.","sentences":["We propose \\textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data.","This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets.","More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks.","In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder.","We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering.","We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains."],"url":"http://arxiv.org/abs/2401.13537v1","category":"hep-ph"}
{"created":"2024-01-24 15:46:25","title":"Finetuning Foundation Models for Joint Analysis Optimization","abstract":"In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets.","sentences":["In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components.","We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets."],"url":"http://arxiv.org/abs/2401.13536v1","category":"hep-ex"}
{"created":"2024-01-24 15:43:38","title":"On the Approximate Core and Nucleon of Flow Games","abstract":"The flow game with public arcs is a cooperative revenue game derived from a flow network. In this game, each player possesses an arc, while certain arcs, known as public arcs, are not owned by any specific player and are accessible to any coalition. The aim of this game is to maximize the flow that can be routed in the network through strategic coalition formation. By exploring its connection to the maximum partially disjoint path problem, we investigate the approximate core and nucleon of the flow game with public arcs. The approximate core is an extension of the core that allows for some deviation in group rationality, while the nucleon is a multiplicative analogue of the nucleolus. In this paper, we provide two complete characterizations for the optimal approximate core and show that the nucleon can be computed in polynomial time.","sentences":["The flow game with public arcs is a cooperative revenue game derived from a flow network.","In this game, each player possesses an arc, while certain arcs, known as public arcs, are not owned by any specific player and are accessible to any coalition.","The aim of this game is to maximize the flow that can be routed in the network through strategic coalition formation.","By exploring its connection to the maximum partially disjoint path problem, we investigate the approximate core and nucleon of the flow game with public arcs.","The approximate core is an extension of the core that allows for some deviation in group rationality, while the nucleon is a multiplicative analogue of the nucleolus.","In this paper, we provide two complete characterizations for the optimal approximate core and show that the nucleon can be computed in polynomial time."],"url":"http://arxiv.org/abs/2401.13535v1","category":"cs.GT"}
{"created":"2024-01-24 15:37:31","title":"QAGait: Revisit Gait Recognition from a Quality Perspective","abstract":"Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait.","sentences":["Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns.","Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research.","However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion).","To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait.","Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures.","We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space.","Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement.","Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority.","Code is available at https://github.com/wzb-bupt/QAGait."],"url":"http://arxiv.org/abs/2401.13531v1","category":"cs.CV"}
{"created":"2024-01-24 15:35:44","title":"Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space","abstract":"Recently, optimization on the Riemannian manifold has provided new insights to the optimization community. In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes. In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow. The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet. By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamics of desired stochastic methods in the corresponded random vector space. Then, the flows of probability measures are naturally obtained by applying Fokker-Planck equation to such SDE. Furthermore, the convergence rates of the proposed Riemannian stochastic flows are proven, and they match the results in Euclidean space.","sentences":["Recently, optimization on the Riemannian manifold has provided new insights to the optimization community.","In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes.","In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence).","In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow.","The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet.","By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamics of desired stochastic methods in the corresponded random vector space.","Then, the flows of probability measures are naturally obtained by applying Fokker-Planck equation to such SDE.","Furthermore, the convergence rates of the proposed Riemannian stochastic flows are proven, and they match the results in Euclidean space."],"url":"http://arxiv.org/abs/2401.13530v1","category":"cs.LG"}
{"created":"2024-01-24 15:25:01","title":"SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation","abstract":"Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.","sentences":["Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers.","However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation.","We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation.","Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling.","It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling.","Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching.","Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions.","Code and models are available at https://github.com/0nutation/SpeechGPT."],"url":"http://arxiv.org/abs/2401.13527v1","category":"cs.CL"}
{"created":"2024-01-24 15:18:22","title":"Non-linearities in cosmological bubble wall dynamics","abstract":"A precise modelling of the dynamics of bubbles nucleated during first-order phase transitions in the early Universe is pivotal for a quantitative determination of various cosmic relics, including the stochastic background of gravitational waves. The equation of motion of the bubble front is affected by the out-of-equilibrium distributions of particle species in the plasma which, in turn, are described by the corresponding Boltzmann equations. In this work we provide a solution to these equations by thoroughly incorporating the non-linearities arising from the population factors. Moreover, our methodology relies on a spectral decomposition that leverages the rotational properties of the collision integral within the Boltzmann equations. This novel approach allows for an efficient and robust computation of both the bubble speed and profile. We also refine our analysis by including the contributions from the electroweak gauge bosons. We find that their impact is dominated by the infrared modes and proves to be non-negligible, contrary to the naive expectations.","sentences":["A precise modelling of the dynamics of bubbles nucleated during first-order phase transitions in the early Universe is pivotal for a quantitative determination of various cosmic relics, including the stochastic background of gravitational waves.","The equation of motion of the bubble front is affected by the out-of-equilibrium distributions of particle species in the plasma which, in turn, are described by the corresponding Boltzmann equations.","In this work we provide a solution to these equations by thoroughly incorporating the non-linearities arising from the population factors.","Moreover, our methodology relies on a spectral decomposition that leverages the rotational properties of the collision integral within the Boltzmann equations.","This novel approach allows for an efficient and robust computation of both the bubble speed and profile.","We also refine our analysis by including the contributions from the electroweak gauge bosons.","We find that their impact is dominated by the infrared modes and proves to be non-negligible, contrary to the naive expectations."],"url":"http://arxiv.org/abs/2401.13522v1","category":"hep-ph"}
{"created":"2024-01-24 15:18:18","title":"CPU efficient numerical code for charged particle transport through insulating straight capillaries","abstract":"The numerical code InCa4D, used for simulating CPU-efficiently the guiding of charged beam particles through insulating straight nano or macro capillaries, is presented in detail. The paper may be regarded as a walk through our numerical code where we will detail how we sample the inserted beam with a given emittance, how we compute the charge deposition and charge dynamics at the interfaces of the insulating capillary, how we evaluate the electric field by accounting for imposed boundary conditions and how we evaluate efficiently the trajectories of the inserted charged particles. The code InCa4D accounts for the correct screening of the deposited charges by polarization charges that appear at the interfaces and for the correct relaxation rates and decay rates in the case where the outer surface of the capillary is covered by a grounded conducting paint or not.","sentences":["The numerical code InCa4D, used for simulating CPU-efficiently the guiding of charged beam particles through insulating straight nano or macro capillaries, is presented in detail.","The paper may be regarded as a walk through our numerical code where we will detail how we sample the inserted beam with a given emittance, how we compute the charge deposition and charge dynamics at the interfaces of the insulating capillary, how we evaluate the electric field by accounting for imposed boundary conditions and how we evaluate efficiently the trajectories of the inserted charged particles.","The code InCa4D accounts for the correct screening of the deposited charges by polarization charges that appear at the interfaces and for the correct relaxation rates and decay rates in the case where the outer surface of the capillary is covered by a grounded conducting paint or not."],"url":"http://arxiv.org/abs/2401.13521v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-01-24 15:15:03","title":"Addressing Data Quality Challenges in Observational Ambulatory Studies: Analysis, Methodologies and Practical Solutions for Wrist-worn Wearable Monitoring","abstract":"Chronic disease management and follow-up are vital for realizing sustained patient well-being and optimal health outcomes. Recent advancements in wearable sensing technologies, particularly wrist-worn devices, offer promising solutions for longitudinal patient follow-up by shifting from subjective, intermittent self-reporting to objective, continuous monitoring. However, collecting and analyzing wearable data presents unique challenges, such as data entry errors, non-wear periods, missing wearable data, and wearable artifacts. We therefore present an in-depth exploration of data analysis challenges tied to wrist-worn wearables and ambulatory label acquisition, using two real-world datasets (i.e., mBrain21 and ETRI lifelog2020). We introduce novel practical countermeasures, including participant compliance visualizations, interaction-triggered questionnaires to assess personal bias, and an optimized wearable non-wear detection pipeline. Further, we propose a visual analytics approach to validate processing pipelines using scalable tools such as tsflex and Plotly-Resampler. Lastly, we investigate the impact of missing wearable data on \"window-of-interest\" analysis methodologies. Prioritizing transparency and reproducibility, we offer open access to our detailed code examples, facilitating adaptation in future wearable research. In conclusion, our contributions provide actionable approaches for wearable data collection and analysis in chronic disease management.","sentences":["Chronic disease management and follow-up are vital for realizing sustained patient well-being and optimal health outcomes.","Recent advancements in wearable sensing technologies, particularly wrist-worn devices, offer promising solutions for longitudinal patient follow-up by shifting from subjective, intermittent self-reporting to objective, continuous monitoring.","However, collecting and analyzing wearable data presents unique challenges, such as data entry errors, non-wear periods, missing wearable data, and wearable artifacts.","We therefore present an in-depth exploration of data analysis challenges tied to wrist-worn wearables and ambulatory label acquisition, using two real-world datasets (i.e., mBrain21 and ETRI lifelog2020).","We introduce novel practical countermeasures, including participant compliance visualizations, interaction-triggered questionnaires to assess personal bias, and an optimized wearable non-wear detection pipeline.","Further, we propose a visual analytics approach to validate processing pipelines using scalable tools such as tsflex and Plotly-Resampler.","Lastly, we investigate the impact of missing wearable data on \"window-of-interest\" analysis methodologies.","Prioritizing transparency and reproducibility, we offer open access to our detailed code examples, facilitating adaptation in future wearable research.","In conclusion, our contributions provide actionable approaches for wearable data collection and analysis in chronic disease management."],"url":"http://arxiv.org/abs/2401.13518v1","category":"cs.CE"}
{"created":"2024-01-24 15:14:05","title":"Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces","abstract":"Deepfake videos are becoming increasingly realistic, showing subtle tampering traces on facial areasthat vary between frames. Consequently, many existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region. To address thislimitation, we propose Delocate, a novel Deepfake detection model that can both recognize andlocalize unknown domain Deepfake videos. Ourmethod consists of two stages named recoveringand localization. In the recovering stage, the modelrandomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, resulting in a relatively good recovery effect for realfaces and a poor recovery effect for fake faces. Inthe localization stage, the output of the recoveryphase and the forgery ground truth mask serve assupervision to guide the forgery localization process. This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions. Ourextensive experiments on four widely used benchmark datasets demonstrate that Delocate not onlyexcels in localizing tampered areas but also enhances cross-domain detection performance.","sentences":["Deepfake videos are becoming increasingly realistic, showing subtle tampering traces on facial areasthat vary between frames.","Consequently, many existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region.","To address thislimitation, we propose Delocate, a novel Deepfake detection model that can both recognize andlocalize unknown domain Deepfake videos.","Ourmethod consists of two stages named recoveringand localization.","In the recovering stage, the modelrandomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, resulting in a relatively good recovery effect for realfaces and a poor recovery effect for fake faces.","Inthe localization stage, the output of the recoveryphase and the forgery ground truth mask serve assupervision to guide the forgery localization process.","This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions.","Ourextensive experiments on four widely used benchmark datasets demonstrate that Delocate not onlyexcels in localizing tampered areas but also enhances cross-domain detection performance."],"url":"http://arxiv.org/abs/2401.13516v1","category":"cs.CV"}
{"created":"2024-01-24 15:10:13","title":"Can GPT-3.5 Generate and Code Discharge Summaries?","abstract":"Objective: To investigate GPT-3.5 in generating and coding medical documents with ICD-10 codes for data augmentation on low-resources labels.   Materials and Methods: Employing GPT-3.5 we generated and coded 9,606 discharge summaries based on lists of ICD-10 code descriptions of patients with infrequent (generation) codes within the MIMIC-IV dataset. Combined with the baseline training set, this formed an augmented training set. Neural coding models were trained on baseline and augmented data and evaluated on a MIMIC-IV test set. We report micro- and macro-F1 scores on the full codeset, generation codes, and their families. Weak Hierarchical Confusion Matrices were employed to determine within-family and outside-of-family coding errors in the latter codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided self-generated data and real MIMIC-IV data. Clinical professionals evaluated the clinical acceptability of the generated documents.   Results: Augmentation slightly hinders the overall performance of the models but improves performance for the generation candidate codes and their families, including one unseen in the baseline training data. Augmented models display lower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the prompted descriptions, but performs poorly on real data. Evaluators note the correctness of generated concepts while suffering in variety, supporting information, and narrative.   Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding. Augmentation positively affects generation code families but mainly benefits codes with existing examples. Augmentation reduces out-of-family errors. Discharge summaries generated by GPT-3.5 state prompted concepts correctly but lack variety, and authenticity in narratives. They are unsuitable for clinical practice.","sentences":["Objective: To investigate GPT-3.5 in generating and coding medical documents with ICD-10 codes for data augmentation on low-resources labels.   ","Materials and Methods: Employing GPT-3.5 we generated and coded 9,606 discharge summaries based on lists of ICD-10 code descriptions of patients with infrequent (generation) codes within the MIMIC-IV dataset.","Combined with the baseline training set, this formed an augmented training set.","Neural coding models were trained on baseline and augmented data and evaluated on a MIMIC-IV test set.","We report micro- and macro-F1 scores on the full codeset, generation codes, and their families.","Weak Hierarchical Confusion Matrices were employed to determine within-family and outside-of-family coding errors in the latter codesets.","The coding performance of GPT-3.5 was evaluated both on prompt-guided self-generated data and real MIMIC-IV data.","Clinical professionals evaluated the clinical acceptability of the generated documents.   ","Results:","Augmentation slightly hinders the overall performance of the models but improves performance for the generation candidate codes and their families, including one unseen in the baseline training data.","Augmented models display lower out-of-family error rates.","GPT-3.5 can identify ICD-10 codes by the prompted descriptions, but performs poorly on real data.","Evaluators note the correctness of generated concepts while suffering in variety, supporting information, and narrative.   ","Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.","Augmentation positively affects generation code families but mainly benefits codes with existing examples.","Augmentation reduces out-of-family errors.","Discharge summaries generated by GPT-3.5 state prompted concepts correctly but lack variety, and authenticity in narratives.","They are unsuitable for clinical practice."],"url":"http://arxiv.org/abs/2401.13512v1","category":"cs.CL"}
{"created":"2024-01-24 15:09:12","title":"Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images","abstract":"Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions. Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue. Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed. In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning. To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&E stained WSIs. For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sections in a 2D histogram. On an independent test set, the model achieved a mean Dice score of 0.981$\\pm$0.033 for tissue segmentation and a mean Dice score of 0.912$\\pm$0.090 for pen marking segmentation. The mean absolute difference between the number of annotated and separated cross-sections was 0.075$\\pm$0.350. Our results demonstrate that the proposed model can accurately segment H&E stained tissue cross-sections and pen markings in WSIs while being robust to many common slide and scanning artifacts. The model with trained model parameters and post-processing method are made publicly available as a Python package called SlideSegmenter.","sentences":["Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions.","Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue.","Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed.","In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning.","To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&E stained WSIs.","For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sections in a 2D histogram.","On an independent test set, the model achieved a mean Dice score of 0.981$\\pm$0.033 for tissue segmentation and a mean Dice score of 0.912$\\pm$0.090 for pen marking segmentation.","The mean absolute difference between the number of annotated and separated cross-sections was 0.075$\\pm$0.350.","Our results demonstrate that the proposed model can accurately segment H&E stained tissue cross-sections and pen markings in WSIs while being robust to many common slide and scanning artifacts.","The model with trained model parameters and post-processing method are made publicly available as a Python package called SlideSegmenter."],"url":"http://arxiv.org/abs/2401.13511v1","category":"eess.IV"}
{"created":"2024-01-24 15:08:53","title":"Quantum Gravity and Random Tensors","abstract":"Random tensors are the natural generalization of random matrices to higher order objects. They provide generating functions for random geometries and, assuming some familiarity with random matrix theory and quantum field theory, we discuss in the first part of this note the applications of such models to quantum gravity. In a second part we review tensor field theories, that is standard field theories in $\\mathbb{R}^d$ but with tensor fields, which lead to a new family of large $N$ conformal field theories relevant for the study of the $AdS/CFT$ correspondence.","sentences":["Random tensors are the natural generalization of random matrices to higher order objects.","They provide generating functions for random geometries and, assuming some familiarity with random matrix theory and quantum field theory, we discuss in the first part of this note the applications of such models to quantum gravity.","In a second part we review tensor field theories, that is standard field theories in $\\mathbb{R}^d$ but with tensor fields, which lead to a new family of large $N$ conformal field theories relevant for the study of the $AdS/CFT$ correspondence."],"url":"http://arxiv.org/abs/2401.13510v1","category":"hep-th"}
{"created":"2024-01-24 15:06:44","title":"TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval","abstract":"This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever.","sentences":["This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present.","For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss.","TPRF learns how to effectively combine the relevance feedback signals from dense passage representations.","Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals.","The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever."],"url":"http://arxiv.org/abs/2401.13509v1","category":"cs.IR"}
{"created":"2024-01-24 14:53:56","title":"Interferometric measurement of the deflection of light by light in air","abstract":"The aim of the DeLLight (Deflection of Light by Light) experiment is to observe for the first time the optical nonlinearity in vacuum, as predicted by Quantum Electrodynamics, by measuring the refraction of a low-intensity focused laser pulse (probe) when crossing the effective vacuum index gradient induced by a high-intensity focused laser pulse (pump). The deflection signal is amplified by using a Sagnac interferometer. Here, we report the first measurement performed with the DeLLight pilot interferometer, of the deflection of light by light in air, with a low-intensity pump. We show that the deflection signal measured by the interferometer is amplified, and is in agreement with the expected signal induced by the optical Kerr effect in air. Moreover, we verify that the signal varies as expected as a function of the pump intensity, the temporal delay between the pump and the probe, and their relative polarisation. These results represent a proof of concept of the DeLLight experimental method based on interferometric amplification.","sentences":["The aim of the DeLLight (Deflection of Light by Light) experiment is to observe for the first time the optical nonlinearity in vacuum, as predicted by Quantum Electrodynamics, by measuring the refraction of a low-intensity focused laser pulse (probe) when crossing the effective vacuum index gradient induced by a high-intensity focused laser pulse (pump).","The deflection signal is amplified by using a Sagnac interferometer.","Here, we report the first measurement performed with the DeLLight pilot interferometer, of the deflection of light by light in air, with a low-intensity pump.","We show that the deflection signal measured by the interferometer is amplified, and is in agreement with the expected signal induced by the optical Kerr effect in air.","Moreover, we verify that the signal varies as expected as a function of the pump intensity, the temporal delay between the pump and the probe, and their relative polarisation.","These results represent a proof of concept of the DeLLight experimental method based on interferometric amplification."],"url":"http://arxiv.org/abs/2401.13506v1","category":"physics.optics"}
{"created":"2024-01-24 14:53:13","title":"Generative Human Motion Stylization in Latent Space","abstract":"Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-arts in style reeanactment, content preservation, and generalization across various applications and settings. Project Page: https://yxmu.foo/GenMoStyle","sentences":["Human motion stylization aims to revise the style of an input motion while keeping its content unaltered.","Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion.","Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code.","During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes.","Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well.","In inference, users can opt to stylize a motion using style cues from a reference motion or a label.","Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution.","Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-arts in style reeanactment, content preservation, and generalization across various applications and settings.","Project Page: https://yxmu.foo/GenMoStyle"],"url":"http://arxiv.org/abs/2401.13505v1","category":"cs.CV"}
{"created":"2024-01-24 14:53:06","title":"Research about the Ability of LLM in the Tamper-Detection Area","abstract":"In recent years, particularly since the early 2020s, Large Language Models (LLMs) have emerged as the most powerful AI tools in addressing a diverse range of challenges, from natural language processing to complex problem-solving in various domains. In the field of tamper detection, LLMs are capable of identifying basic tampering activities.To assess the capabilities of LLMs in more specialized domains, we have collected five different LLMs developed by various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This diverse range of models allows for a comprehensive evaluation of their performance in detecting sophisticated tampering instances.We devised two domains of detection: AI-Generated Content (AIGC) detection and manipulation detection. AIGC detection aims to test the ability to distinguish whether an image is real or AI-generated. Manipulation detection, on the other hand, focuses on identifying tampered images. According to our experiments, most LLMs can identify composite pictures that are inconsistent with logic, and only more powerful LLMs can distinguish logical, but visible signs of tampering to the human eye. All of the LLMs can't identify carefully forged images and very realistic images generated by AI. In the area of tamper detection, LLMs still have a long way to go, particularly in reliably identifying highly sophisticated forgeries and AI-generated images that closely mimic reality.","sentences":["In recent years, particularly since the early 2020s, Large Language Models (LLMs) have emerged as the most powerful AI tools in addressing a diverse range of challenges, from natural language processing to complex problem-solving in various domains.","In the field of tamper detection, LLMs are capable of identifying basic tampering activities.","To assess the capabilities of LLMs in more specialized domains, we have collected five different LLMs developed by various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen.","This diverse range of models allows for a comprehensive evaluation of their performance in detecting sophisticated tampering instances.","We devised two domains of detection: AI-Generated Content (AIGC) detection and manipulation detection.","AIGC detection aims to test the ability to distinguish whether an image is real or AI-generated.","Manipulation detection, on the other hand, focuses on identifying tampered images.","According to our experiments, most LLMs can identify composite pictures that are inconsistent with logic, and only more powerful LLMs can distinguish logical, but visible signs of tampering to the human eye.","All of the LLMs can't identify carefully forged images and very realistic images generated by AI.","In the area of tamper detection, LLMs still have a long way to go, particularly in reliably identifying highly sophisticated forgeries and AI-generated images that closely mimic reality."],"url":"http://arxiv.org/abs/2401.13504v1","category":"cs.CV"}
{"created":"2024-01-24 14:51:33","title":"Learning Representations for Clustering via Partial Information Discrimination and Cross-Level Interaction","abstract":"In this paper, we present a novel deep image clustering approach termed PICI, which enforces the partial information discrimination and the cross-level interaction in a joint learning framework. In particular, we leverage a Transformer encoder as the backbone, through which the masked image modeling with two paralleled augmented views is formulated. After deriving the class tokens from the masked images by the Transformer encoder, three partial information learning modules are further incorporated, including the PISD module for training the auto-encoder via masked image reconstruction, the PICD module for employing two levels of contrastive learning, and the CLI module for mutual interaction between the instance-level and cluster-level subspaces. Extensive experiments have been conducted on six real-world image datasets, which demononstrate the superior clustering performance of the proposed PICI approach over the state-of-the-art deep clustering approaches. The source code is available at https://github.com/Regan-Zhang/PICI.","sentences":["In this paper, we present a novel deep image clustering approach termed PICI, which enforces the partial information discrimination and the cross-level interaction in a joint learning framework.","In particular, we leverage a Transformer encoder as the backbone, through which the masked image modeling with two paralleled augmented views is formulated.","After deriving the class tokens from the masked images by the Transformer encoder, three partial information learning modules are further incorporated, including the PISD module for training the auto-encoder via masked image reconstruction, the PICD module for employing two levels of contrastive learning, and the CLI module for mutual interaction between the instance-level and cluster-level subspaces.","Extensive experiments have been conducted on six real-world image datasets, which demononstrate the superior clustering performance of the proposed PICI approach over the state-of-the-art deep clustering approaches.","The source code is available at https://github.com/Regan-Zhang/PICI."],"url":"http://arxiv.org/abs/2401.13503v1","category":"cs.CV"}
{"created":"2024-01-24 14:51:17","title":"Faster Combinatorial k-Clique Algorithms","abstract":"Detecting if a graph contains a $k$-Clique is one of the most fundamental problems in computer science. The asymptotically fastest algorithm runs in time $O(n^{\\omega k/3})$, where $\\omega$ is the exponent of Boolean matrix multiplication. To date, this is the only technique capable of beating the trivial $O(n^k)$ bound by a polynomial factor. Due to this technique's various limitations, much effort has gone into designing \"combinatorial\" algorithms that improve over exhaustive search via other techniques.   The first contribution of this work is a faster combinatorial algorithm for $k$-Clique, improving Vassilevska's bound of $O(n^{k}/\\log^{k-1}{n})$ by two log factors. Technically, our main result is a new reduction from $k$-Clique to Triangle detection that exploits the same divide-and-conquer at the core of recent combinatorial algorithms by Chan (SODA'15) and Yu (ICALP'15).   Our second contribution is exploiting combinatorial techniques to improve the state-of-the-art (even of non-combinatorial algorithms) for generalizations of the $k$-Clique problem. In particular, we give the first $o(n^k)$ algorithm for $k$-clique in hypergraphs and an $O(n^3/\\log^{2.25}{n} + t)$ algorithm for listing $t$ triangles in a graph.","sentences":["Detecting if a graph contains a $k$-Clique is one of the most fundamental problems in computer science.","The asymptotically fastest algorithm runs in time $O(n^{\\omega k/3})$, where $\\omega$ is the exponent of Boolean matrix multiplication.","To date, this is the only technique capable of beating the trivial $O(n^k)$ bound by a polynomial factor.","Due to this technique's various limitations, much effort has gone into designing \"combinatorial\" algorithms that improve over exhaustive search via other techniques.   ","The first contribution of this work is a faster combinatorial algorithm for $k$-Clique, improving Vassilevska's bound of $O(n^{k}/\\log^{k-1}{n})$ by two log factors.","Technically, our main result is a new reduction from $k$-Clique to Triangle detection that exploits the same divide-and-conquer at the core of recent combinatorial algorithms by Chan (SODA'15) and Yu (ICALP'15).   ","Our second contribution is exploiting combinatorial techniques to improve the state-of-the-art (even of non-combinatorial algorithms) for generalizations of the $k$-Clique problem.","In particular, we give the first $o(n^k)$ algorithm for $k$-clique in hypergraphs and an $O(n^3/\\log^{2.25}{n} + t)$ algorithm for listing $t$ triangles in a graph."],"url":"http://arxiv.org/abs/2401.13502v1","category":"cs.DS"}
{"created":"2024-01-24 14:48:55","title":"Solving nonlinear differential equations on Quantum Computers: A Fokker-Planck approach","abstract":"For quantum computers to become useful tools to physicists, engineers and computational scientists, quantum algorithms for solving nonlinear differential equations need to be developed. Despite recent advances, the quest for a solver that can integrate nonlinear dynamical systems with a quantum advantage, whilst being realisable on available (or near-term) quantum hardware, is an open challenge. In this paper, we propose to transform a nonlinear dynamical system into a linear system, which we integrate with quantum algorithms. Key to the method is the Fokker-Planck equation, which is a non-normal partial differential equation. Three integration strategies are proposed: (i) Forward-Euler stepping by unitary block encoding; (ii) Schroedingerisation, and (iii) Forward-Euler stepping by linear addition of unitaries. We emulate the integration of prototypical nonlinear systems with the proposed quantum solvers, and compare the output with the benchmark solutions of classical integrators. We find that classical and quantum outputs are in good agreement. This paper opens opportunities for solving nonlinear differential equations with quantum algorithms.","sentences":["For quantum computers to become useful tools to physicists, engineers and computational scientists, quantum algorithms for solving nonlinear differential equations need to be developed.","Despite recent advances, the quest for a solver that can integrate nonlinear dynamical systems with a quantum advantage, whilst being realisable on available (or near-term) quantum hardware, is an open challenge.","In this paper, we propose to transform a nonlinear dynamical system into a linear system, which we integrate with quantum algorithms.","Key to the method is the Fokker-Planck equation, which is a non-normal partial differential equation.","Three integration strategies are proposed: (i) Forward-Euler stepping by unitary block encoding; (ii) Schroedingerisation, and (iii) Forward-Euler stepping by linear addition of unitaries.","We emulate the integration of prototypical nonlinear systems with the proposed quantum solvers, and compare the output with the benchmark solutions of classical integrators.","We find that classical and quantum outputs are in good agreement.","This paper opens opportunities for solving nonlinear differential equations with quantum algorithms."],"url":"http://arxiv.org/abs/2401.13500v1","category":"quant-ph"}
{"created":"2024-01-24 14:44:48","title":"LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning","abstract":"Few-shot image classification has emerged as a key challenge in the field of computer vision, highlighting the capability to rapidly adapt to new tasks with minimal labeled data. Existing methods predominantly rely on image-level features or local descriptors, often overlooking the holistic context surrounding these descriptors. In this work, we introduce a novel approach termed \"Local Descriptor with Contextual Augmentation (LDCA)\". Specifically, this method bridges the gap between local and global understanding uniquely by leveraging an adaptive global contextual enhancement module. This module incorporates a visual transformer, endowing local descriptors with contextual awareness capabilities, ranging from broad global perspectives to intricate surrounding nuances. By doing so, LDCA transcends traditional descriptor-based approaches, ensuring each local feature is interpreted within its larger visual narrative. Extensive experiments underscore the efficacy of our method, showing a maximal absolute improvement of 20\\% over the next-best on fine-grained classification datasets, thus demonstrating significant advancements in few-shot classification tasks.","sentences":["Few-shot image classification has emerged as a key challenge in the field of computer vision, highlighting the capability to rapidly adapt to new tasks with minimal labeled data.","Existing methods predominantly rely on image-level features or local descriptors, often overlooking the holistic context surrounding these descriptors.","In this work, we introduce a novel approach termed \"Local Descriptor with Contextual Augmentation (LDCA)\".","Specifically, this method bridges the gap between local and global understanding uniquely by leveraging an adaptive global contextual enhancement module.","This module incorporates a visual transformer, endowing local descriptors with contextual awareness capabilities, ranging from broad global perspectives to intricate surrounding nuances.","By doing so, LDCA transcends traditional descriptor-based approaches, ensuring each local feature is interpreted within its larger visual narrative.","Extensive experiments underscore the efficacy of our method, showing a maximal absolute improvement of 20\\% over the next-best on fine-grained classification datasets, thus demonstrating significant advancements in few-shot classification tasks."],"url":"http://arxiv.org/abs/2401.13499v1","category":"cs.CV"}
{"created":"2024-01-24 14:44:01","title":"Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting","abstract":"Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.","sentences":["Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression.","Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input.","In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll.","We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency.","To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer.","Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work."],"url":"http://arxiv.org/abs/2401.13498v1","category":"cs.SD"}
{"created":"2024-01-24 14:43:57","title":"Emergence of anti-coordinated patterns in snowdrift game by reinforcement learning","abstract":"Patterns by self-organization in nature have garnered significant interest in a range of disciplines due to their intriguing structures. In the context of the snowdrift game (SDG), which is considered as an anti-coordination game, but the anti-coordination patterns are counterintuitively rare. In the work, we introduce a model called the Two-Agents, Two-Action Reinforcement Learning Evolutionary Game ($2\\times 2$ RLEG), and apply it to the SDG on regular lattices. We uncover intriguing phenomena in the form of Anti-Coordinated domains (AC-domains), where different frustration regions are observed and continuous phase transitions at the boundaries are identified. To understand the underlying mechanism, we develop a perturbation theory to analyze the stability of different AC-domains. Our theory accurately partitions the parameter space into non-anti-coordinated, anti-coordinated, and mixed areas, and captures their dependence on the learning parameters. Lastly, abnormal scenarios with a large learning rate and a large discount factor that deviate from the theory are investigated by examining the growth and nucleation of AC-domains. Our work provides insights into the emergence of spatial patterns in nature, and contributes to the development of theory for analysing their structural complexities.","sentences":["Patterns by self-organization in nature have garnered significant interest in a range of disciplines due to their intriguing structures.","In the context of the snowdrift game (SDG), which is considered as an anti-coordination game, but the anti-coordination patterns are counterintuitively rare.","In the work, we introduce a model called the Two-Agents, Two-Action Reinforcement Learning Evolutionary Game ($2\\times 2$ RLEG), and apply it to the SDG on regular lattices.","We uncover intriguing phenomena in the form of Anti-Coordinated domains (AC-domains), where different frustration regions are observed and continuous phase transitions at the boundaries are identified.","To understand the underlying mechanism, we develop a perturbation theory to analyze the stability of different AC-domains.","Our theory accurately partitions the parameter space into non-anti-coordinated, anti-coordinated, and mixed areas, and captures their dependence on the learning parameters.","Lastly, abnormal scenarios with a large learning rate and a large discount factor that deviate from the theory are investigated by examining the growth and nucleation of AC-domains.","Our work provides insights into the emergence of spatial patterns in nature, and contributes to the development of theory for analysing their structural complexities."],"url":"http://arxiv.org/abs/2401.13497v1","category":"physics.soc-ph"}
{"created":"2024-01-24 14:42:05","title":"Towards an Autonomous Compost Turner: Current State of Research","abstract":"This preprint presents the current status of research into the development and application of an autonomous, self-driving compost turner. The aim is to overcome challenges in the composting industry, such as adverse working conditions, by automating the composting process. The preprint provides a comprehensive overview of the overall concept of the self-driving compost turner, including the hardware architecture with sensors, navigation module and control module. In addition, the methodical development of the architecture of concepts, models and their subsequent software integration in ROS using model-based systems engineering is described. The validation and verification of the overall system is carried out in an industrial environment using three scenarios. The capabilities of the compost turner are demonstrated by autonomously following predefined trajectories in the composting plant and performing the required composting tasks. The results show that the autonomous compost turner is capable of performing the required activities. In addition, the compost turner has intelligent processing capabilities for compost data as well as its transmission, visualization and storage in a cloud server. It is important to note that this work is a preprint that represents the current state of research. The authors aim to publish the full paper in a peer-reviewed journal in the near future.","sentences":["This preprint presents the current status of research into the development and application of an autonomous, self-driving compost turner.","The aim is to overcome challenges in the composting industry, such as adverse working conditions, by automating the composting process.","The preprint provides a comprehensive overview of the overall concept of the self-driving compost turner, including the hardware architecture with sensors, navigation module and control module.","In addition, the methodical development of the architecture of concepts, models and their subsequent software integration in ROS using model-based systems engineering is described.","The validation and verification of the overall system is carried out in an industrial environment using three scenarios.","The capabilities of the compost turner are demonstrated by autonomously following predefined trajectories in the composting plant and performing the required composting tasks.","The results show that the autonomous compost turner is capable of performing the required activities.","In addition, the compost turner has intelligent processing capabilities for compost data as well as its transmission, visualization and storage in a cloud server.","It is important to note that this work is a preprint that represents the current state of research.","The authors aim to publish the full paper in a peer-reviewed journal in the near future."],"url":"http://arxiv.org/abs/2401.13493v1","category":"cs.RO"}
{"created":"2024-01-24 14:37:44","title":"Visualization of rank-citation curves for fast detection of h-index anomalies in university metrics","abstract":"University rankings, despite facing criticism, continue to maintain their popularity. In the 2023 Scopus Ranking of Ukrainian Universities, certain institutions stood out due to their high h-index, despite modest publication and citation numbers. This phenomenon can be attributed to influential research topics or involvement in international collaborative research. However, these results may also be due to the authors' own efforts to increase the number of citations of their publications in order to improve their h-index. To investigate this, the publications from the top 30 universities in the ranking were analysed, revealing humpback rank-citation curves for two universities. These humpbacks indicate unusual trends in the citation data, especially considering the high percentage of self-citations and FWCI of analysed papers. While quantitative analysis has limitations, the combination of humped rank-citation curves, self-citations, FWCI, and previous research findings raises concerns about the possible causes of these anomalies in the citation data of the analysed universities. The method presented in this paper can aid ranking compilers and citation databases managers in identifying potential instances of citation data anomalies, emphasizing the importance of expert assessment for accurate conclusions.","sentences":["University rankings, despite facing criticism, continue to maintain their popularity.","In the 2023 Scopus Ranking of Ukrainian Universities, certain institutions stood out due to their high h-index, despite modest publication and citation numbers.","This phenomenon can be attributed to influential research topics or involvement in international collaborative research.","However, these results may also be due to the authors' own efforts to increase the number of citations of their publications in order to improve their h-index.","To investigate this, the publications from the top 30 universities in the ranking were analysed, revealing humpback rank-citation curves for two universities.","These humpbacks indicate unusual trends in the citation data, especially considering the high percentage of self-citations and FWCI of analysed papers.","While quantitative analysis has limitations, the combination of humped rank-citation curves, self-citations, FWCI, and previous research findings raises concerns about the possible causes of these anomalies in the citation data of the analysed universities.","The method presented in this paper can aid ranking compilers and citation databases managers in identifying potential instances of citation data anomalies, emphasizing the importance of expert assessment for accurate conclusions."],"url":"http://arxiv.org/abs/2401.13490v1","category":"cs.DL"}
{"created":"2024-01-24 14:36:08","title":"Fast Inverse Model Transformation: Algebraic Framework for Fast Data Plane Verification","abstract":"Data plane verification (DPV) analyzes routing tables and detects routing abnormalities and policy violations during network operation and planning. Thus, it has become an important tool to harden the networking infrastructure and the computing systems building on top. Substantial advancements have been made in the last decade and state-of-the-art DPV systems can achieve sub-us verification for an update of a single forwarding rule.   In this paper, we introduce fast inverse model transformation (FIMT), the first theoretical framework to systematically model and analyze centralized DPV systems. FIMT reveals the algebraic structure in the model update process, a key step in fast DPV systems. Thus, it can systematically analyze the correctness of several DPV systems, using algebraic properties. The theory also guides the design and implementation of NeoFlash, a refactored version of Flash with new optimization techniques. Evaluations show that NeoFlash outperforms existing state-of-the-art centralized DPV systems in various datasets and reveal insights to key techniques towards fast DPV.","sentences":["Data plane verification (DPV) analyzes routing tables and detects routing abnormalities and policy violations during network operation and planning.","Thus, it has become an important tool to harden the networking infrastructure and the computing systems building on top.","Substantial advancements have been made in the last decade and state-of-the-art DPV systems can achieve sub-us verification for an update of a single forwarding rule.   ","In this paper, we introduce fast inverse model transformation (FIMT), the first theoretical framework to systematically model and analyze centralized DPV systems.","FIMT reveals the algebraic structure in the model update process, a key step in fast DPV systems.","Thus, it can systematically analyze the correctness of several DPV systems, using algebraic properties.","The theory also guides the design and implementation of NeoFlash, a refactored version of Flash with new optimization techniques.","Evaluations show that NeoFlash outperforms existing state-of-the-art centralized DPV systems in various datasets and reveal insights to key techniques towards fast DPV."],"url":"http://arxiv.org/abs/2401.13488v1","category":"cs.NI"}
{"created":"2024-01-24 14:34:59","title":"Separable Physics-Informed Neural Networks for the solution of elasticity problems","abstract":"A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented. Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs). In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations. Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters.","sentences":["A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented.","Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs).","In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations.","Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters."],"url":"http://arxiv.org/abs/2401.13486v1","category":"math.NA"}
{"created":"2024-01-24 14:29:39","title":"How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment","abstract":"Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI, and that participants were more likely to knowingly adopt AI ideas when the task was difficult. Our findings suggest that introducing AI ideas into society may increase collective diversity but not individual creativity.","sentences":["Exposure to large language model output is rapidly increasing.","How will seeing AI-generated ideas affect human ideas?","We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea.","We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure).","Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas.","Hence, we capture the compounding effects of having LLMs 'in the culture loop'.","We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity.","AI made ideas different, not better.","There were no main effects of disclosure.","We also found that self-reported creative people were less influenced by knowing an idea was from AI, and that participants were more likely to knowingly adopt AI ideas when the task was difficult.","Our findings suggest that introducing AI ideas into society may increase collective diversity but not individual creativity."],"url":"http://arxiv.org/abs/2401.13481v1","category":"cs.CY"}
{"created":"2024-01-24 14:28:55","title":"The Dynamics of (Not) Unfollowing Misinformation Spreaders","abstract":"Many studies explore how people 'come into' misinformation exposure. But much less is known about how people 'come out of' misinformation exposure. Do people organically sever ties to misinformation spreaders? And what predicts doing so? Over six months, we tracked the frequency and predictors of ~1M followers unfollowing ~5K health misinformation spreaders on Twitter. We found that misinformation ties are persistent. Monthly unfollowing rates are just 0.52%. Users are also 31% more likely to unfollow non-misinformation spreaders than they are to unfollow misinformation spreaders. Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology. First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later. Second, liberals are more likely to unfollow than conservatives. Overall, we observe strong persistence of misinformation ties. The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place.","sentences":["Many studies explore how people 'come into' misinformation exposure.","But much less is known about how people 'come out of' misinformation exposure.","Do people organically sever ties to misinformation spreaders?","And what predicts doing so?","Over six months, we tracked the frequency and predictors of ~1M followers unfollowing ~5K health misinformation spreaders on Twitter.","We found that misinformation ties are persistent.","Monthly unfollowing rates are just 0.52%.","Users are also 31% more likely to unfollow non-misinformation spreaders than they are to unfollow misinformation spreaders.","Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology.","First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later.","Second, liberals are more likely to unfollow than conservatives.","Overall, we observe strong persistence of misinformation ties.","The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place."],"url":"http://arxiv.org/abs/2401.13480v1","category":"cs.SI"}
{"created":"2024-01-24 14:23:12","title":"SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval","abstract":"Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders. All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR.","sentences":["Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research.","However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role.","To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain.","This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents.","We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines.","We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP.","Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders.","All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR."],"url":"http://arxiv.org/abs/2401.13478v1","category":"cs.IR"}
{"created":"2024-01-24 14:08:38","title":"SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering","abstract":"Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robust to speech recognition errors.","sentences":["Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage.","SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems.","However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered.","This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem.","SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR).","No manually transcribed speech data is needed.","Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robust to speech recognition errors."],"url":"http://arxiv.org/abs/2401.13463v1","category":"cs.CL"}
{"created":"2024-01-24 14:04:08","title":"Growing from Exploration: A self-exploring framework for robots based on foundation models","abstract":"Intelligent robot is the ultimate goal in the robotics field. Existing works leverage learning-based or optimization-based methods to accomplish human-defined tasks. However, the challenge of enabling robots to explore various environments autonomously remains unresolved. In this work, we propose a framework named GExp, which enables robots to explore and learn autonomously without human intervention. To achieve this goal, we devise modules including self-exploration, knowledge-base-building, and close-loop feedback based on foundation models. Inspired by the way that infants interact with the world, GExp encourages robots to understand and explore the environment with a series of self-generated tasks. During the process of exploration, the robot will acquire skills from beneficial experiences that are useful in the future. GExp provides robots with the ability to solve complex tasks through self-exploration. GExp work is independent of prior interactive knowledge and human intervention, allowing it to adapt directly to different scenarios, unlike previous studies that provided in-context examples as few-shot learning. In addition, we propose a workflow of deploying the real-world robot system with self-learned skills as an embodied assistant.","sentences":["Intelligent robot is the ultimate goal in the robotics field.","Existing works leverage learning-based or optimization-based methods to accomplish human-defined tasks.","However, the challenge of enabling robots to explore various environments autonomously remains unresolved.","In this work, we propose a framework named GExp, which enables robots to explore and learn autonomously without human intervention.","To achieve this goal, we devise modules including self-exploration, knowledge-base-building, and close-loop feedback based on foundation models.","Inspired by the way that infants interact with the world, GExp encourages robots to understand and explore the environment with a series of self-generated tasks.","During the process of exploration, the robot will acquire skills from beneficial experiences that are useful in the future.","GExp provides robots with the ability to solve complex tasks through self-exploration.","GExp work is independent of prior interactive knowledge and human intervention, allowing it to adapt directly to different scenarios, unlike previous studies that provided in-context examples as few-shot learning.","In addition, we propose a workflow of deploying the real-world robot system with self-learned skills as an embodied assistant."],"url":"http://arxiv.org/abs/2401.13462v1","category":"cs.RO"}
{"created":"2024-01-24 14:02:09","title":"Multi-Agent Diagnostics for Robustness via Illuminated Diversity","abstract":"In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning. Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which \"masters\" the game through 45 days of training on a large-scale distributed infrastructure. We expose key shortcomings in TiZero's tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems.","sentences":["In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial.","Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase.","This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges.","To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies.","Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings.","We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning.","Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which \"masters\" the game through 45 days of training on a large-scale distributed infrastructure.","We expose key shortcomings in TiZero's tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems."],"url":"http://arxiv.org/abs/2401.13460v1","category":"cs.LG"}
{"created":"2024-01-24 14:01:52","title":"Variational Quantum Eigensolvers with Quantum Gaussian Filters for solving ground-state problems in quantum many-body systems","abstract":"We present a novel quantum algorithm for approximating the ground-state in quantum many-body systems, particularly suited for Noisy Intermediate-Scale Quantum (NISQ) devices. Our approach integrates Variational Quantum Eigensolvers (VQE) with Quantum Gaussian Filters (QGF), utilizing an iterative methodology that discretizes the application of the QGF operator into small, optimized steps through VQE. Demonstrated on the Transverse Field Ising models, our method shows improved convergence speed and accuracy, particularly under noisy conditions, compared to conventional VQE methods. This advancement highlights the potential of our algorithm in effectively addressing complex quantum simulations, marking a significant stride in quantum computing applications within the NISQ era.","sentences":["We present a novel quantum algorithm for approximating the ground-state in quantum many-body systems, particularly suited for Noisy Intermediate-Scale Quantum (NISQ) devices.","Our approach integrates Variational Quantum Eigensolvers (VQE) with Quantum Gaussian Filters (QGF), utilizing an iterative methodology that discretizes the application of the QGF operator into small, optimized steps through VQE.","Demonstrated on the Transverse Field Ising models, our method shows improved convergence speed and accuracy, particularly under noisy conditions, compared to conventional VQE methods.","This advancement highlights the potential of our algorithm in effectively addressing complex quantum simulations, marking a significant stride in quantum computing applications within the NISQ era."],"url":"http://arxiv.org/abs/2401.13459v1","category":"quant-ph"}
{"created":"2024-01-24 13:57:51","title":"Diffusion of an Active Particle Bound to a Generalized Elastic Model: Fractional Langevin Equation","abstract":"We investigate the influence of a self-propelling, out-of-equilibrium active particle on generalized elastic systems, including flexible and semiflexible polymers, fluid membranes, and fluctuating interfaces, while accounting for long-ranged hydrodynamic effects. We derive the fractional Langevin equation governing the dynamics of the active particle, as well as that of any other passive particle (or probe) bound to the elastic system. This equation demonstrates analytically how the active particle dynamics is influenced by the interplay of both the non-equilibrium force and of the viscoelastic environment. Our study explores the diffusional behavior emerging for both the active particle and a distant probe.The active particle undergoes three different surprising and counterintuitive regimes identified by the distinct dynamical time-scales: a pseudo-ballistic initial phase, a drastic decrease of the mobility and an asymptotic subdiffusive regime.","sentences":["We investigate the influence of a self-propelling, out-of-equilibrium active particle on generalized elastic systems, including flexible and semiflexible polymers, fluid membranes, and fluctuating interfaces, while accounting for long-ranged hydrodynamic effects.","We derive the fractional Langevin equation governing the dynamics of the active particle, as well as that of any other passive particle (or probe) bound to the elastic system.","This equation demonstrates analytically how the active particle dynamics is influenced by the interplay of both the non-equilibrium force and of the viscoelastic environment.","Our study explores the diffusional behavior emerging for both the active particle and a distant probe.","The active particle undergoes three different surprising and counterintuitive regimes identified by the distinct dynamical time-scales: a pseudo-ballistic initial phase, a drastic decrease of the mobility and an asymptotic subdiffusive regime."],"url":"http://arxiv.org/abs/2401.13457v1","category":"cond-mat.soft"}
{"created":"2024-01-24 13:42:49","title":"Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation","abstract":"In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance. To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration. It first generates a desensitized public reference data pool with transformation and probability data generation methods. For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function. Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors. During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data. Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data.","sentences":["In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places.","There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance.","Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate.","Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration.","However, existing CL-based recommendations typically use a single reference for all users.","Reference data valuable for one user might be harmful to another, given diverse user preferences.","Users may not offer meaningful soft decisions on items outside their interest scope.","Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance.","To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration.","It first generates a desensitized public reference data pool with transformation and probability data generation methods.","For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function.","Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors.","During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data.","Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data."],"url":"http://arxiv.org/abs/2401.13448v1","category":"cs.IR"}
{"created":"2024-01-24 13:42:24","title":"Symbolic Equation Solving via Reinforcement Learning","abstract":"Machine-learning methods are gradually being adopted in a great variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics. A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations. Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function). Thus far, these rules have usually needed to be discovered and subsequently programmed by humans. Focusing on the paradigmatic example of solving linear equations in symbolic form, we demonstrate how the process of finding elementary transformation rules and step-by-step solutions can be automated using reinforcement learning with deep neural networks.","sentences":["Machine-learning methods are gradually being adopted in a great variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics.","A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations.","Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function).","Thus far, these rules have usually needed to be discovered and subsequently programmed by humans.","Focusing on the paradigmatic example of solving linear equations in symbolic form, we demonstrate how the process of finding elementary transformation rules and step-by-step solutions can be automated using reinforcement learning with deep neural networks."],"url":"http://arxiv.org/abs/2401.13447v1","category":"cs.LG"}
{"created":"2024-01-24 13:36:50","title":"Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption","abstract":"In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs with fewer parameters. In some instances, even ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4. Furthermore, the results indicate a minimal invocation frequency of CGPE on LLMs, suggesting reduced computational overhead. For organizations and individuals facing constraints in computational resources, our research offers significant practical value.","sentences":["In recent times, large language models (LLMs) have showcased remarkable capabilities.","However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries.","While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities.","This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance.","In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities.","Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base.","Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs with fewer parameters.","In some instances, even ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.","Furthermore, the results indicate a minimal invocation frequency of CGPE on LLMs, suggesting reduced computational overhead.","For organizations and individuals facing constraints in computational resources, our research offers significant practical value."],"url":"http://arxiv.org/abs/2401.13444v1","category":"cs.CL"}
{"created":"2024-01-24 13:32:47","title":"Finite-Precision Arithmetic Transceiver for Massive MIMO Systems","abstract":"Efficient implementation of massive multiple-input-multiple-output (MIMO) transceivers is essential for the next-generation wireless networks. To reduce the high computational complexity of the massive MIMO transceiver, in this paper, we propose a new massive MIMO architecture using finite-precision arithmetic. First, we conduct the rounding error analysis and derive the lower bound of the achievable rate for single-input-multiple-output (SIMO) using maximal ratio combining (MRC) and multiple-input-single-output (MISO) systems using maximal ratio transmission (MRT) with finite-precision arithmetic. Then, considering the multi-user scenario, the rounding error analysis of zero-forcing (ZF) detection and precoding is derived by using the normal equations (NE) method. The corresponding lower bounds of the achievable sum rate are also derived and asymptotic analyses are presented. Built upon insights from these analyses and lower bounds, we propose a mixed-precision architecture for massive MIMO systems to offset performance gaps due to finite-precision arithmetic. The corresponding analysis of rounding errors and computational costs is obtained. Simulation results validate the derived bounds and underscore the superiority of the proposed mixed-precision architecture to the conventional structure.","sentences":["Efficient implementation of massive multiple-input-multiple-output (MIMO) transceivers is essential for the next-generation wireless networks.","To reduce the high computational complexity of the massive MIMO transceiver, in this paper, we propose a new massive MIMO architecture using finite-precision arithmetic.","First, we conduct the rounding error analysis and derive the lower bound of the achievable rate for single-input-multiple-output (SIMO) using maximal ratio combining (MRC) and multiple-input-single-output (MISO) systems using maximal ratio transmission (MRT) with finite-precision arithmetic.","Then, considering the multi-user scenario, the rounding error analysis of zero-forcing (ZF) detection and precoding is derived by using the normal equations (NE) method.","The corresponding lower bounds of the achievable sum rate are also derived and asymptotic analyses are presented.","Built upon insights from these analyses and lower bounds, we propose a mixed-precision architecture for massive MIMO systems to offset performance gaps due to finite-precision arithmetic.","The corresponding analysis of rounding errors and computational costs is obtained.","Simulation results validate the derived bounds and underscore the superiority of the proposed mixed-precision architecture to the conventional structure."],"url":"http://arxiv.org/abs/2401.13442v1","category":"cs.IT"}
{"created":"2024-01-24 13:30:18","title":"Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance Control","abstract":"Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings. Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans. This work presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots. We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels. A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a new Cartesian impedance controller. We specifically focus here on planar soft robot-based architected metamaterials, which require the development of a novel control architecture to deal with the peculiar nonlinearities - e.g., non-affinity in control. We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation. We observe that the user reaches the proximity of the setpoint in 66% of steps and that for successful steps, the average response time is 21.5s. We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness.","sentences":["Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings.","Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans.","This work presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots.","We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels.","A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a new Cartesian impedance controller.","We specifically focus here on planar soft robot-based architected metamaterials, which require the development of a novel control architecture to deal with the peculiar nonlinearities - e.g., non-affinity in control.","We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation.","We observe that the user reaches the proximity of the setpoint in 66% of steps and that for successful steps, the average response time is 21.5s.","We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness."],"url":"http://arxiv.org/abs/2401.13441v1","category":"cs.RO"}
{"created":"2024-01-24 13:16:06","title":"Model Predictive Wave Disturbance Rejection for Underwater Soft Robotic Manipulators","abstract":"Inspired by the octopus and other animals living in water, soft robots should naturally lend themselves to underwater operations, as supported by encouraging validations in deep water scenarios. This work deals with equipping soft arms with the intelligence necessary to move precisely in wave-dominated environments, such as shallow waters where marine renewable devices are located. This scenario is substantially more challenging than calm deep water since, at low operational depths, hydrodynamic wave disturbances can represent a significant impediment. We propose a control strategy based on Nonlinear Model Predictive Control that can account for wave disturbances explicitly, optimising control actions by considering an estimate of oncoming hydrodynamic loads. The proposed strategy is validated through a set of tasks covering set-point regulation, trajectory tracking and mechanical failure compensation, all under a broad range of varying significant wave heights and peak spectral periods. The proposed control methodology displays positional error reductions as large as 84% with respect to a baseline controller, proving the effectiveness of the method. These initial findings present a first step in the development and deployment of soft manipulators for performing tasks in hazardous water environments.","sentences":["Inspired by the octopus and other animals living in water, soft robots should naturally lend themselves to underwater operations, as supported by encouraging validations in deep water scenarios.","This work deals with equipping soft arms with the intelligence necessary to move precisely in wave-dominated environments, such as shallow waters where marine renewable devices are located.","This scenario is substantially more challenging than calm deep water since, at low operational depths, hydrodynamic wave disturbances can represent a significant impediment.","We propose a control strategy based on Nonlinear Model Predictive Control that can account for wave disturbances explicitly, optimising control actions by considering an estimate of oncoming hydrodynamic loads.","The proposed strategy is validated through a set of tasks covering set-point regulation, trajectory tracking and mechanical failure compensation, all under a broad range of varying significant wave heights and peak spectral periods.","The proposed control methodology displays positional error reductions as large as 84% with respect to a baseline controller, proving the effectiveness of the method.","These initial findings present a first step in the development and deployment of soft manipulators for performing tasks in hazardous water environments."],"url":"http://arxiv.org/abs/2401.13439v1","category":"cs.RO"}
{"created":"2024-01-24 13:06:57","title":"Generating random Gaussian states","abstract":"We develop a method for the random sampling of (multimode) Gaussian states in terms of their covariance matrix, which we refer to as a random quantum covariance matrix (RQCM). We analyze the distribution of marginals and demonstrate that the eigenvalues of an RQCM converge to a shifted semicircular distribution in the limit of a large number of modes. We provide insights into the entanglement of such states based on the positive partial transpose (PPT) criteria. Additionally, we show that the symplectic eigenvalues of an RQCM converge to a probability distribution that can be characterized using free probability. We present numerical estimates for the probability of a RQCM being separable and, if not, its extendibility degree, for various parameter values and mode bipartitions.","sentences":["We develop a method for the random sampling of (multimode) Gaussian states in terms of their covariance matrix, which we refer to as a random quantum covariance matrix (RQCM).","We analyze the distribution of marginals and demonstrate that the eigenvalues of an RQCM converge to a shifted semicircular distribution in the limit of a large number of modes.","We provide insights into the entanglement of such states based on the positive partial transpose (PPT) criteria.","Additionally, we show that the symplectic eigenvalues of an RQCM converge to a probability distribution that can be characterized using free probability.","We present numerical estimates for the probability of a RQCM being separable and, if not, its extendibility degree, for various parameter values and mode bipartitions."],"url":"http://arxiv.org/abs/2401.13435v1","category":"quant-ph"}
{"created":"2024-01-24 13:05:48","title":"Query Exposure Prediction for Groups of Documents in Rankings","abstract":"The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query. To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model. However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage. Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking. For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions. With this in mind, it is beneficial to predict the amount of exposure that a group of documents is likely to receive in the results of the first stage retrieval process, in order to ensure that there are a sufficient number of documents included from each of the groups. In this paper, we introduce the novel task of query exposure prediction (QEP). Specifically, we propose the first approach for predicting the distribution of exposure that groups of documents will receive for a given query. Our new approach, called GEP, uses lexical information from individual groups of documents to estimate the exposure the groups will receive in a ranking. Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our proposed GEP approach results in exposure predictions that are up to 40 % more accurate than the predictions of adapted existing query performance prediction and resource allocation approaches.","sentences":["The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query.","To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model.","However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage.","Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking.","For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions.","With this in mind, it is beneficial to predict the amount of exposure that a group of documents is likely to receive in the results of the first stage retrieval process, in order to ensure that there are a sufficient number of documents included from each of the groups.","In this paper, we introduce the novel task of query exposure prediction (QEP).","Specifically, we propose the first approach for predicting the distribution of exposure that groups of documents will receive for a given query.","Our new approach, called GEP, uses lexical information from individual groups of documents to estimate the exposure the groups will receive in a ranking.","Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our proposed GEP approach results in exposure predictions that are up to 40 % more accurate than the predictions of adapted existing query performance prediction and resource allocation approaches."],"url":"http://arxiv.org/abs/2401.13434v1","category":"cs.IR"}
{"created":"2024-01-24 13:03:28","title":"Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond","abstract":"Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation cost, we develop a semi-supervised learning scheme to improve warping quality by exploiting unlabeled data. It is formulated through dual transformation between the searched control points of unlabeled data and its graphic augmentation, yielding an implicit correction consistency constraint. Finally, we collect massive unlabeled data to exhibit the benefit of our semi-supervised scheme in rotation correction. Extensive experiments demonstrate the superiority and universality of CoupledTPS over the existing state-of-the-art (SoTA) solutions for rotation correction and beyond. The code and data will be available at https://github.com/nie-lang/CoupledTPS.","sentences":["Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions.","With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion.","In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction.","To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation.","Concretely, we first design an iterative search to predict new control points according to the current latent condition.","Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps.","Besides, in light of the laborious annotation cost, we develop a semi-supervised learning scheme to improve warping quality by exploiting unlabeled data.","It is formulated through dual transformation between the searched control points of unlabeled data and its graphic augmentation, yielding an implicit correction consistency constraint.","Finally, we collect massive unlabeled data to exhibit the benefit of our semi-supervised scheme in rotation correction.","Extensive experiments demonstrate the superiority and universality of CoupledTPS over the existing state-of-the-art (SoTA) solutions for rotation correction and beyond.","The code and data will be available at https://github.com/nie-lang/CoupledTPS."],"url":"http://arxiv.org/abs/2401.13432v1","category":"cs.CV"}
{"created":"2024-01-24 12:58:08","title":"Detection of Correlated Random Vectors","abstract":"In this paper, we investigate the problem of deciding whether two standard normal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and $\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\\mathsf{X}$ and a randomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with correlation $\\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices, and furthermore allow for partial correlations between these two.","sentences":["In this paper, we investigate the problem of deciding whether two standard normal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and $\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not.","This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\\mathsf{X}$ and a randomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with correlation $\\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions.","We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices, and furthermore allow for partial correlations between these two."],"url":"http://arxiv.org/abs/2401.13429v1","category":"cs.IT"}
{"created":"2024-01-24 12:47:41","title":"Wrinkling of fluid deformable surfaces","abstract":"Wrinkling instabilities of thin elastic sheets can be used to generate periodic structures over a wide range of length scales. Viscosity of the thin elastic sheet or its surrounding medium has been shown to be responsible for dynamic processes. While this has been explored for solid as well as liquid thin elastic sheets we here consider wrinkling of fluid deformable surfaces, which show a solid-fluid duality and have been established as model systems for biomembranes and cellular sheets. We use this hydrodynamic theory and numerically explore the formation of wrinkles and their coarsening, either by a continuous reduction of the enclosed volume or the continuous increase of the surface area. Both lead to almost identical results for wrinkle formation and the coarsening process, for which a universal scaling law for the wavenumber is obtained for a broad range of surface viscosity and rate of change of volume or area. However, for large Reynolds numbers and small changes in volume or area wrinkling can be suppressed and surface hydrodynamics allows for global shape changes following the minimal energy configurations of the Helfrich energy for corresponding reduced volumes.","sentences":["Wrinkling instabilities of thin elastic sheets can be used to generate periodic structures over a wide range of length scales.","Viscosity of the thin elastic sheet or its surrounding medium has been shown to be responsible for dynamic processes.","While this has been explored for solid as well as liquid thin elastic sheets we here consider wrinkling of fluid deformable surfaces, which show a solid-fluid duality and have been established as model systems for biomembranes and cellular sheets.","We use this hydrodynamic theory and numerically explore the formation of wrinkles and their coarsening, either by a continuous reduction of the enclosed volume or the continuous increase of the surface area.","Both lead to almost identical results for wrinkle formation and the coarsening process, for which a universal scaling law for the wavenumber is obtained for a broad range of surface viscosity and rate of change of volume or area.","However, for large Reynolds numbers and small changes in volume or area wrinkling can be suppressed and surface hydrodynamics allows for global shape changes following the minimal energy configurations of the Helfrich energy for corresponding reduced volumes."],"url":"http://arxiv.org/abs/2401.13426v1","category":"cond-mat.soft"}
{"created":"2024-01-24 12:32:08","title":"Federated learning with distributed fixed design quantum chips and quantum channels","abstract":"The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.   In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model parameters, but instead sends the operator as a quantum state, the clients are not required to share the model. This allows for the creation of asynchronous learning models. In addition, the model as a quantum state is fed into client-side chips directly; therefore, it does not require measurements on the upcoming quantum state to obtain model parameters in order to compute gradients. This can provide efficiency over the models where parameter vector is sent via classical or quantum channels and local gradients are obtained through the obtained values of these parameters.","sentences":["The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients.","However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected.","Therefore, the quantum version of federated learning can be used to provide more privacy.","Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.   ","In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server.","Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters.","Since the server does not send model parameters, but instead sends the operator as a quantum state, the clients are not required to share the model.","This allows for the creation of asynchronous learning models.","In addition, the model as a quantum state is fed into client-side chips directly; therefore, it does not require measurements on the upcoming quantum state to obtain model parameters in order to compute gradients.","This can provide efficiency over the models where parameter vector is sent via classical or quantum channels and local gradients are obtained through the obtained values of these parameters."],"url":"http://arxiv.org/abs/2401.13421v1","category":"quant-ph"}
{"created":"2024-01-24 12:31:52","title":"Distributed network for measuring climatic parameters in heterogeneous environments: Application in a greenhouse","abstract":"In Mediterranean countries of Southern Europe, the climatic conditions are usually favourable to cultivate greenhouse vegetables but not always for workers. The aim of this study was to design a network of weather stations capable of gathering data of environmental parameters related to the wellbeing of workers in greenhouses in south-eastern Spain. The unevenness of the thermal environment was studied both vertically as well as horizontally following guideline ISO 7726. The results indicate that the greenhouse should be considered a heterogeneous environment, implying that, for an evaluation of the environmental conditions related to thermal stress of the workers inside the greenhouse, measurements should be taken at different points within the greenhouse at three heights (ankle, abdomen, and head).","sentences":["In Mediterranean countries of Southern Europe, the climatic conditions are usually favourable to cultivate greenhouse vegetables but not always for workers.","The aim of this study was to design a network of weather stations capable of gathering data of environmental parameters related to the wellbeing of workers in greenhouses in south-eastern Spain.","The unevenness of the thermal environment was studied both vertically as well as horizontally following guideline ISO 7726.","The results indicate that the greenhouse should be considered a heterogeneous environment, implying that, for an evaluation of the environmental conditions related to thermal stress of the workers inside the greenhouse, measurements should be taken at different points within the greenhouse at three heights (ankle, abdomen, and head)."],"url":"http://arxiv.org/abs/2401.13420v1","category":"cs.DC"}
{"created":"2024-01-24 12:30:04","title":"Serial fusion of multi-modal biometric systems","abstract":"Serial, or sequential, fusion of multiple biometric matchers has been not thoroughly investigated so far. However, this approach exhibits some advantages with respect to the widely adopted parallel approaches. In this paper, we propose a novel theoretical framework for the assessment of performance of such systems, based on a previous work of the authors. Benefits in terms of performance are theoretically evaluated, as well as estimation errors in the model parameters computation. Model is analyzed from the viewpoint of its pros and cons, by mean of preliminary experiments performed on NIST Biometric Score Set 1.","sentences":["Serial, or sequential, fusion of multiple biometric matchers has been not thoroughly investigated so far.","However, this approach exhibits some advantages with respect to the widely adopted parallel approaches.","In this paper, we propose a novel theoretical framework for the assessment of performance of such systems, based on a previous work of the authors.","Benefits in terms of performance are theoretically evaluated, as well as estimation errors in the model parameters computation.","Model is analyzed from the viewpoint of its pros and cons, by mean of preliminary experiments performed on NIST Biometric Score Set 1."],"url":"http://arxiv.org/abs/2401.13418v1","category":"cs.CV"}
{"created":"2024-01-24 12:20:56","title":"Characterizing Perspective Error in Voxel-Based Lidar Scan Matching","abstract":"This paper quantifies an error source that limits the accuracy of lidar scan matching, particularly for voxel-based methods. Lidar scan matching, which is used in dead reckoning (also known as lidar odometry) and mapping, computes the rotation and translation that best align a pair of point clouds. Perspective errors occur when a scene is viewed from different angles, with different surfaces becoming visible or occluded from each viewpoint. To explain perspective anomalies observed in data, this paper models perspective errors for two objects representative of urban landscapes: a cylindrical column and a dual-wall corner. For each object, we provide an analytical model of the perspective error for voxel-based lidar scan matching. We then analyze how perspective errors accumulate as a lidar-equipped vehicle moves past these objects.","sentences":["This paper quantifies an error source that limits the accuracy of lidar scan matching, particularly for voxel-based methods.","Lidar scan matching, which is used in dead reckoning (also known as lidar odometry) and mapping, computes the rotation and translation that best align a pair of point clouds.","Perspective errors occur when a scene is viewed from different angles, with different surfaces becoming visible or occluded from each viewpoint.","To explain perspective anomalies observed in data, this paper models perspective errors for two objects representative of urban landscapes: a cylindrical column and a dual-wall corner.","For each object, we provide an analytical model of the perspective error for voxel-based lidar scan matching.","We then analyze how perspective errors accumulate as a lidar-equipped vehicle moves past these objects."],"url":"http://arxiv.org/abs/2401.13416v1","category":"cs.RO"}
{"created":"2024-01-24 12:18:31","title":"GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition","abstract":"Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.","sentences":["Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection.","To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition.","GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality.","Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities.","Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training."],"url":"http://arxiv.org/abs/2401.13414v1","category":"cs.CV"}
{"created":"2024-01-24 12:14:07","title":"Generating stellar spectra using Neural Networks","abstract":"A new generative technique is presented in this paper that uses Deep Learning to reconstruct stellar spectra based on a set of stellar parameters. Two different Neural Networks were trained allowing the generation of new spectra. First, an autoencoder is trained on a set of BAFGK synthetic data calculated using ATLAS9 model atmospheres and SYNSPEC radiative transfer code. These spectra are calculated in the wavelength range of Gaia RVS between 8 400 and 8 800 {\\AA}. Second, we trained a Fully Dense Neural Network to relate the stellar parameters to the Latent Space of the autoencoder. Finally, we linked the Fully Dense Neural Network to the decoder part of the autoencoder and we built a model that uses as input any combination of $T_{eff}$, $\\log g$, $v_e \\sin i$, [M/H], and $\\xi_t$ and output a normalized spectrum. The generated spectra are shown to represent all the line profiles and flux values as the ones calculated using the classical radiative transfer code. The accuracy of our technique is tested using a stellar parameter determination procedure and the results show that the generated spectra have the same characteristics as the synthetic ones.","sentences":["A new generative technique is presented in this paper that uses Deep Learning to reconstruct stellar spectra based on a set of stellar parameters.","Two different Neural Networks were trained allowing the generation of new spectra.","First, an autoencoder is trained on a set of BAFGK synthetic data calculated using ATLAS9 model atmospheres and SYNSPEC radiative transfer code.","These spectra are calculated in the wavelength range of Gaia RVS between 8 400 and 8 800 {\\AA}.","Second, we trained a Fully Dense Neural Network to relate the stellar parameters to the Latent Space of the autoencoder.","Finally, we linked the Fully Dense Neural Network to the decoder part of the autoencoder and we built a model that uses as input any combination of $T_{eff}$, $\\log g$, $v_e \\sin i$, [M/H], and $\\xi_t$ and output a normalized spectrum.","The generated spectra are shown to represent all the line profiles and flux values as the ones calculated using the classical radiative transfer code.","The accuracy of our technique is tested using a stellar parameter determination procedure and the results show that the generated spectra have the same characteristics as the synthetic ones."],"url":"http://arxiv.org/abs/2401.13411v1","category":"astro-ph.SR"}
{"created":"2024-01-24 12:11:41","title":"How to Forget Clients in Federated Online Learning to Rank?","abstract":"Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \\textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation.   In this paper, we study an effective and efficient unlearning method that can remove a client's contribution without compromising the overall ranker effectiveness and without needing to retrain the global ranker from scratch. A key challenge is how to measure whether the model has unlearned the contributions from the client $c^*$ that has requested removal. For this, we instruct $c^*$ to perform a poisoning attack (add noise to this client updates) and then we measure whether the impact of the attack is lessened when the unlearning process has taken place. Through experiments on four datasets, we demonstrate the effectiveness and efficiency of the unlearning strategy under different combinations of parameter settings.","sentences":["Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \\textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models.","In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system.","In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model.","Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client.","By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation.   ","In this paper, we study an effective and efficient unlearning method that can remove a client's contribution without compromising the overall ranker effectiveness and without needing to retrain the global ranker from scratch.","A key challenge is how to measure whether the model has unlearned the contributions from the client $c^*$ that has requested removal.","For this, we instruct $c^*$ to perform a poisoning attack (add noise to this client updates) and then we measure whether the impact of the attack is lessened when the unlearning process has taken place.","Through experiments on four datasets, we demonstrate the effectiveness and efficiency of the unlearning strategy under different combinations of parameter settings."],"url":"http://arxiv.org/abs/2401.13410v1","category":"cs.CR"}
{"created":"2024-01-24 12:08:58","title":"Causal Perception","abstract":"Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal properties of faithfulness and consistency. We illustrate our framework through a series of decision-making examples and discuss relevant fairness applications. The goal of this work is to position perception as a parameter of interest, useful for extending the standard, single interpretation ADM problem formulation.","sentences":["Perception occurs when two individuals interpret the same information differently.","Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems.","In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging.","In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual.","We also formalize individual experience as additional causal knowledge that comes with and is used by an individual.","Further, we define and discuss loaded attributes, which are attributes prone to evoke perception.","Sensitive attributes, such as gender and race, are clear examples of loaded attributes.","We define two kinds of causal perception, unfaithful and inconsistent, based on the causal properties of faithfulness and consistency.","We illustrate our framework through a series of decision-making examples and discuss relevant fairness applications.","The goal of this work is to position perception as a parameter of interest, useful for extending the standard, single interpretation ADM problem formulation."],"url":"http://arxiv.org/abs/2401.13408v1","category":"cs.AI"}
{"created":"2024-01-24 12:05:06","title":"Increasing, not Diminishing: Investigating the Returns of Highly Maintainable Code","abstract":"Understanding and effectively managing Technical Debt (TD) remains a vital challenge in software engineering. While many studies on code-level TD have been published, few illustrate the business impact of low-quality source code. In this study, we combine two publicly available datasets to study the association between code quality on the one hand, and defect count and implementation time on the other hand. We introduce a value-creation model, derived from regression analyses, to explore relative changes from a baseline. Our results show that the associations vary across different intervals of code quality. Furthermore, the value model suggests strong non-linearities at the extremes of the code quality spectrum. Most importantly, the model suggests amplified returns on investment in the upper end. We discuss the findings within the context of the \"broken windows\" theory and recommend organizations to diligently prevent the introduction of code smells in files with high churn. Finally, we argue that the value-creation model can be used to initiate discussions regarding the return on investment in refactoring efforts.","sentences":["Understanding and effectively managing Technical Debt (TD) remains a vital challenge in software engineering.","While many studies on code-level TD have been published, few illustrate the business impact of low-quality source code.","In this study, we combine two publicly available datasets to study the association between code quality on the one hand, and defect count and implementation time on the other hand.","We introduce a value-creation model, derived from regression analyses, to explore relative changes from a baseline.","Our results show that the associations vary across different intervals of code quality.","Furthermore, the value model suggests strong non-linearities at the extremes of the code quality spectrum.","Most importantly, the model suggests amplified returns on investment in the upper end.","We discuss the findings within the context of the \"broken windows\" theory and recommend organizations to diligently prevent the introduction of code smells in files with high churn.","Finally, we argue that the value-creation model can be used to initiate discussions regarding the return on investment in refactoring efforts."],"url":"http://arxiv.org/abs/2401.13407v1","category":"cs.SE"}
{"created":"2024-01-24 12:01:51","title":"Entanglement harvesting in cosmic string spacetime","abstract":"We study the entanglement harvesting phenomenon for static detectors locally interacting with massless scalar fields in the cosmic string spacetime which is locally flat but with a conical structure characterized by a deficit angle. Specifically, three alignments of the detectors with respect to the string, i.e., parallel and vertical alignments with the detectors on the same side of the string, and vertical alignment with the detectors on two different sides, are examined. For the alignments on the same side of the string, we find that the presence of a cosmic string may either assist or inhibit entanglement harvesting both in the sense of the entanglement harvested and the harvesting-achievable range of interdetector separation depending on the detector-to-string distance, and this is remarkably different from the case of a locally flat spacetime with a reflecting boundary where the boundary always enlarges the harvesting-achievable range. For the alignment with detectors on two different sides of the string, the detectors notably can always harvest more entanglement than those in flat spacetime without a cosmic string, which is in sharp contrast to those on the same side. Interestingly, the presence of a cosmic string enlarges the harvesting-achievable range for the detectors in vertical alignment only in the vicinity of the string, while it always reduces the harvesting-achievable range for the detectors in parallel alignment.","sentences":["We study the entanglement harvesting phenomenon for static detectors locally interacting with massless scalar fields in the cosmic string spacetime which is locally flat but with a conical structure characterized by a deficit angle.","Specifically, three alignments of the detectors with respect to the string, i.e., parallel and vertical alignments with the detectors on the same side of the string, and vertical alignment with the detectors on two different sides, are examined.","For the alignments on the same side of the string, we find that the presence of a cosmic string may either assist or inhibit entanglement harvesting both in the sense of the entanglement harvested and the harvesting-achievable range of interdetector separation depending on the detector-to-string distance, and this is remarkably different from the case of a locally flat spacetime with a reflecting boundary where the boundary always enlarges the harvesting-achievable range.","For the alignment with detectors on two different sides of the string, the detectors notably can always harvest more entanglement than those in flat spacetime without a cosmic string, which is in sharp contrast to those on the same side.","Interestingly, the presence of a cosmic string enlarges the harvesting-achievable range for the detectors in vertical alignment only in the vicinity of the string, while it always reduces the harvesting-achievable range for the detectors in parallel alignment."],"url":"http://arxiv.org/abs/2401.13406v1","category":"quant-ph"}
{"created":"2024-01-24 11:58:30","title":"Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter","abstract":"Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation. In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset). Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically. RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects. The object to grasp is then determined by the confidence score of the segmentation algorithm. Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively. Supplementary material is available at https://sites.google.com/view/synthetic-dataset-generation.","sentences":["Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation.","In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset).","Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically.","RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects.","The object to grasp is then determined by the confidence score of the segmentation algorithm.","Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively.","Supplementary material is available at https://sites.google.com/view/synthetic-dataset-generation."],"url":"http://arxiv.org/abs/2401.13405v1","category":"cs.CV"}
{"created":"2024-01-24 11:57:42","title":"Hidden symmetries of a self-dual monopole","abstract":"The symmetries of a spinning particle in the field of a self-dual monopole are studied from the viewpoint of supersymmetric quantum mechanics.","sentences":["The symmetries of a spinning particle in the field of a self-dual monopole are studied from the viewpoint of supersymmetric quantum mechanics."],"url":"http://arxiv.org/abs/2401.13404v1","category":"hep-th"}
{"created":"2024-01-24 11:55:09","title":"On fixed point theory in partially ordered sets and an application to asymptotic complexity of algorithms","abstract":"The celebrated Kleene fixed point theorem is crucial in the mathematical modelling of recursive specifications in Denotational Semantics. In this paper we discuss whether the hypothesis of the aforementioned result can be weakened. An affirmative answer to the aforesaid inquiry is provided so that a characterization of those properties that a self-mapping must satisfy in order to guarantee that its set of fixed points is non-empty when no notion of completeness are assumed to be satisfied by the partially ordered set. Moreover, the case in which the partially ordered set is coming from a quasi-metric space is treated in depth. Finally, an application of the exposed theory is obtained. Concretely, a mathematical method to discuss the asymptotic complexity of those algorithms whose running time of computing fulfills a recurrence equation is presented. Moreover, the aforesaid method retrieves the fixed point based methods that appear in the literature for asymptotic complexity analysis of algorithms. However, our new method improves the aforesaid methods because it imposes fewer requirements than those that have been assumed in the literature and, in addition, it allows to state simultaneously upper and lower asymptotic bounds for the running time computing.","sentences":["The celebrated Kleene fixed point theorem is crucial in the mathematical modelling of recursive specifications in Denotational Semantics.","In this paper we discuss whether the hypothesis of the aforementioned result can be weakened.","An affirmative answer to the aforesaid inquiry is provided so that a characterization of those properties that a self-mapping must satisfy in order to guarantee that its set of fixed points is non-empty when no notion of completeness are assumed to be satisfied by the partially ordered set.","Moreover, the case in which the partially ordered set is coming from a quasi-metric space is treated in depth.","Finally, an application of the exposed theory is obtained.","Concretely, a mathematical method to discuss the asymptotic complexity of those algorithms whose running time of computing fulfills a recurrence equation is presented.","Moreover, the aforesaid method retrieves the fixed point based methods that appear in the literature for asymptotic complexity analysis of algorithms.","However, our new method improves the aforesaid methods because it imposes fewer requirements than those that have been assumed in the literature and, in addition, it allows to state simultaneously upper and lower asymptotic bounds for the running time computing."],"url":"http://arxiv.org/abs/2401.13400v1","category":"cs.IT"}
{"created":"2024-01-24 11:53:53","title":"Real-time Risk Metrics for Programmatic Stablecoin Crypto Asset-Liability Management (CALM)","abstract":"Stablecoins have turned out to be the \"killer\" use case of the growing digital asset space. However, risk management frameworks, including regulatory ones, have been largely absent. In this paper, we address the critical question of measuring and managing risk in stablecoin protocols, which operate on public blockchain infrastructure. The on-chain environment makes it possible to monitor risk and automate its management via transparent smart-contracts in real-time. We propose two risk metrics covering capitalization and liquidity of stablecoin protocols. We then explore in a case-study type analysis how our risk management framework can be applied to DAI, the biggest decentralized stablecoin by market capitalisation to-date, governed by MakerDAO. Based on our findings, we recommend that the protocol explores implementing automatic capital buffer adjustments and dynamic maturity gap matching. Our analysis demonstrates the practical benefits for scalable (prudential) risk management stemming from real-time availability of high-quality, granular, tamper-resistant on-chain data in the digital asset space. We name this approach Crypto Asset-Liability Management (CALM).","sentences":["Stablecoins have turned out to be the \"killer\" use case of the growing digital asset space.","However, risk management frameworks, including regulatory ones, have been largely absent.","In this paper, we address the critical question of measuring and managing risk in stablecoin protocols, which operate on public blockchain infrastructure.","The on-chain environment makes it possible to monitor risk and automate its management via transparent smart-contracts in real-time.","We propose two risk metrics covering capitalization and liquidity of stablecoin protocols.","We then explore in a case-study type analysis how our risk management framework can be applied to DAI, the biggest decentralized stablecoin by market capitalisation to-date, governed by MakerDAO.","Based on our findings, we recommend that the protocol explores implementing automatic capital buffer adjustments and dynamic maturity gap matching.","Our analysis demonstrates the practical benefits for scalable (prudential) risk management stemming from real-time availability of high-quality, granular, tamper-resistant on-chain data in the digital asset space.","We name this approach Crypto Asset-Liability Management (CALM)."],"url":"http://arxiv.org/abs/2401.13399v1","category":"q-fin.RM"}
{"created":"2024-01-24 11:52:05","title":"Text Categorization Can Enhance Domain-Agnostic Stopword Extraction","abstract":"This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French. By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages. Nevertheless, linguistic variances result in lower detection rates for certain languages. Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category. Uncommon stopwords add depth to text but their classification as stopwords depends on context. Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method. This research enhances NLP for African languages and underscores the importance of text categorization in stopword extraction.","sentences":["This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French.","By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages.","Nevertheless, linguistic variances result in lower detection rates for certain languages.","Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category.","Uncommon stopwords add depth to text but their classification as stopwords depends on context.","Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method.","This research enhances NLP for African languages and underscores the importance of text categorization in stopword extraction."],"url":"http://arxiv.org/abs/2401.13398v1","category":"cs.CL"}
{"created":"2024-01-24 11:45:21","title":"Determining hulls of generalized Reed-Solomon codes from algebraic geometry codes","abstract":"In this paper, we provide conditions that hulls of generalized Reed-Solomon (GRS) codes are also GRS codes from algebraic geometry codes. If the conditions are not satisfied, we provide a method of linear algebra to find the bases of hulls of GRS codes and give formulas to compute their dimensions. Besides, we explain that the conditions are too good to be improved by some examples. Moreover, we show self-orthogonal and self-dual GRS codes.","sentences":["In this paper, we provide conditions that hulls of generalized Reed-Solomon (GRS) codes are also GRS codes from algebraic geometry codes.","If the conditions are not satisfied, we provide a method of linear algebra to find the bases of hulls of GRS codes and give formulas to compute their dimensions.","Besides, we explain that the conditions are too good to be improved by some examples.","Moreover, we show self-orthogonal and self-dual GRS codes."],"url":"http://arxiv.org/abs/2401.13394v1","category":"cs.IT"}
{"created":"2024-01-24 11:41:30","title":"Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics","abstract":"Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a paradigm shift: initially, we should focus on generating the most precise ranking for each subgroup. Following this, individuals should be chosen from these rankings to meet both fairness standards and practical considerations.","sentences":["Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment.","While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups.","Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups.","Such changes are hard to explain and raise concerns regarding the validity of the intervention.","Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied.","This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios.","We propose a paradigm shift: initially, we should focus on generating the most precise ranking for each subgroup.","Following this, individuals should be chosen from these rankings to meet both fairness standards and practical considerations."],"url":"http://arxiv.org/abs/2401.13391v1","category":"cs.LG"}
{"created":"2024-01-24 11:40:53","title":"Memoryless Strategies in Stochastic Reachability Games","abstract":"We study concurrent stochastic reachability games played on finite graphs. Two players, Max and Min, seek respectively to maximize and minimize the probability of reaching a set of target states. We prove that Max has a memoryless strategy that is optimal from all states that have an optimal strategy. Our construction provides an alternative proof of this result by Bordais, Bouyer and Le Roux, and strengthens it, as we allow Max's action sets to be countably infinite.","sentences":["We study concurrent stochastic reachability games played on finite graphs.","Two players, Max and Min, seek respectively to maximize and minimize the probability of reaching a set of target states.","We prove that Max has a memoryless strategy that is optimal from all states that have an optimal strategy.","Our construction provides an alternative proof of this result by Bordais, Bouyer and Le Roux, and strengthens it, as we allow Max's action sets to be countably infinite."],"url":"http://arxiv.org/abs/2401.13390v1","category":"cs.LO"}
{"created":"2024-01-24 11:36:44","title":"UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion","abstract":"Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents \\textbf{UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.","sentences":["Existing text-to-image diffusion models primarily generate images from text prompts.","However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes.","This paper presents \\textbf{UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation.","UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input.","We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency.","A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts.","UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities."],"url":"http://arxiv.org/abs/2401.13388v1","category":"cs.CV"}
{"created":"2024-01-24 11:35:42","title":"A Mathematical Theory of Semantic Communication","abstract":"The year 1948 witnessed the historic moment of the birth of classic information theory (CIT). Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function $H(U)$, channel capacity $C=\\max_{p(x)}I(X;Y)$ and rate-distortion function $R(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d(x,\\hat{x})\\leq D} I(X;\\hat{X})$. Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case. Especially, for band-limited Gaussian channel, we obtain a new channel capacity formula, $C_s=B\\log\\left[S^4\\left(1+\\frac{P}{N_0B}\\right)\\right]$ with the synonymous length $S$.","sentences":["The year 1948 witnessed the historic moment of the birth of classic information theory (CIT).","Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function $H(U)$, channel capacity $C=\\max_{p(x)}I(X;Y)$ and rate-distortion function $R(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d(x,\\hat{x})\\leq D} I(X;\\hat{X})$. Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed.","In this paper, we try to establish a systematic framework of semantic information theory (SIT).","We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information.","Stemming from this core concept, synonymous mapping, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$.","Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem.","We find that the limits of SIT are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory.","In addition, we discuss the semantic information measures in the continuous case.","Especially, for band-limited Gaussian channel, we obtain a new channel capacity formula, $C_s=B\\log\\left[S^4\\left(1+\\frac{P}{N_0B}\\right)\\right]$ with the synonymous length $S$."],"url":"http://arxiv.org/abs/2401.13387v1","category":"cs.IT"}
{"created":"2024-01-24 11:27:32","title":"Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain","abstract":"Face recognition technology has been deployed in various real-life applications. The most sophisticated deep learning-based face recognition systems rely on training millions of face images through complex deep neural networks to achieve high accuracy. It is quite common for clients to upload face images to the service provider in order to access the model inference. However, the face image is a type of sensitive biometric attribute tied to the identity information of each user. Directly exposing the raw face image to the service provider poses a threat to the user's privacy. Current privacy-preserving approaches to face recognition focus on either concealing visual information on model input or protecting model output face embedding. The noticeable drop in recognition accuracy is a pitfall for most methods. This paper proposes a hybrid frequency-color fusion approach to reduce the input dimensionality of face recognition in the frequency domain. Moreover, sparse color information is also introduced to alleviate significant accuracy degradation after adding differential privacy noise. Besides, an identity-specific embedding mapping scheme is applied to protect original face embedding by enlarging the distance among identities. Lastly, secure multiparty computation is implemented for safely computing the embedding distance during model inference. The proposed method performs well on multiple widely used verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy than the state-of-the-art in the 1:N verification scenario.","sentences":["Face recognition technology has been deployed in various real-life applications.","The most sophisticated deep learning-based face recognition systems rely on training millions of face images through complex deep neural networks to achieve high accuracy.","It is quite common for clients to upload face images to the service provider in order to access the model inference.","However, the face image is a type of sensitive biometric attribute tied to the identity information of each user.","Directly exposing the raw face image to the service provider poses a threat to the user's privacy.","Current privacy-preserving approaches to face recognition focus on either concealing visual information on model input or protecting model output face embedding.","The noticeable drop in recognition accuracy is a pitfall for most methods.","This paper proposes a hybrid frequency-color fusion approach to reduce the input dimensionality of face recognition in the frequency domain.","Moreover, sparse color information is also introduced to alleviate significant accuracy degradation after adding differential privacy noise.","Besides, an identity-specific embedding mapping scheme is applied to protect original face embedding by enlarging the distance among identities.","Lastly, secure multiparty computation is implemented for safely computing the embedding distance during model inference.","The proposed method performs well on multiple widely used verification datasets.","Moreover, it has around 2.6% to 4.2% higher accuracy than the state-of-the-art in the 1:N verification scenario."],"url":"http://arxiv.org/abs/2401.13386v1","category":"cs.CV"}
{"created":"2024-01-24 11:24:13","title":"Randomized learning-augmented auctions with revenue guarantees","abstract":"We consider the fundamental problem of designing a truthful single-item auction with the challenging objective of extracting a large fraction of the highest agent valuation as revenue. Following a recent trend in algorithm design, we assume that the agent valuations belong to a known interval, and a (possibly erroneous) prediction for the highest valuation is available. Then, auction design aims for high consistency and robustness, meaning that, for appropriate pairs of values $\\gamma$ and $\\rho$, the extracted revenue should be at least a $\\gamma$- or $\\rho$-fraction of the highest valuation when the prediction is correct for the input instance or not. We characterize all pairs of parameters $\\gamma$ and $\\rho$ so that a randomized $\\gamma$-consistent and $\\rho$-robust auction exists. Furthermore, for the setting in which robustness can be a function of the prediction error, we give sufficient and necessary conditions for the existence of robust auctions and present randomized auctions that extract a revenue that is only a polylogarithmic (in terms of the prediction error) factor away from the highest agent valuation.","sentences":["We consider the fundamental problem of designing a truthful single-item auction with the challenging objective of extracting a large fraction of the highest agent valuation as revenue.","Following a recent trend in algorithm design, we assume that the agent valuations belong to a known interval, and a (possibly erroneous) prediction for the highest valuation is available.","Then, auction design aims for high consistency and robustness, meaning that, for appropriate pairs of values $\\gamma$ and $\\rho$, the extracted revenue should be at least a $\\gamma$- or $\\rho$-fraction of the highest valuation when the prediction is correct for the input instance or not.","We characterize all pairs of parameters $\\gamma$ and $\\rho$ so that a randomized $\\gamma$-consistent and $\\rho$-robust auction exists.","Furthermore, for the setting in which robustness can be a function of the prediction error, we give sufficient and necessary conditions for the existence of robust auctions and present randomized auctions that extract a revenue that is only a polylogarithmic (in terms of the prediction error) factor away from the highest agent valuation."],"url":"http://arxiv.org/abs/2401.13384v1","category":"cs.GT"}
{"created":"2024-01-24 11:23:04","title":"A proof theory of right-linear (omega-)grammars via cyclic proofs","abstract":"Right-linear (or left-linear) grammars are a well-known class of context-free grammars computing just the regular languages. They may naturally be written as expressions with (least) fixed points but with products restricted to letters as left arguments, giving an alternative to the syntax of regular expressions. In this work, we investigate the resulting logical theory of this syntax. Namely, we propose a theory of right-linear algebras (RLA) over of this syntax and a cyclic proof system CRLA for reasoning about them.   We show that CRLA is sound and complete for the intended model of regular languages. From here we recover the same completeness result for RLA by extracting inductive invariants from cyclic proofs, rendering the model of regular languages the free right-linear algebra.   Finally, we extend system CRLA by greatest fixed points, nuCRLA, naturally modelled by languages of omega-words thanks to right-linearity. We show a similar soundness and completeness result of (the guarded fragment of) nuCRLA for the model of omega-regular languages, employing game theoretic techniques.","sentences":["Right-linear (or left-linear) grammars are a well-known class of context-free grammars computing just the regular languages.","They may naturally be written as expressions with (least) fixed points but with products restricted to letters as left arguments, giving an alternative to the syntax of regular expressions.","In this work, we investigate the resulting logical theory of this syntax.","Namely, we propose a theory of right-linear algebras (RLA) over of this syntax and a cyclic proof system CRLA for reasoning about them.   ","We show that CRLA is sound and complete for the intended model of regular languages.","From here we recover the same completeness result for RLA by extracting inductive invariants from cyclic proofs, rendering the model of regular languages the free right-linear algebra.   ","Finally, we extend system CRLA by greatest fixed points, nuCRLA, naturally modelled by languages of omega-words thanks to right-linearity.","We show a similar soundness and completeness result of (the guarded fragment of) nuCRLA for the model of omega-regular languages, employing game theoretic techniques."],"url":"http://arxiv.org/abs/2401.13382v1","category":"cs.LO"}
{"created":"2024-01-24 11:08:01","title":"Radiative losses and radiation-reaction effects at the first post-Newtonian order in Einstein-Cartan theory","abstract":"Gravitational radiation-reaction phenomena occurring in the dynamics of inspiralling compact binary systems are investigated at the first post-Newtonian order beyond the quadrupole approximation in the context of Einstein-Cartan theory, where quantum spin effects are modeled via the Weyssenhoff fluid. We exploit balance equations for the energy and angular momentum to determine the binary orbital decay until the two bodies collide. Our framework deals with both quasi-elliptic and quasi-circular trajectories, which are then smoothly connected. Key observables like the laws of variation of the orbital phase and frequency characterizing the quasi-circular motion are derived analytically. We conclude our analysis with an estimation of the spin contributions at the merger, which are examined both in the time domain and the Fourier frequency space through the stationary wave approximation.","sentences":["Gravitational radiation-reaction phenomena occurring in the dynamics of inspiralling compact binary systems are investigated at the first post-Newtonian order beyond the quadrupole approximation in the context of Einstein-Cartan theory, where quantum spin effects are modeled via the Weyssenhoff fluid.","We exploit balance equations for the energy and angular momentum to determine the binary orbital decay until the two bodies collide.","Our framework deals with both quasi-elliptic and quasi-circular trajectories, which are then smoothly connected.","Key observables like the laws of variation of the orbital phase and frequency characterizing the quasi-circular motion are derived analytically.","We conclude our analysis with an estimation of the spin contributions at the merger, which are examined both in the time domain and the Fourier frequency space through the stationary wave approximation."],"url":"http://arxiv.org/abs/2401.13374v1","category":"gr-qc"}
{"created":"2024-01-24 11:01:15","title":"SVARM-IQ: Efficient Approximation of Any-order Shapley Interactions through Stratification","abstract":"Addressing the limitations of individual attribution scores via the Shapley value (SV), the field of explainable AI (XAI) has recently explored intricate interactions of features or data points. In particular, \\mbox{extensions}~of~the SV, such as the Shapley Interaction Index (SII), have been proposed as a measure to still benefit from the axiomatic basis of the SV. However, similar to the SV, their exact computation remains computationally prohibitive. Hence, we propose with SVARM-IQ a sampling-based approach to efficiently approximate Shapley-based interaction indices of any order. SVARM-IQ can be applied to a broad class of interaction indices, including the SII, by leveraging a novel stratified representation. We provide non-asymptotic theoretical guarantees on its approximation quality and empirically demonstrate that SVARM-IQ achieves state-of-the-art estimation results in practical XAI scenarios on different model classes and application domains.","sentences":["Addressing the limitations of individual attribution scores via the Shapley value (SV), the field of explainable AI (XAI) has recently explored intricate interactions of features or data points.","In particular, \\mbox{extensions}~of~the SV, such as the Shapley Interaction Index (SII), have been proposed as a measure to still benefit from the axiomatic basis of the SV.","However, similar to the SV, their exact computation remains computationally prohibitive.","Hence, we propose with SVARM-IQ a sampling-based approach to efficiently approximate Shapley-based interaction indices of any order.","SVARM-IQ can be applied to a broad class of interaction indices, including the SII, by leveraging a novel stratified representation.","We provide non-asymptotic theoretical guarantees on its approximation quality and empirically demonstrate that SVARM-IQ achieves state-of-the-art estimation results in practical XAI scenarios on different model classes and application domains."],"url":"http://arxiv.org/abs/2401.13371v1","category":"cs.GT"}
{"created":"2024-01-24 10:54:31","title":"Dynamic Epistemic Logic of Resource Bounded Information Mining Agents","abstract":"Logics for resource-bounded agents have been getting more and more attention in recent years since they provide us with more realistic tools for modelling and reasoning about multi-agent systems. While many existing approaches are based on the idea of agents as imperfect reasoners, who must spend their resources to perform logical inference, this is not the only way to introduce resource constraints into logical settings. In this paper we study agents as perfect reasoners, who may purchase a new piece of information from a trustworthy source. For this purpose we propose dynamic epistemic logic for semi-public queries for resource-bounded agents. In this logic (groups of) agents can perform a query (ask a question) about whether some formula is true and receive a correct answer. These queries are called semi-public, because the very fact of the query is public, while the answer is private. We also assume that every query has a cost and every agent has a budget constraint. Finally, our framework allows us to reason about group queries, in which agents may share resources to obtain a new piece of information together. We demonstrate that our logic is complete, decidable and has an efficient model checking procedure.","sentences":["Logics for resource-bounded agents have been getting more and more attention in recent years since they provide us with more realistic tools for modelling and reasoning about multi-agent systems.","While many existing approaches are based on the idea of agents as imperfect reasoners, who must spend their resources to perform logical inference, this is not the only way to introduce resource constraints into logical settings.","In this paper we study agents as perfect reasoners, who may purchase a new piece of information from a trustworthy source.","For this purpose we propose dynamic epistemic logic for semi-public queries for resource-bounded agents.","In this logic (groups of) agents can perform a query (ask a question) about whether some formula is true and receive a correct answer.","These queries are called semi-public, because the very fact of the query is public, while the answer is private.","We also assume that every query has a cost and every agent has a budget constraint.","Finally, our framework allows us to reason about group queries, in which agents may share resources to obtain a new piece of information together.","We demonstrate that our logic is complete, decidable and has an efficient model checking procedure."],"url":"http://arxiv.org/abs/2401.13369v1","category":"cs.LO"}
