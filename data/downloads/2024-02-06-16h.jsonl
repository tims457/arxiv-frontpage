{"created":"2024-02-05 18:59:41","title":"HASSOD: Hierarchical Adaptive Self-Supervised Object Detection","abstract":"The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.","sentences":["The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects.","Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision.","HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image.","Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures.","This additional self-supervised learning task leads to improved detection performance and enhanced interpretability.","Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process.","Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection.","Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io."],"url":"http://arxiv.org/abs/2402.03311v1","category":"cs.CV"}
{"created":"2024-02-05 18:59:36","title":"V-IRL: Grounding Virtual Intelligence in Real Life","abstract":"There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.","sentences":["There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created.","To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds.","How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control?","Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment.","Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe."],"url":"http://arxiv.org/abs/2402.03310v1","category":"cs.AI"}
{"created":"2024-02-05 18:58:38","title":"Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?","abstract":"Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location. Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes.","sentences":["Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows.","These outputs indicate the ability to perform compositional generalization, but how do the models do so?","We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions.","Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance.","En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold.","Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location.","Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks.","These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes."],"url":"http://arxiv.org/abs/2402.03305v1","category":"cs.LG"}
{"created":"2024-02-05 18:58:19","title":"Nevermind: Instruction Override and Moderation in Large Language Models","abstract":"Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines. Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the LLM itself.","sentences":["Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides.","These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak.","Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault.","When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities.","Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.","Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the LLM itself."],"url":"http://arxiv.org/abs/2402.03303v1","category":"cs.CL"}
{"created":"2024-02-05 18:55:32","title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models","abstract":"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.","sentences":["Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.","In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data.","DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.","Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.","The mathematical reasoning capability of DeepSeekMath is attributed to two key factors:","First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline.","Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO."],"url":"http://arxiv.org/abs/2402.03300v1","category":"cs.CL"}
{"created":"2024-02-05 18:54:43","title":"GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models","abstract":"The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.","sentences":["The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures.","One major safety measure is to proactively test the LLMs with jailbreaks prior to the release.","Therefore, such testing will require a method that can generate jailbreaks massively and efficiently.","In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation.","We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks.","Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence.","We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve.","Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.","In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly.","We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics).","We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT).","Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities."],"url":"http://arxiv.org/abs/2402.03299v1","category":"cs.LG"}
{"created":"2024-02-05 18:54:08","title":"Gravitational index of the heterotic string","abstract":"The fundamental heterotic string has a tower of BPS states whose supersymmetric index has an exponential growth in the charges. We construct the saddle-point of the gravitational path integral corresponding to this index. The saddle-point configuration is a supersymmetric rotating non-extremal Euclidean black hole. This configuration is singular in the two-derivative theory. We show that the addition of higher-derivative terms in four-dimensional $N=2$ supergravity resolves the singularity. In doing so, we extend the recently-developed \"new attractor mechanism\" to include the effect of higher-derivative terms. Remarkably, the one-loop, four-derivative F-term contribution to the prepotential leads to a precise match of the gravitational and microscopic index. We also comment, using the effective theory near the horizon, on the possibility of a string-size near-extremal black hole. Our results clarify the meaning of different descriptions of this system in the literature. The thermal state transitions to a winding condensate and a gas of strings without ever reaching a small black hole, while the index is captured by the rotating Euclidean black hole solution and is constant and thus smoothly connected to the microscopic ensemble.","sentences":["The fundamental heterotic string has a tower of BPS states whose supersymmetric index has an exponential growth in the charges.","We construct the saddle-point of the gravitational path integral corresponding to this index.","The saddle-point configuration is a supersymmetric rotating non-extremal Euclidean black hole.","This configuration is singular in the two-derivative theory.","We show that the addition of higher-derivative terms in four-dimensional $N=2$ supergravity resolves the singularity.","In doing so, we extend the recently-developed \"new attractor mechanism\" to include the effect of higher-derivative terms.","Remarkably, the one-loop, four-derivative F-term contribution to the prepotential leads to a precise match of the gravitational and microscopic index.","We also comment, using the effective theory near the horizon, on the possibility of a string-size near-extremal black hole.","Our results clarify the meaning of different descriptions of this system in the literature.","The thermal state transitions to a winding condensate and a gas of strings without ever reaching a small black hole, while the index is captured by the rotating Euclidean black hole solution and is constant and thus smoothly connected to the microscopic ensemble."],"url":"http://arxiv.org/abs/2402.03297v1","category":"hep-th"}
{"created":"2024-02-05 18:53:05","title":"Cusped hyperbolic Lagrangians as mirrors to lines in three-space","abstract":"We construct a Lagrangian in the cotangent bundle of a 3-torus whose projection to the fiber is a neighborhood of a tropical curve with a single 4-valent vertex. This Lagrangian has an isolated conical singular point, and its smooth locus is diffeomorphic to the minimally-twisted five component chain link complement, a cusped hyperbolic 3-manifold. From this singular Lagrangian, we construct an immersed Lagrangian, and determine when it is unobstructed in the wrapped Fukaya category. We show, using quilted Floer theory, that for a generic line in projective 3-space, there is a local system on this immersed Lagrangian such that the resulting object of the wrapped Fukaya category is homologically mirror to an object of the derived category supported on the line.","sentences":["We construct a Lagrangian in the cotangent bundle of a 3-torus whose projection to the fiber is a neighborhood of a tropical curve with a single 4-valent vertex.","This Lagrangian has an isolated conical singular point, and its smooth locus is diffeomorphic to the minimally-twisted five component chain link complement, a cusped hyperbolic 3-manifold.","From this singular Lagrangian, we construct an immersed Lagrangian, and determine when it is unobstructed in the wrapped Fukaya category.","We show, using quilted Floer theory, that for a generic line in projective 3-space, there is a local system on this immersed Lagrangian such that the resulting object of the wrapped Fukaya category is homologically mirror to an object of the derived category supported on the line."],"url":"http://arxiv.org/abs/2402.03296v1","category":"math.SG"}
{"created":"2024-02-05 18:51:17","title":"Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks","abstract":"Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code is publicly available.","sentences":["Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices.","Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning.","The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix.","These requirements are infeasible even with state-of-the-art hardware.","In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix.","Our method enjoys efficient linear memory and time complexity for each iteration.","Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate.","We provide the convergence result of Ginger for non-convex objectives.","Our experiments on different tasks with different model architectures verify the effectiveness of our method.","Our code is publicly available."],"url":"http://arxiv.org/abs/2402.03295v1","category":"cs.LG"}
{"created":"2024-02-05 18:50:39","title":"Flora: Low-Rank Adapters Are Secretly Gradient Compressors","abstract":"Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.","sentences":["Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training.","To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters.","However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance.","In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection.","Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states.","We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.03293v1","category":"cs.LG"}
{"created":"2024-02-05 18:49:17","title":"InstanceDiffusion: Instance-level Control for Image Generation","abstract":"Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, and 25.4% IoU for mask inputs.","sentences":["Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image.","We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models.","InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof.","We propose three major changes to text-to-image models that enable precise instance-level control.","Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances.","InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition.","Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, and 25.4% IoU for mask inputs."],"url":"http://arxiv.org/abs/2402.03290v1","category":"cs.CV"}
{"created":"2024-02-05 18:47:04","title":"Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS","abstract":"Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.","sentences":["Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency.","This is due to the lack of PPA awareness in conventional transformer decoding algorithms.","In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code.","Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models.","For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product."],"url":"http://arxiv.org/abs/2402.03289v1","category":"cs.LG"}
{"created":"2024-02-05 18:43:05","title":"A Lennard-Jones Layer for Distribution Normalization","abstract":"We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization). LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time. This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process. We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution. Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process. The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively. Finally, we apply LJLs to improve the point distribution of a score-based 3D point cloud denoising network. In general, we demonstrate that LJLs are effective for distribution normalization which can be applied at negligible cost without retraining the given neural network.","sentences":["We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization).","LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time.","This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process.","We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution.","Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process.","The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively.","Finally, we apply LJLs to improve the point distribution of a score-based 3D point cloud denoising network.","In general, we demonstrate that LJLs are effective for distribution normalization which can be applied at negligible cost without retraining the given neural network."],"url":"http://arxiv.org/abs/2402.03287v1","category":"cs.LG"}
{"created":"2024-02-05 18:42:34","title":"Training-Free Consistent Text-to-Image Generation","abstract":"Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.","sentences":["Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language.","However, using these models to consistently portray the same subject across diverse prompts remains challenging.","Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model.","These methods require lengthy per-subject optimization or large-scale pre-training.","Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects.","Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model.","We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images.","Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency.","We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step.","Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects."],"url":"http://arxiv.org/abs/2402.03286v1","category":"cs.CV"}
{"created":"2024-02-05 18:39:47","title":"Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models","abstract":"Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing \"conversation forecasting\" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.","sentences":["Effective interlocutors account for the uncertain goals, beliefs, and emotions of others.","But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue.","How well can language models represent inherent uncertainty in conversations?","We propose FortUne Dial, an expansion of the long-standing \"conversation forecasting\" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances.","We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations.","Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size."],"url":"http://arxiv.org/abs/2402.03284v1","category":"cs.CL"}
{"created":"2024-02-05 18:39:04","title":"Towards a Flexible Scale-out Framework for Efficient Visual Data Query Processing","abstract":"There is growing interest in visual data management systems that support queries with specialized operations ranging from resizing an image to running complex machine learning models. With a plethora of such operations, the basic need to receive query responses in minimal time takes a hit, especially when the client desires to run multiple such operations in a single query. Existing systems provide an ad-hoc approach where different solutions are clubbed together to provide an end-to-end visual data management system. Unlike such solutions, the Visual Data Management System (VDMS) natively executes queries with multiple operations, thus providing an end-to-end solution. However, a fixed subset of native operations and a synchronous threading architecture limit its generality and scalability.   In this paper, we develop VDMS-Async that adds the capability to run user-defined operations with VDMS and execute operations within a query on a remote server. VDMS-Async utilizes an event-driven architecture to create an efficient pipeline for executing operations within a query. Our experiments have shown that VDMS-Async reduces the query execution time by 2-3X compared to existing state-of-the-art systems. Further, remote operations coupled with an event-driven architecture enables VDMS-Async to scale query execution time linearly with the addition of every new remote server. We demonstrate a 64X reduction in query execution time when adding 64 remote servers.","sentences":["There is growing interest in visual data management systems that support queries with specialized operations ranging from resizing an image to running complex machine learning models.","With a plethora of such operations, the basic need to receive query responses in minimal time takes a hit, especially when the client desires to run multiple such operations in a single query.","Existing systems provide an ad-hoc approach where different solutions are clubbed together to provide an end-to-end visual data management system.","Unlike such solutions, the Visual Data Management System (VDMS) natively executes queries with multiple operations, thus providing an end-to-end solution.","However, a fixed subset of native operations and a synchronous threading architecture limit its generality and scalability.   ","In this paper, we develop VDMS-Async that adds the capability to run user-defined operations with VDMS and execute operations within a query on a remote server.","VDMS-Async utilizes an event-driven architecture to create an efficient pipeline for executing operations within a query.","Our experiments have shown that VDMS-Async reduces the query execution time by 2-3X compared to existing state-of-the-art systems.","Further, remote operations coupled with an event-driven architecture enables VDMS-Async to scale query execution time linearly with the addition of every new remote server.","We demonstrate a 64X reduction in query execution time when adding 64 remote servers."],"url":"http://arxiv.org/abs/2402.03283v1","category":"cs.DB"}
{"created":"2024-02-05 18:38:55","title":"A Framework for Partially Observed Reward-States in RLHF","abstract":"The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed \"internal states.\" Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. We show that our models and guarantees in both settings generalize and extend existing ones. Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes.","sentences":["The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs.","Neuroscience research shows that human responses to stimuli are known to depend on partially-observed \"internal states.\"","Unfortunately current models of RLHF do not take take this into consideration.","Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment.","To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL).","We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL.","For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI.","For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret.","We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret.","We show that our models and guarantees in both settings generalize and extend existing ones.","Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes."],"url":"http://arxiv.org/abs/2402.03282v1","category":"cs.LG"}
{"created":"2024-02-05 18:35:40","title":"Wild orbits and generalised singularity modules: stratifications and quantisation","abstract":"We study isomorphism classes of untwisted irregular singular meromorphic connections on principal bundles over (wild) Riemann surfaces, for any complex reductive structure group $G$ and polar divisor. In particular we compute the stabilisers of suitable marked points on their principal part orbits, showing the stabilisers are connected and controlled by the corresponding filtration of (Levi factors of) nested parabolic subgroups of $G$; this uniquely determines the orbits as complex homogeneous manifolds for groups of jets of principal $G$-bundle automorphisms. Moreover, when the residue is semisimple we stratify the space of orbits by the stabilisers, relating this to local wild mapping class groups and generalising the Levi stratification of a Cartan subalgebra $\\mathfrak{t} \\subseteq \\mathfrak{g} = \\operatorname{Lie}(G)$: the dense stratum corresponds to the generic setting of irregular isomonodromic deformations \\`a la Jimbo--Miwa--Ueno.   Then we adapt a result of Alekseev--Lachowska to deformation-quantise nongeneric orbits: the $\\ast$-product involves affine-Lie-algebra modules, extending the generalised Verma modules (in the case of regular singularities) and the `singularity' modules of F.--R. (in the case of generic irregular singularities). As in the generic case, the modules contain Whittaker vectors for the Gaiotto--Teschner Virasoro pairs from irregular Liouville conformal field theory; but they now provide all the quotients which are obtained when the corresponding parameters leave the aforementioned dense strata. We also construct Shapovalov forms for the corresponding representations of truncated (holomorphic) current Lie algebras, leading to a conjectural irreducibility criterion. Finally, we use these representations to construct new flat vector bundles of vacua/covacua \\`a la Wess--Zumino--Novikov--Witten, equipped with connections \\`a la Knizhnik--Zamolodchikov.","sentences":["We study isomorphism classes of untwisted irregular singular meromorphic connections on principal bundles over (wild) Riemann surfaces, for any complex reductive structure group $G$ and polar divisor.","In particular we compute the stabilisers of suitable marked points on their principal part orbits, showing the stabilisers are connected and controlled by the corresponding filtration of (Levi factors of) nested parabolic subgroups of $G$; this uniquely determines the orbits as complex homogeneous manifolds for groups of jets of principal $G$-bundle automorphisms.","Moreover, when the residue is semisimple we stratify the space of orbits by the stabilisers, relating this to local wild mapping class groups and generalising the Levi stratification of a Cartan subalgebra $\\mathfrak{t} \\subseteq \\mathfrak{g} = \\operatorname{Lie}(G)$: the dense stratum corresponds to the generic setting of irregular isomonodromic deformations \\`a la Jimbo--Miwa--Ueno.   ","Then we adapt a result of Alekseev--Lachowska to deformation-quantise nongeneric orbits: the $\\ast$-product involves affine-Lie-algebra modules, extending the generalised Verma modules (in the case of regular singularities) and the `singularity' modules of F.--R. (in the case of generic irregular singularities).","As in the generic case, the modules contain Whittaker vectors for the Gaiotto--Teschner Virasoro pairs from irregular Liouville conformal field theory; but they now provide all the quotients which are obtained when the corresponding parameters leave the aforementioned dense strata.","We also construct Shapovalov forms for the corresponding representations of truncated (holomorphic) current Lie algebras, leading to a conjectural irreducibility criterion.","Finally, we use these representations to construct new flat vector bundles of vacua/covacua \\`a la Wess--Zumino--Novikov--Witten, equipped with connections \\`a la Knizhnik--Zamolodchikov."],"url":"http://arxiv.org/abs/2402.03278v1","category":"math.QA"}
{"created":"2024-02-05 18:35:16","title":"Event-based Product Carousel Recommendation with Query-Click Graph","abstract":"Many current recommender systems mainly focus on the product-to-product recommendations and user-to-product recommendations even during the time of events rather than modeling the typical recommendations for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event. Product recommendations for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as recommendations. However, building a recommender system with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products. To fill this gap, we define the novel problem as the event-based product carousel recommendations in e-commerce and propose an effective recommender system based on the query-click bipartite graph. We apply the iterative clustering algorithm over the query-click bipartite graph and infer the event-related aspects by the clusters of queries. The aspect-related recommendations are powered by the click-through rate of products regarding each aspect. We show through experiments that this approach effectively mines product carousels for the target event.","sentences":["Many current recommender systems mainly focus on the product-to-product recommendations and user-to-product recommendations even during the time of events rather than modeling the typical recommendations for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event.","Product recommendations for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as recommendations.","However, building a recommender system with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products.","To fill this gap, we define the novel problem as the event-based product carousel recommendations in e-commerce and propose an effective recommender system based on the query-click bipartite graph.","We apply the iterative clustering algorithm over the query-click bipartite graph and infer the event-related aspects by the clusters of queries.","The aspect-related recommendations are powered by the click-through rate of products regarding each aspect.","We show through experiments that this approach effectively mines product carousels for the target event."],"url":"http://arxiv.org/abs/2402.03277v1","category":"cs.IR"}
{"created":"2024-02-05 18:31:00","title":"Algorithms and Complexity of Difference Logic","abstract":"Difference Logic (DL) is a fragment of linear arithmetics where atoms are constraints x+k <= y for variables x,y (ranging over Q or Z) and integer k. We study the complexity of deciding the truth of existential DL sentences. This problem appears in many contexts: examples include verification, bioinformatics, telecommunications, and spatio-temporal reasoning in AI. We begin by considering sentences in CNF with rational-valued variables. We restrict the allowed clauses via two natural parameters: arity and coefficient bounds. The problem is NP-hard for most choices of these parameters. As a response to this, we refine our understanding by analyzing the time complexity and the parameterized complexity (with respect to well-studied parameters such as primal and incidence treewidth). We obtain a comprehensive picture of the complexity landscape in both cases. Finally, we generalize our results to integer domains and sentences that are not in CNF.","sentences":["Difference Logic (DL) is a fragment of linear arithmetics where atoms are constraints x+k <= y for variables x,y (ranging over Q or Z) and","integer k. We study the complexity of deciding the truth of existential DL sentences.","This problem appears in many contexts: examples include verification, bioinformatics, telecommunications, and spatio-temporal reasoning in AI.","We begin by considering sentences in CNF with rational-valued variables.","We restrict the allowed clauses via two natural parameters: arity and coefficient bounds.","The problem is NP-hard for most choices of these parameters.","As a response to this, we refine our understanding by analyzing the time complexity and the parameterized complexity (with respect to well-studied parameters such as primal and incidence treewidth).","We obtain a comprehensive picture of the complexity landscape in both cases.","Finally, we generalize our results to integer domains and sentences that are not in CNF."],"url":"http://arxiv.org/abs/2402.03273v1","category":"cs.DS"}
{"created":"2024-02-05 18:28:44","title":"Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models","abstract":"In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).","sentences":["In the face of uncertainty, the ability to seek information is of fundamental importance.","In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms).","In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions.","UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward.","In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task)."],"url":"http://arxiv.org/abs/2402.03271v1","category":"cs.CL"}
{"created":"2024-02-05 18:28:44","title":"Phonon and exciton temperature-dependent properties of twisted MoS$_2$","abstract":"In the present work, Raman and photoluminescence spectroscopies were used to study the dynamics of phonons and different excitons of MoS$_2$ bilayer under a rotation of 29$^{\\circ}$ dependent of the temperature. The twisted bilayer (T-2L) of MoS$_2$ was obtained through mechanical exfoliation, and subsequently rotated using a dimethyl polysiloxane (PDMS) substrate and deterministic transferred to a SiO$_2$ substrate. The Raman spectrum of the twisted bilayer presents three peaks E$'$ (386 cm$^{-1}$), A$'_{1}$ (405 cm$^{-1}$) and an FA$'$ peak at approximately 409 cm$^{-1}$ linked to the A$'_{1}$ mode, which is attributed to a Moir\\'e pattern phonon. Both modes (A$'_{1}$ and FA$'$) are dependent of light polarization in a way that demonstrates an effective coupling between the layers. It was also verified through the Gr\\\"uneisen parameter, an increase in the anharmonicity of the mode in the E$'$ plane and a decrease the same for A$'_{1}$. In the PL measurements, the appearance of an exciton in T-2L was verified, which generated a second shoulder measured at $\\approx$1.58 eV attributed to an indirect transition of an I trion. The interaction between the rotated monolayers of MoS$_{2}$ proved to be an important parameter for possible fine-tuning of the properties of bilayer samples.","sentences":["In the present work, Raman and photoluminescence spectroscopies were used to study the dynamics of phonons and different excitons of MoS$_2$ bilayer under a rotation of 29$^{\\circ}$ dependent of the temperature.","The twisted bilayer (T-2L) of MoS$_2$ was obtained through mechanical exfoliation, and subsequently rotated using a dimethyl polysiloxane (PDMS) substrate and deterministic transferred to a SiO$_2$ substrate.","The Raman spectrum of the twisted bilayer presents three peaks E$'$ (386 cm$^{-1}$), A$'_{1}$ (405 cm$^{-1}$) and an FA$'$ peak at approximately 409 cm$^{-1}$ linked to the A$'_{1}$ mode, which is attributed to a Moir\\'e pattern phonon.","Both modes (A$'_{1}$ and FA$'$) are dependent of light polarization in a way that demonstrates an effective coupling between the layers.","It was also verified through the Gr\\\"uneisen parameter, an increase in the anharmonicity of the mode in the E$'$ plane and a decrease the same for A$'_{1}$. In the PL measurements, the appearance of an exciton in T-2L was verified, which generated a second shoulder measured at $\\approx$1.58 eV attributed to an indirect transition of an I trion.","The interaction between the rotated monolayers of MoS$_{2}$ proved to be an important parameter for possible fine-tuning of the properties of bilayer samples."],"url":"http://arxiv.org/abs/2402.03272v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 18:25:51","title":"Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation","abstract":"Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.","sentences":["Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning.","To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time.","We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs).","More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs.","Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason.","Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance."],"url":"http://arxiv.org/abs/2402.03268v1","category":"cs.LG"}
{"created":"2024-02-05 18:25:06","title":"Sojourns of locally self-similar Gaussian processes","abstract":"Given a Gaussian risk process $R(t)=u+c(t)-X(t),t\\ge 0$, the cumulative Parisian ruin probability on a finite time interval $[0,T]$ with respect to $L \\geq 0$ is defined as the probability that the sojourn time that the risk process $R$ spends under the level 0 on this time interval $[0,T]$ exceeds $L$. In this contribution we derive exact asymptotic approximations of the cumulative Parisian ruin probability for a general class of Gaussian processes introduced in [9] assuming that $X$ is locally self-similar. We illustrate our findings with several examples. As a byproduct we show that Berman's constants can be defined alternatively by a self-similar Gaussian process which could be quite different to the fractional Brownian motion.","sentences":["Given a Gaussian risk process $R(t)=u+c(t)-X(t),t\\ge 0$, the cumulative Parisian ruin probability on a finite time interval $","[0,T]$ with respect to $L \\geq 0$ is defined as the probability that the sojourn time that the risk process $R$ spends under the level 0 on this time interval $[0,T]$ exceeds $L$.","In this contribution we derive exact asymptotic approximations of the cumulative Parisian ruin probability for a general class of Gaussian processes introduced in [9] assuming that $X$ is locally self-similar.","We illustrate our findings with several examples.","As a byproduct we show that Berman's constants can be defined alternatively by a self-similar Gaussian process which could be quite different to the fractional Brownian motion."],"url":"http://arxiv.org/abs/2402.03267v1","category":"math.PR"}
{"created":"2024-02-05 18:22:42","title":"Constructing massive particles surfaces in static spacetimes","abstract":"The procedure for constructing the massive particle surfaces in static space-times is described in detail and the equivalence of the main results with the results of the geodesic approach is demonstrated.","sentences":["The procedure for constructing the massive particle surfaces in static space-times is described in detail and the equivalence of the main results with the results of the geodesic approach is demonstrated."],"url":"http://arxiv.org/abs/2402.03266v1","category":"gr-qc"}
{"created":"2024-02-05 18:22:21","title":"MobilityGPT: Enhanced Human Mobility Modeling with a GPT model","abstract":"Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.","sentences":["Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories.","However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits.","To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT).","To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT.","We propose a gravity-based sampling method to train a transformer for semantic sequence similarity.","Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits.","Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories.","Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions."],"url":"http://arxiv.org/abs/2402.03264v1","category":"cs.LG"}
{"created":"2024-02-05 18:14:43","title":"Synthetic spectra are (usually) cellular","abstract":"If $E$ is a connective ring spectrum, then Pstragowski's category $Syn_E$ of $E$-synthetic spectra is generated by the bigraded spheres $S^{i,j}$. In particular, it is equivalent to the category of modules over a filtered ring spectrum.","sentences":["If $E$ is a connective ring spectrum, then Pstragowski's category $Syn_E$ of $E$-synthetic spectra is generated by the bigraded spheres $S^{i,j}$.","In particular, it is equivalent to the category of modules over a filtered ring spectrum."],"url":"http://arxiv.org/abs/2402.03257v1","category":"math.AT"}
{"created":"2024-02-05 18:12:28","title":"Minimum Description Length and Generalization Guarantees for Representation Learning","abstract":"A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the \"Minimum Description Length\" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the \"multi-letter\" relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior. In particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms. Our compressibility approach, which is information-theoretic in nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two essential ingredients: block-coding and lossy-compression. The latter allows our approach to subsume the so-called geometrical compressibility as a special case. To the best knowledge of the authors, the established generalization bounds are the first of their kind for Information Bottleneck (IB) type encoders and representation learning. Finally, we partly exploit the theoretical results by introducing a new data-dependent prior. Numerical simulations illustrate the advantages of well-chosen such priors over classical priors used in IB.","sentences":["A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data.","While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   ","In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the \"Minimum Description Length\" (MDL) of the labels or the latent variables (representations).","Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the \"multi-letter\" relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior.","In particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms.","Our compressibility approach, which is information-theoretic in nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two essential ingredients: block-coding and lossy-compression.","The latter allows our approach to subsume the so-called geometrical compressibility as a special case.","To the best knowledge of the authors, the established generalization bounds are the first of their kind for Information Bottleneck (IB) type encoders and representation learning.","Finally, we partly exploit the theoretical results by introducing a new data-dependent prior.","Numerical simulations illustrate the advantages of well-chosen such priors over classical priors used in IB."],"url":"http://arxiv.org/abs/2402.03254v1","category":"stat.ML"}
{"created":"2024-02-05 18:09:48","title":"Fair Active Ranking from Pairwise Preferences","abstract":"We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\\epsilon, \\delta)$-PACF-Ranking according to a fair objective function that we propose. We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle. Our proposed objective function asks to minimize the $\\ell_q$ norm of the error of the groups, where the error of a group is the $\\ell_p$ norm of the error of all the items within that group, for $p, q \\geq 1$. This generalizes the objective function of $\\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework. Adjusting parameters $p$ and $q$ allows tailoring to specific fairness preferences. We present both group-blind and group-aware algorithms and analyze their sample complexity. We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms. For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds. We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings.","sentences":["We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons.","Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\\epsilon, \\delta)$-PACF-Ranking according to a fair objective function that we propose.","We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle.","Our proposed objective function asks to minimize the $\\ell_q$ norm of the error of the groups, where the error of a group is the $\\ell_p$ norm of the error of all the items within that group, for $p, q \\geq 1$.","This generalizes the objective function of $\\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   ","By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework.","Adjusting parameters $p$ and $q$ allows tailoring to specific fairness preferences.","We present both group-blind and group-aware algorithms and analyze their sample complexity.","We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms.","For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds.","We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings."],"url":"http://arxiv.org/abs/2402.03252v1","category":"cs.LG"}
{"created":"2024-02-05 18:09:33","title":"CLIP Can Understand Depth","abstract":"Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.","sentences":["Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts.","In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment.","By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth.","With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin.","Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework.","Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way.","This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging.","We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2402.03251v1","category":"cs.CV"}
{"created":"2024-02-05 18:08:42","title":"The Fefferman-Phong uncertainty principle for representations of Lie groups and applications","abstract":"We prove a general uncertainty principle for square-integrable irreducible unitary representations of connected Lie groups. The concentration of the matrix coefficients is measured in terms of weighted $L^p$ norms, with weights in the local Muckenhoupt class $A_{\\infty,{\\rm loc}}$ associated with a subRiemannian left-invariant metric of $G$ and a relatively invariant measure. The result is reminiscent of the Fefferman-Phong uncertainty principle, and is new even for the Schr\\\"odinger representation of the reduced Heisenberg group, which corresponds to the short-time Fourier transform. As an application, we give an optimal estimate of the order of magnitude of the bottom of the spectrum and of the essential spectrum of semiclassical anti-Wick operators in $\\mathbb{R}^d$ with a nonnegative symbol $a$ in the class $A_{\\infty}$ (in particular, for polynomial symbols). Precisely, we show that the infimum $\\inf _{(x_0,\\omega_0)\\in{\\mathbb{R}^{2d}}} -\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$ represents (up to multiplicative constants) both a lower bound and an upper bound for the bottom of the spectrum, uniformly with respect to $\\hbar>0$. Similarly the quantity $$\\liminf_{(x_0,\\omega_0)\\to\\infty} -\\!\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$$ represents both a lower bound and an upper bound for the bottom of the essential spectrum, uniformly with respect to $\\hbar>0$.","sentences":["We prove a general uncertainty principle for square-integrable irreducible unitary representations of connected Lie groups.","The concentration of the matrix coefficients is measured in terms of weighted $L^p$ norms, with weights in the local Muckenhoupt class $A_{\\infty,{\\rm loc}}$ associated with a subRiemannian left-invariant metric of $G$ and a relatively invariant measure.","The result is reminiscent of the Fefferman-Phong uncertainty principle, and is new even for the Schr\\\"odinger representation of the reduced Heisenberg group, which corresponds to the short-time Fourier transform.","As an application, we give an optimal estimate of the order of magnitude of the bottom of the spectrum and of the essential spectrum of semiclassical anti-Wick operators in $\\mathbb{R}^d$ with a nonnegative symbol $a$ in the class $A_{\\infty}$ (in particular, for polynomial symbols).","Precisely, we show that the infimum $\\inf _{(x_0,\\omega_0)\\in{\\mathbb{R}^{2d}}} -\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$ represents (up to multiplicative constants) both a lower bound and an upper bound for the bottom of the spectrum, uniformly with respect to $\\hbar>0$. Similarly the quantity $$\\liminf_{(x_0,\\omega_0)\\to\\infty} -\\!\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$$ represents both a lower bound and an upper bound for the bottom of the essential spectrum, uniformly with respect to $\\hbar>0$."],"url":"http://arxiv.org/abs/2402.03250v1","category":"math.CA"}
{"created":"2024-02-05 18:08:22","title":"Nonsense associations in Markov random fields with pairwise dependence","abstract":"Yule (1926) identified the issue of \"nonsense correlations\" in time series data, where dependence within each of two random vectors causes overdispersion -- i.e. variance inflation -- for measures of dependence between the two. During the near century since then, much has been written about nonsense correlations -- but nearly all of it confined to the time series literature. In this paper we provide the first, to our knowledge, rigorous study of this phenomenon for more general forms of (positive) dependence, specifically for Markov random fields on lattices and graphs. We consider both binary and continuous random vectors and three different measures of association: correlation, covariance, and the ordinary least squares coefficient that results from projecting one random vector onto the other. In some settings we find variance inflation consistent with Yule's nonsense correlation. However, surprisingly, we also find variance deflation in some settings, and in others the variance is unchanged under dependence. Perhaps most notably, we find general conditions under which OLS inference that ignores dependence is valid despite positive dependence in the regression errors, contradicting the presentation of OLS in countless textbooks and courses.","sentences":["Yule (1926) identified the issue of \"nonsense correlations\" in time series data, where dependence within each of two random vectors causes overdispersion -- i.e. variance inflation -- for measures of dependence between the two.","During the near century since then, much has been written about nonsense correlations -- but nearly all of it confined to the time series literature.","In this paper we provide the first, to our knowledge, rigorous study of this phenomenon for more general forms of (positive) dependence, specifically for Markov random fields on lattices and graphs.","We consider both binary and continuous random vectors and three different measures of association: correlation, covariance, and the ordinary least squares coefficient that results from projecting one random vector onto the other.","In some settings we find variance inflation consistent with Yule's nonsense correlation.","However, surprisingly, we also find variance deflation in some settings, and in others the variance is unchanged under dependence.","Perhaps most notably, we find general conditions under which OLS inference that ignores dependence is valid despite positive dependence in the regression errors, contradicting the presentation of OLS in countless textbooks and courses."],"url":"http://arxiv.org/abs/2402.03249v1","category":"math.ST"}
{"created":"2024-02-05 18:05:34","title":"HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference","abstract":"Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs significantly reduces the crosstalk effects, thereby increasing the wavelength parallelism in HEANA. Moreover, HEANA employs our invented balanced photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal accumulations to eliminate the need to use reduction networks in HEANA, relieving it from related latency and energy overheads. Our evaluation for the inference of four modern CNNs indicates that HEANA provides improvements of atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency), respectively, for equal-area comparisons, on gmean over two MRR-based analog CNN accelerators from prior work.","sentences":["Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts.","However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations.","These shortcomings collectively hamper the performance and energy efficiency of prior accelerators.","To tackle these shortcomings, we present a novel","Hybrid timE","Amplitude aNalog optical Accelerator, called HEANA.","HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows.","A spectrally hitless arrangement of TAOMs significantly reduces the crosstalk effects, thereby increasing the wavelength parallelism in HEANA.","Moreover, HEANA employs our invented balanced photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal accumulations to eliminate the need to use reduction networks in HEANA, relieving it from related latency and energy overheads.","Our evaluation for the inference of four modern CNNs indicates that HEANA provides improvements of atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency), respectively, for equal-area comparisons, on gmean over two MRR-based analog CNN accelerators from prior work."],"url":"http://arxiv.org/abs/2402.03247v1","category":"cs.AR"}
{"created":"2024-02-05 18:03:53","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","abstract":"Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.","sentences":["Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation.","Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations.","Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions.","Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality.","Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability."],"url":"http://arxiv.org/abs/2402.03246v1","category":"cs.CV"}
{"created":"2024-02-05 18:00:15","title":"On the Popov-Belevitch-Hautus tests for functional observability and output controllability","abstract":"Functional observability and output controllability are properties that establish the conditions respectively for the partial estimation and partial control of the system state. In the special case of full-state observability and controllability, the Popov-Belevitch-Hautus (PBH) tests provide conditions for the properties to hold based on the system eigenspace. Generalizations of the Popov-Belevitch-Hautus (PBH) test have been recently proposed for functional observability and output controllability but were proved to be valid only for diagonalizable systems thus far. Here, we rigorously establish a more general class of systems based on their Jordan decomposition under which a generalized PBH test for functional observability is valid. Likewise, we determine the class of systems under which the generalized PBH test is sufficient and necessary for output controllability. These results have immediate implications for observer and controller design, pole assignment, and optimal placement of sensors and drivers.","sentences":["Functional observability and output controllability are properties that establish the conditions respectively for the partial estimation and partial control of the system state.","In the special case of full-state observability and controllability, the Popov-Belevitch-Hautus (PBH) tests provide conditions for the properties to hold based on the system eigenspace.","Generalizations of the Popov-Belevitch-Hautus (PBH) test have been recently proposed for functional observability and output controllability but were proved to be valid only for diagonalizable systems thus far.","Here, we rigorously establish a more general class of systems based on their Jordan decomposition under which a generalized PBH test for functional observability is valid.","Likewise, we determine the class of systems under which the generalized PBH test is sufficient and necessary for output controllability.","These results have immediate implications for observer and controller design, pole assignment, and optimal placement of sensors and drivers."],"url":"http://arxiv.org/abs/2402.03245v1","category":"math.OC"}
{"created":"2024-02-05 17:59:00","title":"Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills","abstract":"Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.","sentences":["Large language models (LLMs) have recently been used for sequential decision making in interactive environments.","However, leveraging environment reward signals for continual LLM actor improvement is not straightforward.","We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills.","SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill.","These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards.","Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards.","We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement.","SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%."],"url":"http://arxiv.org/abs/2402.03244v1","category":"cs.LG"}
{"created":"2024-02-05 17:57:26","title":"JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching","abstract":"Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.","sentences":["Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations.","However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences.","In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching.","Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks.","We introduce several offline metrics that show that our dataset resembles real-world data.","Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies.","We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability."],"url":"http://arxiv.org/abs/2402.03242v1","category":"cs.CL"}
{"created":"2024-02-05 17:56:41","title":"FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition","abstract":"In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.","sentences":["In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition.","The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs.","However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining.","Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   ","To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task.","Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos.","Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   ","We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings.","FROSTER consistently achieves state-of-the-art performance on all datasets across the board.","Project page: https://visual-ai.github.io/froster."],"url":"http://arxiv.org/abs/2402.03241v1","category":"cs.CV"}
{"created":"2024-02-05 17:46:13","title":"Spin-s Dicke states and their preparation","abstract":"We introduce the notion of $su(2)$ spin-$s$ Dicke states, which are higher-spin generalizations of usual (spin-1/2) Dicke states. These multi-qudit states can be expressed as superpositions of $su(2s+1)$ qudit Dicke states. They satisfy a recursion formula, which we use to formulate an efficient quantum circuit for their preparation, whose size scales as $s k n$ for $k\\ll 2 s n$, where $n$ is the number of qudits and $k$ is the number of times the total spin-lowering operator is applied to the highest-weight state. The algorithm is deterministic and does not require ancillary qudits.","sentences":["We introduce the notion of $su(2)$ spin-$s$ Dicke states, which are higher-spin generalizations of usual (spin-1/2) Dicke states.","These multi-qudit states can be expressed as superpositions of $su(2s+1)$ qudit Dicke states.","They satisfy a recursion formula, which we use to formulate an efficient quantum circuit for their preparation, whose size scales as $s k n$ for $k\\ll 2 s n$, where $n$ is the number of qudits and $k$ is the number of times the total spin-lowering operator is applied to the highest-weight state.","The algorithm is deterministic and does not require ancillary qudits."],"url":"http://arxiv.org/abs/2402.03233v1","category":"quant-ph"}
{"created":"2024-02-05 17:38:49","title":"IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images","abstract":"In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks. Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD. Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities.","sentences":["In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses.","Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites.","In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization.","IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy.","During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization.","Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites.","The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks.","Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD.","Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities."],"url":"http://arxiv.org/abs/2402.03227v1","category":"cs.CV"}
{"created":"2024-02-05 17:37:07","title":"Turbulence modelling in neutron star merger simulations","abstract":"Observations of neutron star mergers have the potential to unveil detailed physics of matter and gravity in regimes inaccessible by other experiments. Quantitative comparisons to theory and parameter estimation require nonlinear numerical simulations. However, the detailed physics of energy and momentum transfer between different scales, and the formation and interaction of small scale structures, which can be probed by detectors, are not captured by current simulations. This is where turbulence enters neutron star modelling. This review will outline the theory and current status of turbulence modelling for relativistic neutron star merger simulations.","sentences":["Observations of neutron star mergers have the potential to unveil detailed physics of matter and gravity in regimes inaccessible by other experiments.","Quantitative comparisons to theory and parameter estimation require nonlinear numerical simulations.","However, the detailed physics of energy and momentum transfer between different scales, and the formation and interaction of small scale structures, which can be probed by detectors, are not captured by current simulations.","This is where turbulence enters neutron star modelling.","This review will outline the theory and current status of turbulence modelling for relativistic neutron star merger simulations."],"url":"http://arxiv.org/abs/2402.03224v1","category":"astro-ph.HE"}
{"created":"2024-02-05 17:34:58","title":"The nonflow issue in connecting anisotropy measurements to hydrodynamics in relativistic heavy-ion collisions","abstract":"Hydrodynamics can describe majority of the measured azimuthal anisotropies in relativistic heavy-ion collisions. Many of the anisotropy measurements are contaminated by nonflow correlations (i.e., those unrelated to global event-wise correlations). Those nonflow contamination can cause incorrectness or compromise the accuracy of the physics extracted from data-hydrodynamics comparison, particularly when one relies on subtle difference in the measurements. In the recent preprint by STAR (arXiv:2401.06625) extracting the Uranium nucleus deformation parameter, nonflow contamination is assessed by subevents in the limited STAR acceptance. In this note, we demonstrate that such assessment is inadequate and illustrate how large an effect nonflow can cause by using the HIJING model, in which all correlations are nonflow and non-hydrodynamic. We thereby conclude that the extracted Uranium deformation parameter is premature and emphasize the importance of an earnest assessment of or correction for nonflow contamination, not only for this STAR analysis but more generally for studies relying on comparing anisotropy measurements to hydrodynamic calculations.","sentences":["Hydrodynamics can describe majority of the measured azimuthal anisotropies in relativistic heavy-ion collisions.","Many of the anisotropy measurements are contaminated by nonflow correlations (i.e., those unrelated to global event-wise correlations).","Those nonflow contamination can cause incorrectness or compromise the accuracy of the physics extracted from data-hydrodynamics comparison, particularly when one relies on subtle difference in the measurements.","In the recent preprint by STAR (arXiv:2401.06625) extracting the Uranium nucleus deformation parameter, nonflow contamination is assessed by subevents in the limited STAR acceptance.","In this note, we demonstrate that such assessment is inadequate and illustrate how large an effect nonflow can cause by using the HIJING model, in which all correlations are nonflow and non-hydrodynamic.","We thereby conclude that the extracted Uranium deformation parameter is premature and emphasize the importance of an earnest assessment of or correction for nonflow contamination, not only for this STAR analysis but more generally for studies relying on comparing anisotropy measurements to hydrodynamic calculations."],"url":"http://arxiv.org/abs/2402.03222v1","category":"nucl-ex"}
{"created":"2024-02-05 17:30:24","title":"Experiment-driven atomistic materials modeling: A case study combining XPS and ML potentials to infer the structure of oxygen-rich amorphous carbon","abstract":"An important yet challenging aspect of atomistic materials modeling is reconciling experimental and computational results. Conventional approaches involve generating numerous configurations through molecular dynamics or Monte Carlo structure optimization and selecting the one with the closest match to experiment. However, this inefficient process is not guaranteed to succeed. We introduce a general method to combine atomistic machine learning (ML) with experimental observables that produces atomistic structures compatible with experiment by design. We use this approach in combination with grand-canonical Monte Carlo within a modified Hamiltonian formalism, to generate configurations that agree with experimental data and are chemically sound (low in energy). We apply our approach to understand the atomistic structure of oxygenated amorphous carbon (a-CO$_{x}$), an intriguing carbon-based material, to answer the question of how much oxygen can be added to carbon before it fully decomposes into CO and CO$_2$. Utilizing an ML-based X-ray photoelectron spectroscopy (XPS) model trained from $GW$ and density functional theory (DFT) data, in conjunction with an ML interatomic potential, we identify a-CO$_{x}$ structures compliant with experimental XPS predictions that are also energetically favorable with respect to DFT. Employing a network analysis, we accurately deconvolve the XPS spectrum into motif contributions, both revealing the inaccuracies inherent to experimental XPS interpretation and granting us atomistic insight into the structure of a-CO$_{x}$. This method generalizes to multiple experimental observables and allows for the elucidation of the atomistic structure of materials directly from experimental data, thereby enabling experiment-driven materials modeling with a degree of realism previously out of reach.","sentences":["An important yet challenging aspect of atomistic materials modeling is reconciling experimental and computational results.","Conventional approaches involve generating numerous configurations through molecular dynamics or Monte Carlo structure optimization and selecting the one with the closest match to experiment.","However, this inefficient process is not guaranteed to succeed.","We introduce a general method to combine atomistic machine learning (ML) with experimental observables that produces atomistic structures compatible with experiment by design.","We use this approach in combination with grand-canonical Monte Carlo within a modified Hamiltonian formalism, to generate configurations that agree with experimental data and are chemically sound (low in energy).","We apply our approach to understand the atomistic structure of oxygenated amorphous carbon (a-CO$_{x}$), an intriguing carbon-based material, to answer the question of how much oxygen can be added to carbon before it fully decomposes into CO and CO$_2$. Utilizing an ML-based X-ray photoelectron spectroscopy (XPS) model trained from $GW$ and density functional theory (DFT) data, in conjunction with an ML interatomic potential, we identify a-CO$_{x}$ structures compliant with experimental XPS predictions that are also energetically favorable with respect to DFT.","Employing a network analysis, we accurately deconvolve the XPS spectrum into motif contributions, both revealing the inaccuracies inherent to experimental XPS interpretation and granting us atomistic insight into the structure of a-CO$_{x}$.","This method generalizes to multiple experimental observables and allows for the elucidation of the atomistic structure of materials directly from experimental data, thereby enabling experiment-driven materials modeling with a degree of realism previously out of reach."],"url":"http://arxiv.org/abs/2402.03219v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 17:26:49","title":"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation","abstract":"In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.","sentences":["In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity.","It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks.","It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications.","It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.","The effective training of M3-Embedding involves the following technical contributions.","We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality.","We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings.","To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility.","The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding."],"url":"http://arxiv.org/abs/2402.03216v1","category":"cs.CL"}
{"created":"2024-02-05 17:25:04","title":"Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?","abstract":"The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.","sentences":["The advent of generative AI images has completely disrupted the art world.","Identifying AI generated images from human art is a challenging problem whose impact is growing over time.","The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery.","This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse.","There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques.","In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings.","We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI).","Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives).","We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness."],"url":"http://arxiv.org/abs/2402.03214v1","category":"cs.CV"}
{"created":"2024-02-05 17:16:43","title":"Inverse regression for spatially distributed functional data","abstract":"Spatially distributed functional data are prevalent in many statistical applications such as meteorology, energy forecasting, census data, disease mapping, and neurological studies. Given their complex and high-dimensional nature, functional data often require dimension reduction methods to extract meaningful information. Inverse regression is one such approach that has become very popular in the past two decades. We study the inverse regression in the framework of functional data observed at irregularly positioned spatial sites. The functional predictor is the sum of a spatially dependent functional effect and a spatially independent functional nugget effect, while the relation between the scalar response and the functional predictor is modeled using the inverse regression framework. For estimation, we consider local linear smoothing with a general weighting scheme, which includes as special cases the schemes under which equal weights are assigned to each observation or to each subject. This framework enables us to present the asymptotic results for different types of sampling plans over time such as non-dense, dense, and ultra-dense. We discuss the domain-expanding infill (DEI) framework for spatial asymptotics, which is a mix of the traditional expanding domain and infill frameworks. The DEI framework overcomes the limitations of traditional spatial asymptotics in the existing literature. Under this unified framework, we develop asymptotic theory and identify conditions that are necessary for the estimated eigen-directions to achieve optimal rates of convergence. Our asymptotic results include pointwise and $L_2$ convergence rates. Simulation studies using synthetic data and an application to a real-world dataset confirm the effectiveness of our methods.","sentences":["Spatially distributed functional data are prevalent in many statistical applications such as meteorology, energy forecasting, census data, disease mapping, and neurological studies.","Given their complex and high-dimensional nature, functional data often require dimension reduction methods to extract meaningful information.","Inverse regression is one such approach that has become very popular in the past two decades.","We study the inverse regression in the framework of functional data observed at irregularly positioned spatial sites.","The functional predictor is the sum of a spatially dependent functional effect and a spatially independent functional nugget effect, while the relation between the scalar response and the functional predictor is modeled using the inverse regression framework.","For estimation, we consider local linear smoothing with a general weighting scheme, which includes as special cases the schemes under which equal weights are assigned to each observation or to each subject.","This framework enables us to present the asymptotic results for different types of sampling plans over time such as non-dense, dense, and ultra-dense.","We discuss the domain-expanding infill (DEI) framework for spatial asymptotics, which is a mix of the traditional expanding domain and infill frameworks.","The DEI framework overcomes the limitations of traditional spatial asymptotics in the existing literature.","Under this unified framework, we develop asymptotic theory and identify conditions that are necessary for the estimated eigen-directions to achieve optimal rates of convergence.","Our asymptotic results include pointwise and $L_2$ convergence rates.","Simulation studies using synthetic data and an application to a real-world dataset confirm the effectiveness of our methods."],"url":"http://arxiv.org/abs/2402.03206v1","category":"math.ST"}
{"created":"2024-02-05 17:15:00","title":"Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems","abstract":"We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor policy reduces power consumption by approximately 8.7% during low-traffic hours and improves energy efficiency by approximately 19% during high-traffic hours, respectively.","sentences":["We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs.","The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference.","A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy.","To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed.","Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies.","Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor policy reduces power consumption by approximately 8.7% during low-traffic hours and improves energy efficiency by approximately 19% during high-traffic hours, respectively."],"url":"http://arxiv.org/abs/2402.03204v1","category":"cs.IT"}
{"created":"2024-02-05 17:13:12","title":"Leveraging IRS Induced Time Delay for Enhanced Physical Layer Security in VLC Systems","abstract":"Indoor visible light communication (VLC) is considered secure against attackers outside the confined area where the light propagates, but it is still susceptible to interception from inside the coverage area. A new technology, intelligent reflecting surfaces (IRS), has been recently introduced, offering a way to enhance physical layer security (PLS). Most research on IRS-assisted VLC assumes the same time of arrival from all reflecting elements and overlooks the effect of time delay and the associated intersymbol interference. This paper tackles, for the first time, the effect of time delay on the secrecy rate in VLC systems. Our results show that, at a fixed light-emitting diode (LED) power of 3W, the secrecy rate can be enhanced by up to 253\\% at random positions for the legitimate user when the eavesdropper is located within a 1-meter radius of the LED. Our results also show that careful allocation of the IRS elements can lead to enhanced PLS even when the eavesdropper has a more favourable position and, thus, a better channel gain than the legitimate user.","sentences":["Indoor visible light communication (VLC) is considered secure against attackers outside the confined area where the light propagates, but it is still susceptible to interception from inside the coverage area.","A new technology, intelligent reflecting surfaces (IRS), has been recently introduced, offering a way to enhance physical layer security (PLS).","Most research on IRS-assisted VLC assumes the same time of arrival from all reflecting elements and overlooks the effect of time delay and the associated intersymbol interference.","This paper tackles, for the first time, the effect of time delay on the secrecy rate in VLC systems.","Our results show that, at a fixed light-emitting diode (LED) power of 3W, the secrecy rate can be enhanced by up to 253\\% at random positions for the legitimate user when the eavesdropper is located within a 1-meter radius of the LED.","Our results also show that careful allocation of the IRS elements can lead to enhanced PLS even when the eavesdropper has a more favourable position and, thus, a better channel gain than the legitimate user."],"url":"http://arxiv.org/abs/2402.03202v1","category":"cs.IT"}
{"created":"2024-02-05 17:12:21","title":"Guidance with Spherical Gaussian Constraint for Conditional Diffusion","abstract":"Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint. Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods. Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements. Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency.","sentences":["Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training.","While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes.","This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed.","We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance.","To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions.","DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps.","Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint.","Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods.","Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements.","Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency."],"url":"http://arxiv.org/abs/2402.03201v1","category":"cs.LG"}
{"created":"2024-02-05 17:03:03","title":"Blow-up Whitney forms, shadow forms, and Poisson processes","abstract":"The Whitney forms on a simplex $T$ admit high-order generalizations that have received a great deal of attention in numerical analysis. Less well-known are the shadow forms of Brasselet, Goresky, and MacPherson. These forms generalize the Whitney forms, but have rational coefficients, allowing singularities near the faces of $T$. Motivated by numerical problems that exhibit these kinds of singularities, we introduce degrees of freedom for the shadow $k$-forms that are well-suited for finite element implementations. In particular, we show that the degrees of freedom for the shadow forms are given by integration over the $k$-dimensional faces of the blow-up $\\tilde T$ of the simplex $T$. Consequently, we obtain an isomorphism between the cohomology of the complex of shadow forms and the cellular cohomology of $\\tilde T$, which vanishes except in degree zero. Additionally, we discover a surprising probabilistic interpretation of shadow forms in terms of Poisson processes. This perspective simplifies several proofs and gives a way of computing bases for the shadow forms using a straightforward combinatorial calculation.","sentences":["The Whitney forms on a simplex $T$ admit high-order generalizations that have received a great deal of attention in numerical analysis.","Less well-known are the shadow forms of Brasselet, Goresky, and MacPherson.","These forms generalize the Whitney forms, but have rational coefficients, allowing singularities near the faces of $T$. Motivated by numerical problems that exhibit these kinds of singularities, we introduce degrees of freedom for the shadow $k$-forms that are well-suited for finite element implementations.","In particular, we show that the degrees of freedom for the shadow forms are given by integration over the $k$-dimensional faces of the blow-up $\\tilde T$ of the simplex $T$. Consequently, we obtain an isomorphism between the cohomology of the complex of shadow forms and the cellular cohomology of $\\tilde T$, which vanishes except in degree zero.","Additionally, we discover a surprising probabilistic interpretation of shadow forms in terms of Poisson processes.","This perspective simplifies several proofs and gives a way of computing bases for the shadow forms using a straightforward combinatorial calculation."],"url":"http://arxiv.org/abs/2402.03198v1","category":"math.NA"}
{"created":"2024-02-05 17:00:48","title":"Accommodating the 130 GeV Charged Higgs Boson in the General Two-Higgs Doublet Model","abstract":"Charged Higgs bosons are common predictions in most extensions of the Standard Model (SM) Higgs sector. Therefore, their observation would elucidate the nature of the Higgs sector. Motivated by the ATLAS collaboration's latest analysis performed with $139~\\text{fb}^{-1}$ of Run 2 data intended to search for charged Higgs boson, produced in top quark decay and subsequently decaying via $H^\\pm \\rightarrow cb$, where an excess with a local significance of $3\\sigma$ is observed at $m_{H^\\pm} = 130~\\rm{GeV}$, we discuss here the possibility of explaining such excess in the context of the general 2-Higgs Doublet Model (2HDM type-III), after satisfying all theoretical and up-to-date experimental constraints. We also propose phenomenological scenarios to further explore the mass region around 130 GeV in the four Yukawa types of the 2HDM type-III and suggest alternative decay channel $H^\\pm \\rightarrow cs$ and/or $H^\\pm \\rightarrow W^{*\\pm}h$ to probe the nature of the observed excess (if it is not a statistical fluctuation). Future searches for $H^\\pm$ will be critical in confirming or refuting the first hint of a light charged Higgs boson at the","sentences":["Charged Higgs bosons are common predictions in most extensions of the Standard Model (SM) Higgs sector.","Therefore, their observation would elucidate the nature of the Higgs sector.","Motivated by the ATLAS collaboration's latest analysis performed with $139~\\text{fb}^{-1}$ of Run 2 data intended to search for charged Higgs boson, produced in top quark decay and subsequently decaying via $H^\\pm \\rightarrow cb$, where an excess with a local significance of $3\\sigma$ is observed at $m_{H^\\pm} = 130~\\rm{GeV}$, we discuss here the possibility of explaining such excess in the context of the general 2-Higgs Doublet Model (2HDM type-III), after satisfying all theoretical and up-to-date experimental constraints.","We also propose phenomenological scenarios to further explore the mass region around 130 GeV in the four Yukawa types of the 2HDM type-III and suggest alternative decay channel $H^\\pm \\rightarrow cs$ and/or $H^\\pm \\rightarrow W^{*\\pm}h$ to probe the nature of the observed excess (if it is not a statistical fluctuation).","Future searches for $H^\\pm$ will be critical in confirming or refuting the first hint of a light charged Higgs boson at the"],"url":"http://arxiv.org/abs/2402.03195v1","category":"hep-ph"}
{"created":"2024-02-05 17:00:32","title":"Spinning $Q$-ball Superradiance in 3+1D","abstract":"Recently, it has been found that a Q-ball can amplify waves incident upon it, due to rotation in the internal space and the interaction of the two modes in the complex scalar field. While the spherically symmetric 3D case has been investigated previously, here we explore the 3D axi-symmetric case, which is numerically much more challenging. The difficulty comes because a partial wave expansion is needed, and the different partial waves can not be separated, for either the background spinning Q-ball solution or the perturbative scattering on top of it. A relaxation method and a high dimensional shooting method are applied to compute the Q-ball solutions and the amplification factors respectively. We also classify the behavior of the amplification factors and we discuss their bounds and the superradiance criteria.","sentences":["Recently, it has been found that a Q-ball can amplify waves incident upon it, due to rotation in the internal space and the interaction of the two modes in the complex scalar field.","While the spherically symmetric 3D case has been investigated previously, here we explore the 3D axi-symmetric case, which is numerically much more challenging.","The difficulty comes because a partial wave expansion is needed, and the different partial waves can not be separated, for either the background spinning Q-ball solution or the perturbative scattering on top of it.","A relaxation method and a high dimensional shooting method are applied to compute the Q-ball solutions and the amplification factors respectively.","We also classify the behavior of the amplification factors and we discuss their bounds and the superradiance criteria."],"url":"http://arxiv.org/abs/2402.03193v1","category":"hep-th"}
{"created":"2024-02-05 16:56:11","title":"Unified Hallucination Detection for Multimodal Large Language Models","abstract":"Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.","sentences":["Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination.","The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment.","Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity.","In response to these challenges, our work expands the investigative horizons of hallucination detection.","We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods.","Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly.","We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis.","We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations."],"url":"http://arxiv.org/abs/2402.03190v1","category":"cs.CL"}
{"created":"2024-02-05 16:53:56","title":"Nagata Dimension and Lipschitz Extensions Into Quasi-Banach Spaces","abstract":"Given two metric spaces $\\mathcal N \\subseteq \\mathcal M$ in inclusion and $0<p\\leq 1$, we wish to determine the smallest constant $\\mathfrak{t}_p (\\mathcal N, \\mathcal M)$ such that any Lipschitz map $f: \\mathcal N \\to Z$ into any $p$-Banach space $Z$ can be extended to a Lipschitz map $f' : \\mathcal M \\to Z$ satisfying $\\operatorname{Lip} f' \\leq \\mathfrak{t}_p (\\mathcal N, \\mathcal M)\\cdot \\operatorname{Lip} f$. In this article, we prove that if $\\mathcal N$ has finite Nagata dimension at most $d$ with constant $\\gamma$, then $\\mathfrak{t}_p (\\mathcal N, \\mathcal M) \\lesssim_p \\gamma \\cdot (d+1)^{1/p -1} \\cdot \\log (d+2)$ for all $0<p\\leq 1$. We show that examples of spaces with finite Nagata dimension include doubling spaces, as well as minor-excluded metric graphs. We also establish that the constant $\\mathfrak{t}_p (\\mathcal N, \\mathcal M)$ generally increases as $p$ approaches zero.","sentences":["Given two metric spaces $\\mathcal N \\subseteq \\mathcal M$ in inclusion and $0<p\\leq 1$, we wish to determine the smallest constant $\\mathfrak{t}_p (\\mathcal N, \\mathcal M)$ such that any Lipschitz map $f: \\mathcal N \\to Z$ into any $p$-Banach space $Z$ can be extended to a Lipschitz map $f' :","\\mathcal M \\to Z$ satisfying $\\operatorname{Lip} f' \\leq \\mathfrak{t}_p (\\mathcal N, \\mathcal M)\\cdot \\operatorname{Lip} f$.","In this article, we prove that if $\\mathcal N$ has finite Nagata dimension at most $d$ with constant $\\gamma$, then $\\mathfrak{t}_p (\\mathcal N, \\mathcal M) \\lesssim_p \\gamma \\cdot (d+1)^{1/p -1} \\cdot \\log (d+2)$ for all $0<p\\leq 1$. We show that examples of spaces with finite Nagata dimension include doubling spaces, as well as minor-excluded metric graphs.","We also establish that the constant $\\mathfrak{t}_p (\\mathcal N, \\mathcal M)$ generally increases as $p$ approaches zero."],"url":"http://arxiv.org/abs/2402.03189v1","category":"math.FA"}
{"created":"2024-02-05 16:53:54","title":"Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms","abstract":"Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.","sentences":["Advances in face swapping have enabled the automatic generation of highly realistic faces.","Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes.","Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process.","We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect.","We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes.","We confirm that viewed face swaps do elicit uncanny responses from viewers.","Our proposed improvements significant reduce viewing angle errors between face swaps and their source material.","Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks.","Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications.","Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach."],"url":"http://arxiv.org/abs/2402.03188v1","category":"cs.CV"}
{"created":"2024-02-05 16:48:46","title":"A theorem on local scale invariance","abstract":"We give a general proof that the only static and spherically symmetric, vacuum solutions of the local scale invariant conformally coupled scalar theory of gravity, are conformal to the Schwarzschild-de Sitter solution. In this context the physical relevance of local scale symmetry is discussed.","sentences":["We give a general proof that the only static and spherically symmetric, vacuum solutions of the local scale invariant conformally coupled scalar theory of gravity, are conformal to the Schwarzschild-de Sitter solution.","In this context the physical relevance of local scale symmetry is discussed."],"url":"http://arxiv.org/abs/2402.03184v1","category":"gr-qc"}
{"created":"2024-02-05 16:47:13","title":"Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning","abstract":"Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating the contributions among meta environments in the meta-model built, which fits better with the characteristic of configuration data that is known to dramatically differ between different environments. Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup. All code and data can be found at our repository: https://github.com/ideas-labo/SeMPL.","sentences":["Learning and predicting the performance of given software configurations are of high importance to many software engineering activities.","While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments.","In this paper, we target configuration performance learning under multiple environments.","We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment.","What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time.","The order of training naturally allows discriminating the contributions among meta environments in the meta-model built, which fits better with the characteristic of configuration data that is known to dramatically differ between different environments.","Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup.","All code and data can be found at our repository: https://github.com/ideas-labo/SeMPL."],"url":"http://arxiv.org/abs/2402.03183v1","category":"cs.SE"}
{"created":"2024-02-05 16:46:35","title":"Empowering Time Series Analysis with Large Language Models: A Survey","abstract":"Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.","sentences":["Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks.","However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.","Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications.","In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis.","Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs.","Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group.","We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains.","Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs."],"url":"http://arxiv.org/abs/2402.03182v1","category":"cs.LG"}
{"created":"2024-02-05 16:46:16","title":"C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models","abstract":"Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.","sentences":["Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments.","Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored.","In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks.","We propose C-RAG, the first framework to certify generation risks for RAG models.","Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk.","We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts.","We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial.","Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models."],"url":"http://arxiv.org/abs/2402.03181v1","category":"cs.AI"}
{"created":"2024-02-05 16:45:02","title":"Bounds of restriction of characters to submanifolds of maximal tori","abstract":"Matrix coefficients and in particular characters of irreducible representations of a compact Lie group are special Laplacian eigenfunctions thereon. In the Laplacian eigenvalue aspect, we prove sharp $L^p$ bounds of restriction of characters for all $p>0$ and of general matrix coefficients for all $p\\geq 2$, to maximal tori and their submanifolds.","sentences":["Matrix coefficients and in particular characters of irreducible representations of a compact Lie group are special Laplacian eigenfunctions thereon.","In the Laplacian eigenvalue aspect, we prove sharp $L^p$ bounds of restriction of characters for all $p>0$ and of general matrix coefficients for all $p\\geq 2$, to maximal tori and their submanifolds."],"url":"http://arxiv.org/abs/2402.03178v1","category":"math.RT"}
{"created":"2024-02-05 16:43:53","title":"Comparison of Topic Modelling Approaches in the Banking Context","abstract":"Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.","sentences":["Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems.","The approach is vital for service industries to monitor their customer discussions.","The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document.","Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture.","We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches.","Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463."],"url":"http://arxiv.org/abs/2402.03176v1","category":"cs.IR"}
{"created":"2024-02-05 16:42:10","title":"The Matrix: A Bayesian learning model for LLMs","abstract":"In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications.","sentences":["In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs).","We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle.","Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix.","We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior.","Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated.","Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications."],"url":"http://arxiv.org/abs/2402.03175v1","category":"cs.LG"}
{"created":"2024-02-05 16:41:02","title":"Multi: Multimodal Understanding Leaderboard with Text and Images","abstract":"Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.","sentences":["Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community.","Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions.","This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests.","It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning.","Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats.","We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces.","Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%.","Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI."],"url":"http://arxiv.org/abs/2402.03173v1","category":"cs.CL"}
{"created":"2024-02-05 16:40:23","title":"Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings","abstract":"Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification.","sentences":["Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches.","This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work.","We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently.","The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms.","Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance.","Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification."],"url":"http://arxiv.org/abs/2402.03172v1","category":"cs.CL"}
{"created":"2024-02-05 16:38:30","title":"A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation","abstract":"This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.","sentences":["This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold.","Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal.","These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime.","This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization.","We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit."],"url":"http://arxiv.org/abs/2402.03169v1","category":"stat.ML"}
{"created":"2024-02-05 16:36:17","title":"Data-driven reconstruction of limit cycle position provides side information for improved model identification with SINDy","abstract":"Many important systems in nature are characterized by oscillations. To understand and interpret such behavior, researchers use the language of mathematical models, often in the form of differential equations. Nowadays, these equations can be derived using data-driven machine learning approaches, such as the white-box method 'Sparse Identification of Nonlinear Dynamics' (SINDy). In this paper, we show that to ensure the identification of sparse and meaningful models, it is crucial to identify the correct position of the system limit cycle in phase space. Therefore, we propose how the limit cycle position and the system's nullclines can be identified by applying SINDy to the data set with varying offsets, using three model evaluation criteria (complexity, coefficient of determination, generalization error). We successfully test the method on an oscillatory FitzHugh-Nagumo model and a more complex model consisting of two coupled cubic differential equations. Finally, we demonstrate that using this additional side information on the limit cycle in phase space can improve the success of model identification efforts in oscillatory systems.","sentences":["Many important systems in nature are characterized by oscillations.","To understand and interpret such behavior, researchers use the language of mathematical models, often in the form of differential equations.","Nowadays, these equations can be derived using data-driven machine learning approaches, such as the white-box method 'Sparse Identification of Nonlinear Dynamics' (SINDy).","In this paper, we show that to ensure the identification of sparse and meaningful models, it is crucial to identify the correct position of the system limit cycle in phase space.","Therefore, we propose how the limit cycle position and the system's nullclines can be identified by applying SINDy to the data set with varying offsets, using three model evaluation criteria (complexity, coefficient of determination, generalization error).","We successfully test the method on an oscillatory FitzHugh-Nagumo model and a more complex model consisting of two coupled cubic differential equations.","Finally, we demonstrate that using this additional side information on the limit cycle in phase space can improve the success of model identification efforts in oscillatory systems."],"url":"http://arxiv.org/abs/2402.03168v1","category":"nlin.AO"}
{"created":"2024-02-05 16:35:29","title":"RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification","abstract":"The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions. A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique. Nonetheless, manually performing these tasks is labor-intensive and prone to human error. Various automated methods have been proposed to address this problem. However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps. This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors. The framework consists of a fully convolutional neural network with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps. Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches. In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency. The model code, weights, and predictions are publicly available at https://github.com/j-morano/rrwnet.","sentences":["The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions.","A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique.","Nonetheless, manually performing these tasks is labor-intensive and prone to human error.","Various automated methods have been proposed to address this problem.","However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps.","This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors.","The framework consists of a fully convolutional neural network with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps.","Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches.","In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency.","The model code, weights, and predictions are publicly available at https://github.com/j-morano/rrwnet."],"url":"http://arxiv.org/abs/2402.03166v1","category":"cs.CV"}
{"created":"2024-02-05 16:32:12","title":"Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories","abstract":"Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus. The most commonly used approach represents time by adding a real-valued fluent $\\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation. We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects. We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators. %that only allow comparisons against fixed rationals. With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable. Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence that is a successful execution of a given program.","sentences":["Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus.","The most commonly used approach represents time by adding a real-valued fluent $\\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation.","We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects.","We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators.","%that only allow comparisons against fixed rationals.","With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable.","Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence that is a successful execution of a given program."],"url":"http://arxiv.org/abs/2402.03164v1","category":"cs.AI"}
{"created":"2024-02-05 16:30:57","title":"Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion","abstract":"Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page: https://direct-a-video.github.io/.","sentences":["Recent text-to-video diffusion models have achieved impressive progress.","In practice, users often desire the ability to control object motion and camera movement independently for customized video creation.","However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models.","In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video.","We propose a simple yet effective strategy for the decoupled control of object motion and camera movement.","Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization.","For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters.","We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation.","Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios.","Extensive experiments demonstrate the superiority and effectiveness of our method.","Project page: https://direct-a-video.github.io/."],"url":"http://arxiv.org/abs/2402.03162v1","category":"cs.CV"}
{"created":"2024-02-05 16:30:49","title":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization","abstract":"In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.","sentences":["In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos.","Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics.","In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions.","These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text.","At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content.","Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation.","Our code and models will be available at https://video-lavit.github.io."],"url":"http://arxiv.org/abs/2402.03161v1","category":"cs.CV"}
{"created":"2024-02-05 16:29:16","title":"Equations of genus $4$ curves from their theta constants","abstract":"In this article we give explicit formulas for the equations of a generic genus $4$ curve in terms of its theta constants. The method uses 20 tritangent planes as well as the Prym construction and the beautiful classical geometry around it.","sentences":["In this article we give explicit formulas for the equations of a generic genus $4$ curve in terms of its theta constants.","The method uses 20 tritangent planes as well as the Prym construction and the beautiful classical geometry around it."],"url":"http://arxiv.org/abs/2402.03160v1","category":"math.AG"}
{"created":"2024-02-05 16:28:29","title":"State Dependent and Independent Uncertainty Relations for Skew Informations and Standard Deviations","abstract":"To understand the direct impact of noncommutativity of incompatible observables, the commutator of incompatible observables should be present explicitly in any uncertainty relation. The Robertson-Heisenberg uncertainty relation contains such commutator and thus implies that both the standard deviations of incompatible observables can not attain their respective minimum values (i.e., zero). In this work, we derive state dependent uncertainty relations (in which commutators are explicitly present) and state independent uncertainty relations based on Wigner-Yanase (-Dyson) skew information. Also we derive uncertainty equality based on Wigner-Yanase (-Dyson) skew information and standard deviation for mixed states. We show that for pure states, Wigner-Yanase skew information based state independent uncertainty relations become standard deviation based state independent uncertainty relations which turn out to be tighter uncertainty relations than the ones given in the work of Giorda et al. [Phys. Rev. A 99, 052121 (2019)] for some cases, and we generalize the work of Giorda et al. for arbitrary operators. State independent uncertainty relation for Wigner-Yanase skew informations of a collection of quantum channels is also derived. We show that state dependent and independent uncertainty relations based on a more general version of skew information called generalized skew information appeared in Yang et al. [Phys. Rev. A 106, 052401 (2022)] which includes the Wigner-Yanase (-Dyson) skew information and Fisher information as special cases hold. In a spin-1/2 system, we derive state independent uncertainty inequalities and equalities for different form of generalized skew informations and standard deviations, and discuss in details.","sentences":["To understand the direct impact of noncommutativity of incompatible observables, the commutator of incompatible observables should be present explicitly in any uncertainty relation.","The Robertson-Heisenberg uncertainty relation contains such commutator and thus implies that both the standard deviations of incompatible observables can not attain their respective minimum values (i.e., zero).","In this work, we derive state dependent uncertainty relations (in which commutators are explicitly present) and state independent uncertainty relations based on Wigner-Yanase (-Dyson) skew information.","Also we derive uncertainty equality based on Wigner-Yanase (-Dyson) skew information and standard deviation for mixed states.","We show that for pure states, Wigner-Yanase skew information based state independent uncertainty relations become standard deviation based state independent uncertainty relations which turn out to be tighter uncertainty relations than the ones given in the work of Giorda et al.","[Phys. Rev.","A 99, 052121 (2019)] for some cases, and we generalize the work of Giorda et al. for arbitrary operators.","State independent uncertainty relation for Wigner-Yanase skew informations of a collection of quantum channels is also derived.","We show that state dependent and independent uncertainty relations based on a more general version of skew information called generalized skew information appeared in Yang et al.","[Phys. Rev.","A 106, 052401 (2022)] which includes the Wigner-Yanase (-Dyson) skew information and Fisher information as special cases hold.","In a spin-1/2 system, we derive state independent uncertainty inequalities and equalities for different form of generalized skew informations and standard deviations, and discuss in details."],"url":"http://arxiv.org/abs/2402.03159v1","category":"quant-ph"}
{"created":"2024-02-05 16:19:55","title":"Holonomy operator for spin connection and spatial scalar curvature operator in loop quantum gravity","abstract":"In this article we propose a new construction of the spatial scalar curvature operator in (1+3)-dimensional LQG based on the twisted geometry. The starting point of the construction is to express the holonomy of the spin connection on a graph in terms of the twisted geometry variables, and we check that this expression reproduces the spin connection in terms of triads in a certain continuum limit. The spatial scalar curvature in terms of twisted geometry is obtained by considering the composition of the holonomy of the spin connection on the loops. With the twisted geometry parametrization of the holonomy-flux phase space, we further express the holonomy of the spin connection and the spatial scalar curvature on a graph in terms of fluxes. Finally, they are promoted as well-defined operators by replacing the fluxes with ordered flux operators.","sentences":["In this article we propose a new construction of the spatial scalar curvature operator in (1+3)-dimensional LQG based on the twisted geometry.","The starting point of the construction is to express the holonomy of the spin connection on a graph in terms of the twisted geometry variables, and we check that this expression reproduces the spin connection in terms of triads in a certain continuum limit.","The spatial scalar curvature in terms of twisted geometry is obtained by considering the composition of the holonomy of the spin connection on the loops.","With the twisted geometry parametrization of the holonomy-flux phase space, we further express the holonomy of the spin connection and the spatial scalar curvature on a graph in terms of fluxes.","Finally, they are promoted as well-defined operators by replacing the fluxes with ordered flux operators."],"url":"http://arxiv.org/abs/2402.03154v1","category":"gr-qc"}
{"created":"2024-02-05 16:19:53","title":"Learning solutions of parametric Navier-Stokes with physics-informed neural networks","abstract":"We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE). Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE. We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters. We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest. Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters. Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function. We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum. Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels.","sentences":["We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE).","Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE.","We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters.","We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest.","Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters.","Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function.","We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum.","Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels."],"url":"http://arxiv.org/abs/2402.03153v1","category":"cs.CE"}
{"created":"2024-02-05 16:17:58","title":"Axion-like Quasiparticles and Topological States of Matter: Finite Density Corrections of the Chiral Anomaly Vertex","abstract":"We investigate the general structure of the chiral anomaly $AVV/AAA$ and $(LLL, RRR)$ vertices, in the presence of chemical potentials in perturbation theory. The study finds application in anomalous transport, whenever chirally unbalanced matter is present, with propagating external currents that are classically conserved. Examples are topological materials and the chiral magnetic effect in the plasma state of matter of the early universe. We classify the minimal number of form factors of the $AVV$ parameterization, by a complete analysis of the Schouten identities in the presence of a heat bath. We show that the longitudinal (anomaly) sector in the axial-vector channel, for on-shell and off-shell photons, is protected against corrections coming from the insertion of a chemical potential in the fermion loop. When the photons are on-shell, we prove that also the transverse sector, in the same channel, is $\\mu$-independent and vanishes. The related effective action is shown to be always described by the exchange of a massless anomaly pole, as in the case of vanishing chemical potentials. The pole is interpreted as an interpolating axion-like quasiparticle generated by the anomaly. In each axial-vector channel, it is predicted to be a correlated fermion/antifermion pseudoscalar (axion-like) quasiparticle appearing in the response function, once the material is subjected to an external chiral perturbation. The cancellation of the $\\mu$ dependence extends to any chiral current within the Standard Model, including examples like $B$ (baryon), $L$ (lepton), and $B-L$. This holds true irrespective of whether these currents exhibit anomalies.","sentences":["We investigate the general structure of the chiral anomaly $AVV/AAA$ and $(LLL, RRR)$ vertices, in the presence of chemical potentials in perturbation theory.","The study finds application in anomalous transport, whenever chirally unbalanced matter is present, with propagating external currents that are classically conserved.","Examples are topological materials and the chiral magnetic effect in the plasma state of matter of the early universe.","We classify the minimal number of form factors of the $AVV$ parameterization, by a complete analysis of the Schouten identities in the presence of a heat bath.","We show that the longitudinal (anomaly) sector in the axial-vector channel, for on-shell and off-shell photons, is protected against corrections coming from the insertion of a chemical potential in the fermion loop.","When the photons are on-shell, we prove that also the transverse sector, in the same channel, is $\\mu$-independent and vanishes.","The related effective action is shown to be always described by the exchange of a massless anomaly pole, as in the case of vanishing chemical potentials.","The pole is interpreted as an interpolating axion-like quasiparticle generated by the anomaly.","In each axial-vector channel, it is predicted to be a correlated fermion/antifermion pseudoscalar (axion-like) quasiparticle appearing in the response function, once the material is subjected to an external chiral perturbation.","The cancellation of the $\\mu$ dependence extends to any chiral current within the Standard Model, including examples like $B$ (baryon), $L$ (lepton), and $B-L$.","This holds true irrespective of whether these currents exhibit anomalies."],"url":"http://arxiv.org/abs/2402.03151v1","category":"hep-ph"}
{"created":"2024-02-05 16:16:17","title":"A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators","abstract":"Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organizations with three different orders of these manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2) Aggregation-Splitting-Modulation-Weighting (ASMW), and (3) Splitting-Modulation-Weighting-Aggregation (SMWA). We show that these organizations affect the crosstalk noise and optical signal losses in different magnitudes, which renders these organizations with different levels of processing parallelism at the circuit level, and different magnitudes of throughput and energy-area efficiency at the system level. Our evaluation results for four CNN models show that SMWA organization achieves up to 4.4$\\times$, 5$\\times$, and 5.2$\\times$ better throughput, energy efficiency, and area-energy efficiency, respectively, compared to ASMW and MASW organizations on average.","sentences":["Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency.","To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals.","The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance.","In this paper, we conduct a detailed analysis of accelerator organizations with three different orders of these manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2) Aggregation-Splitting-Modulation-Weighting (ASMW), and (3) Splitting-Modulation-Weighting-Aggregation (SMWA).","We show that these organizations affect the crosstalk noise and optical signal losses in different magnitudes, which renders these organizations with different levels of processing parallelism at the circuit level, and different magnitudes of throughput and energy-area efficiency at the system level.","Our evaluation results for four CNN models show that SMWA organization achieves up to 4.4$\\times$, 5$\\times$, and 5.2$\\times$ better throughput, energy efficiency, and area-energy efficiency, respectively, compared to ASMW and MASW organizations on average."],"url":"http://arxiv.org/abs/2402.03149v1","category":"cs.AR"}
{"created":"2024-02-05 16:12:36","title":"SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems","abstract":"The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion. To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming. We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods.","sentences":["The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD).","In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion.","To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming.","We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.03145v1","category":"eess.SY"}
{"created":"2024-02-05 16:12:27","title":"Computing Generic Fibres of Polynomial Ideals with FGLM and Hensel Lifting","abstract":"We describe a version of the FGLM algorithm that can be used to compute generic fibers of positive-dimensional polynomial ideals. It combines the FGLM algorithm with a Hensel lifting strategy. We show that this algorithm has a complexity quasi-linear in the number of lifting steps. Some provided experimental data also demonstrates the practical efficacy of our algorithm. Additionally, we sketch a related Hensel lifting method to compute Gr\\\"obner bases using so-called tracers.","sentences":["We describe a version of the FGLM algorithm that can be used to compute generic fibers of positive-dimensional polynomial ideals.","It combines the FGLM algorithm with a Hensel lifting strategy.","We show that this algorithm has a complexity quasi-linear in the number of lifting steps.","Some provided experimental data also demonstrates the practical efficacy of our algorithm.","Additionally, we sketch a related Hensel lifting method to compute Gr\\\"obner bases using so-called tracers."],"url":"http://arxiv.org/abs/2402.03144v1","category":"cs.SC"}
{"created":"2024-02-05 16:11:03","title":"Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task","abstract":"Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficiency and policy performance.","sentences":["Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays.","State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments.","To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments.","Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task.","We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task.","On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficiency and policy performance."],"url":"http://arxiv.org/abs/2402.03141v1","category":"cs.LG"}
{"created":"2024-02-05 16:08:58","title":"Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations","abstract":"In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.","sentences":["In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem.","We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant.","We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts.","Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations.","Overall, this presents a pathway for integrating pre-trained biases into exploration.","We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings."],"url":"http://arxiv.org/abs/2402.03138v1","category":"cs.LG"}
{"created":"2024-02-05 16:03:44","title":"Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games","abstract":"The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatross is able to exploit weak agents in the competitive game of Battlesnake. Additionally, it yields an improvement of 37.6% compared to previous state of the art in the cooperative Overcooked benchmark.","sentences":["The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go.","However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge.","In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all.","Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games.","To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play.","Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength.","We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games.","In contrast to AlphaZero, Albatross is able to exploit weak agents in the competitive game of Battlesnake.","Additionally, it yields an improvement of 37.6% compared to previous state of the art in the cooperative Overcooked benchmark."],"url":"http://arxiv.org/abs/2402.03136v1","category":"cs.AI"}
{"created":"2024-02-05 16:01:15","title":"GPU-Accelerated 3D Polygon Visibility Volumes for Synergistic Perception and Navigation","abstract":"UAV missions often require specific geometric constraints to be satisfied between ground locations and the vehicle location. Such requirements are typical for contexts where line-of-sight must be maintained between the vehicle location and the ground control location and are also important in surveillance applications where the UAV wishes to be able to sense, e.g., with a camera sensor, a specific region within a complex geometric environment. This problem is further complicated when the ground location is generalized to a convex 2D polygonal region. This article describes the theory and implementation of a system which can quickly calculate the 3D volume that encloses all 3D coordinates from which a 2D convex planar region can be entirely viewed; referred to as a visibility volume. The proposed approach computes visibility volumes using a combination of depth map computation using GPU-acceleration and geometric boolean operations. Solutions to this problem require complex 3D geometric analysis techniques that must execute using arbitrary precision arithmetic on a collection of discontinuous and non-analytic surfaces. Post-processing steps incorporate navigational constraints to further restrict the enclosed coordinates to include both visibility and navigation constraints. Integration of sensing visibility constraints with navigational constraints yields a range of navigable space where a vehicle will satisfy both perceptual sensing and navigational needs of the mission. This algorithm then provides a synergistic perception and navigation sensitive solution yielding a volume of coordinates in 3D that satisfy both the mission path and sensing needs.","sentences":["UAV missions often require specific geometric constraints to be satisfied between ground locations and the vehicle location.","Such requirements are typical for contexts where line-of-sight must be maintained between the vehicle location and the ground control location and are also important in surveillance applications where the UAV wishes to be able to sense, e.g., with a camera sensor, a specific region within a complex geometric environment.","This problem is further complicated when the ground location is generalized to a convex 2D polygonal region.","This article describes the theory and implementation of a system which can quickly calculate the 3D volume that encloses all 3D coordinates from which a 2D convex planar region can be entirely viewed; referred to as a visibility volume.","The proposed approach computes visibility volumes using a combination of depth map computation using GPU-acceleration and geometric boolean operations.","Solutions to this problem require complex 3D geometric analysis techniques that must execute using arbitrary precision arithmetic on a collection of discontinuous and non-analytic surfaces.","Post-processing steps incorporate navigational constraints to further restrict the enclosed coordinates to include both visibility and navigation constraints.","Integration of sensing visibility constraints with navigational constraints yields a range of navigable space where a vehicle will satisfy both perceptual sensing and navigational needs of the mission.","This algorithm then provides a synergistic perception and navigation sensitive solution yielding a volume of coordinates in 3D that satisfy both the mission path and sensing needs."],"url":"http://arxiv.org/abs/2402.03135v1","category":"cs.RO"}
{"created":"2024-02-05 15:56:19","title":"User-Centric Evaluation of ChatGPT Capability of Generating R Program Code","abstract":"This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters. Our experiments demonstrated that ChatGPT is in general highly capable of generating high quality R program code as well as textual explanations although it may fail on hard programming tasks. The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using ChatGPT to generate code.","sentences":["This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input.","A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs.","The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts.","In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed.","In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters.","Our experiments demonstrated that ChatGPT is in general highly capable of generating high quality R program code as well as textual explanations although it may fail on hard programming tasks.","The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using ChatGPT to generate code."],"url":"http://arxiv.org/abs/2402.03130v1","category":"cs.SE"}
{"created":"2024-02-05 15:50:36","title":"Behavioral transition of a fish school in a crowded environment","abstract":"In open water, social fish gather to form schools, in which fish generally align with each other. In this work, we study how this social behavior evolves when perturbed by artificial obstacles. We measure the collective behavior of a group of zebrafish in the presence of a periodic array of pillars. When pillar density is low, the fish regroup with a typical inter-distance and a well-polarized state with parallel orientations, similar to their behavior in open water conditions. Above a critical density of pillars, their social interactions, which are mostly based on vision, are screened and the fish spread randomly through the aquarium, orienting themselves along the free axes of the pillar lattice. The abrupt transition from natural to artificial orientation happens when the pillar inter-distance is comparable to the social distance of the fish, i.e., their most probable inter-distance. We develop a stochastic model of the relative orientation between fish pairs, taking into account alignment, anti-alignment and tumbling, from a distribution biased by the environment. This model provides a good description of the experimental probability distribution of the relative orientation between the fish and captures the behavioral transition. Using the model to fit the experimental data provides qualitative information on the evolution of cognitive parameters, such as the alignment or the tumbling rates, as the pillar density increases. At high pillar density, we find that the artificial environment imposes its geometrical constraints to the fish school, drastically increasing the tumbling rate.","sentences":["In open water, social fish gather to form schools, in which fish generally align with each other.","In this work, we study how this social behavior evolves when perturbed by artificial obstacles.","We measure the collective behavior of a group of zebrafish in the presence of a periodic array of pillars.","When pillar density is low, the fish regroup with a typical inter-distance and a well-polarized state with parallel orientations, similar to their behavior in open water conditions.","Above a critical density of pillars, their social interactions, which are mostly based on vision, are screened and the fish spread randomly through the aquarium, orienting themselves along the free axes of the pillar lattice.","The abrupt transition from natural to artificial orientation happens when the pillar inter-distance is comparable to the social distance of the fish, i.e., their most probable inter-distance.","We develop a stochastic model of the relative orientation between fish pairs, taking into account alignment, anti-alignment and tumbling, from a distribution biased by the environment.","This model provides a good description of the experimental probability distribution of the relative orientation between the fish and captures the behavioral transition.","Using the model to fit the experimental data provides qualitative information on the evolution of cognitive parameters, such as the alignment or the tumbling rates, as the pillar density increases.","At high pillar density, we find that the artificial environment imposes its geometrical constraints to the fish school, drastically increasing the tumbling rate."],"url":"http://arxiv.org/abs/2402.03123v1","category":"physics.bio-ph"}
{"created":"2024-02-05 15:48:43","title":"Towards multiqudit quantum processor based on a $^{171}$Yb$^{+}$ ion string: Realizing basic quantum algorithms","abstract":"We demonstrate a quantum processor based on a 3D linear Paul trap that uses $^{171}$Yb$^{+}$ ions with 8 individually controllable four-level qudits (ququarts), which is computationally equivalent to a 16-qubit quantum processor. The design of the developed ion trap provides high secular frequencies, low heating rate, which together with individual addressing and readout optical systems allows executing quantum algorithms. In each of the 8 ions, we use four electronic levels coupled by E2 optical transition at 435nm for qudit encoding. We present the results of single- and two- qubit operations benchmarking, generation of a 5-particle Greenberger-Horne-Zeilinger entangled state, and realizing basic quantum algorithms, including Bernstein-Vazirani and Grover's search algorithms as well as H$_2$ and LiH molecular simulations. Our results pave the way to scalable qudit-based quantum processors using trapped ions.","sentences":["We demonstrate a quantum processor based on a 3D linear Paul trap that uses $^{171}$Yb$^{+}$ ions with 8 individually controllable four-level qudits (ququarts), which is computationally equivalent to a 16-qubit quantum processor.","The design of the developed ion trap provides high secular frequencies, low heating rate, which together with individual addressing and readout optical systems allows executing quantum algorithms.","In each of the 8 ions, we use four electronic levels coupled by E2 optical transition at 435nm for qudit encoding.","We present the results of single- and two- qubit operations benchmarking, generation of a 5-particle Greenberger-Horne-Zeilinger entangled state, and realizing basic quantum algorithms, including Bernstein-Vazirani and Grover's search algorithms as well as H$_2$ and LiH molecular simulations.","Our results pave the way to scalable qudit-based quantum processors using trapped ions."],"url":"http://arxiv.org/abs/2402.03121v1","category":"quant-ph"}
{"created":"2024-02-05 15:48:41","title":"Cosmography with next-generation gravitational wave detectors","abstract":"Advancements in cosmology through next-generation ground-based gravitational wave observatories will bring in a paradigm shift. We explore the pivotal role that gravitational-wave standard sirens will play in inferring cosmological parameters with next-generation observatories, not only achieving exquisite precision but also opening up unprecedented redshifts. We examine the merits and the systematic biases involved in gravitational-wave standard sirens utilizing binary black holes, binary neutron stars, and neutron star-black hole mergers. Further, we estimate the precision of bright sirens, golden dark sirens, and spectral sirens for these binary coalescences and compare the abilities of various next-generation observatories (A^sharp, Cosmic Explorer, Einstein Telescope, and their possible networks). When combining different sirens, we find sub-percent precision over more than 10 billion years of cosmic evolution for the Hubble expansion rate $H(z)$. This work presents a broad view of opportunities to precisely measure the cosmic expansion rate, decipher the elusive dark energy and dark matter, and potentially discover new physics in the uncharted Universe with next-generation gravitational-wave detectors.","sentences":["Advancements in cosmology through next-generation ground-based gravitational wave observatories will bring in a paradigm shift.","We explore the pivotal role that gravitational-wave standard sirens will play in inferring cosmological parameters with next-generation observatories, not only achieving exquisite precision but also opening up unprecedented redshifts.","We examine the merits and the systematic biases involved in gravitational-wave standard sirens utilizing binary black holes, binary neutron stars, and neutron star-black hole mergers.","Further, we estimate the precision of bright sirens, golden dark sirens, and spectral sirens for these binary coalescences and compare the abilities of various next-generation observatories (A^sharp, Cosmic Explorer, Einstein Telescope, and their possible networks).","When combining different sirens, we find sub-percent precision over more than 10 billion years of cosmic evolution for the Hubble expansion rate $H(z)$.","This work presents a broad view of opportunities to precisely measure the cosmic expansion rate, decipher the elusive dark energy and dark matter, and potentially discover new physics in the uncharted Universe with next-generation gravitational-wave detectors."],"url":"http://arxiv.org/abs/2402.03120v1","category":"gr-qc"}
{"created":"2024-02-05 15:47:54","title":"Good Teachers Explain: Explanation-Enhanced Knowledge Distillation","abstract":"Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations.","sentences":["Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models.","While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function.","It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers.","In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student.","Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations."],"url":"http://arxiv.org/abs/2402.03119v1","category":"cs.CV"}
{"created":"2024-02-05 15:44:43","title":"Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism","abstract":"Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions. However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges. Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds. Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions. This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions.","sentences":["Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions.","However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges.","Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds.","Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions.","This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions."],"url":"http://arxiv.org/abs/2402.03112v1","category":"cs.LG"}
{"created":"2024-02-05 15:44:16","title":"Computing roadmaps in unbounded smooth real algebraic sets II: algorithm and complexity","abstract":"A roadmap for an algebraic set $V$ defined by polynomials with coefficients in some real field, say $\\mathbb{R}$, is an algebraic curve contained in $V$ whose intersection with all connected components of $V\\cap\\mathbb{R}^{n}$ is connected. These objects, introduced by Canny, can be used to answer connectivity queries over $V\\cap \\mathbb{R}^{n}$ provided that they are required to contain the finite set of query points $\\mathcal{P}\\subset V$; in this case,we say that the roadmap is associated to $(V, \\mathcal{P})$.   In this paper, we make effective a connectivity result we previously proved, to design a Monte Carlo algorithm which, on input (i) a finite sequence of polynomials defining $V$ (and satisfying some regularity assumptions) and (ii) an algebraic representation of finitely many query points $\\mathcal{P}$ in $V$, computes a roadmap for $(V, \\mathcal{P})$. This algorithm generalizes the nearly optimal one introduced by the last two authors by dropping a boundedness assumption on the real trace of $V$.   The output size and running times of our algorithm are both polynomial in $(nD)^{n\\log d}$, where $D$ is the maximal degree of the input equations and $d$ is the dimension of $V$. As far as we know, the best previously known algorithm dealing with such sets has an output size and running time polynomial in $(nD)^{n\\log^2 n}$.","sentences":["A roadmap for an algebraic set $V$ defined by polynomials with coefficients in some real field, say $\\mathbb{R}$, is an algebraic curve contained in $V$ whose intersection with all connected components of $V\\cap\\mathbb{R}^{n}$ is connected.","These objects, introduced by Canny, can be used to answer connectivity queries over $V\\cap \\mathbb{R}^{n}$ provided that they are required to contain the finite set of query points $\\mathcal{P}\\subset V$; in this case,we say that the roadmap is associated to $(V, \\mathcal{P})$.   ","In this paper, we make effective a connectivity result we previously proved, to design a Monte Carlo algorithm which, on input (i) a finite sequence of polynomials defining $V$ (and satisfying some regularity assumptions) and (ii) an algebraic representation of finitely many query points $\\mathcal{P}$ in $V$, computes a roadmap for $(V, \\mathcal{P})$.","This algorithm generalizes the nearly optimal one introduced by the last two authors by dropping a boundedness assumption on the real trace of $V$.   The output size and running times of our algorithm are both polynomial in $(nD)^{n\\log d}$, where $D$ is the maximal degree of the input equations and $d$ is the dimension of $V$. As far as we know, the best previously known algorithm dealing with such sets has an output size and running time polynomial in $(nD)^{n\\log^2 n}$."],"url":"http://arxiv.org/abs/2402.03111v1","category":"cs.SC"}
{"created":"2024-02-05 15:38:01","title":"Non-Stationary Latent Auto-Regressive Bandits","abstract":"We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\\tilde{O}(k\\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.","sentences":["We consider the stochastic multi-armed bandit problem with non-stationary rewards.","We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$.","We call this new environment the latent AR bandit.","Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment.","If the AR order $k$ is known, we propose an algorithm that achieves $\\tilde{O}(k\\sqrt{T})$ regret in this setting.","Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified."],"url":"http://arxiv.org/abs/2402.03110v1","category":"cs.LG"}
{"created":"2024-02-05 15:34:24","title":"Groups generated by pattern avoiding permutations","abstract":"We study groups generated by sets of pattern avoiding permutations. In the first part of the paper we prove some general results concerning the structure of such groups. In the second part we carry out a case-by-case analysis of groups generated by permutations avoiding few short patterns.","sentences":["We study groups generated by sets of pattern avoiding permutations.","In the first part of the paper we prove some general results concerning the structure of such groups.","In the second part we carry out a case-by-case analysis of groups generated by permutations avoiding few short patterns."],"url":"http://arxiv.org/abs/2402.03107v1","category":"math.CO"}
{"created":"2024-02-05 15:33:38","title":"DARTS: Diffusion Approximated Residual Time Sampling for Low Variance Time-of-flight Rendering in Homogeneous Scattering Medium","abstract":"Time-of-flight (ToF) devices have greatly propelled the advancement of various multi-modal perception applications. However, due to complexity in both sampling path construction and vertex connection in time domain, it is extremely challenging to accurately render time-resolved information in ToF device simulation, particularly in scenes involving complex geometric structures, diverse materials and volumetric scattering media. Existing works either exhibit significant bias or variance in ToF rendering tasks or prove ineffective in scenes involving participating media and camera-warped settings. To address this challenge, in this paper, we integrate the transient diffusion theory into path construction to generate the unbiased full transport of time-resolved radiance. Additionally, we devise an elliptical sampling method to provide controllable vertex connection satisfying any required photon traversal time. To our knowledge, our work is the first to explore importance sampling according to transient radiance, enabling temporal path construction of higher quality in multiple scattering settings. Extensive experiments show that our sampling method can significantly improve both quality and efficiency of ToF rendering within both path tracing and photon-based frameworks, with at least a 5x MSE reduction versus SOTA methods in equal rendering time. Our method introduces no memory overhead and negligible extra computation compared to the boost in speed, providing a straightforward plug-in for various existing rendering frameworks.","sentences":["Time-of-flight (ToF) devices have greatly propelled the advancement of various multi-modal perception applications.","However, due to complexity in both sampling path construction and vertex connection in time domain, it is extremely challenging to accurately render time-resolved information in ToF device simulation, particularly in scenes involving complex geometric structures, diverse materials and volumetric scattering media.","Existing works either exhibit significant bias or variance in ToF rendering tasks or prove ineffective in scenes involving participating media and camera-warped settings.","To address this challenge, in this paper, we integrate the transient diffusion theory into path construction to generate the unbiased full transport of time-resolved radiance.","Additionally, we devise an elliptical sampling method to provide controllable vertex connection satisfying any required photon traversal time.","To our knowledge, our work is the first to explore importance sampling according to transient radiance, enabling temporal path construction of higher quality in multiple scattering settings.","Extensive experiments show that our sampling method can significantly improve both quality and efficiency of ToF rendering within both path tracing and photon-based frameworks, with at least a 5x MSE reduction versus SOTA methods in equal rendering time.","Our method introduces no memory overhead and negligible extra computation compared to the boost in speed, providing a straightforward plug-in for various existing rendering frameworks."],"url":"http://arxiv.org/abs/2402.03106v1","category":"cs.GR"}
{"created":"2024-02-05 15:31:41","title":"Scoped Effects as Parameterized Algebraic Theories","abstract":"Notions of computation can be modelled by monads. Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions. However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources. Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories. We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories. The translation enables a new approach to equational reasoning for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations. We demonstrate the power of our fresh perspective by way of equational characterizations of several known models of scoped effects.","sentences":["Notions of computation can be modelled by monads.","Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions.","However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources.","Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories.","We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories.","The translation enables a new approach to equational reasoning for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations.","We demonstrate the power of our fresh perspective by way of equational characterizations of several known models of scoped effects."],"url":"http://arxiv.org/abs/2402.03103v1","category":"cs.PL"}
{"created":"2024-02-05 15:29:28","title":"A flow approach to the generalized KPZ equation","abstract":"We show that the flow approach of Duch [Duc21] can be adapted to prove local well-posedness for the generalised KPZ equation. The key step is to extend the flow approach so that it can accommodate semilinear equations involving smooth functions of the solution instead of only polynomials - this is accomplished by introducing coordinates for the flow built out of the elementary differentials associated to the equation.","sentences":["We show that the flow approach of Duch [Duc21] can be adapted to prove local well-posedness for the generalised KPZ equation.","The key step is to extend the flow approach so that it can accommodate semilinear equations involving smooth functions of the solution instead of only polynomials - this is accomplished by introducing coordinates for the flow built out of the elementary differentials associated to the equation."],"url":"http://arxiv.org/abs/2402.03101v1","category":"math.PR"}
{"created":"2024-02-05 15:28:43","title":"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases","abstract":"Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system's key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$.","sentences":["Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction.","Automatic prompt engineering is essential to achieve optimized performance from LLMs.","Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt.","However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases.","In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent.","During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset.","We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation.","Our method outperforms state-of-the-art methods with a limited number of annotated samples.","Furthermore, we validate the advantages of each one of the system's key components.","Our system is built in a modular way, facilitating easy adaptation to other tasks.","The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$."],"url":"http://arxiv.org/abs/2402.03099v1","category":"cs.CL"}
{"created":"2024-02-05 15:28:02","title":"Renormalization of conformal infinity as a stretched horizon","abstract":"In this paper, we provide a comprehensive study of asymptotically flat spacetime in even dimensions $d\\geq 4$. We analyze the most general boundary condition and asymptotic symmetry compatible with Penrose's definition of asymptotic null infinity $\\mathscr{I}$ through conformal compactification. Following Penrose's prescription and using a minimal version of the Bondi-Sachs gauge, we show that $\\mathscr{I}$ is naturally equipped with a Carrollian stress tensor whose radial derivative defines the asymptotic Weyl tensor. This analysis describes asymptotic infinity as a stretched horizon in the conformally compactified spacetime. We establish that charge aspects conservation can be written as Carrollian Bianchi identities for the asymptotic Weyl tensor. We then provide a covariant renormalization for the asymptotic symplectic potential, which results in a finite symplectic flux and asymptotic charges.","sentences":["In this paper, we provide a comprehensive study of asymptotically flat spacetime in even dimensions $d\\geq 4$. We analyze the most general boundary condition and asymptotic symmetry compatible with Penrose's definition of asymptotic null infinity $\\mathscr{I}$ through conformal compactification.","Following Penrose's prescription and using a minimal version of the Bondi-Sachs gauge, we show that $\\mathscr{I}$ is naturally equipped with a Carrollian stress tensor whose radial derivative defines the asymptotic Weyl tensor.","This analysis describes asymptotic infinity as a stretched horizon in the conformally compactified spacetime.","We establish that charge aspects conservation can be written as Carrollian Bianchi identities for the asymptotic Weyl tensor.","We then provide a covariant renormalization for the asymptotic symplectic potential, which results in a finite symplectic flux and asymptotic charges."],"url":"http://arxiv.org/abs/2402.03097v1","category":"gr-qc"}
{"created":"2024-02-05 15:25:40","title":"Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics","abstract":"Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations. Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones. Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability and more effective explanations for model vulnerabilities, indicating their great potential as generic adversarial examples. The code and pre-trained models were available at https://github.com/shuaili1027/MAELS.git.","sentences":["Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations.","Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability.","In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones.","Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability and more effective explanations for model vulnerabilities, indicating their great potential as generic adversarial examples.","The code and pre-trained models were available at https://github.com/shuaili1027/MAELS.git."],"url":"http://arxiv.org/abs/2402.03095v1","category":"cs.CV"}
{"created":"2024-02-05 15:25:32","title":"Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector","abstract":"This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting open-set detectors directly for CD-FSOD. Sequentially, to overcome the performance degradation issue and also to answer the second proposed question, we endeavor to enhance the vanilla DE-ViT. With several novel components including finetuning, a learnable prototype module, and a lightweight attention module, we present an improved Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO). Experiments show that our CD-ViTO achieves impressive results on both out-of-domain and in-domain target datasets, establishing new SOTAs for both CD-FSOD and FSOD. All the datasets, codes, and models will be released to the community.","sentences":["This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples.","While transformer-based open-set detectors e.g., DE-ViT~\\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD?","2) If no, how to enhance the results of open-set methods when faced with significant domain gaps?","To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values.","Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets.","This indicates the failure of adopting open-set detectors directly for CD-FSOD.","Sequentially, to overcome the performance degradation issue and also to answer the second proposed question, we endeavor to enhance the vanilla DE-ViT. With several novel components including finetuning, a learnable prototype module, and a lightweight attention module, we present an improved Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO).","Experiments show that our CD-ViTO achieves impressive results on both out-of-domain and in-domain target datasets, establishing new SOTAs for both CD-FSOD and FSOD.","All the datasets, codes, and models will be released to the community."],"url":"http://arxiv.org/abs/2402.03094v1","category":"cs.CV"}
{"created":"2024-02-05 15:24:13","title":"AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey","abstract":"With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift. Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences. Among those applications, medicine is considered one of the most promising areas. In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention. This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies. To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain.","sentences":["With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift.","Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences.","Among those applications, medicine is considered one of the most promising areas.","In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services.","By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention.","This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies.","To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain."],"url":"http://arxiv.org/abs/2402.03093v1","category":"cs.CV"}
{"created":"2024-02-05 15:23:04","title":"Optimal rate of convergence in periodic homogenization of viscous Hamilton-Jacobi equations","abstract":"We study the optimal rate of convergence in periodic homogenization of the viscous Hamilton-Jacobi equation $u^\\varepsilon_t + H(\\frac{x}{\\varepsilon},Du^\\varepsilon) = \\varepsilon \\Delta u^\\varepsilon$ in $\\mathbb R^n\\times (0,\\infty)$ subject to a given initial datum. We prove that $\\|u^\\varepsilon-u\\|_{L^\\infty(\\mathbb R^n \\times [0,T])} \\leq C(1+T) \\sqrt{\\varepsilon}$ for any given $T>0$. Moreover, we show that the $O(\\sqrt{\\varepsilon})$ rate is optimal in general, both theoretically and through numerical experiments. Finally, we propose a numerical scheme for the approximation of the effective Hamiltonian based on a finite element approximation of approximate corrector problems.","sentences":["We study the optimal rate of convergence in periodic homogenization of the viscous Hamilton-Jacobi equation $u^\\varepsilon_t + H(\\frac{x}{\\varepsilon},Du^\\varepsilon) = \\varepsilon \\Delta u^\\varepsilon$ in $\\mathbb R^n\\times (0,\\infty)$ subject to a given initial datum.","We prove that $\\|u^\\varepsilon-u\\|_{L^\\infty(\\mathbb R^n \\times","[0,T])} \\leq C(1+T)","\\sqrt{\\varepsilon}$ for any given $T>0$.","Moreover, we show that the $O(\\sqrt{\\varepsilon})$ rate is optimal in general, both theoretically and through numerical experiments.","Finally, we propose a numerical scheme for the approximation of the effective Hamiltonian based on a finite element approximation of approximate corrector problems."],"url":"http://arxiv.org/abs/2402.03091v1","category":"math.AP"}
{"created":"2024-02-05 15:22:56","title":"Sampling in quasi shift-invariant spaces and Gabor frames generated by ratios of exponential polynomials","abstract":"We introduce two families of generators (functions) $\\mathcal{G}$ that consist of entire and meromorphic functions enjoying a certain periodicity property and contain the classical Gaussian and hyperbolic secant generators. Sharp results are proved on the density of separated sets that provide non-uniform sampling for the shift-invariant and quasi shift-invariant spaces generated by elements of these families. As an application, we obtain new sharp results on the density of semi-regular lattices for the Gabor frames generated by elements from these families.","sentences":["We introduce two families of generators (functions) $\\mathcal{G}$ that consist of entire and meromorphic functions enjoying a certain periodicity property and contain the classical Gaussian and hyperbolic secant generators.","Sharp results are proved on the density of separated sets that provide non-uniform sampling for the shift-invariant and quasi shift-invariant spaces generated by elements of these families.","As an application, we obtain new sharp results on the density of semi-regular lattices for the Gabor frames generated by elements from these families."],"url":"http://arxiv.org/abs/2402.03090v1","category":"math.FA"}
{"created":"2024-02-05 15:17:04","title":"Locally unitary quantum state evolution is local","abstract":"We study the localization properties of bipartite channels, whose action on a subsystem yields a unitary channel. In particular we show that, under such channel, the subsystem must evolve independent of its environment. This point of view is another way to verify certain well-known conservation laws of quantum information in a generalized way. A no-go theorem for non classical conditional semantics in quantum computation is obtained as an intermediate result.","sentences":["We study the localization properties of bipartite channels, whose action on a subsystem yields a unitary channel.","In particular we show that, under such channel, the subsystem must evolve independent of its environment.","This point of view is another way to verify certain well-known conservation laws of quantum information in a generalized way.","A no-go theorem for non classical conditional semantics in quantum computation is obtained as an intermediate result."],"url":"http://arxiv.org/abs/2402.03088v1","category":"quant-ph"}
{"created":"2024-02-05 15:14:36","title":"XNLP-hardness of Parameterized Problems on Planar Graphs","abstract":"The class XNLP consists of (parameterized) problems that can be solved nondeterministically in $f(k)n^{O(1)}$ time and $f(k)\\log n$ space, where $n$ is the size of the input instance and $k$ the parameter. The class XALP consists of problems that can be solved in the above time and space with access to an additional stack. These two classes are a \"natural home\" for many standard graph problems and their generalizations.   In this paper, we show the hardness of several problems on planar graphs, parameterized by outerplanarity, treewidth and pathwidth, thus strengthening several existing results. In particular, we show the XNLP-hardness of the following problems parameterized by outerplanarity: All-or-Nothing Flow, Target Outdegree Orientation, Capacitated (Red-Blue) Dominating Set, Target Set Selections etc. We also show the XNLP-completeness of Scattered Set parameterized by pathwidth and XALP-completeness parameterized by treewidth and outerplanarity.","sentences":["The class XNLP consists of (parameterized) problems that can be solved nondeterministically in $f(k)n^{O(1)}$ time and $f(k)\\log n$ space, where $n$ is the size of the input instance and $k$ the parameter.","The class XALP consists of problems that can be solved in the above time and space with access to an additional stack.","These two classes are a \"natural home\" for many standard graph problems and their generalizations.   ","In this paper, we show the hardness of several problems on planar graphs, parameterized by outerplanarity, treewidth and pathwidth, thus strengthening several existing results.","In particular, we show the XNLP-hardness of the following problems parameterized by outerplanarity: All-or-Nothing Flow, Target Outdegree Orientation, Capacitated (Red-Blue) Dominating Set, Target Set Selections etc.","We also show the XNLP-completeness of Scattered Set parameterized by pathwidth and XALP-completeness parameterized by treewidth and outerplanarity."],"url":"http://arxiv.org/abs/2402.03087v1","category":"cs.CC"}
{"created":"2024-02-05 15:13:30","title":"Irrational moments and signatures of higher-rank gauge theories in diluted classical spin liquids","abstract":"Classical spin liquids (CSLs) have proved to be a fruitful setting for the emergence of exotic gauge theories. Vacancy clusters in CSLs can introduce gauge charges into the system, and the resulting behavior in turn reveals the nature of the underlying theory. We study these effects for a series of CSLs on the honeycomb lattice. We find that dilution leads to the emergence of effective free spins with tuneable, and generally irrational, size. For a specific higher-rank CSL, described by a symmetric tensor gauge fields, dilution produces non-decaying spin textures with a characteristic quadrupolar angular structure, and infinite-ranged interactions between dilution clusters.","sentences":["Classical spin liquids (CSLs) have proved to be a fruitful setting for the emergence of exotic gauge theories.","Vacancy clusters in CSLs can introduce gauge charges into the system, and the resulting behavior in turn reveals the nature of the underlying theory.","We study these effects for a series of CSLs on the honeycomb lattice.","We find that dilution leads to the emergence of effective free spins with tuneable, and generally irrational, size.","For a specific higher-rank CSL, described by a symmetric tensor gauge fields, dilution produces non-decaying spin textures with a characteristic quadrupolar angular structure, and infinite-ranged interactions between dilution clusters."],"url":"http://arxiv.org/abs/2402.03083v1","category":"cond-mat.str-el"}
{"created":"2024-02-05 15:13:20","title":"Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing","abstract":"Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial context are seamlessly integrated into various tasks. Furthermore, we explore available public datasets and benchmark the reviewed methods on several widely-used datasets. Finally, we identify principal challenges and potential avenues for future research. Our aim is to establish this survey as a fundamental resource, fostering continued exploration and innovation in the dynamic area of visual text processing.","sentences":["Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain.","Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models.","However, challenges persist due to the unique properties and features that distinguish text from general objects.","Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study.","In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field.","Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms.","Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial context are seamlessly integrated into various tasks.","Furthermore, we explore available public datasets and benchmark the reviewed methods on several widely-used datasets.","Finally, we identify principal challenges and potential avenues for future research.","Our aim is to establish this survey as a fundamental resource, fostering continued exploration and innovation in the dynamic area of visual text processing."],"url":"http://arxiv.org/abs/2402.03082v1","category":"cs.CV"}
{"created":"2024-02-05 15:12:15","title":"Preference-Conditioned Language-Guided Abstraction","abstract":"Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of the task and knowledge of behavioral change between states, we query the LM for possible hidden preferences; second, given the most likely preference, we query the LM to construct the state abstraction. In this framework, the LM is also able to ask the human directly when uncertain about its own estimate. We demonstrate our framework's ability to construct effective preference-conditioned abstractions in simulated experiments, a user study, as well as on a real Spot robot performing mobile manipulation tasks.","sentences":["Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations.","Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning.","However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone.","How do we construct abstractions to capture these latent preferences?","We observe that how humans behave reveals how they see the world.","Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions.","In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred.","In our framework, we use the LM in two ways: first, given a text description of the task and knowledge of behavioral change between states, we query the LM for possible hidden preferences; second, given the most likely preference, we query the LM to construct the state abstraction.","In this framework, the LM is also able to ask the human directly when uncertain about its own estimate.","We demonstrate our framework's ability to construct effective preference-conditioned abstractions in simulated experiments, a user study, as well as on a real Spot robot performing mobile manipulation tasks."],"url":"http://arxiv.org/abs/2402.03081v1","category":"cs.RO"}
{"created":"2024-02-05 15:10:51","title":"Improving Atmospheric Processes in Earth System Models with Deep Learning Ensembles and Stochastic Parameterizations","abstract":"Deep learning has proven to be a valuable tool to represent subgrid processes in climate models, but most application cases have so far used idealized settings and deterministic approaches. Here, we develop ensemble and stochastic parameterizations with calibrated uncertainty quantification to learn subgrid convective and turbulent processes and surface radiative fluxes of a superparameterization embedded in an Earth System Model (ESM). We explore three methods to construct stochastic parameterizations: 1) a single Deep Neural Network (DNN) with Monte Carlo Dropout; 2) a multi-network ensemble; and 3) a Variational Encoder Decoder with latent space perturbation. We show that the multi-network ensembles improve the representation of convective processes in the planetary boundary layer compared to individual DNNs. The respective uncertainty quantification illustrates that the two latter methods are advantageous compared to a dropout-based DNN ensemble regarding the spread of convective processes. We develop a novel partial coupling strategy to sidestep issues in condensate emulation to evaluate the multi-network parameterizations in online runs coupled to the ESM. We can conduct Earth-like stable runs over more than 5 months with the ensemble approach, while such simulations using individual DNNs fail within days. Moreover, we show that our novel ensemble parameterizations improve the representation of extreme precipitation and the underlying diurnal cycle compared to a traditional parameterization, although faithfully representing the mean precipitation pattern remains challenging. Our results pave the way towards a new generation of parameterizations using machine learning with realistic uncertainty quantification that significantly improve the representation of subgrid effects.","sentences":["Deep learning has proven to be a valuable tool to represent subgrid processes in climate models, but most application cases have so far used idealized settings and deterministic approaches.","Here, we develop ensemble and stochastic parameterizations with calibrated uncertainty quantification to learn subgrid convective and turbulent processes and surface radiative fluxes of a superparameterization embedded in an Earth System Model (ESM).","We explore three methods to construct stochastic parameterizations: 1) a single Deep Neural Network (DNN) with Monte Carlo Dropout; 2) a multi-network ensemble; and 3) a Variational Encoder Decoder with latent space perturbation.","We show that the multi-network ensembles improve the representation of convective processes in the planetary boundary layer compared to individual DNNs.","The respective uncertainty quantification illustrates that the two latter methods are advantageous compared to a dropout-based DNN ensemble regarding the spread of convective processes.","We develop a novel partial coupling strategy to sidestep issues in condensate emulation to evaluate the multi-network parameterizations in online runs coupled to the ESM.","We can conduct Earth-like stable runs over more than 5 months with the ensemble approach, while such simulations using individual DNNs fail within days.","Moreover, we show that our novel ensemble parameterizations improve the representation of extreme precipitation and the underlying diurnal cycle compared to a traditional parameterization, although faithfully representing the mean precipitation pattern remains challenging.","Our results pave the way towards a new generation of parameterizations using machine learning with realistic uncertainty quantification that significantly improve the representation of subgrid effects."],"url":"http://arxiv.org/abs/2402.03079v1","category":"physics.ao-ph"}
{"created":"2024-02-05 15:04:58","title":"A moment-based Hermite WENO scheme with unified stencils for hyperbolic conservation laws","abstract":"In this paper, a fifth-order moment-based Hermite weighted essentially non-oscillatory scheme with unified stencils (termed as HWENO-U) is proposed for hyperbolic conservation laws. The main idea of the HWENO-U scheme is to modify the first-order moment by a HWENO limiter only in the time discretizations using the same information of spatial reconstructions, in which the limiter not only overcomes spurious oscillations well, but also ensures the stability of the fully-discrete scheme. For the HWENO reconstructions, a new scale-invariant nonlinear weight is designed by incorporating only the integral average values of the solution, which keeps all properties of the original one while is more robust for simulating challenging problems with sharp scale variations. Compared with previous HWENO schemes, the advantages of the HWENO-U scheme are: (1) a simpler implemented process involving only a single HWENO reconstruction applied throughout the entire procedures without any modifications for the governing equations; (2) increased efficiency by utilizing the same candidate stencils, reconstructed polynomials, and linear and nonlinear weights in both the HWENO limiter and spatial reconstructions; (3) reduced problem-specific dependencies and improved rationality, as the nonlinear weights are identical for the function $u$ and its non-zero multiple $\\zeta u$. Besides, the proposed scheme retains the advantages of previous HWENO schemes, including compact reconstructed stencils and the utilization of artificial linear weights. Extensive benchmarks are carried out to validate the accuracy, efficiency, resolution, and robustness of the proposed scheme.","sentences":["In this paper, a fifth-order moment-based Hermite weighted essentially non-oscillatory scheme with unified stencils (termed as HWENO-U) is proposed for hyperbolic conservation laws.","The main idea of the HWENO-U scheme is to modify the first-order moment by a HWENO limiter only in the time discretizations using the same information of spatial reconstructions, in which the limiter not only overcomes spurious oscillations well, but also ensures the stability of the fully-discrete scheme.","For the HWENO reconstructions, a new scale-invariant nonlinear weight is designed by incorporating only the integral average values of the solution, which keeps all properties of the original one while is more robust for simulating challenging problems with sharp scale variations.","Compared with previous HWENO schemes, the advantages of the HWENO-U scheme are: (1) a simpler implemented process involving only a single HWENO reconstruction applied throughout the entire procedures without any modifications for the governing equations; (2) increased efficiency by utilizing the same candidate stencils, reconstructed polynomials, and linear and nonlinear weights in both the HWENO limiter and spatial reconstructions; (3) reduced problem-specific dependencies and improved rationality, as the nonlinear weights are identical for the function $u$ and its non-zero multiple $\\zeta u$.","Besides, the proposed scheme retains the advantages of previous HWENO schemes, including compact reconstructed stencils and the utilization of artificial linear weights.","Extensive benchmarks are carried out to validate the accuracy, efficiency, resolution, and robustness of the proposed scheme."],"url":"http://arxiv.org/abs/2402.03074v1","category":"math.NA"}
{"created":"2024-02-05 15:02:35","title":"Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning","abstract":"We investigated the human capacity to acquire multiple visuomotor mappings for de novo skills. Using a grid navigation paradigm, we tested whether contextual cues implemented as different \"grid worlds\", allow participants to learn two distinct key-mappings more efficiently. Our results indicate that when contextual information is provided, task performance is significantly better. The same held true for meta-reinforcement learning agents that differed in whether or not they receive contextual information when performing the task. We evaluated their accuracy in predicting human performance in the task and analyzed their internal representations. The results indicate that contextual cues allow the formation of separate representations in space and time when using different visuomotor mappings, whereas the absence of them favors sharing one representation. While both strategies can allow learning of multiple visuomotor mappings, we showed contextual cues provide a computational advantage in terms of how many mappings can be learned.","sentences":["We investigated the human capacity to acquire multiple visuomotor mappings for de novo skills.","Using a grid navigation paradigm, we tested whether contextual cues implemented as different \"grid worlds\", allow participants to learn two distinct key-mappings more efficiently.","Our results indicate that when contextual information is provided, task performance is significantly better.","The same held true for meta-reinforcement learning agents that differed in whether or not they receive contextual information when performing the task.","We evaluated their accuracy in predicting human performance in the task and analyzed their internal representations.","The results indicate that contextual cues allow the formation of separate representations in space and time when using different visuomotor mappings, whereas the absence of them favors sharing one representation.","While both strategies can allow learning of multiple visuomotor mappings, we showed contextual cues provide a computational advantage in terms of how many mappings can be learned."],"url":"http://arxiv.org/abs/2402.03072v1","category":"q-bio.NC"}
{"created":"2024-02-05 15:00:10","title":"Fixed Point Theorems in Computability Theory","abstract":"We give a quick survey of the various fixed point theorems in computability theory, partial combinatory algebra, and the theory of numberings, as well as generalizations based on those. We also point out several open problems connected to these.","sentences":["We give a quick survey of the various fixed point theorems in computability theory, partial combinatory algebra, and the theory of numberings, as well as generalizations based on those.","We also point out several open problems connected to these."],"url":"http://arxiv.org/abs/2402.03069v1","category":"math.LO"}
{"created":"2024-02-05 14:59:35","title":"A Note on Rounding Matchings in General Graphs","abstract":"In this note, we revisit the rounding algorithm of Wajc. Wajc gave a fully-adaptive randomized algorithm that rounds a dynamic fractional matching in an unweighted bipartite graph to an integral matching of nearly the same value in $O(\\text{poly}(\\log n,\\frac{1}{\\varepsilon}))$ update time. We give show that the guarantees of this algorithm hold for general graphs as well. Additionally, we show useful properties of this subroutine which have applications in rounding weighted fractional matchings.","sentences":["In this note, we revisit the rounding algorithm of Wajc.","Wajc gave a fully-adaptive randomized algorithm that rounds a dynamic fractional matching in an unweighted bipartite graph to an integral matching of nearly the same value in $O(\\text{poly}(\\log n,\\frac{1}{\\varepsilon}))$ update time.","We give show that the guarantees of this algorithm hold for general graphs as well.","Additionally, we show useful properties of this subroutine which have applications in rounding weighted fractional matchings."],"url":"http://arxiv.org/abs/2402.03068v1","category":"cs.DS"}
{"created":"2024-02-05 14:59:29","title":"Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian","abstract":"This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this paper can be significant for re-searchers working with other morphologically rich low-resource languages and short text.","sentences":["This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language.","We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian.","We also compared it to LDA and NMF on fully preprocessed text.","The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination.","Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text.","When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal.","Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited.","The findings of this paper can be significant for re-searchers working with other morphologically rich low-resource languages and short text."],"url":"http://arxiv.org/abs/2402.03067v1","category":"cs.CL"}
{"created":"2024-02-05 14:58:46","title":"Generalized knots-quivers correspondence","abstract":"We propose a generalized version of knots-quivers correspondence, where the quiver series variables specialize to arbitrary powers of the knot HOMFLY-PT polynomial series variable. We explicitely compute quivers for large classes of knots, as well as many homologically thick 9- and 10-crossings knots, including the ones with the super-exponential growth property of colored HOMFLY-PT polyomials. In addition, we propose a new, compact, quiver-like form for the colored HOMFLY-PT polynomials, where the structure of colored differentials is manifest. In particular, this form partially explains the non-uniqueness of quivers corresponding to a given knot via knots-quivers correspondence.","sentences":["We propose a generalized version of knots-quivers correspondence, where the quiver series variables specialize to arbitrary powers of the knot HOMFLY-PT polynomial series variable.","We explicitely compute quivers for large classes of knots, as well as many homologically thick","9-","and","10-crossings knots, including the ones with the super-exponential growth property of colored HOMFLY-PT polyomials.","In addition, we propose a new, compact, quiver-like form for the colored HOMFLY-PT polynomials, where the structure of colored differentials is manifest.","In particular, this form partially explains the non-uniqueness of quivers corresponding to a given knot via knots-quivers correspondence."],"url":"http://arxiv.org/abs/2402.03066v1","category":"math.QA"}
{"created":"2024-02-05 14:37:23","title":"The footprint of nuclear saturation properties on the neutron star $f$ mode oscillation frequencies: a machine learning approach","abstract":"We investigate the intricate relationships between the non-radial \\(f\\) mode oscillation frequencies of neutron stars (NS)s and the corresponding nuclear matter equation of state (EOS) using a machine learning (ML) approach within the ambit of the relativistic mean field (RMF) framework for nuclear matter. With two distinct parameterizations of the Walecka model, namely, (1) with non-linear self interactions of the scalar field (NL) and, (2) a density dependent Bayesian model (DDB), we perform a thorough examination of the \\(f\\) mode frequency in relation to various nuclear saturation properties. The correlations between the \\(f\\) mode frequencies and nuclear saturation properties reveal, through various analytical and ML methods, the complex nature of NSs and their potential as the cosmic laboratory for studying extreme states of matter. A principal component analysis (PCA) has been performed using mixed datasets from DDB and NL models to discriminate the relative importance of the different components of the EOS on the $f$ mode frequencies. Additionally, a {\\it Random forest feature importance} analysis also elucidates the distinct roles of these properties in determining the \\(f\\) mode frequency across a spectrum of NS masses. Our findings are further supported by symbolic regression searches, yielding high-accuracy relations with strong Pearson coefficients and minimal errors. These relations suggest new methodologies for probing NS core characteristics, such as energy density, pressure, and speed of sound from observations of non-radial \\(f\\) mode oscillations of NSs.","sentences":["We investigate the intricate relationships between the non-radial \\(f\\) mode oscillation frequencies of neutron stars (NS)s and the corresponding nuclear matter equation of state (EOS) using a machine learning (ML) approach within the ambit of the relativistic mean field (RMF) framework for nuclear matter.","With two distinct parameterizations of the Walecka model, namely, (1) with non-linear self interactions of the scalar field (NL) and, (2) a density dependent Bayesian model (DDB), we perform a thorough examination of the \\(f\\) mode frequency in relation to various nuclear saturation properties.","The correlations between the \\(f\\) mode frequencies and nuclear saturation properties reveal, through various analytical and ML methods, the complex nature of NSs and their potential as the cosmic laboratory for studying extreme states of matter.","A principal component analysis (PCA) has been performed using mixed datasets from DDB and NL models to discriminate the relative importance of the different components of the EOS on the $f$ mode frequencies.","Additionally, a {\\it Random forest feature importance} analysis also elucidates the distinct roles of these properties in determining the \\(f\\) mode frequency across a spectrum of NS masses.","Our findings are further supported by symbolic regression searches, yielding high-accuracy relations with strong Pearson coefficients and minimal errors.","These relations suggest new methodologies for probing NS core characteristics, such as energy density, pressure, and speed of sound from observations of non-radial \\(f\\) mode oscillations of NSs."],"url":"http://arxiv.org/abs/2402.03054v1","category":"nucl-th"}
{"created":"2024-02-05 14:36:51","title":"Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations","abstract":"In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the \"Melayu\" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All models released at https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99","sentences":["In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs.","We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   ","For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   ","In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context.","Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the \"Melayu\" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   ","These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   ","All models released at https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99"],"url":"http://arxiv.org/abs/2402.03053v1","category":"cs.CL"}
{"created":"2024-02-05 14:35:01","title":"New modalities of cortical electrophysiology, perspectives in medical research and human physiology","abstract":"Recent advances in material technology and in micro- and nano-electronics have profoundly changed the design of intracranial electrophysiology electrodes. It is now possible to manufacture electrodes that record cortical activity at a spatial resolution that was previously unthinkable. This high spatial resolution enables recording of the functional structures of the brain, and differentiation of the activity of the different types of neurons composing them. In this paper, we present a review of the different types of electrodes now available, and then suggest one of the first applications for such high resolution electrodes, namely a means to better characterise the mechanisms that generate focal seizures in epileptics. Finally, we reflect more broadly on prospects for their future use.","sentences":["Recent advances in material technology and in micro- and nano-electronics have profoundly changed the design of intracranial electrophysiology electrodes.","It is now possible to manufacture electrodes that record cortical activity at a spatial resolution that was previously unthinkable.","This high spatial resolution enables recording of the functional structures of the brain, and differentiation of the activity of the different types of neurons composing them.","In this paper, we present a review of the different types of electrodes now available, and then suggest one of the first applications for such high resolution electrodes, namely a means to better characterise the mechanisms that generate focal seizures in epileptics.","Finally, we reflect more broadly on prospects for their future use."],"url":"http://arxiv.org/abs/2402.03051v1","category":"q-bio.NC"}
{"created":"2024-02-05 14:33:56","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models","abstract":"In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with a running demo App at https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for broader research centered on instruction data.","sentences":["In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs).","To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality.","Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing.","To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction.","EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with a running demo App at https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for broader research centered on instruction data."],"url":"http://arxiv.org/abs/2402.03049v1","category":"cs.CL"}
{"created":"2024-02-05 14:32:00","title":"Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning","abstract":"In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, Open RL Benchmark comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of Open RL Benchmark in practice. To the best of our knowledge, Open RL Benchmark is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.","sentences":["In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms.","However, the complete raw data of the learning curves are rarely available.","As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone.","We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics.","Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data.","At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years.","Open RL Benchmark covers a wide range of RL libraries and reference implementations.","Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it.","In addition, Open RL Benchmark comes with a command-line interface (CLI) for easy fetching and generating figures to present the results.","In this document, we include two case studies to demonstrate the usefulness of Open RL Benchmark in practice.","To the best of our knowledge, Open RL Benchmark is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field."],"url":"http://arxiv.org/abs/2402.03046v1","category":"cs.LG"}
{"created":"2024-02-05 14:31:19","title":"Elementary processes in dilatational plasticity of glasses","abstract":"Materials typically fail under complex stress states, essentially involving dilatational (volumetric) components that eventually lead to material decohesion/separation. It is therefore important to understand dilatational irreversible deformation -- i.e., dilatational plasticity -- en route to failure. In the context of glasses, much focus has been given to shear (volume-preserving) plasticity, both in terms of the stress states considered and the corresponding material response. Here, using a recently-developed methodology and extensive computer simulations, we shed basic light on the elementary processes mediating dilatational plasticity in glasses. We show that plastic instabilities, corresponding to singularities of the glass Hessian, generically feature both dilatational and shear irreversible strain components. The relative magnitude and statistics of the strain components depend both on the symmetry of the driving stress (e.g., shear vs.~hydrostatic tension) and on the cohesive (attractive) part of the interatomic interaction. We further show that the tensorial shear component of the plastic strain is generally non-planar and also extract the characteristic volume of plastic instabilities. Elucidating the fundamental properties of the elementary micro-mechanical building blocks of plasticity in glasses sets the stage for addressing larger-scale, collective phenomena in dilatational plasticity such as topological changes in the form of cavitation and ductile-to-brittle transitions. As a first step in this direction, we show that the elastic moduli markedly soften during dilatational plastic deformation approaching cavitation.","sentences":["Materials typically fail under complex stress states, essentially involving dilatational (volumetric) components that eventually lead to material decohesion/separation.","It is therefore important to understand dilatational irreversible deformation -- i.e., dilatational plasticity -- en route to failure.","In the context of glasses, much focus has been given to shear (volume-preserving) plasticity, both in terms of the stress states considered and the corresponding material response.","Here, using a recently-developed methodology and extensive computer simulations, we shed basic light on the elementary processes mediating dilatational plasticity in glasses.","We show that plastic instabilities, corresponding to singularities of the glass Hessian, generically feature both dilatational and shear irreversible strain components.","The relative magnitude and statistics of the strain components depend both on the symmetry of the driving stress (e.g., shear vs.~hydrostatic tension) and on the cohesive (attractive) part of the interatomic interaction.","We further show that the tensorial shear component of the plastic strain is generally non-planar and also extract the characteristic volume of plastic instabilities.","Elucidating the fundamental properties of the elementary micro-mechanical building blocks of plasticity in glasses sets the stage for addressing larger-scale, collective phenomena in dilatational plasticity such as topological changes in the form of cavitation and ductile-to-brittle transitions.","As a first step in this direction, we show that the elastic moduli markedly soften during dilatational plastic deformation approaching cavitation."],"url":"http://arxiv.org/abs/2402.03044v1","category":"cond-mat.soft"}
{"created":"2024-02-05 14:29:54","title":"SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach","abstract":"Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT across various experiments. We find that, in sentiment analysis task of a movie review dataset, SIDU-TXT excels in both functionally and human-grounded evaluations, demonstrating superior performance through quantitative and qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the application-grounded evaluation within the sensitive and complex legal domain of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable performances, each with its own set of strengths and weaknesses. However, both methods fall short of entirely fulfilling the sophisticated criteria of expert expectations, highlighting the imperative need for additional research in XAI methods suitable for such domains.","sentences":["Explainable AI (XAI) aids in deciphering 'black-box' models.","While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area.","In this paper, we delve into the applicability of XAI methods for the text domain.","In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data.","The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions.","Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT across various experiments.","We find that, in sentiment analysis task of a movie review dataset, SIDU-TXT excels in both functionally and human-grounded evaluations, demonstrating superior performance through quantitative and qualitative analyses compared to benchmarks like Grad-CAM and LIME.","In the application-grounded evaluation within the sensitive and complex legal domain of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable performances, each with its own set of strengths and weaknesses.","However, both methods fall short of entirely fulfilling the sophisticated criteria of expert expectations, highlighting the imperative need for additional research in XAI methods suitable for such domains."],"url":"http://arxiv.org/abs/2402.03043v1","category":"cs.CL"}
{"created":"2024-02-05 14:29:05","title":"Semi-Passive Intelligent Reflecting Surface Enabled Sensing Systems","abstract":"Intelligent reflecting surface (IRS) has garnered growing interest and attention due to its potential for facilitating and supporting wireless communications and sensing. This paper studies a semi-passive IRS-enabled sensing system, where an IRS consists of both passive reflecting elements and active sensors. Our goal is to minimize the Cram\\'{e}r-Rao bound (CRB) for parameter estimation under both point and extended target cases. Towards this goal, we begin by deriving the CRB for the direction-of-arrival (DoA) estimation in closed-form and then theoretically analyze the IRS reflecting elements and sensors allocation design based on the CRB under the point target case with a single-antenna base station (BS). To efficiently solve the corresponding optimization problem for the case with a multi-antenna BS, we propose an efficient algorithm by jointly optimizing the IRS phase shifts and the BS beamformers. Under the extended target case, the CRB for the target response matrix (TRM) estimation is minimized via the optimization of the BS transmit beamformers. Moreover, we explore the influence of various system parameters on the CRB and compare these effects to those observed under the point target case. Simulation results show the effectiveness of the semi-passive IRS and our proposed beamforming design for improving the performance of the sensing system.","sentences":["Intelligent reflecting surface (IRS) has garnered growing interest and attention due to its potential for facilitating and supporting wireless communications and sensing.","This paper studies a semi-passive IRS-enabled sensing system, where an IRS consists of both passive reflecting elements and active sensors.","Our goal is to minimize the Cram\\'{e}r-Rao bound (CRB) for parameter estimation under both point and extended target cases.","Towards this goal, we begin by deriving the CRB for the direction-of-arrival (DoA) estimation in closed-form and then theoretically analyze the IRS reflecting elements and sensors allocation design based on the CRB under the point target case with a single-antenna base station (BS).","To efficiently solve the corresponding optimization problem for the case with a multi-antenna BS, we propose an efficient algorithm by jointly optimizing the IRS phase shifts and the BS beamformers.","Under the extended target case, the CRB for the target response matrix (TRM) estimation is minimized via the optimization of the BS transmit beamformers.","Moreover, we explore the influence of various system parameters on the CRB and compare these effects to those observed under the point target case.","Simulation results show the effectiveness of the semi-passive IRS and our proposed beamforming design for improving the performance of the sensing system."],"url":"http://arxiv.org/abs/2402.03042v1","category":"eess.SP"}
{"created":"2024-02-05 14:24:46","title":"InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions","abstract":"We introduce $\\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met. Code, models, and demo are available at https://github.com/invictus717/InteractiveVideo","sentences":["We introduce $\\textit{InteractiveVideo}$, a user-centric framework for video generation.","Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc.","We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process.","This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions.","With $\\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video.","They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met.","Code, models, and demo are available at https://github.com/invictus717/InteractiveVideo"],"url":"http://arxiv.org/abs/2402.03040v1","category":"cs.CV"}
{"created":"2024-02-05 14:23:43","title":"Automatic Combination of Sample Selection Strategies for Few-Shot Learning","abstract":"In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a strong modality, dataset and approach dependence for the majority of strategies as well as their dependence on the number of shots - demonstrating that the sample selection strategies play a significant role for lower number of shots, but regresses to random selection at higher number of shots.","sentences":["In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success.","Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only.","In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets.","In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies.","The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning.","We also show a strong modality, dataset and approach dependence for the majority of strategies as well as their dependence on the number of shots - demonstrating that the sample selection strategies play a significant role for lower number of shots, but regresses to random selection at higher number of shots."],"url":"http://arxiv.org/abs/2402.03038v1","category":"cs.LG"}
{"created":"2024-02-05 14:22:25","title":"An Investigation of the Compressed Sensing Phase in Unsourced Multiple Access","abstract":"A vast population of low-cost low-power transmitters sporadically sending small amounts of data over a common wireless medium is one of the main scenarios for Internet of things (IoT) data communications. At the medium access, the use of grant-free solutions may be preferred to reduce overhead even at the cost of multiple-access interference. Unsourced multiple access (UMA) has been recently established as relevant framework for energy efficient grant-free protocols. The use of a compressed sensing (CS) transmission phase is key in one of the two main classes of UMA protocols, yet little attention has been posed to sparse greedy algorithms as orthogonal matching pursuit (OMP) and its variants. We analyze their performance and provide relevant guidance on how to optimally setup the CS phase. Minimum average transmission power and minimum number of channel uses are investigated together with the performance in terms of receiver operating characteristic (ROC). Interestingly, we show how the basic OMP and generalized OMP (gOMP) are the most competitive algorithms in their class.","sentences":["A vast population of low-cost low-power transmitters sporadically sending small amounts of data over a common wireless medium is one of the main scenarios for Internet of things (IoT) data communications.","At the medium access, the use of grant-free solutions may be preferred to reduce overhead even at the cost of multiple-access interference.","Unsourced multiple access (UMA) has been recently established as relevant framework for energy efficient grant-free protocols.","The use of a compressed sensing (CS) transmission phase is key in one of the two main classes of UMA protocols, yet little attention has been posed to sparse greedy algorithms as orthogonal matching pursuit (OMP) and its variants.","We analyze their performance and provide relevant guidance on how to optimally setup the CS phase.","Minimum average transmission power and minimum number of channel uses are investigated together with the performance in terms of receiver operating characteristic (ROC).","Interestingly, we show how the basic OMP and generalized OMP (gOMP) are the most competitive algorithms in their class."],"url":"http://arxiv.org/abs/2402.03037v1","category":"cs.IT"}
{"created":"2024-02-05 14:21:17","title":"Modelling nanomagnet vertex dynamics through Coulomb charges","abstract":"We investigate the magnetization dynamics in nanomagnet vertices often found in artificial spin ices. Our analysis involves creating a simplified model that depicts edge magnetization using magnetic charges. We utilize the model to explore the energy landscape, its associated curvatures, and the fundamental modes. Our study uncovers specific magnonic regimes and transitions between magnetization states, marked by zero-modes, which can be understood within the framework of Landau theory. To verify our model, we compare it with micromagnetic simulations, demonstrating a noteworthy agreement.","sentences":["We investigate the magnetization dynamics in nanomagnet vertices often found in artificial spin ices.","Our analysis involves creating a simplified model that depicts edge magnetization using magnetic charges.","We utilize the model to explore the energy landscape, its associated curvatures, and the fundamental modes.","Our study uncovers specific magnonic regimes and transitions between magnetization states, marked by zero-modes, which can be understood within the framework of Landau theory.","To verify our model, we compare it with micromagnetic simulations, demonstrating a noteworthy agreement."],"url":"http://arxiv.org/abs/2402.03036v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 14:17:47","title":"Understanding voltage-controlled magnetic anisotropy effect for the manipulation of dipolar-dominated propagating spin waves","abstract":"Spin waves, known for their ability to propagate without the involvement of moving charges, hold immense promise for on-chip information transfer and processing, offering a path toward post-CMOS computing technologies. This study investigates the potential synergy between propagating Damon-Eshbach spin waves and voltage-controlled magnetization in the pursuit of environmentally sustainable computing solutions. Employing micromagnetic simulations, we assess the feasibility of utilizing spin waves in DE mode in conjunction with localized voltage-induced alterations in surface anisotropy to enable low-energy logic operations. Our findings underscore the critical importance of selecting an optimal excitation frequency and gate width, which significantly influence the efficiency of the phase shift induced in propagating spin waves. Notably, we demonstrate that a realistic phase shift of 2.5$\\left[ \\pi \\ \\text{mrad}\\right]$ can be achieved at a Co(5nm)/MgO material system via the VCMA effect. Moreover, by tuning the excitation frequency, Co layer thickness, gate width, and the use of a GdO\\textsubscript{x} dielectric, we illustrate the potential to enhance the phase shift by a factor of 200 when compared to MgO dielectrics. This research contributes valuable insights towards developing next-generation computing technologies with reduced energy consumption.","sentences":["Spin waves, known for their ability to propagate without the involvement of moving charges, hold immense promise for on-chip information transfer and processing, offering a path toward post-CMOS computing technologies.","This study investigates the potential synergy between propagating Damon-Eshbach spin waves and voltage-controlled magnetization in the pursuit of environmentally sustainable computing solutions.","Employing micromagnetic simulations, we assess the feasibility of utilizing spin waves in DE mode in conjunction with localized voltage-induced alterations in surface anisotropy to enable low-energy logic operations.","Our findings underscore the critical importance of selecting an optimal excitation frequency and gate width, which significantly influence the efficiency of the phase shift induced in propagating spin waves.","Notably, we demonstrate that a realistic phase shift of 2.5$\\left[ \\pi \\ \\text{mrad}\\right]$ can be achieved at a Co(5nm)/MgO material system via the VCMA effect.","Moreover, by tuning the excitation frequency, Co layer thickness, gate width, and the use of a GdO\\textsubscript{x} dielectric, we illustrate the potential to enhance the phase shift by a factor of 200 when compared to MgO dielectrics.","This research contributes valuable insights towards developing next-generation computing technologies with reduced energy consumption."],"url":"http://arxiv.org/abs/2402.03033v1","category":"physics.app-ph"}
{"created":"2024-02-05 14:12:36","title":"A new generalized inverse for rectangular matrices. A general approach","abstract":"Rao and Mitra in 1972 introduced two different types of constraints to extend the concept of Bott-Duffin inverse and defined a new constrained inverse. Mary in 2011 defined the inverse along an element that generalizes the Moore-Penrose and Drazin inverses in a semigroup. Drazin in 2012 introduced the $(b,c)$-inverse generalizing the Mary inverse. In 2017, Raki\\'c noted that the Rao-Mitra inverse is a direct precursor of the $(b,c)$-inverse. In this paper, we introduce the notion of $EF$-inverse as a unified approach to the aforementioned generalized inverses. Moreover, we show that the recently introduced generalized bilateral inverses that in turn contain the OMP, MPO, and MPOMP inverses can also be considered as special cases of the $EF$-inverse.","sentences":["Rao and Mitra in 1972 introduced two different types of constraints to extend the concept of Bott-Duffin inverse and defined a new constrained inverse.","Mary in 2011 defined the inverse along an element that generalizes the Moore-Penrose and Drazin inverses in a semigroup.","Drazin in 2012 introduced the $(b,c)$-inverse generalizing the Mary inverse.","In 2017, Raki\\'c noted that the Rao-Mitra inverse is a direct precursor of the $(b,c)$-inverse.","In this paper, we introduce the notion of $EF$-inverse as a unified approach to the aforementioned generalized inverses.","Moreover, we show that the recently introduced generalized bilateral inverses that in turn contain the OMP, MPO, and MPOMP inverses can also be considered as special cases of the $EF$-inverse."],"url":"http://arxiv.org/abs/2402.03029v1","category":"math.RA"}
{"created":"2024-02-05 14:06:15","title":"Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation","abstract":"Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, incorporating this operator to improve the accuracy of every type of aggregation-based model without altering the learning process. Extensive experiments substantiate our theoretical findings and demonstrate PipEA's significant performance gains over state-of-the-art weakly supervised EA methods. Our work not only advances the field but also enhances our comprehension of aggregation-based weakly supervised EA.","sentences":["Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments.","Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored.","In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models.","Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities.","We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated.","Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs.","We develop a general EA framework, PipEA, incorporating this operator to improve the accuracy of every type of aggregation-based model without altering the learning process.","Extensive experiments substantiate our theoretical findings and demonstrate PipEA's significant performance gains over state-of-the-art weakly supervised EA methods.","Our work not only advances the field but also enhances our comprehension of aggregation-based weakly supervised EA."],"url":"http://arxiv.org/abs/2402.03025v1","category":"cs.IR"}
{"created":"2024-02-05 14:05:48","title":"Improved analysis of isovector nucleon matrix elements with $N_f=2+1$ flavors of $\\mathcal{O}(a)$ improved Wilson fermions","abstract":"We present an update of our determination of the isovector charges $g_A^{u-d}$, $g_S^{u-d}$ and $g_T^{u-d}$, and the isovector twist-2 forward matrix elements $\\langle x\\rangle_{u-d}$, $\\langle x\\rangle_{\\Delta u-\\Delta d}$ and $\\langle x\\rangle_{\\delta u-\\delta d}$ on the $N_\\mathrm{f}=2+1$ gauge ensembles generated by the Coordinated Lattice Simulations (CLS) effort. We have significantly extended our coverage of the parameter space by adding ensembles at the physical pion mass and fine lattice spacing, at nearly-physical pion masses and very fine lattice spacings, and at very large physical lattice volumes, enabling a well-controlled extrapolation to the physical point. Another major improvement is achieved owing to the extended range of source-sink separations, which allows us to perform two-state fits to summed correlator ratios, leading to a much higher level of control over excited-state effects. Systematic uncertainties from the chiral, continuum and infinite-volume extrapolations are incorporated via model averages based on the Akaike Information Criterion. Our final results at the physical point are $g_A^{u-d} = 1.254(19)_\\mathrm{stat}(15)_\\mathrm{sys}[24]_\\mathrm{total}$, $g_S^{u-d} = 1.203(77)_\\mathrm{stat}(81)_\\mathrm{sys}[112]_\\mathrm{total}$, $g_T^{u-d} = 0.993(15)_\\mathrm{stat}(05)_\\mathrm{sys}[16]_\\mathrm{total}$, $\\langle x\\rangle_{u-d} = 0.153(15)_\\mathrm{stat}(10)_\\mathrm{sys}[17]_\\mathrm{total}$, $\\langle x\\rangle_{\\Delta u - \\Delta d} = 0.207(15)_\\mathrm{stat}(06)_\\mathrm{sys}[16]_\\mathrm{total}$, and $\\langle x\\rangle_{\\delta u - \\delta d} = 0.195(17)_\\mathrm{stat}(15)_\\mathrm{sys}[23]_\\mathrm{total}$. While our results for the isovector charges are in excellent agreement with the FLAG\\,21 averages, we note that our error for the tensor charge $g_T^{u-d}$ is considerably smaller.","sentences":["We present an update of our determination of the isovector charges $g_A^{u-d}$, $g_S^{u-d}$ and $g_T^{u-d}$, and the isovector twist-2 forward matrix elements $\\langle x\\rangle_{u-d}$, $\\langle x\\rangle_{\\Delta u-\\Delta d}$ and $\\langle x\\rangle_{\\delta u-\\delta d}$ on the $N_\\mathrm{f}=2+1$ gauge ensembles generated by the Coordinated Lattice Simulations (CLS) effort.","We have significantly extended our coverage of the parameter space by adding ensembles at the physical pion mass and fine lattice spacing, at nearly-physical pion masses and very fine lattice spacings, and at very large physical lattice volumes, enabling a well-controlled extrapolation to the physical point.","Another major improvement is achieved owing to the extended range of source-sink separations, which allows us to perform two-state fits to summed correlator ratios, leading to a much higher level of control over excited-state effects.","Systematic uncertainties from the chiral, continuum and infinite-volume extrapolations are incorporated via model averages based on the Akaike Information Criterion.","Our final results at the physical point are $g_A^{u-d} = 1.254(19)_\\mathrm{stat}(15)_\\mathrm{sys}[24]_\\mathrm{total}$, $g_S^{u-d} = 1.203(77)_\\mathrm{stat}(81)_\\mathrm{sys}[112]_\\mathrm{total}$, $g_T^{u-d} = 0.993(15)_\\mathrm{stat}(05)_\\mathrm{sys}[16]_\\mathrm{total}$, $\\langle x\\rangle_{u-d} = 0.153(15)_\\mathrm{stat}(10)_\\mathrm{sys}[17]_\\mathrm{total}$, $\\langle x\\rangle_{\\Delta u - \\Delta d} = 0.207(15)_\\mathrm{stat}(06)_\\mathrm{sys}[16]_\\mathrm{total}$, and $\\langle x\\rangle_{\\delta u - \\delta d} = 0.195(17)_\\mathrm{stat}(15)_\\mathrm{sys}[23]_\\mathrm{total}$.","While our results for the isovector charges are in excellent agreement with the FLAG\\,21 averages, we note that our error for the tensor charge $g_T^{u-d}$ is considerably smaller."],"url":"http://arxiv.org/abs/2402.03024v1","category":"hep-lat"}
{"created":"2024-02-05 13:55:54","title":"Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches","abstract":"Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.","sentences":["Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications.","Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years.","This survey provides a comprehensive overview of the field's latest advancements.","Initially, FSL is formally defined, and its relationship with different learning fields is presented.","A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described.","Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed."],"url":"http://arxiv.org/abs/2402.03017v1","category":"cs.LG"}
{"created":"2024-02-05 13:55:23","title":"Robust Angle Finding for Generalized Quantum Signal Processing","abstract":"Quantum Signal Processing (QSP), together with the quantum singular value transformation, is one of the central quantum algorithms due to its efficiency and generality in many fields including quantum simulation, quantum machine learning, and quantum cryptography. The largest bottleneck of QSP and its family is its difficulty in finding the phase angle sequence for signal processing. We find that this is in particular prominent when one employs the generalized formalism of the QSP, or the GQSP, to employ arbitrary single-qubit unitaries for signal processing operator. In this work, we extend the framework of GQSP and propose a robust angle finding algorithm. The proposed angle finding algorithm, based on Prony's method, successfully generates angle sequence of precision $10^{-13}$ up to polynomial degrees of hundreds within a second. By applying our method to Hamiltonian simulation, we find that the number of calls, or queries, to signal operators are essentially halved compared to the ordinary framework of QSP.","sentences":["Quantum Signal Processing (QSP), together with the quantum singular value transformation, is one of the central quantum algorithms due to its efficiency and generality in many fields including quantum simulation, quantum machine learning, and quantum cryptography.","The largest bottleneck of QSP and its family is its difficulty in finding the phase angle sequence for signal processing.","We find that this is in particular prominent when one employs the generalized formalism of the QSP, or the GQSP, to employ arbitrary single-qubit unitaries for signal processing operator.","In this work, we extend the framework of GQSP and propose a robust angle finding algorithm.","The proposed angle finding algorithm, based on Prony's method, successfully generates angle sequence of precision $10^{-13}$ up to polynomial degrees of hundreds within a second.","By applying our method to Hamiltonian simulation, we find that the number of calls, or queries, to signal operators are essentially halved compared to the ordinary framework of QSP."],"url":"http://arxiv.org/abs/2402.03016v1","category":"quant-ph"}
{"created":"2024-02-05 13:52:56","title":"Whom to Trust? Elective Learning for Distributed Gaussian Process Regression","abstract":"This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.","sentences":["This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs).","The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness.","The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect.","Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP.","Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications."],"url":"http://arxiv.org/abs/2402.03014v1","category":"cs.LG"}
{"created":"2024-02-05 13:47:55","title":"Homotopy equivalences and Grothendieck duality over rings with finite Gorenstein weak global dimension","abstract":"Let $R$ be a ring with Gwgldim$(R)<\\infty$. We obtain a triangle-equivalence $\\mathrm{K}(R\\text{-}\\mathrm{GProj})\\simeq \\mathrm{K}(R\\text{-}\\mathrm{GInj})$ which restricts to a triangle-equivalence $\\mathrm{K}(R\\text{-}\\mathrm{Proj})$ $\\simeq \\mathrm{K}(R\\text{-}\\mathrm{Inj})$. This class of rings includes, among others, (left) Gorenstein rings, Ding-Chen rings and the more general Gorenstein $n$-coherent rings ($n\\in \\mathbb{N}\\cup \\{\\infty\\}, n\\geq 2$). As application, we establish some triangle-equivalences of Grothendieck duality over Ding-Chen rings and Gorenstein $n$-coherent rings.","sentences":["Let $R$ be a ring with Gwgldim$(R)<\\infty$. We obtain a triangle-equivalence $\\mathrm{K}(R\\text{-}\\mathrm{GProj})\\simeq \\mathrm{K}(R\\text{-}\\mathrm{GInj})$ which restricts to a triangle-equivalence $\\mathrm{K}(R\\text{-}\\mathrm{Proj})$ $\\simeq \\mathrm{K}(R\\text{-}\\mathrm{Inj})$.","This class of rings includes, among others, (left) Gorenstein rings, Ding-Chen rings and the more general Gorenstein $n$-coherent rings ($n\\in \\mathbb{N}\\cup \\{\\infty\\}, n\\geq 2$).","As application, we establish some triangle-equivalences of Grothendieck duality over Ding-Chen rings and Gorenstein $n$-coherent rings."],"url":"http://arxiv.org/abs/2402.03010v1","category":"math.RA"}
{"created":"2024-02-05 13:47:53","title":"UniMem: Towards a Unified View of Long-Context Large Language Models","abstract":"Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines.","sentences":["Long-context processing is a critical ability that constrains the applicability of large language models.","Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments.","In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs.","UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods.","We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths.","Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms.","Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines."],"url":"http://arxiv.org/abs/2402.03009v1","category":"cs.CL"}
{"created":"2024-02-05 13:40:07","title":"A structure-preserving reconstruction scheme for compressible single- and multi-phase flows based on artificial neural networks","abstract":"In the present study, we introduce an advanced reconstruction indicator, deepMTBVD, which is an evolution of the MUSCL-THINC-BVD algorithm\\cite{RN2, RN12}. This novel indicator is developed by employing a deep learning neural network to train on numerical results derived from 1D Euler equations, thereby enhancing the capability to discern the most suitable reconstruction scheme. In particular, we have designed a multilayer perceptron for offline training, culminating in a simple two-hidden-layer neural network. This network identifies the reconstruction schemes employed within the target cell. Our innovative deepMTBVD indicator stands out due to its problem-independent nature, relying solely on the relative values of neighboring cells. This approach eliminates the need to pre-construct MUSCL and THINC schemes at each time step, thereby reducing the computational effort. Without further modifications, the deepMTBVD seamlessly lends itself to direct application in the compressible multiphase model and straightforward to multi-dimensional cases on Cartesian grids. Numerical results demonstrate that deepMTBVD performs as well as the MUSCL-THINC-BVD scheme in accurately capturing shock waves, contact discontinuities, material interfaces, and vertical solutions with enhanced sharpness and reduced oscillations in smooth regions. Compared with the MUSCL scheme, the deepMTBVD exhibits lower numerical dissipation and superior resolution of complex flow structures, thereby enhancing the overall quality of the solution.","sentences":["In the present study, we introduce an advanced reconstruction indicator, deepMTBVD, which is an evolution of the MUSCL-THINC-BVD algorithm\\cite{RN2, RN12}.","This novel indicator is developed by employing a deep learning neural network to train on numerical results derived from 1D Euler equations, thereby enhancing the capability to discern the most suitable reconstruction scheme.","In particular, we have designed a multilayer perceptron for offline training, culminating in a simple two-hidden-layer neural network.","This network identifies the reconstruction schemes employed within the target cell.","Our innovative deepMTBVD indicator stands out due to its problem-independent nature, relying solely on the relative values of neighboring cells.","This approach eliminates the need to pre-construct MUSCL and THINC schemes at each time step, thereby reducing the computational effort.","Without further modifications, the deepMTBVD seamlessly lends itself to direct application in the compressible multiphase model and straightforward to multi-dimensional cases on Cartesian grids.","Numerical results demonstrate that deepMTBVD performs as well as the MUSCL-THINC-BVD scheme in accurately capturing shock waves, contact discontinuities, material interfaces, and vertical solutions with enhanced sharpness and reduced oscillations in smooth regions.","Compared with the MUSCL scheme, the deepMTBVD exhibits lower numerical dissipation and superior resolution of complex flow structures, thereby enhancing the overall quality of the solution."],"url":"http://arxiv.org/abs/2402.03002v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 13:34:21","title":"Text-Guided Image Clustering","abstract":"Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.","sentences":["Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations.","Those are usually in the form of text, begging the question of using text as an abstraction for image clustering.","Current image clustering methods, however, neglect the use of generated textual descriptions.","We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text.","Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models.","Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features.","Additionally, we propose a counting-based cluster explainability method.","Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests.","Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text."],"url":"http://arxiv.org/abs/2402.02996v1","category":"cs.LG"}
{"created":"2024-02-05 13:31:28","title":"Decoding-time Realignment of Language Models","abstract":"Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.","sentences":["Aligning language models with human preferences is crucial for reducing errors and biases in these models.","Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model.","Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment.","Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths.","This process, however, is resource-intensive, especially for large models.","To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining.","DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models.","It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset."],"url":"http://arxiv.org/abs/2402.02992v1","category":"cs.LG"}
{"created":"2024-02-05 13:28:58","title":"Poisson-Lie analogues of spin Sutherland models revisited","abstract":"We continue our study of such generalizations of spin Sutherland models that descend from `master integrable systems' living on Heisenberg doubles of compact semisimple Lie groups. The master systems represent Poisson--Lie counterparts of the systems of free motion modeled on the respective cotangent bundles and their reduction relies on taking quotient with respect to a suitable conjugation action of the compact Lie group. We present an enhanced exposition of the reductions and prove rigorously for the first time that the reduced systems possess the property of degenerate integrability on the dense open subset of the Poisson quotient space corresponding to the principal orbit type for the pertinent group action. After restriction to a smaller dense open subset, degenerate integrability on the generic symplectic leaves is demonstrated as well. The paper also contains a novel description of the reduced Poisson structure and a careful elaboration of the scaling limit whereby our reduced systems turn into the spin Sutherland models.","sentences":["We continue our study of such generalizations of spin Sutherland models that descend from `master integrable systems' living on Heisenberg doubles of compact semisimple Lie groups.","The master systems represent Poisson--Lie counterparts of the systems of free motion modeled on the respective cotangent bundles and their reduction relies on taking quotient with respect to a suitable conjugation action of the compact Lie group.","We present an enhanced exposition of the reductions and prove rigorously for the first time that the reduced systems possess the property of degenerate integrability on the dense open subset of the Poisson quotient space corresponding to the principal orbit type for the pertinent group action.","After restriction to a smaller dense open subset, degenerate integrability on the generic symplectic leaves is demonstrated as well.","The paper also contains a novel description of the reduced Poisson structure and a careful elaboration of the scaling limit whereby our reduced systems turn into the spin Sutherland models."],"url":"http://arxiv.org/abs/2402.02990v1","category":"math-ph"}
{"created":"2024-02-05 13:27:41","title":"DexDiffuser: Generating Dexterous Grasps with Diffusion Models","abstract":"We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\\% higher grasp success rate.","sentences":["We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds.","DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator.","DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps.","We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).","Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\\% higher grasp success rate."],"url":"http://arxiv.org/abs/2402.02989v1","category":"cs.RO"}
{"created":"2024-02-05 13:16:38","title":"A Safety-Adapted Loss for Pedestrian Detection in Automated Driving","abstract":"In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone.","sentences":["In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU).","As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector.","However, those approaches do not consider the safety factor in the deep neural network (DNN) training process.","Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality.","Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians.","In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training.","We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality.","Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone."],"url":"http://arxiv.org/abs/2402.02986v1","category":"cs.LG"}
{"created":"2024-02-05 13:16:12","title":"Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing","abstract":"Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation. The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation. Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself.","sentences":["Two challenges are presented when parsing road scenes in UAV images.","First, the high resolution of UAV images makes processing difficult.","Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models.","In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.","Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images.","Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information.","Following that, a self-supervised representation learning network extracts feature representations from all masked regions.","Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster.","The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation.","The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation.","Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself."],"url":"http://arxiv.org/abs/2402.02985v1","category":"cs.CV"}
{"created":"2024-02-05 13:12:33","title":"Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin","abstract":"This comprehensive review article delves into the intricate realm of fault-tolerant control (FTC) schemes tailored for robotic manipulators. Our exploration spans the historical evolution of FTC, tracing its development over time, and meticulously examines the recent breakthroughs fueled by the synergistic integration of cutting-edge technologies such as artificial intelligence (AI), machine learning (ML), and digital twin technologies (DTT). The article places a particular emphasis on the transformative influence these contemporary trends exert on the landscape of robotic manipulator control and fault tolerance.   By delving into the historical context, our aim is to provide a comprehensive understanding of the evolution of FTC schemes. This journey encompasses the transition from model-based and signal-based schemes to the role of sensors, setting the stage for an exploration of the present-day paradigm shift enabled by AI, ML, and DTT. The narrative unfolds as we dissect the intricate interplay between these advanced technologies and their applications in enhancing fault tolerance within the domain of robotic manipulators. Our review critically evaluates the impact of these advancements, shedding light on the novel methodologies, techniques, and applications that have emerged in recent times.   The overarching goal of this article is to present a comprehensive perspective on the current state of fault diagnosis and fault-tolerant control within the context of robotic manipulators, positioning our exploration within the broader framework of AI, ML, and DTT advancements. Through a meticulous examination of both historical foundations and contemporary innovations, this review significantly contributes to the existing body of knowledge, offering valuable insights for researchers, practitioners, and enthusiasts navigating the dynamic landscape of robotic manipulator control.","sentences":["This comprehensive review article delves into the intricate realm of fault-tolerant control (FTC) schemes tailored for robotic manipulators.","Our exploration spans the historical evolution of FTC, tracing its development over time, and meticulously examines the recent breakthroughs fueled by the synergistic integration of cutting-edge technologies such as artificial intelligence (AI), machine learning (ML), and digital twin technologies (DTT).","The article places a particular emphasis on the transformative influence these contemporary trends exert on the landscape of robotic manipulator control and fault tolerance.   ","By delving into the historical context, our aim is to provide a comprehensive understanding of the evolution of FTC schemes.","This journey encompasses the transition from model-based and signal-based schemes to the role of sensors, setting the stage for an exploration of the present-day paradigm shift enabled by AI, ML, and DTT.","The narrative unfolds as we dissect the intricate interplay between these advanced technologies and their applications in enhancing fault tolerance within the domain of robotic manipulators.","Our review critically evaluates the impact of these advancements, shedding light on the novel methodologies, techniques, and applications that have emerged in recent times.   ","The overarching goal of this article is to present a comprehensive perspective on the current state of fault diagnosis and fault-tolerant control within the context of robotic manipulators, positioning our exploration within the broader framework of AI, ML, and DTT advancements.","Through a meticulous examination of both historical foundations and contemporary innovations, this review significantly contributes to the existing body of knowledge, offering valuable insights for researchers, practitioners, and enthusiasts navigating the dynamic landscape of robotic manipulator control."],"url":"http://arxiv.org/abs/2402.02980v1","category":"cs.RO"}
{"created":"2024-02-05 13:09:55","title":"Fully Generalized Reactivity(1) Synthesis","abstract":"Generalized Reactivity(1) (GR(1)) synthesis is a reactive synthesis approach in which the specification is split into two parts: a symbolic game graph, describing the safe transitions of a system, a liveness specification in a subset of Linear Temporal Logic (LTL) on top of it. Many specifications can naturally be written in this restricted form, and the restriction gives rise to a scalable synthesis procedure -- the reasons for the high popularity of the approach. For specifications even slightly beyond GR(1), however, the approach is inapplicable. This necessitates a transition to synthesizers for full LTL specifications, introducing a huge efficiency drop. This paper proposes a synthesis approach that smoothly bridges the efficiency gap from GR(1) to LTL by unifying synthesis for both classes of specifications. The approach leverages a recently introduced canonical representation of omega-regular languages based on a chain of good-for-games co-B\\\"uchi automata (COCOA). By constructing COCOA for the liveness part of a specification, we can then build a fixpoint formula that can be efficiently evaluated on the symbolic game graph. The COCOA-based synthesis approach outperforms standard approaches and retains the efficiency of GR(1) synthesis for specifications in GR(1) form and those with few non-GR(1) specification parts.","sentences":["Generalized Reactivity(1) (GR(1))","synthesis is a reactive synthesis approach in which the specification is split into two parts: a symbolic game graph, describing the safe transitions of a system, a liveness specification in a subset of Linear Temporal Logic (LTL) on top of it.","Many specifications can naturally be written in this restricted form, and the restriction gives rise to a scalable synthesis procedure -- the reasons for the high popularity of the approach.","For specifications even slightly beyond GR(1), however, the approach is inapplicable.","This necessitates a transition to synthesizers for full LTL specifications, introducing a huge efficiency drop.","This paper proposes a synthesis approach that smoothly bridges the efficiency gap from GR(1) to LTL by unifying synthesis for both classes of specifications.","The approach leverages a recently introduced canonical representation of omega-regular languages based on a chain of good-for-games co-B\\\"uchi automata (COCOA).","By constructing COCOA for the liveness part of a specification, we can then build a fixpoint formula that can be efficiently evaluated on the symbolic game graph.","The COCOA-based synthesis approach outperforms standard approaches and retains the efficiency of GR(1) synthesis for specifications in GR(1) form and those with few non-GR(1) specification parts."],"url":"http://arxiv.org/abs/2402.02979v1","category":"cs.FL"}
{"created":"2024-02-05 13:06:35","title":"Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL","abstract":"Metamodeling is a general approach to expressing knowledge about classes and properties in an ontology. It is a desirable modeling feature in multiple applications that simplifies the extension and reuse of ontologies. Nevertheless, allowing metamodeling without restrictions is problematic for several reasons, mainly due to undecidability issues. Practical languages, therefore, forbid classes to occur as instances of other classes or treat such occurrences as semantically different objects. Specifically, meta-querying in SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter approach, thereby effectively not supporting meta-queries. However, several extensions enabling different metamodeling features have been proposed over the last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2 QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in Lenzerini et al. (2015) and Lenzerini et al. (2020); Cima et al. (2017). A reduction from OWL 2 QL to Datalog for meta-querying was proposed in Cima et al. (2017). In this paper, we experiment with various logic programming tools that support Datalog querying to determine their suitability as back-ends to MSER query answering. These tools stem from different logic programming paradigms (Prolog, pure Datalog, Answer Set Programming, Hybrid Knowledge Bases). Our work shows that the Datalog approach to MSER querying is practical also for sizeable ontologies with limited resources (time and memory). This paper significantly extends Qureshi & Faber (2021) by a more detailed experimental analysis and more background. Under consideration in Theory and Practice of Logic Programming (TPLP).","sentences":["Metamodeling is a general approach to expressing knowledge about classes and properties in an ontology.","It is a desirable modeling feature in multiple applications that simplifies the extension and reuse of ontologies.","Nevertheless, allowing metamodeling without restrictions is problematic for several reasons, mainly due to undecidability issues.","Practical languages, therefore, forbid classes to occur as instances of other classes or treat such occurrences as semantically different objects.","Specifically, meta-querying in SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter approach, thereby effectively not supporting meta-queries.","However, several extensions enabling different metamodeling features have been proposed over the last decade.","This paper deals with the Metamodeling Semantics (MS) over OWL 2 QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in Lenzerini et al.","(2015) and Lenzerini et al. (2020);","Cima et al. (2017).","A reduction from OWL 2 QL to Datalog for meta-querying was proposed in Cima et al. (2017).","In this paper, we experiment with various logic programming tools that support Datalog querying to determine their suitability as back-ends to MSER query answering.","These tools stem from different logic programming paradigms (Prolog, pure Datalog, Answer Set Programming, Hybrid Knowledge Bases).","Our work shows that the Datalog approach to MSER querying is practical also for sizeable ontologies with limited resources (time and memory).","This paper significantly extends Qureshi & Faber (2021) by a more detailed experimental analysis and more background.","Under consideration in Theory and Practice of Logic Programming (TPLP)."],"url":"http://arxiv.org/abs/2402.02978v1","category":"cs.AI"}
{"created":"2024-02-05 12:58:29","title":"Variational Flow Models: Flowing in Your Style","abstract":"We introduce a variational inference interpretation for models of \"posterior flows\" - generalizations of \"probability flows\" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as \"Variational Flow Models\". Additionally, we propose a systematic training-free method to transform the posterior flow of a \"linear\" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct \"linear\" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental results substantiate the advantages of our framework.","sentences":["We introduce a variational inference interpretation for models of \"posterior flows\" - generalizations of \"probability flows\" to a broader class of stochastic processes not necessarily diffusion processes.","We coin the resulting models as \"Variational Flow Models\".","Additionally, we propose a systematic training-free method to transform the posterior flow of a \"linear\" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow.","This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow.","The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct \"linear\" stochastic processes.","Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency.","Rigorous theoretical analysis and extensive experimental results substantiate the advantages of our framework."],"url":"http://arxiv.org/abs/2402.02977v1","category":"cs.LG"}
{"created":"2024-02-05 12:58:03","title":"Boosting, Voting Classifiers and Randomized Sample Compression Schemes","abstract":"In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms based on sub-sampling.","sentences":["In boosting, we aim to leverage multiple weak learners to produce a strong learner.","At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners.","While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners.","In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size.","We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms based on sub-sampling."],"url":"http://arxiv.org/abs/2402.02976v1","category":"cs.LG"}
{"created":"2024-02-05 12:50:30","title":"Retrieval-Augmented Score Distillation for Text-to-3D Generation","abstract":"Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.","sentences":["Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry.","Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem.","However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data.","To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream.","We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process.","To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation.","We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes.","We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency.","Project page is available at https://ku-cvlab.github.io/RetDream/."],"url":"http://arxiv.org/abs/2402.02972v1","category":"cs.CV"}
{"created":"2024-02-05 12:47:19","title":"Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features","abstract":"Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to their low WS, random features provably cannot learn to distinguish between two sentences that differ only in a single word; in contrast, due to their high WS, random attention features have higher generalization capabilities. We validate our theoretical results with experimental evidence over the BERT-Base word embeddings of the imdb review dataset.","sentences":["Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks.","In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long.","Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features.","We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map.","The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU).","In contrast, the WS of standard random features is of order $1/\\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context.","We then translate these results on the word sensitivity into generalization bounds: due to their low WS, random features provably cannot learn to distinguish between two sentences that differ only in a single word; in contrast, due to their high WS, random attention features have higher generalization capabilities.","We validate our theoretical results with experimental evidence over the BERT-Base word embeddings of the imdb review dataset."],"url":"http://arxiv.org/abs/2402.02969v1","category":"stat.ML"}
{"created":"2024-02-05 12:47:09","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives","abstract":"Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS","sentences":["Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems.","In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding.","Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene.","In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes.","Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms.","These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability.","Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models.","To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS"],"url":"http://arxiv.org/abs/2402.02968v1","category":"cs.CV"}
{"created":"2024-02-05 12:40:56","title":"Multem 3: An updated and revised version of the program for transmission and band calculations of photonic crystals","abstract":"We present here Multem 3, an updated and revised version of Multem 2, which syntax has been upgraded to Fortran 2018, with the source code being divided into modules. Multem 3 is equipped with LAPACK, the state-of-the art Faddeeva complex error function routine, and the Bessel function package AMOS. The amendments significantly improve both the speed, convergence, and precision of Multem 2. Increased stability allows to freely increase the cut-off value LMAX on the number of spherical vector wave functions and the cut-off value RMAX controlling the maximal length of reciprocal vectors taken into consideration. An immediate bonus is that Multem 3 can be reliably used to describe bound states in the continuum (BICs). To ensure convergence of the layer coupling scheme, it appears that appreciably larger values of convergence paramaters LMAX and RMAX are required than those reported in numerous published work in the past using Multem 2. We hope that Multem 3 will become a reliable and fast alternative to generic commercial software, such as COMSOL Multiphysics, CST Microwave Studio, or Ansys HFSS, and that it will become the code of choice for various optimization tasks for a large number of research groups. The improvements concern the core part of Multem 2, which is common to the extensions of Multem 2 for acoustic and elastic multiple scattering and to the original layer-Kohn-Korringa-Rostocker (LKKR) code. Therefore, the enhancements presented here can be readily applied to the above codes as well.","sentences":["We present here Multem 3, an updated and revised version of Multem 2, which syntax has been upgraded to Fortran 2018, with the source code being divided into modules.","Multem 3 is equipped with LAPACK, the state-of-the art Faddeeva complex error function routine, and the Bessel function package AMOS.","The amendments significantly improve both the speed, convergence, and precision of Multem 2.","Increased stability allows to freely increase the cut-off value LMAX on the number of spherical vector wave functions and the cut-off value RMAX controlling the maximal length of reciprocal vectors taken into consideration.","An immediate bonus is that Multem 3 can be reliably used to describe bound states in the continuum (BICs).","To ensure convergence of the layer coupling scheme, it appears that appreciably larger values of convergence paramaters LMAX and RMAX are required than those reported in numerous published work in the past using Multem 2.","We hope that Multem 3 will become a reliable and fast alternative to generic commercial software, such as COMSOL Multiphysics, CST Microwave Studio, or Ansys HFSS, and that it will become the code of choice for various optimization tasks for a large number of research groups.","The improvements concern the core part of Multem 2, which is common to the extensions of Multem 2 for acoustic and elastic multiple scattering and to the original layer-Kohn-Korringa-Rostocker (LKKR) code.","Therefore, the enhancements presented here can be readily applied to the above codes as well."],"url":"http://arxiv.org/abs/2402.02962v1","category":"physics.comp-ph"}
{"created":"2024-02-05 12:36:08","title":"Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs","abstract":"Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework. In this framework, each UAV acts as an independent agent, aiming to maintain inter UAV cooperative behavior. The proposed approach utilizes the finite state Markov decision process to account for UAVs velocity constraints and the relationship between their trajectories and state space. A low complexity distributed state action reward state action algorithm is presented to determine UAVs optimal sequential decision making policies over training episodes. The extensive simulation results validate the proposed analysis and offer valuable insights into the optimal UAV trajectories. The derived trajectories demonstrate superior average UAV association performance compared to benchmark techniques such as Q learning and particle swarm optimization.","sentences":["Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications.","The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users.","Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points.","This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs.","Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints.","Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework.","In this framework, each UAV acts as an independent agent, aiming to maintain inter UAV cooperative behavior.","The proposed approach utilizes the finite state Markov decision process to account for UAVs velocity constraints and the relationship between their trajectories and state space.","A low complexity distributed state action reward state action algorithm is presented to determine UAVs optimal sequential decision making policies over training episodes.","The extensive simulation results validate the proposed analysis and offer valuable insights into the optimal UAV trajectories.","The derived trajectories demonstrate superior average UAV association performance compared to benchmark techniques such as Q learning and particle swarm optimization."],"url":"http://arxiv.org/abs/2402.02957v1","category":"eess.SY"}
{"created":"2024-02-05 12:34:03","title":"AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image","abstract":"The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains. We also adopt adversarial learning into the framework to further reduce the gap between source and target domains. Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly.","sentences":["The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing.","However, it plays a crucial role in the management of forests.","The huge variety of trees in varied topography severely hinders tree counting models to perform well.","The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees.","Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains.","It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps.","For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains.","We also adopt adversarial learning into the framework to further reduce the gap between source and target domains.","Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly."],"url":"http://arxiv.org/abs/2402.02956v1","category":"cs.CV"}
{"created":"2024-02-05 12:33:28","title":"Global approximate controllability of quantum systems by form perturbations and applications","abstract":"We provide sufficient conditions for the approximate controllability of infinite-dimensional quantum control systems corresponding to form perturbations of the drift Hamiltonian modulated by a control function. We rely on previous results on controllability of quantum bilinear control systems and obtain a priori $L^1$-bounds of the controls for generic initial and target states. We apply a stability result for the non-autonomous Schr\\\"odinger equation to extend the results to systems defined by form perturbations, including singular perturbations. As an application of our results, we prove approximate controllability of a quantum particle in a one-dimensional box with a point-interaction with tuneable strength at the centre of the box.","sentences":["We provide sufficient conditions for the approximate controllability of infinite-dimensional quantum control systems corresponding to form perturbations of the drift Hamiltonian modulated by a control function.","We rely on previous results on controllability of quantum bilinear control systems and obtain a priori $L^1$-bounds of the controls for generic initial and target states.","We apply a stability result for the non-autonomous Schr\\\"odinger equation to extend the results to systems defined by form perturbations, including singular perturbations.","As an application of our results, we prove approximate controllability of a quantum particle in a one-dimensional box with a point-interaction with tuneable strength at the centre of the box."],"url":"http://arxiv.org/abs/2402.02955v1","category":"math.OC"}
{"created":"2024-02-05 12:31:19","title":"Unraveling the Key of Machine Learning Solutions for Android Malware Detection","abstract":"Android malware detection serves as the front line against malicious apps. With the rapid advancement of machine learning (ML), ML-based Android malware detection has attracted increasing attention due to its capability of automatically capturing malicious patterns from Android APKs. These learning-driven methods have reported promising results in detecting malware. However, the absence of an in-depth analysis of current research progress makes it difficult to gain a holistic picture of the state of the art in this area.   This paper presents a comprehensive investigation to date into ML-based Android malware detection with empirical and quantitative analysis. We first survey the literature, categorizing contributions into a taxonomy based on the Android feature engineering and ML modeling pipeline. Then, we design a general-propose framework for ML-based Android malware detection, re-implement 12 representative approaches from different research communities, and evaluate them from three primary dimensions, i.e., effectiveness, robustness, and efficiency. The evaluation reveals that ML-based approaches still face open challenges and provides insightful findings like more powerful ML models are not the silver bullet for designing better malware detectors. We further summarize our findings and put forth recommendations to guide future research.","sentences":["Android malware detection serves as the front line against malicious apps.","With the rapid advancement of machine learning (ML), ML-based Android malware detection has attracted increasing attention due to its capability of automatically capturing malicious patterns from Android APKs.","These learning-driven methods have reported promising results in detecting malware.","However, the absence of an in-depth analysis of current research progress makes it difficult to gain a holistic picture of the state of the art in this area.   ","This paper presents a comprehensive investigation to date into ML-based Android malware detection with empirical and quantitative analysis.","We first survey the literature, categorizing contributions into a taxonomy based on the Android feature engineering and ML modeling pipeline.","Then, we design a general-propose framework for ML-based Android malware detection, re-implement 12 representative approaches from different research communities, and evaluate them from three primary dimensions, i.e., effectiveness, robustness, and efficiency.","The evaluation reveals that ML-based approaches still face open challenges and provides insightful findings like more powerful ML models are not the silver bullet for designing better malware detectors.","We further summarize our findings and put forth recommendations to guide future research."],"url":"http://arxiv.org/abs/2402.02953v1","category":"cs.CR"}
{"created":"2024-02-05 12:31:18","title":"On Least Squares Estimation in Softmax Gating Mixture of Experts","abstract":"Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model. Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze. As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model. In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the rates for estimating strongly identifiable experts, namely the widely used feed forward networks with activation functions $\\mathrm{sigmoid}(\\cdot)$ and $\\tanh(\\cdot)$, are substantially faster than those of polynomial experts, which we show to exhibit a surprising slow estimation rate. Our findings have important practical implications for expert selection.","sentences":["Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model.","Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze.","As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model.","In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored.","We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions.","We demonstrate that the rates for estimating strongly identifiable experts, namely the widely used feed forward networks with activation functions $\\mathrm{sigmoid}(\\cdot)$ and $\\tanh(\\cdot)$, are substantially faster than those of polynomial experts, which we show to exhibit a surprising slow estimation rate.","Our findings have important practical implications for expert selection."],"url":"http://arxiv.org/abs/2402.02952v1","category":"stat.ML"}
{"created":"2024-02-05 12:25:02","title":"Semantic Entropy Can Simultaneously Benefit Transmission Efficiency and Channel Security of Wireless Semantic Communications","abstract":"Recently proliferated deep learning-based semantic communications (DLSC) focus on how transmitted symbols efficiently convey a desired meaning to the destination. However, the sensitivity of neural models and the openness of wireless channels cause the DLSC system to be extremely fragile to various malicious attacks. This inspires us to ask a question: ``Can we further exploit the advantages of transmission efficiency in wireless semantic communications while also alleviating its security disadvantages?''. Keeping this in mind, we propose SemEntropy, a novel method that answers the above question by exploring the semantics of data for both adaptive transmission and physical layer encryption. Specifically, we first introduce semantic entropy, which indicates the expectation of various semantic scores regarding the transmission goal of the DLSC. Equipped with such semantic entropy, we can dynamically assign informative semantics to Orthogonal Frequency Division Multiplexing (OFDM) subcarriers with better channel conditions in a fine-grained manner. We also use the entropy to guide semantic key generation to safeguard communications over open wireless channels. By doing so, both transmission efficiency and channel security can be simultaneously improved. Extensive experiments over various benchmarks show the effectiveness of the proposed SemEntropy. We discuss the reason why our proposed method benefits secure transmission of DLSC, and also give some interesting findings, e.g., SemEntropy can keep the semantic accuracy remain 95\\% with 60\\% less transmission.","sentences":["Recently proliferated deep learning-based semantic communications (DLSC) focus on how transmitted symbols efficiently convey a desired meaning to the destination.","However, the sensitivity of neural models and the openness of wireless channels cause the DLSC system to be extremely fragile to various malicious attacks.","This inspires us to ask a question: ``Can we further exploit the advantages of transmission efficiency in wireless semantic communications while also alleviating its security disadvantages?''.","Keeping this in mind, we propose SemEntropy, a novel method that answers the above question by exploring the semantics of data for both adaptive transmission and physical layer encryption.","Specifically, we first introduce semantic entropy, which indicates the expectation of various semantic scores regarding the transmission goal of the DLSC.","Equipped with such semantic entropy, we can dynamically assign informative semantics to Orthogonal Frequency Division Multiplexing (OFDM) subcarriers with better channel conditions in a fine-grained manner.","We also use the entropy to guide semantic key generation to safeguard communications over open wireless channels.","By doing so, both transmission efficiency and channel security can be simultaneously improved.","Extensive experiments over various benchmarks show the effectiveness of the proposed SemEntropy.","We discuss the reason why our proposed method benefits secure transmission of DLSC, and also give some interesting findings, e.g., SemEntropy can keep the semantic accuracy remain 95\\% with 60\\% less transmission."],"url":"http://arxiv.org/abs/2402.02950v1","category":"cs.CR"}
{"created":"2024-02-05 12:19:49","title":"Superelliptic Affine Lie algebras and orthogonal polynomials","abstract":"We construct two families of orthogonal polynomials associated with the universal central extensions of the superelliptic Lie algebras. These polynomials satisfy certain fourth order linear differential equations, and one of the families is a particular collection of associated ultraspherical polynomials. We show that the generating functions of the polynomials satisfy fourth order linear PDEs. Since these generating function can be represented by superelliptic integrals, we have examples of linear PDEs of fourth order with explicit solutions without complete integrability.","sentences":["We construct two families of orthogonal polynomials associated with the universal central extensions of the superelliptic Lie algebras.","These polynomials satisfy certain fourth order linear differential equations, and one of the families is a particular collection of associated ultraspherical polynomials.","We show that the generating functions of the polynomials satisfy fourth order linear PDEs.","Since these generating function can be represented by superelliptic integrals, we have examples of linear PDEs of fourth order with explicit solutions without complete integrability."],"url":"http://arxiv.org/abs/2402.02947v1","category":"math.RT"}
{"created":"2024-02-05 12:18:26","title":"Stochastic ordering of extreme order statistics in Archimax copula","abstract":"An extension of Archimax copula class in more than two random variables ( Multivariate ) was introduced in (J\\'agr 2011) for describing dependency structures among random variables in higher dimension, and some properties of Archimax copula were explored in (Charpentier et al. 2014). In this article, some results for stochastic ordering of extreme order statistics in (Li and Fang 2015) are generalized and proved in Archimax copula. Stochastic ordering of sample extremes for PHR models is generalized and proved in Archimax copula. Examples with graphical illustrations are also presented.","sentences":["An extension of Archimax copula class in more than two random variables ( Multivariate ) was introduced in (J\\'agr 2011) for describing dependency structures among random variables in higher dimension, and some properties of Archimax copula were explored in (Charpentier et al. 2014).","In this article, some results for stochastic ordering of extreme order statistics in (Li and Fang 2015) are generalized and proved in Archimax copula.","Stochastic ordering of sample extremes for PHR models is generalized and proved in Archimax copula.","Examples with graphical illustrations are also presented."],"url":"http://arxiv.org/abs/2402.02945v1","category":"math.ST"}
{"created":"2024-02-05 11:58:08","title":"Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss","abstract":"Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively.","sentences":["Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting.","However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results.","In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators.","We use the Cubemap Projection (CMP) format as network input.","The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region.","The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM.","Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2402.02936v1","category":"eess.IV"}
{"created":"2024-02-05 11:56:11","title":"Quantum loop corrections in the modified gravity model of Starobinsky inflation with primordial black hole production","abstract":"A modified gravity model of Starobinsky inflation and primordial black hole production was proposed in good (within $1\\sigma$) agreement with current measurements of the cosmic microwave background radiation. The model is an extension of the singularity-free Appleby-Battye-Starobinsky model by the $R^4$-term with different values of the parameters whose fine-tuning leads to efficient production of primordial black holes on smaller scales with the asteroid-size masses between $10^{16}$ g and $10^{20}$ g. Those primordial black holes may be part (or the whole) of the current dark matter, while the proposed model can be confirmed or falsified by detection or absence of the induced gravitational waves with the frequencies about $10^{-2}$ Hz. The main purpose of this paper was to estimate the size of quantum (loop) corrections in the model. It was found their relative contribution to the power spectrum of scalar perturbations is about $10^{-5}$ or less, so that the model is not ruled out by the quantum corrections.","sentences":["A modified gravity model of Starobinsky inflation and primordial black hole production was proposed in good (within $1\\sigma$) agreement with current measurements of the cosmic microwave background radiation.","The model is an extension of the singularity-free Appleby-Battye-Starobinsky model by the $R^4$-term with different values of the parameters whose fine-tuning leads to efficient production of primordial black holes on smaller scales with the asteroid-size masses between $10^{16}$ g and $10^{20}$ g.","Those primordial black holes may be part (or the whole) of the current dark matter, while the proposed model can be confirmed or falsified by detection or absence of the induced gravitational waves with the frequencies about $10^{-2}$ Hz.","The main purpose of this paper was to estimate the size of quantum (loop) corrections in the model.","It was found their relative contribution to the power spectrum of scalar perturbations is about $10^{-5}$ or less, so that the model is not ruled out by the quantum corrections."],"url":"http://arxiv.org/abs/2402.02934v1","category":"gr-qc"}
{"created":"2024-02-05 11:42:19","title":"Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes","abstract":"Most scenes are illuminated by several light sources, where the traditional assumption of uniform illumination is invalid. This issue is ignored in most color constancy methods, primarily due to the complex spatial impact of multiple light sources on the image. Moreover, most existing multi-illuminant methods fail to preserve the smooth change of illumination, which stems from spatial dependencies in natural images. Motivated by this, we propose a novel multi-illuminant color constancy method, by learning pixel-wise illumination maps caused by multiple light sources. The proposed method enforces smoothness within neighboring pixels, by regularizing the training with the total variation loss. Moreover, a bilateral filter is provisioned further to enhance the natural appearance of the estimated images, while preserving the edges. Additionally, we propose a label-smoothing technique that enables the model to generalize well despite the uncertainties in ground truth. Quantitative and qualitative experiments demonstrate that the proposed method outperforms the state-of-the-art.","sentences":["Most scenes are illuminated by several light sources, where the traditional assumption of uniform illumination is invalid.","This issue is ignored in most color constancy methods, primarily due to the complex spatial impact of multiple light sources on the image.","Moreover, most existing multi-illuminant methods fail to preserve the smooth change of illumination, which stems from spatial dependencies in natural images.","Motivated by this, we propose a novel multi-illuminant color constancy method, by learning pixel-wise illumination maps caused by multiple light sources.","The proposed method enforces smoothness within neighboring pixels, by regularizing the training with the total variation loss.","Moreover, a bilateral filter is provisioned further to enhance the natural appearance of the estimated images, while preserving the edges.","Additionally, we propose a label-smoothing technique that enables the model to generalize well despite the uncertainties in ground truth.","Quantitative and qualitative experiments demonstrate that the proposed method outperforms the state-of-the-art."],"url":"http://arxiv.org/abs/2402.02922v1","category":"cs.CV"}
{"created":"2024-02-05 11:41:37","title":"Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation","abstract":"Process mining provides methods to analyse event logs generated by information systems during the execution of processes. It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce. To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process. Existing approaches to behavioral pattern mining, however, suffer from two limitations. First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality. Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant. In this paper, we address these limitations to facilitate the analysis of complex, flexible processes based on behavioral patterns. Specifically, we improve COBPAM, our initial behavioral pattern mining algorithm, by an incremental procedure to evaluate the quality of pattern candidates, optimizing thereby its efficiency. Targeting a more effective use of the resulting patterns, we further propose pruning strategies for redundant patterns and show how relations between the remaining patterns are extracted and visualized to provide process insights. Our experiments with diverse real-world datasets indicate a considerable reduction of the runtime needed for pattern mining, while a qualitative assessment highlights how relations between patterns guide the analysis of the underlying process.","sentences":["Process mining provides methods to analyse event logs generated by information systems during the execution of processes.","It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce.","To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process.","Existing approaches to behavioral pattern mining, however, suffer from two limitations.","First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality.","Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant.","In this paper, we address these limitations to facilitate the analysis of complex, flexible processes based on behavioral patterns.","Specifically, we improve COBPAM, our initial behavioral pattern mining algorithm, by an incremental procedure to evaluate the quality of pattern candidates, optimizing thereby its efficiency.","Targeting a more effective use of the resulting patterns, we further propose pruning strategies for redundant patterns and show how relations between the remaining patterns are extracted and visualized to provide process insights.","Our experiments with diverse real-world datasets indicate a considerable reduction of the runtime needed for pattern mining, while a qualitative assessment highlights how relations between patterns guide the analysis of the underlying process."],"url":"http://arxiv.org/abs/2402.02921v1","category":"cs.DB"}
{"created":"2024-02-05 11:38:33","title":"Attractors in STGR with Boundary Correction","abstract":"We investigate the asymptotic behavior of the cosmological field equations in Symmetric Teleparallel General Relativity, where a nonlinear function of the boundary term is introduced instead of the cosmological constant to describe the acceleration phase of the universe. Our analysis reveals constraints on the free parameters necessary for the existence of an attractor that accurately represents acceleration. However, we also identify asymptotic solutions depicting Big Rip and Big Crunch singularities. To avoid these solutions, we must impose constraints on the phase-space, requiring specific initial conditions.","sentences":["We investigate the asymptotic behavior of the cosmological field equations in Symmetric Teleparallel General Relativity, where a nonlinear function of the boundary term is introduced instead of the cosmological constant to describe the acceleration phase of the universe.","Our analysis reveals constraints on the free parameters necessary for the existence of an attractor that accurately represents acceleration.","However, we also identify asymptotic solutions depicting Big Rip and Big Crunch singularities.","To avoid these solutions, we must impose constraints on the phase-space, requiring specific initial conditions."],"url":"http://arxiv.org/abs/2402.02919v1","category":"gr-qc"}
{"created":"2024-02-05 11:37:47","title":"Entangled multiplets, asymmetry, and quantum Mpemba effect in dissipative systems","abstract":"Recently, the entanglement asymmetry emerged as an informative tool to understand dynamical symmetry restoration in out-of-equilibrium quantum many-body systems after a quantum quench. For integrable systems the asymmetry can be understood in the space-time scaling limit via the quasiparticle picture, as it was pointed out in Ref. [1]. However, a quasiparticle picture for quantum quenches from generic initial states was still lacking. Here we conjecture a full-fledged quasiparticle picture for the charged moments of the reduced density matrix, which are the main ingredients to construct the asymmetry. Our formula works for quenches producing entangled multiplets of an arbitrary number of excitations. We benchmark our results in the $XX$ spin chain. First, by using an elementary approach based on the multidimensional stationary phase approximation we provide an $\\textit{ab initio}$ rigorous derivation of the dynamics of the charged moments for the quench treated in [2]. Then, we show that the same results can be straightforwardly obtained within our quasiparticle picture. As a byproduct of our analysis, we obtain a general criterion ensuring a vanishing entanglement asymmetry at long times. Next, by using the Lindblad master equation, we study the effect of gain and loss dissipation on the entanglement asymmetry. Specifically, we investigate the fate of the so-called quantum Mpemba effect (QME) in the presence of dissipation. We show that dissipation can induce QME even if unitary dynamics that does not show it, and we provide a quasiparticle-based interpretation of the condition for the QME.","sentences":["Recently, the entanglement asymmetry emerged as an informative tool to understand dynamical symmetry restoration in out-of-equilibrium quantum many-body systems after a quantum quench.","For integrable systems the asymmetry can be understood in the space-time scaling limit via the quasiparticle picture, as it was pointed out in Ref.","[1].","However, a quasiparticle picture for quantum quenches from generic initial states was still lacking.","Here we conjecture a full-fledged quasiparticle picture for the charged moments of the reduced density matrix, which are the main ingredients to construct the asymmetry.","Our formula works for quenches producing entangled multiplets of an arbitrary number of excitations.","We benchmark our results in the $XX$ spin chain.","First, by using an elementary approach based on the multidimensional stationary phase approximation we provide an $\\textit{ab initio}$ rigorous derivation of the dynamics of the charged moments for the quench treated in [2].","Then, we show that the same results can be straightforwardly obtained within our quasiparticle picture.","As a byproduct of our analysis, we obtain a general criterion ensuring a vanishing entanglement asymmetry at long times.","Next, by using the Lindblad master equation, we study the effect of gain and loss dissipation on the entanglement asymmetry.","Specifically, we investigate the fate of the so-called quantum Mpemba effect (QME) in the presence of dissipation.","We show that dissipation can induce QME even if unitary dynamics that does not show it, and we provide a quasiparticle-based interpretation of the condition for the QME."],"url":"http://arxiv.org/abs/2402.02918v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 11:32:13","title":"A Computational Model for the Assessment of Mutual Intelligibility Among Closely Related Languages","abstract":"Closely related languages show linguistic similarities that allow speakers of one language to understand speakers of another language without having actively learned it. Mutual intelligibility varies in degree and is typically tested in psycholinguistic experiments. To study mutual intelligibility computationally, we propose a computer-assisted method using the Linear Discriminative Learner, a computational model developed to approximate the cognitive processes by which humans learn languages, which we expand with multilingual semantic vectors and multilingual sound classes. We test the model on cognate data from German, Dutch, and English, three closely related Germanic languages. We find that our model's comprehension accuracy depends on 1) the automatic trimming of inflections and 2) the language pair for which comprehension is tested. Our multilingual modelling approach does not only offer new methodological findings for automatic testing of mutual intelligibility across languages but also extends the use of Linear Discriminative Learning to multilingual settings.","sentences":["Closely related languages show linguistic similarities that allow speakers of one language to understand speakers of another language without having actively learned it.","Mutual intelligibility varies in degree and is typically tested in psycholinguistic experiments.","To study mutual intelligibility computationally, we propose a computer-assisted method using the Linear Discriminative Learner, a computational model developed to approximate the cognitive processes by which humans learn languages, which we expand with multilingual semantic vectors and multilingual sound classes.","We test the model on cognate data from German, Dutch, and English, three closely related Germanic languages.","We find that our model's comprehension accuracy depends on 1) the automatic trimming of inflections and 2) the language pair for which comprehension is tested.","Our multilingual modelling approach does not only offer new methodological findings for automatic testing of mutual intelligibility across languages but also extends the use of Linear Discriminative Learning to multilingual settings."],"url":"http://arxiv.org/abs/2402.02915v1","category":"cs.CL"}
{"created":"2024-02-05 11:30:04","title":"The feasibility of ultra-relativistic bubbles in SMEFT","abstract":"A first order electroweak phase transition probes physics beyond the Standard Model on multiple frontiers and therefore is of immense interest for theoretical exploration. We conduct a model-independent study of the effects of relevant dimension 6 operators, of the Standard Model Effective Field Theory, on electroweak phase transition. We use a thermally corrected and renormalization group improved potential and study its impact on nucleation temperature. We then outline bubble dynamics that lead to ultra-relativistic bubble wall velocities which are mainly motivated from the viewpoint of gravitational wave detection. We highlight the ranges of the Wilson coefficients that give rise to such bubble wall velocities and predict gravitational wave spectra generated by such transitions which can be tested in future experiments.","sentences":["A first order electroweak phase transition probes physics beyond the Standard Model on multiple frontiers and therefore is of immense interest for theoretical exploration.","We conduct a model-independent study of the effects of relevant dimension 6 operators, of the Standard Model Effective Field Theory, on electroweak phase transition.","We use a thermally corrected and renormalization group improved potential and study its impact on nucleation temperature.","We then outline bubble dynamics that lead to ultra-relativistic bubble wall velocities which are mainly motivated from the viewpoint of gravitational wave detection.","We highlight the ranges of the Wilson coefficients that give rise to such bubble wall velocities and predict gravitational wave spectra generated by such transitions which can be tested in future experiments."],"url":"http://arxiv.org/abs/2402.02914v1","category":"hep-ph"}
{"created":"2024-02-05 11:26:32","title":"Inversion points of the accretion flows onto super-spinning Kerr attractors","abstract":"We study the accretion flows towards a central Kerr super-spinning attractor, discussing the formation of the flow inversion points, defined by condition $u^{\\phi}=0$ on the particles flow axial velocity. We locate two closed surfaces, defining \\emph{inversion coronas} (spherical shells), surrounding the central attractor. The coronas analysis highlights observational aspects distinguishing the central attractors and providing indications on their spin and the orbiting fluids.   The inversion corona is a closed region, generally of small extension and thickness, which is for the counter-rotating flows of the order of $\\lesssim 1.4 M$ (central attractor mass) on the vertical rotational axis.   There are no co-rotating inversion points (from co-rotating flows). The results point to strong signatures of the Kerr super-spinars, provided in both accretion and jet flows. With very narrow thickness, and varying little with the fluid initial conditions and the emission process details, inversion coronas can have remarkable observational significance for primordial Kerr super-spinars predicted by string theory. The corona region closest to the central attractor is the most observably recognizable and active part, distinguishing black holes solutions from super-spinars. Our analysis expounds the Lense--Thirring effects and repulsive gravity effects in the super-spinning ergoregions.","sentences":["We study the accretion flows towards a central Kerr super-spinning attractor, discussing the formation of the flow inversion points, defined by condition $u^{\\phi}=0$ on the particles flow axial velocity.","We locate two closed surfaces, defining \\emph{inversion coronas} (spherical shells), surrounding the central attractor.","The coronas analysis highlights observational aspects distinguishing the central attractors and providing indications on their spin and the orbiting fluids.   ","The inversion corona is a closed region, generally of small extension and thickness, which is for the counter-rotating flows of the order of $\\lesssim 1.4 M$ (central attractor mass) on the vertical rotational axis.   ","There are no co-rotating inversion points (from co-rotating flows).","The results point to strong signatures of the Kerr super-spinars, provided in both accretion and jet flows.","With very narrow thickness, and varying little with the fluid initial conditions and the emission process details, inversion coronas can have remarkable observational significance for primordial Kerr super-spinars predicted by string theory.","The corona region closest to the central attractor is the most observably recognizable and active part, distinguishing black holes solutions from super-spinars.","Our analysis expounds the Lense--Thirring effects and repulsive gravity effects in the super-spinning ergoregions."],"url":"http://arxiv.org/abs/2402.02911v1","category":"gr-qc"}
{"created":"2024-02-05 11:25:45","title":"DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network","abstract":"The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete range of exercises (macro labels). The DS-MS-TCN model surpasses existing state-of-the-art deep learning models, achieving f1-scores exceeding 80% and Intersection over Union (IoU) f1-scores surpassing 60% for all four exercises evaluated. Notably, the model outperforms the prior study utilizing the sliding window technique, eliminating the need for post-processing stages and window size tuning. To our knowledge, we are the first to present a novel perspective on enhancing Human Activity Recognition (HAR) systems through the recognition of each repetition of activities.","sentences":["The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength.","Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness.","This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives.","A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments.","The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function.","In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels).","Subsequent stages extend the recognition to encompass the complete range of exercises (macro labels).","The DS-MS-TCN model surpasses existing state-of-the-art deep learning models, achieving f1-scores exceeding 80% and Intersection over Union (IoU) f1-scores surpassing 60% for all four exercises evaluated.","Notably, the model outperforms the prior study utilizing the sliding window technique, eliminating the need for post-processing stages and window size tuning.","To our knowledge, we are the first to present a novel perspective on enhancing Human Activity Recognition (HAR) systems through the recognition of each repetition of activities."],"url":"http://arxiv.org/abs/2402.02910v1","category":"cs.LG"}
{"created":"2024-02-05 11:23:36","title":"Transition to chaos and magnetic field generation in rotating Rayleigh-B\u00e9nard convection","abstract":"Hydrodynamic and magnetohydrodynamic convective attractors in three-dimensional rotating Rayleigh-B\\'enard convection are studied numerically by varying the Taylor and Rayleigh numbers as control parameters. First, an analysis of hydrodynamic attractors and their bifurcations is conducted, where routes to chaos via quasiperiodicity are identified. Second, the behaviour of the magnetohydrodynamic system is investigated by introducing a seed magnetic field and measuring its growth or decay as a function of the Taylor number, while keeping the Rayleigh number fixed. Analysis of the attractors shows that rotation has a significant impact on magnetic field generation in Rayleigh-B\\'enard convection, with the critical magnetic Prandtl number changing nonmonotonically with the rotation rate. It is argued that a nonhysteretic blowout bifurcation with on-off intermittency is responsible for the transitions to dynamo.","sentences":["Hydrodynamic and magnetohydrodynamic convective attractors in three-dimensional rotating Rayleigh-B\\'enard convection are studied numerically by varying the Taylor and Rayleigh numbers as control parameters.","First, an analysis of hydrodynamic attractors and their bifurcations is conducted, where routes to chaos via quasiperiodicity are identified.","Second, the behaviour of the magnetohydrodynamic system is investigated by introducing a seed magnetic field and measuring its growth or decay as a function of the Taylor number, while keeping the Rayleigh number fixed.","Analysis of the attractors shows that rotation has a significant impact on magnetic field generation in Rayleigh-B\\'enard convection, with the critical magnetic Prandtl number changing nonmonotonically with the rotation rate.","It is argued that a nonhysteretic blowout bifurcation with on-off intermittency is responsible for the transitions to dynamo."],"url":"http://arxiv.org/abs/2402.02908v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 11:22:14","title":"ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis","abstract":"Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available.","sentences":["Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures.","Each approach offers specific strengths but also comes with specific limitations in their applicability.","This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility.","ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account.","Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods.","Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR.","Code is available."],"url":"http://arxiv.org/abs/2402.02906v1","category":"cs.CV"}
{"created":"2024-02-05 11:16:32","title":"Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows","abstract":"This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-controlled digital twin with complete musculoskeletal models of the human body is expected to be useful in designing experiments and validating rehabilitation theory before experiments on real human subjects.","sentences":["This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model.","By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model.","We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments.","The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities.","This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches.","An RL-controlled digital twin with complete musculoskeletal models of the human body is expected to be useful in designing experiments and validating rehabilitation theory before experiments on real human subjects."],"url":"http://arxiv.org/abs/2402.02904v1","category":"cs.RO"}
{"created":"2024-02-05 11:14:31","title":"The eXtended Virtual Element Method for elliptic problems with weakly singular solutions","abstract":"This paper introduces a novel eXtended virtual element method, an extension of the conforming virtual element method. The XVEM is formulated by incorporating appropriate enrichment functions in the local spaces. The method is designed to handle highly generic enrichment functions, including singularities arising from fractured domains. By achieving consistency on the enrichment space, the method is proven to achieve arbitrary approximation orders even in the presence of singular solutions. The paper includes a complete convergence analysis under general assumptions on mesh regularity, and numerical experiments validating the method's accuracy on various mesh families, demonstrating optimal convergence rates in the $L^2$- and $H^1$-norms on fractured or L-shaped domains.","sentences":["This paper introduces a novel eXtended virtual element method, an extension of the conforming virtual element method.","The XVEM is formulated by incorporating appropriate enrichment functions in the local spaces.","The method is designed to handle highly generic enrichment functions, including singularities arising from fractured domains.","By achieving consistency on the enrichment space, the method is proven to achieve arbitrary approximation orders even in the presence of singular solutions.","The paper includes a complete convergence analysis under general assumptions on mesh regularity, and numerical experiments validating the method's accuracy on various mesh families, demonstrating optimal convergence rates in the $L^2$- and $H^1$-norms on fractured or L-shaped domains."],"url":"http://arxiv.org/abs/2402.02902v1","category":"math.NA"}
{"created":"2024-02-05 11:05:20","title":"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models","abstract":"While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.","sentences":["While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents.","Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues.","In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm.","We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners.","Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments."],"url":"http://arxiv.org/abs/2402.02896v1","category":"cs.CL"}
{"created":"2024-02-05 11:00:11","title":"Yet another way from field theory to gravity","abstract":"It is shown that target space diffeomorphism invariance of a generic Lagrangian for a set of scalar fields leads to an analog of Einstein equations for the geometry of a level set of these fields.","sentences":["It is shown that target space diffeomorphism invariance of a generic Lagrangian for a set of scalar fields leads to an analog of Einstein equations for the geometry of a level set of these fields."],"url":"http://arxiv.org/abs/2402.02891v1","category":"gr-qc"}
{"created":"2024-02-05 10:57:48","title":"Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding","abstract":"The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task. Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods.","sentences":["The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy.","However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources.","In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data.","We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data.","Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task.","Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods."],"url":"http://arxiv.org/abs/2402.02889v1","category":"cs.SD"}
{"created":"2024-02-05 10:56:05","title":"Boundary Toda Conformal Field Theory from the path integral","abstract":"Toda Conformal Field Theories (CFTs hereafter) are generalizations of Liouville CFT where the underlying field is no longer scalar but takes values in a finite-dimensional vector space. The algebra of symmetry of such models is given by $W$-algebras, which contain the Virasoro algebra as a subalgebra. In contrast with Liouville CFT, and in the presence of a boundary, there may exist non-trivial automorphisms of the associated $W$-algebra, corresponding to different boundary conditions for the field of the theory.   Based on this particular feature, we provide in this document a probabilistic construction of Toda CFTs on a Riemann surface with or without boundary. To be more specific we define different classes of models in relation with the different types of boundary conditions suggested by the non-triviality of the automorphism group of the $W$-algebra. To do so, we rely on a probabilistic framework based on Gaussian Free Fields and Gaussian Multiplicative Chaos, and make sense of \"Cardy's doubling trick\" to construct the underlying field of the theory.","sentences":["Toda Conformal Field Theories (CFTs hereafter) are generalizations of Liouville CFT where the underlying field is no longer scalar but takes values in a finite-dimensional vector space.","The algebra of symmetry of such models is given by $W$-algebras, which contain the Virasoro algebra as a subalgebra.","In contrast with Liouville CFT, and in the presence of a boundary, there may exist non-trivial automorphisms of the associated $W$-algebra, corresponding to different boundary conditions for the field of the theory.   ","Based on this particular feature, we provide in this document a probabilistic construction of Toda CFTs on a Riemann surface with or without boundary.","To be more specific we define different classes of models in relation with the different types of boundary conditions suggested by the non-triviality of the automorphism group of the $W$-algebra.","To do so, we rely on a probabilistic framework based on Gaussian Free Fields and Gaussian Multiplicative Chaos, and make sense of \"Cardy's doubling trick\" to construct the underlying field of the theory."],"url":"http://arxiv.org/abs/2402.02888v1","category":"math.PR"}
{"created":"2024-02-05 10:54:14","title":"A Review on Building Blocks of Decentralized Artificial Intelligence","abstract":"Artificial intelligence is transforming our lives, and technological progress and transfer from the academic and theoretical sphere to the real world are accelerating yearly. But during that progress and transition, several open problems and questions need to be addressed for the field to develop ethically, such as digital privacy, ownership, and control. These are some of the reasons why the currently most popular approaches of artificial intelligence, i.e., centralized AI (CEAI), are questionable, with other directions also being widely explored, such as decentralized artificial intelligence (DEAI), to solve some of the most reaching problems. This paper provides a systematic literature review (SLR) of existing work in the field of DEAI, presenting the findings of 71 identified studies. The paper's primary focus is identifying the building blocks of DEAI solutions and networks, tackling the DEAI analysis from a bottom-up approach. In the end, future directions of research and open problems are proposed.","sentences":["Artificial intelligence is transforming our lives, and technological progress and transfer from the academic and theoretical sphere to the real world are accelerating yearly.","But during that progress and transition, several open problems and questions need to be addressed for the field to develop ethically, such as digital privacy, ownership, and control.","These are some of the reasons why the currently most popular approaches of artificial intelligence, i.e., centralized AI (CEAI), are questionable, with other directions also being widely explored, such as decentralized artificial intelligence (DEAI), to solve some of the most reaching problems.","This paper provides a systematic literature review (SLR) of existing work in the field of DEAI, presenting the findings of 71 identified studies.","The paper's primary focus is identifying the building blocks of DEAI solutions and networks, tackling the DEAI analysis from a bottom-up approach.","In the end, future directions of research and open problems are proposed."],"url":"http://arxiv.org/abs/2402.02885v1","category":"cs.AI"}
{"created":"2024-02-05 10:48:52","title":"Doubly nonlinear diffusive PDEs: new existence results via generalized Wasserstein gradient flows","abstract":"We prove an existence result for a large class of PDEs with a nonlinear Wasserstein gradient flow structure. We use the classical theory of Wasserstein gradient flow to derive an EDI formulation of our PDE and prove that under some integrability assumptions on the initial condition the PDE is satisfied in the sense of distributions.","sentences":["We prove an existence result for a large class of PDEs with a nonlinear Wasserstein gradient flow structure.","We use the classical theory of Wasserstein gradient flow to derive an EDI formulation of our PDE and prove that under some integrability assumptions on the initial condition the PDE is satisfied in the sense of distributions."],"url":"http://arxiv.org/abs/2402.02882v1","category":"math.AP"}
{"created":"2024-02-05 10:40:50","title":"A classical density functional theory for solvation across length scales","abstract":"A central aim of multiscale modeling is to use results from the Schr\\\"odinger Equation to predict phenomenology on length scales that far exceed those of typical molecular correlations. In this work, we present a new approach rooted in classical density functional theory (cDFT) that allows us to accurately describe the solvation of apolar solutes across length scales. Our approach builds on the Lum, Chandler and Weeks (LCW) theory of hydrophobicity [J. Phys. Chem. B 103, 4570 (1999)] by constructing a free energy functional that uses a slowly-varying component of the density field as a reference. From a practical viewpoint, the theory we present is numerically simpler and generalizes to solutes with soft-core repulsion more easily than LCW theory. Furthermore, we also provide two important conceptual insights. First, the coarse-graining of the density field emerges naturally and justifies a coarse-graining length much smaller than the molecular diameter of water. Second, by assessing the local compressibility and its critical scaling behavior, we demonstrate that our LCW-style cDFT approach contains the physics of critical drying, which has been emphasized as an essential aspect of hydrophobicity by recent theories. As our theory is parameterized on the two-body direct correlation function of the uniform fluid and the liquid-vapor surface tension, it straightforwardly captures the temperature dependence of solvation. Moreover, we use our theory to describe solvation at a first-principles level, on length scales that vastly exceed what is accessible to molecular simulations.","sentences":["A central aim of multiscale modeling is to use results from the Schr\\\"odinger Equation to predict phenomenology on length scales that far exceed those of typical molecular correlations.","In this work, we present a new approach rooted in classical density functional theory (cDFT) that allows us to accurately describe the solvation of apolar solutes across length scales.","Our approach builds on the Lum, Chandler and Weeks (LCW) theory of hydrophobicity [J. Phys.","Chem.","B 103, 4570 (1999)]","by constructing a free energy functional that uses a slowly-varying component of the density field as a reference.","From a practical viewpoint, the theory we present is numerically simpler and generalizes to solutes with soft-core repulsion more easily than LCW theory.","Furthermore, we also provide two important conceptual insights.","First, the coarse-graining of the density field emerges naturally and justifies a coarse-graining length much smaller than the molecular diameter of water.","Second, by assessing the local compressibility and its critical scaling behavior, we demonstrate that our LCW-style cDFT approach contains the physics of critical drying, which has been emphasized as an essential aspect of hydrophobicity by recent theories.","As our theory is parameterized on the two-body direct correlation function of the uniform fluid and the liquid-vapor surface tension, it straightforwardly captures the temperature dependence of solvation.","Moreover, we use our theory to describe solvation at a first-principles level, on length scales that vastly exceed what is accessible to molecular simulations."],"url":"http://arxiv.org/abs/2402.02873v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 10:36:24","title":"Heat Equations and Hearing the Genus on p-adic Mumford Curves via Automorphic Forms","abstract":"A self-adjoint operator is constructed on the $L_2$-functions on the $K$-rational points $X(K)$ of a Mumford curve $X$ defined over a non-archimedean local field $K$. It generates a Feller semi-group, and the corresponding heat equation describes a Markov process on $X(K)$. Its spectrum is non-positive, contains zero and has finitely many limit points which are the only non-eigenvalues, and correspond to the zeros of a given regular differential 1-form on $X(K)$. This allows to recover the genus of X from the spectrum. The hyperelliptic case allows in principle an explicit genus extraction.","sentences":["A self-adjoint operator is constructed on the $L_2$-functions on the $K$-rational points $X(K)$ of a Mumford curve $X$ defined over a non-archimedean local field $K$. It generates a Feller semi-group, and the corresponding heat equation describes a Markov process on $X(K)$. Its spectrum is non-positive, contains zero and has finitely many limit points which are the only non-eigenvalues, and correspond to the zeros of a given regular differential 1-form on $X(K)$. This allows to recover the genus of X from the spectrum.","The hyperelliptic case allows in principle an explicit genus extraction."],"url":"http://arxiv.org/abs/2402.02869v1","category":"math.NT"}
{"created":"2024-02-05 10:26:28","title":"On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification","abstract":"Speech intelligibility can be affected by multiple factors, such as noisy environments, channel distortions or physiological issues. In this work, we deal with the problem of automatic prediction of the speech intelligibility level in this latter case. Starting from our previous work, a non-intrusive system based on LSTM networks with attention mechanism designed for this task, we present two main contributions. In the first one, it is proposed the use of per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information. In the second one, two different strategies for the combination of per-frame acoustic log-mel and modulation spectrograms into the LSTM framework are explored: at decision level or late fusion and at utterance level or Weighted-Pooling (WP) fusion. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. On the one hand, results show that attentional LSTM networks are able to adequately modeling the modulation spectrograms sequences producing similar classification rates as in the case of log-mel spectrograms. On the other hand, both combination strategies, late and WP fusion, outperform the single-feature systems, suggesting that per-frame log-mel and modulation spectrograms carry complementary information for the task of speech intelligibility prediction, than can be effectively exploited by the LSTM-based architectures, being the system with the WP fusion strategy and Attention-Pooling the one that achieves best results.","sentences":["Speech intelligibility can be affected by multiple factors, such as noisy environments, channel distortions or physiological issues.","In this work, we deal with the problem of automatic prediction of the speech intelligibility level in this latter case.","Starting from our previous work, a non-intrusive system based on LSTM networks with attention mechanism designed for this task, we present two main contributions.","In the first one, it is proposed the use of per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information.","In the second one, two different strategies for the combination of per-frame acoustic log-mel and modulation spectrograms into the LSTM framework are explored: at decision level or late fusion and at utterance level or Weighted-Pooling (WP) fusion.","The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity.","On the one hand, results show that attentional LSTM networks are able to adequately modeling the modulation spectrograms sequences producing similar classification rates as in the case of log-mel spectrograms.","On the other hand, both combination strategies, late and WP fusion, outperform the single-feature systems, suggesting that per-frame log-mel and modulation spectrograms carry complementary information for the task of speech intelligibility prediction, than can be effectively exploited by the LSTM-based architectures, being the system with the WP fusion strategy and Attention-Pooling the one that achieves best results."],"url":"http://arxiv.org/abs/2402.02865v1","category":"eess.AS"}
{"created":"2024-02-05 10:18:47","title":"Importance sampling for online variational learning","abstract":"This article addresses online variational estimation in state-space models. We focus on learning the smoothing distribution, i.e. the joint distribution of the latent states given the observations, using a variational approach together with Monte Carlo importance sampling. We propose an efficient algorithm for computing the gradient of the evidence lower bound (ELBO) in the context of streaming data, where observations arrive sequentially. Our contributions include a computationally efficient online ELBO estimator, demonstrated performance in offline and true online settings, and adaptability for computing general expectations under joint smoothing distributions.","sentences":["This article addresses online variational estimation in state-space models.","We focus on learning the smoothing distribution, i.e. the joint distribution of the latent states given the observations, using a variational approach together with Monte Carlo importance sampling.","We propose an efficient algorithm for computing the gradient of the evidence lower bound (ELBO) in the context of streaming data, where observations arrive sequentially.","Our contributions include a computationally efficient online ELBO estimator, demonstrated performance in offline and true online settings, and adaptability for computing general expectations under joint smoothing distributions."],"url":"http://arxiv.org/abs/2402.02859v1","category":"stat.AP"}
{"created":"2024-02-05 10:16:29","title":"Generalized dynamical phase reduction for stochastic oscillators","abstract":"Phase reduction is an important tool for studying coupled and driven oscillators. The question of how to generalize phase reduction to stochastic oscillators remains actively debated. In this work, we propose a method to derive a self-contained stochastic phase equation of the form $d\\vartheta = a_\\vartheta(\\vartheta)dt + \\sqrt{2D_\\vartheta(\\vartheta)}\\,dW_\\vartheta(t)$ that is valid not only for noise-perturbed limit cycles, but also for noise-induced oscillations. We show that our reduction captures the asymptotic statistics of qualitatively different stochastic oscillators, and use it to infer their phase-response properties.","sentences":["Phase reduction is an important tool for studying coupled and driven oscillators.","The question of how to generalize phase reduction to stochastic oscillators remains actively debated.","In this work, we propose a method to derive a self-contained stochastic phase equation of the form $d\\vartheta = a_\\vartheta(\\vartheta)dt + \\sqrt{2D_\\vartheta(\\vartheta)}\\,dW_\\vartheta(t)$ that is valid not only for noise-perturbed limit cycles, but also for noise-induced oscillations.","We show that our reduction captures the asymptotic statistics of qualitatively different stochastic oscillators, and use it to infer their phase-response properties."],"url":"http://arxiv.org/abs/2402.02856v1","category":"math-ph"}
{"created":"2024-02-05 10:07:10","title":"Amorphous quantum magnets in a two-dimensional Rydberg atom array","abstract":"Amorphous solids, i.e., systems which feature well-defined short-range properties but lack long-range order, constitute an important research topic in condensed matter. While their microscopic structure is known to differ from their crystalline counterpart, there are still many open questions concerning the emergent collective behavior in amorphous materials. This is particularly the case in the quantum regime, where the numerical simulations are extremely challenging. In this article, we instead propose to explore amorphous quantum magnets with an analog quantum simulator. To this end, we first present an algorithm to generate amorphous quantum magnets, suitable for Rydberg simulators of the Ising model. Subsequently, we use semiclassical approaches to get a preliminary insight of the physics of the model. In particular, we calculate mean-field phase diagrams, and use the linear-spin-wave theory to study localization properties and dynamical structure factors of the excitations. Finally, we outline an experimental proposal based on Rydberg atoms in programmable tweezer arrays, thus opening the road towards the study of amorphous quantum magnets in regimes difficult to simulate classically.","sentences":["Amorphous solids, i.e., systems which feature well-defined short-range properties but lack long-range order, constitute an important research topic in condensed matter.","While their microscopic structure is known to differ from their crystalline counterpart, there are still many open questions concerning the emergent collective behavior in amorphous materials.","This is particularly the case in the quantum regime, where the numerical simulations are extremely challenging.","In this article, we instead propose to explore amorphous quantum magnets with an analog quantum simulator.","To this end, we first present an algorithm to generate amorphous quantum magnets, suitable for Rydberg simulators of the Ising model.","Subsequently, we use semiclassical approaches to get a preliminary insight of the physics of the model.","In particular, we calculate mean-field phase diagrams, and use the linear-spin-wave theory to study localization properties and dynamical structure factors of the excitations.","Finally, we outline an experimental proposal based on Rydberg atoms in programmable tweezer arrays, thus opening the road towards the study of amorphous quantum magnets in regimes difficult to simulate classically."],"url":"http://arxiv.org/abs/2402.02852v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-05 10:06:24","title":"Enhancing Compositional Generalization via Compositional Feature Alignment","abstract":"Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.","sentences":["Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions.","In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination.","This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations.","To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge.","To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen.","We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models.","We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models.","Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning."],"url":"http://arxiv.org/abs/2402.02851v1","category":"cs.CV"}
{"created":"2024-02-05 10:03:28","title":"An Attention Long Short-Term Memory based system for automatic classification of speech intelligibility","abstract":"Speech intelligibility can be degraded due to multiple factors, such as noisy environments, technical difficulties or biological conditions. This work is focused on the development of an automatic non-intrusive system for predicting the speech intelligibility level in this latter case. The main contribution of our research on this topic is the use of Long Short-Term Memory (LSTM) networks with log-mel spectrograms as input features for this purpose. In addition, this LSTM-based system is further enhanced by the incorporation of a simple attention mechanism that is able to determine the more relevant frames to this task. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. Results show that the attention LSTM architecture outperforms both, a reference Support Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based system with Mean-Pooling.","sentences":["Speech intelligibility can be degraded due to multiple factors, such as noisy environments, technical difficulties or biological conditions.","This work is focused on the development of an automatic non-intrusive system for predicting the speech intelligibility level in this latter case.","The main contribution of our research on this topic is the use of Long Short-Term Memory (LSTM) networks with log-mel spectrograms as input features for this purpose.","In addition, this LSTM-based system is further enhanced by the incorporation of a simple attention mechanism that is able to determine the more relevant frames to this task.","The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity.","Results show that the attention LSTM architecture outperforms both, a reference Support Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based system with Mean-Pooling."],"url":"http://arxiv.org/abs/2402.02850v1","category":"eess.AS"}
{"created":"2024-02-05 10:02:16","title":"Do we need decay-preserving error estimate for solving parabolic equations with initial singularity?","abstract":"Solutions exhibiting weak initial singularities arise in various equations, including diffusion and subdiffusion equations. When employing the well-known L1 scheme to solve subdiffusion equations with weak singularities, numerical simulations reveal that this scheme exhibits varying convergence rates for different choices of model parameters (i.e., domain size, final time $T$, and reaction coefficient $\\kappa$). This elusive phenomenon is not unique to the L1 scheme but is also observed in other numerical methods for reaction-diffusion equations such as the backward Euler (IE) scheme, Crank-Nicolson (C-N) scheme, and two-step backward differentiation formula (BDF2) scheme. The existing literature lacks an explanation for the existence of two different convergence regimes, which has puzzled us for a long while and motivated us to study this inconsistency between the standard convergence theory and numerical experiences. In this paper, we provide a general methodology to systematically obtain error estimates that incorporate the exponential decaying feature of the solution. We term this novel error estimate the `decay-preserving error estimate' and apply it to the aforementioned IE, C-N, and BDF2 schemes. Our decay-preserving error estimate consists of a low-order term with an exponential coefficient and a high-order term with an algebraic coefficient, both of which depend on the model parameters. Our estimates reveal that the varying convergence rates are caused by a trade-off between these two components in different model parameter regimes. By considering the model parameters, we capture different states of the convergence rate that traditional error estimates fail to explain. This approach retains more properties of the continuous solution. We validate our analysis with numerical results.","sentences":["Solutions exhibiting weak initial singularities arise in various equations, including diffusion and subdiffusion equations.","When employing the well-known L1 scheme to solve subdiffusion equations with weak singularities, numerical simulations reveal that this scheme exhibits varying convergence rates for different choices of model parameters (i.e., domain size, final time $T$, and reaction coefficient $\\kappa$).","This elusive phenomenon is not unique to the L1 scheme but is also observed in other numerical methods for reaction-diffusion equations such as the backward Euler (IE) scheme, Crank-Nicolson (C-N) scheme, and two-step backward differentiation formula (BDF2) scheme.","The existing literature lacks an explanation for the existence of two different convergence regimes, which has puzzled us for a long while and motivated us to study this inconsistency between the standard convergence theory and numerical experiences.","In this paper, we provide a general methodology to systematically obtain error estimates that incorporate the exponential decaying feature of the solution.","We term this novel error estimate the `decay-preserving error estimate' and apply it to the aforementioned IE, C-N, and BDF2 schemes.","Our decay-preserving error estimate consists of a low-order term with an exponential coefficient and a high-order term with an algebraic coefficient, both of which depend on the model parameters.","Our estimates reveal that the varying convergence rates are caused by a trade-off between these two components in different model parameter regimes.","By considering the model parameters, we capture different states of the convergence rate that traditional error estimates fail to explain.","This approach retains more properties of the continuous solution.","We validate our analysis with numerical results."],"url":"http://arxiv.org/abs/2402.02849v1","category":"math.NA"}
{"created":"2024-02-05 09:59:51","title":"A general integral identity with applications to a reverse Serrin problem","abstract":"We prove a new general differential identity and an associated integral identity, which entails a pair of solutions of the Poisson equation with constant source term. This generalizes a formula that the first and third authors previously proved and used to obtain quantitative estimates of spherical symmetry for the Serrin overdetermined boundary value problem. As a first application, we prove a quantitative symmetry result for the reverse Serrin problem, which we introduce for the first time in this paper. In passing, we obtain a rigidity result for solutions of the aforementioned Poisson equation subject to a constant Neumann condition.","sentences":["We prove a new general differential identity and an associated integral identity, which entails a pair of solutions of the Poisson equation with constant source term.","This generalizes a formula that the first and third authors previously proved and used to obtain quantitative estimates of spherical symmetry for the Serrin overdetermined boundary value problem.","As a first application, we prove a quantitative symmetry result for the reverse Serrin problem, which we introduce for the first time in this paper.","In passing, we obtain a rigidity result for solutions of the aforementioned Poisson equation subject to a constant Neumann condition."],"url":"http://arxiv.org/abs/2402.02845v1","category":"math.AP"}
{"created":"2024-02-05 09:57:15","title":"Comparing Knowledge Sources for Open-Domain Scientific Claim Verification","abstract":"The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different information retrieval techniques. We show that PubMed works better with specialized biomedical claims, while Wikipedia is more suited for everyday health concerns. Likewise, BM25 excels in retrieval precision, while semantic search in recall of relevant evidence. We discuss the results, outline frequent retrieval patterns and challenges, and provide promising future directions.","sentences":["The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims.","The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus.","This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence.","In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems.","We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings.","While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different information retrieval techniques.","We show that PubMed works better with specialized biomedical claims, while Wikipedia is more suited for everyday health concerns.","Likewise, BM25 excels in retrieval precision, while semantic search in recall of relevant evidence.","We discuss the results, outline frequent retrieval patterns and challenges, and provide promising future directions."],"url":"http://arxiv.org/abs/2402.02844v1","category":"cs.CL"}
{"created":"2024-02-05 09:56:38","title":"Double Dyck Path Algebra Representations From DAHA","abstract":"The double Dyck path algebra $\\mathbb{A}_{q,t}$ was introduced by Carlsson-Mellit in their proof of the Shuffle Theorem. A variant of this algebra, $\\mathbb{B}_{q,t}$, was introduced by Carlsson-Gorsky-Mellit in their study of the parabolic flag Hilbert schemes of points in $\\mathbb{C}^2$ showing that $\\mathbb{B}_{q,t}$ acts naturally on the equivariant $K$-theory of these spaces. The algebraic relations defining $\\mathbb{B}_{q,t}$ appear superficially similar to those of the positive double affine Hecke algebras (DAHA) in type $GL$, $\\mathscr{D}_n^{+}$, introduced by Cherednik. In this paper we provide a general method for constructing $\\mathbb{B}_{q,t}$ representations from DAHA representations. In particular, every $\\mathscr{D}_n^{+}$ module yields a representation of a subalgebra $\\mathbb{B}_{q,t}^{(n)}$ of $\\mathbb{B}_{q,t}$ and special families of compatible DAHA representations give representations of $\\mathbb{B}_{q,t}$. These constructions are functorial. Lastly, we will construct a large family of $\\mathbb{B}_{q,t}$ representations indexed by partitions using this method related to the Murnaghan-type representations of the positive elliptic Hall algebra introduced previously by the author.","sentences":["The double Dyck path algebra $\\mathbb{A}_{q,t}$ was introduced by Carlsson-Mellit in their proof of the Shuffle Theorem.","A variant of this algebra, $\\mathbb{B}_{q,t}$, was introduced by Carlsson-Gorsky-Mellit in their study of the parabolic flag Hilbert schemes of points in $\\mathbb{C}^2$ showing that $\\mathbb{B}_{q,t}$ acts naturally on the equivariant $K$-theory of these spaces.","The algebraic relations defining $\\mathbb{B}_{q,t}$ appear superficially similar to those of the positive double affine Hecke algebras (DAHA) in type $GL$, $\\mathscr{D}_n^{+}$, introduced by Cherednik.","In this paper we provide a general method for constructing $\\mathbb{B}_{q,t}$ representations from DAHA representations.","In particular, every $\\mathscr{D}_n^{+}$ module yields a representation of a subalgebra $\\mathbb{B}_{q,t}^{(n)}$ of $\\mathbb{B}_{q,t}$ and special families of compatible DAHA representations give representations of $\\mathbb{B}_{q,t}$.","These constructions are functorial.","Lastly, we will construct a large family of $\\mathbb{B}_{q,t}$ representations indexed by partitions using this method related to the Murnaghan-type representations of the positive elliptic Hall algebra introduced previously by the author."],"url":"http://arxiv.org/abs/2402.02843v1","category":"math.RT"}
{"created":"2024-02-05 09:53:08","title":"Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One","abstract":"Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common \"interest amnesia\" problem, and a solution exists to mitigate it simultaneously. We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest. Inspired by the observation, we propose a novel and unified framework in the retrieval stage, \"Trinity\", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity recognizes underdelivered themes and remains stable when facing emerging hot topics. Trinity is more appropriate for large-scale industry scenarios because of its modest computational overheads. Its derived retrievers have been deployed on the recommender system of Douyin, significantly improving user experience and retention. We believe that such practical experience can be well generalized to other scenarios.","sentences":["Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works.","However, most of them only consider one interest in isolation, while neglecting their interrelationships.","In this paper, we argue that these tasks suffer from a common \"interest amnesia\" problem, and a solution exists to mitigate it simultaneously.","We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest.","Inspired by the observation, we propose a novel and unified framework in the retrieval stage, \"Trinity\", to solve interest amnesia problem and improve multiple interest modeling tasks.","We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters.","Based on these histograms, Trinity recognizes underdelivered themes and remains stable when facing emerging hot topics.","Trinity is more appropriate for large-scale industry scenarios because of its modest computational overheads.","Its derived retrievers have been deployed on the recommender system of Douyin, significantly improving user experience and retention.","We believe that such practical experience can be well generalized to other scenarios."],"url":"http://arxiv.org/abs/2402.02842v1","category":"cs.IR"}
{"created":"2024-02-05 09:45:09","title":"Performance optimization of continuous variable quantum teleportation with generalized photon-varying non-Gaussian operations","abstract":"Continuous variable quantum teleportation provides a path to the long-distance transmission of quantum states. Photon-varying non-Gaussian operations have been shown to improve the fidelity of quantum teleportation when integrated into the protocol. However, given a fixed non-Gaussian operation, the achievable fidelity varies with different input states. An operation that increases the fidelity for teleporting one class of states might do the contrary for other classes of states. A performance metric suitable for different input states is missing. For a given type of non-Gaussian operation, the achievable fidelity also varies with parameters associated with the operation. Previous work only focuses on particular settings of the parameters. Optimization over the parameters is also missing. In this work, we build a framework for photon-varying non-Gaussian operations for multi-mode states, upon which we propose a performance metric suitable for arbitrary teleportation input states. We then apply the new metric to evaluate different types of non-Gaussian operations. Starting from simple multi-photon photon subtraction and photon addition, we find that increasing the number of ancillary photons involved in the operation does not guarantee performance improvement. We then investigate combinations of the operations mentioned above, finding that operations that approximate a particular form provide the best improvement. The results provided here will be valuable for real-world implementations of quantum teleportation networks and applications that harness the non-Gaussianity of quantum states.","sentences":["Continuous variable quantum teleportation provides a path to the long-distance transmission of quantum states.","Photon-varying non-Gaussian operations have been shown to improve the fidelity of quantum teleportation when integrated into the protocol.","However, given a fixed non-Gaussian operation, the achievable fidelity varies with different input states.","An operation that increases the fidelity for teleporting one class of states might do the contrary for other classes of states.","A performance metric suitable for different input states is missing.","For a given type of non-Gaussian operation, the achievable fidelity also varies with parameters associated with the operation.","Previous work only focuses on particular settings of the parameters.","Optimization over the parameters is also missing.","In this work, we build a framework for photon-varying non-Gaussian operations for multi-mode states, upon which we propose a performance metric suitable for arbitrary teleportation input states.","We then apply the new metric to evaluate different types of non-Gaussian operations.","Starting from simple multi-photon photon subtraction and photon addition, we find that increasing the number of ancillary photons involved in the operation does not guarantee performance improvement.","We then investigate combinations of the operations mentioned above, finding that operations that approximate a particular form provide the best improvement.","The results provided here will be valuable for real-world implementations of quantum teleportation networks and applications that harness the non-Gaussianity of quantum states."],"url":"http://arxiv.org/abs/2402.02835v1","category":"quant-ph"}
{"created":"2024-02-05 09:27:40","title":"Shaping High-Order Diffraction-Free Beams Through Continuous Superposition of Bessel Beams","abstract":"Recognized for their non-diffracting properties, Bessel beams can be conveniently combined to generate the so-called Frozen Waves, which are monochromatic beams endowed with topological charge and whose longitudinal intensity pattern can be shaped according to a previously chosen function. Continuous superposition of Bessel beams is specially suitable for micrometer-scale domains, being highly relevant for applications in optical tweezers, particle trapping and atom guidance. Previous studies have successfully constructed micrometer Frozen Wave solutions with null topological charge; nevertheless, some challenges persist in obtaining exact solutions when dealing with higher topological charge values (i.e., higher-order micrometer Frozen Waves). Typically, a topological charge raising operator is used to elevate the order; however, such solutions face significant issues when the orders become excessively high. In this paper, based in continuous superposition of higher order Bessel beams, we develop a novel analytic and exact solution for higher-order micrometer Frozen Waves, so obtaining a method for modeling the intensity of beams with arbitrary topological charge within micrometer spatial domains. We also investigate the behavior of the electric field's longitudinal component in such highly non-paraxial regimes through a vectorial treatment.","sentences":["Recognized for their non-diffracting properties, Bessel beams can be conveniently combined to generate the so-called Frozen Waves, which are monochromatic beams endowed with topological charge and whose longitudinal intensity pattern can be shaped according to a previously chosen function.","Continuous superposition of Bessel beams is specially suitable for micrometer-scale domains, being highly relevant for applications in optical tweezers, particle trapping and atom guidance.","Previous studies have successfully constructed micrometer Frozen Wave solutions with null topological charge; nevertheless, some challenges persist in obtaining exact solutions when dealing with higher topological charge values (i.e., higher-order micrometer Frozen Waves).","Typically, a topological charge raising operator is used to elevate the order; however, such solutions face significant issues when the orders become excessively high.","In this paper, based in continuous superposition of higher order Bessel beams, we develop a novel analytic and exact solution for higher-order micrometer Frozen Waves, so obtaining a method for modeling the intensity of beams with arbitrary topological charge within micrometer spatial domains.","We also investigate the behavior of the electric field's longitudinal component in such highly non-paraxial regimes through a vectorial treatment."],"url":"http://arxiv.org/abs/2402.02828v1","category":"physics.optics"}
{"created":"2024-02-05 09:24:52","title":"PowerGraph: A power grid benchmark dataset for graph neural networks","abstract":"Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of cascading failures leveraging the knowledge of the system state at the onset of the cascade. We develop PowerGraph, a graph dataset modeling cascading failures in power grids, designed for two purposes, namely, i) training GNN models for different graph-level tasks including multi-class classification, binary classification, and regression, and ii) explaining GNN models. The dataset generated via a physics-based cascading failure model ensures the generality of the operating and environmental conditions by spanning diverse failure scenarios. In addition, we foster the use of the dataset to benchmark GNN explainability methods by assigning ground-truth edge-level explanations. PowerGraph helps the development of better GNN models for graph-level tasks and explainability, critical in many domains ranging from chemistry to biology, where the systems and processes can be described as graphs.","sentences":["Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines.","The community currently lacks public datasets of electrical power grids for GNN applications.","Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques.","Power grids are complex engineered networks that are naturally amenable to graph representations.","Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques.","To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids.","Historical blackout datasets are scarce and incomplete.","The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures.","Instead, we propose using machine learning models for the online detection of cascading failures leveraging the knowledge of the system state at the onset of the cascade.","We develop PowerGraph, a graph dataset modeling cascading failures in power grids, designed for two purposes, namely, i) training GNN models for different graph-level tasks including multi-class classification, binary classification, and regression, and ii) explaining GNN models.","The dataset generated via a physics-based cascading failure model ensures the generality of the operating and environmental conditions by spanning diverse failure scenarios.","In addition, we foster the use of the dataset to benchmark GNN explainability methods by assigning ground-truth edge-level explanations.","PowerGraph helps the development of better GNN models for graph-level tasks and explainability, critical in many domains ranging from chemistry to biology, where the systems and processes can be described as graphs."],"url":"http://arxiv.org/abs/2402.02827v1","category":"cs.LG"}
{"created":"2024-02-05 09:15:53","title":"GWAI: Harnessing Artificial Intelligence for Enhancing Gravitational Wave Data Analysis","abstract":"Gravitational wave (GW) astronomy has opened new frontiers in understanding the cosmos, while the integration of artificial intelligence (AI) in science promises to revolutionize data analysis methodologies. However, a significant gap exists, as there is currently no dedicated platform that enables scientists to develop, test, and evaluate AI algorithms efficiently. To address this gap, we introduce GWAI, a pioneering AI-centered software platform designed for gravitational wave data analysis. GWAI contains a three-layered architecture that emphasizes simplicity, modularity, and flexibility, covering the entire analysis pipeline. GWAI aims to accelerate scientific discoveries, bridging the gap between advanced AI techniques and astrophysical research.","sentences":["Gravitational wave (GW) astronomy has opened new frontiers in understanding the cosmos, while the integration of artificial intelligence (AI) in science promises to revolutionize data analysis methodologies.","However, a significant gap exists, as there is currently no dedicated platform that enables scientists to develop, test, and evaluate AI algorithms efficiently.","To address this gap, we introduce GWAI, a pioneering AI-centered software platform designed for gravitational wave data analysis.","GWAI contains a three-layered architecture that emphasizes simplicity, modularity, and flexibility, covering the entire analysis pipeline.","GWAI aims to accelerate scientific discoveries, bridging the gap between advanced AI techniques and astrophysical research."],"url":"http://arxiv.org/abs/2402.02825v1","category":"gr-qc"}
{"created":"2024-02-05 09:10:32","title":"Evading Data Contamination Detection for Language Models is (too) Easy","abstract":"Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.","sentences":["Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another.","However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements.","While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection.","We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks.","To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods.","This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods."],"url":"http://arxiv.org/abs/2402.02823v1","category":"cs.LG"}
{"created":"2024-02-05 09:03:45","title":"Striking the right tone: towards a self-consistent framework for measuring black hole ringdowns","abstract":"The ringdown portion of a binary black hole merger consists of a sum of modes, each containing an infinite number of tones that are exponentially damped sinusoids. In principle, these can be measured as gravitational-waves with observatories like LIGO/Virgo/KAGRA, however in practice it is unclear how many tones can be meaningfully resolved. We investigate the consistency and resolvability of the overtones of the quadrupolar l = m = 2 mode by starting at late times when the gravitational waveform is expected to be well-approximated by the l m n = 220 tone alone. We present a Bayesian inference framework to measure the tones in numerical relativity data. We measure tones at different start times, checking for consistency: we classify a tone as stably recovered if and only if the 95% credible intervals for amplitude and phase at time t overlap with the credible intervals at all subsequent times. We test the first four overtones of the fundamental mode and find that the 220 and 221 tones can be measured consistently with the inclusion of additional overtones. The 222 tone measurements can be stabilised when we include the 223 tone, but only in a narrow time window, after which it is too weak to measure. The 223 tone recovery appears to be unstable, and does not become stable with the introduction of the 224 tone. Within our framework, the ringdown of the fundamental mode of binary black hole waveforms can be self-consistently described by three to four tones, which are stable from 10 M after the peak strain. However, additional tones are not obviously required because the fit amplitudes are consistent with zero. We conclude that recent claims of overtone detection are not necessarily an exercise in over-fitting; the observed tones can be self-consistently modelled, although, not until ~ 10 M from peak strain with our four-tone model.","sentences":["The ringdown portion of a binary black hole merger consists of a sum of modes, each containing an infinite number of tones that are exponentially damped sinusoids.","In principle, these can be measured as gravitational-waves with observatories like LIGO/Virgo/KAGRA, however in practice it is unclear how many tones can be meaningfully resolved.","We investigate the consistency and resolvability of the overtones of the quadrupolar l = m = 2 mode by starting at late times when the gravitational waveform is expected to be well-approximated by the l m n = 220 tone alone.","We present a Bayesian inference framework to measure the tones in numerical relativity data.","We measure tones at different start times, checking for consistency: we classify a tone as stably recovered if and only if the 95% credible intervals for amplitude and phase at time t overlap with the credible intervals at all subsequent times.","We test the first four overtones of the fundamental mode and find that the 220 and 221 tones can be measured consistently with the inclusion of additional overtones.","The 222 tone measurements can be stabilised when we include the 223 tone, but only in a narrow time window, after which it is too weak to measure.","The 223 tone recovery appears to be unstable, and does not become stable with the introduction of the 224 tone.","Within our framework, the ringdown of the fundamental mode of binary black hole waveforms can be self-consistently described by three to four tones, which are stable from 10 M after the peak strain.","However, additional tones are not obviously required because the fit amplitudes are consistent with zero.","We conclude that recent claims of overtone detection are not necessarily an exercise in over-fitting; the observed tones can be self-consistently modelled, although, not until ~ 10 M from peak strain with our four-tone model."],"url":"http://arxiv.org/abs/2402.02819v1","category":"gr-qc"}
{"created":"2024-02-05 09:01:31","title":"Investigating the influence of particle size and shape on froth flotation based benefication of lithium-rich minerals in slags","abstract":"The demand for lithium, as well as other critical resources, needed for electrochemical energy storage is expected to grow significantly in the future. Slags obtained from pyrometallurgical recycling represent a promising resource of valuable materials, among them lithium and rare earth elements found in artificial minerals particulate phases. This study investigates the flotation separation of engineered artificial minerals (EnAMs) in slags, such as lithium aluminate and gehlenite as valuable and gangue phases, respectively. Flotation experiments are carried out in a Partridge-Smith cell using oleic acid (OA) as a benchmark surfactant. Particle characterization is performed using SEM-based Mineral Liberation Analysis (MLA), which provides particle discrete information. From this information, bivariate Tromp functions based on non-parametric kernel density estimation are computed to characterize the separation behavior with respect to particle descriptors. This approach enables investigating the influence of particle size and shape on separation behavior of EnAMs. Furthermore, these results allow for the optimization of flotation experiments for enriching Li-bearing EnAMs.","sentences":["The demand for lithium, as well as other critical resources, needed for electrochemical energy storage is expected to grow significantly in the future.","Slags obtained from pyrometallurgical recycling represent a promising resource of valuable materials, among them lithium and rare earth elements found in artificial minerals particulate phases.","This study investigates the flotation separation of engineered artificial minerals (EnAMs) in slags, such as lithium aluminate and gehlenite as valuable and gangue phases, respectively.","Flotation experiments are carried out in a Partridge-Smith cell using oleic acid (OA) as a benchmark surfactant.","Particle characterization is performed using SEM-based Mineral Liberation Analysis (MLA), which provides particle discrete information.","From this information, bivariate Tromp functions based on non-parametric kernel density estimation are computed to characterize the separation behavior with respect to particle descriptors.","This approach enables investigating the influence of particle size and shape on separation behavior of EnAMs.","Furthermore, these results allow for the optimization of flotation experiments for enriching Li-bearing EnAMs."],"url":"http://arxiv.org/abs/2402.02818v1","category":"cond-mat.soft"}
{"created":"2024-02-05 08:35:33","title":"Are Sounds Sound for Phylogenetic Reconstruction?","abstract":"In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, to the gold standard phylogenies than phylogenies reconstructed from sound correspondences.","sentences":["In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees.","However, to date, computational approaches have typically not taken this potential into account.","Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences.","Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction.","Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, to the gold standard phylogenies than phylogenies reconstructed from sound correspondences."],"url":"http://arxiv.org/abs/2402.02807v1","category":"cs.CL"}
{"created":"2024-02-05 18:41:46","title":"Estimating position-dependent and anisotropic diffusivity tensors from molecular dynamics trajectories: Existing methods and future outlook","abstract":"Confinement can substantially alter the physicochemical properties of materials by breaking translational isotropy and rendering all physical properties position-dependent. Molecular dynamics (MD) simulations have proven instrumental in characterizing such spatial heterogeneities and probing the impact of confinement on materials' properties. For static properties, this is a straightforward task and can be achieved via simple spatial binning. Such an approach, however, cannot be readily applied to transport coefficients due to lack of natural extensions of autocorrelations used for their calculation in the bulk. The prime example of this challenge is diffusivity, which, in the bulk, can be readily estimated from the particles' mobility statistics, which satisfy the Fokker-Planck equation. Under confinement, however, such statistics will follow the Smoluchowski equation, which lacks a closed-form analytical solution. This brief review explores the rich history of estimating profiles of the diffusivity tensor from MD simulations and discusses various approximate methods and algorithms developed for this purpose. Beside discussing heuristic extensions of bulk methods, we overview more rigorous algorithms, including kernel-based methods, Bayesian approaches, and operator discretization techniques. Additionally, we outline methods based on applying biasing potentials or imposing constraints on tracer particles. Finally, we discuss approaches that estimate diffusivity from mean first passage time or committor probability profiles, a conceptual framework originally developed in the context of collective variable spaces describing rare events in computational chemistry and biology. In summary, this paper offers a concise survey of diverse approaches for estimating diffusivity from MD trajectories, highlighting challenges and opportunities in this area.","sentences":["Confinement can substantially alter the physicochemical properties of materials by breaking translational isotropy and rendering all physical properties position-dependent.","Molecular dynamics (MD) simulations have proven instrumental in characterizing such spatial heterogeneities and probing the impact of confinement on materials' properties.","For static properties, this is a straightforward task and can be achieved via simple spatial binning.","Such an approach, however, cannot be readily applied to transport coefficients due to lack of natural extensions of autocorrelations used for their calculation in the bulk.","The prime example of this challenge is diffusivity, which, in the bulk, can be readily estimated from the particles' mobility statistics, which satisfy the Fokker-Planck equation.","Under confinement, however, such statistics will follow the Smoluchowski equation, which lacks a closed-form analytical solution.","This brief review explores the rich history of estimating profiles of the diffusivity tensor from MD simulations and discusses various approximate methods and algorithms developed for this purpose.","Beside discussing heuristic extensions of bulk methods, we overview more rigorous algorithms, including kernel-based methods, Bayesian approaches, and operator discretization techniques.","Additionally, we outline methods based on applying biasing potentials or imposing constraints on tracer particles.","Finally, we discuss approaches that estimate diffusivity from mean first passage time or committor probability profiles, a conceptual framework originally developed in the context of collective variable spaces describing rare events in computational chemistry and biology.","In summary, this paper offers a concise survey of diverse approaches for estimating diffusivity from MD trajectories, highlighting challenges and opportunities in this area."],"url":"http://arxiv.org/abs/2402.03285v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 18:21:34","title":"Small area estimation of forest biomass via a two-stage model for continuous zero-inflated data","abstract":"The U.S. Forest Inventory & Analysis Program (FIA) collects data on and monitors the trends of forests in the United States. FIA is increasingly interested in monitoring forest attributes such as biomass at fine geographic and temporal scales, resulting in a need for assessment and development of small area estimation techniques applied to forest inventory contexts. We implement a small area estimator and parametric bootstrap estimator that account for zero-inflation in biomass data via a two-stage model-based approach. We compare the performance of this estimator to the post-stratified estimator and to the unit and area-level empirical best linear unbiased prediction (EBLUP) estimators. We conduct a simulation study with counties in Nevada, U.S. based on actual sampled plot data and remote sensing data products. Results show that the zero-inflation estimator has the lowest relative bias and the smallest empirical root mean square error. Moreover, the 95% confidence interval coverages of the zero-inflation estimator and the unit-level EBLUP are more accurate than the other two estimators. To further illustrate the practical utility, we employ a data application across the 2019 measurement year in Nevada. We introduce the R package, saeczi, which efficiently implements the zero-inflation estimator and its mean square error estimator.","sentences":["The U.S. Forest Inventory & Analysis Program (FIA) collects data on and monitors the trends of forests in the United States.","FIA is increasingly interested in monitoring forest attributes such as biomass at fine geographic and temporal scales, resulting in a need for assessment and development of small area estimation techniques applied to forest inventory contexts.","We implement a small area estimator and parametric bootstrap estimator that account for zero-inflation in biomass data via a two-stage model-based approach.","We compare the performance of this estimator to the post-stratified estimator and to the unit and area-level empirical best linear unbiased prediction (EBLUP) estimators.","We conduct a simulation study with counties in Nevada, U.S. based on actual sampled plot data and remote sensing data products.","Results show that the zero-inflation estimator has the lowest relative bias and the smallest empirical root mean square error.","Moreover, the 95% confidence interval coverages of the zero-inflation estimator and the unit-level EBLUP are more accurate than the other two estimators.","To further illustrate the practical utility, we employ a data application across the 2019 measurement year in Nevada.","We introduce the R package, saeczi, which efficiently implements the zero-inflation estimator and its mean square error estimator."],"url":"http://arxiv.org/abs/2402.03263v1","category":"stat.AP"}
{"created":"2024-02-05 17:37:46","title":"FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion","abstract":"As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.","sentences":["As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples.","Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance.","We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function.","Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories.","Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks.","The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks."],"url":"http://arxiv.org/abs/2402.03226v1","category":"cs.LG"}
{"created":"2024-02-05 17:16:02","title":"Bad Science Matrices","abstract":"Inspired by the bad scientist who keeps repeating an experiment 20 times to get a single outcome with $p < 0.05$, we consider matrices $A \\in \\mathbb{R}^{n \\times n}$ whose columns are normalized in $\\ell^2$ and for which $2^{-n}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}}$ is large. They correspond to affine transformations of the discrete unit cube to points with, on average, at least one large coordinate. Such matrices can be seen as a collection of fair tests on a fair coin where at least one outcome is typically atypical. We prove that, as $n \\rightarrow \\infty$, the quantity can scale as   $$ \\max_{A \\in \\mathbb{R}^{n \\times n}} \\frac{1}{2^{n}}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}} = (1+o(1)) \\cdot \\sqrt{2\\log{n}}.$$ We also present candidate maximizers up to dimension $n \\leq 8$ which appear to be highly structured and have nice closed-form solutions.","sentences":["Inspired by the bad scientist who keeps repeating an experiment 20 times to get a single outcome with $p < 0.05$, we consider matrices $A \\in \\mathbb{R}^{n \\times n}$ whose columns are normalized in $\\ell^2$ and for which $2^{-n}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}}$ is large.","They correspond to affine transformations of the discrete unit cube to points with, on average, at least one large coordinate.","Such matrices can be seen as a collection of fair tests on a fair coin where at least one outcome is typically atypical.","We prove that, as $n \\rightarrow \\infty$, the quantity can scale as   $$ \\max_{A \\in \\mathbb{R}^{n \\times n}} \\frac{1}{2^{n}}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}} = (1+o(1))","\\cdot \\sqrt{2\\log{n}}.$$","We also present candidate maximizers up to dimension $n \\leq 8$ which appear to be highly structured and have nice closed-form solutions."],"url":"http://arxiv.org/abs/2402.03205v1","category":"math.FA"}
{"created":"2024-02-05 16:24:12","title":"DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation","abstract":"This paper introduces DogSurf - a newapproach of using quadruped robots to help visually impaired people navigate in real world. The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop. A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots. A dataset was collected on a Unitree Go1 Edu robot. The dataset and code have been posted to the public domain.","sentences":["This paper introduces DogSurf - a newapproach of using quadruped robots to help visually impaired people navigate in real world.","The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop.","A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots.","A dataset was collected on a Unitree Go1 Edu robot.","The dataset and code have been posted to the public domain."],"url":"http://arxiv.org/abs/2402.03156v1","category":"cs.RO"}
{"created":"2024-02-05 16:13:54","title":"Detecting Scams Using Large Language Models","abstract":"Large Language Models (LLMs) have gained prominence in various applications, including security. This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity. Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams. We present notable security applications of LLMs and discuss the unique challenges posed by scams. Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems. Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails. The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks. The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats.","sentences":["Large Language Models (LLMs) have gained prominence in various applications, including security.","This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity.","Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams.","We present notable security applications of LLMs and discuss the unique challenges posed by scams.","Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems.","Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails.","The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks.","The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats."],"url":"http://arxiv.org/abs/2402.03147v1","category":"cs.CR"}
{"created":"2024-02-05 15:36:03","title":"Perceived Vulnerability to Disease Scale: Factorial structure, reliability, and validity in times of Portugal's COVID-19 pandemic lockdown","abstract":"The present study examines the factor structure of a Portuguese version of the Perceived Vulnerability to Disease Scale (PVD), designed to assess individual differences in chronic concerns about transmission of infectious diseases. Method: Data from a Portuguese convenience sample (n=1203), collected during the first Covid-19 pandemic lockdown. Results: the scale revealed, through an exploratory factor analysis (EFA) and a confirmatory factor analysis (CFA), a slight superiority of a three-factor model over the existing two-factor models of the 15-item original PVD and of the 10-item PVD established with another Portuguese sample (Ferreira et al., 2022). Conclusions: This higher level of differentiation in terms of a perceived resistance to infectious diseases could be explained by the pandemic context which may have differentiated the responses regarding the perception of Resistance. On the other hand, this new factor increases the comprehensive and evaluative dimension and implications of the construct assessed by PVD.","sentences":["The present study examines the factor structure of a Portuguese version of the Perceived Vulnerability to Disease Scale (PVD), designed to assess individual differences in chronic concerns about transmission of infectious diseases.","Method: Data from a Portuguese convenience sample (n=1203), collected during the first Covid-19 pandemic lockdown.","Results: the scale revealed, through an exploratory factor analysis (EFA) and a confirmatory factor analysis (CFA), a slight superiority of a three-factor model over the existing two-factor models of the 15-item original PVD and of the 10-item PVD established with another Portuguese sample (Ferreira et al., 2022).","Conclusions: This higher level of differentiation in terms of a perceived resistance to infectious diseases could be explained by the pandemic context which may have differentiated the responses regarding the perception of Resistance.","On the other hand, this new factor increases the comprehensive and evaluative dimension and implications of the construct assessed by PVD."],"url":"http://arxiv.org/abs/2402.03108v1","category":"stat.AP"}
{"created":"2024-02-05 15:33:34","title":"Reply to: Effects of density and temperature variations on the metallicity of Mrk 71","abstract":"In Chen et al., 2023 (C23; arXiv:2304.09898), we introduced a new method to directly measure temperature fluctuations and applied it to a nearby dwarf galaxy, Mrk 71, finding a temperature fluctuation parameter $t^2 = 0.008\\pm 0.043$. This result is lower by $\\sim 2\\sigma$ than the value required to explain the abundance discrepancy (AD) in this object. In the Matters Arising article submitted by Mendez-Delgado et al. (arXiv:2310.01197), the authors claim that using the same data presented in C23 in a different way, it is possible to conclude that the measurements are consistent with a larger $t^2 \\simeq 0.1$ inferred indirectly from recombination lines (RLs). However, this requires a higher density such that the infrared [O III] 52 $\\mu$m and [O III] 88 $\\mu$m lines -- which form the basis of the direct measurement method -- are mutually inconsistent. Moreover, to reach agreement between the direct $t^2$ measurement and the larger $t^2$ value inferred from RLs requires systematically varying four parameters by $\\sim 1\\sigma$ from their best-determined values, which collectively amount to a $\\sim2\\sigma$ difference, consistent with the significance ($\\sim 2 \\sigma$) originally reported in C23. Therefore, we conclude that the results of C23 hold, and that the combined optical and infrared [O III] data disfavour $t^2 \\simeq 0.1$ at the $\\approx2\\sigma$ level in Mrk 71. Future work is nonetheless warranted to better understand the AD associated with both optical and infrared emission line analysis.","sentences":["In Chen et al., 2023 (C23; arXiv:2304.09898), we introduced a new method to directly measure temperature fluctuations and applied it to a nearby dwarf galaxy, Mrk 71, finding a temperature fluctuation parameter $t^2 = 0.008\\pm 0.043$. This result is lower by $\\sim 2\\sigma$ than the value required to explain the abundance discrepancy (AD) in this object.","In the Matters Arising article submitted by Mendez-Delgado et al. (arXiv:2310.01197), the authors claim that using the same data presented in C23 in a different way, it is possible to conclude that the measurements are consistent with a larger $t^2 \\simeq 0.1$ inferred indirectly from recombination lines (RLs).","However, this requires a higher density such that the infrared [O III] 52 $\\mu$m and [O III] 88","$\\mu$m lines -- which form the basis of the direct measurement method -- are mutually inconsistent.","Moreover, to reach agreement between the direct $t^2$ measurement and the larger $t^2$ value inferred from RLs requires systematically varying four parameters by $\\sim 1\\sigma$ from their best-determined values, which collectively amount to a $\\sim2\\sigma$ difference, consistent with the significance ($\\sim 2 \\sigma$) originally reported in C23.","Therefore, we conclude that the results of C23 hold, and that the combined optical and infrared [O III] data disfavour $t^2 \\simeq 0.1$ at the $\\approx2\\sigma$ level in Mrk 71.","Future work is nonetheless warranted to better understand the AD associated with both optical and infrared emission line analysis."],"url":"http://arxiv.org/abs/2402.03105v1","category":"astro-ph.GA"}
{"created":"2024-02-05 14:45:07","title":"How many vector charmonium(-like) states sit in the energy range from $4.2$ to $4.35$ GeV?","abstract":"In recent years many vector charmonium(-like) states were reported by different electron-positron collider experiments above $4.2$ GeV. However, so far, there not only exists sizable tension in the parameters of those states, but there is also no consensus on the number of the vector states in this energy range. To some extend, this might be caused by the fact that the experimental data were typically analyzed in single channel analyses employing overlapping Breit-Wigner functions, in particular ignoring the effect of opening thresholds. In this study, we focus on the mass range between $4.2$ and $4.35$ GeV, conducting a comprehensive analysis of eight different final states in $e^+ e^-$ annihilation. Our findings demonstrate that, within this mass range, a single vector charmonium-like state, exhibiting properties consistent with a $D_1\\bar D$ molecular structure and characterized by a pole location $\\sqrt{s_\\text{pole}^{Y(4230)}}=\\left( 4227{\\pm} 3 {-} \\frac{i}{2}(50^{+10}_{-5}) \\right) \\text{MeV}$, can effectively describe all the collected data. This is made possible by allowing for an interference with the well-established vector chamonium $\\psi(4160)$ along with the inclusion of the $D_1\\bar D$ threshold effect. Moreover, in contrast to experimental analyses, our study reveals that the highly asymmetric total cross sections for $e^+e^-\\to J/\\psi\\pi\\pi$ and $e^+e^-\\to J/\\psi K\\bar K$ around 4230 MeV stem from the same physics, rooted in the approximate SU(3) flavor symmetry of QCD.","sentences":["In recent years many vector charmonium(-like) states were reported by different electron-positron collider experiments above $4.2$ GeV.","However, so far, there not only exists sizable tension in the parameters of those states, but there is also no consensus on the number of the vector states in this energy range.","To some extend, this might be caused by the fact that the experimental data were typically analyzed in single channel analyses employing overlapping Breit-Wigner functions, in particular ignoring the effect of opening thresholds.","In this study, we focus on the mass range between $4.2$ and $4.35$ GeV, conducting a comprehensive analysis of eight different final states in $e^+ e^-$ annihilation.","Our findings demonstrate that, within this mass range, a single vector charmonium-like state, exhibiting properties consistent with a $D_1\\bar D$ molecular structure and characterized by a pole location $\\sqrt{s_\\text{pole}^{Y(4230)}}=\\left( 4227{\\pm} 3 {-} \\frac{i}{2}(50^{+10}_{-5}) \\right) \\text{MeV}$, can effectively describe all the collected data.","This is made possible by allowing for an interference with the well-established vector chamonium $\\psi(4160)$ along with the inclusion of the $D_1\\bar D$ threshold effect.","Moreover, in contrast to experimental analyses, our study reveals that the highly asymmetric total cross sections for $e^+e^-\\to J/\\psi\\pi\\pi$ and $e^+e^-\\to J/\\psi K\\bar K$ around 4230 MeV stem from the same physics, rooted in the approximate SU(3) flavor symmetry of QCD."],"url":"http://arxiv.org/abs/2402.03057v1","category":"hep-ph"}
{"created":"2024-02-05 13:57:32","title":"Tracing d-d transitions in FePS$_{3}$ on ultrafast time scales","abstract":"Excitations between localized 3d states of transition metal ions within crystalline solids, commonly known as d-d transitions, play a pivotal role in diverse phenomena across solid state physics, materials science, and chemistry. These transitions contribute to the coloration in transition metal oxides, catalytic processes on oxide surfaces, and high-temperature superconductivity. They also couple optical excitation to quantized collective phenomena such as phonons and magnons in magnetic systems. Until now, an experimental method to unravel the complex quasiparticle dynamics associated with d-d transitions has remained elusive. We bridge this gap by demonstrating that d-d transitions can be distinctly traced in momentum space and time using time- and angle-resolved photoelectron spectroscopy (trARPES). Through this approach, we can assign specific momentum-dependent characteristics and elucidate the decay mechanisms of specific d-d transitions in FePS$_{3}$, a two-dimensional van der Waals antiferromagnet with a rich array of quantum phenomena stemming from d-d transitions. This study pioneers the use of ARPES in probing the dynamics of d-d transitions across a wide spectrum of solid-state systems.","sentences":["Excitations between localized 3d states of transition metal ions within crystalline solids, commonly known as d-d transitions, play a pivotal role in diverse phenomena across solid state physics, materials science, and chemistry.","These transitions contribute to the coloration in transition metal oxides, catalytic processes on oxide surfaces, and high-temperature superconductivity.","They also couple optical excitation to quantized collective phenomena such as phonons and magnons in magnetic systems.","Until now, an experimental method to unravel the complex quasiparticle dynamics associated with d-d transitions has remained elusive.","We bridge this gap by demonstrating that d-d transitions can be distinctly traced in momentum space and time using time- and angle-resolved photoelectron spectroscopy (trARPES).","Through this approach, we can assign specific momentum-dependent characteristics and elucidate the decay mechanisms of specific d-d transitions in FePS$_{3}$, a two-dimensional van der Waals antiferromagnet with a rich array of quantum phenomena stemming from d-d transitions.","This study pioneers the use of ARPES in probing the dynamics of d-d transitions across a wide spectrum of solid-state systems."],"url":"http://arxiv.org/abs/2402.03018v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 12:41:30","title":"One-class anomaly detection through color-to-thermal AI for building envelope inspection","abstract":"We present a label-free method for detecting anomalies during thermographic inspection of building envelopes. It is based on the AI-driven prediction of thermal distributions from color images. Effectively the method performs as a one-class classifier of the thermal image regions with high mismatch between the predicted and actual thermal distributions. The algorithm can learn to identify certain features as normal or anomalous by selecting the target sample used for training. We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges. The method can be implemented to assist human professionals during routine building inspections or combined with mobile platforms for automating examination of large areas.","sentences":["We present a label-free method for detecting anomalies during thermographic inspection of building envelopes.","It is based on the AI-driven prediction of thermal distributions from color images.","Effectively the method performs as a one-class classifier of the thermal image regions with high mismatch between the predicted and actual thermal distributions.","The algorithm can learn to identify certain features as normal or anomalous by selecting the target sample used for training.","We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges.","The method can be implemented to assist human professionals during routine building inspections or combined with mobile platforms for automating examination of large areas."],"url":"http://arxiv.org/abs/2402.02963v1","category":"eess.IV"}
{"created":"2024-02-05 12:08:17","title":"Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey","abstract":"The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations. Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and ViTs and their collective impact on shaping the future of CV architectures.","sentences":["The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV).","This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches.","The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations.","Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and ViTs and their collective impact on shaping the future of CV architectures."],"url":"http://arxiv.org/abs/2402.02941v1","category":"cs.CV"}
{"created":"2024-02-05 11:10:27","title":"Bayesian Federated Inference for regression models with heterogeneous multi-center populations","abstract":"To estimate accurately the parameters of a regression model, the sample size must be large enough relative to the number of possible predictors for the model. In practice, sufficient data is often lacking, which can lead to overfitting of the model and, as a consequence, unreliable predictions of the outcome of new patients. Pooling data from different data sets collected in different (medical) centers would alleviate this problem, but is often not feasible due to privacy regulation or logistic problems. An alternative route would be to analyze the local data in the centers separately and combine the statistical inference results with the Bayesian Federated Inference (BFI) methodology. The aim of this approach is to compute from the inference results in separate centers what would have been found if the statistical analysis was performed on the combined data. We explain the methodology under homogeneity and heterogeneity across the populations in the separate centers, and give real life examples for better understanding. Excellent performance of the proposed methodology is shown. An R-package to do all the calculations has been developed and is illustrated in this paper. The mathematical details are given in the Appendix.","sentences":["To estimate accurately the parameters of a regression model, the sample size must be large enough relative to the number of possible predictors for the model.","In practice, sufficient data is often lacking, which can lead to overfitting of the model and, as a consequence, unreliable predictions of the outcome of new patients.","Pooling data from different data sets collected in different (medical) centers would alleviate this problem, but is often not feasible due to privacy regulation or logistic problems.","An alternative route would be to analyze the local data in the centers separately and combine the statistical inference results with the Bayesian Federated Inference (BFI) methodology.","The aim of this approach is to compute from the inference results in separate centers what would have been found if the statistical analysis was performed on the combined data.","We explain the methodology under homogeneity and heterogeneity across the populations in the separate centers, and give real life examples for better understanding.","Excellent performance of the proposed methodology is shown.","An R-package to do all the calculations has been developed and is illustrated in this paper.","The mathematical details are given in the Appendix."],"url":"http://arxiv.org/abs/2402.02898v1","category":"stat.AP"}
{"created":"2024-02-05 09:09:22","title":"Higgs Precision Analysis of the Full LHC Run 1 and Run 2 Data","abstract":"We perform global fits of the Higgs boson couplings to the full Higgs datasets collected at the LHC with the integrated luminosities per experiment of approximately 5/fb at 7 TeV, 20/fb at 8 TeV, and up to 139/fb at 13 TeV. Our combined analysis based on the experimental signal strengths used in this work and the theoretical ones elaborated for our analysis reliably reproduce the results in the literature. We reveal that the LHC Higgs precision data are no longer best described by the SM Higgs boson taking account of extensive and comprehensive CP-conserving and CP-violating scenarios found in several well-motivated models beyond the SM. Especially, in most of the fits considered in this work, we observe that the best-fitted values of the normalized Yukawa couplings are about $2\\sigma$ below the corresponding SM ones with the $1\\sigma$ errors of 3-5%. On the other hand, the gauge-Higgs couplings are consistent with the SM with the $1\\sigma$ errors of 2-3%. Incidentally, the reduced Yukawa couplings help to explain the excess of the $H\\to Z\\gamma$ signal strength of $2.2\\pm 0.7$ recently reported by the ATLAS and CMS collaborations.","sentences":["We perform global fits of the Higgs boson couplings to the full Higgs datasets collected at the LHC with the integrated luminosities per experiment of approximately 5/fb at 7 TeV, 20/fb at 8 TeV, and up to 139/fb at 13 TeV. Our combined analysis based on the experimental signal strengths used in this work and the theoretical ones elaborated for our analysis reliably reproduce the results in the literature.","We reveal that the LHC Higgs precision data are no longer best described by the SM Higgs boson taking account of extensive and comprehensive CP-conserving and CP-violating scenarios found in several well-motivated models beyond the SM.","Especially, in most of the fits considered in this work, we observe that the best-fitted values of the normalized Yukawa couplings are about $2\\sigma$ below the corresponding SM ones with the $1\\sigma$ errors of 3-5%.","On the other hand, the gauge-Higgs couplings are consistent with the SM with the $1\\sigma$ errors of 2-3%.","Incidentally, the reduced Yukawa couplings help to explain the excess of the $H\\to Z\\gamma$ signal strength of $2.2\\pm 0.7$ recently reported by the ATLAS and CMS collaborations."],"url":"http://arxiv.org/abs/2402.02822v1","category":"hep-ph"}
{"created":"2024-02-05 08:26:33","title":"Graph-enhanced Large Language Models in Asynchronous Plan Reasoning","abstract":"Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.","sentences":["Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs.","Can large language models (LLMs) succeed at this task?","Here, we present the first large-scale study investigating this question.","We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow.","We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results.","We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices.","We see our study as an exciting step towards using LLMs as efficient autonomous agents."],"url":"http://arxiv.org/abs/2402.02805v1","category":"cs.AI"}
{"created":"2024-02-05 08:25:22","title":"Large Language Model Distilling Medication Recommendation Model","abstract":"The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function. Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.","sentences":["The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs.","Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities.","Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon.","To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs).","Our research aims to transform existing medication recommendation methodologies using LLMs.","In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER).","We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively.","However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs.","We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function.","Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector.","To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model.","Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient.","To ease the reproducibility of our experiments, we release the implementation code online."],"url":"http://arxiv.org/abs/2402.02803v1","category":"cs.IR"}
{"created":"2024-02-05 08:19:56","title":"KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models","abstract":"The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embedding of LLaMA suffices to reach the fine-tuning translation performance. Code and model will be released to the public.","sentences":["The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network.","Do winning tickets exist for LLMs in fine-tuning scenarios?","How can we find such winning tickets?","In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning.","Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning.","We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning.","Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM.","Surprisingly, we find that fine-tuning 18 tokens' embedding of LLaMA suffices to reach the fine-tuning translation performance.","Code and model will be released to the public."],"url":"http://arxiv.org/abs/2402.02801v1","category":"cs.CL"}
{"created":"2024-02-05 07:59:38","title":"Rethinking Optimization and Architecture for Tiny Language Models","abstract":"The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas. Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\\pi$-1B Pro. Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance. The code will be released soon (https://github.com/YuchuanTian/RethinkTinyLM).","sentences":["The power of large language models (LLMs) has been demonstrated through numerous data and computing resources.","However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required.","Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully.","In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component.","Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy.","Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training.","Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas.","Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\\pi$-1B Pro.","Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance.","The code will be released soon (https://github.com/YuchuanTian/RethinkTinyLM)."],"url":"http://arxiv.org/abs/2402.02791v1","category":"cs.CL"}
{"created":"2024-02-05 07:52:04","title":"Artificial-intelligence-based surrogate solution of dissipative quantum dynamics: physics-informed reconstruction of the universal propagator","abstract":"The accurate (or even approximate) solution of the equations that govern the dynamics of dissipative quantum systems remains a challenging task for quantum science. While several algorithms have been designed to solve those equations with different degrees of flexibility, they rely mainly on highly expensive iterative schemes. Most recently, deep neural networks have been used for quantum dynamics but current architectures are highly dependent on the physics of the particular system and usually limited to population dynamics. Here we introduce an artificial-intelligence-based surrogate model that solves dissipative quantum dynamics by parameterizing quantum propagators as Fourier neural operators, which we train using both dataset and physics-informed loss functions. Compared with conventional algorithms, our quantum neural propagator avoids time-consuming iterations and provides a universal super-operator that can be used to evolve any initial quantum state for arbitrarily long times. To illustrate the wide applicability of the approach, we employ our quantum neural propagator to compute population dynamics and time-correlation functions of the Fenna-Matthews-Olson complex.","sentences":["The accurate (or even approximate) solution of the equations that govern the dynamics of dissipative quantum systems remains a challenging task for quantum science.","While several algorithms have been designed to solve those equations with different degrees of flexibility, they rely mainly on highly expensive iterative schemes.","Most recently, deep neural networks have been used for quantum dynamics but current architectures are highly dependent on the physics of the particular system and usually limited to population dynamics.","Here we introduce an artificial-intelligence-based surrogate model that solves dissipative quantum dynamics by parameterizing quantum propagators as Fourier neural operators, which we train using both dataset and physics-informed loss functions.","Compared with conventional algorithms, our quantum neural propagator avoids time-consuming iterations and provides a universal super-operator that can be used to evolve any initial quantum state for arbitrarily long times.","To illustrate the wide applicability of the approach, we employ our quantum neural propagator to compute population dynamics and time-correlation functions of the Fenna-Matthews-Olson complex."],"url":"http://arxiv.org/abs/2402.02788v1","category":"quant-ph"}
{"created":"2024-02-05 07:30:32","title":"Dual Knowledge Distillation for Efficient Sound Event Detection","abstract":"Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowledge distillation having merely one-third of the baseline model's parameters, demonstrates superior performance in terms of PSDS1 and PSDS2. This highlights the importance of proposed dual knowledge distillation for compact SED systems, which can be ideal for edge devices.","sentences":["Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals.","This becomes challenging particularly for on-device applications, where computational resources are limited.","To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work.","Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters.","This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation.","Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning.","On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowledge distillation having merely one-third of the baseline model's parameters, demonstrates superior performance in terms of PSDS1 and PSDS2.","This highlights the importance of proposed dual knowledge distillation for compact SED systems, which can be ideal for edge devices."],"url":"http://arxiv.org/abs/2402.02781v1","category":"cs.SD"}
{"created":"2024-02-05 07:20:38","title":"Chiral switching of many-body steady states in a dissipative Rydberg gas","abstract":"Dissipative Rydberg gases are an outstanding platform for the investigation of many-body quantum open systems. Despite the wealth of existing studies, the non-equilibrium dynamics of dissipative Rydberg gases are rarely examined or harnessed from the perspective of non-Hermitian physics, which is but intrinsic to open systems. Here we report the experimental observation of a chiral switching between many-body steady states in a dissipative thermal Rydberg vapor, where the interplay of many-body effects and non-Hermiticity plays a key role. Specifically, as the parameters are adiabatically varied around a closed contour, depending on the chirality of the parameter modulation, the Rydberg vapor can change between two collective steady states with distinct Rydberg excitations and optical transmissions. Adopting a mean-field description, we reveal that both the existence of the bistable steady states and chiral dynamics derive from an exceptional structure in the parameter space, where multiple steady states of the many-body Liouvillian superoperator coalesce. We demonstrate that both the exceptional structure and the resulting state-switching dynamics are tunable through microwave dressing and temperature variations, confirming their reliance on the many-body dissipative nature of the Rydberg vapor.","sentences":["Dissipative Rydberg gases are an outstanding platform for the investigation of many-body quantum open systems.","Despite the wealth of existing studies, the non-equilibrium dynamics of dissipative Rydberg gases are rarely examined or harnessed from the perspective of non-Hermitian physics, which is but intrinsic to open systems.","Here we report the experimental observation of a chiral switching between many-body steady states in a dissipative thermal Rydberg vapor, where the interplay of many-body effects and non-Hermiticity plays a key role.","Specifically, as the parameters are adiabatically varied around a closed contour, depending on the chirality of the parameter modulation, the Rydberg vapor can change between two collective steady states with distinct Rydberg excitations and optical transmissions.","Adopting a mean-field description, we reveal that both the existence of the bistable steady states and chiral dynamics derive from an exceptional structure in the parameter space, where multiple steady states of the many-body Liouvillian superoperator coalesce.","We demonstrate that both the exceptional structure and the resulting state-switching dynamics are tunable through microwave dressing and temperature variations, confirming their reliance on the many-body dissipative nature of the Rydberg vapor."],"url":"http://arxiv.org/abs/2402.02779v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-05 07:05:17","title":"Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate","abstract":"Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into the swamp of complex patterns in data, making LoT a valuable addition to the current machine learning frameworks.","sentences":["Generalization remains a central challenge in machine learning.","In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization.","Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach.","LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners.","The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback.","Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data.","It suggests the effectiveness of LoT in identifying generalizable information without falling into the swamp of complex patterns in data, making LoT a valuable addition to the current machine learning frameworks."],"url":"http://arxiv.org/abs/2402.02769v1","category":"cs.LG"}
{"created":"2024-02-05 07:02:43","title":"Intent Profiling and Translation Through Emergent Communication","abstract":"To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution. In intent-based methods, users and applications express their intent in a high-level abstract language to the network. Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications' intents and map them to different network capabilities. Therefore, in this work, we propose an AI-based framework for intent profiling and translation. We consider a scenario where applications interacting with the network express their needs for network services in their domain language. The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable. Instead, a framework based on emergent communication is proposed for intent profiling, in which applications express their abstract quality-of-experience (QoE) intents to the network through emergent communication messages. Subsequently, the network learns how to interpret these communication messages and map them to network capabilities (i.e., slices) to guarantee the requested Quality-of-Service (QoS). Simulation results show that the proposed method outperforms self-learning slicing and other baselines, and achieves a performance close to the perfect knowledge baseline.","sentences":["To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution.","In intent-based methods, users and applications express their intent in a high-level abstract language to the network.","Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications' intents and map them to different network capabilities.","Therefore, in this work, we propose an AI-based framework for intent profiling and translation.","We consider a scenario where applications interacting with the network express their needs for network services in their domain language.","The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable.","Instead, a framework based on emergent communication is proposed for intent profiling, in which applications express their abstract quality-of-experience (QoE) intents to the network through emergent communication messages.","Subsequently, the network learns how to interpret these communication messages and map them to network capabilities (i.e., slices) to guarantee the requested Quality-of-Service (QoS).","Simulation results show that the proposed method outperforms self-learning slicing and other baselines, and achieves a performance close to the perfect knowledge baseline."],"url":"http://arxiv.org/abs/2402.02768v1","category":"cs.IT"}
{"created":"2024-02-05 06:52:53","title":"List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation","abstract":"The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates reranking and truncation via generative paradigm based on encoder-decoder architecture. We also design the novel loss functions for joint optimization to make the model learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q\\&A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs.","sentences":["The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs).","List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation.","Reranking finely re-scores the documents in the list.","Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents.","Previous studies treat them as two separate tasks and model them separately.","However, the separation is not optimal.","First, it is hard to share the contextual information of the ranking list between the two tasks.","Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage.","To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently.","GenRT integrates reranking and truncation via generative paradigm based on encoder-decoder architecture.","We also design the novel loss functions for joint optimization to make the model learn both tasks.","Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks.","Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages.","Experiments on public learning-to-rank benchmarks and open-domain Q\\&A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs."],"url":"http://arxiv.org/abs/2402.02764v1","category":"cs.IR"}
{"created":"2024-02-05 05:25:33","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","abstract":"Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.","sentences":["Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming.","Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors.","However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially.","This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies.","In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step.","We leverage existing face re-aging and style transfer networks, both trained within the same PR domain.","Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance.","Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain.","This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization.","Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability."],"url":"http://arxiv.org/abs/2402.02733v1","category":"cs.CV"}
{"created":"2024-02-05 05:22:58","title":"A Generative Approach to Surrogate-based Black-box Attacks","abstract":"Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs. These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples. State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs. The goal is to learn the decision boundaries of the target. The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes. With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates. Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries. The distribution learned by the generative surrogate can be used to craft adversarial examples that have imperceptible differences from the original samples but belong to other classes. The proposed generative approach results in attacks with remarkably high attack success rates on various targets and datasets.","sentences":["Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs.","These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples.","State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs.","The goal is to learn the decision boundaries of the target.","The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes.","With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates.","Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries.","The distribution learned by the generative surrogate can be used to craft adversarial examples that have imperceptible differences from the original samples but belong to other classes.","The proposed generative approach results in attacks with remarkably high attack success rates on various targets and datasets."],"url":"http://arxiv.org/abs/2402.02732v1","category":"cs.LG"}
{"created":"2024-02-05 04:28:08","title":"Denoising Time Cycle Modeling for Recommendation","abstract":"Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems. We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for recommendation. Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods.","sentences":["Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems.","We argue that existing methods ignore the variety of temporal patterns of user behaviors.","We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance.","In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item.","DiCycle is able to explicitly model diverse time cycle patterns for recommendation.","Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods."],"url":"http://arxiv.org/abs/2402.02718v1","category":"cs.IR"}
{"created":"2024-02-05 04:25:24","title":"Understanding the planning of LLM agents: A survey","abstract":"As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.","sentences":["As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention.","This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability.","We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory.","Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed."],"url":"http://arxiv.org/abs/2402.02716v1","category":"cs.AI"}
{"created":"2024-02-05 04:17:49","title":"Position Paper: What Can Large Language Models Tell Us about Time Series Analysis","abstract":"Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.","sentences":["Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications.","Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase.","Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks.","In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence.","Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering.","We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts.","Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research."],"url":"http://arxiv.org/abs/2402.02713v1","category":"cs.LG"}
{"created":"2024-02-05 03:39:39","title":"Representation Surgery for Multi-Task Model Merging","abstract":"Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called \"Surgery\" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model. We then designed an unsupervised optimization objective that updates the Surgery module by minimizing the distance between the merged model's representation and the individual model's representation. Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes.","sentences":["Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization.","Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL.","However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias.","That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL.","In this paper, we propose a representation surgery solution called \"Surgery\" to reduce representation bias in the merged model.","Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model.","We then designed an unsupervised optimization objective that updates the Surgery module by minimizing the distance between the merged model's representation and the individual model's representation.","Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes."],"url":"http://arxiv.org/abs/2402.02705v1","category":"cs.LG"}
{"created":"2024-02-05 03:27:52","title":"Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence","abstract":"Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DMControl Generalization Benchmark (DMC-GB).","sentences":["Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL).","In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment.","Many practical algorithms are proposed to handle this problem.","However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work.","In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors.","Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap.","Our theoretical results are supported by the empirical evidence in the DMControl Generalization Benchmark (DMC-GB)."],"url":"http://arxiv.org/abs/2402.02701v1","category":"cs.LG"}
{"created":"2024-02-05 03:21:23","title":"Beyond Expectations: Learning with Stochastic Dominance Made Practical","abstract":"Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\\textbf{i)}$, the original concept of stochastic dominance only provides a $\\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning.   In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a simple and computationally efficient approach for finding the optimal solution in terms of stochastic dominance, which can be seamlessly plugged into many learning tasks. Numerical experiments demonstrate that the proposed method achieves comparable performance as standard risk-neutral strategies and obtains better trade-offs against risk across a variety of applications including supervised learning, reinforcement learning, and portfolio optimization.","sentences":["Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations.","Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\\textbf{i)}$, the original concept of stochastic dominance only provides a $\\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning.   ","In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance.","We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables.","We next develop a simple and computationally efficient approach for finding the optimal solution in terms of stochastic dominance, which can be seamlessly plugged into many learning tasks.","Numerical experiments demonstrate that the proposed method achieves comparable performance as standard risk-neutral strategies and obtains better trade-offs against risk across a variety of applications including supervised learning, reinforcement learning, and portfolio optimization."],"url":"http://arxiv.org/abs/2402.02698v1","category":"cs.LG"}
{"created":"2024-02-05 03:20:28","title":"Causal Feature Selection for Responsible Machine Learning","abstract":"Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in high-stakes applications.","sentences":["Machine Learning (ML) has become an integral aspect of many real-world applications.","As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness.","Responsible ML involves many issues.","This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization.","Feature selection plays a pivotal role in the responsible ML tasks.","However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance.","This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML.","By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in high-stakes applications."],"url":"http://arxiv.org/abs/2402.02696v1","category":"cs.LG"}
{"created":"2024-02-05 03:15:26","title":"Exploiting Class Probabilities for Black-box Sentence-level Attacks","abstract":"Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.","sentences":["Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers.","Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities.","Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels.","Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks.","We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets."],"url":"http://arxiv.org/abs/2402.02695v1","category":"cs.CL"}
{"created":"2024-02-05 03:00:08","title":"ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module ensuring easy portability and remote tracking of operational logistics to the last mile","abstract":"The COVID-19 pandemic has profoundly reshaped our lives, prompting a search for solutions to its far-reaching effects. Vaccines emerged as a beacon of hope, yet reaching remote areas faces last-mile hurdles and cost issues due to loss of vaccine potency due to poor temperature regulation of the storage units and unanticipated vaccine wastage en route, a common occurrence in conventional vaccine transportation methods. We introduce ALIVE, a low-cost Interactive Vaccine Storage Environment module. ALIVE provides an off-grid, self-sufficient solution for vaccine storage and transport, enabled by active cooling technology. ALIVE's innovation lies in its integration with the Internet of Things (IoT), allowing real-time monitoring and control. This IoT-enabled Application Programming Interface (API) features a data acquisition and environment parameter control system, managing oversight and decision-making. ALIVE's compact, lightweight design makes it adaptable to various logistical scenarios, while its versatility enables it to maintain both time-invariant and time-dependent thermophysical and spatial parameters. Operationalized through a PID algorithm, ALIVE ensures precise temperature control within the vaccine chamber. Its dynamic features, such as remote actuation and data sharing, demonstrate its adaptability and potential applications. Despite the frugal nature of development, the system promises significant benefits, including reduced vaccine loss and remote monitoring advantages. Collaborations with healthcare partners seek to further enhance ALIVE's readiness and expand its impact. ALIVE revolutionizes vaccine logistics, offering scalable, cost-effective solutions for bridging accessibility gaps in challenging distribution scenarios. Its adaptability positions it for widespread application, from last-mile vaccine delivery to environment-controlled supply chains and beyond.","sentences":["The COVID-19 pandemic has profoundly reshaped our lives, prompting a search for solutions to its far-reaching effects.","Vaccines emerged as a beacon of hope, yet reaching remote areas faces last-mile hurdles and cost issues due to loss of vaccine potency due to poor temperature regulation of the storage units and unanticipated vaccine wastage en route, a common occurrence in conventional vaccine transportation methods.","We introduce ALIVE, a low-cost Interactive Vaccine Storage Environment module.","ALIVE provides an off-grid, self-sufficient solution for vaccine storage and transport, enabled by active cooling technology.","ALIVE's innovation lies in its integration with the Internet of Things (IoT), allowing real-time monitoring and control.","This IoT-enabled Application Programming Interface (API) features a data acquisition and environment parameter control system, managing oversight and decision-making.","ALIVE's compact, lightweight design makes it adaptable to various logistical scenarios, while its versatility enables it to maintain both time-invariant and time-dependent thermophysical and spatial parameters.","Operationalized through a PID algorithm, ALIVE ensures precise temperature control within the vaccine chamber.","Its dynamic features, such as remote actuation and data sharing, demonstrate its adaptability and potential applications.","Despite the frugal nature of development, the system promises significant benefits, including reduced vaccine loss and remote monitoring advantages.","Collaborations with healthcare partners seek to further enhance ALIVE's readiness and expand its impact.","ALIVE revolutionizes vaccine logistics, offering scalable, cost-effective solutions for bridging accessibility gaps in challenging distribution scenarios.","Its adaptability positions it for widespread application, from last-mile vaccine delivery to environment-controlled supply chains and beyond."],"url":"http://arxiv.org/abs/2402.02691v1","category":"eess.SY"}
{"created":"2024-02-05 02:54:50","title":"Poisson Process for Bayesian Optimization","abstract":"BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian process (GP). However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO). Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it. Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified by abundant experiments. The results on both simulated and real-world benchmarks, including hyperparameter optimization (HPO) and neural architecture search (NAS), show the effectiveness of PoPBO.","sentences":["BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian process (GP).","However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired.","To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO).","Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it.","Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified by abundant experiments.","The results on both simulated and real-world benchmarks, including hyperparameter optimization (HPO) and neural architecture search (NAS), show the effectiveness of PoPBO."],"url":"http://arxiv.org/abs/2402.02687v1","category":"cs.LG"}
{"created":"2024-02-05 02:35:11","title":"Equivariant Symmetry Breaking Sets","abstract":"Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the size of these sets equates to data efficiency. We prove that minimizing these sets translates to a well studied group theory problem, and tabulate solutions to this problem for the point groups. Finally, we provide some examples of symmetry breaking to demonstrate how our approach works in practice.","sentences":["Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries.","By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input.","However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one.","Hence, it is imperative that we understand how to systematically break symmetry in ENNs.","In this work, we propose a novel symmetry breaking framework that is fully equivariant.","We emphasize that our approach is general and applicable to equivariance under any group.","To achieve this, we introduce the idea of symmetry breaking sets (SBS).","Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs.","We show there is a natural way to define equivariance on these sets, which gives an additional constraint.","Minimizing the size of these sets equates to data efficiency.","We prove that minimizing these sets translates to a well studied group theory problem, and tabulate solutions to this problem for the point groups.","Finally, we provide some examples of symmetry breaking to demonstrate how our approach works in practice."],"url":"http://arxiv.org/abs/2402.02681v1","category":"cs.LG"}
{"created":"2024-02-05 02:32:09","title":"Large Language Models are Geographically Biased","abstract":"Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs.","sentences":["Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm.","As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy.","We propose to study what LLMs know about the world we live in through the lens of geography.","This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion.","We show various problematic geographic biases, which we define as systemic errors in geospatial predictions.","Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89).","We then show that LLMs exhibit common biases across a range of objective and subjective topics.","In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70).","Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs."],"url":"http://arxiv.org/abs/2402.02680v1","category":"cs.CL"}
{"created":"2024-02-05 02:26:24","title":"Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating","abstract":"Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model. Furthermore, explanatory scores were estimated based on counterfactual probabilities. Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph. Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown.","sentences":["Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions.","Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features.","The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known.","However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases.","Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known.","This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model.","Furthermore, explanatory scores were estimated based on counterfactual probabilities.","Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph.","Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan.","We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown."],"url":"http://arxiv.org/abs/2402.02678v1","category":"cs.LG"}
{"created":"2024-02-05 02:21:11","title":"Verifiable evaluations of machine learning models using zkSNARKs","abstract":"In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a new transparency paradigm in the verifiable evaluation of private models.","sentences":["In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value.","These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs.","This work presents a method of verifiable model evaluation using model inference through zkSNARKs.","The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs.","These verifiable attestations can be performed on any standard neural network model with varying compute requirements.","For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions.","This presents a new transparency paradigm in the verifiable evaluation of private models."],"url":"http://arxiv.org/abs/2402.02675v1","category":"cs.LG"}
{"created":"2024-02-05 02:18:56","title":"A Unified Framework of Multi-Stage Multi-Winner Voting: An Axiomatic Exploration","abstract":"Multi-winner voting plays a crucial role in selecting representative committees based on voter preferences. Previous research has predominantly focused on single-stage voting rules, which are susceptible to manipulation during preference collection. In order to mitigate manipulation and increase the cost associated with it, we propose the introduction of multiple stages in the voting procedure, leading to the development of a unified framework of multi-stage multi-winner voting rules. To shed light on this framework of voting methods, we conduct an axiomatic study, establishing provable conditions for achieving desired axioms within our model. Our theoretical findings can serve as a guide for the selection of appropriate multi-stage multi-winner voting rules.","sentences":["Multi-winner voting plays a crucial role in selecting representative committees based on voter preferences.","Previous research has predominantly focused on single-stage voting rules, which are susceptible to manipulation during preference collection.","In order to mitigate manipulation and increase the cost associated with it, we propose the introduction of multiple stages in the voting procedure, leading to the development of a unified framework of multi-stage multi-winner voting rules.","To shed light on this framework of voting methods, we conduct an axiomatic study, establishing provable conditions for achieving desired axioms within our model.","Our theoretical findings can serve as a guide for the selection of appropriate multi-stage multi-winner voting rules."],"url":"http://arxiv.org/abs/2402.02673v1","category":"cs.GT"}
{"created":"2024-02-05 01:10:51","title":"GraphRT: A graph-based deep learning model for predicting the retention time of peptides","abstract":"GraphRT is a graph based deep learning model that predicts the retention time (RT) of peptides in liquid chromatography tandem mass spectrometry (LC MSMS) experiments. Each amino acid is represented as a graph, capturing its atomic and structural properties through a graph neural network. This enables the model to understand not just the chemical composition of each amino acid, but also the intricate relationships between its atoms. The sequential context of the peptide the order and interaction of amino acids in the sequence is then encoded using recurrent neural networks. This dual approach of graph based and sequential modelling allows for a comprehensive understanding of both the individual characteristics of amino acids and their collective behaviour in a peptide sequence. GraphRT outperforms all current state of the art models and can predict retention time for peptides containing unseen modifications.","sentences":["GraphRT is a graph based deep learning model that predicts the retention time (RT) of peptides in liquid chromatography tandem mass spectrometry (LC MSMS) experiments.","Each amino acid is represented as a graph, capturing its atomic and structural properties through a graph neural network.","This enables the model to understand not just the chemical composition of each amino acid, but also the intricate relationships between its atoms.","The sequential context of the peptide the order and interaction of amino acids in the sequence is then encoded using recurrent neural networks.","This dual approach of graph based and sequential modelling allows for a comprehensive understanding of both the individual characteristics of amino acids and their collective behaviour in a peptide sequence.","GraphRT outperforms all current state of the art models and can predict retention time for peptides containing unseen modifications."],"url":"http://arxiv.org/abs/2402.02661v1","category":"q-bio.BM"}
{"created":"2024-02-05 00:57:51","title":"Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision","abstract":"Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier). Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models.","sentences":["Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving.","In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation.","MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions.","Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work.","Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier).","Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models."],"url":"http://arxiv.org/abs/2402.02658v1","category":"cs.AI"}
{"created":"2024-02-05 00:56:30","title":"RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews","abstract":"Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations. In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using LLMs to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research.","sentences":["Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences.","Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations.","In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes.","We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis.","RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%).","Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements.","Our study highlights the opportunities and challenges in using LLMs to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research."],"url":"http://arxiv.org/abs/2402.02656v1","category":"cs.CL"}
{"created":"2024-02-05 00:48:56","title":"Vision-Language Models Provide Promptable Representations for Reinforcement Learning","abstract":"Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings.","sentences":["Humans can quickly learn new behaviors by leveraging background world knowledge.","In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch.","We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL.","We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information.","We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat.","We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings.","We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings."],"url":"http://arxiv.org/abs/2402.02651v1","category":"cs.LG"}
{"created":"2024-02-05 00:44:28","title":"Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses","abstract":"Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the other hand, to mitigate the effects of the aforementioned inconsistencies, we present a novel method of recursively revising the initial incorrect reasoning provided by the LLM by repetitively breaking down each incorrect step into smaller individual problems.","sentences":["Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input.","The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one.","These behaviors decrease the reliability and validity of the responses provided by these models.","In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study.","The CoF system takes in an open-ended multi-step question.","Then, we repetitively provide meaningless feedback requesting another attempt.","Our preliminary experiments show that such feedback only decreases the quality of the response.","On the other hand, to mitigate the effects of the aforementioned inconsistencies, we present a novel method of recursively revising the initial incorrect reasoning provided by the LLM by repetitively breaking down each incorrect step into smaller individual problems."],"url":"http://arxiv.org/abs/2402.02648v1","category":"cs.CL"}
{"created":"2024-02-04 23:42:02","title":"LLM-Enhanced Data Management","abstract":"Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.","sentences":["Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years.","However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context).","Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning).","However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks.","To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy.","LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering.","LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities.","LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions.","We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics.","We also summarize the open research challenges of LLMDB."],"url":"http://arxiv.org/abs/2402.02643v1","category":"cs.DB"}
{"created":"2024-02-04 23:39:03","title":"Object Graph Programming","abstract":"We introduce Object Graph Programming (OGO), which enables reading and modifying an object graph (i.e., the entire state of the object heap) via declarative queries. OGO models the objects and their relations in the heap as an object graph thereby treating the heap as a graph database: each node in the graph is an object (e.g., an instance of a class or an instance of a metadata class) and each edge is a relation between objects (e.g., a field of one object references another object). We leverage Cypher, the most popular query language for graph databases, as OGO's query language. Unlike LINQ, which uses collections (e.g., List) as a source of data, OGO views the entire object graph as a single \"collection\". OGO is ideal for querying collections (just like LINQ), introspecting the runtime system state (e.g., finding all instances of a given class or accessing fields via reflection), and writing assertions that have access to the entire program state. We prototyped OGO for Java in two ways: (a) by translating an object graph into a Neo4j database on which we run Cypher queries, and (b) by implementing our own in-memory graph query engine that directly queries the object heap. We used OGO to rewrite hundreds of statements in large open-source projects into OGO queries. We report our experience and performance of our prototypes.","sentences":["We introduce Object Graph Programming (OGO), which enables reading and modifying an object graph (i.e., the entire state of the object heap) via declarative queries.","OGO models the objects and their relations in the heap as an object graph thereby treating the heap as a graph database: each node in the graph is an object (e.g., an instance of a class or an instance of a metadata class) and each edge is a relation between objects (e.g., a field of one object references another object).","We leverage Cypher, the most popular query language for graph databases, as OGO's query language.","Unlike LINQ, which uses collections (e.g., List) as a source of data, OGO views the entire object graph as a single \"collection\".","OGO is ideal for querying collections (just like LINQ), introspecting the runtime system state (e.g., finding all instances of a given class or accessing fields via reflection), and writing assertions that have access to the entire program state.","We prototyped OGO for Java in two ways: (a) by translating an object graph into a Neo4j database on which we run Cypher queries, and (b) by implementing our own in-memory graph query engine that directly queries the object heap.","We used OGO to rewrite hundreds of statements in large open-source projects into OGO queries.","We report our experience and performance of our prototypes."],"url":"http://arxiv.org/abs/2402.02642v1","category":"cs.SE"}
{"created":"2024-02-04 22:12:29","title":"Enhancing Transformer RNNs with Multiple Temporal Perspectives","abstract":"We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, ensuring consistent efficiency across various sequence lengths. The empirical results and ablation studies included in our research validate the effectiveness of our approach, showcasing improved performance across multiple benchmarks. The code, model weights and datasets are open-sourced at: https://github.com/RazvanDu/TemporalRNNs.","sentences":["We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data.","This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context.","To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state.","Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\\%$ of the original number of parameters.","Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training.","The resulting model maintains linear computational complexity during prompt inference, ensuring consistent efficiency across various sequence lengths.","The empirical results and ablation studies included in our research validate the effectiveness of our approach, showcasing improved performance across multiple benchmarks.","The code, model weights and datasets are open-sourced at: https://github.com/RazvanDu/TemporalRNNs."],"url":"http://arxiv.org/abs/2402.02625v1","category":"cs.LG"}
{"created":"2024-02-04 22:09:28","title":"A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control","abstract":"Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight sets. We conceive a RL agent not to learn in a continuous space but to proactively anticipate upcoming control tasks and to choose the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, context-dependent. Hence, even an untrained RL agent guarantees a safe and optimal performance. Experimental results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front.","sentences":["Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task.","Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights.","However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime.","Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC).","However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states.","To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight sets.","We conceive a RL agent not to learn in a continuous space but to proactively anticipate upcoming control tasks and to choose the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, context-dependent.","Hence, even an untrained RL agent guarantees a safe and optimal performance.","Experimental results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front."],"url":"http://arxiv.org/abs/2402.02624v1","category":"cs.RO"}
{"created":"2024-02-04 21:54:25","title":"Efficient Market Dynamics: Unraveling Informational Efficiency in UK Horse Racing Betting Markets Through Betfair's Time Series Analysis","abstract":"Using Betfair's time series data, an analysis of the United Kingdom (UK) horse racing market reveals an interesting paradox: a market with short tails, rapidly decaying autocorrelations, and no long-term memory. There seems to be a remarkably high level of informational efficiency in betting exchange returns, in contrast to financial assets that are characterized by heavy tails and volatility clustering. The generalized Gaussian unconditional distribution with a light tail point to a market where knowledge is quickly assimilated and reflected in prices. This is further supported by the extremely quick fading of autocorrelations and the absence of gain-loss asymmetry. Therefore, in addition to measuring long-range memory, the Hurst exponent also shows mean reversion, a sign that markets respond quickly to fresh information.","sentences":["Using Betfair's time series data, an analysis of the United Kingdom (UK) horse racing market reveals an interesting paradox: a market with short tails, rapidly decaying autocorrelations, and no long-term memory.","There seems to be a remarkably high level of informational efficiency in betting exchange returns, in contrast to financial assets that are characterized by heavy tails and volatility clustering.","The generalized Gaussian unconditional distribution with a light tail point to a market where knowledge is quickly assimilated and reflected in prices.","This is further supported by the extremely quick fading of autocorrelations and the absence of gain-loss asymmetry.","Therefore, in addition to measuring long-range memory, the Hurst exponent also shows mean reversion, a sign that markets respond quickly to fresh information."],"url":"http://arxiv.org/abs/2402.02623v1","category":"cs.CE"}
{"created":"2024-02-04 20:56:09","title":"PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?","abstract":"Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such challenging problems. We also show how feedback from smaller solved instances can help improve this reasoning ability.","sentences":["Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering.","In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models.","Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku.","These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes.","Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution.","We present PuzzleBench a dataset of 31 such challenging puzzles.","We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark.","In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such challenging problems.","We also show how feedback from smaller solved instances can help improve this reasoning ability."],"url":"http://arxiv.org/abs/2402.02611v1","category":"cs.AI"}
{"created":"2024-02-04 20:27:10","title":"Models of High-Level Computation","abstract":"Classical models of computation have been successful in capturing the very essence of individual computing devices. Although they are useful to understand computability power and limitations in the small, such models are not suitable to study large-scale complex computations. Accordingly, plenty of formalisms have been proposed in the last half century as an attempt to raise the level of abstraction, with the aim of describing not only a single computing device but interactions among a collection of them. In this paper, we encompass such formalisms into a common framework which we refer to as Models of High-Level Computation. We particularly discuss the semantics, some of the key properties, paradigms and future directions of such models.","sentences":["Classical models of computation have been successful in capturing the very essence of individual computing devices.","Although they are useful to understand computability power and limitations in the small, such models are not suitable to study large-scale complex computations.","Accordingly, plenty of formalisms have been proposed in the last half century as an attempt to raise the level of abstraction, with the aim of describing not only a single computing device but interactions among a collection of them.","In this paper, we encompass such formalisms into a common framework which we refer to as Models of High-Level Computation.","We particularly discuss the semantics, some of the key properties, paradigms and future directions of such models."],"url":"http://arxiv.org/abs/2402.02602v1","category":"cs.LO"}
{"created":"2024-02-04 20:23:15","title":"Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach","abstract":"Adversarial Malware Generation (AMG), the generation of adversarial malware variants to strengthen Deep Learning (DL)-based malware detectors has emerged as a crucial tool in the development of proactive cyberdefense. However, the majority of extant works offer subtle perturbations or additions to executable files and do not explore full-file obfuscation. In this study, we show that an open-source encryption tool coupled with a Reinforcement Learning (RL) framework can successfully obfuscate malware to evade state-of-the-art malware detection engines and outperform techniques that use advanced modification methods. Our results show that the proposed method improves the evasion rate from 27%-49% compared to widely-used state-of-the-art reinforcement learning-based methods.","sentences":["Adversarial Malware Generation (AMG), the generation of adversarial malware variants to strengthen Deep Learning (DL)-based malware detectors has emerged as a crucial tool in the development of proactive cyberdefense.","However, the majority of extant works offer subtle perturbations or additions to executable files and do not explore full-file obfuscation.","In this study, we show that an open-source encryption tool coupled with a Reinforcement Learning (RL) framework can successfully obfuscate malware to evade state-of-the-art malware detection engines and outperform techniques that use advanced modification methods.","Our results show that the proposed method improves the evasion rate from 27%-49% compared to widely-used state-of-the-art reinforcement learning-based methods."],"url":"http://arxiv.org/abs/2402.02600v1","category":"cs.CR"}
{"created":"2024-02-04 20:18:23","title":"Synthesizing Follow-Up Drive Data for Enhanced Road Safety in Intelligent Driving Function Systems","abstract":"This study underscores the vital importance of intelligent driving functions in enhancing road safety and driving comfort. Central to our research is the challenge of obtaining sufficient test data for evaluating these functions, especially in high-risk, safety-critical driving scenarios. Such scenarios often suffer from a dearth of available data, primarily due to their inherent complexity and the risks involved.   Addressing this gap, our research introduces a novel methodology designed to create a wide array of diverse and realistic safety-critical driving scenarios. This approach significantly broadens the testing spectrum for driver assistance systems and autonomous vehicle functions. We particularly focus on the follow-up drive scenario due to its high relevance in practical applications. Here, vehicle movements are intricately modeled using kinematic equations, incorporating factors like driver reaction times. We vary parameters to generate a spectrum of plausible driving scenarios.   The utilization of the Difference Space Stopping (DSS) metric is a pivotal element in our research. This metric plays a crucial role in the safety evaluation of follow-up drives, facilitating a more thorough and comprehensive validation process. By doing so, our methodology enhances the reliability and safety assessment of driver assistance and autonomous driving systems, specifically tailored for the most challenging and safety-critical scenarios.","sentences":["This study underscores the vital importance of intelligent driving functions in enhancing road safety and driving comfort.","Central to our research is the challenge of obtaining sufficient test data for evaluating these functions, especially in high-risk, safety-critical driving scenarios.","Such scenarios often suffer from a dearth of available data, primarily due to their inherent complexity and the risks involved.   ","Addressing this gap, our research introduces a novel methodology designed to create a wide array of diverse and realistic safety-critical driving scenarios.","This approach significantly broadens the testing spectrum for driver assistance systems and autonomous vehicle functions.","We particularly focus on the follow-up drive scenario due to its high relevance in practical applications.","Here, vehicle movements are intricately modeled using kinematic equations, incorporating factors like driver reaction times.","We vary parameters to generate a spectrum of plausible driving scenarios.   ","The utilization of the Difference Space Stopping (DSS) metric is a pivotal element in our research.","This metric plays a crucial role in the safety evaluation of follow-up drives, facilitating a more thorough and comprehensive validation process.","By doing so, our methodology enhances the reliability and safety assessment of driver assistance and autonomous driving systems, specifically tailored for the most challenging and safety-critical scenarios."],"url":"http://arxiv.org/abs/2402.02598v1","category":"cs.RO"}
{"created":"2024-02-04 20:00:45","title":"Unified Training of Universal Time Series Forecasting Transformers","abstract":"Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.","sentences":["Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models.","The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks.","However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data.","To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai).","Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models.","Code, model weights, and data will be released."],"url":"http://arxiv.org/abs/2402.02592v1","category":"cs.LG"}
{"created":"2024-02-04 19:54:44","title":"On the performance of phonetic algorithms in microtext normalization","abstract":"User-generated content published on microblogging social networks constitutes a priceless source of information. However, microtexts usually deviate from the standard lexical and grammatical rules of the language, thus making its processing by traditional intelligent systems very difficult. As an answer, microtext normalization consists in transforming those non-standard microtexts into standard well-written texts as a preprocessing step, allowing traditional approaches to continue with their usual processing. Given the importance of phonetic phenomena in non-standard text formation, an essential element of the knowledge base of a normalizer would be the phonetic rules that encode these phenomena, which can be found in the so-called phonetic algorithms.   In this work we experiment with a wide range of phonetic algorithms for the English language. The aim of this study is to determine the best phonetic algorithms within the context of candidate generation for microtext normalization. In other words, we intend to find those algorithms that taking as input non-standard terms to be normalized allow us to obtain as output the smallest possible sets of normalization candidates which still contain the corresponding target standard words. As it will be stated, the choice of the phonetic algorithm will depend heavily on the capabilities of the candidate selection mechanism which we usually find at the end of a microtext normalization pipeline. The faster it can make the right choices among big enough sets of candidates, the more we can sacrifice on the precision of the phonetic algorithms in favour of coverage in order to increase the overall performance of the normalization system.   KEYWORDS: microtext normalization; phonetic algorithm; fuzzy matching; Twitter; texting","sentences":["User-generated content published on microblogging social networks constitutes a priceless source of information.","However, microtexts usually deviate from the standard lexical and grammatical rules of the language, thus making its processing by traditional intelligent systems very difficult.","As an answer, microtext normalization consists in transforming those non-standard microtexts into standard well-written texts as a preprocessing step, allowing traditional approaches to continue with their usual processing.","Given the importance of phonetic phenomena in non-standard text formation, an essential element of the knowledge base of a normalizer would be the phonetic rules that encode these phenomena, which can be found in the so-called phonetic algorithms.   ","In this work we experiment with a wide range of phonetic algorithms for the English language.","The aim of this study is to determine the best phonetic algorithms within the context of candidate generation for microtext normalization.","In other words, we intend to find those algorithms that taking as input non-standard terms to be normalized allow us to obtain as output the smallest possible sets of normalization candidates which still contain the corresponding target standard words.","As it will be stated, the choice of the phonetic algorithm will depend heavily on the capabilities of the candidate selection mechanism which we usually find at the end of a microtext normalization pipeline.","The faster it can make the right choices among big enough sets of candidates, the more we can sacrifice on the precision of the phonetic algorithms in favour of coverage in order to increase the overall performance of the normalization system.   ","KEYWORDS: microtext normalization; phonetic algorithm; fuzzy matching; Twitter;","texting"],"url":"http://arxiv.org/abs/2402.02591v1","category":"cs.CL"}
{"created":"2024-02-04 18:18:57","title":"Search for a heavy neutral lepton that mixes predominantly with the tau neutrino","abstract":"We report a search for a heavy neutral lepton (HNL) that mixes predominantly with $\\nu_\\tau$. The search utilizes data collected with the Belle detector at the KEKB asymmetric energy $e^+ e^-$ collider. The data sample was collected at and just below the center-of-mass energies of the $\\Upsilon(4S)$ and $\\Upsilon(5S)$ resonances and has an integrated luminosity of $915~\\textrm{fb}^{-1}$, corresponding to $(836\\pm 12)\\times 10^6$ $e^+e^\\to\\tau^+\\tau^-$ events. We search for production of the HNL (denoted $N$) in the decay $\\tau^-\\to \\pi^- N$ followed by its decay via $N \\to \\mu^+\\mu^- \\nu_\\tau$. The search focuses on the parameter-space region in which the HNL is long lived, so that the $\\mu^+\\mu^-$ originate from a common vertex that is significantly displaced from the collision point of the KEKB beams. Consistent with the expected background yield, one event is observed in the data sample after application of all the event-selection criteria. We report limits on the mixing parameter of the HNL with the $\\tau$ neutrino as a function of the HNL mass.","sentences":["We report a search for a heavy neutral lepton (HNL) that mixes predominantly with $\\nu_\\tau$. The search utilizes data collected with the Belle detector at the KEKB asymmetric energy $e^+ e^-$ collider.","The data sample was collected at and just below the center-of-mass energies of the $\\Upsilon(4S)$ and $\\Upsilon(5S)$ resonances and has an integrated luminosity of $915~\\textrm{fb}^{-1}$, corresponding to $(836\\pm 12)\\times 10^6$ $e^+e^\\to\\tau^+\\tau^-$ events.","We search for production of the HNL (denoted $N$) in the decay $\\tau^-\\to \\pi^- N$ followed by its decay via $N \\to \\mu^+\\mu^- \\nu_\\tau$. The search focuses on the parameter-space region in which the HNL is long lived, so that the $\\mu^+\\mu^-$ originate from a common vertex that is significantly displaced from the collision point of the KEKB beams.","Consistent with the expected background yield, one event is observed in the data sample after application of all the event-selection criteria.","We report limits on the mixing parameter of the HNL with the $\\tau$ neutrino as a function of the HNL mass."],"url":"http://arxiv.org/abs/2402.02580v1","category":"hep-ex"}
{"created":"2024-02-04 16:45:01","title":"DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models","abstract":"Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on five representative reasoning tasks show that DefInt consistently achieves state-of-the-art reasoning accuracy and solution diversity. More importantly, it substantially reduces the token cost by 49%-79% compared to the second accurate baselines. Specifically, the open-ended tasks have an average 75% token cost reduction. Code repo with all prompts will be released upon publication.","sentences":["Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems.","Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces.","Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs.","By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1.","If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process.","Experiments on five representative reasoning tasks show that DefInt consistently achieves state-of-the-art reasoning accuracy and solution diversity.","More importantly, it substantially reduces the token cost by 49%-79% compared to the second accurate baselines.","Specifically, the open-ended tasks have an average 75% token cost reduction.","Code repo with all prompts will be released upon publication."],"url":"http://arxiv.org/abs/2402.02563v1","category":"cs.CL"}
{"created":"2024-02-04 16:18:01","title":"Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials","abstract":"Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to investigate the final layers of trained model. Then, fine tuned the trained model using iterative null projection. The results shows that model accuracy improved. During experimentation, I observed that size of the probe has affect on the fine tuning process.","sentences":["Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few.","One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.","However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context.","Adversarial and robust testing is performed to ensure the integrity of models output.","But, ambiguity still persists.","In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used.","Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial.","I investigated the model for feature learnt with respect to natural logic.","To achieve the target, I trained task specific probes.","Used these probes to investigate the final layers of trained model.","Then, fine tuned the trained model using iterative null projection.","The results shows that model accuracy improved.","During experimentation, I observed that size of the probe has affect on the fine tuning process."],"url":"http://arxiv.org/abs/2402.02558v1","category":"cs.CL"}
{"created":"2024-02-04 15:54:37","title":"Neur2BiLO: Neural Bilevel Optimization","abstract":"Bilevel optimization deals with nested problems in which a leader takes the first decision to minimize their objective function while accounting for a follower's best-response reaction. Constrained bilevel problems with integer variables are particularly notorious for their hardness. While exact solvers have been proposed for mixed-integer linear bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case. On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope. Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program. Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for the bilevel knapsack interdiction problem, the \"critical node game\" from network security, a donor-recipient healthcare problem, and discrete network design from transportation planning. These problems are diverse in that they have linear or non-linear objectives/constraints and integer or mixed-integer variables, making Neur2BiLO unique in its versatility.","sentences":["Bilevel optimization deals with nested problems in which a leader takes the first decision to minimize their objective function while accounting for a follower's best-response reaction.","Constrained bilevel problems with integer variables are particularly notorious for their hardness.","While exact solvers have been proposed for mixed-integer linear bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case.","On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope.","Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program.","Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for the bilevel knapsack interdiction problem, the \"critical node game\" from network security, a donor-recipient healthcare problem, and discrete network design from transportation planning.","These problems are diverse in that they have linear or non-linear objectives/constraints and integer or mixed-integer variables, making Neur2BiLO unique in its versatility."],"url":"http://arxiv.org/abs/2402.02552v1","category":"math.OC"}
{"created":"2024-02-04 15:52:46","title":"\"What's my model inside of?\": Exploring the role of environments for grounded natural language understanding","abstract":"In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of large language models on challenging commonsense reasoning tasks. We leveraged the richer supervision provided by text-based game environments to develop Breakpoint Transformers, a novel approach to modeling intermediate semantic information in long narrative or procedural texts. Finally, we integrated theories on the role of environments in collective human intelligence to propose a design for AI-augmented \"social thinking environments\" for knowledge workers like scientists.","sentences":["In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition.","Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research.","Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments.","Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development.","We developed novel training and annotation approaches for procedural text understanding based on text-based game environments.","We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of large language models on challenging commonsense reasoning tasks.","We leveraged the richer supervision provided by text-based game environments to develop Breakpoint Transformers, a novel approach to modeling intermediate semantic information in long narrative or procedural texts.","Finally, we integrated theories on the role of environments in collective human intelligence to propose a design for AI-augmented \"social thinking environments\" for knowledge workers like scientists."],"url":"http://arxiv.org/abs/2402.02548v1","category":"cs.CL"}
{"created":"2024-02-04 15:50:42","title":"Integration of cognitive tasks into artificial general intelligence test for large models","abstract":"During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifaceted social scenarios; and embodied intelligence, denoting the ability to interact with its physical environment. To assess the multidimensional intelligence of large models, the AGI test consists of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community. We propose that the complexity of AGI testing tasks should increase commensurate with the advancements in large models. We underscore the necessity for the interpretation of test results to avoid false negatives and false positives. We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.","sentences":["During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application.","However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models.","In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities.","The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifaceted social scenarios; and embodied intelligence, denoting the ability to interact with its physical environment.","To assess the multidimensional intelligence of large models, the AGI test consists of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community.","We propose that the complexity of AGI testing tasks should increase commensurate with the advancements in large models.","We underscore the necessity for the interpretation of test results to avoid false negatives and false positives.","We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society."],"url":"http://arxiv.org/abs/2402.02547v1","category":"cs.AI"}
{"created":"2024-02-04 15:48:20","title":"Classification of Tennis Actions Using Deep Learning","abstract":"Recent advances of deep learning makes it possible to identify specific events in videos with greater precision. This has great relevance in sports like tennis in order to e.g., automatically collect game statistics, or replay actions of specific interest for game strategy or player improvements. In this paper, we investigate the potential and the challenges of using deep learning to classify tennis actions. Three models of different size, all based on the deep learning architecture SlowFast were trained and evaluated on the academic tennis dataset THETIS. The best models achieve a generalization accuracy of 74 %, demonstrating a good performance for tennis action classification. We provide an error analysis for the best model and pinpoint directions for improvement of tennis datasets in general. We discuss the limitations of the data set, general limitations of current publicly available tennis data-sets, and future steps needed to make progress.","sentences":["Recent advances of deep learning makes it possible to identify specific events in videos with greater precision.","This has great relevance in sports like tennis in order to e.g., automatically collect game statistics, or replay actions of specific interest for game strategy or player improvements.","In this paper, we investigate the potential and the challenges of using deep learning to classify tennis actions.","Three models of different size, all based on the deep learning architecture SlowFast were trained and evaluated on the academic tennis dataset THETIS.","The best models achieve a generalization accuracy of 74 %, demonstrating a good performance for tennis action classification.","We provide an error analysis for the best model and pinpoint directions for improvement of tennis datasets in general.","We discuss the limitations of the data set, general limitations of current publicly available tennis data-sets, and future steps needed to make progress."],"url":"http://arxiv.org/abs/2402.02545v1","category":"cs.CV"}
{"created":"2024-02-04 15:46:43","title":"LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model","abstract":"The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.","sentences":["The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains.","In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors.","To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images.","Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method.","Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain."],"url":"http://arxiv.org/abs/2402.02544v1","category":"cs.CV"}
{"created":"2024-02-04 15:10:34","title":"Absolute convergence and error thresholds in non-active adaptive sampling","abstract":"Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.","sentences":["Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size.","In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described.","We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection.","The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme.","Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study."],"url":"http://arxiv.org/abs/2402.02522v1","category":"cs.CL"}
{"created":"2024-02-04 15:08:50","title":"Neuromorphic hardware for sustainable AI data centers","abstract":"As humans advance toward a higher level of artificial intelligence, it is always at the cost of escalating computational resource consumption, which requires developing novel solutions to meet the exponential growth of AI computing demand. Neuromorphic hardware takes inspiration from how the brain processes information and promises energy-efficient computing of AI workloads. Despite its potential, neuromorphic hardware has not found its way into commercial AI data centers. In this article, we try to analyze the underlying reasons for this and derive requirements and guidelines to promote neuromorphic systems for efficient and sustainable cloud computing: We first review currently available neuromorphic hardware systems and collect examples where neuromorphic solutions excel conventional AI processing on CPUs and GPUs. Next, we identify applications, models and algorithms which are commonly deployed in AI data centers as further directions for neuromorphic algorithms research. Last, we derive requirements and best practices for the hardware and software integration of neuromorphic systems into data centers. With this article, we hope to increase awareness of the challenges of integrating neuromorphic hardware into data centers and to guide the community to enable sustainable and energy-efficient AI at scale.","sentences":["As humans advance toward a higher level of artificial intelligence, it is always at the cost of escalating computational resource consumption, which requires developing novel solutions to meet the exponential growth of AI computing demand.","Neuromorphic hardware takes inspiration from how the brain processes information and promises energy-efficient computing of AI workloads.","Despite its potential, neuromorphic hardware has not found its way into commercial AI data centers.","In this article, we try to analyze the underlying reasons for this and derive requirements and guidelines to promote neuromorphic systems for efficient and sustainable cloud computing: We first review currently available neuromorphic hardware systems and collect examples where neuromorphic solutions excel conventional AI processing on CPUs and GPUs.","Next, we identify applications, models and algorithms which are commonly deployed in AI data centers as further directions for neuromorphic algorithms research.","Last, we derive requirements and best practices for the hardware and software integration of neuromorphic systems into data centers.","With this article, we hope to increase awareness of the challenges of integrating neuromorphic hardware into data centers and to guide the community to enable sustainable and energy-efficient AI at scale."],"url":"http://arxiv.org/abs/2402.02521v1","category":"cs.ET"}
{"created":"2024-02-04 15:07:49","title":"SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving","abstract":"This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 & 2 motion forecasting benchmarks compared with other state-of-the-art methods. Furthermore, its lightweight design and low inference latency make SIMPL highly extensible and promising for real-world onboard deployment. We open-source the code at https://github.com/HKUST-Aerial-Robotics/SIMPL.","sentences":["This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles.","Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants.","To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting.","Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks.","As a strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 & 2 motion forecasting benchmarks compared with other state-of-the-art methods.","Furthermore, its lightweight design and low inference latency make SIMPL highly extensible and promising for real-world onboard deployment.","We open-source the code at https://github.com/HKUST-Aerial-Robotics/SIMPL."],"url":"http://arxiv.org/abs/2402.02519v1","category":"cs.RO"}
{"created":"2024-02-04 15:02:17","title":"Adaptive scheduling for adaptive sampling in POS taggers construction","abstract":"We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematurely.   The proposal has been evaluated on the basis of its reliability to identify the convergence of models, corroborating our expectations. While a concrete halting condition is used for testing, users can choose any condition whatsoever to suit their own specific needs.","sentences":["We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers.","The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration.","In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time.","The algorithm proves to be formally correct regarding our working hypotheses.","Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition.","We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematurely.   ","The proposal has been evaluated on the basis of its reliability to identify the convergence of models, corroborating our expectations.","While a concrete halting condition is used for testing, users can choose any condition whatsoever to suit their own specific needs."],"url":"http://arxiv.org/abs/2402.02516v1","category":"cs.CL"}
{"created":"2024-02-04 15:00:52","title":"Modeling of learning curves with applications to pos tagging","abstract":"An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of accuracy gain, with the purpose of measuring how much work is needed to achieve a certain degree of performance. The second relates the comparison of efficiency between systems at training time, with the objective of completing this task only for the one that best suits our requirements. The prediction of accuracy is also a valuable item of information for customizing systems, since we can estimate in advance the impact of settings on both the performance and the development costs. Using the generation of part-of-speech taggers as an example application, the experimental results are consistent with our expectations.","sentences":["An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced.","We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed.","The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition.","This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   ","Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process.","The proposal is of interest in at least three operational procedures.","The first is the anticipation of accuracy gain, with the purpose of measuring how much work is needed to achieve a certain degree of performance.","The second relates the comparison of efficiency between systems at training time, with the objective of completing this task only for the one that best suits our requirements.","The prediction of accuracy is also a valuable item of information for customizing systems, since we can estimate in advance the impact of settings on both the performance and the development costs.","Using the generation of part-of-speech taggers as an example application, the experimental results are consistent with our expectations."],"url":"http://arxiv.org/abs/2402.02515v1","category":"cs.CL"}
{"created":"2024-02-04 14:57:20","title":"Early stopping by correlating online indicators in neural networks","abstract":"In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, we choose to work in the sphere of natural language processing, an operational continuum increasingly based on machine learning. As a case study, we focus on parser generation, one of the most demanding and complex tasks in the domain. The selection of cross-validation as a canary function enables an actual comparison with the most representative early stopping conditions based on overfitting identification, pointing to a promising start toward an optimal bias and variance control.","sentences":["In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced.","This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling.","Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting.","That way, we provide a formal basis for decision making in terms of interrupting the learning process.   ","As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability.","With a view to illustrating the effectiveness of the halting condition described, we choose to work in the sphere of natural language processing, an operational continuum increasingly based on machine learning.","As a case study, we focus on parser generation, one of the most demanding and complex tasks in the domain.","The selection of cross-validation as a canary function enables an actual comparison with the most representative early stopping conditions based on overfitting identification, pointing to a promising start toward an optimal bias and variance control."],"url":"http://arxiv.org/abs/2402.02513v1","category":"cs.LG"}
{"created":"2024-02-04 14:51:49","title":"PoCo: Policy Composition from and for Heterogeneous Robot Learning","abstract":"Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments. See https://liruiw.github.io/policycomp for more details .","sentences":["Training general robotic policies from heterogeneous data for different tasks is a significant challenge.","Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos.","Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult.","In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models.","Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time.","We train our method on simulation, human, and real robot data and evaluate in tool-use tasks.","The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments.","See https://liruiw.github.io/policycomp for more details ."],"url":"http://arxiv.org/abs/2402.02511v1","category":"cs.RO"}
{"created":"2024-02-04 14:43:54","title":"Deconstructing the spin susceptibility of a cuprate superconductor","abstract":"A major obstacle to understanding high-Tc cuprates is that superconductivity precludes observing normal-state properties at low temperatures. One prime example is the normal-state spin susceptibility: although its decrease upon cooling far above Tc typifies pseudogap behavior, its behavior at low temperatures is generally unknown. Here, our measurements in high magnetic fields expose the spin susceptibility of YBa2Cu3Oy down to low temperatures. Even though superconductivity is suppressed by the field, we uncover two thermally-activated contributions alongside a residual susceptibility at T=0 due to gapless excitations. We relate these two distinct gaps to short-range charge-density waves and to the formation of spin singlets similar to those found in certain quantum spin systems. These phenomena thus collectively contribute to the pseudogap in the spin susceptibility at low temperature, supplementing short-lived antiferromagnetism known to initiate pseudogap behavior at high temperatures. We therefore propose that the pseudogap should be regarded as a composite property.","sentences":["A major obstacle to understanding high-Tc cuprates is that superconductivity precludes observing normal-state properties at low temperatures.","One prime example is the normal-state spin susceptibility: although its decrease upon cooling far above Tc typifies pseudogap behavior, its behavior at low temperatures is generally unknown.","Here, our measurements in high magnetic fields expose the spin susceptibility of YBa2Cu3Oy down to low temperatures.","Even though superconductivity is suppressed by the field, we uncover two thermally-activated contributions alongside a residual susceptibility at T=0 due to gapless excitations.","We relate these two distinct gaps to short-range charge-density waves and to the formation of spin singlets similar to those found in certain quantum spin systems.","These phenomena thus collectively contribute to the pseudogap in the spin susceptibility at low temperature, supplementing short-lived antiferromagnetism known to initiate pseudogap behavior at high temperatures.","We therefore propose that the pseudogap should be regarded as a composite property."],"url":"http://arxiv.org/abs/2402.02508v1","category":"cond-mat.supr-con"}
{"created":"2024-02-04 14:18:45","title":"Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning","abstract":"In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help design more generalizable and robust robotic models.","sentences":["In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud.","Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance.","This remains consistent in both scenarios: training from scratch and utilizing pretraining.","Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance.","The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks.","We will open-source all our codes and checkpoints, hoping that our insights can help design more generalizable and robust robotic models."],"url":"http://arxiv.org/abs/2402.02500v1","category":"cs.RO"}
{"created":"2024-02-04 14:12:51","title":"Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion","abstract":"Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions. In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability. To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features. A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information. Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work. We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approaches and the conventional optimization-based baseline.","sentences":["Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions.","In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability.","To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features.","A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information.","Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work.","We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approaches and the conventional optimization-based baseline."],"url":"http://arxiv.org/abs/2402.02498v1","category":"eess.IV"}
{"created":"2024-02-04 14:03:42","title":"Exact Numerical Solution of Stochastic Master Equations for Conditional Spin Squeezing","abstract":"Stochastic master equations are often used to describe conditional spin squeezing of atomic ensemble, but are limited so far to the systems with few atoms due to the exponentially increased Hilbert space. In this article, we present an exact numerical solution of these equations for systems with identical atoms by mapping identical density matrix elements to a single quantity characterized by collective quantum numbers, and apply it to the system with hundred atoms in a bad cavity subject to a homodyne detection. We demonstrate that the spin squeezing can be vividly illustrated by the Gaussian-like distribution of the collective density matrix elements, and we examine the influence of the probe field strength and polarization, the detection efficiency, the spontaneous emission rate and the number of atoms. Our exact approach can play an important role in gauging the approximate approaches applied for systems with more atoms, such as Gaussian-state formalism and stochastic mean-field approach, and it permits also exploration of entanglement effects beyond these approaches.","sentences":["Stochastic master equations are often used to describe conditional spin squeezing of atomic ensemble, but are limited so far to the systems with few atoms due to the exponentially increased Hilbert space.","In this article, we present an exact numerical solution of these equations for systems with identical atoms by mapping identical density matrix elements to a single quantity characterized by collective quantum numbers, and apply it to the system with hundred atoms in a bad cavity subject to a homodyne detection.","We demonstrate that the spin squeezing can be vividly illustrated by the Gaussian-like distribution of the collective density matrix elements, and we examine the influence of the probe field strength and polarization, the detection efficiency, the spontaneous emission rate and the number of atoms.","Our exact approach can play an important role in gauging the approximate approaches applied for systems with more atoms, such as Gaussian-state formalism and stochastic mean-field approach, and it permits also exploration of entanglement effects beyond these approaches."],"url":"http://arxiv.org/abs/2402.02495v1","category":"quant-ph"}
{"created":"2024-02-04 13:28:23","title":"Joint User Detection and Localization in Near-Field Using Reconfigurable Intelligent Surfaces","abstract":"This letter studies the problem of jointly detecting active user equipments (UEs) and estimating their location in the near field, wherein the base station (BS) is unaware of the number of active (or inactive) UEs and their positions. The system is equipped with multiple reconfigurable intelligent surfaces (RISs) that aid the process of inspecting the coverage area of the BS with adequate localization resolution providing a low-complexity solution for detection and location estimation. To address this problem, we propose to utilize the additional degrees of freedom due to the additional inspection points provided by the RISs. Specifically, we propose an iterative detection procedure, where multiple inspections are jointly considered, allowing the BS to assign known pilots to previously detected UEs and thereby to provide a structured channel access. Also, the problem of multiple access interference is explored and identified as a limiting performance factor for the activity detection. The results show that, with a proper implementation of the RISs, our proposed scheme can detect/localize the UEs with high accuracy, augmenting benchmark UE detection schemes to a spatially aware detection.","sentences":["This letter studies the problem of jointly detecting active user equipments (UEs) and estimating their location in the near field, wherein the base station (BS) is unaware of the number of active (or inactive) UEs and their positions.","The system is equipped with multiple reconfigurable intelligent surfaces (RISs) that aid the process of inspecting the coverage area of the BS with adequate localization resolution providing a low-complexity solution for detection and location estimation.","To address this problem, we propose to utilize the additional degrees of freedom due to the additional inspection points provided by the RISs.","Specifically, we propose an iterative detection procedure, where multiple inspections are jointly considered, allowing the BS to assign known pilots to previously detected UEs and thereby to provide a structured channel access.","Also, the problem of multiple access interference is explored and identified as a limiting performance factor for the activity detection.","The results show that, with a proper implementation of the RISs, our proposed scheme can detect/localize the UEs with high accuracy, augmenting benchmark UE detection schemes to a spatially aware detection."],"url":"http://arxiv.org/abs/2402.02488v1","category":"eess.SP"}
{"created":"2024-02-04 13:16:29","title":"BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback","abstract":"Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies Bayes theorem to derive an intractable posterior distribution where the RM is explicitly represented. BRAIn then distills this posterior into an amortized inference network through self-normalized importance sampling, leading to a scalable offline algorithm that significantly outperforms prior art in summarization and AntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for specific RM choices.","sentences":["Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner.","These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance.","However, they leave behind important features of the PPO approach.","Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model.","In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.","BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies Bayes theorem to derive an intractable posterior distribution where the RM is explicitly represented.","BRAIn then distills this posterior into an amortized inference network through self-normalized importance sampling, leading to a scalable offline algorithm that significantly outperforms prior art in summarization and AntropicHH tasks.","BRAIn also has interesting connections to PPO and DPO for specific RM choices."],"url":"http://arxiv.org/abs/2402.02479v1","category":"cs.LG"}
{"created":"2024-02-04 13:15:59","title":"Why are hyperbolic neural networks effective? A study on hierarchical representation capability","abstract":"Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space. However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations. In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments. Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis. Experiments show that HNNs cannot achieve the theoretical optimal embedding. The HRC is significantly affected by the optimization objectives and hierarchical structures, and enhancing HRC through pre-training strategies can significantly improve the performance of HNNs.","sentences":["Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space.","However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations.","In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments.","Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis.","Experiments show that HNNs cannot achieve the theoretical optimal embedding.","The HRC is significantly affected by the optimization objectives and hierarchical structures, and enhancing HRC through pre-training strategies can significantly improve the performance of HNNs."],"url":"http://arxiv.org/abs/2402.02478v1","category":"cs.LG"}
{"created":"2024-02-04 13:02:48","title":"Probing Planck scale physics with Belle II and LHCb","abstract":"With the advent of Belle II and the LHCb upgrade, the precision measurements of various B-Physics observables are on cards. This holds significant potential for delving into physics beyond the standard model of electroweak interactions. These measurements can also serve as means to establish limits on phenomena occurring at much finer length scales, such as quantum decoherence, which may arise due to potential discreteness in space-time or non-trivial topological effects. In this work, we set up the formalism to investigate the impact of quantum decoherence on several potential observables in $B$ meson systems. The approach employs the trace-preserving Kraus operator formalism, extending unitary evolution to non-unitary dynamics while maintaining complete positivity. In this formalism, the decoherence effects are parametrized in terms of a single parameter. Through the analysis of purely leptonic, semileptonic, and non-leptonic decays of $B$ mesons, we identify observables that could, in principle, be influenced by decoherence. The theoretical expressions are provided without neglecting the impact of decay width difference ($\\Delta \\Gamma$) and $CP$ violation in mixing. Considering that many of these observables can be measured with high precision using the abundant data collected by LHCb and Belle II, our formalism can be applied to establish constraints on the decoherence parameter through multiple decay channels. This offers an alternative set-up for such studies, which, at present, are predominantly conducted in the neutrino sector.","sentences":["With the advent of Belle II and the LHCb upgrade, the precision measurements of various B-Physics observables are on cards.","This holds significant potential for delving into physics beyond the standard model of electroweak interactions.","These measurements can also serve as means to establish limits on phenomena occurring at much finer length scales, such as quantum decoherence, which may arise due to potential discreteness in space-time or non-trivial topological effects.","In this work, we set up the formalism to investigate the impact of quantum decoherence on several potential observables in $B$ meson systems.","The approach employs the trace-preserving Kraus operator formalism, extending unitary evolution to non-unitary dynamics while maintaining complete positivity.","In this formalism, the decoherence effects are parametrized in terms of a single parameter.","Through the analysis of purely leptonic, semileptonic, and non-leptonic decays of $B$ mesons, we identify observables that could, in principle, be influenced by decoherence.","The theoretical expressions are provided without neglecting the impact of decay width difference ($\\Delta \\Gamma$) and $CP$ violation in mixing.","Considering that many of these observables can be measured with high precision using the abundant data collected by LHCb and Belle II, our formalism can be applied to establish constraints on the decoherence parameter through multiple decay channels.","This offers an alternative set-up for such studies, which, at present, are predominantly conducted in the neutrino sector."],"url":"http://arxiv.org/abs/2402.02470v1","category":"hep-ph"}
{"created":"2024-02-04 13:02:27","title":"Fast Peer Adaptation with Context-aware Exploration","abstract":"Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds that involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed (Predator-Prey-W) games with peer agents. We demonstrate that our method induces more active exploration behavior, achieving faster adaptation and better outcomes than existing methods.","sentences":["Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games.","To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation.","However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon.","In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes.","This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident.","We evaluate our method on diverse testbeds that involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed (Predator-Prey-W) games with peer agents.","We demonstrate that our method induces more active exploration behavior, achieving faster adaptation and better outcomes than existing methods."],"url":"http://arxiv.org/abs/2402.02468v1","category":"cs.AI"}
{"created":"2024-02-04 12:29:40","title":"A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer","abstract":"Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space, overcoming previously known non-Euclidean challenge. (4) Our proposed novel edge-centric GPT pretraining task is effective in graph fields, underscoring its success in both representation and generation.","sentences":["Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information?","The non-Euclidean property have posed a long term challenge in graph modeling.","Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge.","We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence.","We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks.","(2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation.","(3) Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space, overcoming previously known non-Euclidean challenge.","(4) Our proposed novel edge-centric GPT pretraining task is effective in graph fields, underscoring its success in both representation and generation."],"url":"http://arxiv.org/abs/2402.02464v1","category":"cs.LG"}
{"created":"2024-02-04 12:07:23","title":"A Risk-aware Planning Framework of UGVs in Off-Road Environment","abstract":"Planning module is an essential component of intelligent vehicle study. In this paper, we address the risk-aware planning problem of UGVs through a global-local planning framework which seamlessly integrates risk assessment methods. In particular, a global planning algorithm named Coarse2fine A* is proposed, which incorporates a potential field approach to enhance the safety of the planning results while ensuring the efficiency of the algorithm. A deterministic sampling method for local planning is leveraged and modified to suit off-road environment. It also integrates a risk assessment model to emphasize the avoidance of local risks. The performance of the algorithm is demonstrated through simulation experiments by comparing it with baseline algorithms, where the results of Coarse2fine A* are shown to be approximately 30% safer than those of the baseline algorithms. The practicality and effectiveness of the proposed planning framework are validated by deploying it on a real-world system consisting of a control center and a practical UGV platform.","sentences":["Planning module is an essential component of intelligent vehicle study.","In this paper, we address the risk-aware planning problem of UGVs through a global-local planning framework which seamlessly integrates risk assessment methods.","In particular, a global planning algorithm named Coarse2fine A* is proposed, which incorporates a potential field approach to enhance the safety of the planning results while ensuring the efficiency of the algorithm.","A deterministic sampling method for local planning is leveraged and modified to suit off-road environment.","It also integrates a risk assessment model to emphasize the avoidance of local risks.","The performance of the algorithm is demonstrated through simulation experiments by comparing it with baseline algorithms, where the results of Coarse2fine A* are shown to be approximately 30% safer than those of the baseline algorithms.","The practicality and effectiveness of the proposed planning framework are validated by deploying it on a real-world system consisting of a control center and a practical UGV platform."],"url":"http://arxiv.org/abs/2402.02457v1","category":"cs.RO"}
{"created":"2024-02-04 11:49:51","title":"AI Art Neural Constellation: Revealing the Collective and Contrastive State of AI-Generated and Human Art","abstract":"Discovering the creative potentials of a random signal to various artistic expressions in aesthetic and conceptual richness is a ground for the recent success of generative machine learning as a way of art creation. To understand the new artistic medium better, we conduct a comprehensive analysis to position AI-generated art within the context of human art heritage. Our comparative analysis is based on an extensive dataset, dubbed ``ArtConstellation,'' consisting of annotations about art principles, likability, and emotions for 6,000 WikiArt and 3,200 AI-generated artworks. After training various state-of-the-art generative models, art samples are produced and compared with WikiArt data on the last hidden layer of a deep-CNN trained for style classification. We actively examined the various art principles to interpret the neural representations and used them to drive the comparative knowledge about human and AI-generated art. A key finding in the semantic analysis is that AI-generated artworks are visually related to the principle concepts for modern period art made in 1800-2000. In addition, through Out-Of-Distribution (OOD) and In-Distribution (ID) detection in CLIP space, we find that AI-generated artworks are ID to human art when they depict landscapes and geometric abstract figures, while detected as OOD when the machine art consists of deformed and twisted figures. We observe that machine-generated art is uniquely characterized by incomplete and reduced figuration. Lastly, we conducted a human survey about emotional experience. Color composition and familiar subjects are the key factors of likability and emotions in art appreciation. We propose our whole methodologies and collected dataset as our analytical framework to contrast human and AI-generated art, which we refer to as ``ArtNeuralConstellation''. Code is available at: https://github.com/faixan-khan/ArtNeuralConstellation","sentences":["Discovering the creative potentials of a random signal to various artistic expressions in aesthetic and conceptual richness is a ground for the recent success of generative machine learning as a way of art creation.","To understand the new artistic medium better, we conduct a comprehensive analysis to position AI-generated art within the context of human art heritage.","Our comparative analysis is based on an extensive dataset, dubbed ``ArtConstellation,'' consisting of annotations about art principles, likability, and emotions for 6,000 WikiArt and 3,200 AI-generated artworks.","After training various state-of-the-art generative models, art samples are produced and compared with WikiArt data on the last hidden layer of a deep-CNN trained for style classification.","We actively examined the various art principles to interpret the neural representations and used them to drive the comparative knowledge about human and AI-generated art.","A key finding in the semantic analysis is that AI-generated artworks are visually related to the principle concepts for modern period art made in 1800-2000.","In addition, through Out-Of-Distribution (OOD) and In-Distribution (ID) detection in CLIP space, we find that AI-generated artworks are ID to human art when they depict landscapes and geometric abstract figures, while detected as OOD when the machine art consists of deformed and twisted figures.","We observe that machine-generated art is uniquely characterized by incomplete and reduced figuration.","Lastly, we conducted a human survey about emotional experience.","Color composition and familiar subjects are the key factors of likability and emotions in art appreciation.","We propose our whole methodologies and collected dataset as our analytical framework to contrast human and AI-generated art, which we refer to as ``ArtNeuralConstellation''.","Code is available at: https://github.com/faixan-khan/ArtNeuralConstellation"],"url":"http://arxiv.org/abs/2402.02453v1","category":"cs.CV"}
{"created":"2024-02-04 11:42:16","title":"XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics","abstract":"With the rise of complex cyber devices Cyber Forensics (CF) is facing many new challenges. For example, there are dozens of systems running on smartphones, each with more than millions of downloadable applications. Sifting through this large amount of data and making sense requires new techniques, such as from the field of Artificial Intelligence (AI). To apply these techniques successfully in CF, we need to justify and explain the results to the stakeholders of CF, such as forensic analysts and members of the court, for them to make an informed decision. If we want to apply AI successfully in CF, there is a need to develop trust in AI systems. Some other factors in accepting the use of AI in CF are to make AI authentic, interpretable, understandable, and interactive. This way, AI systems will be more acceptable to the public and ensure alignment with legal standards. An explainable AI (XAI) system can play this role in CF, and we call such a system XAI-CF. XAI-CF is indispensable and is still in its infancy. In this paper, we explore and make a case for the significance and advantages of XAI-CF. We strongly emphasize the need to build a successful and practical XAI-CF system and discuss some of the main requirements and prerequisites of such a system. We present a formal definition of the terms CF and XAI-CF and a comprehensive literature review of previous works that apply and utilize XAI to build and increase trust in CF. We discuss some challenges facing XAI-CF. We also provide some concrete solutions to these challenges. We identify key insights and future research directions for building XAI applications for CF. This paper is an effort to explore and familiarize the readers with the role of XAI applications in CF, and we believe that our work provides a promising basis for future researchers interested in XAI-CF.","sentences":["With the rise of complex cyber devices Cyber Forensics (CF) is facing many new challenges.","For example, there are dozens of systems running on smartphones, each with more than millions of downloadable applications.","Sifting through this large amount of data and making sense requires new techniques, such as from the field of Artificial Intelligence (AI).","To apply these techniques successfully in CF, we need to justify and explain the results to the stakeholders of CF, such as forensic analysts and members of the court, for them to make an informed decision.","If we want to apply AI successfully in CF, there is a need to develop trust in AI systems.","Some other factors in accepting the use of AI in CF are to make AI authentic, interpretable, understandable, and interactive.","This way, AI systems will be more acceptable to the public and ensure alignment with legal standards.","An explainable AI (XAI) system can play this role in CF, and we call such a system XAI-CF.","XAI-CF is indispensable and is still in its infancy.","In this paper, we explore and make a case for the significance and advantages of XAI-CF.","We strongly emphasize the need to build a successful and practical XAI-CF system and discuss some of the main requirements and prerequisites of such a system.","We present a formal definition of the terms CF and XAI-CF and a comprehensive literature review of previous works that apply and utilize XAI to build and increase trust in CF.","We discuss some challenges facing XAI-CF.","We also provide some concrete solutions to these challenges.","We identify key insights and future research directions for building XAI applications for CF.","This paper is an effort to explore and familiarize the readers with the role of XAI applications in CF, and we believe that our work provides a promising basis for future researchers interested in XAI-CF."],"url":"http://arxiv.org/abs/2402.02452v1","category":"cs.CR"}
{"created":"2024-02-04 10:59:52","title":"LQER: Low-Rank Quantization Error Reconstruction for LLMs","abstract":"Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.","sentences":["Post-training quantization of Large Language Models (LLMs) is challenging.","In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability.","LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization.","Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations.","Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\\times$ fewer hardware resources than the leading state-of-the-art method.","We will open-source our framework once the paper is accepted."],"url":"http://arxiv.org/abs/2402.02446v1","category":"cs.LG"}
{"created":"2024-02-04 10:41:40","title":"TopoX: A Suite of Python Packages for Machine Learning on Topological Domains","abstract":"We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.","sentences":["We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes.","topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains.","The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team."],"url":"http://arxiv.org/abs/2402.02441v1","category":"cs.LG"}
{"created":"2024-02-04 10:30:23","title":"DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching","abstract":"In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and trajectory optimization methods (DT).","sentences":["In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets.","However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions.","To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories.","DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms.","Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies.","Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and trajectory optimization methods (DT)."],"url":"http://arxiv.org/abs/2402.02439v1","category":"cs.LG"}
{"created":"2024-02-04 09:40:22","title":"Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback","abstract":"Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of mainstream RL environments. We then establish a systematic pipeline of crowdsourced annotations, resulting in large-scale annotated datasets comprising more than 15 million steps across 30+ popular tasks. Through extensive experiments, the results in the collected datasets demonstrate competitive performance compared to those from well-designed manual rewards. We evaluate various design choices and offer insights into their strengths and potential areas of improvement. We wish to build valuable open-source platforms, datasets, and baselines to facilitate the development of more robust and reliable RLHF solutions based on realistic human feedback. The website is available at https://uni-rlhf.github.io/.","sentences":["Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences.","It is crucial to consider diverse human feedback types and various learning methods in different environments.","However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks.","To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF.","It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems.","Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations.","Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of mainstream RL environments.","We then establish a systematic pipeline of crowdsourced annotations, resulting in large-scale annotated datasets comprising more than 15 million steps across 30+ popular tasks.","Through extensive experiments, the results in the collected datasets demonstrate competitive performance compared to those from well-designed manual rewards.","We evaluate various design choices and offer insights into their strengths and potential areas of improvement.","We wish to build valuable open-source platforms, datasets, and baselines to facilitate the development of more robust and reliable RLHF solutions based on realistic human feedback.","The website is available at https://uni-rlhf.github.io/."],"url":"http://arxiv.org/abs/2402.02423v1","category":"cs.LG"}
{"created":"2024-02-04 09:24:51","title":"Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction","abstract":"Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on different open-source and API-based models. Remarkably, Aligner-7B improves 11 different LLMs by 18% in helpfulness and 23% in harmlessness on average (GPT-4 by 26.9% and 17.5%). When finetuning (strong) Llama2-70B with (weak) Aligner-13B's supervision, we can improve Llama2 by 8.2% in helpfulness and 61.6% in harmlessness. See our dataset and code at https://aligner2024.github.io","sentences":["Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods.","However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters.","Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers.","Our Aligner offers several key advantages.","Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources.","Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost.","Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on different open-source and API-based models.","Remarkably, Aligner-7B improves 11 different LLMs by 18% in helpfulness and 23% in harmlessness on average (GPT-4 by 26.9% and 17.5%).","When finetuning (strong) Llama2-70B with (weak) Aligner-13B's supervision, we can improve Llama2 by 8.2% in helpfulness and 61.6% in harmlessness.","See our dataset and code at https://aligner2024.github.io"],"url":"http://arxiv.org/abs/2402.02416v1","category":"cs.CL"}
{"created":"2024-02-04 09:15:02","title":"Quantum Secret Sharing Enhanced: Utilizing W States for Anonymous and Secure Communication","abstract":"Quantum secret sharing (QSS) is the result of merging the principles of quantum mechanics with secret information sharing. It enables a sender to share a secret among receivers, and the receivers can then collectively recover the secret when the need arises. To enhance the practicality of these quantum protocols, an innovative concept of quantum anonymous secret sharing (QASS) is advanced. In this paper, we propose a QASS protocol via W states, which can share secrets while ensuring recover-ability, recover-security, and recover-anonymity. We have rigorously evaluated our protocols, verifying their accuracy and fortifying their security against scenarios involving the active adversary. This includes considerations for dishonest receivers and non-receivers. Moreover, acknowledging the imperfections inherent in real-world communication channels, we have also undertaken an exhaustive analysis of our protocol's security and effectiveness in a quantum network where some form of noise is present. Our investigations reveal that W states exhibit good performance in mitigating noise interference, making them apt for practical applications.","sentences":["Quantum secret sharing (QSS) is the result of merging the principles of quantum mechanics with secret information sharing.","It enables a sender to share a secret among receivers, and the receivers can then collectively recover the secret when the need arises.","To enhance the practicality of these quantum protocols, an innovative concept of quantum anonymous secret sharing (QASS) is advanced.","In this paper, we propose a QASS protocol via W states, which can share secrets while ensuring recover-ability, recover-security, and recover-anonymity.","We have rigorously evaluated our protocols, verifying their accuracy and fortifying their security against scenarios involving the active adversary.","This includes considerations for dishonest receivers and non-receivers.","Moreover, acknowledging the imperfections inherent in real-world communication channels, we have also undertaken an exhaustive analysis of our protocol's security and effectiveness in a quantum network where some form of noise is present.","Our investigations reveal that W states exhibit good performance in mitigating noise interference, making them apt for practical applications."],"url":"http://arxiv.org/abs/2402.02413v1","category":"quant-ph"}
{"created":"2024-02-04 08:41:20","title":"Angle Robustness Unmanned Aerial Vehicle Navigation in GNSS-Denied Scenarios","abstract":"Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task. Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation. However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings. In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks. Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation. To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368. Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing. Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively.","sentences":["Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task.","Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation.","However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings.","In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks.","Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation.","To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368.","Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing.","Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively."],"url":"http://arxiv.org/abs/2402.02405v1","category":"cs.RO"}
{"created":"2024-02-04 08:24:13","title":"AI-Generated Content Enhanced Computer-Aided Diagnosis Model for Thyroid Nodules: A ChatGPT-Style Assistant","abstract":"An artificial intelligence-generated content-enhanced computer-aided diagnosis (AIGC-CAD) model, designated as ThyGPT, has been developed. This model, inspired by the architecture of ChatGPT, could assist radiologists in assessing the risk of thyroid nodules through semantic-level human-machine interaction. A dataset comprising 19,165 thyroid nodule ultrasound cases from Zhejiang Cancer Hospital was assembled to facilitate the training and validation of the model. After training, ThyGPT could automatically evaluate thyroid nodule and engage in effective communication with physicians through human-computer interaction. The performance of ThyGPT was rigorously quantified using established metrics such as the receiver operating characteristic (ROC) curve, area under the curve (AUC), sensitivity, and specificity. The empirical findings revealed that radiologists, when supplemented with ThyGPT, markedly surpassed the diagnostic acumen of their peers utilizing traditional methods as well as the performance of the model in isolation. These findings suggest that AIGC-CAD systems, exemplified by ThyGPT, hold the promise to fundamentally transform the diagnostic workflows of radiologists in forthcoming years.","sentences":["An artificial intelligence-generated content-enhanced computer-aided diagnosis (AIGC-CAD) model, designated as ThyGPT, has been developed.","This model, inspired by the architecture of ChatGPT, could assist radiologists in assessing the risk of thyroid nodules through semantic-level human-machine interaction.","A dataset comprising 19,165 thyroid nodule ultrasound cases from Zhejiang Cancer Hospital was assembled to facilitate the training and validation of the model.","After training, ThyGPT could automatically evaluate thyroid nodule and engage in effective communication with physicians through human-computer interaction.","The performance of ThyGPT was rigorously quantified using established metrics such as the receiver operating characteristic (ROC) curve, area under the curve (AUC), sensitivity, and specificity.","The empirical findings revealed that radiologists, when supplemented with ThyGPT, markedly surpassed the diagnostic acumen of their peers utilizing traditional methods as well as the performance of the model in isolation.","These findings suggest that AIGC-CAD systems, exemplified by ThyGPT, hold the promise to fundamentally transform the diagnostic workflows of radiologists in forthcoming years."],"url":"http://arxiv.org/abs/2402.02401v1","category":"cs.CV"}
{"created":"2024-02-04 08:23:41","title":"FreDF: Learning to Forecast in Frequency Domain","abstract":"Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models.","sentences":["Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences.","Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence.","Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence.","This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models.","In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain.","Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models."],"url":"http://arxiv.org/abs/2402.02399v1","category":"cs.LG"}
{"created":"2024-02-04 08:11:45","title":"DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models","abstract":"Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods.","sentences":["Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine.","These fields often grapple with decision-making under uncertainty, a critical yet challenging task.","In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases.","To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments.","DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process.","We validate our framework on decision-making environments involving real agriculture and finance data.","Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods."],"url":"http://arxiv.org/abs/2402.02392v1","category":"cs.AI"}
{"created":"2024-02-04 07:59:06","title":"Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning","abstract":"Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources.It contains practical models across various domains.","sentences":["Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives.","This process demands labor-intensive endeavors and multidisciplinary expertise.","Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process.","However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs.","Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process.","In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems.","Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios.","In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning.","To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization.","Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources.","It contains practical models across various domains."],"url":"http://arxiv.org/abs/2402.02388v1","category":"cs.CL"}
{"created":"2024-02-04 07:55:01","title":"A Survey on Robotics with Foundation Models: toward Embodied AI","abstract":"While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.","sentences":["While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments.","Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks.","They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective.","This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control.","Moreover, we showcase their commonly used datasets, simulators, and benchmarks.","Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse."],"url":"http://arxiv.org/abs/2402.02385v1","category":"cs.RO"}
{"created":"2024-02-04 07:44:06","title":"Empowering Computing and Networks Convergence System with Distributed Cooperative Routing","abstract":"The emergence of intelligent applications and recent advances in the fields of computing and networks are driving the development of computing and networks convergence (CNC) system. However, existing researches failed to achieve comprehensive scheduling optimization of computing and network resources. This shortfall results in some requirements of computing requests unable to be guaranteed in an end-to-end service pattern, negatively impacting the development of CNC systems. In this article, we propose a distributed cooperative routing framework for the CNC system to ensure the deadline requirements and minimize the computation cost of requests. The framework includes trading plane, management plane, control plane and forwarding plane. The cross-plane cooperative end-to-end routing schemes consider both computation efficiency of heterogeneous servers and the network congestion degrees while making routing plan, thereby determining where to execute requests and corresponding routing paths. Simulations results substantiates the performance of our routing schemes in scheduling computing requests in the CNC system.","sentences":["The emergence of intelligent applications and recent advances in the fields of computing and networks are driving the development of computing and networks convergence (CNC) system.","However, existing researches failed to achieve comprehensive scheduling optimization of computing and network resources.","This shortfall results in some requirements of computing requests unable to be guaranteed in an end-to-end service pattern, negatively impacting the development of CNC systems.","In this article, we propose a distributed cooperative routing framework for the CNC system to ensure the deadline requirements and minimize the computation cost of requests.","The framework includes trading plane, management plane, control plane and forwarding plane.","The cross-plane cooperative end-to-end routing schemes consider both computation efficiency of heterogeneous servers and the network congestion degrees while making routing plan, thereby determining where to execute requests and corresponding routing paths.","Simulations results substantiates the performance of our routing schemes in scheduling computing requests in the CNC system."],"url":"http://arxiv.org/abs/2402.02381v1","category":"cs.NI"}
{"created":"2024-02-04 07:39:06","title":"Evaluating Large Language Models in Analysing Classroom Dialogue","abstract":"This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in coding between the model and human coders, with some discrepancies in specific codes. These findings highlight the strong potential of LLM in teaching evaluation and facilitation.","sentences":["This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement.","Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process.","The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes.","These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model.","This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues.","Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated.","Results indicate substantial time savings with GPT-4, and a high degree of consistency in coding between the model and human coders, with some discrepancies in specific codes.","These findings highlight the strong potential of LLM in teaching evaluation and facilitation."],"url":"http://arxiv.org/abs/2402.02380v1","category":"cs.CL"}
{"created":"2024-02-04 06:39:01","title":"Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation","abstract":"Recent advancements in self-supervised learning have unlocked the potential to harness unlabeled data for auxiliary tasks, facilitating the learning of beneficial priors. This has been particularly advantageous in fields like medical image analysis, where labeled data are scarce. Although effective for classification tasks, this methodology has shown limitations in more complex applications, such as medical image segmentation. In this paper, we introduce Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS), a dedicated self-supervised framework tailored for medical image segmentation. We evaluate MedSASS against existing state-of-the-art methods across four diverse medical datasets, showcasing its superiority. MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods. Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements of 14.4% for CNNs and 6% for ViT-based architectures compared to existing state-of-the-art self-supervised strategies.","sentences":["Recent advancements in self-supervised learning have unlocked the potential to harness unlabeled data for auxiliary tasks, facilitating the learning of beneficial priors.","This has been particularly advantageous in fields like medical image analysis, where labeled data are scarce.","Although effective for classification tasks, this methodology has shown limitations in more complex applications, such as medical image segmentation.","In this paper, we introduce Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS), a dedicated self-supervised framework tailored for medical image segmentation.","We evaluate MedSASS against existing state-of-the-art methods across four diverse medical datasets, showcasing its superiority.","MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods.","Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements of 14.4% for CNNs and 6% for ViT-based architectures compared to existing state-of-the-art self-supervised strategies."],"url":"http://arxiv.org/abs/2402.02367v1","category":"cs.CV"}
{"created":"2024-02-04 06:23:05","title":"The Developmental Landscape of In-Context Learning","abstract":"We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.","sentences":["We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks.","We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space.","We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity."],"url":"http://arxiv.org/abs/2402.02364v1","category":"cs.LG"}
{"created":"2024-02-04 06:11:54","title":"Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE","abstract":"Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The concept of gauge symmetries sheds light on the complex behavior of deep learning models through physics and provides us with a unifying perspective for analyzing various machine learning architectures.","sentences":["Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning.","This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures.","By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries.","We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity.","Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs.","We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries.","The concept of gauge symmetries sheds light on the complex behavior of deep learning models through physics and provides us with a unifying perspective for analyzing various machine learning architectures."],"url":"http://arxiv.org/abs/2402.02362v1","category":"cs.LG"}
{"created":"2024-02-04 05:08:25","title":"System size and shape dependences of collective flow fluctuations in relativistic nuclear collisions","abstract":"Quantum fluctuations plays an essential role in forming the collective flow of hadrons observed in relativistic heavy-ion collisions. Event-by-event fluctuations of the collective flow can arise from various sources, such as the fluctuations in the initial geometry, hydrodynamic expansion, hadronization and hadronic evolution of the nuclear matter, while the exact contribution from each source is still an open question. Using a (3+1)-dimensional relativistic hydrodynamic model coupled to a Monte-Carlo Glauber initial condition, Cooper-Frye particlization and a hadronic transport model, we explore the system size and shape dependences of the collective flow fluctuations in Au+Au, Cu+Au and O+O collisions at $\\sqrt{s_\\mathrm{NN}}=200$ GeV. The particle yields, mean transverse momenta, 2-particle and 4-particle cumulant elliptic flows ($v_2\\{2\\}$ and $v_2\\{4\\}$) from our calculation agree with the currently existing data from RHIC. Different centrality dependences of the flow fluctuations, quantified by the $v_2\\{4\\}/v_2\\{2\\}$ ratio, are found for different collision systems due to their different sizes and shapes. By comparing $v_2\\{4\\}/v_2\\{2\\}$ between different hadron species, and comparing $v_2\\{4\\}/v_2\\{2\\}$ to the initial state geometric fluctuations quantified by the cumulant eccentricity ratio $\\varepsilon_2\\{4\\}/\\varepsilon_2\\{2\\}$, we find that while the initial state fluctuations are the main source of the $v_2$ fluctuations in large collision systems, other sources like hadronization and hadronic afterburner can significantly affect the $v_2$ fluctuations in small systems.","sentences":["Quantum fluctuations plays an essential role in forming the collective flow of hadrons observed in relativistic heavy-ion collisions.","Event-by-event fluctuations of the collective flow can arise from various sources, such as the fluctuations in the initial geometry, hydrodynamic expansion, hadronization and hadronic evolution of the nuclear matter, while the exact contribution from each source is still an open question.","Using a (3+1)-dimensional relativistic hydrodynamic model coupled to a Monte-Carlo Glauber initial condition, Cooper-Frye particlization and a hadronic transport model, we explore the system size and shape dependences of the collective flow fluctuations in Au+Au, Cu+Au and O+O collisions at $\\sqrt{s_\\mathrm{NN}}=200$ GeV.","The particle yields, mean transverse momenta, 2-particle and 4-particle cumulant elliptic flows ($v_2\\{2\\}$ and $v_2\\{4\\}$) from our calculation agree with the currently existing data from RHIC.","Different centrality dependences of the flow fluctuations, quantified by the $v_2\\{4\\}/v_2\\{2\\}$ ratio, are found for different collision systems due to their different sizes and shapes.","By comparing $v_2\\{4\\}/v_2\\{2\\}$ between different hadron species, and comparing $v_2\\{4\\}/v_2\\{2\\}$ to the initial state geometric fluctuations quantified by the cumulant eccentricity ratio $\\varepsilon_2\\{4\\}/\\varepsilon_2\\{2\\}$, we find that while the initial state fluctuations are the main source of the $v_2$ fluctuations in large collision systems, other sources like hadronization and hadronic afterburner can significantly affect the $v_2$ fluctuations in small systems."],"url":"http://arxiv.org/abs/2402.02348v1","category":"nucl-th"}
{"created":"2024-02-04 05:03:06","title":"Stereographic Spherical Sliced Wasserstein Distances","abstract":"Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both speed and accuracy through a wide range of numerical studies, including gradient flows and self-supervised learning.","sentences":["Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning.","The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures.","This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance.","We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation.","Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both speed and accuracy through a wide range of numerical studies, including gradient flows and self-supervised learning."],"url":"http://arxiv.org/abs/2402.02345v1","category":"cs.LG"}
{"created":"2024-02-04 04:55:54","title":"MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters","abstract":"This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.","sentences":["This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance.","Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training.","More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses.","We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications."],"url":"http://arxiv.org/abs/2402.02342v1","category":"cs.LG"}
{"created":"2024-02-04 04:28:02","title":"Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation","abstract":"Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a latent state. Projection loss is then employed to ensure the generated poses are well aligned in 2D space for high-quality optimization. Furthermore, we utilize the uncertainty of each joint to determine how much each joint is allowed for optimization. The effectiveness and superiority of the proposed framework are validated through extensive experiments on two challenging datasets: Human3.6M and MPI-INF-3DHP. Notably, our approach outperforms the previous best result by a large margin of 4.5% on Human3.6M. Our source code will be open-sourced.","sentences":["Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization.","In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance.","We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem.","To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints.","Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint.","For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a latent state.","Projection loss is then employed to ensure the generated poses are well aligned in 2D space for high-quality optimization.","Furthermore, we utilize the uncertainty of each joint to determine how much each joint is allowed for optimization.","The effectiveness and superiority of the proposed framework are validated through extensive experiments on two challenging datasets: Human3.6M and MPI-INF-3DHP.","Notably, our approach outperforms the previous best result by a large margin of 4.5% on Human3.6M. Our source code will be open-sourced."],"url":"http://arxiv.org/abs/2402.02339v1","category":"cs.CV"}
{"created":"2024-02-04 04:07:39","title":"Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning","abstract":"Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive bias for deep learning on tabular data. Code is available at https://github.com/aigc-apps/AMFormer.","sentences":["Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered.","This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning.","To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer.","Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization.","This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features.","Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive bias for deep learning on tabular data.","Code is available at https://github.com/aigc-apps/AMFormer."],"url":"http://arxiv.org/abs/2402.02334v1","category":"cs.LG"}
{"created":"2024-02-04 03:47:10","title":"Enhance Reasoning for Large Language Models in the Game Werewolf","abstract":"This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.","sentences":["This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents.","Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques.","The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge.","Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning.","We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning.","Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation.","Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker.","This paper also contributes the largest dataset for social deduction games to date."],"url":"http://arxiv.org/abs/2402.02330v1","category":"cs.AI"}
{"created":"2024-02-04 02:06:57","title":"A Survey of Large Language Models in Finance (FinLLMs)","abstract":"Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services. Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges. Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets. In addition, we provide eight advanced financial NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency. To support AI research in finance, we compile a collection of accessible datasets and evaluation benchmarks on GitHub.","sentences":["Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services.","Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited.","This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges.","Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs.","Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods.","Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets.","In addition, we provide eight advanced financial NLP tasks and datasets for developing more sophisticated FinLLMs.","Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency.","To support AI research in finance, we compile a collection of accessible datasets and evaluation benchmarks on GitHub."],"url":"http://arxiv.org/abs/2402.02315v1","category":"cs.CL"}
{"created":"2024-02-04 01:55:00","title":"Selecting Large Language Model to Fine-tune via Rectified Scaling Law","abstract":"The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known \"power phase\" but also the previously unobserved \"pre-power phase\". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of \"pre-learned data size\" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection.","sentences":["The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options.","Given constrained resources, fine-tuning all models and making selections afterward is unrealistic.","In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws.","Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known \"power phase\" but also the previously unobserved \"pre-power phase\".","We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically.","To address this, we introduce the concept of \"pre-learned data size\" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better.","By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection."],"url":"http://arxiv.org/abs/2402.02314v1","category":"cs.LG"}
{"created":"2024-02-03 22:55:31","title":"Future Directions in Foundations of Graph Machine Learning","abstract":"Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.","sentences":["Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences.","Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete.","Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques.","However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques.","In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization."],"url":"http://arxiv.org/abs/2402.02287v1","category":"cs.LG"}
{"created":"2024-02-03 22:51:17","title":"Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation","abstract":"Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignment module with recursive upsampling architecture for accurate and efficient spatial alignment between multi-scale score maps. Finally, the Adaptive Scores Fusion Module (ASFM) is proposed to adaptively fuse multi-scale scores so that the final prediction can favor objects of multiple scales. Comprehensive experiments on three benchmark datasets including Cityscapes, CamVid and PASCAL-Context show the effectiveness and efficiency of our method. In particular, we achieve a better balance between speed and accuracy than state-of-the-art real-time methods on Cityscapes and CamVid datasets. Code is available at: https://github.com/Yanhua-Zhang/MFARANet.","sentences":["Real-time semantic segmentation is a crucial research for real-world applications.","However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy.","In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important.","To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed.","We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone.","Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference.","Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignment module with recursive upsampling architecture for accurate and efficient spatial alignment between multi-scale score maps.","Finally, the Adaptive Scores Fusion Module (ASFM) is proposed to adaptively fuse multi-scale scores so that the final prediction can favor objects of multiple scales.","Comprehensive experiments on three benchmark datasets including Cityscapes, CamVid and PASCAL-Context show the effectiveness and efficiency of our method.","In particular, we achieve a better balance between speed and accuracy than state-of-the-art real-time methods on Cityscapes and CamVid datasets.","Code is available at: https://github.com/Yanhua-Zhang/MFARANet."],"url":"http://arxiv.org/abs/2402.02286v1","category":"cs.CV"}
{"created":"2024-02-03 22:49:00","title":"SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking","abstract":"In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \\method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\\method} results in $4-5%$ improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of the performance compared to the few-shot setup using human-annotated training data. Our synthetic data and code can be accessed at https://github.com/apple/ml-synthdst","sentences":["In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST).","However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data.","Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible.","While zero-shot learning requires no training data, it significantly lags behind the few-shot setup.","Thus, `\\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \\method, a data generation framework tailored for DST, utilizing LLMs.","Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations.","Few-shot learning using data from {\\method} results in $4-5%$ improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4.","Remarkably, our few-shot learning approach recovers nearly $98%$ of the performance compared to the few-shot setup using human-annotated training data.","Our synthetic data and code can be accessed at https://github.com/apple/ml-synthdst"],"url":"http://arxiv.org/abs/2402.02285v1","category":"cs.CL"}
{"created":"2024-02-03 22:40:56","title":"Radio properties of high-redshift galaxies at $z \\geq 1$","abstract":"Study of high-redshift radio galaxies (HzRGs) can shed light on the active galactic nuclei (AGNs) evolution in massive elliptical galaxies. The vast majority of observed high-redshift AGNs are quasars, and there are very few radio galaxies at redshifts $z>3$. We present the radio properties of 173 sources optically identified with radio galaxies at $z\\geqslant1$ with flux densities $S_{1.4}\\geqslant20$ mJy. Literature data were collected for compilation of broadband radio spectra, estimation of radio variability, radio luminosity, and radio loudness. Almost 60% of the galaxies have steep or ultra-steep radio spectra; 22% have flat, inverted, upturn, and complex spectral shapes, and 18% have peaked spectra (PS). The majority of the PS sources in the sample (20/31) are megahertz-peaked spectrum sources candidates, i.e. possibly very young and compact radio galaxies. The median values of the variability indices at 11 and 5 GHz are $V_{S_{11}}=0.14$ and $V_{S_{5}}=0.13$, which generally indicates a weak or moderate character of the long-term variability of the studied galaxies. The typical radio luminosity and radio loudness are $L_{5}=10^{43}$ - $10^{44}$ erg*s$^{-1}$ and $\\log R=3$ - $4$ respectively. We have found less prominent features of the bright compact radio cores for our sample compared to high-redshift quasars at $z\\geq3$. The variety of the obtained radio properties shows the different conditions for the formation of radio emission sources in galaxies.","sentences":["Study of high-redshift radio galaxies (HzRGs) can shed light on the active galactic nuclei (AGNs) evolution in massive elliptical galaxies.","The vast majority of observed high-redshift AGNs are quasars, and there are very few radio galaxies at redshifts $z>3$. We present the radio properties of 173 sources optically identified with radio galaxies at $z\\geqslant1$ with flux densities $S_{1.4}\\geqslant20$ mJy.","Literature data were collected for compilation of broadband radio spectra, estimation of radio variability, radio luminosity, and radio loudness.","Almost 60% of the galaxies have steep or ultra-steep radio spectra; 22% have flat, inverted, upturn, and complex spectral shapes, and 18% have peaked spectra (PS).","The majority of the PS sources in the sample (20/31) are megahertz-peaked spectrum sources candidates, i.e. possibly very young and compact radio galaxies.","The median values of the variability indices at 11 and 5 GHz are $V_{S_{11}}=0.14$ and $V_{S_{5}}=0.13$, which generally indicates a weak or moderate character of the long-term variability of the studied galaxies.","The typical radio luminosity and radio loudness are $L_{5}=10^{43}$ - $10^{44}$ erg*s$^{-1}$ and $\\log R=3$ - $4$ respectively.","We have found less prominent features of the bright compact radio cores for our sample compared to high-redshift quasars at $z\\geq3$. The variety of the obtained radio properties shows the different conditions for the formation of radio emission sources in galaxies."],"url":"http://arxiv.org/abs/2402.02283v1","category":"astro-ph.GA"}
{"created":"2024-02-03 22:08:11","title":"SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach","abstract":"This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these multiplicative training needs. To decouple object signatures from environmental conditions, we employ a Conditional Variational Autoencoder (CVAE) that allows us to reduce data collection needs from multiplicative to (nearly) linear, while synthetically generating (data for) the missing conditions. To obtain robustness with respect to dynamic disturbances, a session-aware temporal contrastive learning approach is taken. Integrating the aforementioned two approaches, SudokuSens significantly improves the robustness of deep learning for IoT applications. We explore the degree to which SudokuSens benefits downstream inference tasks in different data sets and discuss conditions under which the approach is particularly effective.","sentences":["This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection.","The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive.","The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced.","To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered.","Our framework substantially reduces these multiplicative training needs.","To decouple object signatures from environmental conditions, we employ a Conditional Variational Autoencoder (CVAE) that allows us to reduce data collection needs from multiplicative to (nearly) linear, while synthetically generating (data for) the missing conditions.","To obtain robustness with respect to dynamic disturbances, a session-aware temporal contrastive learning approach is taken.","Integrating the aforementioned two approaches, SudokuSens significantly improves the robustness of deep learning for IoT applications.","We explore the degree to which SudokuSens benefits downstream inference tasks in different data sets and discuss conditions under which the approach is particularly effective."],"url":"http://arxiv.org/abs/2402.02275v1","category":"cs.LG"}
{"created":"2024-02-03 22:04:22","title":"InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification","abstract":"Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting. On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification. The InceptionCapsule approach is presented to alleviate these two problems. This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet. It also uses the output of Inception middle layers to generate rich vectors. Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique. Kvasir data and BUSI with the GT dataset were used to evaluate this approach. This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir. In the BUSI with GT dataset, the proposed approach achieved accuracy=98.88, Precision=95.34, and F1-score=93.74, which are acceptable results compared to other approaches in the literature.","sentences":["Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting.","On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification.","The InceptionCapsule approach is presented to alleviate these two problems.","This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet.","It also uses the output of Inception middle layers to generate rich vectors.","Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique.","Kvasir data and BUSI with the GT dataset were used to evaluate this approach.","This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir.","In the BUSI with GT dataset, the proposed approach achieved accuracy=98.88, Precision=95.34, and F1-score=93.74, which are acceptable results compared to other approaches in the literature."],"url":"http://arxiv.org/abs/2402.02274v1","category":"cs.CV"}
{"created":"2024-02-03 21:29:31","title":"Federated Learning with New Knowledge: Fundamentals, Advances, and Futures","abstract":"Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued. It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge. The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development. In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms. For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process. Furthermore, we comprehensively discuss the potential future directions for FL with new knowledge, considering a variety of factors such as scenario setups, efficiency, and security. There is also a continuously updating repository for this topic: https://github.com/conditionWang/FLNK.","sentences":["Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued.","It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge.","The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development.","In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms.","For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process.","Furthermore, we comprehensively discuss the potential future directions for FL with new knowledge, considering a variety of factors such as scenario setups, efficiency, and security.","There is also a continuously updating repository for this topic: https://github.com/conditionWang/FLNK."],"url":"http://arxiv.org/abs/2402.02268v1","category":"cs.LG"}
{"created":"2024-02-03 21:12:36","title":"MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers","abstract":"Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this \"benign confidence property\" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose \"MixedNUTS\", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points, sacrificing merely 0.87 points in robust accuracy.","sentences":["Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models.","Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches.","Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this \"benign confidence property\" can reconcile accuracy and robustness in an ensemble setting.","To achieve so, we propose \"MixedNUTS\", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm.","MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output.","On CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points, sacrificing merely 0.87 points in robust accuracy."],"url":"http://arxiv.org/abs/2402.02263v1","category":"cs.LG"}
{"created":"2024-02-03 20:58:09","title":"Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model","abstract":"Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention. In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach). RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis. The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts. By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (STD) of 0.0009. It also reaches over 97.5% mean AUC value with an STD of 0.0013. In the meanwhile, RoBERTa-CNN outperforms competitive methods, demonstrating the robustness and ability to capture nuanced linguistic patterns for suicidal intentions. Therefore, RoBERTa-CNN can detect suicide intention on text data very well.","sentences":["Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention.","In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach).","RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis.","The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts.","By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets.","To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results.","For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (STD) of 0.0009.","It also reaches over 97.5% mean AUC value with an STD of 0.0013.","In the meanwhile, RoBERTa-CNN outperforms competitive methods, demonstrating the robustness and ability to capture nuanced linguistic patterns for suicidal intentions.","Therefore, RoBERTa-CNN can detect suicide intention on text data very well."],"url":"http://arxiv.org/abs/2402.02262v1","category":"cs.CL"}
{"created":"2024-02-03 20:33:39","title":"XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction","abstract":"Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences. Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extensive experiments on several real-world datasets show that our XTSFormer outperforms several baseline methods in prediction performance.","sentences":["Event prediction aims to forecast the time and type of a future event based on a historical event sequence.","Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences.","Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data.","To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data.","Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism.","These scales are determined by a bottom-up clustering algorithm.","Extensive experiments on several real-world datasets show that our XTSFormer outperforms several baseline methods in prediction performance."],"url":"http://arxiv.org/abs/2402.02258v1","category":"cs.LG"}
{"created":"2024-02-03 19:40:41","title":"Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget","abstract":"We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.","sentences":["We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers.","It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote.","We prove a theorem that runs counter to conventional wisdom.","If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples.","Our result follows from a non-trivial application of Cram\\'er's theorem, a staple in the theory of large deviations.","We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations.","In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound."],"url":"http://arxiv.org/abs/2402.02249v1","category":"cs.LG"}
{"created":"2024-02-03 19:24:45","title":"ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images","abstract":"In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.","sentences":["In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet.","Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques.","Tesseract OCR engine [37] is used for this process.","Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy.","Labeling process is done according to whether each text obtained as a result of OCR is a table element or not.","In this study, a multilayer artificial neural network model is used.","The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes.","As a result of the training, the F1 score is $0.92$."],"url":"http://arxiv.org/abs/2402.02246v1","category":"cs.CV"}
{"created":"2024-02-03 19:12:20","title":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey","abstract":"Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is available at https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.","sentences":["Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks.","However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands.","In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications.","This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements.","First, we provide a formal definition of PEFT and discuss model pre-training methods.","We then categorize existing methods into three categories: addition-based, partial-based, and unified-based.","Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges.","A comprehensive collection of resources is available at https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning."],"url":"http://arxiv.org/abs/2402.02242v1","category":"cs.CV"}
{"created":"2024-02-03 18:31:23","title":"Malaria incidence and prevalence: An ecological analysis through Six Sigma approach","abstract":"Malaria is the leading cause of death globally, especially in sub-Saharan African countries claiming over 400,000 deaths globally each year, underscoring the critical need for continued efforts to combat this preventable and treatable disease. The objective of this study is to provide statistical guidance on the optimal preventive and control measures against malaria. Data have been collected from reliable sources, such as World Health Organization, UNICEF, Our World in Data, and STATcompiler. Data were categorized according to the factors and sub-factors related to deaths caused by malaria. These factors and sub-factors were determined based on root cause analysis and data sources. Using JMP 16 Pro software, both linear and multiple linear regression were conducted to analyze the data. The analyses aimed to establish a linear relationship between the dependent variable (malaria deaths in the overall population) and independent variables, such as life expectancy, malaria prevalence in children, net usage, indoor residual spraying usage, literate population, and population with inadequate sanitation in each selected sample country. The statistical analysis revealed that using insecticide treated nets (ITNs) by children and individuals significantly decreased the death count, as 1,000 individuals sleeping under ITNs could reduce the death count by eight. Based on the statistical analysis, this study suggests more rigorous research on the usage of ITNs.","sentences":["Malaria is the leading cause of death globally, especially in sub-Saharan African countries claiming over 400,000 deaths globally each year, underscoring the critical need for continued efforts to combat this preventable and treatable disease.","The objective of this study is to provide statistical guidance on the optimal preventive and control measures against malaria.","Data have been collected from reliable sources, such as World Health Organization, UNICEF, Our World in Data, and STATcompiler.","Data were categorized according to the factors and sub-factors related to deaths caused by malaria.","These factors and sub-factors were determined based on root cause analysis and data sources.","Using JMP 16 Pro software, both linear and multiple linear regression were conducted to analyze the data.","The analyses aimed to establish a linear relationship between the dependent variable (malaria deaths in the overall population) and independent variables, such as life expectancy, malaria prevalence in children, net usage, indoor residual spraying usage, literate population, and population with inadequate sanitation in each selected sample country.","The statistical analysis revealed that using insecticide treated nets (ITNs) by children and individuals significantly decreased the death count, as 1,000 individuals sleeping under ITNs could reduce the death count by eight.","Based on the statistical analysis, this study suggests more rigorous research on the usage of ITNs."],"url":"http://arxiv.org/abs/2402.02233v1","category":"stat.OT"}
{"created":"2024-02-05 18:58:11","title":"Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining","abstract":"Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%. The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba","sentences":["Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies.","However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism.","Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling.","Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden.","However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis.","This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining.","Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models.","Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models.","Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%.","The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba"],"url":"http://arxiv.org/abs/2402.03302v1","category":"cs.CV"}
{"created":"2024-02-05 18:44:45","title":"The origin of kinematically-persistent planes of satellite galaxies as driven by the early evolution of the local Cosmic Web in $\u039b$CDM","abstract":"Kinematically-persistent planes of satellites (KPPs) are fixed sets of satellites co-orbiting around their host galaxy, whose orbital poles are conserved and clustered across long cosmic time intervals. They play the role of 'skeletons', ensuring the long-term durability of positional planes. We explore the physical processes behind their formation in terms of the dynamics of the local Cosmic Web (CW), characterized via the so-called Lagrangian Volumes (LVs) built up around two zoom-in, cosmological hydro-simulations of MW-mass disk galaxy + satellites systems, where three KPPs have been identified. By analyzing the LVs deformations in terms of the reduced Tensor of Inertia (TOI), we find an outstanding alignment between the LV principal directions and KPP satellites' orbital poles. The most compressive local mass flows (along the $\\hat{e}_3$ eigenvector) are strong at early times, feeding the so-called $\\hat{e}_3$-structure, while the smallest TOI axis rapidly decreases. The $\\hat{e}_3$-structure collapse marks the end of this regime and is the timescale for the establishment of satellite orbital pole clustering when the Universe is $\\lesssim$ 4 Gyr old. KPP proto-satellites aligned with $\\hat{e}_3$ are those whose orbital poles are either aligned from early times, or have been successfully bent at $\\hat{e}_3$-structure collapse. KPP satellites associated to $\\hat{e}_1$ tend to have early trajectories already parallel to $\\hat{e}_3$. We show that KPPs can arise as a result of the $\\Lambda$CDM-predicted large-scale dynamics acting on particular sets of proto-satellites, the same dynamics that shape the local CW environment.","sentences":["Kinematically-persistent planes of satellites (KPPs) are fixed sets of satellites co-orbiting around their host galaxy, whose orbital poles are conserved and clustered across long cosmic time intervals.","They play the role of 'skeletons', ensuring the long-term durability of positional planes.","We explore the physical processes behind their formation in terms of the dynamics of the local Cosmic Web (CW), characterized via the so-called Lagrangian Volumes (LVs) built up around two zoom-in, cosmological hydro-simulations of MW-mass disk galaxy + satellites systems, where three KPPs have been identified.","By analyzing the LVs deformations in terms of the reduced Tensor of Inertia (TOI), we find an outstanding alignment between the LV principal directions and KPP satellites' orbital poles.","The most compressive local mass flows (along the $\\hat{e}_3$ eigenvector) are strong at early times, feeding the so-called $\\hat{e}_3$-structure, while the smallest TOI axis rapidly decreases.","The $\\hat{e}_3$-structure collapse marks the end of this regime and is the timescale for the establishment of satellite orbital pole clustering when the Universe is $\\lesssim$ 4 Gyr old.","KPP proto-satellites aligned with $\\hat{e}_3$ are those whose orbital poles are either aligned from early times, or have been successfully bent at $\\hat{e}_3$-structure collapse.","KPP satellites associated to $\\hat{e}_1$ tend to have early trajectories already parallel to $\\hat{e}_3$. We show that KPPs can arise as a result of the $\\Lambda$CDM-predicted large-scale dynamics acting on particular sets of proto-satellites, the same dynamics that shape the local CW environment."],"url":"http://arxiv.org/abs/2402.03288v1","category":"astro-ph.GA"}
{"created":"2024-02-05 18:34:02","title":"An approximation of the Collatz map and a lower bound for the average total stopping time","abstract":"Define the (accelerated) Collatz map on the positive integers by $C_\\mathbb{N}(n)=\\frac{n}{2}$ if $n$ is even and $C_\\mathbb{N}(n)=\\frac{3n+1}{2}$ if $n$ is odd. We show that $C_\\mathbb{N}$ can be approximated by multiplication with $\\frac{3^{\\frac{1}{2}}}{2}$ in the sense that the set of $n$ for which $(\\frac{3^{\\frac{1}{2}}}{2})^kn^{1-\\epsilon}\\leq C^k_\\mathbb{N}(n)\\leq (\\frac{3^{\\frac{1}{2}}}{2})^kn^{1+\\epsilon}$ for all $0\\leq k\\leq \\frac{1}{1-\\frac{\\log_23}{2}}\\log_2n$ has natural density $1$ for every $\\epsilon>0$. Let $\\tau(n)$ be the minimal $k\\in\\mathbb{N}$ for which $C^k_\\mathbb{N}(n)=1$ if there exist such a $k$ and set $\\tau(n)=\\infty$ otherwise. As an application of the above we show that $\\liminf_{x\\rightarrow\\infty}\\frac{1}{x\\log_2x}\\sum_{m=1}^{\\lfloor x\\rfloor}\\tau(m)\\geq \\frac{1}{1-\\frac{\\log_23}{2}}$, partially answering a question of Crandall and Shanks. We show also that assuming the Collatz Conjecture is true in the strong sense that $\\tau(n)\\in O(\\log_2n)$ then $\\lim_{x\\rightarrow\\infty}\\frac{1}{x\\log_2x}\\sum_{m=1}^{\\lfloor x\\rfloor}\\tau(m)= \\frac{1}{1-\\frac{\\log_23}{2}}$.","sentences":["Define the (accelerated) Collatz map on the positive integers by $C_\\mathbb{N}(n)=\\frac{n}{2}$ if $n$ is even and $C_\\mathbb{N}(n)=\\frac{3n+1}{2}$ if $n$ is odd.","We show that $C_\\mathbb{N}$ can be approximated by multiplication with $\\frac{3^{\\frac{1}{2}}}{2}$ in the sense that the set of $n$ for which $(\\frac{3^{\\frac{1}{2}}}{2})^kn^{1-\\epsilon}\\leq C^k_\\mathbb{N}(n)\\leq (\\frac{3^{\\frac{1}{2}}}{2})^kn^{1+\\epsilon}$ for all $0\\leq k\\leq \\frac{1}{1-\\frac{\\log_23}{2}}\\log_2n$ has natural density $1$ for every $\\epsilon>0$. Let $\\tau(n)$ be the minimal $k\\in\\mathbb{N}$ for which $C^k_\\mathbb{N}(n)=1$ if there exist such a $k$ and set $\\tau(n)=\\infty$ otherwise.","As an application of the above we show that $\\liminf_{x\\rightarrow\\infty}\\frac{1}{x\\log_2x}\\sum_{m=1}^{\\lfloor x\\rfloor}\\tau(m)\\geq \\frac{1}{1-\\frac{\\log_23}{2}}$, partially answering a question of Crandall and Shanks.","We show also that assuming the Collatz Conjecture is true in the strong sense that $\\tau(n)\\in O(\\log_2n)$ then $\\lim_{x\\rightarrow\\infty}\\frac{1}{x\\log_2x}\\sum_{m=1}^{\\lfloor x\\rfloor}\\tau(m)= \\frac{1}{1-\\frac{\\log_23}{2}}$."],"url":"http://arxiv.org/abs/2402.03276v1","category":"math.DS"}
{"created":"2024-02-05 18:31:44","title":"Bounding the Weisfeiler-Leman Dimension via a Depth Analysis of I/R-Trees","abstract":"The Weisfeiler-Leman (WL) dimension is an established measure for the inherent descriptive complexity of graphs and relational structures. It corresponds to the number of variables that are needed and sufficient to define the object of interest in a counting version of first-order logic (FO). These bounded-variable counting logics were even candidates to capture graph isomorphism, until a celebrated construction due to Cai, F\\\"urer, and Immerman [Combinatorica 1992] showed that $\\Omega(n)$ variables are required to distinguish all non-isomorphic $n$-vertex graphs.   Still, very little is known about the precise number of variables required and sufficient to define every $n$-vertex graph. For the bounded-variable (non-counting) FO fragments, Pikhurko, Veith, and Verbitsky [Discret. Appl. Math. 2006] provided an upper bound of $\\frac{n+3}{2}$ and showed that it is essentially tight. Our main result yields that, in the presence of counting quantifiers, $\\frac{n}{4} + o(n)$ variables suffice. This shows that counting does allow us to save variables when defining graphs. As an application of our techniques, we also show new bounds in terms of the vertex cover number of the graph.   To obtain the results, we introduce a new concept called the WL depth of a graph. We use it to analyze branching trees within the Individualization/Refinement (I/R) paradigm from the domain of isomorphism algorithms. We extend the recursive procedure from the I/R paradigm by the possibility of splitting the graphs into independent parts. Then we bound the depth of the obtained branching trees, which translates into bounds on the WL dimension and thereby on the number of variables that suffice to define the graphs.","sentences":["The Weisfeiler-Leman (WL) dimension is an established measure for the inherent descriptive complexity of graphs and relational structures.","It corresponds to the number of variables that are needed and sufficient to define the object of interest in a counting version of first-order logic (FO).","These bounded-variable counting logics were even candidates to capture graph isomorphism, until a celebrated construction due to Cai, F\\\"urer, and Immerman [Combinatorica 1992] showed that $\\Omega(n)$ variables are required to distinguish all non-isomorphic $n$-vertex graphs.   ","Still, very little is known about the precise number of variables required and sufficient to define every $n$-vertex graph.","For the bounded-variable (non-counting) FO fragments, Pikhurko, Veith, and Verbitsky [Discret.","Appl.","Math. 2006] provided an upper bound of $\\frac{n+3}{2}$ and showed that it is essentially tight.","Our main result yields that, in the presence of counting quantifiers, $\\frac{n}{4} + o(n)$ variables suffice.","This shows that counting does allow us to save variables when defining graphs.","As an application of our techniques, we also show new bounds in terms of the vertex cover number of the graph.   ","To obtain the results, we introduce a new concept called the WL depth of a graph.","We use it to analyze branching trees within the Individualization/Refinement (I/R) paradigm from the domain of isomorphism algorithms.","We extend the recursive procedure from the I/R paradigm by the possibility of splitting the graphs into independent parts.","Then we bound the depth of the obtained branching trees, which translates into bounds on the WL dimension and thereby on the number of variables that suffice to define the graphs."],"url":"http://arxiv.org/abs/2402.03274v1","category":"cs.DM"}
{"created":"2024-02-05 18:27:46","title":"Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT Protocol","abstract":"The large number of sensors and actuators that make up the Internet of Things obliges these systems to use diverse technologies and protocols. This means that IoT networks are more heterogeneous than traditional networks. This gives rise to new challenges in cybersecurity to protect these systems and devices which are characterized by being connected continuously to the Internet. Intrusion detection systems (IDS) are used to protect IoT systems from the various anomalies and attacks at the network level. Intrusion Detection Systems (IDS) can be improved through machine learning techniques. Our work focuses on creating classification models that can feed an IDS using a dataset containing frames under attacks of an IoT system that uses the MQTT protocol. We have addressed two types of method for classifying the attacks, ensemble methods and deep learning models, more specifically recurrent networks with very satisfactory results.","sentences":["The large number of sensors and actuators that make up the Internet of Things obliges these systems to use diverse technologies and protocols.","This means that IoT networks are more heterogeneous than traditional networks.","This gives rise to new challenges in cybersecurity to protect these systems and devices which are characterized by being connected continuously to the Internet.","Intrusion detection systems (IDS) are used to protect IoT systems from the various anomalies and attacks at the network level.","Intrusion Detection Systems (IDS) can be improved through machine learning techniques.","Our work focuses on creating classification models that can feed an IDS using a dataset containing frames under attacks of an IoT system that uses the MQTT protocol.","We have addressed two types of method for classifying the attacks, ensemble methods and deep learning models, more specifically recurrent networks with very satisfactory results."],"url":"http://arxiv.org/abs/2402.03270v1","category":"cs.LG"}
{"created":"2024-02-05 18:27:27","title":"ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds","abstract":"Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models. Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds. In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text. We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations. By representing animal sounds with text, we effectively treat them as a \"foreign language,\" and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance.","sentences":["Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models.","Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds.","In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text.","We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations.","By representing animal sounds with text, we effectively treat them as a \"foreign language,\" and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance."],"url":"http://arxiv.org/abs/2402.03269v1","category":"cs.SD"}
{"created":"2024-02-05 18:15:50","title":"Freeze-Tag in $L_1$ has Wake-up Time Five","abstract":"The Freeze-Tag Problem, introduced in Arkin et al. (SODA'02) consists of waking up a swarm of $n$ robots, starting from a single active robot. In the basic geometric version, every robot is given coordinates in the plane. As soon as a robot is awakened, it can move towards inactive robots to wake them up. The goal is to minimize the wake-up time of the last robot, the makespan.   Despite significant progress on the computational complexity of this problem and on approximation algorithms, the characterization of exact bounds on the makespan remains one of the main open questions. In this paper, we settle this question for the $\\ell_1$-norm, showing that a makespan of at most $5r$ can always be achieved, where $r$ is the maximum distance between the initial active robot and any sleeping robot. Moreover, a schedule achieving a makespan of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time and the makespan are optimal. This implies a new upper bound of $5\\sqrt{2}r \\approx 7.07r$ on the makespan in the $\\ell_2$-norm, improving the best known bound so far $(5+2\\sqrt{2}+\\sqrt{5})r \\approx 10.06r$.","sentences":["The Freeze-Tag Problem, introduced in Arkin et al.","(SODA'02) consists of waking up a swarm of $n$ robots, starting from a single active robot.","In the basic geometric version, every robot is given coordinates in the plane.","As soon as a robot is awakened, it can move towards inactive robots to wake them up.","The goal is to minimize the wake-up time of the last robot, the makespan.   ","Despite significant progress on the computational complexity of this problem and on approximation algorithms, the characterization of exact bounds on the makespan remains one of the main open questions.","In this paper, we settle this question for the $\\ell_1$-norm, showing that a makespan of at most $5r$ can always be achieved, where $r$ is the maximum distance between the initial active robot and any sleeping robot.","Moreover, a schedule achieving a makespan of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time and the makespan are optimal.","This implies a new upper bound of $5\\sqrt{2}r \\approx 7.07r$ on the makespan in the $\\ell_2$-norm, improving the best known bound so far $(5+2\\sqrt{2}+\\sqrt{5})r \\approx 10.06r$."],"url":"http://arxiv.org/abs/2402.03258v1","category":"cs.DS"}
{"created":"2024-02-05 18:05:53","title":"On a question of Gary G. Gundersen concerning meromorphic functions sharing three distinct values IM and a fourth value CM","abstract":"In 1992, Gundersen (Complex Var. Elliptic Equ.20 (1992), no. 1-4, 99-106.) proposed the following famous open question: if two non-constant meromorphic functions share three values IM and share a fourth value CM, then do the functions necessarily share all four values CM? The open question is a long-standing question in the studies of the Nevanlinna$'$s value distribution theory of meromorphic functions, and has not been completely resolved by now. In this paper, we prove that if two distinct non-constant meromorphic functions $f$ and $g$ of finite order share $0,$ $1,$ $c$ IM and $\\infty$ CM, where $c$ is a finite complex value such that $c\\not\\in\\{0,1\\},$ then $f$ and $g$ share $0,$ $1,$ $c,$ $\\infty$ CM. Applying the main result obtained in this paper, we completely resolve a question proposed by Gary G. Gundersen on Page 458 of his paper (J. London Math. Soc. 20(1979), no. 2, 457-466.)concerning the nonexistence of two distinct non-constant meromorphic functions sharing three distinct values DM and a fourth value CM. The obtained result also improves the corresponding result on Pages 109-117 in (E. Mues, Bemerkungen zum vier-punkte-satz, Complex Methods on Partial Diferential Equations, 109-117, Math. Res. 53, Akademie-Verlag, Berlin, 1989.) concerning the nonexistence of two distinct non-constant entire functions that share three distinct finite values DM. Examples are provided to show that the main results obtained in this paper, in a sense, are best possible.","sentences":["In 1992, Gundersen (Complex Var.","Elliptic Equ.20 (1992), no. 1-4, 99-106.) proposed the following famous open question: if two non-constant meromorphic functions share three values IM and share a fourth value CM, then do the functions necessarily share all four values CM?","The open question is a long-standing question in the studies of the Nevanlinna$'$s value distribution theory of meromorphic functions, and has not been completely resolved by now.","In this paper, we prove that if two distinct non-constant meromorphic functions $f$ and $g$ of finite order share $0,$ $1,$ $c$ IM and $\\infty$ CM, where $c$ is a finite complex value such that $c\\not\\in\\{0,1\\},$ then $f$ and $g$ share $0,$ $1,$ $c,$ $\\infty$ CM.","Applying the main result obtained in this paper, we completely resolve a question proposed by Gary G. Gundersen on Page 458 of his paper (J. London Math.","Soc.","20(1979)",", no. 2, 457-466.)concerning the nonexistence of two distinct non-constant meromorphic functions sharing three distinct values DM and a fourth value CM.","The obtained result also improves the corresponding result on Pages 109-117 in (E. Mues, Bemerkungen zum vier-punkte-satz, Complex Methods on Partial Diferential Equations, 109-117, Math. Res. 53, Akademie-Verlag, Berlin, 1989.)","concerning the nonexistence of two distinct non-constant entire functions that share three distinct finite values DM.","Examples are provided to show that the main results obtained in this paper, in a sense, are best possible."],"url":"http://arxiv.org/abs/2402.03248v1","category":"math.CV"}
{"created":"2024-02-05 17:55:51","title":"Bluesky and the AT Protocol: Usable Decentralized Social Media","abstract":"Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 3 million registered users in the following year. In this paper we introduce the architecture of Bluesky and the AT Protocol, which is inspired by the web itself, but modernized to include streams of real-time updates and cryptographic authentication. We explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation.","sentences":["Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media.","It was launched in private beta in February 2023, and has grown to over 3 million registered users in the following year.","In this paper we introduce the architecture of Bluesky and the AT Protocol, which is inspired by the web itself, but modernized to include streams of real-time updates and cryptographic authentication.","We explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature.","The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation."],"url":"http://arxiv.org/abs/2402.03239v1","category":"cs.DC"}
{"created":"2024-02-05 17:51:43","title":"Spectroscopic Insights into the Quiescent Stages of RS Ophiuchi (2006-2021): Photoionization Modeling and Accretion Dynamics","abstract":"This paper presents a comprehensive spectroscopic analysis of the nova RS Ophiuchi during its quiescent stage, spanning a duration of approximately 13 years. The spectra exhibit prominent low-ionization emission features, including hydrogen, helium, iron, and TiO absorption features originating from the cool secondary component. The CLOUDY photoionization code is employed to model these spectra, allowing us to estimate various physical parameters such as temperature, luminosity, and hydrogen density, along with elemental abundances and accretion rate. The central ionizing sources exhibit temperatures in the range of $1.05 - 1.8~\\times 10^4$ K and luminosities between $0.1 - 7.9~\\times 10^{30}$ \\ergs. Notably, \\ion{He}{} displays an overabundance from 2008 to 2016, returning to solar values by 2020, while \\ion{Fe}{} appears subsolar from 2008 to 2014 but becomes overabundant from 2006 onward. The mean accretion rate, as calculated from the model, is approximately $1.254 \\times 10^{-8} M_{\\odot}$ yr$^{-1}$. About 47\\% of the critical mass was accreted after April, 2020 ($\\sim$15 months before the 2021 outburst), and approximately 88\\% of the critical mass was accreted after July 20, 2018. This non-uniform accretion rate suggests a more rapid approach towards reaching the critical mass in the final years, possibly attributed to the heightened gravitational pull resulting from previously accreted matter, influencing the accretion dynamics as the system approaches the critical mass limit.","sentences":["This paper presents a comprehensive spectroscopic analysis of the nova RS Ophiuchi during its quiescent stage, spanning a duration of approximately 13 years.","The spectra exhibit prominent low-ionization emission features, including hydrogen, helium, iron, and TiO absorption features originating from the cool secondary component.","The CLOUDY photoionization code is employed to model these spectra, allowing us to estimate various physical parameters such as temperature, luminosity, and hydrogen density, along with elemental abundances and accretion rate.","The central ionizing sources exhibit temperatures in the range of $1.05 - 1.8~\\times 10^4$ K and luminosities between $0.1 - 7.9~\\times 10^{30}$ \\ergs.","Notably, \\ion{He}{} displays an overabundance from 2008 to 2016, returning to solar values by 2020, while \\ion{Fe}{} appears subsolar from 2008 to 2014 but becomes overabundant from 2006 onward.","The mean accretion rate, as calculated from the model, is approximately $1.254 \\times 10^{-8} M_{\\odot}$","yr$^{-1}$. About 47\\% of the critical mass was accreted after April, 2020 ($\\sim$15 months before the 2021 outburst), and approximately 88\\% of the critical mass was accreted after July 20, 2018.","This non-uniform accretion rate suggests a more rapid approach towards reaching the critical mass in the final years, possibly attributed to the heightened gravitational pull resulting from previously accreted matter, influencing the accretion dynamics as the system approaches the critical mass limit."],"url":"http://arxiv.org/abs/2402.03234v1","category":"astro-ph.SR"}
{"created":"2024-02-05 17:43:02","title":"CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models","abstract":"Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic surgery. Our study systematically examines the impact of different attention mechanisms, number of resolution stages, and network configurations on segmentation accuracy and computational complexity. To allow cross-reference with other recent benchmarking studies, we also included a performance assessment of the BTCV abdominal structural segmentation. With the STUNet ranking at the top, our study demonstrated the value of CNN-based U-shaped models for the investigated tasks and the benefit of residual blocks in network configuration designs to boost segmentation performance.","sentences":["Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms.","Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet.","To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations.","Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare.","We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic surgery.","Our study systematically examines the impact of different attention mechanisms, number of resolution stages, and network configurations on segmentation accuracy and computational complexity.","To allow cross-reference with other recent benchmarking studies, we also included a performance assessment of the BTCV abdominal structural segmentation.","With the STUNet ranking at the top, our study demonstrated the value of CNN-based U-shaped models for the investigated tasks and the benefit of residual blocks in network configuration designs to boost segmentation performance."],"url":"http://arxiv.org/abs/2402.03230v1","category":"cs.CV"}
{"created":"2024-02-05 17:41:52","title":"Disentangling high order effects in the transfer entropy","abstract":"Transfer Entropy (TE), the main approach to determine the directed information flow within a network system, can be biased (in defect or excess), both in the pairwise and conditioned calculation, due to high order dependencies among the two dynamic processes under consideration and the remaining processes in the system which are used in conditioning. Here we propose a novel approach which, instead of conditioning the TE on all the network processes other than driver and target like in its fully conditioned version, or not conditioning at all like in the pairwise approach, searches both for the multiplet of variables leading to the maximum information flow and for those minimizing it, providing a decomposition of the TE in unique, redundant and synergistic atoms. Our approach allows to quantify the relative importance of high order effects, w.r.t. pure two-body effects, in the information transfer between two processes, and to highlight those processes which accompany the driver to build those high order effects. We report an application of the proposed approach in climatology, analyzing data from El Ni\\~{n}o and the Southern Oscillation.","sentences":["Transfer Entropy (TE), the main approach to determine the directed information flow within a network system, can be biased (in defect or excess), both in the pairwise and conditioned calculation, due to high order dependencies among the two dynamic processes under consideration and the remaining processes in the system which are used in conditioning.","Here we propose a novel approach which, instead of conditioning the TE on all the network processes other than driver and target like in its fully conditioned version, or not conditioning at all like in the pairwise approach, searches both for the multiplet of variables leading to the maximum information flow and for those minimizing it, providing a decomposition of the TE in unique, redundant and synergistic atoms.","Our approach allows to quantify the relative importance of high order effects, w.r.t.","pure two-body effects, in the information transfer between two processes, and to highlight those processes which accompany the driver to build those high order effects.","We report an application of the proposed approach in climatology, analyzing data from El Ni\\~{n}o and the Southern Oscillation."],"url":"http://arxiv.org/abs/2402.03229v1","category":"physics.data-an"}
{"created":"2024-02-05 17:40:55","title":"Dynamical complex networks for understanding motility induced phase separation in Active Brownian Particles","abstract":"We investigate the dynamics of active Brownian particles (ABP) through a temporal complex network framework, focusing on degree probability distribution, average path length, and clusterization across Motility-Induced Phase Separation (MIPS) regions. In the single MIPS phase, particle encounters resemble a random graph. Transitioning toward the MIPS frontier introduces a non-power-law distribution, while the phase-separated region exhibits a double power-law distribution. Numerical analysis indicates a shift from random to power-law complex networks based on the system's MIPS-phase position. In summary, our numerical and theoretical analysis reveals the phase transition of the system between two states and how this transition manifests itself in the complex network, showing an evident change between a random network and a network that tends to have scale-free behavior.","sentences":["We investigate the dynamics of active Brownian particles (ABP) through a temporal complex network framework, focusing on degree probability distribution, average path length, and clusterization across Motility-Induced Phase Separation (MIPS) regions.","In the single MIPS phase, particle encounters resemble a random graph.","Transitioning toward the MIPS frontier introduces a non-power-law distribution, while the phase-separated region exhibits a double power-law distribution.","Numerical analysis indicates a shift from random to power-law complex networks based on the system's MIPS-phase position.","In summary, our numerical and theoretical analysis reveals the phase transition of the system between two states and how this transition manifests itself in the complex network, showing an evident change between a random network and a network that tends to have scale-free behavior."],"url":"http://arxiv.org/abs/2402.03228v1","category":"physics.bio-ph"}
{"created":"2024-02-05 17:24:15","title":"Angular Momentum Transport in Binary Star Formation: The Enhancement of Magneto-Rotational Instability and Role of Outflows","abstract":"The formation of binary stars is highly influenced by magnetic fields, which play a crucial role in transporting angular momentum. We conducted three-dimensional numerical simulations of binary star accretion via a circumbinary disk, taking into account a magnetic field perpendicular to the disk and an infalling envelope. Our simulations reproduce the following phenomena: (1) spiral arms associated with circumstellar disks, (2) turbulence in the circumbinary disk, induced by magneto-rotational instability (MRI), (3) a fast outflow launched from each circumstellar disk, and (4) a slow outflow from the circumbinary disk. The binary models exhibit a higher $\\alpha$-parameter than the corresponding single star models, indicating that the binary stars enhance MRI turbulence. Moreover, an infalling envelope also enhance the turbulence, leading to a high $\\alpha$-parameter. While the spiral arms promotes radial flow, causing transfer of mass and angular momentum within the circumbinary disk, the MRI turbulence and outflows are main drivers of angular momentum transfer to reduce the specific angular momentum of the system.","sentences":["The formation of binary stars is highly influenced by magnetic fields, which play a crucial role in transporting angular momentum.","We conducted three-dimensional numerical simulations of binary star accretion via a circumbinary disk, taking into account a magnetic field perpendicular to the disk and an infalling envelope.","Our simulations reproduce the following phenomena: (1) spiral arms associated with circumstellar disks, (2) turbulence in the circumbinary disk, induced by magneto-rotational instability (MRI), (3) a fast outflow launched from each circumstellar disk, and (4) a slow outflow from the circumbinary disk.","The binary models exhibit a higher $\\alpha$-parameter than the corresponding single star models, indicating that the binary stars enhance MRI turbulence.","Moreover, an infalling envelope also enhance the turbulence, leading to a high $\\alpha$-parameter.","While the spiral arms promotes radial flow, causing transfer of mass and angular momentum within the circumbinary disk, the MRI turbulence and outflows are main drivers of angular momentum transfer to reduce the specific angular momentum of the system."],"url":"http://arxiv.org/abs/2402.03212v1","category":"astro-ph.SR"}
{"created":"2024-02-05 17:22:41","title":"Fast classical simulation of Harvard/QuEra IQP circuits","abstract":"Establishing an advantage for (white-box) computations by a quantum computer against its classical counterpart is currently a key goal for the quantum computation community. A quantum advantage is achieved once a certain computational capability of a quantum computer is so complex that it can no longer be reproduced by classical means, and as such, the quantum advantage can be seen as a continued negotiation between classical simulations and quantum computational experiments.   A recent publication (Bluvstein et al., Nature 626:58-65, 2024) introduces a type of Instantaneous Quantum Polynomial-Time (IQP) computation complemented by a $48$-qubit (logical) experimental demonstration using quantum hardware. The authors state that the ``simulation of such logical circuits is challenging'' and project the simulation time to grow rapidly with the number of CNOT layers added, see Figure 5d/bottom therein. However, we report a classical simulation algorithm that takes only $0.00257947$ seconds to compute an amplitude for the $48$-qubit computation, which is roughly $10^3$ times faster than that reported by the original authors. Our algorithm is furthermore not subject to a significant decline in performance due to the additional CNOT layers. We simulated these types of IQP computations for up to $96$ qubits, taking an average of $4.16629$ seconds to compute a single amplitude, and estimated that a $192$-qubit simulation should be tractable for computations relying on Tensor Processing Units.","sentences":["Establishing an advantage for (white-box) computations by a quantum computer against its classical counterpart is currently a key goal for the quantum computation community.","A quantum advantage is achieved once a certain computational capability of a quantum computer is so complex that it can no longer be reproduced by classical means, and as such, the quantum advantage can be seen as a continued negotiation between classical simulations and quantum computational experiments.   ","A recent publication (Bluvstein et al., Nature 626:58-65, 2024) introduces a type of Instantaneous Quantum Polynomial-Time (IQP) computation complemented by a $48$-qubit (logical) experimental demonstration using quantum hardware.","The authors state that the ``simulation of such logical circuits is challenging'' and project the simulation time to grow rapidly with the number of CNOT layers added, see Figure 5d/bottom therein.","However, we report a classical simulation algorithm that takes only $0.00257947$ seconds to compute an amplitude for the $48$-qubit computation, which is roughly $10^3$ times faster than that reported by the original authors.","Our algorithm is furthermore not subject to a significant decline in performance due to the additional CNOT layers.","We simulated these types of IQP computations for up to $96$ qubits, taking an average of $4.16629$ seconds to compute a single amplitude, and estimated that a $192$-qubit simulation should be tractable for computations relying on Tensor Processing Units."],"url":"http://arxiv.org/abs/2402.03211v1","category":"quant-ph"}
{"created":"2024-02-05 17:00:39","title":"Direct Antenna Frequency-Hopped M-FSK Modulation With Time-Modulated Arrays","abstract":"We present an innovative approach that simultaneously enables direct antenna frequency-hopped M-ary frequency shift keying (DAFH-MFSK) modulation and beamsteering through the use of time-modulated arrays (TMAs). The distinctive feature of our approach lies in the modulation of the TMA excitations with binary periodic sequences, which can be easily frequency-adjusted and time-delayed to simultaneously allow for DAFH-MFSK direct antenna modulation and beamsteering. Notably, our TMA proposal offers a distinct advantage over conventional architectures in terms of performance metrics, including reduced insertion losses and enhanced phase resolution for beamsteering, while also simplifying hardware complexity.","sentences":["We present an innovative approach that simultaneously enables direct antenna frequency-hopped M-ary frequency shift keying (DAFH-MFSK) modulation and beamsteering through the use of time-modulated arrays (TMAs).","The distinctive feature of our approach lies in the modulation of the TMA excitations with binary periodic sequences, which can be easily frequency-adjusted and time-delayed to simultaneously allow for DAFH-MFSK direct antenna modulation and beamsteering.","Notably, our TMA proposal offers a distinct advantage over conventional architectures in terms of performance metrics, including reduced insertion losses and enhanced phase resolution for beamsteering, while also simplifying hardware complexity."],"url":"http://arxiv.org/abs/2402.03194v1","category":"eess.SP"}
{"created":"2024-02-05 16:45:38","title":"Cool-chic video: Learned video coding with 800 parameters","abstract":"We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall. To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity. It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies. The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV. The system is made open-source: orange-opensource.github.io/Cool-Chic.","sentences":["We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall.","To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity.","It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies.","The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV.","The system is made open-source: orange-opensource.github.io/Cool-Chic."],"url":"http://arxiv.org/abs/2402.03179v1","category":"eess.IV"}
{"created":"2024-02-05 16:41:17","title":"Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression","abstract":"Consensus control in multi-agent systems has received significant attention and practical implementation across various domains. However, managing consensus control under unknown dynamics remains a significant challenge for control design due to system uncertainties and environmental disturbances. This paper presents a novel learning-based distributed control law, augmented by an auxiliary dynamics. Gaussian processes are harnessed to compensate for the unknown components of the multi-agent system. For continuous enhancement in predictive performance of Gaussian process model, a data-efficient online learning strategy with a decentralized event-triggered mechanism is proposed. Furthermore, the control performance of the proposed approach is ensured via the Lyapunov theory, based on a probabilistic guarantee for prediction error bounds. To demonstrate the efficacy of the proposed learning-based controller, a comparative analysis is conducted, contrasting it with both conventional distributed control laws and offline learning methodologies.","sentences":["Consensus control in multi-agent systems has received significant attention and practical implementation across various domains.","However, managing consensus control under unknown dynamics remains a significant challenge for control design due to system uncertainties and environmental disturbances.","This paper presents a novel learning-based distributed control law, augmented by an auxiliary dynamics.","Gaussian processes are harnessed to compensate for the unknown components of the multi-agent system.","For continuous enhancement in predictive performance of Gaussian process model, a data-efficient online learning strategy with a decentralized event-triggered mechanism is proposed.","Furthermore, the control performance of the proposed approach is ensured via the Lyapunov theory, based on a probabilistic guarantee for prediction error bounds.","To demonstrate the efficacy of the proposed learning-based controller, a comparative analysis is conducted, contrasting it with both conventional distributed control laws and offline learning methodologies."],"url":"http://arxiv.org/abs/2402.03174v1","category":"eess.SY"}
{"created":"2024-02-05 16:39:15","title":"Homograph Attacks on Maghreb Sentiment Analyzers","abstract":"We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in \"Arabizi\". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.","sentences":["We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries.","Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in \"Arabizi\".","The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning."],"url":"http://arxiv.org/abs/2402.03171v1","category":"cs.CL"}
{"created":"2024-02-05 16:39:12","title":"Is Mamba Capable of In-Context Learning?","abstract":"This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.","sentences":["This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers.","We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems.","Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL.","Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations.","Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences."],"url":"http://arxiv.org/abs/2402.03170v1","category":"cs.LG"}
{"created":"2024-02-05 16:35:30","title":"Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity","abstract":"Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transient iteration complexity under more relaxed assumptions compared to existing methods. Numerical experiments validate our theoretical findings.","sentences":["Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures.","To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness.","However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures.","In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms.","D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transient iteration complexity under more relaxed assumptions compared to existing methods.","Numerical experiments validate our theoretical findings."],"url":"http://arxiv.org/abs/2402.03167v1","category":"math.OC"}
{"created":"2024-02-05 16:33:29","title":"Risk-Aware MPC for Stochastic Systems with Runtime Temporal Logics","abstract":"This paper concerns the risk-aware control of stochastic systems with temporal logic specifications dynamically assigned during runtime. Conventional risk-aware control typically assumes that all specifications are predefined and remain unchanged during runtime. In this paper, we propose a novel, provably correct control scheme for linear systems with unbounded stochastic disturbances that dynamically evaluates the feasibility of runtime signal temporal logic specifications and automatically reschedules the control inputs. The method guarantees the probabilistic satisfaction of newly accepted runtime specifications without sacrificing the satisfaction of the previously accepted ones. The proposed control method is validated by a robotic motion planning case study. The idea of closed-loop control rescheduling with probabilistic risk guarantees provides a novel solution for runtime control synthesis of stochastic systems.","sentences":["This paper concerns the risk-aware control of stochastic systems with temporal logic specifications dynamically assigned during runtime.","Conventional risk-aware control typically assumes that all specifications are predefined and remain unchanged during runtime.","In this paper, we propose a novel, provably correct control scheme for linear systems with unbounded stochastic disturbances that dynamically evaluates the feasibility of runtime signal temporal logic specifications and automatically reschedules the control inputs.","The method guarantees the probabilistic satisfaction of newly accepted runtime specifications without sacrificing the satisfaction of the previously accepted ones.","The proposed control method is validated by a robotic motion planning case study.","The idea of closed-loop control rescheduling with probabilistic risk guarantees provides a novel solution for runtime control synthesis of stochastic systems."],"url":"http://arxiv.org/abs/2402.03165v1","category":"eess.SY"}
{"created":"2024-02-05 16:31:03","title":"Linguistic features for sentence difficulty prediction in ABSA","abstract":"One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: \"Laptops\", \"Restaurants\", and \"MTSC\" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their characteristics. We employ two ways of defining sentence difficulty. The first one is binary and labels a sentence as difficult if the classifiers fail to correctly predict the sentiment polarity. The second one is a six-level scale based on how many of the top five best-performing classifiers can correctly predict the sentiment polarity. We also define 9 linguistic features that, combined, aim at estimating the difficulty at sentence level.","sentences":["One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance.","Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect.","Aspect-based sentiment analysis is a well-studied topic with many available data sets and models.","However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis.","In this paper, we explore this question by conducting an experiment with three data sets: \"Laptops\", \"Restaurants\", and \"MTSC\" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets.","We study the impact of domain diversity and syntactic diversity on difficulty.","We use a combination of classifiers to identify the most difficult sentences and analyze their characteristics.","We employ two ways of defining sentence difficulty.","The first one is binary and labels a sentence as difficult if the classifiers fail to correctly predict the sentiment polarity.","The second one is a six-level scale based on how many of the top five best-performing classifiers can correctly predict the sentiment polarity.","We also define 9 linguistic features that, combined, aim at estimating the difficulty at sentence level."],"url":"http://arxiv.org/abs/2402.03163v1","category":"cs.CL"}
{"created":"2024-02-05 16:27:59","title":"Optimal and Near-Optimal Adaptive Vector Quantization","abstract":"Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.","sentences":["Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets.","The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case.","However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   ","We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity.","We also present an even faster near-optimal algorithm for large inputs.","Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications."],"url":"http://arxiv.org/abs/2402.03158v1","category":"cs.LG"}
{"created":"2024-02-05 16:24:27","title":"Trustworthiness of Optimality Condition Violation in Inverse Optimal Control Methods Based on the Minimum Principle","abstract":"In this work, we analyze the applicability of Inverse Optimal Control (IOC) methods based on the minimum principle (MP). The IOC method determines unknown cost functions in a single- or multi-agent setting from observed system trajectories by minimizing the so-called residual error, i.e. the extent to which the optimality conditions of the MP are violated with a current guess of cost functions. The main assumption of the IOC method to recover cost functions such that the resulting trajectories match the observed ones is that the given trajectories are the result of an OC problem with a known parameterized cost function structure. However, in practice, when the IOC method is used to identify the behavior of unknown agents, e.g. humans, this assumption cannot be guaranteed. Hence, we introduce the notion of the trustworthiness of the residual error and provide necessary conditions for it to define when the IOC method based on the MP is still applicable to such problems. From the necessary conditions, we conclude that the residual-based IOC method cannot be used to validate OC models for unknown agents. Finally, we illustrate this problem by validating a differential game model for the collision avoidance behavior between two mobile robots with human operators.","sentences":["In this work, we analyze the applicability of Inverse Optimal Control (IOC) methods based on the minimum principle (MP).","The IOC method determines unknown cost functions in a single- or multi-agent setting from observed system trajectories by minimizing the so-called residual error, i.e. the extent to which the optimality conditions of the MP are violated with a current guess of cost functions.","The main assumption of the IOC method to recover cost functions such that the resulting trajectories match the observed ones is that the given trajectories are the result of an OC problem with a known parameterized cost function structure.","However, in practice, when the IOC method is used to identify the behavior of unknown agents, e.g. humans, this assumption cannot be guaranteed.","Hence, we introduce the notion of the trustworthiness of the residual error and provide necessary conditions for it to define when the IOC method based on the MP is still applicable to such problems.","From the necessary conditions, we conclude that the residual-based IOC method cannot be used to validate OC models for unknown agents.","Finally, we illustrate this problem by validating a differential game model for the collision avoidance behavior between two mobile robots with human operators."],"url":"http://arxiv.org/abs/2402.03157v1","category":"math.OC"}
{"created":"2024-02-05 16:19:31","title":"Rational Maps of Balls and their Associated Groups","abstract":"Given a proper, rational map of balls, D'Angelo and Xiao introduced five natural groups encoding properties of the map. We study these groups using a recently discovered normal form for rational maps of balls. Using this normal form, we also provide several new groups associated to the map.","sentences":["Given a proper, rational map of balls, D'Angelo and Xiao introduced five natural groups encoding properties of the map.","We study these groups using a recently discovered normal form for rational maps of balls.","Using this normal form, we also provide several new groups associated to the map."],"url":"http://arxiv.org/abs/2402.03152v1","category":"math.CV"}
{"created":"2024-02-05 16:13:00","title":"A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning","abstract":"In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications.","sentences":["In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data.","A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows.","In this paper we tackle this issue by using a multi-step objective to train one-step models.","Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons.","We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments.","To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system.","Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons.","Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications."],"url":"http://arxiv.org/abs/2402.03146v1","category":"cs.LG"}
{"created":"2024-02-05 16:11:43","title":"Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models","abstract":"Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN.","sentences":["Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields.","Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications.","In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE).","KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state.","This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings.","Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%.","In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness.","Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN."],"url":"http://arxiv.org/abs/2402.03142v1","category":"cs.LG"}
{"created":"2024-02-05 15:59:04","title":"Revivals, or the Talbot effect, for the Airy equation","abstract":"We study Dirichlet-type problems for the simplest third-order linear dispersive PDE, commonly called the Airy equation. Such problems have not been extensively studied, perhaps due to the complexity of the spectral structure of the spatial operator. Our specific interest is to determine whether the peculiar phenomenon of revivals, also known as Talbot effect, is supported by these boundary conditions, which for third order problems are not reducible to periodic ones. We prove that this is the case only for a very special choice of the boundary conditions, for which a new type of weak cusp revival phenomenon has been recently discovered. We also give some new results on the functional class of the solution for other cases.","sentences":["We study Dirichlet-type problems for the simplest third-order linear dispersive PDE, commonly called the Airy equation.","Such problems have not been extensively studied, perhaps due to the complexity of the spectral structure of the spatial operator.","Our specific interest is to determine whether the peculiar phenomenon of revivals, also known as Talbot effect, is supported by these boundary conditions, which for third order problems are not reducible to periodic ones.","We prove that this is the case only for a very special choice of the boundary conditions, for which a new type of weak cusp revival phenomenon has been recently discovered.","We also give some new results on the functional class of the solution for other cases."],"url":"http://arxiv.org/abs/2402.03133v1","category":"math.AP"}
{"created":"2024-02-05 15:58:56","title":"Entropy of Singular Suspensions","abstract":"In this work, we investigate diffeomorphisms whose positiveness of topological entropy is destroyed by singular suspensions. We show that this phenomenon is rare in the set of $C^1$-diffeomorphisms. Precisely, we prove that for an open and dense set of $C^1$-diffeomorphism positive topological entropy is preserved by singular suspensions, even for suspensions with infinitely many singularities. We prove a similar result to the conservative diffeomorphisms. We apply our techniques to show that every expansive singular suspension $C^{1+\\epsilon}$-flow over a three-dimensional manifold has positive topological entropy. Finally, we explore this phenomenon for Anosov dynamics, showing that to nullify the topological entropy for Anosov suspension flow, the set of singularities must capture the non-wandering set.","sentences":["In this work, we investigate diffeomorphisms whose positiveness of topological entropy is destroyed by singular suspensions.","We show that this phenomenon is rare in the set of $C^1$-diffeomorphisms.","Precisely, we prove that for an open and dense set of $C^1$-diffeomorphism positive topological entropy is preserved by singular suspensions, even for suspensions with infinitely many singularities.","We prove a similar result to the conservative diffeomorphisms.","We apply our techniques to show that every expansive singular suspension $C^{1+\\epsilon}$-flow over a three-dimensional manifold has positive topological entropy.","Finally, we explore this phenomenon for Anosov dynamics, showing that to nullify the topological entropy for Anosov suspension flow, the set of singularities must capture the non-wandering set."],"url":"http://arxiv.org/abs/2402.03132v1","category":"math.DS"}
{"created":"2024-02-05 15:49:57","title":"Design Space Exploration for Particle Detector Read-out Implementations in Matlab and Simulink on the Example of the SHiP SBT","abstract":"On a very fundamental level, particle detectors share similar requirements for their read-out chain. This is reflected in the way that typical read-out solutions are developed, where a previous design is taken and modified to fit some changes in requirements. One of the two common approaches is the current-based read-out, where the waveform of the sensor output is sampled in order to later extract information from there. This approach is used in many detector applications using scintillation based detectors, including PET. With this contribution, we will introduce how we use Matlab in order to simulate the read-out electronics of particle detectors. We developed this simulation approach as a base for our ongoing development of software-defined read-out ASICs that cover the requirements of a variety of particle detector types. Simulink was chosen as a base for our developments as it allows simulation of mixed-signal systems and comes with built-in toolkits to aid in developments of such systems. With our approach, we want to take a new look at how we approach designing such a read-out, with a focus on digital signal processing close to the sensor, making use of known signal characteristics and modern methods of communications engineering. We are taking into account the time profile of an event, the bandwidth-limiting properties of the sensor and attached electronics, digitization stages and finally the parameterization of approaches for digital processing of the signal. We will show how we are applying the design approach to the development of a read-out for the proposed SHiP SBT detector, which is a scintillation based detector relying on SiPMs sensors, using this as an example for our modelling approach and show preliminary results.","sentences":["On a very fundamental level, particle detectors share similar requirements for their read-out chain.","This is reflected in the way that typical read-out solutions are developed, where a previous design is taken and modified to fit some changes in requirements.","One of the two common approaches is the current-based read-out, where the waveform of the sensor output is sampled in order to later extract information from there.","This approach is used in many detector applications using scintillation based detectors, including PET.","With this contribution, we will introduce how we use Matlab in order to simulate the read-out electronics of particle detectors.","We developed this simulation approach as a base for our ongoing development of software-defined read-out ASICs that cover the requirements of a variety of particle detector types.","Simulink was chosen as a base for our developments as it allows simulation of mixed-signal systems and comes with built-in toolkits to aid in developments of such systems.","With our approach, we want to take a new look at how we approach designing such a read-out, with a focus on digital signal processing close to the sensor, making use of known signal characteristics and modern methods of communications engineering.","We are taking into account the time profile of an event, the bandwidth-limiting properties of the sensor and attached electronics, digitization stages and finally the parameterization of approaches for digital processing of the signal.","We will show how we are applying the design approach to the development of a read-out for the proposed SHiP SBT detector, which is a scintillation based detector relying on SiPMs sensors, using this as an example for our modelling approach and show preliminary results."],"url":"http://arxiv.org/abs/2402.03122v1","category":"physics.ins-det"}
{"created":"2024-02-05 15:45:55","title":"Discovering interpretable models of scientific image data with deep learning","abstract":"How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images? Can we use such models to derive scientific insight from the data? In this paper, we propose some methods for achieving this. In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data. We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data. We find that such methods can produce highly parsimonious models that achieve $\\sim98\\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity. We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon.","sentences":["How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images?","Can we use such models to derive scientific insight from the data?","In this paper, we propose some methods for achieving this.","In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data.","We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data.","We find that such methods can produce highly parsimonious models that achieve $\\sim98\\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity.","We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon."],"url":"http://arxiv.org/abs/2402.03115v1","category":"cs.LG"}
{"created":"2024-02-05 15:37:02","title":"Computing with Clocks","abstract":"Clocks are a central part of many computing paradigms, and are mainly used to synchronise the delicate operation of switching, necessary to drive modern computational processes. Unfortunately, this synchronisation process is reaching a natural ``apocalypse''. No longer can clock scaling be used as a blunt tool to accelerate computation, we are up against the natural limits of switching and synchronisation across large processors. Therefore, we need to rethink how time is utilised in computation, using it more naturally in the role of representing data. This can be achieved by using a time interval delineated by discrete start and end events, and by re-casting computational operations into the time domain. With this, computer systems can be developed that are naturally scaleable in time and space, and can use ambient time references built to the best effort of the available technology.   Our ambition is to better manage the energy/computation time trade-off, and to explicitly embed the resolution of the data in the time domain. We aim to recast calculations into the ``for free'' format that time offers, and in addition, perform these calculations at the highest clock or oscillator resolution possible.","sentences":["Clocks are a central part of many computing paradigms, and are mainly used to synchronise the delicate operation of switching, necessary to drive modern computational processes.","Unfortunately, this synchronisation process is reaching a natural ``apocalypse''.","No longer can clock scaling be used as a blunt tool to accelerate computation, we are up against the natural limits of switching and synchronisation across large processors.","Therefore, we need to rethink how time is utilised in computation, using it more naturally in the role of representing data.","This can be achieved by using a time interval delineated by discrete start and end events, and by re-casting computational operations into the time domain.","With this, computer systems can be developed that are naturally scaleable in time and space, and can use ambient time references built to the best effort of the available technology.   ","Our ambition is to better manage the energy/computation time trade-off, and to explicitly embed the resolution of the data in the time domain.","We aim to recast calculations into the ``for free'' format that time offers, and in addition, perform these calculations at the highest clock or oscillator resolution possible."],"url":"http://arxiv.org/abs/2402.03109v1","category":"cs.ET"}
{"created":"2024-02-05 15:30:56","title":"Electron Paramagnetic Resonance spectroscopy of a scheelite crystal using microwave photon counting","abstract":"Counting the microwave photons emitted by an ensemble of electron spins when they relax radiatively has recently been introduced as a sensitive new method for electron paramagnetic resonance spectroscopy at millikelvin temperatures. Here, we apply this spin fluorescence method to a scheelite crystal of CaWO4, finding some known ($\\mathrm{Er}^{3+}$, $\\mathrm{Yb}^{3+}$, $\\mathrm{Nd}^{3+}$ and $\\mathrm{Fe}^{3+}$) and other unknown paramagnetic impurities. Investigating the zero nuclear spin isotope ($I=0$) transition of $\\mathrm{Er}^{3+}:\\mathrm{CaWO}_4$ as a model system, we provide a quantitative analysis of the time-dependent photon counting rate following an excitation pulse, as a function of its power. The achieved signal-to-noise ratio is found to be an order of magnitude higher than the one obtained by inductively-detected Hahn echo under identical conditions. Finally, we use spin fluorescence spectroscopy at low excitation power to probe the properties of rare-earth-ions close to a metallic wire deposited on the surface; our data reveal line distortion caused by the mechanical strain imparted by the thermal contractions of the metal relative to the underlying crystal. Coherent oscillations are also observed for the most highly strained ions.","sentences":["Counting the microwave photons emitted by an ensemble of electron spins when they relax radiatively has recently been introduced as a sensitive new method for electron paramagnetic resonance spectroscopy at millikelvin temperatures.","Here, we apply this spin fluorescence method to a scheelite crystal of CaWO4, finding some known ($\\mathrm{Er}^{3+}$, $\\mathrm{Yb}^{3+}$, $\\mathrm{Nd}^{3+}$ and $\\mathrm{Fe}^{3+}$) and other unknown paramagnetic impurities.","Investigating the zero nuclear spin isotope ($I=0$) transition of $\\mathrm{Er}^{3+}:\\mathrm{CaWO}_4$ as a model system, we provide a quantitative analysis of the time-dependent photon counting rate following an excitation pulse, as a function of its power.","The achieved signal-to-noise ratio is found to be an order of magnitude higher than the one obtained by inductively-detected Hahn echo under identical conditions.","Finally, we use spin fluorescence spectroscopy at low excitation power to probe the properties of rare-earth-ions close to a metallic wire deposited on the surface; our data reveal line distortion caused by the mechanical strain imparted by the thermal contractions of the metal relative to the underlying crystal.","Coherent oscillations are also observed for the most highly strained ions."],"url":"http://arxiv.org/abs/2402.03102v1","category":"quant-ph"}
{"created":"2024-02-05 15:28:14","title":"The Eigenvalue Problem for the Complex Hessian Operator on $m$-Pseudoconvex Manifolds","abstract":"We establish $C^{1,1}$-regularity and uniqueness of the first eigenfunction of the complex Hessian operator on strongly $m$-pseudoconvex manifolds, along with a variational formula for the first eigenvalue. From these results, we derive a number of applications, including a bifurcation-type theorem and geometric bounds for the eigenvalue.","sentences":["We establish $C^{1,1}$-regularity and uniqueness of the first eigenfunction of the complex Hessian operator on strongly $m$-pseudoconvex manifolds, along with a variational formula for the first eigenvalue.","From these results, we derive a number of applications, including a bifurcation-type theorem and geometric bounds for the eigenvalue."],"url":"http://arxiv.org/abs/2402.03098v1","category":"math.CV"}
{"created":"2024-02-05 15:11:16","title":"Electronic stopping power: the IAEA database and state of the art experimental knowledge","abstract":"In this contribution, we review the state of the art of the experimental electronic stopping power data available in the IAEA database. We examine thoroughly the abundance or lack of data, quality of recent measurements, and trends. We analyze the evolution in time of the experimental stopping power measurements, characteristics, achievements, and failures. We also discuss comparisons with SRIM-2013 calculations for many interesting cases. Sparsely measured systems and energy regions are observed, stressing the need for new and reliable data and independent theoretical predictions.","sentences":["In this contribution, we review the state of the art of the experimental electronic stopping power data available in the IAEA database.","We examine thoroughly the abundance or lack of data, quality of recent measurements, and trends.","We analyze the evolution in time of the experimental stopping power measurements, characteristics, achievements, and failures.","We also discuss comparisons with SRIM-2013 calculations for many interesting cases.","Sparsely measured systems and energy regions are observed, stressing the need for new and reliable data and independent theoretical predictions."],"url":"http://arxiv.org/abs/2402.03080v1","category":"physics.atm-clus"}
{"created":"2024-02-05 15:01:57","title":"Controlling magnon-photon coupling in a planar geometry","abstract":"The tunability of magnons enables their interaction with various other quantum excitations, including photons, paving the route for novel hybrid quantum systems. Here, we study magnon-photon coupling using a high-quality factor split-ring resonator and single-crystal yttrium iron garnet (YIG) spheres at room temperature. We investigate the dependence of the coupling strength on the size of the sphere and find that the coupling is stronger for spheres with a larger diameter as predicted by theory. Furthermore, we demonstrate strong magnon-photon coupling by varying the position of the YIG sphere within the resonator. Our experimental results reveal the expected correlation between the coupling strength and the rf magnetic field. These findings demonstrate the control of coherent magnon-photon coupling through the theoretically predicted square-root dependence on the spin density in the ferromagnetic medium and the magnetic dipolar interaction in a planar resonator.","sentences":["The tunability of magnons enables their interaction with various other quantum excitations, including photons, paving the route for novel hybrid quantum systems.","Here, we study magnon-photon coupling using a high-quality factor split-ring resonator and single-crystal yttrium iron garnet (YIG) spheres at room temperature.","We investigate the dependence of the coupling strength on the size of the sphere and find that the coupling is stronger for spheres with a larger diameter as predicted by theory.","Furthermore, we demonstrate strong magnon-photon coupling by varying the position of the YIG sphere within the resonator.","Our experimental results reveal the expected correlation between the coupling strength and the rf magnetic field.","These findings demonstrate the control of coherent magnon-photon coupling through the theoretically predicted square-root dependence on the spin density in the ferromagnetic medium and the magnetic dipolar interaction in a planar resonator."],"url":"http://arxiv.org/abs/2402.03071v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 15:01:14","title":"Bound impurities in a one-dimensional Bose lattice gas: low-energy properties and quench-induced dynamics","abstract":"We study two mobile bosonic impurities immersed in a one-dimensional optical lattice and interacting with a bosonic bath. We employ the exact diagonalization method for small periodic lattices to study stationary properties and dynamics. We consider the branch of repulsive interactions that induce the formation of bound impurities, akin to the bipolaron problem. A comprehensive study of ground-state and low-energy properties is presented, including the characterization of the critical strength for the formation of bound impurities. We also study the dynamics induced after an interaction-quench to examine the stability of the bound impurities. We reveal that after large interaction quenches from strong to weak interactions the system can show large oscillations over time with revivals of the dimer states. We find that the oscillations are driven by selected eigenstates with phase-separated configurations.","sentences":["We study two mobile bosonic impurities immersed in a one-dimensional optical lattice and interacting with a bosonic bath.","We employ the exact diagonalization method for small periodic lattices to study stationary properties and dynamics.","We consider the branch of repulsive interactions that induce the formation of bound impurities, akin to the bipolaron problem.","A comprehensive study of ground-state and low-energy properties is presented, including the characterization of the critical strength for the formation of bound impurities.","We also study the dynamics induced after an interaction-quench to examine the stability of the bound impurities.","We reveal that after large interaction quenches from strong to weak interactions the system can show large oscillations over time with revivals of the dimer states.","We find that the oscillations are driven by selected eigenstates with phase-separated configurations."],"url":"http://arxiv.org/abs/2402.03070v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-05 14:53:20","title":"Energy transport in diffusive waveguides","abstract":"The guiding and transport of energy, for example of electromagnetic waves underpins many technologies that have shaped modern society, ranging from long distance optical fibre telecommunications to on-chip optical processors. Traditionally, a mechanism is required that exponentially localises the waves or particles in the confinement region, e.g. total internal reflection at a boundary. We introduce a waveguiding mechanism that relies on a different origin for the exponential confinement and that arises due to the physics of diffusion. We demonstrate this concept using light and show that photon density waves can propagate as a guided mode along a core-structure embedded in a scattering, opaque material, enhancing light transmission by orders of magnitude and along non-trivial, e.g. curved trajectories. This waveguiding mechanism can also occur naturally, for example in the cerebral spinal fluid surrounding the brain, along tendons in the human body and is to be expected in other systems that follow the same physics e.g. neutron diffusion.","sentences":["The guiding and transport of energy, for example of electromagnetic waves underpins many technologies that have shaped modern society, ranging from long distance optical fibre telecommunications to on-chip optical processors.","Traditionally, a mechanism is required that exponentially localises the waves or particles in the confinement region, e.g. total internal reflection at a boundary.","We introduce a waveguiding mechanism that relies on a different origin for the exponential confinement and that arises due to the physics of diffusion.","We demonstrate this concept using light and show that photon density waves can propagate as a guided mode along a core-structure embedded in a scattering, opaque material, enhancing light transmission by orders of magnitude and along non-trivial, e.g. curved trajectories.","This waveguiding mechanism can also occur naturally, for example in the cerebral spinal fluid surrounding the brain, along tendons in the human body and is to be expected in other systems that follow the same physics e.g. neutron diffusion."],"url":"http://arxiv.org/abs/2402.03064v1","category":"physics.optics"}
{"created":"2024-02-05 14:53:04","title":"Independent set reconfiguration in H-free graphs","abstract":"Given a graph $G$ and two independent sets of $G$, the independent set reconfiguration problem asks whether one independent set can be transformed into the other by moving a single vertex at a time, such that at each intermediate step we have an independent set of $G$. We study the complexity of this problem for $H$-free graphs under the token sliding and token jumping rule. Our contribution is twofold. First, we prove a reconfiguration analogue of Alekseev's theorem, showing that the problem is PSPACE-complete unless $H$ is a path or a subdivision of the claw. We then show that under the token sliding rule, the problem admits a polynomial-time algorithm if the input graph is fork-free.","sentences":["Given a graph $G$ and two independent sets of $G$, the independent set reconfiguration problem asks whether one independent set can be transformed into the other by moving a single vertex at a time, such that at each intermediate step we have an independent set of $G$. We study the complexity of this problem for $H$-free graphs under the token sliding and token jumping rule.","Our contribution is twofold.","First, we prove a reconfiguration analogue of Alekseev's theorem, showing that the problem is PSPACE-complete unless $H$ is a path or a subdivision of the claw.","We then show that under the token sliding rule, the problem admits a polynomial-time algorithm if the input graph is fork-free."],"url":"http://arxiv.org/abs/2402.03063v1","category":"cs.DM"}
{"created":"2024-02-05 14:52:33","title":"High Strain Engineering of a Suspended WSSe Monolayer Membrane by Indentation and Measured by Tip-enhanced Photoluminescence","abstract":"Straintronics involves the manipulation and regulation of the electronic characteristics of 2D materials through the use of macro- and nano-scale strain engineering. In this study, we utilized an atomic force microscope (AFM) coupled with an optical system to perform indentation measurements and tip-enhanced photoluminescence (TEPL), allowing us to extract the local optical response of a suspended monolayer membrane of ternary WSSe at various levels of deformation, up to strains of 10%. The photoluminescence signal is modelled considering the deformation, stress distribution and strain dependence of the WSSe band structure. We observe an additional TEPL signal that exhibits significant variation under strain, with 64 meV per percent of elongation. This peak is linked to the highly strained 2D material lying right underneath the tip. We discuss the amplification of the signal and its relation to the excitonic funnelling effect in a more comprehensive model. We will also compare the diffusion caused by Auger recombination against the radiative excitonic decay. We use TEPL to examine and comprehend the local physics of 2D semi-conducting materials subjected to extreme mechanical strain. Chemical vapour deposition-fabricated 2D ternaries possess high strain resistance, comparable to the benchmark MoS2, and a high Young's modulus of 273 GPa.","sentences":["Straintronics involves the manipulation and regulation of the electronic characteristics of 2D materials through the use of macro- and nano-scale strain engineering.","In this study, we utilized an atomic force microscope (AFM) coupled with an optical system to perform indentation measurements and tip-enhanced photoluminescence (TEPL), allowing us to extract the local optical response of a suspended monolayer membrane of ternary WSSe at various levels of deformation, up to strains of 10%.","The photoluminescence signal is modelled considering the deformation, stress distribution and strain dependence of the WSSe band structure.","We observe an additional TEPL signal that exhibits significant variation under strain, with 64 meV per percent of elongation.","This peak is linked to the highly strained 2D material lying right underneath the tip.","We discuss the amplification of the signal and its relation to the excitonic funnelling effect in a more comprehensive model.","We will also compare the diffusion caused by Auger recombination against the radiative excitonic decay.","We use TEPL to examine and comprehend the local physics of 2D semi-conducting materials subjected to extreme mechanical strain.","Chemical vapour deposition-fabricated 2D ternaries possess high strain resistance, comparable to the benchmark MoS2, and a high Young's modulus of 273 GPa."],"url":"http://arxiv.org/abs/2402.03061v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 14:43:50","title":"Note on quasi-optimal error estimates for the pressure for shear-thickening fluids","abstract":"In this paper, we derive quasi-optimal a priori error estimates for the kinematic pressure for a Local Discontinuous Galerkin (LDG) approximation of steady systems of $p$-Navier-Stokes type in the case of shear-thickening, i.e., in the case $p>2$, imposing a new mild Muckenhoupt regularity condition.","sentences":["In this paper, we derive quasi-optimal a priori error estimates for the kinematic pressure for a Local Discontinuous Galerkin (LDG) approximation of steady systems of $p$-Navier-Stokes type in the case of shear-thickening, i.e., in the case $p>2$, imposing a new mild Muckenhoupt regularity condition."],"url":"http://arxiv.org/abs/2402.03056v1","category":"math.NA"}
{"created":"2024-02-05 14:35:02","title":"Cluster parking functions","abstract":"The cluster complex on one hand, parking functions on the other hand, are two combinatorial (po)sets that can be associated to a finite real reflection group. Cluster parking functions are obtained by taking an appropriate fiber product (over noncrossing partitions). There is a natural structure of simplicial complex on these objects, and our main goal is to show that it has the homotopy type of a (pure) wedge of spheres. The unique nonzero homology group (as a representation of the underlying reflection group) is a sign-twisted parking representation, which is the same as Gordon's quotient of diagonal coinvariants. Along the way, we prove some properties of the poset of parking functions. We also provide a long list of remaining open problems.","sentences":["The cluster complex on one hand, parking functions on the other hand, are two combinatorial (po)sets that can be associated to a finite real reflection group.","Cluster parking functions are obtained by taking an appropriate fiber product (over noncrossing partitions).","There is a natural structure of simplicial complex on these objects, and our main goal is to show that it has the homotopy type of a (pure) wedge of spheres.","The unique nonzero homology group (as a representation of the underlying reflection group) is a sign-twisted parking representation, which is the same as Gordon's quotient of diagonal coinvariants.","Along the way, we prove some properties of the poset of parking functions.","We also provide a long list of remaining open problems."],"url":"http://arxiv.org/abs/2402.03052v1","category":"math.CO"}
{"created":"2024-02-05 14:34:14","title":"A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems","abstract":"In this paper, we examine the research conducted in the field of Nepali Automatic Speech Recognition (ASR). The primary objective of this survey is to conduct a comprehensive review of the works on Nepali Automatic Speech Recognition Systems completed to date, explore the different datasets used, examine the technology utilized, and take account of the obstacles encountered in implementing the Nepali ASR system. In tandem with the global trends of ever-increasing research on speech recognition based research, the number of Nepalese ASR-related projects are also growing. Nevertheless, the investigation of language and acoustic models of the Nepali language has not received adequate attention compared to languages that possess ample resources. In this context, we provide a framework as well as directions for future investigations.","sentences":["In this paper, we examine the research conducted in the field of Nepali Automatic Speech Recognition (ASR).","The primary objective of this survey is to conduct a comprehensive review of the works on Nepali Automatic Speech Recognition Systems completed to date, explore the different datasets used, examine the technology utilized, and take account of the obstacles encountered in implementing the Nepali ASR system.","In tandem with the global trends of ever-increasing research on speech recognition based research, the number of Nepalese ASR-related projects are also growing.","Nevertheless, the investigation of language and acoustic models of the Nepali language has not received adequate attention compared to languages that possess ample resources.","In this context, we provide a framework as well as directions for future investigations."],"url":"http://arxiv.org/abs/2402.03050v1","category":"cs.SD"}
{"created":"2024-02-05 14:33:52","title":"Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies","abstract":"This work presents an innovative learning-based approach to tackle the tracking control problem of Euler-Lagrange multi-agent systems with partially unknown dynamics operating under switching communication topologies. The approach leverages a correlation-aware cooperative algorithm framework built upon Gaussian process regression, which adeptly captures inter-agent correlations for uncertainty predictions. A standout feature is its exceptional efficiency in deriving the aggregation weights achieved by circumventing the computationally intensive posterior variance calculations. Through Lyapunov stability analysis, the distributed control law ensures bounded tracking errors with high probability. Simulation experiments validate the protocol's efficacy in effectively managing complex scenarios, establishing it as a promising solution for robust tracking control in multi-agent systems characterized by uncertain dynamics and dynamic communication structures.","sentences":["This work presents an innovative learning-based approach to tackle the tracking control problem of Euler-Lagrange multi-agent systems with partially unknown dynamics operating under switching communication topologies.","The approach leverages a correlation-aware cooperative algorithm framework built upon Gaussian process regression, which adeptly captures inter-agent correlations for uncertainty predictions.","A standout feature is its exceptional efficiency in deriving the aggregation weights achieved by circumventing the computationally intensive posterior variance calculations.","Through Lyapunov stability analysis, the distributed control law ensures bounded tracking errors with high probability.","Simulation experiments validate the protocol's efficacy in effectively managing complex scenarios, establishing it as a promising solution for robust tracking control in multi-agent systems characterized by uncertain dynamics and dynamic communication structures."],"url":"http://arxiv.org/abs/2402.03048v1","category":"cs.MA"}
{"created":"2024-02-05 14:32:57","title":"PFDM: Parser-Free Virtual Try-on via Diffusion Model","abstract":"Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can \"wear\" garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models.","sentences":["Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision.","However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling.","To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM).","Given two images, PFDM can \"wear\" garments on the target person seamlessly by implicitly warping without any other information.","To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons.","Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism.","Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models."],"url":"http://arxiv.org/abs/2402.03047v1","category":"cs.CV"}
{"created":"2024-02-05 14:18:35","title":"Interface behavior for the solutions of a mass conserving free boundary problem modeling cell polarization","abstract":"We consider a parabolic non-local free boundary problem that has been derived as a limit of a bulk-surface reaction-diffusion system which models cell polarization. In previous papers, we have established well-posedness of this problem and derived conditions on the initial data that imply continuity of the free boundary as $t\\to 0$. In this paper we extend the qualitative study of the free boundary by considering axisymmetric data. Under additional monotonicity assumptions on the data we prove global continuity of the free boundary. On the other hand, if the initial data violate a \"no-fattening\" condition we show that the free boundary can oscillate as $t \\to 0$.","sentences":["We consider a parabolic non-local free boundary problem that has been derived as a limit of a bulk-surface reaction-diffusion system which models cell polarization.","In previous papers, we have established well-posedness of this problem and derived conditions on the initial data that imply continuity of the free boundary as $t\\to 0$.","In this paper we extend the qualitative study of the free boundary by considering axisymmetric data.","Under additional monotonicity assumptions on the data we prove global continuity of the free boundary.","On the other hand, if the initial data violate a \"no-fattening\" condition we show that the free boundary can oscillate as $t \\to 0$."],"url":"http://arxiv.org/abs/2402.03034v1","category":"math.AP"}
{"created":"2024-02-05 14:12:35","title":"Functional SDE approximation inspired by a deep operator network architecture","abstract":"A novel approach to approximate solutions of Stochastic Differential Equations (SDEs) by Deep Neural Networks is derived and analysed. The architecture is inspired by the notion of Deep Operator Networks (DeepONets), which is based on operator learning in function spaces in terms of a reduced basis also represented in the network. In our setting, we make use of a polynomial chaos expansion (PCE) of stochastic processes and call the corresponding architecture SDEONet. The PCE has been used extensively in the area of uncertainty quantification (UQ) with parametric partial differential equations. This however is not the case with SDE, where classical sampling methods dominate and functional approaches are seen rarely. A main challenge with truncated PCEs occurs due to the drastic growth of the number of components with respect to the maximum polynomial degree and the number of basis elements. The proposed SDEONet architecture aims to alleviate the issue of exponential complexity by learning an optimal sparse truncation of the Wiener chaos expansion. A complete convergence and complexity analysis is presented, making use of recent Neural Network approximation results. Numerical experiments illustrate the promising performance of the suggested approach in 1D and higher dimensions.","sentences":["A novel approach to approximate solutions of Stochastic Differential Equations (SDEs) by Deep Neural Networks is derived and analysed.","The architecture is inspired by the notion of Deep Operator Networks (DeepONets), which is based on operator learning in function spaces in terms of a reduced basis also represented in the network.","In our setting, we make use of a polynomial chaos expansion (PCE) of stochastic processes and call the corresponding architecture SDEONet.","The PCE has been used extensively in the area of uncertainty quantification (UQ) with parametric partial differential equations.","This however is not the case with SDE, where classical sampling methods dominate and functional approaches are seen rarely.","A main challenge with truncated PCEs occurs due to the drastic growth of the number of components with respect to the maximum polynomial degree and the number of basis elements.","The proposed SDEONet architecture aims to alleviate the issue of exponential complexity by learning an optimal sparse truncation of the Wiener chaos expansion.","A complete convergence and complexity analysis is presented, making use of recent Neural Network approximation results.","Numerical experiments illustrate the promising performance of the suggested approach in 1D and higher dimensions."],"url":"http://arxiv.org/abs/2402.03028v1","category":"math.NA"}
{"created":"2024-02-05 14:06:37","title":"L\u00e9vy areas, Wong Zakai anomalies in diffusive limits of Deterministic Lagrangian Multi-Time Dynamics","abstract":"Stochastic modelling necessitates an interpretation of noise. In this paper, we describe the loss of deterministically stable behaviour in a fundamental fluid mechanics problem, conditional to whether noise is introduced in the sense of It\\^o, Stratonovich or a limit of Wong-Zakai type. We examine this comparison in the wider context of discretising stochastic differential equations with and without the L\\'evy area. From the numerical viewpoint, we demonstrate performing higher order discretisations with the use of a L\\'evy area can lead to the loss of conserved area and angle quantities. Such behaviour is not physically expected in the Stratonovich model. Conversely, we study Stochastic Advection by Lie Transport and its derivation from homogenisation theory, which introduces drift corrections of the same class naturally. From the viewpoint of homogenisation, the qualitative properties of the Wong-Zakai anomaly are physically motivated as arising due to correlations from a fast and mean scale fluid decomposition.","sentences":["Stochastic modelling necessitates an interpretation of noise.","In this paper, we describe the loss of deterministically stable behaviour in a fundamental fluid mechanics problem, conditional to whether noise is introduced in the sense of It\\^o, Stratonovich or a limit of Wong-Zakai type.","We examine this comparison in the wider context of discretising stochastic differential equations with and without the L\\'evy area.","From the numerical viewpoint, we demonstrate performing higher order discretisations with the use of a L\\'evy area can lead to the loss of conserved area and angle quantities.","Such behaviour is not physically expected in the Stratonovich model.","Conversely, we study Stochastic Advection by Lie Transport and its derivation from homogenisation theory, which introduces drift corrections of the same class naturally.","From the viewpoint of homogenisation, the qualitative properties of the Wong-Zakai anomaly are physically motivated as arising due to correlations from a fast and mean scale fluid decomposition."],"url":"http://arxiv.org/abs/2402.03026v1","category":"math.DS"}
{"created":"2024-02-05 14:03:36","title":"Farber's conjecture and beyond","abstract":"We survey two decades of work on the (sequential) topological complexity of configuration spaces of graphs (ordered and unordered), aiming to give an account that is unifying, elementary, and self-contained. We discuss the traditional approach through cohomology, with its limitations, and the more modern approach through asphericity and the fundamental group, explaining how they are in fact variations on the same core ideas. We close with a list of open problems in the field.","sentences":["We survey two decades of work on the (sequential) topological complexity of configuration spaces of graphs (ordered and unordered), aiming to give an account that is unifying, elementary, and self-contained.","We discuss the traditional approach through cohomology, with its limitations, and the more modern approach through asphericity and the fundamental group, explaining how they are in fact variations on the same core ideas.","We close with a list of open problems in the field."],"url":"http://arxiv.org/abs/2402.03022v1","category":"math.AT"}
{"created":"2024-02-05 14:00:46","title":"Elementary vibrational model for thermal conductivity of Lennard-Jones fluids: Applicability domain and accuracy level","abstract":"Exact mechanisms of thermal conductivity in liquids are not well understood, despite rich research history. A vibrational model of energy transfer in dense simple liquids with soft pairwise interactions seems adequate to partially fill this gap. The purpose of the present paper is to define its applicability domain and to demonstrate how well it works within the identified applicability domain in the important case of the Lennard-Jones model system. The existing results from molecular dynamics simulations are used for this purpose. Additionally, we show that a freezing density scaling approach represents a very powerful tool to estimate the thermal conductivity coefficient across essentially the entire gas-liquid region of the phase diagram, including metastable regions. A simple practical expression serving this purpose is proposed.","sentences":["Exact mechanisms of thermal conductivity in liquids are not well understood, despite rich research history.","A vibrational model of energy transfer in dense simple liquids with soft pairwise interactions seems adequate to partially fill this gap.","The purpose of the present paper is to define its applicability domain and to demonstrate how well it works within the identified applicability domain in the important case of the Lennard-Jones model system.","The existing results from molecular dynamics simulations are used for this purpose.","Additionally, we show that a freezing density scaling approach represents a very powerful tool to estimate the thermal conductivity coefficient across essentially the entire gas-liquid region of the phase diagram, including metastable regions.","A simple practical expression serving this purpose is proposed."],"url":"http://arxiv.org/abs/2402.03020v1","category":"cond-mat.soft"}
{"created":"2024-02-05 13:46:04","title":"On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions","abstract":"Experiments in engineering are typically conducted in controlled environments where parameters can be set to any desired value. This assumes that the same applies in a real-world setting -- an assumption that is often incorrect as many experiments are influenced by uncontrollable environmental conditions such as temperature, humidity and wind speed. When optimising such experiments, the focus should lie on finding optimal values conditionally on these uncontrollable variables. This article extends Bayesian optimisation to the optimisation of systems in changing environments that include controllable and uncontrollable parameters. The extension fits a global surrogate model over all controllable and environmental variables but optimises only the controllable parameters conditional on measurements of the uncontrollable variables. The method is validated on two synthetic test functions and the effects of the noise level, the number of the environmental parameters, the parameter fluctuation, the variability of the uncontrollable parameters, and the effective domain size are investigated. ENVBO, the proposed algorithm resulting from this investigation, is applied to a wind farm simulator with eight controllable and one environmental parameter. ENVBO finds solutions for the full domain of the environmental variable that outperforms results from optimisation algorithms that only focus on a fixed environmental value in all but one case while using a fraction of their evaluation budget. This makes the proposed approach very sample-efficient and cost-effective. An off-the-shelf open-source version of ENVBO is available via the NUBO Python package.","sentences":["Experiments in engineering are typically conducted in controlled environments where parameters can be set to any desired value.","This assumes that the same applies in a real-world setting -- an assumption that is often incorrect as many experiments are influenced by uncontrollable environmental conditions such as temperature, humidity and wind speed.","When optimising such experiments, the focus should lie on finding optimal values conditionally on these uncontrollable variables.","This article extends Bayesian optimisation to the optimisation of systems in changing environments that include controllable and uncontrollable parameters.","The extension fits a global surrogate model over all controllable and environmental variables but optimises only the controllable parameters conditional on measurements of the uncontrollable variables.","The method is validated on two synthetic test functions and the effects of the noise level, the number of the environmental parameters, the parameter fluctuation, the variability of the uncontrollable parameters, and the effective domain size are investigated.","ENVBO, the proposed algorithm resulting from this investigation, is applied to a wind farm simulator with eight controllable and one environmental parameter.","ENVBO finds solutions for the full domain of the environmental variable that outperforms results from optimisation algorithms that only focus on a fixed environmental value in all but one case while using a fraction of their evaluation budget.","This makes the proposed approach very sample-efficient and cost-effective.","An off-the-shelf open-source version of ENVBO is available via the NUBO Python package."],"url":"http://arxiv.org/abs/2402.03006v1","category":"cs.LG"}
{"created":"2024-02-05 13:39:19","title":"Entanglement phase transitions in non-Hermitian Kitaev chains","abstract":"The intricate interplay between unitary evolution and projective measurements could induce entanglement phase transitions in the nonequilibrium dynamics of quantum many-particle systems. In this work, we uncover loss-induced entanglement transitions in non-Hermitian topological superconductors. In prototypical Kitaev chains with local particle losses and varying hopping and pairing ranges, the bipartite entanglement entropy of steady states is found to scale logarithmically versus the system size in topologically nontrivial phases and become independent of the system size in the trivial phase. Notably, the scaling coefficients of log-law entangled phases are distinguishable when the underlying system resides in different topological phases. Log-law to log-law and log-law to area-law entanglement phase transitions are further identified when the system switches between different topological phases and goes from a topologically nontrivial to a trivial phase, respectively. These findings not only establish the relationships among spectral, topological and entanglement properties in a class of non-Hermitian topological superconductors, but also provide an efficient means to dynamically reveal their distinctive topological features.","sentences":["The intricate interplay between unitary evolution and projective measurements could induce entanglement phase transitions in the nonequilibrium dynamics of quantum many-particle systems.","In this work, we uncover loss-induced entanglement transitions in non-Hermitian topological superconductors.","In prototypical Kitaev chains with local particle losses and varying hopping and pairing ranges, the bipartite entanglement entropy of steady states is found to scale logarithmically versus the system size in topologically nontrivial phases and become independent of the system size in the trivial phase.","Notably, the scaling coefficients of log-law entangled phases are distinguishable when the underlying system resides in different topological phases.","Log-law to log-law and log-law to area-law entanglement phase transitions are further identified when the system switches between different topological phases and goes from a topologically nontrivial to a trivial phase, respectively.","These findings not only establish the relationships among spectral, topological and entanglement properties in a class of non-Hermitian topological superconductors, but also provide an efficient means to dynamically reveal their distinctive topological features."],"url":"http://arxiv.org/abs/2402.03001v1","category":"quant-ph"}
{"created":"2024-02-05 13:37:55","title":"Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training System for Improvisation","abstract":"Improvisation is a vital but often neglected aspect of traditional piano teaching. Challenges such as difficulty in assessment and subjectivity have hindered its effective instruction. Technological approaches, including augmentation, aim to enhance piano instruction, but the specific application of digital augmentation for piano improvisation is under-explored. This paper outlines a co-design process developing an Augmented Reality (AR) Piano Improvisation Training System, ImproVISe, involving improvisation teachers. The prototype, featuring basic improvisation concepts, was created and refined through expert interaction. Their insights guided the identification of objectives, tools, interaction metaphors, and software features. The findings offer design guidelines and recommendations to address challenges in assessing piano improvisation in a learning context.","sentences":["Improvisation is a vital but often neglected aspect of traditional piano teaching.","Challenges such as difficulty in assessment and subjectivity have hindered its effective instruction.","Technological approaches, including augmentation, aim to enhance piano instruction, but the specific application of digital augmentation for piano improvisation is under-explored.","This paper outlines a co-design process developing an Augmented Reality (AR) Piano Improvisation Training System, ImproVISe, involving improvisation teachers.","The prototype, featuring basic improvisation concepts, was created and refined through expert interaction.","Their insights guided the identification of objectives, tools, interaction metaphors, and software features.","The findings offer design guidelines and recommendations to address challenges in assessing piano improvisation in a learning context."],"url":"http://arxiv.org/abs/2402.02999v1","category":"cs.HC"}
{"created":"2024-02-05 13:34:19","title":"XiHe: A Data-Driven Model for Global Ocean Eddy-Resolving Forecasting","abstract":"The leading operational Global Ocean Forecasting Systems (GOFSs) use physics-driven numerical forecasting models that solve the partial differential equations with expensive computation. Recently, specifically in atmosphere weather forecasting, data-driven models have demonstrated significant potential for speeding up environmental forecasting by orders of magnitude, but there is still no data-driven GOFS that matches the forecasting accuracy of the numerical GOFSs. In this paper, we propose the first data-driven 1/12{\\deg} resolution global ocean eddy-resolving forecasting model named XiHe, which is established from the 25-year France Mercator Ocean International's daily GLORYS12 reanalysis data. XiHe is a hierarchical transformer-based framework coupled with two special designs. One is the land-ocean mask mechanism for focusing exclusively on the global ocean circulation. The other is the ocean-specific block for effectively capturing both local ocean information and global teleconnection. Extensive experiments are conducted under satellite observations, in situ observations, and the IV-TT Class 4 evaluation framework of the world's leading operational GOFSs from January 2019 to December 2020. The results demonstrate that XiHe achieves stronger forecast performance in all testing variables than existing leading operational numerical GOFSs including Mercator Ocean Physical SYstem (PSY4), Global Ice Ocean Prediction System (GIOPS), BLUElinK OceanMP AS (BLK), and Forecast Ocean Assimilation Model (FOAM). Particularly, the accuracy of ocean current forecasting of XiHe out to 60 days is even better than that of PSY4 in just 10 days. Additionally, XiHe is able to forecast the large-scale circulation and the mesoscale eddies. Furthermore, it can make a 10-day forecast in only 0.36 seconds, which accelerates the forecast speed by thousands of times compared to the traditional numerical GOFSs.","sentences":["The leading operational Global Ocean Forecasting Systems (GOFSs) use physics-driven numerical forecasting models that solve the partial differential equations with expensive computation.","Recently, specifically in atmosphere weather forecasting, data-driven models have demonstrated significant potential for speeding up environmental forecasting by orders of magnitude, but there is still no data-driven GOFS that matches the forecasting accuracy of the numerical GOFSs.","In this paper, we propose the first data-driven 1/12{\\deg} resolution global ocean eddy-resolving forecasting model named XiHe, which is established from the 25-year France Mercator Ocean International's daily GLORYS12 reanalysis data.","XiHe is a hierarchical transformer-based framework coupled with two special designs.","One is the land-ocean mask mechanism for focusing exclusively on the global ocean circulation.","The other is the ocean-specific block for effectively capturing both local ocean information and global teleconnection.","Extensive experiments are conducted under satellite observations, in situ observations, and the IV-TT Class 4 evaluation framework of the world's leading operational GOFSs from January 2019 to December 2020.","The results demonstrate that XiHe achieves stronger forecast performance in all testing variables than existing leading operational numerical GOFSs including Mercator Ocean Physical SYstem (PSY4), Global Ice Ocean Prediction System (GIOPS), BLUElinK OceanMP AS (BLK), and Forecast Ocean Assimilation Model (FOAM).","Particularly, the accuracy of ocean current forecasting of XiHe out to 60 days is even better than that of PSY4 in just 10 days.","Additionally, XiHe is able to forecast the large-scale circulation and the mesoscale eddies.","Furthermore, it can make a 10-day forecast in only 0.36 seconds, which accelerates the forecast speed by thousands of times compared to the traditional numerical GOFSs."],"url":"http://arxiv.org/abs/2402.02995v1","category":"physics.ao-ph"}
{"created":"2024-02-05 13:33:53","title":"Extreme statistics and extreme events in dynamical models of turbulence","abstract":"We present a study of the intermittent properties of a shell model of turbulence with unprecedented statistics, about $\\sim 10^7$ eddy turn over time, achieved thanks to an implementation on a large-scale parallel GPU factory. This allows us to quantify the inertial range anomalous scaling properties of the velocity fluctuations up to the $24$-th order moment. Through a careful assessment of the statistical and systematic uncertainties, we show that none of the phenomenological and theoretical models previously proposed in the literature to predict the anomalous power-law exponents in the inertial range is in agreement with our high-precision numerical measurements. We find that at asymptotically large moments, the anomalous exponents tend towards a linear scaling, suggesting that extreme turbulent events are dominated by one leading singularity. We found that systematic corrections to scaling induced by the infrared and ultraviolet (viscous) cut-offs are the main limitations to precision for low-order moments, while large orders are mainly affected by the finite statistical samples. The unprecedentedly high fidelity numerical results reported in this work offer an ideal benchmark for the development of future theoretical models of intermittency for either extreme events (high-order moments) or typical fluctuations (low-order moments). For the latter, we show that we achieve a precision in the determination of the inertial range scaling properties of the order of one part over ten thousand (5th significant digit), which must be considered a record for out-of-equilibrium fluid-mechanics systems and models.","sentences":["We present a study of the intermittent properties of a shell model of turbulence with unprecedented statistics, about $\\sim 10^7$ eddy turn over time, achieved thanks to an implementation on a large-scale parallel GPU factory.","This allows us to quantify the inertial range anomalous scaling properties of the velocity fluctuations up to the $24$-th order moment.","Through a careful assessment of the statistical and systematic uncertainties, we show that none of the phenomenological and theoretical models previously proposed in the literature to predict the anomalous power-law exponents in the inertial range is in agreement with our high-precision numerical measurements.","We find that at asymptotically large moments, the anomalous exponents tend towards a linear scaling, suggesting that extreme turbulent events are dominated by one leading singularity.","We found that systematic corrections to scaling induced by the infrared and ultraviolet (viscous) cut-offs are the main limitations to precision for low-order moments, while large orders are mainly affected by the finite statistical samples.","The unprecedentedly high fidelity numerical results reported in this work offer an ideal benchmark for the development of future theoretical models of intermittency for either extreme events (high-order moments) or typical fluctuations (low-order moments).","For the latter, we show that we achieve a precision in the determination of the inertial range scaling properties of the order of one part over ten thousand (5th significant digit), which must be considered a record for out-of-equilibrium fluid-mechanics systems and models."],"url":"http://arxiv.org/abs/2402.02994v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 13:15:52","title":"Quantum circuits with free fermions in disguise","abstract":"Recently multiple families of spin chain models were found, which have a free fermionic spectrum, even though they are not solvable by a Jordan-Wigner transformation. Instead, the free fermions emerge as a result of a rather intricate construction. In this work we consider the quantum circuit formulation of the problem. We construct circuits using local unitary gates built from the terms in the local Hamiltonians of the respective models, and ask the question: which circuit geometries (sequence of gates) lead to a free fermionic spectrum? Our main example is the 4-fermion model of Fendley, where we construct free fermionic circuits with various geometries. In certain cases we prove the free fermionic nature, while for other geometries we confirm it numerically. Surprisingly, we find that many standard brickwork circuits are not free fermionic, but we identify certain symmetric constructions which are.","sentences":["Recently multiple families of spin chain models were found, which have a free fermionic spectrum, even though they are not solvable by a Jordan-Wigner transformation.","Instead, the free fermions emerge as a result of a rather intricate construction.","In this work we consider the quantum circuit formulation of the problem.","We construct circuits using local unitary gates built from the terms in the local Hamiltonians of the respective models, and ask the question: which circuit geometries (sequence of gates) lead to a free fermionic spectrum?","Our main example is the 4-fermion model of Fendley, where we construct free fermionic circuits with various geometries.","In certain cases we prove the free fermionic nature, while for other geometries we confirm it numerically.","Surprisingly, we find that many standard brickwork circuits are not free fermionic, but we identify certain symmetric constructions which are."],"url":"http://arxiv.org/abs/2402.02984v1","category":"quant-ph"}
{"created":"2024-02-05 12:56:22","title":"Putting Context in Context: the Impact of Discussion Structure on Text Classification","abstract":"Current text classification approaches usually focus on the content to be classified. Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions. Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited. In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model. We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way. Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity). Indeed, we show that contextual information on smaller datasets from other classification tasks does not yield significant improvements. Our framework, based on local discussion networks, allows the integration of structural information, while minimising user profiling, thus preserving their privacy.","sentences":["Current text classification approaches usually focus on the content to be classified.","Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions.","Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited.","In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model.","We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way.","Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity).","Indeed, we show that contextual information on smaller datasets from other classification tasks does not yield significant improvements.","Our framework, based on local discussion networks, allows the integration of structural information, while minimising user profiling, thus preserving their privacy."],"url":"http://arxiv.org/abs/2402.02975v1","category":"cs.CL"}
{"created":"2024-02-05 12:46:12","title":"Atomistic-to-continuum convergence for quasi-static crack growth in brittle materials","abstract":"We study the atomistic-to-continuum limit for a model of a quasi-static crack evolution driven by time-dependent boundary conditions. We consider a two-dimensional atomic mass spring system whose interactions are modeled by classical interaction potentials, supplemented by a suitable irreversibility condition accounting for the breaking of atmoic bonding. In a simultaneous limit of vanishing interatomic distance and discretized time step, we identify a continuum model of quasi-static crack growth in brittle fracture featuring an irreversibility condition, a global stability, and an energy balance. The proof of global stability relies on a careful adaptation of the jump-transfer argument by Francfort and Larsen to the atomistic setting.","sentences":["We study the atomistic-to-continuum limit for a model of a quasi-static crack evolution driven by time-dependent boundary conditions.","We consider a two-dimensional atomic mass spring system whose interactions are modeled by classical interaction potentials, supplemented by a suitable irreversibility condition accounting for the breaking of atmoic bonding.","In a simultaneous limit of vanishing interatomic distance and discretized time step, we identify a continuum model of quasi-static crack growth in brittle fracture featuring an irreversibility condition, a global stability, and an energy balance.","The proof of global stability relies on a careful adaptation of the jump-transfer argument by Francfort and Larsen to the atomistic setting."],"url":"http://arxiv.org/abs/2402.02966v1","category":"math.AP"}
{"created":"2024-02-05 12:39:05","title":"Controlling flow patterns and topology in active emulsions","abstract":"Active emulsions and liquid crystalline shells are intriguing and experimentally realisable types of topological matter. Here we numerically study the morphology and spatiotemporal dynamics of a double emulsion, where one or two passive small droplets are embedded in a larger active droplet. We find activity introduces a variety of rich and nontrivial nonequilibrium states in the system. First, a double emulsion with a single active droplet becomes self-motile, and there is a transition between translational and rotational motion: both of these regimes remain defect-free, hence topologically trivial. Second, a pair of particles nucleate one or more disclination loops, with conformational dynamics resembling a rotor or chaotic oscillator, accessed by tuning activity. In the first state a single, topologically charged, disclination loop powers the rotation. In the latter state, this disclination stretches and writhes in 3D, continuously undergoing recombination to yield an example of an active living polymer. These emulsions can be self-assembled in the lab, and provide a pathway to form flow and topology patterns in active matter in a controllable way, as opposed to bulk systems that typically yield active turbulence.","sentences":["Active emulsions and liquid crystalline shells are intriguing and experimentally realisable types of topological matter.","Here we numerically study the morphology and spatiotemporal dynamics of a double emulsion, where one or two passive small droplets are embedded in a larger active droplet.","We find activity introduces a variety of rich and nontrivial nonequilibrium states in the system.","First, a double emulsion with a single active droplet becomes self-motile, and there is a transition between translational and rotational motion: both of these regimes remain defect-free, hence topologically trivial.","Second, a pair of particles nucleate one or more disclination loops, with conformational dynamics resembling a rotor or chaotic oscillator, accessed by tuning activity.","In the first state a single, topologically charged, disclination loop powers the rotation.","In the latter state, this disclination stretches and writhes in 3D, continuously undergoing recombination to yield an example of an active living polymer.","These emulsions can be self-assembled in the lab, and provide a pathway to form flow and topology patterns in active matter in a controllable way, as opposed to bulk systems that typically yield active turbulence."],"url":"http://arxiv.org/abs/2402.02960v1","category":"cond-mat.soft"}
{"created":"2024-02-05 12:38:26","title":"On the functional equation of twisted Ruelle zeta function and Fried's conjecture","abstract":"Let $M$ be a finite volume hyperbolic Riemann surface with arbitrary signature, and let $\\chi$ be an arbitrary $m$-dimensional multiplier system of weight $k$. Let $R(s,\\chi)$ be the associated Ruelle zeta function, and $\\varphi(s,\\chi)$ the determinant of the scattering matrix. We prove the functional equation that $R(s,\\chi)\\varphi(s,\\chi) = R(-s,\\chi)\\varphi(s,\\chi)H(s,\\chi)$ where $H(s,\\chi)$ is a meromorphic function of order one explicitly determined using the topological data of $M$ and of $\\chi$, and the trigonometric function $\\sin(s)$. From this, we determine the order of the divisor of $R(s,\\chi)$ at $s=0$ and compute the lead coefficient in its Laurent expansion at $s=0$. When combined with results by Kitano and by Yamaguchi, we prove further instances of the Fried conjecture, which states that the R-torsion of the above data is simply expressed in terms of $R(0,\\chi)$.","sentences":["Let $M$ be a finite volume hyperbolic Riemann surface with arbitrary signature, and let $\\chi$ be an arbitrary $m$-dimensional multiplier system of weight $k$. Let $R(s,\\chi)$ be the associated Ruelle zeta function, and $\\varphi(s,\\chi)$ the determinant of the scattering matrix.","We prove the functional equation that $R(s,\\chi)\\varphi(s,\\chi) = R(-s,\\chi)\\varphi(s,\\chi)H(s,\\chi)$ where $H(s,\\chi)$ is a meromorphic function of order one explicitly determined using the topological data of $M$ and of $\\chi$, and the trigonometric function $\\sin(s)$. From this, we determine the order of the divisor of $R(s,\\chi)$ at $s=0$ and compute the lead coefficient in its Laurent expansion at $s=0$. When combined with results by Kitano and by Yamaguchi, we prove further instances of the Fried conjecture, which states that the R-torsion of the above data is simply expressed in terms of $R(0,\\chi)$."],"url":"http://arxiv.org/abs/2402.02959v1","category":"math.NT"}
{"created":"2024-02-05 12:33:05","title":"Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach","abstract":"A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \\citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our experimental results show that the algorithms leveraging these findings can scale up to much larger multi-player games without compromising optimality.","sentences":["A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \\citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames.","However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity.","This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society.","To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time.","Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity.","Our experimental results show that the algorithms leveraging these findings can scale up to much larger multi-player games without compromising optimality."],"url":"http://arxiv.org/abs/2402.02954v1","category":"cs.GT"}
{"created":"2024-02-05 12:21:16","title":"Kernel PCA for Out-of-Distribution Detection","abstract":"Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\\mathcal{O}(1)$ time complexity in inference. Extensive empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA-based detector in efficiency and efficacy with state-of-the-art OoD detection performances.","sentences":["Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs).","Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data.","The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings.","In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns.","We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components.","Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\\mathcal{O}(1)$ time complexity in inference.","Extensive empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA-based detector in efficiency and efficacy with state-of-the-art OoD detection performances."],"url":"http://arxiv.org/abs/2402.02949v1","category":"cs.LG"}
{"created":"2024-02-05 12:19:16","title":"HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space","abstract":"In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, \"inner\" convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity.","sentences":["In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems.","By placing it after a Hough Transform layer, \"inner\" convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift.","These properties were not presented in Hough Transform alone.","Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network.","Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity."],"url":"http://arxiv.org/abs/2402.02946v1","category":"cs.CV"}
{"created":"2024-02-05 12:06:06","title":"Fractional damping induces resonant behavior in the Duffing oscillator","abstract":"The interaction between the fractional order parameter and the damping parameter can play a relevant role for introducing different dynamical behaviors in a physical system. Here, we study the Duffing oscillator with a fractional damping term. Our findings show that for certain values of the fractional order parameter, the damping parameter, and the forcing amplitude high oscillations amplitude can be induced. This phenomenon is due to the appearance of a resonance in the Duffing oscillator only when the damping term is fractional.","sentences":["The interaction between the fractional order parameter and the damping parameter can play a relevant role for introducing different dynamical behaviors in a physical system.","Here, we study the Duffing oscillator with a fractional damping term.","Our findings show that for certain values of the fractional order parameter, the damping parameter, and the forcing amplitude high oscillations amplitude can be induced.","This phenomenon is due to the appearance of a resonance in the Duffing oscillator only when the damping term is fractional."],"url":"http://arxiv.org/abs/2402.02940v1","category":"nlin.CD"}
{"created":"2024-02-05 12:01:35","title":"Bidirectional Zigzag Growth from Clusters of Active Colloidal Shakers","abstract":"Driven or self-propelling particles moving in viscoelastic fluids recently emerge as novel class of active systems showing a complex yet rich set of phenomena due to the non-Newtonian nature of the dispersing medium. Here we investigate the one-dimensional growth of clusters made of active colloidal shakers, which are realized by oscillating magnetic rotors dispersed within a viscoelastic fluid and at different concentration of the dissolved polymer. These magnetic particles when actuated by an oscillating field display a flow profile similar to that of a shaker force dipole, i.e. without any net propulsion. We design a protocol to assemble clusters of colloidal shakers and induce their controlled expansion into elongated zigzag structures. We observe a power law growth of the mean chain length and use theoretical arguments to explain the measured $1/3$ exponent. These arguments agree well with both experiments and particle based numerical simulations.","sentences":["Driven or self-propelling particles moving in viscoelastic fluids recently emerge as novel class of active systems showing a complex yet rich set of phenomena due to the non-Newtonian nature of the dispersing medium.","Here we investigate the one-dimensional growth of clusters made of active colloidal shakers, which are realized by oscillating magnetic rotors dispersed within a viscoelastic fluid and at different concentration of the dissolved polymer.","These magnetic particles when actuated by an oscillating field display a flow profile similar to that of a shaker force dipole, i.e. without any net propulsion.","We design a protocol to assemble clusters of colloidal shakers and induce their controlled expansion into elongated zigzag structures.","We observe a power law growth of the mean chain length and use theoretical arguments to explain the measured $1/3$ exponent.","These arguments agree well with both experiments and particle based numerical simulations."],"url":"http://arxiv.org/abs/2402.02939v1","category":"cond-mat.soft"}
{"created":"2024-02-05 12:00:31","title":"Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM","abstract":"With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention. Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services. This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools. This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster. The experimental results show that this system executes the restoration process within 15 s without human intervention, enabling rapid recovery. This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments. Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM). The necessity of scheduling through this predict is made clearer through comparison with experiments without scheduling, demonstrating its ability to prevent performance degradation. This research highlights the efficiency and necessity of automatic recovery systems in cloud environments, setting a new direction for future research.","sentences":["With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention.","Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services.","This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools.","This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster.","The experimental results show that this system executes the restoration process within 15 s without human intervention, enabling rapid recovery.","This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments.","Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM).","The necessity of scheduling through this predict is made clearer through comparison with experiments without scheduling, demonstrating its ability to prevent performance degradation.","This research highlights the efficiency and necessity of automatic recovery systems in cloud environments, setting a new direction for future research."],"url":"http://arxiv.org/abs/2402.02938v1","category":"cs.DC"}
{"created":"2024-02-05 11:52:23","title":"Embedding Hardware Approximations in Discrete Genetic-based Training for Printed MLPs","abstract":"Printed Electronics (PE) stands out as a promisingtechnology for widespread computing due to its distinct attributes, such as low costs and flexible manufacturing. Unlike traditional silicon-based technologies, PE enables stretchable, conformal,and non-toxic hardware. However, PE are constrained by larger feature sizes, making it challenging to implement complex circuits such as machine learning (ML) classifiers. Approximate computing has been proven to reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs). In this paper, we maximize the benefits of approximate computing by integrating hardware approximation into the MLP training process. Due to the discrete nature of hardware approximation, we propose and implement a genetic-based, approximate, hardware-aware training approach specifically designed for printed MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction compared to the baseline while outperforming state of-the-art approximate and stochastic printed MLPs.","sentences":["Printed Electronics (PE) stands out as a promisingtechnology for widespread computing due to its distinct attributes, such as low costs and flexible manufacturing.","Unlike traditional silicon-based technologies, PE enables stretchable, conformal,and non-toxic hardware.","However, PE are constrained by larger feature sizes, making it challenging to implement complex circuits such as machine learning (ML) classifiers.","Approximate computing has been proven to reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs).","In this paper, we maximize the benefits of approximate computing by integrating hardware approximation into the MLP training process.","Due to the discrete nature of hardware approximation, we propose and implement a genetic-based, approximate, hardware-aware training approach specifically designed for printed MLPs.","For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction compared to the baseline while outperforming state of-the-art approximate and stochastic printed MLPs."],"url":"http://arxiv.org/abs/2402.02930v1","category":"cs.AR"}
{"created":"2024-02-05 11:47:45","title":"Instance Segmentation XXL-CT Challenge of a Historic Airplane","abstract":"Instance segmentation of compound objects in XXL-CT imagery poses a unique challenge in non-destructive testing. This complexity arises from the lack of known reference segmentation labels, limited applicable segmentation tools, as well as partially degraded image quality. To asses recent advancements in the field of machine learning-based image segmentation, the \"Instance Segmentation XXL-CT Challenge of a Historic Airplane\" was conducted. The challenge aimed to explore automatic or interactive instance segmentation methods for an efficient delineation of the different aircraft components, such as screws, rivets, metal sheets or pressure tubes. We report the organization and outcome of this challenge and describe the capabilities and limitations of the submitted segmentation methods.","sentences":["Instance segmentation of compound objects in XXL-CT imagery poses a unique challenge in non-destructive testing.","This complexity arises from the lack of known reference segmentation labels, limited applicable segmentation tools, as well as partially degraded image quality.","To asses recent advancements in the field of machine learning-based image segmentation, the \"Instance Segmentation XXL-CT Challenge of a Historic Airplane\" was conducted.","The challenge aimed to explore automatic or interactive instance segmentation methods for an efficient delineation of the different aircraft components, such as screws, rivets, metal sheets or pressure tubes.","We report the organization and outcome of this challenge and describe the capabilities and limitations of the submitted segmentation methods."],"url":"http://arxiv.org/abs/2402.02928v1","category":"cs.CV"}
{"created":"2024-02-05 11:47:39","title":"New approaches and error assessment to snow cover thickness and density using air temperature data at different heights","abstract":"Snow poles are inexpensive systems composed of a wooden mast with temperature sensors affixed at varying heights with the purpose of estimating the snow depth. They are frequently utilised in cold, remote regions where the maintenance of complex monitoring instruments becomes impractical. In this study, snow cover thickness is determined using different methods, based on the thermal behaviour of air temperature measured by a snow pole on Deception Island, Antarctica. The methods are compared to high-resolution measurements of snow depth obtained using an ultrasonic sensor at the same site. A new modified method is proposed and shown to give the best results. Errors and sensitivity to chosen thresholds of the various methods have been compared. Sensitivity tests have been also conducted to evaluate the impact of missing data from some of the sensors. Finally, the insulating effect on the thermal signal produced by the snow is used to obtain information on the snowpack density. Promising results have been found from this effort, opening new possibilities for the usage of snow poles and may lead to future studies.","sentences":["Snow poles are inexpensive systems composed of a wooden mast with temperature sensors affixed at varying heights with the purpose of estimating the snow depth.","They are frequently utilised in cold, remote regions where the maintenance of complex monitoring instruments becomes impractical.","In this study, snow cover thickness is determined using different methods, based on the thermal behaviour of air temperature measured by a snow pole on Deception Island, Antarctica.","The methods are compared to high-resolution measurements of snow depth obtained using an ultrasonic sensor at the same site.","A new modified method is proposed and shown to give the best results.","Errors and sensitivity to chosen thresholds of the various methods have been compared.","Sensitivity tests have been also conducted to evaluate the impact of missing data from some of the sensors.","Finally, the insulating effect on the thermal signal produced by the snow is used to obtain information on the snowpack density.","Promising results have been found from this effort, opening new possibilities for the usage of snow poles and may lead to future studies."],"url":"http://arxiv.org/abs/2402.02927v1","category":"physics.ao-ph"}
{"created":"2024-02-05 11:26:46","title":"Nonlocal Diffusion Elliptic System Modelling The Behaviour Of a Bacteria And a Living Nutrient","abstract":"In this paper, we discuss the existence and uniqueness of coexistence states for a class of non-local elliptic system. This problem models the behaviour of a bacteria and a living nutrient, whose diffusion depends on the population of the bacteria in a non-local and nonlinear way. Mainly, we employ bifurcation methods and the Implicit Function Theorem to obtain the existence and uniqueness of positive solution.","sentences":["In this paper, we discuss the existence and uniqueness of coexistence states for a class of non-local elliptic system.","This problem models the behaviour of a bacteria and a living nutrient, whose diffusion depends on the population of the bacteria in a non-local and nonlinear way.","Mainly, we employ bifurcation methods and the Implicit Function Theorem to obtain the existence and uniqueness of positive solution."],"url":"http://arxiv.org/abs/2402.02912v1","category":"math.AP"}
{"created":"2024-02-05 11:25:42","title":"Digital Twin for Grey Box modeling of Multistory residential building thermal dynamics","abstract":"Buildings energy efficiency is a widely researched topic, which is rapidly gaining popularity due to rising environmental concerns and the need for energy independence. In Northern Europe heating energy alone accounts for up to 70 percent of the total building energy consumption. Industry 4.0 technologies such as IoT, big data, cloud computing and machine learning, along with the creation of predictive and proactive digital twins, can help to reduce this number. However, buildings thermal dynamics is a very complex process that depends on many variables. As a result, commonly used physics-based white box models are time-consuming and require vast expertise. On the contrary, black box forecasting models, which rely primarily on building energy consumption data, lack fundamental insights and hinder re-use. In this study we propose an architecture to facilitate grey box modelling of building thermal dynamics while integrating real time IoT data with 3D representation of buildings. The architecture is validated in a case study creating a digital twin platform that enables users to define the thermal dynamics of buildings based on physical laws and real data, thus facilitating informed decision making for the best heating energy optimization strategy. Also, the created user interface enables stakeholders such as facility managers, energy providers or governing bodies to analyse, compare and evaluate buildings thermal dynamics without extensive expertise or time resources.","sentences":["Buildings energy efficiency is a widely researched topic, which is rapidly gaining popularity due to rising environmental concerns and the need for energy independence.","In Northern Europe heating energy alone accounts for up to 70 percent of the total building energy consumption.","Industry 4.0 technologies such as IoT, big data, cloud computing and machine learning, along with the creation of predictive and proactive digital twins, can help to reduce this number.","However, buildings thermal dynamics is a very complex process that depends on many variables.","As a result, commonly used physics-based white box models are time-consuming and require vast expertise.","On the contrary, black box forecasting models, which rely primarily on building energy consumption data, lack fundamental insights and hinder re-use.","In this study we propose an architecture to facilitate grey box modelling of building thermal dynamics while integrating real time IoT data with 3D representation of buildings.","The architecture is validated in a case study creating a digital twin platform that enables users to define the thermal dynamics of buildings based on physical laws and real data, thus facilitating informed decision making for the best heating energy optimization strategy.","Also, the created user interface enables stakeholders such as facility managers, energy providers or governing bodies to analyse, compare and evaluate buildings thermal dynamics without extensive expertise or time resources."],"url":"http://arxiv.org/abs/2402.02909v1","category":"stat.AP"}
{"created":"2024-02-05 11:01:57","title":"On the Performance of RIS-Aided Spatial Modulation for Downlink Transmission","abstract":"In this study, we explore the performance of a reconfigurable reflecting surface (RIS)-assisted transmit spatial modulation (SM) system for downlink transmission, wherein the deployment of RIS serves the purpose of blind area coverage within the channel. At the receiving end, we present three detectors, i.e., maximum likelihood (ML) detector, two-stage ML detection, and greedy detector to recover the transmitted signal. By utilizing the ML detector, we initially derive the conditional pair error probability expression for the proposed scheme. Subsequently, we leverage the central limit theorem (CLT) to obtain the probability density function of the combined channel. Following this, the Gaussian-Chebyshev quadrature method is applied to derive a closed-form expression for the unconditional pair error probability and establish the union tight upper bound for the average bit error probability (ABEP). Furthermore, we derive a closed-form expression for the ergodic capacity of the proposed RIS-SM scheme. Monte Carlo simulations are conducted not only to assess the complexity and reliability of the three detection algorithms but also to validate the results obtained through theoretical derivation results.","sentences":["In this study, we explore the performance of a reconfigurable reflecting surface (RIS)-assisted transmit spatial modulation (SM) system for downlink transmission, wherein the deployment of RIS serves the purpose of blind area coverage within the channel.","At the receiving end, we present three detectors, i.e., maximum likelihood (ML) detector, two-stage ML detection, and greedy detector to recover the transmitted signal.","By utilizing the ML detector, we initially derive the conditional pair error probability expression for the proposed scheme.","Subsequently, we leverage the central limit theorem (CLT) to obtain the probability density function of the combined channel.","Following this, the Gaussian-Chebyshev quadrature method is applied to derive a closed-form expression for the unconditional pair error probability and establish the union tight upper bound for the average bit error probability (ABEP).","Furthermore, we derive a closed-form expression for the ergodic capacity of the proposed RIS-SM scheme.","Monte Carlo simulations are conducted not only to assess the complexity and reliability of the three detection algorithms but also to validate the results obtained through theoretical derivation results."],"url":"http://arxiv.org/abs/2402.02893v1","category":"cs.IT"}
{"created":"2024-02-05 11:00:14","title":"Motion-Aware Video Frame Interpolation","abstract":"Video frame interpolation methodologies endeavor to create novel frames betwixt extant ones, with the intent of augmenting the video's frame frequency. However, current methods are prone to image blurring and spurious artifacts in challenging scenarios involving occlusions and discontinuous motion. Moreover, they typically rely on optical flow estimation, which adds complexity to modeling and computational costs. To address these issues, we introduce a Motion-Aware Video Frame Interpolation (MA-VFI) network, which directly estimates intermediate optical flow from consecutive frames by introducing a novel hierarchical pyramid module. It not only extracts global semantic relationships and spatial details from input frames with different receptive fields, enabling the model to capture intricate motion patterns, but also effectively reduces the required computational cost and complexity. Subsequently, a cross-scale motion structure is presented to estimate and refine intermediate flow maps by the extracted features. This approach facilitates the interplay between input frame features and flow maps during the frame interpolation process and markedly heightens the precision of the intervening flow delineations. Finally, a discerningly fashioned loss centered around an intermediate flow is meticulously contrived, serving as a deft rudder to skillfully guide the prognostication of said intermediate flow, thereby substantially refining the precision of the intervening flow mappings. Experiments illustrate that MA-VFI surpasses several representative VFI methods across various datasets, and can enhance efficiency while maintaining commendable efficacy.","sentences":["Video frame interpolation methodologies endeavor to create novel frames betwixt extant ones, with the intent of augmenting the video's frame frequency.","However, current methods are prone to image blurring and spurious artifacts in challenging scenarios involving occlusions and discontinuous motion.","Moreover, they typically rely on optical flow estimation, which adds complexity to modeling and computational costs.","To address these issues, we introduce a Motion-Aware Video Frame Interpolation (MA-VFI) network, which directly estimates intermediate optical flow from consecutive frames by introducing a novel hierarchical pyramid module.","It not only extracts global semantic relationships and spatial details from input frames with different receptive fields, enabling the model to capture intricate motion patterns, but also effectively reduces the required computational cost and complexity.","Subsequently, a cross-scale motion structure is presented to estimate and refine intermediate flow maps by the extracted features.","This approach facilitates the interplay between input frame features and flow maps during the frame interpolation process and markedly heightens the precision of the intervening flow delineations.","Finally, a discerningly fashioned loss centered around an intermediate flow is meticulously contrived, serving as a deft rudder to skillfully guide the prognostication of said intermediate flow, thereby substantially refining the precision of the intervening flow mappings.","Experiments illustrate that MA-VFI surpasses several representative VFI methods across various datasets, and can enhance efficiency while maintaining commendable efficacy."],"url":"http://arxiv.org/abs/2402.02892v1","category":"cs.CV"}
{"created":"2024-02-05 10:59:12","title":"Black-Box Approximation and Optimization with Hierarchical Tucker Decomposition","abstract":"We develop a new method HTBB for the multidimensional black-box approximation and gradient-free optimization, which is based on the low-rank hierarchical Tucker decomposition with the use of the MaxVol indices selection procedure. Numerical experiments for 14 complex model problems demonstrate the robustness of the proposed method for dimensions up to 1000, while it shows significantly more accurate results than classical gradient-free optimization methods, as well as approximation and optimization methods based on the popular tensor train decomposition, which represents a simpler case of a tensor network.","sentences":["We develop a new method HTBB for the multidimensional black-box approximation and gradient-free optimization, which is based on the low-rank hierarchical Tucker decomposition with the use of the MaxVol indices selection procedure.","Numerical experiments for 14 complex model problems demonstrate the robustness of the proposed method for dimensions up to 1000, while it shows significantly more accurate results than classical gradient-free optimization methods, as well as approximation and optimization methods based on the popular tensor train decomposition, which represents a simpler case of a tensor network."],"url":"http://arxiv.org/abs/2402.02890v1","category":"cs.LG"}
{"created":"2024-02-05 10:54:17","title":"Time-Distributed Backdoor Attacks on Federated Spiking Learning","abstract":"This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.","sentences":["This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data.","Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks.","We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage.","Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance.","Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance.","Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness.","In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM.","Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs.","This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks."],"url":"http://arxiv.org/abs/2402.02886v1","category":"cs.CR"}
{"created":"2024-02-05 10:47:46","title":"Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks","abstract":"Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum (NISQ) devices requires the optimal utilization of limited quantum resources. The commonly used gate-based QML models are convenient for software engineers, but their expressivity is restricted by the permissible circuit depth within a finite coherence time. In contrast, pulse-based models enable the construction of \"infinitely\" deep quantum neural networks within the same coherence time, which may unleash greater expressive power for complex learning tasks. In this paper, we investigate this potential from the perspective of quantum control theory. We first indicate that the nonlinearity of pulse-based models comes from the encoding process that can be viewed as the continuous limit of data-reuploading in gate-based models. Subsequently, we prove that the pulse-based model can approximate arbitrary nonlinear functions when the underlying physical system is ensemble controllable. Under this condition, numerical simulations show that the expressivity can be enhanced by either increasing the pulse length or the number of qubits. As anticipated, we demonstrate through numerical examples that the pulse-based model can unleash more expressive power compared to the gate-based model. These findings establish a theoretical foundation for understanding and designing expressive QML models using NISQ devices.","sentences":["Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum (NISQ) devices requires the optimal utilization of limited quantum resources.","The commonly used gate-based QML models are convenient for software engineers, but their expressivity is restricted by the permissible circuit depth within a finite coherence time.","In contrast, pulse-based models enable the construction of \"infinitely\" deep quantum neural networks within the same coherence time, which may unleash greater expressive power for complex learning tasks.","In this paper, we investigate this potential from the perspective of quantum control theory.","We first indicate that the nonlinearity of pulse-based models comes from the encoding process that can be viewed as the continuous limit of data-reuploading in gate-based models.","Subsequently, we prove that the pulse-based model can approximate arbitrary nonlinear functions when the underlying physical system is ensemble controllable.","Under this condition, numerical simulations show that the expressivity can be enhanced by either increasing the pulse length or the number of qubits.","As anticipated, we demonstrate through numerical examples that the pulse-based model can unleash more expressive power compared to the gate-based model.","These findings establish a theoretical foundation for understanding and designing expressive QML models using NISQ devices."],"url":"http://arxiv.org/abs/2402.02880v1","category":"quant-ph"}
{"created":"2024-02-05 10:47:00","title":"Deformation of formal schemes through local homology","abstract":"Deformation theory is treated for locally notherian formal schemes (non necessarily smooth). The cotangent complex is defined in the derived category through the homology localization functor. The basic properties and results of a deformation theory are proved. And the complex is described for regular closed immersions and complete intersection morphisms of formal schemes.","sentences":["Deformation theory is treated for locally notherian formal schemes (non necessarily smooth).","The cotangent complex is defined in the derived category through the homology localization functor.","The basic properties and results of a deformation theory are proved.","And the complex is described for regular closed immersions and complete intersection morphisms of formal schemes."],"url":"http://arxiv.org/abs/2402.02879v1","category":"math.AG"}
{"created":"2024-02-05 10:46:44","title":"The Gaia RVS benchmark stars II. A sample of stars selected for their Gaia high radial velocity","abstract":"The Gaia satellite has already provided the astronomical community with three data releases, and the Radial Velocity Spectrometer (RVS) on board Gaia has provided the radial velocity for 33 million stars. When deriving the radial velocity from the RVS spectra, several stars are measured to have large values. To verify the credibility of these measurements, we selected some bright stars with the modulus of radial velocity in excess of 500\\ to be observed with SOPHIE at OHP and UVES at VLT. This paper is devoted to investigating the chemical composition of the stars observed with UVES. We derived atmospheric parameters using Gaia photometry and parallaxes, and we performed a chemical analysis using the code. We find that the sample consists of metal-poor stars, although none have extremely low metallicities. The abundance patterns match what has been found in other samples of metal-poor stars selected irrespective of their radial velocities. We highlight the presence of three stars with low Cu and Zn abundances that are likely descendants of pair-instability supernovae. Two stars are apparently younger than 1\\,Ga, and their masses exceed twice the turn-off mass of metal-poor populations. This makes it unlikely that they are blue stragglers because it would imply they formed from triple or multiple systems. We suggest instead that they are young metal-poor stars accreted from a dwarf galaxy. Finally, we find that the star RVS721 is associated with the Gjoll stream, which itself is associated with the Globular Cluster NGC\\,3201.","sentences":["The Gaia satellite has already provided the astronomical community with three data releases, and the Radial Velocity Spectrometer (RVS) on board Gaia has provided the radial velocity for 33 million stars.","When deriving the radial velocity from the RVS spectra, several stars are measured to have large values.","To verify the credibility of these measurements, we selected some bright stars with the modulus of radial velocity in excess of 500\\ to be observed with SOPHIE at OHP and UVES at VLT.","This paper is devoted to investigating the chemical composition of the stars observed with UVES.","We derived atmospheric parameters using Gaia photometry and parallaxes, and we performed a chemical analysis using the code.","We find that the sample consists of metal-poor stars, although none have extremely low metallicities.","The abundance patterns match what has been found in other samples of metal-poor stars selected irrespective of their radial velocities.","We highlight the presence of three stars with low Cu and Zn abundances that are likely descendants of pair-instability supernovae.","Two stars are apparently younger than 1\\,Ga, and their masses exceed twice the turn-off mass of metal-poor populations.","This makes it unlikely that they are blue stragglers because it would imply they formed from triple or multiple systems.","We suggest instead that they are young metal-poor stars accreted from a dwarf galaxy.","Finally, we find that the star RVS721 is associated with the Gjoll stream, which itself is associated with the Globular Cluster NGC\\,3201."],"url":"http://arxiv.org/abs/2402.02878v1","category":"astro-ph.GA"}
{"created":"2024-02-05 10:41:31","title":"Morse frames","abstract":"In the context of discrete Morse theory, we introduce Morse frames, which are maps that associate a set of critical simplexes to all simplexes. The main example of Morse frames are the Morse references. In particular, these Morse references allow computing Morse complexes, an important tool for homology. We highlight the link between Morse references and gradient flows. We also propose a novel presentation of the Annotation algorithm for persistent cohomology, as a variant of a Morse frame. Finally, we propose another construction, that takes advantage of the Morse reference for computing the Betti numbers in mod 2 arithmetic.","sentences":["In the context of discrete Morse theory, we introduce Morse frames, which are maps that associate a set of critical simplexes to all simplexes.","The main example of Morse frames are the Morse references.","In particular, these Morse references allow computing Morse complexes, an important tool for homology.","We highlight the link between Morse references and gradient flows.","We also propose a novel presentation of the Annotation algorithm for persistent cohomology, as a variant of a Morse frame.","Finally, we propose another construction, that takes advantage of the Morse reference for computing the Betti numbers in mod 2 arithmetic."],"url":"http://arxiv.org/abs/2402.02874v1","category":"cs.DM"}
{"created":"2024-02-05 10:36:48","title":"Statistics without Interpretation: A Sober Look at Explainable Machine Learning","abstract":"In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.","sentences":["In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used.","We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation.","Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature.","In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted.","They should also clarify what questions about the function can and cannot be answered given the explanations.","Our argument is based on the distinction between statistics and their interpretation.","It also relies on parallels between explainable machine learning and applied statistics."],"url":"http://arxiv.org/abs/2402.02870v1","category":"cs.LG"}
{"created":"2024-02-05 10:18:15","title":"Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning","abstract":"We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.","sentences":["We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization.","Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts.","This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system.","The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain.","We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark.","We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent."],"url":"http://arxiv.org/abs/2402.02858v1","category":"cs.LG"}
{"created":"2024-02-05 10:16:20","title":"Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation","abstract":"In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodically evaluating and dynamically adjusting each weight's significance and the model's sparsity distribution during the training. This approach ensures a consistent and minimal parameter budget throughout the full learning lifecycle, paving the way for \"end-to-end\" efficiency from training to inference. Our extensive experimental results underline DSL's effectiveness, significantly reducing training and inference costs while delivering comparable recommendation performance.","sentences":["In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment.","This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations.","Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints.","These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design.","Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations.","Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models.","DSL innovatively trains a lightweight sparse model from scratch, periodically evaluating and dynamically adjusting each weight's significance and the model's sparsity distribution during the training.","This approach ensures a consistent and minimal parameter budget throughout the full learning lifecycle, paving the way for \"end-to-end\" efficiency from training to inference.","Our extensive experimental results underline DSL's effectiveness, significantly reducing training and inference costs while delivering comparable recommendation performance."],"url":"http://arxiv.org/abs/2402.02855v1","category":"cs.IR"}
{"created":"2024-02-05 10:10:10","title":"Small inertia limit for coupled kinetic swarming models","abstract":"We investigate various versions of multi-dimensional systems involving many species, modeling aggregation phenomena through nonlocal interaction terms. We establish a rigorous connection between kinetic and macroscopic descriptions by considering the small-inertia limit at the kinetic level. The results are proven either under smoothness assumptions on all interaction kernels or under singular assumptions for \\emph{self-interaction} potentials. Utilizing different techniques in the two cases, we demonstrate the existence of a solution to the kinetic system, provide uniform estimates with respect to the inertia parameter, and show convergence towards the corresponding macroscopic system as the inertia approaches zero.","sentences":["We investigate various versions of multi-dimensional systems involving many species, modeling aggregation phenomena through nonlocal interaction terms.","We establish a rigorous connection between kinetic and macroscopic descriptions by considering the small-inertia limit at the kinetic level.","The results are proven either under smoothness assumptions on all interaction kernels or under singular assumptions for \\emph{self-interaction} potentials.","Utilizing different techniques in the two cases, we demonstrate the existence of a solution to the kinetic system, provide uniform estimates with respect to the inertia parameter, and show convergence towards the corresponding macroscopic system as the inertia approaches zero."],"url":"http://arxiv.org/abs/2402.02854v1","category":"math.AP"}
{"created":"2024-02-05 10:07:37","title":"Repeated-Root Cyclic Codes with Optimal Parameters or Best Parameters Known","abstract":"Cyclic codes are the most studied subclass of linear codes and widely used in data storage and communication systems. Many cyclic codes have optimal parameters or the best parameters known. They are divided into simple-root cyclic codes and repeated-root cyclic codes. Although there are a huge number of references on cyclic codes, few of them are on repeated-root cyclic codes. Hence, repeated-root cyclic codes are rarely studied. There are a few families of distance-optimal repeated-root binary and $p$-ary cyclic codes for odd prime $p$ in the literature. However, it is open whether there exists an infinite family of distance-optimal repeated-root cyclic codes over $\\bF_q$ for each even $q \\geq 4$.   In this paper, three infinite families of distance-optimal repeated-root cyclic codes with minimum distance 3 or 4 are constructed; two other infinite families of repeated-root cyclic codes with minimum distance 3 or 4 are developed; four infinite families of repeated-root cyclic codes with minimum distance 6 or 8 are presented; and two infinite families of repeated-root binary cyclic codes with parameters $[2n, k, d \\geq (n-1)/\\log_2 n]$, where $n=2^m-1$ and $k \\geq n$, are constructed. In addition, 27 repeated-root cyclic codes of length up to $254$ over $\\bF_q$ for $q \\in \\{2, 4, 8\\}$ with optimal parameters or best parameters known are obtained in this paper. The results of this paper show that repeated-root cyclic codes could be very attractive and are worth of further investigation.","sentences":["Cyclic codes are the most studied subclass of linear codes and widely used in data storage and communication systems.","Many cyclic codes have optimal parameters or the best parameters known.","They are divided into simple-root cyclic codes and repeated-root cyclic codes.","Although there are a huge number of references on cyclic codes, few of them are on repeated-root cyclic codes.","Hence, repeated-root cyclic codes are rarely studied.","There are a few families of distance-optimal repeated-root binary and $p$-ary cyclic codes for odd prime $p$ in the literature.","However, it is open whether there exists an infinite family of distance-optimal repeated-root cyclic codes over $\\bF_q$ for each even $q \\geq 4$.   ","In this paper, three infinite families of distance-optimal repeated-root cyclic codes with minimum distance 3 or 4 are constructed; two other infinite families of repeated-root cyclic codes with minimum distance 3 or 4 are developed; four infinite families of repeated-root cyclic codes with minimum distance 6 or 8 are presented; and two infinite families of repeated-root binary cyclic codes with parameters $[2n, k, d \\geq (n-1)/\\log_2 n]$, where $n=2^m-1$ and $k \\geq n$, are constructed.","In addition, 27 repeated-root cyclic codes of length up to $254$ over $\\bF_q$ for $q \\in \\{2, 4, 8\\}$ with optimal parameters or best parameters known are obtained in this paper.","The results of this paper show that repeated-root cyclic codes could be very attractive and are worth of further investigation."],"url":"http://arxiv.org/abs/2402.02853v1","category":"cs.IT"}
{"created":"2024-02-05 10:01:56","title":"Information theoretic measures on quantum droplets in ultracold atomic systems","abstract":"We consider Shannon entropy, Fisher information, R\\'enyi entropy, and Tsallis entropy to study the transition from localized phase to droplet phase in Bose-Einstein condensates. Based on an effective Gross-Pitaevskii equation with Lee-Huang-Yang correction, we calculate density distributions both in localized and the droplet phases and show that the entropy measures can efficiently detect the transition from localized to droplet phase. We find that the Shannon entropies in coordinate space increase both in localized and droplet phases. The increase of entropy in droplet phase is abrupt while it is gradual in localized phase. We observe an opposite trend in the case of Fisher information. These results are found to be consistent with the R\\'enyi and Tsallis entropic measures.","sentences":["We consider Shannon entropy, Fisher information, R\\'enyi entropy, and Tsallis entropy to study the transition from localized phase to droplet phase in Bose-Einstein condensates.","Based on an effective Gross-Pitaevskii equation with Lee-Huang-Yang correction, we calculate density distributions both in localized and the droplet phases and show that the entropy measures can efficiently detect the transition from localized to droplet phase.","We find that the Shannon entropies in coordinate space increase both in localized and droplet phases.","The increase of entropy in droplet phase is abrupt while it is gradual in localized phase.","We observe an opposite trend in the case of Fisher information.","These results are found to be consistent with the R\\'enyi and Tsallis entropic measures."],"url":"http://arxiv.org/abs/2402.02848v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-05 10:00:34","title":"A unified rule format for bounded nondeterminism in SOS with terms as labels","abstract":"We present a unified rule format for structural operational semantics with terms as labels that guarantees that the associated labelled transition system has some bounded-nondeterminism property. The properties we consider include finite branching, initials finiteness and image finiteness.","sentences":["We present a unified rule format for structural operational semantics with terms as labels that guarantees that the associated labelled transition system has some bounded-nondeterminism property.","The properties we consider include finite branching, initials finiteness and image finiteness."],"url":"http://arxiv.org/abs/2402.02847v1","category":"cs.LO"}
{"created":"2024-02-05 09:51:16","title":"The exponential turnpike property for periodic linear quadratic optimal control problems in infinite dimension","abstract":"In this paper, we establish an exponential periodic turnpike property for linear quadratic optimal control problems governed by periodic systems in infinite dimension. We show that the optimal trajectory converges exponentially to a periodic orbit when the time horizon tends to infinity. Similar results are obtained for the optimal control and adjoint state. Our proof is based on the large time behavior of solutions of operator differential Riccati equations with periodic coefficients.","sentences":["In this paper, we establish an exponential periodic turnpike property for linear quadratic optimal control problems governed by periodic systems in infinite dimension.","We show that the optimal trajectory converges exponentially to a periodic orbit when the time horizon tends to infinity.","Similar results are obtained for the optimal control and adjoint state.","Our proof is based on the large time behavior of solutions of operator differential Riccati equations with periodic coefficients."],"url":"http://arxiv.org/abs/2402.02841v1","category":"math.OC"}
{"created":"2024-02-05 09:51:01","title":"Measuring topological invariants for higher-order exceptional points in quantum multipartite systems","abstract":"Owing to the presence of exceptional points (EPs), non-Hermitian (NH) systems can display intriguing topological phenomena without Hermitian analogs. However, experimental characteristics of exceptional topological invariants have been restricted to second-order EPs (EP2s) in classical or semiclassical systems. We here propose an NH multi-qubit model with higher-order EPs, each of which is underlain by a multifold-degenerate multipartite entangled eigenstate. We implement the three-qubit model by controllably coupling a superconducting qubit to two microwave resonators, one serving as a Hermitian photonic qubit while the other as an NH qubit. We experimentally quantify the topological invariant for an EP3, by mapping out the complex eigenspectra along a loop surrounding this EP3 in the parameter space. The nonclassicality of the realized topology is manifested by the observed quantum correlations in the corresponding eigenstates. Our results extend research of exceptional topology to fully quantum-mechanical models with multi-partite entangled eigenstates. We further demonstrate the non-reciprocal transmission of a single photon, during which the photon is nonlocally shared by three individual elements.","sentences":["Owing to the presence of exceptional points (EPs), non-Hermitian (NH) systems can display intriguing topological phenomena without Hermitian analogs.","However, experimental characteristics of exceptional topological invariants have been restricted to second-order EPs (EP2s) in classical or semiclassical systems.","We here propose an NH multi-qubit model with higher-order EPs, each of which is underlain by a multifold-degenerate multipartite entangled eigenstate.","We implement the three-qubit model by controllably coupling a superconducting qubit to two microwave resonators, one serving as a Hermitian photonic qubit while the other as an NH qubit.","We experimentally quantify the topological invariant for an EP3, by mapping out the complex eigenspectra along a loop surrounding this EP3 in the parameter space.","The nonclassicality of the realized topology is manifested by the observed quantum correlations in the corresponding eigenstates.","Our results extend research of exceptional topology to fully quantum-mechanical models with multi-partite entangled eigenstates.","We further demonstrate the non-reciprocal transmission of a single photon, during which the photon is nonlocally shared by three individual elements."],"url":"http://arxiv.org/abs/2402.02839v1","category":"quant-ph"}
{"created":"2024-02-05 09:48:46","title":"HAPI-FHIR Server Implementation to Enhancing Interoperability among Primary Care Health Information Systems in Sri Lanka: Review of the Technical Use Case","abstract":"This review underscores the vital role of interoperability in digital health, advocating for a standardized framework. It focuses on implementing a Fast Healthcare Interoperability Resources (FHIR) server, addressing technical, semantic, and process challenges. FHIR's adaptability ensures uniformity within Primary Care Health Information Systems, fostering interoperability. Patient data management complexities highlight the pivotal role of semantic interoperability in seamless patient care. FHIR standards enhance these efforts, offering multiple pathways for data search. The ADR-guided FHIR server implementation systematically addresses challenges related to patient identity, biometrics, and data security. The detailed development phases emphasize architecture, API integration, and security. The concluding stages incorporate forward-looking approaches, including HHIMS Synthetic Dataset testing. Envisioning FHIR integration as transformative, it anticipates a responsive healthcare environment aligned with the evolving digital health landscape, ensuring comprehensive, dynamic, and interconnected systems for efficient data exchange and access.","sentences":["This review underscores the vital role of interoperability in digital health, advocating for a standardized framework.","It focuses on implementing a Fast Healthcare Interoperability Resources (FHIR) server, addressing technical, semantic, and process challenges.","FHIR's adaptability ensures uniformity within Primary Care Health Information Systems, fostering interoperability.","Patient data management complexities highlight the pivotal role of semantic interoperability in seamless patient care.","FHIR standards enhance these efforts, offering multiple pathways for data search.","The ADR-guided FHIR server implementation systematically addresses challenges related to patient identity, biometrics, and data security.","The detailed development phases emphasize architecture, API integration, and security.","The concluding stages incorporate forward-looking approaches, including HHIMS Synthetic Dataset testing.","Envisioning FHIR integration as transformative, it anticipates a responsive healthcare environment aligned with the evolving digital health landscape, ensuring comprehensive, dynamic, and interconnected systems for efficient data exchange and access."],"url":"http://arxiv.org/abs/2402.02838v1","category":"cs.CY"}
{"created":"2024-02-05 09:48:07","title":"With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations","abstract":"Topics play an important role in the global organisation of a conversation as what is currently discussed constrains the possible contributions of the participant. Understanding the way topics are organised in interaction would provide insight on the structure of dialogue beyond the sequence of utterances. However, studying this high-level structure is a complex task that we try to approach by first segmenting dialogues into smaller topically coherent sets of utterances. Understanding the interactions between these segments would then enable us to propose a model of topic organisation at a dialogue level. In this paper we work with open-domain conversations and try to reach a comparable level of accuracy as recent machine learning based topic segmentation models but with a formal approach. The features we identify as meaningful for this task help us understand better the topical structure of a conversation.","sentences":["Topics play an important role in the global organisation of a conversation as what is currently discussed constrains the possible contributions of the participant.","Understanding the way topics are organised in interaction would provide insight on the structure of dialogue beyond the sequence of utterances.","However, studying this high-level structure is a complex task that we try to approach by first segmenting dialogues into smaller topically coherent sets of utterances.","Understanding the interactions between these segments would then enable us to propose a model of topic organisation at a dialogue level.","In this paper we work with open-domain conversations and try to reach a comparable level of accuracy as recent machine learning based topic segmentation models but with a formal approach.","The features we identify as meaningful for this task help us understand better the topical structure of a conversation."],"url":"http://arxiv.org/abs/2402.02837v1","category":"cs.CL"}
{"created":"2024-02-05 09:45:38","title":"Perceptual Learned Image Compression via End-to-End JND-Based Optimization","abstract":"Emerging Learned image Compression (LC) achieves significant improvements in coding efficiency by end-to-end training of neural networks for compression. An important benefit of this approach over traditional codecs is that any optimization criteria can be directly applied to the encoder-decoder networks during training. Perceptual optimization of LC to comply with the Human Visual System (HVS) is among such criteria, which has not been fully explored yet. This paper addresses this gap by proposing a novel framework to integrate Just Noticeable Distortion (JND) principles into LC. Leveraging existing JND datasets, three perceptual optimization methods are proposed to integrate JND into the LC training process: (1) Pixel-Wise JND Loss (PWL) prioritizes pixel-by-pixel fidelity in reproducing JND characteristics, (2) Image-Wise JND Loss (IWL) emphasizes on overall imperceptible degradation levels, and (3) Feature-Wise JND Loss (FWL) aligns the reconstructed image features with perceptually significant features. Experimental evaluations demonstrate the effectiveness of JND integration, highlighting improvements in rate-distortion performance and visual quality, compared to baseline methods. The proposed methods add no extra complexity after training.","sentences":["Emerging Learned image Compression (LC) achieves significant improvements in coding efficiency by end-to-end training of neural networks for compression.","An important benefit of this approach over traditional codecs is that any optimization criteria can be directly applied to the encoder-decoder networks during training.","Perceptual optimization of LC to comply with the Human Visual System (HVS) is among such criteria, which has not been fully explored yet.","This paper addresses this gap by proposing a novel framework to integrate Just Noticeable Distortion (JND) principles into LC.","Leveraging existing JND datasets, three perceptual optimization methods are proposed to integrate JND into the LC training process: (1) Pixel-Wise JND Loss (PWL) prioritizes pixel-by-pixel fidelity in reproducing JND characteristics, (2) Image-Wise JND Loss (IWL) emphasizes on overall imperceptible degradation levels, and (3) Feature-Wise JND Loss (FWL) aligns the reconstructed image features with perceptually significant features.","Experimental evaluations demonstrate the effectiveness of JND integration, highlighting improvements in rate-distortion performance and visual quality, compared to baseline methods.","The proposed methods add no extra complexity after training."],"url":"http://arxiv.org/abs/2402.02836v1","category":"eess.IV"}
{"created":"2024-02-05 09:44:30","title":"Behavior Tree Capabilities for Dynamic Multi-Robot Task Allocation with Heterogeneous Robot Teams","abstract":"While individual robots are becoming increasingly capable, with new sensors and actuators, the complexity of expected missions increased exponentially in comparison. To cope with this complexity, heterogeneous teams of robots have become a significant research interest in recent years. Making effective use of the robots and their unique skills in a team is challenging. Dynamic runtime conditions often make static task allocations infeasible, therefore requiring a dynamic, capability-aware allocation of tasks to team members. To this end, we propose and implement a system that allows a user to specify missions using Bheavior Trees (BTs), which can then, at runtime, be dynamically allocated to the current robot team. The system allows to statically model an individual robot's capabilities within our ros_bt_py BT framework. It offers a runtime auction system to dynamically allocate tasks to the most capable robot in the current team. The system leverages utility values and pre-conditions to ensure that the allocation improves the overall mission execution quality while preventing faulty assignments. To evaluate the system, we simulated a find-and-decontaminate mission with a team of three heterogeneous robots and analyzed the utilization and overall mission times as metrics. Our results show that our system can improve the overall effectiveness of a team while allowing for intuitive mission specification and flexibility in the team composition.","sentences":["While individual robots are becoming increasingly capable, with new sensors and actuators, the complexity of expected missions increased exponentially in comparison.","To cope with this complexity, heterogeneous teams of robots have become a significant research interest in recent years.","Making effective use of the robots and their unique skills in a team is challenging.","Dynamic runtime conditions often make static task allocations infeasible, therefore requiring a dynamic, capability-aware allocation of tasks to team members.","To this end, we propose and implement a system that allows a user to specify missions using Bheavior Trees (BTs), which can then, at runtime, be dynamically allocated to the current robot team.","The system allows to statically model an individual robot's capabilities within our ros_bt_py BT framework.","It offers a runtime auction system to dynamically allocate tasks to the most capable robot in the current team.","The system leverages utility values and pre-conditions to ensure that the allocation improves the overall mission execution quality while preventing faulty assignments.","To evaluate the system, we simulated a find-and-decontaminate mission with a team of three heterogeneous robots and analyzed the utilization and overall mission times as metrics.","Our results show that our system can improve the overall effectiveness of a team while allowing for intuitive mission specification and flexibility in the team composition."],"url":"http://arxiv.org/abs/2402.02833v1","category":"cs.RO"}
{"created":"2024-02-05 09:36:21","title":"Automatic Detection of Depression in Speech Using Ensemble Convolutional Neural Networks","abstract":"This paper proposes a speech-based method for automatic depression classification. The system is based on ensemble learning for Convolutional Neural Networks (CNNs) and is evaluated using the data and the experimental protocol provided in the Depression Classification Sub-Challenge (DCC) at the 2016 Audio-Visual Emotion Challenge (AVEC-2016). In the pre-processing phase, speech files are represented as a sequence of log-spectrograms and randomly sampled to balance positive and negative samples. For the classification task itself, first, a more suitable architecture for this task, based on One-Dimensional Convolutional Neural Networks, is built. Secondly, several of these CNN-based models are trained with different initializations and then the corresponding individual predictions are fused by using an Ensemble Averaging algorithm and combined per speaker to get an appropriate final decision. The proposed ensemble system achieves satisfactory results on the DCC at the AVEC-2016 in comparison with a reference system based on Support Vector Machines and hand-crafted features, with a CNN+LSTM-based system called DepAudionet, and with the case of a single CNN-based classifier.","sentences":["This paper proposes a speech-based method for automatic depression classification.","The system is based on ensemble learning for Convolutional Neural Networks (CNNs) and is evaluated using the data and the experimental protocol provided in the Depression Classification Sub-Challenge (DCC) at the 2016 Audio-Visual Emotion Challenge (AVEC-2016).","In the pre-processing phase, speech files are represented as a sequence of log-spectrograms and randomly sampled to balance positive and negative samples.","For the classification task itself, first, a more suitable architecture for this task, based on One-Dimensional Convolutional Neural Networks, is built.","Secondly, several of these CNN-based models are trained with different initializations and then the corresponding individual predictions are fused by using an Ensemble Averaging algorithm and combined per speaker to get an appropriate final decision.","The proposed ensemble system achieves satisfactory results on the DCC at the AVEC-2016 in comparison with a reference system based on Support Vector Machines and hand-crafted features, with a CNN+LSTM-based system called DepAudionet, and with the case of a single CNN-based classifier."],"url":"http://arxiv.org/abs/2402.02830v1","category":"eess.AS"}
{"created":"2024-02-05 09:06:57","title":"Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective","abstract":"Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together with a carefully designed \"target attention\" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction. Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods. This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models.","sentences":["Time series Anomaly Detection (AD) plays a crucial role for web systems.","Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures.","Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection.","However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously.","To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series.","To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data.","Together with a carefully designed \"target attention\" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction.","Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods.","This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models."],"url":"http://arxiv.org/abs/2402.02820v1","category":"cs.LG"}
{"created":"2024-02-05 08:56:24","title":"Intersectional Two-sided Fairness in Recommendation","abstract":"Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods.","sentences":["Fairness of recommender systems (RS) has attracted increasing attention recently.","Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously.","However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously.","To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR).","Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups.","Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups.","Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.02816v1","category":"cs.IR"}
{"created":"2024-02-05 08:53:05","title":"Interconnected Renormalization of Hubbard Bands and Green's Function Zeros in Mott Insulators Induced by Strong Magnetic Fluctuations","abstract":"We analyze the role of spatial electronic correlations and, in particular, of the magnetic fluctuations in Mott insulators. A half-filled Hubbard model is solved at large strength of the repulsion $U$ on a two-dimensional square lattice using an advanced diagrammatic non-perturbative approach capable of going beyond Hartree-Fock and single-site dynamical mean-field theories. We show that at high temperatures the magnetic fluctuations are weak, and the electronic self-energy of the system is mainly local and is well reproduced by the atomic (Hubbard-I) approximation. Lowering the temperature toward the low-temperature magnetically ordered phase, the non-locality of the self-energy becomes crucial in determining the dispersion of the Hubbard bands and of the Green's function zeros. We therefore establish a precise link between the Luttinger surface and the momentum-structure of the Hubbard bands.","sentences":["We analyze the role of spatial electronic correlations and, in particular, of the magnetic fluctuations in Mott insulators.","A half-filled Hubbard model is solved at large strength of the repulsion $U$ on a two-dimensional square lattice using an advanced diagrammatic non-perturbative approach capable of going beyond Hartree-Fock and single-site dynamical mean-field theories.","We show that at high temperatures the magnetic fluctuations are weak, and the electronic self-energy of the system is mainly local and is well reproduced by the atomic (Hubbard-I) approximation.","Lowering the temperature toward the low-temperature magnetically ordered phase, the non-locality of the self-energy becomes crucial in determining the dispersion of the Hubbard bands and of the Green's function zeros.","We therefore establish a precise link between the Luttinger surface and the momentum-structure of the Hubbard bands."],"url":"http://arxiv.org/abs/2402.02814v1","category":"cond-mat.str-el"}
{"created":"2024-02-05 08:38:03","title":"A Hybrid Finite-Difference-Particle Method for Chemotaxis Models","abstract":"Chemotaxis systems play a crucial role in modeling the dynamics of bacterial and cellular behaviors, including propagation, aggregation, and pattern formation, all under the influence of chemical signals. One notable characteristic of these systems is their ability to simulate concentration phenomena, where cell density undergoes rapid growth near specific concentration points or along certain curves. Such growth can result in singular, spiky structures and lead to finite-time blowups.   Our investigation focuses on the dynamics of the Patlak-Keller-Segel chemotaxis system and its two-species extensions. In the latter case, different species may exhibit distinct chemotactic sensitivities, giving rise to very different rates of cell density growth. Such a situation may be extremely challenging for numerical methods as they may fail to accurately capture the blowup of the slower-growing species mainly due to excessive numerical dissipation.   In this paper, we propose a hybrid finite-difference-particle (FDP) method, in which a particle method is used to solve the chemotaxis equation(s), while finite difference schemes are employed to solve the chemoattractant equation. Thanks to the low-dissipation nature of the particle method, the proposed hybrid scheme is particularly adept at capturing the blowup behaviors in both one- and two-species cases. The proposed hybrid FDP methods are tested on a series of challenging examples, and the obtained numerical results demonstrate that our hybrid method can provide sharp resolution of the singular structures even with a relatively small number of particles. Moreover, in the two-species case, our method adeptly captures the blowing-up solution for the component with lower chemotactic sensitivity, a feature not observed in other works.","sentences":["Chemotaxis systems play a crucial role in modeling the dynamics of bacterial and cellular behaviors, including propagation, aggregation, and pattern formation, all under the influence of chemical signals.","One notable characteristic of these systems is their ability to simulate concentration phenomena, where cell density undergoes rapid growth near specific concentration points or along certain curves.","Such growth can result in singular, spiky structures and lead to finite-time blowups.   ","Our investigation focuses on the dynamics of the Patlak-Keller-Segel chemotaxis system and its two-species extensions.","In the latter case, different species may exhibit distinct chemotactic sensitivities, giving rise to very different rates of cell density growth.","Such a situation may be extremely challenging for numerical methods as they may fail to accurately capture the blowup of the slower-growing species mainly due to excessive numerical dissipation.   ","In this paper, we propose a hybrid finite-difference-particle (FDP) method, in which a particle method is used to solve the chemotaxis equation(s), while finite difference schemes are employed to solve the chemoattractant equation.","Thanks to the low-dissipation nature of the particle method, the proposed hybrid scheme is particularly adept at capturing the blowup behaviors in both one-","and two-species cases.","The proposed hybrid FDP methods are tested on a series of challenging examples, and the obtained numerical results demonstrate that our hybrid method can provide sharp resolution of the singular structures even with a relatively small number of particles.","Moreover, in the two-species case, our method adeptly captures the blowing-up solution for the component with lower chemotactic sensitivity, a feature not observed in other works."],"url":"http://arxiv.org/abs/2402.02808v1","category":"math.NA"}
{"created":"2024-02-05 08:28:57","title":"Uncertainty Quantification of Phase Transition Problems with an Injection Boundary","abstract":"We develop an enthalpy-based modeling and computational framework to quantify uncertainty in Stefan problems with an injection boundary. Inspired by airfoil icing studies, we consider a system featuring an injection boundary inducing domain changes and a free boundary separating phases, resulting in two types of moving boundaries. Our proposed enthalpy-based formulation seamlessly integrates thermal diffusion across the domain with energy fluxes at the boundaries, addressing a modified injection condition for boundary movement. Uncertainty then stems from random variations in the injection boundary. The primary focus of our Uncertainty Quantification (UQ) centers on investigating the effects of uncertainty on free boundary propagation. Through mapping to a reference domain, we derive an enthalpy-based numerical scheme tailored to the transformed coordinate system, facilitating a simple and efficient simulation. Numerical and UQ studies in one and two dimensions validate the proposed model and the extended enthalpy method. They offer intriguing insights into ice accretion and other multiphysics processes involving phase transitions.","sentences":["We develop an enthalpy-based modeling and computational framework to quantify uncertainty in Stefan problems with an injection boundary.","Inspired by airfoil icing studies, we consider a system featuring an injection boundary inducing domain changes and a free boundary separating phases, resulting in two types of moving boundaries.","Our proposed enthalpy-based formulation seamlessly integrates thermal diffusion across the domain with energy fluxes at the boundaries, addressing a modified injection condition for boundary movement.","Uncertainty then stems from random variations in the injection boundary.","The primary focus of our Uncertainty Quantification (UQ) centers on investigating the effects of uncertainty on free boundary propagation.","Through mapping to a reference domain, we derive an enthalpy-based numerical scheme tailored to the transformed coordinate system, facilitating a simple and efficient simulation.","Numerical and UQ studies in one and two dimensions validate the proposed model and the extended enthalpy method.","They offer intriguing insights into ice accretion and other multiphysics processes involving phase transitions."],"url":"http://arxiv.org/abs/2402.02806v1","category":"math.NA"}
{"created":"2024-02-05 08:23:40","title":"Redistribution with Needs","abstract":"We take an axiomatic approach to study redistribution problems when agents report income and needs. We formalize axioms reflecting ethical and operational principles such as additivity, impartiality and individual rationality. Different combinations of those axioms characterize three focal rules (laissez faire, full redistribution, and need-adjusted full redistribution) as well as compromises among them. We also uncover the structure of those compromises exploring the Lorenz dominance criterion as well as majority voting. Our analysis provides an axiomatic justification for a linear income tax system. We conclude our analysis resorting to Eurostat's Household Budget Survey from where we illustrate the different redistribution patterns accounting for needs across European countries.","sentences":["We take an axiomatic approach to study redistribution problems when agents report income and needs.","We formalize axioms reflecting ethical and operational principles such as additivity, impartiality and individual rationality.","Different combinations of those axioms characterize three focal rules (laissez faire, full redistribution, and need-adjusted full redistribution) as well as compromises among them.","We also uncover the structure of those compromises exploring the Lorenz dominance criterion as well as majority voting.","Our analysis provides an axiomatic justification for a linear income tax system.","We conclude our analysis resorting to Eurostat's Household Budget Survey from where we illustrate the different redistribution patterns accounting for needs across European countries."],"url":"http://arxiv.org/abs/2402.02802v1","category":"econ.TH"}
{"created":"2024-02-05 08:13:39","title":"X-ray studies of neutron stars and the equation of state","abstract":"The understanding of neutron star equation of state hinges on a comprehensive analysis of multi-messenger, multi-wavelength data. The recent scrutiny of PSR J0030+0451 data by NICER introduces complexities, unveiling a tension with another X-ray observation of the central compact object in HESS J1731-347, specifically concerning the mass-radius constraint of low-mass neutron stars. This tension persists when integrating NICER's updated data with LIGO/Virgo's gravitational-wave data from the GW170817 binary neutron star merger. Despite attempts to reconcile these disparate observations, the current combined data still can not distinguish different types of neutron stars -- whether they are pure neutron stars or hybrid stars. Bayesian inference indicates only modest changes in the posterior ranges of parameters related to the nuclear matter and deconfinement phase transition. This ongoing exploration underscores the intricate challenges in precisely characterizing neutron stars. It also points out that it is possible to probe the equation of state at different density regimes from future more accurate radii of neutron stars with various masses.","sentences":["The understanding of neutron star equation of state hinges on a comprehensive analysis of multi-messenger, multi-wavelength data.","The recent scrutiny of PSR J0030+0451 data by NICER introduces complexities, unveiling a tension with another X-ray observation of the central compact object in HESS J1731-347, specifically concerning the mass-radius constraint of low-mass neutron stars.","This tension persists when integrating NICER's updated data with LIGO/Virgo's gravitational-wave data from the GW170817 binary neutron star merger.","Despite attempts to reconcile these disparate observations, the current combined data still can not distinguish different types of neutron stars -- whether they are pure neutron stars or hybrid stars.","Bayesian inference indicates only modest changes in the posterior ranges of parameters related to the nuclear matter and deconfinement phase transition.","This ongoing exploration underscores the intricate challenges in precisely characterizing neutron stars.","It also points out that it is possible to probe the equation of state at different density regimes from future more accurate radii of neutron stars with various masses."],"url":"http://arxiv.org/abs/2402.02799v1","category":"astro-ph.HE"}
{"created":"2024-02-05 08:11:58","title":"A Comprehensive Numerical Approach to Coil Placement in Cerebral Aneurysms: Mathematical Modeling and In Silico Occlusion Classification","abstract":"Endovascular coil embolization is one of the primary treatment techniques for cerebral aneurysms. Although it is a well established and minimally invasive method, it bears the risk of sub-optimal coil placement which can lead to incomplete occlusion of the aneurysm possibly causing recurrence. One of the key features of coils is that they have an imprinted natural shape supporting the fixation within the aneurysm. For the spatial discretization our mathematical coil model is based on the Discrete Elastic Rod model which results in a dimension-reduced 1D system of differential equations. We include bending and twisting responses to account for the coils natural curvature. Collisions between coil segments and the aneurysm-wall are handled by an efficient contact algorithm that relies on an octree based collision detection. The numerical solution of the model is obtained by a symplectic semi-implicit Euler time stepping method. Our model can be easily incorporated into blood flow simulations of embolized aneurysms. In order to differentiate optimal from sub-optimal placements, we employ a suitable in silico Raymond-Roy type occlusion classification and measure the local packing density in the aneurysm at its neck, wall-region and core. We investigate the impact of uncertainties in the coil parameters and embolization procedure. To this end, we vary the position and the angle of insertion of the microcatheter, and approximate the local packing density distributions by evaluating sample statistics.","sentences":["Endovascular coil embolization is one of the primary treatment techniques for cerebral aneurysms.","Although it is a well established and minimally invasive method, it bears the risk of sub-optimal coil placement which can lead to incomplete occlusion of the aneurysm possibly causing recurrence.","One of the key features of coils is that they have an imprinted natural shape supporting the fixation within the aneurysm.","For the spatial discretization our mathematical coil model is based on the Discrete Elastic Rod model which results in a dimension-reduced 1D system of differential equations.","We include bending and twisting responses to account for the coils natural curvature.","Collisions between coil segments and the aneurysm-wall are handled by an efficient contact algorithm that relies on an octree based collision detection.","The numerical solution of the model is obtained by a symplectic semi-implicit Euler time stepping method.","Our model can be easily incorporated into blood flow simulations of embolized aneurysms.","In order to differentiate optimal from sub-optimal placements, we employ a suitable in silico Raymond-Roy type occlusion classification and measure the local packing density in the aneurysm at its neck, wall-region and core.","We investigate the impact of uncertainties in the coil parameters and embolization procedure.","To this end, we vary the position and the angle of insertion of the microcatheter, and approximate the local packing density distributions by evaluating sample statistics."],"url":"http://arxiv.org/abs/2402.02798v1","category":"cs.CE"}
{"created":"2024-02-05 08:10:16","title":"Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects","abstract":"Surface defect inspection plays an important role in the process of industrial manufacture and production. Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales. The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion. The attention map makes the model pay more attention to defect features. The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input. The obtained DRF features capture rich context information with a large range of receptive fields. Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods. Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.","sentences":["Surface defect inspection plays an important role in the process of industrial manufacture and production.","Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on.","To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network.","JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features.","The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects.","In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales.","The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion.","The attention map makes the model pay more attention to defect features.","The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input.","The obtained DRF features capture rich context information with a large range of receptive fields.","Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods.","Meanwhile, our method reaches a real-time defect detection speed of 66 FPS."],"url":"http://arxiv.org/abs/2402.02797v1","category":"cs.CV"}
{"created":"2024-02-05 08:06:03","title":"A Learning-Based Caching Mechanism for Edge Content Delivery","abstract":"With the advent of 5G networks and the rise of the Internet of Things (IoT), Content Delivery Networks (CDNs) are increasingly extending into the network edge. This shift introduces unique challenges, particularly due to the limited cache storage and the diverse request patterns at the edge. These edge environments can host traffic classes characterized by varied object-size distributions and object-access patterns. Such complexity makes it difficult for traditional caching strategies, which often rely on metrics like request frequency or time intervals, to be effective. Despite these complexities, the optimization of edge caching is crucial. Improved byte hit rates at the edge not only alleviate the load on the network backbone but also minimize operational costs and expedite content delivery to end-users.   In this paper, we introduce HR-Cache, a comprehensive learning-based caching framework grounded in the principles of Hazard Rate (HR) ordering, a rule originally formulated to compute an upper bound on cache performance. HR-Cache leverages this rule to guide future object eviction decisions. It employs a lightweight machine learning model to learn from caching decisions made based on HR ordering, subsequently predicting the \"cache-friendliness\" of incoming requests. Objects deemed \"cache-averse\" are placed into cache as priority candidates for eviction. Through extensive experimentation, we demonstrate that HR-Cache not only consistently enhances byte hit rates compared to existing state-of-the-art methods but also achieves this with minimal prediction overhead.   Our experimental results, using three real-world traces and one synthetic trace, indicate that HR-Cache consistently achieves 2.2-14.6% greater WAN traffic savings than LRU. It outperforms not only heuristic caching strategies but also the state-of-the-art learning-based algorithm.","sentences":["With the advent of 5G networks and the rise of the Internet of Things (IoT), Content Delivery Networks (CDNs) are increasingly extending into the network edge.","This shift introduces unique challenges, particularly due to the limited cache storage and the diverse request patterns at the edge.","These edge environments can host traffic classes characterized by varied object-size distributions and object-access patterns.","Such complexity makes it difficult for traditional caching strategies, which often rely on metrics like request frequency or time intervals, to be effective.","Despite these complexities, the optimization of edge caching is crucial.","Improved byte hit rates at the edge not only alleviate the load on the network backbone but also minimize operational costs and expedite content delivery to end-users.   ","In this paper, we introduce HR-Cache, a comprehensive learning-based caching framework grounded in the principles of Hazard Rate (HR) ordering, a rule originally formulated to compute an upper bound on cache performance.","HR-Cache leverages this rule to guide future object eviction decisions.","It employs a lightweight machine learning model to learn from caching decisions made based on HR ordering, subsequently predicting the \"cache-friendliness\" of incoming requests.","Objects deemed \"cache-averse\" are placed into cache as priority candidates for eviction.","Through extensive experimentation, we demonstrate that HR-Cache not only consistently enhances byte hit rates compared to existing state-of-the-art methods but also achieves this with minimal prediction overhead.   ","Our experimental results, using three real-world traces and one synthetic trace, indicate that HR-Cache consistently achieves 2.2-14.6% greater WAN traffic savings than LRU.","It outperforms not only heuristic caching strategies but also the state-of-the-art learning-based algorithm."],"url":"http://arxiv.org/abs/2402.02795v1","category":"cs.NI"}
{"created":"2024-02-05 08:04:20","title":"Convolutional restricted Boltzmann machine (CRBM) correlated variational wave function for the Hubbard model on a square lattice: Mott metal-insulator transition","abstract":"We use a convolutional restricted Boltzmann machine (CRBM) neural network to construct a variational wave function (WF) for the Hubbard model on a square lattice and study it using the variational Monte Carlo (VMC) method. In the wave function, the CRBM acts as a correlation factor to a mean-field BCS state. The number of variational parameters in the WF does not grow automatically with the lattice size and it is computationally much more efficient compared to other neural network based WFs. We find that in the intermediate to strong coupling regime of the model at half-filling, the wave function outperforms even the highly accurate long range backflow-Jastrow correlated wave function. Using the WF, we study the ground state of the half-filled model as a function of onsite Coulomb repulsion $U$. We consider two cases for the next-nearest-neighbor hopping parameter, e.g., $t'=0$ as well as a frustrated model case with $t'\\neq 0$. By examining several quantities, e.g., double occupancy, charge gap, momentum distribution, and spin-spin correlations, we find that the weekly correlated phase in both cases is paramagnetic metallic (PM). As $U$ is increased, the system undergoes a first-order Mott transition to an insulating state at a critical $U_c$, the value of which depends upon $t'$. The Mott state in both cases is spin gapped with long range antiferromagnetic (AF) order. Remarkably, the AF order emerges spontaneously from the wave function which does not have any explicitly broken symmetry in it. Apart from some quantitative differences in the results for the two values of $t'$, we find some interesting qualitative differences in the way the Mott transition takes place in the two cases.","sentences":["We use a convolutional restricted Boltzmann machine (CRBM) neural network to construct a variational wave function (WF) for the Hubbard model on a square lattice and study it using the variational Monte Carlo (VMC) method.","In the wave function, the CRBM acts as a correlation factor to a mean-field BCS state.","The number of variational parameters in the WF does not grow automatically with the lattice size and it is computationally much more efficient compared to other neural network based WFs.","We find that in the intermediate to strong coupling regime of the model at half-filling, the wave function outperforms even the highly accurate long range backflow-Jastrow correlated wave function.","Using the WF, we study the ground state of the half-filled model as a function of onsite Coulomb repulsion $U$. We consider two cases for the next-nearest-neighbor hopping parameter, e.g., $t'=0$ as well as a frustrated model case with $t'\\neq 0$.","By examining several quantities, e.g., double occupancy, charge gap, momentum distribution, and spin-spin correlations, we find that the weekly correlated phase in both cases is paramagnetic metallic (PM).","As $U$ is increased, the system undergoes a first-order Mott transition to an insulating state at a critical $U_c$, the value of which depends upon $t'$. The Mott state in both cases is spin gapped with long range antiferromagnetic (AF) order.","Remarkably, the AF order emerges spontaneously from the wave function which does not have any explicitly broken symmetry in it.","Apart from some quantitative differences in the results for the two values of $t'$, we find some interesting qualitative differences in the way the Mott transition takes place in the two cases."],"url":"http://arxiv.org/abs/2402.02794v1","category":"cond-mat.str-el"}
{"created":"2024-02-05 07:59:40","title":"Neural networks for differential games","abstract":"We study deterministic optimal control problems for differential games with finite horizon. We propose new approximations of the strategies in feedback form, and show error estimates and a convergence result of the value in some weak sense for one of the formulations. This result applies in particular to neural networks approximations. This work follows some ideas introduced in Bokanowski, Prost and Warin (PDEA, 2023) for deterministic optimal control problems, yet with a simplified approach for the error estimates, which allows to consider a global optimization scheme instead of a time-marching scheme. We also give a new approximation result between the continuous and the semi-discrete optimal control value in the game setting, improving the classical convergence order under some assumptions on the dynamical system. Numerical examples are performed on elementary academic problems related to backward reachability, with exact analytic solutions given, as well as a two-player game in presence of state constraints. We use stochastic gradient type algorithms in order to deal with the min-max problem.","sentences":["We study deterministic optimal control problems for differential games with finite horizon.","We propose new approximations of the strategies in feedback form, and show error estimates and a convergence result of the value in some weak sense for one of the formulations.","This result applies in particular to neural networks approximations.","This work follows some ideas introduced in Bokanowski, Prost and Warin (PDEA, 2023) for deterministic optimal control problems, yet with a simplified approach for the error estimates, which allows to consider a global optimization scheme instead of a time-marching scheme.","We also give a new approximation result between the continuous and the semi-discrete optimal control value in the game setting, improving the classical convergence order under some assumptions on the dynamical system.","Numerical examples are performed on elementary academic problems related to backward reachability, with exact analytic solutions given, as well as a two-player game in presence of state constraints.","We use stochastic gradient type algorithms in order to deal with the min-max problem."],"url":"http://arxiv.org/abs/2402.02792v1","category":"math.OC"}
{"created":"2024-02-05 07:51:37","title":"Moment propagation of a Vlasov-Poisson system for ions flow in the quasi-neutral regime","abstract":"In light of recent work in the global well-posedness of solutions for an ionic Vlasov-Poisson system, as demonstrated by Griffin-Pickering and Iacobelli, the current work focuses on the moment propagation of the corresponding system in quasi-neutral regime. Such moment propagation result relies on an estimate of $Q_*(t)=|V(t;0,x,v)-V(0;0,x,v)|$, where $V(s;t,x,v)$ represents the solution of the characteristic ordinary differential equation associated with the Vlasov-Poisson system.","sentences":["In light of recent work in the global well-posedness of solutions for an ionic Vlasov-Poisson system, as demonstrated by Griffin-Pickering and Iacobelli, the current work focuses on the moment propagation of the corresponding system in quasi-neutral regime.","Such moment propagation result relies on an estimate of $Q_*(t)=|V(t;0,x,v)-V(0;0,x,v)|$, where $V(s;t,x,v)$ represents the solution of the characteristic ordinary differential equation associated with the Vlasov-Poisson system."],"url":"http://arxiv.org/abs/2402.02786v1","category":"math.AP"}
{"created":"2024-02-05 07:44:06","title":"A tale of two pairs: on the origin of the pseudogap end point in the high-T$_{c}$ cuprate superconductors","abstract":"There are two seemingly unrelated puzzles about the cuprate superconductors. The first puzzle concerns the strong non-BCS behavior around $x_{c}$, the end point of the superconducting dome on the overdoped side, where the cuparte is believed to be well described by the fermi liquid theory. This is the most evident in the observed $\\rho_{s}(0)-T_{c}$ scaling and the large amount of uncondensed optical spectral weight at low energy. The second puzzle concerns the remarkable robustness of the d-wave pairing against the inevitable disorder effect in such a doped system, which is also totally unexpected from the conventional BCS picture. Here we show that these two puzzles are deeply connected to the origin of a third puzzle about the cuprate superconductors, namely, the mysterious quantum critical behavior observed around $x^{*}$, the so called pseudogap end point. Through a systematic variational optimization of the disordered 2D $t-J$ model from the resonating valence bond(RVB) perspective, we find that the d-wave pairing in this model is remarkably more robust against the disorder effect than that in a conventional d-wave BCS superconductor. We find that such remarkable robustness can be attributed to the spin-charge separation mechanism in the RVB picture, through which the d-wave RVB pairing of the charge neutral spinons becomes essentially immune to the impurity potential. We propose that there exists a Mott transition at $x^{*}$, where the RVB pairing in the underdoped regime is transmuted into the increasingly more BCS-like pairing for $x>x^{*}$, whose increasing fragility against the disorder effect leads to the non-BCS behavior and the ultimate suppression of superconductivity around $x_{c}$.","sentences":["There are two seemingly unrelated puzzles about the cuprate superconductors.","The first puzzle concerns the strong non-BCS behavior around $x_{c}$, the end point of the superconducting dome on the overdoped side, where the cuparte is believed to be well described by the fermi liquid theory.","This is the most evident in the observed $\\rho_{s}(0)-T_{c}$ scaling and the large amount of uncondensed optical spectral weight at low energy.","The second puzzle concerns the remarkable robustness of the d-wave pairing against the inevitable disorder effect in such a doped system, which is also totally unexpected from the conventional BCS picture.","Here we show that these two puzzles are deeply connected to the origin of a third puzzle about the cuprate superconductors, namely, the mysterious quantum critical behavior observed around $x^{*}$, the so called pseudogap end point.","Through a systematic variational optimization of the disordered 2D $t-J$ model from the resonating valence bond(RVB) perspective, we find that the d-wave pairing in this model is remarkably more robust against the disorder effect than that in a conventional d-wave BCS superconductor.","We find that such remarkable robustness can be attributed to the spin-charge separation mechanism in the RVB picture, through which the d-wave RVB pairing of the charge neutral spinons becomes essentially immune to the impurity potential.","We propose that there exists a Mott transition at $x^{*}$, where the RVB pairing in the underdoped regime is transmuted into the increasingly more BCS-like pairing for $x>x^{*}$, whose increasing fragility against the disorder effect leads to the non-BCS behavior and the ultimate suppression of superconductivity around $x_{c}$."],"url":"http://arxiv.org/abs/2402.02785v1","category":"cond-mat.str-el"}
{"created":"2024-02-05 07:17:53","title":"Boundary control of generalized Korteweg-de Vries-Burgers-Huxley equation: Well-Posedness, Stabilization and Numerical Studies","abstract":"A boundary control problem for the following generalized Korteweg-de Vries-Burgers-Huxley equation: $$u_t=\\nu u_{xx}-\\mu u_{xxx}-\\alpha u^{\\delta}u_x+\\beta u(1-u^{\\delta})(u^{\\delta}-\\gamma), \\ x\\in[0,1], \\ t>0,$$ where $\\nu,\\mu,\\alpha,\\beta>0,$ $\\delta\\in[1,\\infty)$, $\\gamma\\in(0,1)$ subject to Neumann boundary conditions is considered in this work. We first establish the well-posedness of the Neumann boundary value problem by an application of monotonicity arguments, the Hartman-Stampacchia theorem, the Minty-Browder theorem, and the Crandall-Liggett theorem. The additional difficulties caused by the third order linear term is successfully handled by proving a proper version of the Minty-Browder theorem. By using suitable feedback boundary controls, we demonstrate $\\mathrm{L}^2$- and $\\mathrm{H}^1$-stability properties of the closed-loop system for sufficiently large $\\nu>0$. The analytical conclusions from this work are supported and validated by numerical investigations.","sentences":["A boundary control problem for the following generalized Korteweg-de Vries-Burgers-Huxley equation: $$u_t=\\nu u_{xx}-\\mu u_{xxx}-\\alpha u^{\\delta}u_x+\\beta u(1-u^{\\delta})(u^{\\delta}-\\gamma), \\ x\\in[0,1], \\ t>0,$$ where $\\nu,\\mu,\\alpha,\\beta>0,$ $\\delta\\in[1,\\infty)$, $\\gamma\\in(0,1)$ subject to Neumann boundary conditions is considered in this work.","We first establish the well-posedness of the Neumann boundary value problem by an application of monotonicity arguments, the Hartman-Stampacchia theorem, the Minty-Browder theorem, and the Crandall-Liggett theorem.","The additional difficulties caused by the third order linear term is successfully handled by proving a proper version of the Minty-Browder theorem.","By using suitable feedback boundary controls, we demonstrate $\\mathrm{L}^2$- and $\\mathrm{H}^1$-stability properties of the closed-loop system for sufficiently large $\\nu>0$. The analytical conclusions from this work are supported and validated by numerical investigations."],"url":"http://arxiv.org/abs/2402.02776v1","category":"math.AP"}
{"created":"2024-02-05 07:14:18","title":"Accelerating Matroid Optimization through Fast Imprecise Oracles","abstract":"Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times. Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model. In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information. We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid. We design and analyze practical algorithms which only use few clean queries w.r.t. the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem. Notably, we prove that our algorithms are, in many respects, best-possible. Further, we outline extensions to other matroid oracle types, non-free dirty oracles and other matroid problems.","sentences":["Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times.","Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model.","In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information.","We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid.","We design and analyze practical algorithms which only use few clean queries w.r.t.","the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem.","Notably, we prove that our algorithms are, in many respects, best-possible.","Further, we outline extensions to other matroid oracle types, non-free dirty oracles and other matroid problems."],"url":"http://arxiv.org/abs/2402.02774v1","category":"cs.DS"}
{"created":"2024-02-05 07:05:22","title":"On existence of traveling wave of an HBV infection dynamics model: A novel approach","abstract":"In this work, a hepatitis B virus infection dynamics model is proposed including the spatial dependence of viruses. The existence of traveling waves for the proposed model is established through the application of the celebrated Gersgorin theorem. The procedure followed to establish the existence of a traveling wave solution is innovative and probably the first attempt of this particular approach. The elasticity of basic reproduction number with respect to some model parameters are also shown. Furthermore, the effects of spatial diffusivity of the viruses on infection are studied, and it is noticed that due to the diffusion, viruses spread rapidly throughout the liver.","sentences":["In this work, a hepatitis B virus infection dynamics model is proposed including the spatial dependence of viruses.","The existence of traveling waves for the proposed model is established through the application of the celebrated Gersgorin theorem.","The procedure followed to establish the existence of a traveling wave solution is innovative and probably the first attempt of this particular approach.","The elasticity of basic reproduction number with respect to some model parameters are also shown.","Furthermore, the effects of spatial diffusivity of the viruses on infection are studied, and it is noticed that due to the diffusion, viruses spread rapidly throughout the liver."],"url":"http://arxiv.org/abs/2402.02770v1","category":"math.DS"}
{"created":"2024-02-05 06:37:09","title":"Transmission Line Detection Based on Improved Hough Transform","abstract":"To address the challenges of low detection accuracy and high false positive rates of transmission lines in UAV (Unmanned Aerial Vehicle) images, we explore the linear features and spatial distribution. We introduce an enhanced stochastic Hough transform technique tailored for detecting transmission lines in complex backgrounds. By employing the Hessian matrix for initial preprocessing of transmission lines, and utilizing boundary search and pixel row segmentation, our approach distinguishes transmission line areas from the background. We significantly reduce both false positives and missed detections, thereby improving the accuracy of transmission line identification. Experiments demonstrate that our method not only processes images more rapidly, but also yields superior detection results compared to conventional and random Hough transform methods.","sentences":["To address the challenges of low detection accuracy and high false positive rates of transmission lines in UAV (Unmanned Aerial Vehicle) images, we explore the linear features and spatial distribution.","We introduce an enhanced stochastic Hough transform technique tailored for detecting transmission lines in complex backgrounds.","By employing the Hessian matrix for initial preprocessing of transmission lines, and utilizing boundary search and pixel row segmentation, our approach distinguishes transmission line areas from the background.","We significantly reduce both false positives and missed detections, thereby improving the accuracy of transmission line identification.","Experiments demonstrate that our method not only processes images more rapidly, but also yields superior detection results compared to conventional and random Hough transform methods."],"url":"http://arxiv.org/abs/2402.02761v1","category":"cs.CV"}
{"created":"2024-02-05 06:32:10","title":"Compound Poisson distributions for random dynamical systems using probabilistic approximations","abstract":"We obtain quenched hitting distributions to be compound Poissonian for a certain class of random dynamical systems. The theory is general and designed to accommodate non-uniformly expanding behavior and targets that do not overlap much with the region where uniformity breaks. Based on annealed and quenched polynomial decay of correlations, our quenched result adopts annealed Kac-type time-normalization and finds limits to be noise-independent. The technique involves a probabilistic blockapproximation where the quenched hit-counting function up to annealed Kac-normalized time is split into equally sized blocks which are mimicked by an independency of random variables distributed just like each of them. The theory is made operational due to a result that allows certain hitting quantities to be recovered from return quantities. Our application is to a class of random piecewise expanding one-dimensional systems, casting new light on the well-known deterministic dichotomy between periodic and aperiodic points, their usual extremal index formula EI=1-1/JT^p(x_0) , and recovering the PolyaAeppli case for general Bernoulli-driven systems, but distinct behavior otherwise. Future and on-going investigations aim to produce and accommodate examples of bonafide nonuniformly expanding random systems and targets approaching their neutral points.","sentences":["We obtain quenched hitting distributions to be compound Poissonian for a certain class of random dynamical systems.","The theory is general and designed to accommodate non-uniformly expanding behavior and targets that do not overlap much with the region where uniformity breaks.","Based on annealed and quenched polynomial decay of correlations, our quenched result adopts annealed Kac-type time-normalization and finds limits to be noise-independent.","The technique involves a probabilistic blockapproximation where the quenched hit-counting function up to annealed Kac-normalized time is split into equally sized blocks which are mimicked by an independency of random variables distributed just like each of them.","The theory is made operational due to a result that allows certain hitting quantities to be recovered from return quantities.","Our application is to a class of random piecewise expanding one-dimensional systems, casting new light on the well-known deterministic dichotomy between periodic and aperiodic points, their usual extremal index formula EI=1-1/JT^p(x_0) , and recovering the PolyaAeppli case for general Bernoulli-driven systems, but distinct behavior otherwise.","Future and on-going investigations aim to produce and accommodate examples of bonafide nonuniformly expanding random systems and targets approaching their neutral points."],"url":"http://arxiv.org/abs/2402.02759v1","category":"math.DS"}
{"created":"2024-02-05 18:59:31","title":"AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion","abstract":"Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering--based surface reconstruction methods. A website visualizing the results of our paper is located at this address: https://aoneus.github.io/","sentences":["Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring.","Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements.","In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging.","Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements.","By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines.","Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering--based surface reconstruction methods.","A website visualizing the results of our paper is located at this address: https://aoneus.github.io/"],"url":"http://arxiv.org/abs/2402.03309v1","category":"cs.CV"}
{"created":"2024-02-05 18:54:10","title":"Mg nanostructures with controlled dominant c-plane or m-plane facets by DC magnetron sputter deposition","abstract":"Magnesium nanostructures find increased use in applications for hydrogen storage, catalysis, waste treatment, and heat storage to name a few. Currently, most nanoparticles are made using a chemical synthesis approach, necessitating the use of organic solvents and yielding material covered in ligands. To apply these nanoparticles, one has to use them in paints or slurries for coating of surfaces, which again produces waste. In this communication we explore the possibilities of making magnesium nanostructures by a physical technique of magnetron sputtering and to control their crystallographic properties, i.e. the type of the dominating crystalline faces building up the external surface of the particle. We show that by applying different process parameters, it is possible to obtain dominating c-plane, mixed or dominating m-plane nanostructures. Since the surface-related adsorption processes are strongly related to the type of the crystalline plane, this report presents a clean, waste-free and large-scale approach to develop tailored nanostructured Mg coatings.","sentences":["Magnesium nanostructures find increased use in applications for hydrogen storage, catalysis, waste treatment, and heat storage to name a few.","Currently, most nanoparticles are made using a chemical synthesis approach, necessitating the use of organic solvents and yielding material covered in ligands.","To apply these nanoparticles, one has to use them in paints or slurries for coating of surfaces, which again produces waste.","In this communication we explore the possibilities of making magnesium nanostructures by a physical technique of magnetron sputtering and to control their crystallographic properties, i.e. the type of the dominating crystalline faces building up the external surface of the particle.","We show that by applying different process parameters, it is possible to obtain dominating c-plane, mixed or dominating m-plane nanostructures.","Since the surface-related adsorption processes are strongly related to the type of the crystalline plane, this report presents a clean, waste-free and large-scale approach to develop tailored nanostructured Mg coatings."],"url":"http://arxiv.org/abs/2402.03298v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 18:21:05","title":"The $B_{s}\\to \u03bc^{+}\u03bc^{-}\u03b3$ decay rate at large $q^{2}$ from lattice QCD","abstract":"We determine, by means of lattice QCD calculations, the local form factors describing the $B_{s}\\to \\mu^{+}\\mu^{-}\\gamma$ decay. For this analysis we make use of the gauge configurations produced by the ETM Collaboration with $N_{f}=2+1+1$ flavour of Wilson-Clover twisted-mass fermions at maximal twist. To obtain the $B_{s}$ meson form-factors, we perform simulations for several heavy-strange meson masses $m_{H_{s}}$ in the range $m_{H_{s}} \\in [ m_{D_{s}}, 2 m_{D_{s}} ]$, and extrapolate to the physical $B_{s}$ meson point $m_{B_{s}}\\simeq 5.367~{\\rm GeV}$ making use of the HQET scaling laws. We cover the region of large di-muon invariant masses $\\sqrt{q^{2}} > 4.16\\,{\\rm GeV}$, and use our results to determine the branching fraction for $B_{s}\\to \\mu^{+}\\mu^{-}\\gamma$, which has been recently measured by LHCb in the region $\\sqrt{q^{2}} > 4.9\\,{\\rm GeV}$. The largest contribution to the uncertainty in the partial branching fractions at values of $\\sqrt{q^{2}} < 4.8\\,{\\rm GeV}$ is now due to resonance and other long-distance effects, including those from \"charming penguins\", which we estimate by summing over the contributions from the $J_P=1^-$ charmonium resonances.","sentences":["We determine, by means of lattice QCD calculations, the local form factors describing the $B_{s}\\to \\mu^{+}\\mu^{-}\\gamma$ decay.","For this analysis we make use of the gauge configurations produced by the ETM Collaboration with $N_{f}=2+1+1$ flavour of Wilson-Clover twisted-mass fermions at maximal twist.","To obtain the $B_{s}$ meson form-factors, we perform simulations for several heavy-strange meson masses $m_{H_{s}}$ in the range $m_{H_{s}} \\in [ m_{D_{s}}, 2 m_{D_{s}} ]$, and extrapolate to the physical $B_{s}$ meson point $m_{B_{s}}\\simeq 5.367~{\\rm GeV}$","making use of the HQET scaling laws.","We cover the region of large di-muon invariant masses $\\sqrt{q^{2}} > 4.16\\,{\\rm GeV}$, and use our results to determine the branching fraction for $B_{s}\\to \\mu^{+}\\mu^{-}\\gamma$, which has been recently measured by LHCb in the region $\\sqrt{q^{2}} >","4.9\\,{\\rm GeV}$.","The largest contribution to the uncertainty in the partial branching fractions at values of $\\sqrt{q^{2}} < 4.8\\,{\\rm GeV}$ is now due to resonance and other long-distance effects, including those from \"charming penguins\", which we estimate by summing over the contributions from the $J_P=1^-$ charmonium resonances."],"url":"http://arxiv.org/abs/2402.03262v1","category":"hep-lat"}
{"created":"2024-02-05 18:14:28","title":"Learning Best-in-Class Policies for the Predict-then-Optimize Framework","abstract":"We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware learning.","sentences":["We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework.","These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods.","Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows.","This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings.","This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric.","Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware learning."],"url":"http://arxiv.org/abs/2402.03256v1","category":"cs.LG"}
{"created":"2024-02-05 17:01:15","title":"Lightweight Masking Against Static Power Side-Channel Attacks","abstract":"This paper presents a novel defense strategy against static power side-channel attacks (PSCAs), a critical threat to cryptographic security. Our method is based on (1) carefully tuning high-Vth versus low-Vth cell selection during synthesis, accounting for both security and timing impact, and (2), at runtime, randomly switching the operation between these cells. This approach serves to significantly obscure static power patterns, which are at the heart of static PSCAs. Our experimental results on a commercial 28nm node show a drastic increase in the effort required for a successful attack, namely up to 96 times more traces. When compared to prior countermeasures, ours incurs little cost, making it a lightweight defense.","sentences":["This paper presents a novel defense strategy against static power side-channel attacks (PSCAs), a critical threat to cryptographic security.","Our method is based on (1) carefully tuning high-Vth versus low-Vth cell selection during synthesis, accounting for both security and timing impact, and (2), at runtime, randomly switching the operation between these cells.","This approach serves to significantly obscure static power patterns, which are at the heart of static PSCAs.","Our experimental results on a commercial 28nm node show a drastic increase in the effort required for a successful attack, namely up to 96 times more traces.","When compared to prior countermeasures, ours incurs little cost, making it a lightweight defense."],"url":"http://arxiv.org/abs/2402.03196v1","category":"cs.CR"}
{"created":"2024-02-05 16:50:39","title":"Charting The Evolution of Solidity Error Handling","abstract":"The usage of error handling in Solidity smart contracts is vital because smart contracts perform transactions that should be verified. Transactions that are not carefully handled, may lead to program crashes and vulnerabilities, implying financial loss and legal consequences. While Solidity designers attempt to constantly update the language with new features, including error-handling (EH) features, it is necessary for developers to promptly absorb how to use them. We conduct a large-scale empirical study on 283K unique open-source smart contracts to identify patterns regarding the usage of Solidity EH features over time. Overall, the usage of most EH features is limited. However, we observe an upward trend (> 60%) in the usage of a Solidity-tailored EH feature, i.e., require. This indicates that designers of modern programming languages may consider making error handling more tailored to the purposes of each language. Our analysis on 102 versions of the Solidity documentation indicates the volatile nature of Solidity, as the language changes frequently, i.e., there are changes on EH features once or twice a year. Such frequent releases may confuse smart contract developers, discouraging them to carefully read the Solidity documentation, and correctly adopt EH features. Furthermore, our findings reveal that nearly 70% of the examined smart contracts are exposed to potential failures due to missing error handing, e.g., unchecked external calls. Therefore, the use of EH features should be further supported via a more informative documentation containing (1) representative and meaningful examples and (2) details about the impact of potential EH misuses.","sentences":["The usage of error handling in Solidity smart contracts is vital because smart contracts perform transactions that should be verified.","Transactions that are not carefully handled, may lead to program crashes and vulnerabilities, implying financial loss and legal consequences.","While Solidity designers attempt to constantly update the language with new features, including error-handling (EH) features, it is necessary for developers to promptly absorb how to use them.","We conduct a large-scale empirical study on 283K unique open-source smart contracts to identify patterns regarding the usage of Solidity EH features over time.","Overall, the usage of most EH features is limited.","However, we observe an upward trend (> 60%) in the usage of a Solidity-tailored EH feature, i.e., require.","This indicates that designers of modern programming languages may consider making error handling more tailored to the purposes of each language.","Our analysis on 102 versions of the Solidity documentation indicates the volatile nature of Solidity, as the language changes frequently, i.e., there are changes on EH features once or twice a year.","Such frequent releases may confuse smart contract developers, discouraging them to carefully read the Solidity documentation, and correctly adopt EH features.","Furthermore, our findings reveal that nearly 70% of the examined smart contracts are exposed to potential failures due to missing error handing, e.g., unchecked external calls.","Therefore, the use of EH features should be further supported via a more informative documentation containing (1) representative and meaningful examples and (2) details about the impact of potential EH misuses."],"url":"http://arxiv.org/abs/2402.03186v1","category":"cs.SE"}
{"created":"2024-02-05 16:15:11","title":"Proof Theory and Decision Procedures for Deontic STIT Logics","abstract":"This paper addresses the automation of reasoning with deontic STIT logics by means of proof theory. Our methodology consists of leveraging sound and cut-free complete sequent-style calculi to write a proof-search algorithm deciding deontic, multi-agent STIT logics with (un)limited choice. In order to ensure the termination of our proof-search algorithm, we introduce a special loop-checking mechanism. Despite the acknowledged potential for deontic reasoning in the context of autonomous vehicles and other areas of AI, this work is the first to provide a syntactic decision procedure for deontic STIT logics. Our proof-search procedures are designed to provide verifiable witnesses/certificates of the (in)validity of formulae, which permit an analysis of the (non)theoremhood of formulae and act as explanations thereof. We utilize our proof-search algorithm to address agent-based normative reasoning tasks such as compliance checking.","sentences":["This paper addresses the automation of reasoning with deontic STIT logics by means of proof theory.","Our methodology consists of leveraging sound and cut-free complete sequent-style calculi to write a proof-search algorithm deciding deontic, multi-agent STIT logics with (un)limited choice.","In order to ensure the termination of our proof-search algorithm, we introduce a special loop-checking mechanism.","Despite the acknowledged potential for deontic reasoning in the context of autonomous vehicles and other areas of AI, this work is the first to provide a syntactic decision procedure for deontic STIT logics.","Our proof-search procedures are designed to provide verifiable witnesses/certificates of the (in)validity of formulae, which permit an analysis of the (non)theoremhood of formulae and act as explanations thereof.","We utilize our proof-search algorithm to address agent-based normative reasoning tasks such as compliance checking."],"url":"http://arxiv.org/abs/2402.03148v1","category":"cs.LO"}
{"created":"2024-02-05 15:51:49","title":"How Free is Parameter-Free Stochastic Optimization?","abstract":"We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound.","sentences":["We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters.","Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc.","In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms.","We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions.","Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound."],"url":"http://arxiv.org/abs/2402.03126v1","category":"cs.LG"}
{"created":"2024-02-05 15:51:38","title":"Shape Manipulation of Bevel-Tip Needles for Prostate Biopsy Procedures: A Comparison of Two Resolved-Rate Controllers","abstract":"Prostate cancer diagnosis continues to encounter challenges, often due to imprecise needle placement in standard biopsies. Several control strategies have been developed to compensate for needle tip prediction inaccuracies, however none were compared against each other, and it is unclear whether any of them can be safely and universally applied in clinical settings. This paper compares the performance of two resolved-rate controllers, derived from a mechanics-based and a data-driven approach, for bevel-tip needle control using needle shape manipulation through a template. We demonstrate for a simulated 12-core biopsy procedure under model parameter uncertainty that the mechanics-based controller can better reach desired targets when only the final goal configuration is presented even with uncertainty on model parameters estimation, and that providing a feasible needle path is crucial in ensuring safe surgical outcomes when either controller is used for needle shape manipulation.","sentences":["Prostate cancer diagnosis continues to encounter challenges, often due to imprecise needle placement in standard biopsies.","Several control strategies have been developed to compensate for needle tip prediction inaccuracies, however none were compared against each other, and it is unclear whether any of them can be safely and universally applied in clinical settings.","This paper compares the performance of two resolved-rate controllers, derived from a mechanics-based and a data-driven approach, for bevel-tip needle control using needle shape manipulation through a template.","We demonstrate for a simulated 12-core biopsy procedure under model parameter uncertainty that the mechanics-based controller can better reach desired targets when only the final goal configuration is presented even with uncertainty on model parameters estimation, and that providing a feasible needle path is crucial in ensuring safe surgical outcomes when either controller is used for needle shape manipulation."],"url":"http://arxiv.org/abs/2402.03125v1","category":"cs.RO"}
{"created":"2024-02-05 15:51:34","title":"Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks","abstract":"Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction. We believe soft labels in classification tasks are worth further attention in gradient inversion attacks.","sentences":["Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework.","Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints.","Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels.","In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process.","In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods.","Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction.","We believe soft labels in classification tasks are worth further attention in gradient inversion attacks."],"url":"http://arxiv.org/abs/2402.03124v1","category":"cs.CR"}
{"created":"2024-02-05 15:47:54","title":"Integrating Random Regret Minimization-Based Discrete Choice Models with Mixed Integer Linear Programming for Revenue Optimization","abstract":"This paper explores the critical domain of Revenue Management (RM) within Operations Research (OR), focusing on intricate pricing dynamics. Utilizing Mixed Integer Linear Programming (MILP) models, the study enhances revenue optimization by considering product prices as decision variables and emphasizing the interplay between demand and supply. Recent advancements in Discrete Choice Models (DCMs), particularly those rooted in Random Regret Minimization (RRM) theory, are investigated as potent alternatives to established Random Utility Maximization (RUM) based DCMs. Despite the widespread use of DCMs in RM, a significant gap exists between cutting-edge RRM-based models and their practical integration into RM strategies. The study addresses this gap by incorporating an advanced RRM-based DCM into MILP models, addressing pricing challenges in both capacitated and uncapacitated supply scenarios. The developed models demonstrate feasibility and offer diverse interpretations of consumer choice behavior, drawing inspiration from established RUM-based frameworks. This research contributes to bridging the existing gap in the application of advanced DCMs within practical RM implementations.","sentences":["This paper explores the critical domain of Revenue Management (RM) within Operations Research (OR), focusing on intricate pricing dynamics.","Utilizing Mixed Integer Linear Programming (MILP) models, the study enhances revenue optimization by considering product prices as decision variables and emphasizing the interplay between demand and supply.","Recent advancements in Discrete Choice Models (DCMs), particularly those rooted in Random Regret Minimization (RRM) theory, are investigated as potent alternatives to established Random Utility Maximization (RUM) based DCMs.","Despite the widespread use of DCMs in RM, a significant gap exists between cutting-edge RRM-based models and their practical integration into RM strategies.","The study addresses this gap by incorporating an advanced RRM-based DCM into MILP models, addressing pricing challenges in both capacitated and uncapacitated supply scenarios.","The developed models demonstrate feasibility and offer diverse interpretations of consumer choice behavior, drawing inspiration from established RUM-based frameworks.","This research contributes to bridging the existing gap in the application of advanced DCMs within practical RM implementations."],"url":"http://arxiv.org/abs/2402.03118v1","category":"math.OC"}
{"created":"2024-02-05 15:45:59","title":"Feature-Action Design Patterns for Storytelling Visualizations with Time Series Data","abstract":"We present a method to create storytelling visualization with time series data. Many personal decisions nowadays rely on access to dynamic data regularly, as we have seen during the COVID-19 pandemic. It is thus desirable to construct storytelling visualization for dynamic data that is selected by an individual for a specific context. Because of the need to tell data-dependent stories, predefined storyboards based on known data cannot accommodate dynamic data easily nor scale up to many different individuals and contexts. Motivated initially by the need to communicate time series data during the COVID-19 pandemic, we developed a novel computer-assisted method for meta-authoring of stories, which enables the design of storyboards that include feature-action patterns in anticipation of potential features that may appear in dynamically arrived or selected data. In addition to meta-storyboards involving COVID-19 data, we also present storyboards for telling stories about progress in a machine learning workflow. Our approach is complementary to traditional methods for authoring storytelling visualization, and provides an efficient means to construct data-dependent storyboards for different data-streams of similar contexts.","sentences":["We present a method to create storytelling visualization with time series data.","Many personal decisions nowadays rely on access to dynamic data regularly, as we have seen during the COVID-19 pandemic.","It is thus desirable to construct storytelling visualization for dynamic data that is selected by an individual for a specific context.","Because of the need to tell data-dependent stories, predefined storyboards based on known data cannot accommodate dynamic data easily nor scale up to many different individuals and contexts.","Motivated initially by the need to communicate time series data during the COVID-19 pandemic, we developed a novel computer-assisted method for meta-authoring of stories, which enables the design of storyboards that include feature-action patterns in anticipation of potential features that may appear in dynamically arrived or selected data.","In addition to meta-storyboards involving COVID-19 data, we also present storyboards for telling stories about progress in a machine learning workflow.","Our approach is complementary to traditional methods for authoring storytelling visualization, and provides an efficient means to construct data-dependent storyboards for different data-streams of similar contexts."],"url":"http://arxiv.org/abs/2402.03116v1","category":"cs.HC"}
{"created":"2024-02-05 15:44:49","title":"Optimal sampling for stochastic and natural gradient descent","abstract":"We consider the problem of optimising the expected value of a loss functional over a nonlinear model class of functions, assuming that we have only access to realisations of the gradient of the loss. This is a classical task in statistics, machine learning and physics-informed machine learning. A straightforward solution is to replace the exact objective with a Monte Carlo estimate before employing standard first-order methods like gradient descent, which yields the classical stochastic gradient descent method. But replacing the true objective with an estimate ensues a ``generalisation error''. Rigorous bounds for this error typically require strong compactness and Lipschitz continuity assumptions while providing a very slow decay with sample size. We propose a different optimisation strategy relying on a natural gradient descent in which the true gradient is approximated in local linearisations of the model class via (quasi-)projections based on optimal sampling methods. Under classical assumptions on the loss and the nonlinear model class, we prove that this scheme converges almost surely monotonically to a stationary point of the true objective and we provide convergence rates.","sentences":["We consider the problem of optimising the expected value of a loss functional over a nonlinear model class of functions, assuming that we have only access to realisations of the gradient of the loss.","This is a classical task in statistics, machine learning and physics-informed machine learning.","A straightforward solution is to replace the exact objective with a Monte Carlo estimate before employing standard first-order methods like gradient descent, which yields the classical stochastic gradient descent method.","But replacing the true objective with an estimate ensues a ``generalisation error''.","Rigorous bounds for this error typically require strong compactness and Lipschitz continuity assumptions while providing a very slow decay with sample size.","We propose a different optimisation strategy relying on a natural gradient descent in which the true gradient is approximated in local linearisations of the model class via (quasi-)projections based on optimal sampling methods.","Under classical assumptions on the loss and the nonlinear model class, we prove that this scheme converges almost surely monotonically to a stationary point of the true objective and we provide convergence rates."],"url":"http://arxiv.org/abs/2402.03113v1","category":"math.OC"}
{"created":"2024-02-05 15:29:04","title":"Inter-city infections and the role of size heterogeneity in containment strategies","abstract":"Our goal is to compare the efficiency of strategies applied on a regional scale to control the spread of a pathogen between urban units, in the following called cities. We model the infection dynamics on a random network where each node represents a city. We take the size of a city as a parameter for its connectivity to other cities. Once a certain threshold number of citizens in a city gets infected the city is isolated to prevent further spread. We consider two different strategies to define the threshold depending on the city size. In strategy $(P)$ a proportion $p$ of citizens need to get infected before isolation, while in strategy $(U)$ an uniform threshold $L$ is chosen for all cities. If city sizes were constant both strategies would coincide. However, in reality city size distributions are heavy-tailed. In this case in strategy $(P)$ the infection of a large city can lead to a much larger number of secondary infections of cities than in strategy $(U)$. To quantify the extend to which these infections fuel the epidemic we benchmark the two strategies against each other by comparing the number of individuals that get isolated when the number of individuals that get infected is under both strategies the same. Furthermore, we define and determine a basic reproduction numbers $R^{(U)}_0$ and $R^{(P)}$ for variant (P) and (U) that represent the initial strengths at which the infection spreads between cities. We fit our model to mobility data from France, Japan and Poland to identify the parameter regimes of our model that are empirically relevant. Furthermore, we investigate by means of simulations to which extend the geographic structure of the city distribution (that we neglect in our model) influences the infection dynamics between cities.","sentences":["Our goal is to compare the efficiency of strategies applied on a regional scale to control the spread of a pathogen between urban units, in the following called cities.","We model the infection dynamics on a random network where each node represents a city.","We take the size of a city as a parameter for its connectivity to other cities.","Once a certain threshold number of citizens in a city gets infected the city is isolated to prevent further spread.","We consider two different strategies to define the threshold depending on the city size.","In strategy $(P)$ a proportion $p$ of citizens need to get infected before isolation, while in strategy $(U)$ an uniform threshold $L$ is chosen for all cities.","If city sizes were constant both strategies would coincide.","However, in reality city size distributions are heavy-tailed.","In this case in strategy $(P)$ the infection of a large city can lead to a much larger number of secondary infections of cities than in strategy $(U)$. To quantify the extend to which these infections fuel the epidemic we benchmark the two strategies against each other by comparing the number of individuals that get isolated when the number of individuals that get infected is under both strategies the same.","Furthermore, we define and determine a basic reproduction numbers $R^{(U)}_0$ and $R^{(P)}$ for variant (P) and (U) that represent the initial strengths at which the infection spreads between cities.","We fit our model to mobility data from France, Japan and Poland to identify the parameter regimes of our model that are empirically relevant.","Furthermore, we investigate by means of simulations to which extend the geographic structure of the city distribution (that we neglect in our model) influences the infection dynamics between cities."],"url":"http://arxiv.org/abs/2402.03100v1","category":"physics.soc-ph"}
{"created":"2024-02-05 15:10:42","title":"The three dimensional magneto-hydrostatic equations with Grad-Rubin boundary value","abstract":"In this work, we study the well-posedness of the three dimensional magneto-hydrostatic equation under Grad-Rubin boundary value conditions. The proof relies on a fixed point argument to construct solutions to an elliptic-hyperbolic problem in a perturbative regime by means of pseudo-differential operators with symbols with limited regularity in H\\\"older spaces. As a byproduct, the employed technique in this work is more flexible and simplifies the arguments of the proof for the previous two-dimensional setting.","sentences":["In this work, we study the well-posedness of the three dimensional magneto-hydrostatic equation under Grad-Rubin boundary value conditions.","The proof relies on a fixed point argument to construct solutions to an elliptic-hyperbolic problem in a perturbative regime by means of pseudo-differential operators with symbols with limited regularity in H\\\"older spaces.","As a byproduct, the employed technique in this work is more flexible and simplifies the arguments of the proof for the previous two-dimensional setting."],"url":"http://arxiv.org/abs/2402.03078v1","category":"math.AP"}
{"created":"2024-02-05 14:42:45","title":"Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty","abstract":"We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement learning. We report empirical evaluations demonstrating PAC's enhanced stability and improved performance over the state of the art in diverse continuous control problems.","sentences":["We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off.","PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training.","The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis.","This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process.","PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art.","The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement learning.","We report empirical evaluations demonstrating PAC's enhanced stability and improved performance over the state of the art in diverse continuous control problems."],"url":"http://arxiv.org/abs/2402.03055v1","category":"cs.LG"}
{"created":"2024-02-05 14:13:10","title":"Rejection-Sampled Universal Quantization for Smaller Quantization Errors","abstract":"We construct a randomized vector quantizer which has a smaller maximum error compared to all known lattice quantizers with the same entropy for dimensions 5, 6, ..., 48, and also has a smaller mean squared error compared to known lattice quantizers with the same entropy for dimensions 35, ..., 48, in the high resolution limit. Moreover, our randomized quantizer has a desirable property that the quantization error is always uniform over the ball and independent of the input. Our construction is based on applying rejection sampling on universal quantization, which allows us to shape the error distribution to be any continuous distribution, not only uniform distributions over basic cells of a lattice as in conventional dithered quantization. We also characterize the high SNR limit of one-shot channel simulation for any additive noise channel under a mild assumption (e.g., the AWGN channel), up to an additive constant of 1.45 bits.","sentences":["We construct a randomized vector quantizer which has a smaller maximum error compared to all known lattice quantizers with the same entropy for dimensions 5, 6, ..., 48, and also has a smaller mean squared error compared to known lattice quantizers with the same entropy for dimensions 35, ..., 48, in the high resolution limit.","Moreover, our randomized quantizer has a desirable property that the quantization error is always uniform over the ball and independent of the input.","Our construction is based on applying rejection sampling on universal quantization, which allows us to shape the error distribution to be any continuous distribution, not only uniform distributions over basic cells of a lattice as in conventional dithered quantization.","We also characterize the high SNR limit of one-shot channel simulation for any additive noise channel under a mild assumption (e.g., the AWGN channel), up to an additive constant of 1.45 bits."],"url":"http://arxiv.org/abs/2402.03030v1","category":"cs.IT"}
{"created":"2024-02-05 14:00:53","title":"Data-induced multiscale losses and efficient multirate gradient descent schemes","abstract":"This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions. This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data. Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages.","sentences":["This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning.","A dataset is multiscale if its distribution shows large variations in scale across different directions.","This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data.","Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing.","This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages."],"url":"http://arxiv.org/abs/2402.03021v1","category":"cs.LG"}
{"created":"2024-02-05 13:54:34","title":"Open-separating dominating codes in graphs","abstract":"Using dominating sets to separate vertices of graphs is a well-studied problem in the larger domain of identification problems. In such problems, the objective is typically to separate any two vertices of a graph by their unique neighbourhoods in a suitably chosen dominating set of the graph. Such a dominating and separating set is often referred to as a \\emph{code} in the literature. Depending on the types of dominating and separating sets used, various problems arise under various names in the literature. In this paper, we introduce a new problem in the same realm of identification problems whereby the code, called the \\emph{open-separating dominating code}, or the \\emph{OSD-code} for short, is a dominating set and uses open neighbourhoods for separating vertices. The paper studies the fundamental properties concerning the existence, hardness and minimality of OSD-codes. Due to the emergence of a close and yet difficult to establish relation of the OSD-codes with another well-studied code in the literature called the open locating dominating codes, or OLD-codes for short, we compare the two on various graph classes. Finally, we also provide an equivalent reformulation of the problem of finding OSD-codes of a graph as a covering problem in a suitable hypergraph and discuss the polyhedra associated with OSD-codes, again in relation to OLD-codes of some graph classes already studied in this context.","sentences":["Using dominating sets to separate vertices of graphs is a well-studied problem in the larger domain of identification problems.","In such problems, the objective is typically to separate any two vertices of a graph by their unique neighbourhoods in a suitably chosen dominating set of the graph.","Such a dominating and separating set is often referred to as a \\emph{code} in the literature.","Depending on the types of dominating and separating sets used, various problems arise under various names in the literature.","In this paper, we introduce a new problem in the same realm of identification problems whereby the code, called the \\emph{open-separating dominating code}, or the \\emph{OSD-code} for short, is a dominating set and uses open neighbourhoods for separating vertices.","The paper studies the fundamental properties concerning the existence, hardness and minimality of OSD-codes.","Due to the emergence of a close and yet difficult to establish relation of the OSD-codes with another well-studied code in the literature called the open locating dominating codes, or OLD-codes for short, we compare the two on various graph classes.","Finally, we also provide an equivalent reformulation of the problem of finding OSD-codes of a graph as a covering problem in a suitable hypergraph and discuss the polyhedra associated with OSD-codes, again in relation to OLD-codes of some graph classes already studied in this context."],"url":"http://arxiv.org/abs/2402.03015v1","category":"math.CO"}
{"created":"2024-02-05 13:52:04","title":"On the uniqueness of maximal solvable extensions of nilpotent Lie algebras","abstract":"In the present paper we prove that under certain conditions on finite-dimensional nilpotent Lie algebra there exists a unique (up to isomorphism) maximal solvable extension of the nilpotent Lie algebra. This extension is isomorphic to the semidirect sum of the nilpotent Lie algebra and its a maximal torus. The comparisons with some existing results are given, as well. In addition, the criteria of completeness of solvable Lie algebras obtained in terms of its nilradical. In particular, it is established that a solvable Lie algebra is complete if and only if it is maximal solvable extension of an nilpotent Lie algebra that satisfies the mentioned conditions.","sentences":["In the present paper we prove that under certain conditions on finite-dimensional nilpotent Lie algebra there exists a unique (up to isomorphism) maximal solvable extension of the nilpotent Lie algebra.","This extension is isomorphic to the semidirect sum of the nilpotent Lie algebra and its a maximal torus.","The comparisons with some existing results are given, as well.","In addition, the criteria of completeness of solvable Lie algebras obtained in terms of its nilradical.","In particular, it is established that a solvable Lie algebra is complete if and only if it is maximal solvable extension of an nilpotent Lie algebra that satisfies the mentioned conditions."],"url":"http://arxiv.org/abs/2402.03012v1","category":"math.RA"}
{"created":"2024-02-05 13:46:22","title":"Excitonic thermalization bottleneck in twisted TMD heterostructures","abstract":"Twisted van der Waals heterostructures show an intriguing interface exciton physics including hybridization effects and emergence of moir\\'e potentials. Recent experiments have revealed that moir\\'e-trapped excitons exhibit a remarkable dynamics, where excited states show lifetimes that are several orders of magnitude longer than those in monolayers. The origin of this behaviour is still under debate. Based on a microscopic many-particle approach, we investigate the phonon-driven relaxation cascade of non-equilibrium moir\\'e excitons in the exemplary MoSe$_2$-WSe$_2$ heterostructure. We track the exciton relaxation pathway across different moir\\'e mini-bands and identify the phonon-scattering channels assisting the spatial redistribution of excitons into low-energy pockets of the moir\\'e potential. We unravel a phonon bottleneck in the flat band structure at low twist angles preventing excitons to fully thermalize into the lowest state explaining the measured enhanced emission intensity of excited moir\\'e excitons. Overall, our work provides important insights into exciton relaxation dynamics in flatband exciton materials.","sentences":["Twisted van der Waals heterostructures show an intriguing interface exciton physics including hybridization effects and emergence of moir\\'e potentials.","Recent experiments have revealed that moir\\'e-trapped excitons exhibit a remarkable dynamics, where excited states show lifetimes that are several orders of magnitude longer than those in monolayers.","The origin of this behaviour is still under debate.","Based on a microscopic many-particle approach, we investigate the phonon-driven relaxation cascade of non-equilibrium moir\\'e excitons in the exemplary MoSe$_2$-WSe$_2$ heterostructure.","We track the exciton relaxation pathway across different moir\\'e mini-bands and identify the phonon-scattering channels assisting the spatial redistribution of excitons into low-energy pockets of the moir\\'e potential.","We unravel a phonon bottleneck in the flat band structure at low twist angles preventing excitons to fully thermalize into the lowest state explaining the measured enhanced emission intensity of excited moir\\'e excitons.","Overall, our work provides important insights into exciton relaxation dynamics in flatband exciton materials."],"url":"http://arxiv.org/abs/2402.03007v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 13:45:37","title":"Construction and evaluation of optimal diagnostic tests with application to hepatocellular carcinoma diagnosis","abstract":"Accurate diagnostic tests are crucial to ensure effective treatment, screening, and surveillance of diseases. However, the limited accuracy of individual biomarkers often hinders comprehensive screening. The heterogeneity of many diseases, particularly cancer, calls for the use of several biomarkers together into a composite diagnostic test. In this paper, we present a novel multivariate model that optimally combines multiple biomarkers using the likelihood ratio function. The model's parameters directly translate into computationally simple diagnostic accuracy measures. Additionally, our method allows for reliable predictions even in scenarios where specific biomarker measurements are unavailable and can guide the selection of biomarker combinations under resource constraints. We conduct simulation studies to compare the performance to popular classification and discriminant analysis methods. We utilize the approach to construct an optimal diagnostic test for hepatocellular carcinoma, a cancer type known for the absence of a single ideal marker. An accompanying R implementation is made available for reproducing all results.","sentences":["Accurate diagnostic tests are crucial to ensure effective treatment, screening, and surveillance of diseases.","However, the limited accuracy of individual biomarkers often hinders comprehensive screening.","The heterogeneity of many diseases, particularly cancer, calls for the use of several biomarkers together into a composite diagnostic test.","In this paper, we present a novel multivariate model that optimally combines multiple biomarkers using the likelihood ratio function.","The model's parameters directly translate into computationally simple diagnostic accuracy measures.","Additionally, our method allows for reliable predictions even in scenarios where specific biomarker measurements are unavailable and can guide the selection of biomarker combinations under resource constraints.","We conduct simulation studies to compare the performance to popular classification and discriminant analysis methods.","We utilize the approach to construct an optimal diagnostic test for hepatocellular carcinoma, a cancer type known for the absence of a single ideal marker.","An accompanying R implementation is made available for reproducing all results."],"url":"http://arxiv.org/abs/2402.03004v1","category":"stat.ME"}
{"created":"2024-02-05 13:41:22","title":"[Citation needed] Data usage and citation practices in medical imaging conferences","abstract":"Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage. In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \\url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.","sentences":["Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used.","As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage.","In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \\url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets.","We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL.","We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned.","Our findings demonstrate the concentration of the usage of a limited set of datasets.","We also highlight different citing practices, making the automation of tracking difficult."],"url":"http://arxiv.org/abs/2402.03003v1","category":"cs.CV"}
{"created":"2024-02-05 11:57:23","title":"Nuclear mass table in deformed relativistic Hartree-Bogoliubov theory in continuum, II: Even-$Z$ nuclei","abstract":"The mass table in the deformed relativistic Hartree-Bogoliubov theory in continuum (DRHBc) with the PC-PK1 density functional has been established for even-$Z$ nuclei with $8\\le Z\\le120$, extended from the previous work for even-even nuclei [Zhang $\\it{et~al.}$ (DRHBc Mass Table Collaboration), At. Data Nucl. Data Tables 144, 101488 (2022)]. The calculated binding energies, two-nucleon and one-neutron separation energies, root-mean-square (rms) radii of neutron, proton, matter, and charge distributions, quadrupole deformations, and neutron and proton Fermi surfaces are tabulated and compared with available experimental data. A total of 4829 even-$Z$ nuclei are predicted to be bound, with an rms deviation of 1.477 MeV from the 1244 mass data. Good agreement with the available experimental odd-even mass differences, $\\alpha$ decay energies, and charge radii is also achieved. The description accuracy for nuclear masses and nucleon separation energies as well as the prediction for drip lines is compared with the results obtained from other relativistic and nonrelativistic density functional. The comparison shows that the DRHBc theory with PC-PK1 provides an excellent microscopic description for the masses of even-$Z$ nuclei. The systematics of the nucleon separation energies, odd-even mass differences, pairing energies, two-nucleon gaps, $\\alpha$ decay energies, rms radii, quadrupole deformations, potential energy curves, neutron density distributions, and neutron mean-field potentials are discussed.","sentences":["The mass table in the deformed relativistic Hartree-Bogoliubov theory in continuum (DRHBc) with the PC-PK1 density functional has been established for even-$Z$ nuclei with $8\\le Z\\le120$, extended from the previous work for even-even nuclei","[Zhang $\\it{et~al.}$ (DRHBc Mass Table Collaboration), At.","Data Nucl.","Data Tables 144, 101488 (2022)].","The calculated binding energies, two-nucleon and one-neutron separation energies, root-mean-square (rms) radii of neutron, proton, matter, and charge distributions, quadrupole deformations, and neutron and proton Fermi surfaces are tabulated and compared with available experimental data.","A total of 4829 even-$Z$ nuclei are predicted to be bound, with an rms deviation of 1.477 MeV from the 1244 mass data.","Good agreement with the available experimental odd-even mass differences, $\\alpha$ decay energies, and charge radii is also achieved.","The description accuracy for nuclear masses and nucleon separation energies as well as the prediction for drip lines is compared with the results obtained from other relativistic and nonrelativistic density functional.","The comparison shows that the DRHBc theory with PC-PK1 provides an excellent microscopic description for the masses of even-$Z$ nuclei.","The systematics of the nucleon separation energies, odd-even mass differences, pairing energies, two-nucleon gaps, $\\alpha$ decay energies, rms radii, quadrupole deformations, potential energy curves, neutron density distributions, and neutron mean-field potentials are discussed."],"url":"http://arxiv.org/abs/2402.02935v1","category":"nucl-th"}
{"created":"2024-02-05 11:55:50","title":"InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks","abstract":"Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks. We demonstrate variations of the InterpretCC architecture for text and tabular data across several real-world benchmarks: six online education courses, news classification, breast cancer diagnosis, and review sentiment.","sentences":["Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees).","These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions.","In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction.","We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks.","We demonstrate variations of the InterpretCC architecture for text and tabular data across several real-world benchmarks: six online education courses, news classification, breast cancer diagnosis, and review sentiment."],"url":"http://arxiv.org/abs/2402.02933v1","category":"cs.LG"}
{"created":"2024-02-05 11:50:15","title":"Quasi-Frobenius algebras in finite tensor categories","abstract":"We introduce the notion of a quasi-Frobenius algebra in a finite tensor category $\\mathcal{C}$ and give equivalent conditions for an algebra in $\\mathcal{C}$ to be quasi-Frobenius. A quasi-Frobenius algebra in $\\mathcal{C}$ is not necessarily Frobenius, however, we show that an algebra $A$ in $\\mathcal{C}$ is quasi-Frobenius if and only if $A$ is Morita equivalent to a Frobenius algebra in $\\mathcal{C}$. We also show that the class of symmetric Frobenius algebras in $\\mathcal{C}$ is closed under the Morita equivalence provided that $\\mathcal{C}$ is pivotal so that the symmetricity makes sense.","sentences":["We introduce the notion of a quasi-Frobenius algebra in a finite tensor category $\\mathcal{C}$ and give equivalent conditions for an algebra in $\\mathcal{C}$ to be quasi-Frobenius.","A quasi-Frobenius algebra in $\\mathcal{C}$ is not necessarily Frobenius, however, we show that an algebra $A$ in $\\mathcal{C}$ is quasi-Frobenius if and only if $A$ is Morita equivalent to a Frobenius algebra in $\\mathcal{C}$. We also show that the class of symmetric Frobenius algebras in $\\mathcal{C}$ is closed under the Morita equivalence provided that $\\mathcal{C}$ is pivotal so that the symmetricity makes sense."],"url":"http://arxiv.org/abs/2402.02929v1","category":"math.QA"}
{"created":"2024-02-05 11:47:36","title":"Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer","abstract":"Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture with link prediction head saves much computation time while simultaneously yielding superior performance.","sentences":["Identification of cognates across related languages is one of the primary problems in historical linguistics.","Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc.","Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters.","In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection.","Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information.","We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture with link prediction head saves much computation time while simultaneously yielding superior performance."],"url":"http://arxiv.org/abs/2402.02926v1","category":"cs.CL"}
{"created":"2024-02-05 11:03:01","title":"Noisy group testing via spatial coupling","abstract":"We study the problem of identifying a small set $k\\sim n^\\theta$, $0<\\theta<1$, of infected individuals within a large population of size $n$ by testing groups of individuals simultaneously. All tests are conducted concurrently. The goal is to minimise the total number of tests required. In this paper we make the (realistic) assumption that tests are noisy, i.e.\\ that a group that contains an infected individual may return a negative test result or one that does not contain an infected individual may return a positive test results with a certain probability. The noise need not be symmetric. We develop an algorithm called SPARC that correctly identifies the set of infected individuals up to $o(k)$ errors with high probability with the asymptotically minimum number of tests. Additionally, we develop an algorithm called SPEX that exactly identifies the set of infected individuals w.h.p. with a number of tests that matches the information-theoretic lower bound for the constant column design, a powerful and well-studied test design.","sentences":["We study the problem of identifying a small set $k\\sim n^\\theta$, $0<\\theta<1$, of infected individuals within a large population of size $n$ by testing groups of individuals simultaneously.","All tests are conducted concurrently.","The goal is to minimise the total number of tests required.","In this paper we make the (realistic) assumption that tests are noisy, i.e.\\ that a group that contains an infected individual may return a negative test result or one that does not contain an infected individual may return a positive test results with a certain probability.","The noise need not be symmetric.","We develop an algorithm called SPARC that correctly identifies the set of infected individuals up to $o(k)$ errors with high probability with the asymptotically minimum number of tests.","Additionally, we develop an algorithm called SPEX that exactly identifies the set of infected individuals w.h.p.","with a number of tests that matches the information-theoretic lower bound for the constant column design, a powerful and well-studied test design."],"url":"http://arxiv.org/abs/2402.02895v1","category":"cs.DM"}
{"created":"2024-02-05 10:39:32","title":"How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning","abstract":"We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of in-context learning. Our code will be released on github.","sentences":["We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method.","In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token.","In deep layers, in-context heads make great contributions.","In each in-context head, the value-output matrix extracts the labels' features.","Query and key matrices compute the attention weights between the input text and each demonstration.","The larger the attention weight is, the more label information is transferred into the last token for predicting the next word.","Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration.","Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions.","We conduct experiments on GPT2 large, Llama 7B, 13B and 30B.","The results can support our analysis.","Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of in-context learning.","Our code will be released on github."],"url":"http://arxiv.org/abs/2402.02872v1","category":"cs.CL"}
{"created":"2024-02-05 10:20:53","title":"Leveraging Noisy Observations in Zero-Sum Games","abstract":"This paper studies an instance of zero-sum games in which one player (the leader) commits to its opponent (the follower) to choose its actions by sampling a given probability measure (strategy). The actions of the leader are observed by the follower as the output of an arbitrary channel. In response to that, the follower chooses its action based on its current information, that is, the leader's commitment and the corresponding noisy observation of its action. Within this context, the equilibrium of the game with noisy action observability is shown to always exist and the necessary conditions for its uniqueness are identified. Interestingly, the noisy observations have important impact on the cardinality of the follower's set of best responses. Under particular conditions, such a set of best responses is proved to be a singleton almost surely. The proposed model captures any channel noise with a density with respect to the Lebesgue measure. As an example, the case in which the channel is described by a Gaussian probability measure is investigated.","sentences":["This paper studies an instance of zero-sum games in which one player (the leader) commits to its opponent (the follower) to choose its actions by sampling a given probability measure (strategy).","The actions of the leader are observed by the follower as the output of an arbitrary channel.","In response to that, the follower chooses its action based on its current information, that is, the leader's commitment and the corresponding noisy observation of its action.","Within this context, the equilibrium of the game with noisy action observability is shown to always exist and the necessary conditions for its uniqueness are identified.","Interestingly, the noisy observations have important impact on the cardinality of the follower's set of best responses.","Under particular conditions, such a set of best responses is proved to be a singleton almost surely.","The proposed model captures any channel noise with a density with respect to the Lebesgue measure.","As an example, the case in which the channel is described by a Gaussian probability measure is investigated."],"url":"http://arxiv.org/abs/2402.02861v1","category":"cs.GT"}
{"created":"2024-02-05 10:00:28","title":"Machine Learning Resistant Amorphous Silicon Physically Unclonable Functions (PUFs)","abstract":"We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si) cavities as physically unclonable functions (PUF). Machine learning attacks on integrated electronic PUFs have been demonstrated to be very effective at modeling PUF behavior. Such attacks on integrated a-Si photonic PUFs are investigated through application of algorithms including linear regression, k-nearest neighbor, decision tree ensembles (random forests and gradient boosted trees), and deep neural networks (DNNs). We found that DNNs performed the best among all the algorithms studied but still failed to completely break the a-Si PUF security which we quantify through a private information metric. Furthermore, machine learning resistance of a-Si PUFs were found to be directly related to the strength of their nonlinear response.","sentences":["We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si) cavities as physically unclonable functions (PUF).","Machine learning attacks on integrated electronic PUFs have been demonstrated to be very effective at modeling PUF behavior.","Such attacks on integrated a-Si photonic PUFs are investigated through application of algorithms including linear regression, k-nearest neighbor, decision tree ensembles (random forests and gradient boosted trees), and deep neural networks (DNNs).","We found that DNNs performed the best among all the algorithms studied but still failed to completely break the a-Si PUF security which we quantify through a private information metric.","Furthermore, machine learning resistance of a-Si PUFs were found to be directly related to the strength of their nonlinear response."],"url":"http://arxiv.org/abs/2402.02846v1","category":"physics.optics"}
{"created":"2024-02-05 09:44:49","title":"Shortened LLaMA: A Simple Depth Pruning for Large Language Models","abstract":"Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.","sentences":["Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs.","Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers.","Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged.","Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency.","In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance.","Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective.","We hope this work can help deploy LLMs on local and edge devices."],"url":"http://arxiv.org/abs/2402.02834v1","category":"cs.LG"}
{"created":"2024-02-05 09:42:42","title":"On the Uniqueness of K\u00e4hler-Einstein Polygons in Mutation-Equivalence Classes","abstract":"We study a subclass of K\\\"ahler-Einstein Fano polygons and how they behave under mutation. The polygons of interest are K\\\"ahler-Einstein Fano triangles and symmetric Fano polygons. In particular, we find an explicit bound for the number of these polygons in an arbitrary mutation-equivalence class.   An important mutation-invariant of a Fano polygon is its singularity content. We extend the notion of singularity content and prove that it is still a mutation-invariant. We use this to show that if two symmetric Fano polygons are mutation-equivalent, then they are isomorphic. We further show that if two K\\\"ahler-Einstein Fano triangles are mutation-equivalent, then they are isomorphic. Finally, we show that if a symmetric Fano polygon is mutation-equivalent to a K\\\"ahler-Einstein triangle, then they are isomorphic. Thus, each mutation-equivalence class has at most one Fano polygon which is either a K\\\"ahler-Einstein triangle or symmetric.   A recent conjecture states that all K\\\"ahler-Einstein Fano polygons are either triangles or are symmetric. We provide a counterexample $P$ to this conjecture and discuss several of its properties. For instance, we compute iterated barycentric transformations of $P$ and find that (a) the K\\\"ahler-Einstein property is not preserved by the barycentric transformation, and (b) $P$ is of strict type $B_2$. Finally, we find examples of K\\\"ahler-Einstein Fano polygons which are not minimal.","sentences":["We study a subclass of K\\\"ahler-Einstein Fano polygons and how they behave under mutation.","The polygons of interest are K\\\"ahler-Einstein Fano triangles and symmetric Fano polygons.","In particular, we find an explicit bound for the number of these polygons in an arbitrary mutation-equivalence class.   ","An important mutation-invariant of a Fano polygon is its singularity content.","We extend the notion of singularity content and prove that it is still a mutation-invariant.","We use this to show that if two symmetric Fano polygons are mutation-equivalent, then they are isomorphic.","We further show that if two K\\\"ahler-Einstein Fano triangles are mutation-equivalent, then they are isomorphic.","Finally, we show that if a symmetric Fano polygon is mutation-equivalent to a K\\\"ahler-Einstein triangle, then they are isomorphic.","Thus, each mutation-equivalence class has at most one Fano polygon which is either a K\\\"ahler-Einstein triangle or symmetric.   ","A recent conjecture states that all K\\\"ahler-Einstein Fano polygons are either triangles or are symmetric.","We provide a counterexample $P$ to this conjecture and discuss several of its properties.","For instance, we compute iterated barycentric transformations of $P$ and find that (a) the K\\\"ahler-Einstein property is not preserved by the barycentric transformation, and (b) $P$ is of strict type $B_2$. Finally, we find examples of K\\\"ahler-Einstein Fano polygons which are not minimal."],"url":"http://arxiv.org/abs/2402.02832v1","category":"math.CO"}
{"created":"2024-02-05 09:37:52","title":"What do we teach to engineering students: embedded ethics, morality, and politics","abstract":"In the past few years, calls for integrating ethics modules in engineering curricula have multiplied. Despite this positive trend, a number of issues with these embedded programs remains. First, learning goals are underspecified. A second limitation is the conflation of different dimensions under the same banner, in particular confusion between ethics curricula geared towards addressing the ethics of individual conduct and curricula geared towards addressing ethics at the societal level. In this article, we propose a tripartite framework to overcome these difficulties. Our framework analytically decomposes an ethics module into three dimensions. First, there is the ethical dimension, which pertains to the learning goals. Second, there is the moral dimension, which addresses the moral relevance of engineers conduct. Finally, there is the political dimension, which scales up issues of moral relevance at the civic level. All in all, our framework has two advantages. First, it provides analytic clarity, i.e. it enables course instructors to locate ethical dilemmas in either the moral or political realm and to make use of the tools and resources from moral and political philosophy. Second, it depicts a comprehensive ethical training, which enables students to both reason about moral issues in the abstract, and to socially contextualize potential solutions.","sentences":["In the past few years, calls for integrating ethics modules in engineering curricula have multiplied.","Despite this positive trend, a number of issues with these embedded programs remains.","First, learning goals are underspecified.","A second limitation is the conflation of different dimensions under the same banner, in particular confusion between ethics curricula geared towards addressing the ethics of individual conduct and curricula geared towards addressing ethics at the societal level.","In this article, we propose a tripartite framework to overcome these difficulties.","Our framework analytically decomposes an ethics module into three dimensions.","First, there is the ethical dimension, which pertains to the learning goals.","Second, there is the moral dimension, which addresses the moral relevance of engineers conduct.","Finally, there is the political dimension, which scales up issues of moral relevance at the civic level.","All in all, our framework has two advantages.","First, it provides analytic clarity, i.e. it enables course instructors to locate ethical dilemmas in either the moral or political realm and to make use of the tools and resources from moral and political philosophy.","Second, it depicts a comprehensive ethical training, which enables students to both reason about moral issues in the abstract, and to socially contextualize potential solutions."],"url":"http://arxiv.org/abs/2402.02831v1","category":"cs.CY"}
{"created":"2024-02-05 09:15:20","title":"FAIR-USE4OS: From open source to Open Source","abstract":"This paper extends the FAIR (Findable, Accessible, Interoperable, Reusable) guidelines to provide criteria for assessing if software is Open Source. By adding 'USE' (User-Centered, Sustainable, Equitable), software development can adhere to open source best practice by incorporating user-input early on, ensuring front-end designs are accessible to all possible stakeholders, and planning long-term sustainability alongside software design. The FAIR-USE4OS guidelines will allow funders and researchers to more effectively evaluate and plan Open Source software projects. There is good evidence of funders increasingly mandating that all funded research software is open-source; however, even under the FAIR guidelines, this could simply mean software released on GitHub with a Zenodo DOI. By employing the FAIR-USE4OS guidelines, best practice can be demonstrated from the very beginning of the design process and the software has the greatest chance of success by being truly 'Open Source'.","sentences":["This paper extends the FAIR (Findable, Accessible, Interoperable, Reusable) guidelines to provide criteria for assessing if software is Open Source.","By adding 'USE' (User-Centered, Sustainable, Equitable), software development can adhere to open source best practice by incorporating user-input early on, ensuring front-end designs are accessible to all possible stakeholders, and planning long-term sustainability alongside software design.","The FAIR-USE4OS guidelines will allow funders and researchers to more effectively evaluate and plan Open Source software projects.","There is good evidence of funders increasingly mandating that all funded research software is open-source; however, even under the FAIR guidelines, this could simply mean software released on GitHub with a Zenodo DOI.","By employing the FAIR-USE4OS guidelines, best practice can be demonstrated from the very beginning of the design process and the software has the greatest chance of success by being truly 'Open Source'."],"url":"http://arxiv.org/abs/2402.02824v1","category":"cs.SE"}
{"created":"2024-02-05 09:08:18","title":"Sub-kpc scale gas density histogram of the Galactic molecular gas: a new statistical method to characterise galactic-scale gas structures","abstract":"To understand physical properties of the interstellar medium (ISM) on various scales, we investigate it at parsec resolution on the kiloparsec scale. Here, we report on the sub-kpc scale Gas Density Histogram (GDH) of the Milky Way. The GDH is a density probability distribution function (PDF) of the gas volume density. Using this method, we are free from an identification of individual molecular clouds and their spatial structures. We use survey data of $^{12}$CO and $^{13}$CO ($J$=1-0) emission in the Galactic plane ($l = 10^{\\circ}$-$50^{\\circ}$) obtained as a part of the FOREST Unbiased Galactic plane Imaging survey with the Nobeyama 45m telescope (FUGIN). We make a GDH for every channel map of $2^{\\circ} \\times 2^{\\circ}$ area, including the blank sky component, and without setting cloud boundaries. This is a different approach from previous works for molecular clouds. The GDH fits well to a single or double log-normal distribution, which we name the low-density log-normal (L-LN) and high-density log-normal (H-LN) components, respectively. The multi-log-normal components suggest that the L-LN and H-LN components originate from two different stages of structure formation in the ISM. Moreover, we find that both the volume ratios of H-LN components to total ($f_{\\mathrm{H}}$) and the width of the L-LN along the gas density axis ($\\sigma_{\\rm{L}}$) show coherent structure in the Galactic-plane longitude-velocity diagram. It is possible that these GDH parameters are related to strong galactic shocks and other weak shocks in the Milky Way.","sentences":["To understand physical properties of the interstellar medium (ISM) on various scales, we investigate it at parsec resolution on the kiloparsec scale.","Here, we report on the sub-kpc scale Gas Density Histogram (GDH) of the Milky Way.","The GDH is a density probability distribution function (PDF) of the gas volume density.","Using this method, we are free from an identification of individual molecular clouds and their spatial structures.","We use survey data of $^{12}$CO and $^{13}$CO ($J$=1-0) emission in the Galactic plane ($l = 10^{\\circ}$-$50^{\\circ}$) obtained as a part of the FOREST Unbiased Galactic plane Imaging survey with the Nobeyama 45m telescope (FUGIN).","We make a GDH for every channel map of $2^{\\circ} \\times 2^{\\circ}$ area, including the blank sky component, and without setting cloud boundaries.","This is a different approach from previous works for molecular clouds.","The GDH fits well to a single or double log-normal distribution, which we name the low-density log-normal (L-LN) and high-density log-normal (H-LN) components, respectively.","The multi-log-normal components suggest that the L-LN and H-LN components originate from two different stages of structure formation in the ISM.","Moreover, we find that both the volume ratios of H-LN components to total ($f_{\\mathrm{H}}$) and the width of the L-LN along the gas density axis ($\\sigma_{\\rm{L}}$) show coherent structure in the Galactic-plane longitude-velocity diagram.","It is possible that these GDH parameters are related to strong galactic shocks and other weak shocks in the Milky Way."],"url":"http://arxiv.org/abs/2402.02821v1","category":"astro-ph.GA"}
{"created":"2024-02-05 08:59:47","title":"Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing","abstract":"Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \\emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \\emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attribute cannot be used at the prediction phase.   Leveraging our theoretical results, we design methods that learn fair Bayes-optimal classifiers under bilinear disparity constraints. Our methods cover three popular approaches to fairness-aware classification, via pre-processing (Fair Up- and Down-Sampling), in-processing (Fair Cost-Sensitive Classification) and post-processing (a Fair Plug-In Rule). Our methods control disparity directly while achieving near-optimal fairness-accuracy tradeoffs. We show empirically that our methods compare favorably to existing algorithms.","sentences":["Machine learning algorithms may have disparate impacts on protected groups.","To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints.","We introduce the notion of \\emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \\emph{bilinear disparity measures}, which are also linear in the group-wise regression functions.","We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   ","We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma.","For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules.","Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attribute cannot be used at the prediction phase.   ","Leveraging our theoretical results, we design methods that learn fair Bayes-optimal classifiers under bilinear disparity constraints.","Our methods cover three popular approaches to fairness-aware classification, via pre-processing (Fair Up- and Down-Sampling), in-processing (Fair Cost-Sensitive Classification) and post-processing (a Fair Plug-In Rule).","Our methods control disparity directly while achieving near-optimal fairness-accuracy tradeoffs.","We show empirically that our methods compare favorably to existing algorithms."],"url":"http://arxiv.org/abs/2402.02817v1","category":"stat.ML"}
{"created":"2024-02-05 08:40:33","title":"Orientation-dependent Josephson effect in spin-singlet superconductor/altermagnet/spin-triplet superconductor junctions","abstract":"We study the Josephson effect in the spin-singlet superconductor/altermagnet/spin-triplet superconductor junctions using the Green's function method. The current-phase difference relationships in the junctions strongly depend on the orientation of altermagnet and the types of the Cooper pairs. For the orientation angle equal to odd multiples of $\\pi/4$, the current-phase difference relationships are of the $\\sin{2\\phi}$ type, which are irrespective of the pairing wave functions in superconductors. For the other orientation angles, the emergence of the lowest order current becomes possible and its form, $\\sin\\phi$ or $\\cos\\phi$, depends on the pairing wave functions in superconductors. The $\\phi_{0}$ phase and the $0$-$\\pi$ transition can be realized in our junctions due to the appearance of the lowest order current. The selection rules for the lowest order current are presented. The symmetric relations satisfied by the current-phase difference relationships are analyzed through considering the transformations of the junctions under the mirror reflection, the time-reversal and the spin rotation operations. Our results not only provide a method to detect the intrinsic spin-triplet superconductivity but also possess application values in the design of the field-free quantum devices.","sentences":["We study the Josephson effect in the spin-singlet superconductor/altermagnet/spin-triplet superconductor junctions using the Green's function method.","The current-phase difference relationships in the junctions strongly depend on the orientation of altermagnet and the types of the Cooper pairs.","For the orientation angle equal to odd multiples of $\\pi/4$, the current-phase difference relationships are of the $\\sin{2\\phi}$ type, which are irrespective of the pairing wave functions in superconductors.","For the other orientation angles, the emergence of the lowest order current becomes possible and its form, $\\sin\\phi$ or $\\cos\\phi$, depends on the pairing wave functions in superconductors.","The $\\phi_{0}$ phase and the $0$-$\\pi$ transition can be realized in our junctions due to the appearance of the lowest order current.","The selection rules for the lowest order current are presented.","The symmetric relations satisfied by the current-phase difference relationships are analyzed through considering the transformations of the junctions under the mirror reflection, the time-reversal and the spin rotation operations.","Our results not only provide a method to detect the intrinsic spin-triplet superconductivity but also possess application values in the design of the field-free quantum devices."],"url":"http://arxiv.org/abs/2402.02810v1","category":"cond-mat.supr-con"}
{"created":"2024-02-05 07:56:02","title":"Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)","abstract":"In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit (TeLU), a novel neural network activation function, represented as $f(x) = x{\\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of conventional activation functions like ReLU, GELU, and Mish by addressing the vanishing and, to an extent, the exploding gradient problems. Our theoretical analysis and empirical assessments reveal that TeLU outperforms existing activation functions in stability and robustness, effectively adjusting activation outputs' mean towards zero for enhanced training stability and convergence. Extensive evaluations against popular activation functions (ReLU, GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including Resnet-50, demonstrate TeLU's lower variance and superior performance, even under hyperparameter conditions optimized for other functions. In large-scale tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet, encompassing 860 scenarios, TeLU consistently showcased its effectiveness, positioning itself as a potential new standard for neural network activation functions, boosting stability and performance in diverse deep learning applications.","sentences":["In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit (TeLU), a novel neural network activation function, represented as $f(x)","= x{\\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of conventional activation functions like ReLU, GELU, and Mish by addressing the vanishing and, to an extent, the exploding gradient problems.","Our theoretical analysis and empirical assessments reveal that TeLU outperforms existing activation functions in stability and robustness, effectively adjusting activation outputs' mean towards zero for enhanced training stability and convergence.","Extensive evaluations against popular activation functions (ReLU, GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including Resnet-50, demonstrate TeLU's lower variance and superior performance, even under hyperparameter conditions optimized for other functions.","In large-scale tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet, encompassing 860 scenarios, TeLU consistently showcased its effectiveness, positioning itself as a potential new standard for neural network activation functions, boosting stability and performance in diverse deep learning applications."],"url":"http://arxiv.org/abs/2402.02790v1","category":"cs.LG"}
{"created":"2024-02-05 07:41:44","title":"A gluing operation for dimer quivers","abstract":"In this article we introduce a gluing operation on dimer models. This allows us to construct dimer quivers on arbitrary surfaces. We study how the associated dimer and boundary algebras behave under the gluing and how to determine them from the gluing components. We also use this operation to construct homogeneous dimer quivers on annuli.","sentences":["In this article we introduce a gluing operation on dimer models.","This allows us to construct dimer quivers on arbitrary surfaces.","We study how the associated dimer and boundary algebras behave under the gluing and how to determine them from the gluing components.","We also use this operation to construct homogeneous dimer quivers on annuli."],"url":"http://arxiv.org/abs/2402.02784v1","category":"math.CO"}
{"created":"2024-02-05 07:16:46","title":"Instant square lattice structured illumination microscopy: an optimal strategy towards photon-saving and real-time super-resolution observation","abstract":"Over the past decade, structured illumination microscopy (SIM) has found its niche in super-resolution (SR) microscopy due to its fast imaging speed and low excitation intensity. However, due to the significantly higher light dose compared to wide-field microscopy and the time-consuming post-processing procedures, long-term, real-time, super-resolution observation of living cells is still out of reach for most SIM setups, which inevitably limits its routine use by cell biologists. Here, we describe square lattice SIM (SL-SIM) for long-duration live cell imaging by using the square lattice optical field as illumination, which allows continuous super-resolved observation over long periods of time. In addition, by extending the previous joint spatial-frequency reconstruction concept to SL-SIM, a high-speed reconstruction strategy is validated in the GPU environment, whose reconstruction time is even shorter than image acquisition time, thus enabling real-time observation. We have demonstrated the potential of SL-SIM on various biological applications, ranging from microtubule cytoskeleton dynamics to the interactions of mitochondrial cristae and DNAs in COS7 cells. The inherent lower light dose and user-friendly workflow of the SL-SIM could help make long-duration, real-time and super-resolved observations accessible to biological laboratories.","sentences":["Over the past decade, structured illumination microscopy (SIM) has found its niche in super-resolution (SR) microscopy due to its fast imaging speed and low excitation intensity.","However, due to the significantly higher light dose compared to wide-field microscopy and the time-consuming post-processing procedures, long-term, real-time, super-resolution observation of living cells is still out of reach for most SIM setups, which inevitably limits its routine use by cell biologists.","Here, we describe square lattice SIM (SL-SIM) for long-duration live cell imaging by using the square lattice optical field as illumination, which allows continuous super-resolved observation over long periods of time.","In addition, by extending the previous joint spatial-frequency reconstruction concept to SL-SIM, a high-speed reconstruction strategy is validated in the GPU environment, whose reconstruction time is even shorter than image acquisition time, thus enabling real-time observation.","We have demonstrated the potential of SL-SIM on various biological applications, ranging from microtubule cytoskeleton dynamics to the interactions of mitochondrial cristae and DNAs in COS7 cells.","The inherent lower light dose and user-friendly workflow of the SL-SIM could help make long-duration, real-time and super-resolved observations accessible to biological laboratories."],"url":"http://arxiv.org/abs/2402.02775v1","category":"physics.optics"}
{"created":"2024-02-05 06:59:01","title":"Chiral perturbative solution for the type-I seesaw mechanism in next-to-leading order","abstract":"In this letter, we perform chiral perturbative diagonalization of the type-I seesaw mechanism by hierarchical singular values $\\lambda_{i}$ of the Dirac mass matrix $m_{D}$ up to the next-to-leading order (NLO). Since the mass matrix of right-handed neutrinos $M_{R}$ has parity symmetries under $\\lambda_{i} \\leftrightarrow - \\lambda_{i}$, the singular values $M_{i}$ and mixing angles of diagonalization of $M_{R}$ are written by only even and odd orders of $\\lambda_{i}$ respectively.   We confirm this fact by specific perturbative expansions of the third- and the fourth-order by $\\lambda_{i}$. As a result, as long as the chiral perturbation theory is valid, the NLO contributions are generally suppressed by $O(\\lambda_{i}^{2} / \\labmda_{j}^{2}) \\lesssim 1\\%$ compared to the leading-order expressions that have sufficient accuracies.","sentences":["In this letter, we perform chiral perturbative diagonalization of the type-I seesaw mechanism by hierarchical singular values $\\lambda_{i}$ of the Dirac mass matrix $m_{D}$ up to the next-to-leading order (NLO).","Since the mass matrix of right-handed neutrinos $M_{R}$ has parity symmetries under $\\lambda_{i} \\leftrightarrow - \\lambda_{i}$, the singular values $M_{i}$ and mixing angles of diagonalization of $M_{R}$ are written by only even and odd orders of $\\lambda_{i}$ respectively.   ","We confirm this fact by specific perturbative expansions of the third- and the fourth-order by $\\lambda_{i}$. As a result, as long as the chiral perturbation theory is valid, the NLO contributions are generally suppressed by $O(\\lambda_{i}^{2} / \\labmda_{j}^{2}) \\lesssim 1\\%$ compared to the leading-order expressions that have sufficient accuracies."],"url":"http://arxiv.org/abs/2402.02767v1","category":"hep-ph"}
{"created":"2024-02-05 06:53:05","title":"Charmonium states in a coupled-channel model","abstract":"We systematically investigate the mass spectrum and two-body open-charm strong decays of charmonium states in a coupled-channel model where the $^3P_0$ quark-antiquark pair creation mechanism is employed. The results of masses, mass shifts, proportions of the $c\\bar{c}$ component, and open-charm decay widths are provided. The $S$-$D$ wave mixing angles and di-electric decay widths for vector mesons are also presented. Based on our results, we find that the $\\psi(3770)$, $\\psi(4040)$, $\\psi(4160)$, $\\psi(4360)$, and $\\psi(4415)$ can be assigned as the $1^3D_1$-, $3^3S_1$-, $2^3D_1$-, $4^3S_1$-, and $3^3D_1$-dominated charmonium states, respectively. The $\\psi_3(3842)$ is a good candidate of the $\\psi_3(1D)$ charmonium state. The calculated mass and strong decay width of $\\chi_{c1}(2P)$ with significant continuum contribution ($\\sim$57\\%) favor the charmonium interpretation for the mysterious $\\chi_{c1}(3872)$. When considering the large uncertainty in the observed decay width, the possibility to assign the $\\chi_{c0}(3860)$ as the $\\chi_{c0}(2P)$ charmonium state cannot be ruled out. One may describe well the properties of $\\chi_{c2}(3930)$ with the $\\chi_{c2}(2P)$ charmonium. The predictions on properties of other $c\\bar{c}$ states can be tested by future experiments.","sentences":["We systematically investigate the mass spectrum and two-body open-charm strong decays of charmonium states in a coupled-channel model where the $^3P_0$ quark-antiquark pair creation mechanism is employed.","The results of masses, mass shifts, proportions of the $c\\bar{c}$ component, and open-charm decay widths are provided.","The $S$-$D$ wave mixing angles and di-electric decay widths for vector mesons are also presented.","Based on our results, we find that the $\\psi(3770)$, $\\psi(4040)$, $\\psi(4160)$, $\\psi(4360)$, and $\\psi(4415)$ can be assigned as the $1^3D_1$-, $3^3S_1$-, $2^3D_1$-, $4^3S_1$-, and $3^3D_1$-dominated charmonium states, respectively.","The $\\psi_3(3842)$ is a good candidate of the $\\psi_3(1D)$ charmonium state.","The calculated mass and strong decay width of $\\chi_{c1}(2P)$ with significant continuum contribution ($\\sim$57\\%) favor the charmonium interpretation for the mysterious $\\chi_{c1}(3872)$. When considering the large uncertainty in the observed decay width, the possibility to assign the $\\chi_{c0}(3860)$ as the $\\chi_{c0}(2P)$ charmonium state cannot be ruled out.","One may describe well the properties of $\\chi_{c2}(3930)$ with the $\\chi_{c2}(2P)$ charmonium.","The predictions on properties of other $c\\bar{c}$ states can be tested by future experiments."],"url":"http://arxiv.org/abs/2402.02765v1","category":"hep-ph"}
{"created":"2024-02-05 06:25:10","title":"Quantification of the volume-fraction reduction of sheared fragile glass-forming liquids and its impact on rheology","abstract":"This study determines the volume-fraction reduction of sheared fragile glass-forming liquids. We consider a group of hypothetical systems that consist of particles with anisotropic particle-size modulations yet have almost the same average particle configuration as actual systems under shear flow. Our molecular dynamics (MD) simulations demonstrate that one specific hypothetical system can reproduce the relaxation dynamics of an actual sheared system, and we identify the shear-flow effect on the particle size with anisotropic size-modulation of this specific system. Then, based on the determination of the particle size and the resultant volume fraction, we rationalize how slight decreases in the volume fraction significantly reduce the viscosity snf provide a nonlinear constitutive equation. Notably, the obtained rheological predictions, including the crossover shear rate from Newtonian to non-Newtonian behavior, can be expressed only in terms of experimental observables, showing a good agreement with the MD simulation results. Our perspective on the volume fraction under shear flow may provide new insights into the conventional concept of free-volume.","sentences":["This study determines the volume-fraction reduction of sheared fragile glass-forming liquids.","We consider a group of hypothetical systems that consist of particles with anisotropic particle-size modulations yet have almost the same average particle configuration as actual systems under shear flow.","Our molecular dynamics (MD) simulations demonstrate that one specific hypothetical system can reproduce the relaxation dynamics of an actual sheared system, and we identify the shear-flow effect on the particle size with anisotropic size-modulation of this specific system.","Then, based on the determination of the particle size and the resultant volume fraction, we rationalize how slight decreases in the volume fraction significantly reduce the viscosity snf provide a nonlinear constitutive equation.","Notably, the obtained rheological predictions, including the crossover shear rate from Newtonian to non-Newtonian behavior, can be expressed only in terms of experimental observables, showing a good agreement with the MD simulation results.","Our perspective on the volume fraction under shear flow may provide new insights into the conventional concept of free-volume."],"url":"http://arxiv.org/abs/2402.02757v1","category":"cond-mat.soft"}
{"created":"2024-02-05 06:24:53","title":"Metasurface lens that is converging or diverging depending on transmission direction enables ultra-compact MEMS tunable reflective lens","abstract":"A conventional refractive lens surface can act as a positive (converging) or negative (diverging) lens, but the same surface cannot act as both. We show that a geometric phase metasurface lens can have the unique property of acting both as a positive or negative lens upon transmission through its front or rear side, respectively. This offers certain freedom in compound lens design, where one combines focusing and defocusing operations. We utilize this property to make an ultra-compact, varifocal reflective lens, where a metasurface is placed in front of a novel long-stroke piezoelectric MEMS-micromirror. A large theoretical diopter tunability of 6330 m$^{-1}$ is enabled due to innovative thin-film piezoelectric MEMS design, offering 62 $\\mu$m displacement at 40V and low power, along with rapid actuation in the kHz region. The achieved MEMS-displacement is an order of magnitude larger than previously reported out-of-plane mechanical metasurface actuation. Since both metasurface and micromirror are flat, the presented reflective lens can be assembled without need for a spacer. It is therefore well suited for wafer-level silicon fabrication at high volumes and low cost. A proof-of-concept implementation using a 1550nm NIR metalens is demonstrated, attaining on the order of 1121 m$^{-1}$ diopter change for a focal length shift of 270 $\\mu$m caused by a 53 $\\mu$m micromirror displacement.","sentences":["A conventional refractive lens surface can act as a positive (converging) or negative (diverging) lens, but the same surface cannot act as both.","We show that a geometric phase metasurface lens can have the unique property of acting both as a positive or negative lens upon transmission through its front or rear side, respectively.","This offers certain freedom in compound lens design, where one combines focusing and defocusing operations.","We utilize this property to make an ultra-compact, varifocal reflective lens, where a metasurface is placed in front of a novel long-stroke piezoelectric MEMS-micromirror.","A large theoretical diopter tunability of 6330 m$^{-1}$ is enabled due to innovative thin-film piezoelectric MEMS design, offering 62 $\\mu$m displacement at 40V and low power, along with rapid actuation in the kHz region.","The achieved MEMS-displacement is an order of magnitude larger than previously reported out-of-plane mechanical metasurface actuation.","Since both metasurface and micromirror are flat, the presented reflective lens can be assembled without need for a spacer.","It is therefore well suited for wafer-level silicon fabrication at high volumes and low cost.","A proof-of-concept implementation using a 1550nm NIR metalens is demonstrated, attaining on the order of 1121 m$^{-1}$ diopter change for a focal length shift of 270 $\\mu$m caused by a 53 $\\mu$m micromirror displacement."],"url":"http://arxiv.org/abs/2402.02755v1","category":"physics.optics"}
{"created":"2024-02-05 06:17:26","title":"Crosstalk Attacks and Defence in a Shared Quantum Computing Environment","abstract":"Quantum computing has the potential to provide solutions to problems that are intractable on classical computers, but the accuracy of the current generation of quantum computers suffer from the impact of noise or errors such as leakage, crosstalk, dephasing, and amplitude damping among others. As the access to quantum computers is almost exclusively in a shared environment through cloud-based services, it is possible that an adversary can exploit crosstalk noise to disrupt quantum computations on nearby qubits, even carefully designing quantum circuits to purposely lead to wrong answers. In this paper, we analyze the extent and characteristics of crosstalk noise through tomography conducted on IBM Quantum computers, leading to an enhanced crosstalk simulation model. Our results indicate that crosstalk noise is a significant source of errors on IBM quantum hardware, making crosstalk based attack a viable threat to quantum computing in a shared environment. Based on our crosstalk simulator benchmarked against IBM hardware, we assess the impact of crosstalk attacks and develop strategies for mitigating crosstalk effects. Through a systematic set of simulations, we assess the effectiveness of three crosstalk attack mitigation strategies, namely circuit separation, qubit allocation optimization via reinforcement learning, and the use of spectator qubits, and show that they all overcome crosstalk attacks with varying degrees of success and help to secure quantum computing in a shared platform.","sentences":["Quantum computing has the potential to provide solutions to problems that are intractable on classical computers, but the accuracy of the current generation of quantum computers suffer from the impact of noise or errors such as leakage, crosstalk, dephasing, and amplitude damping among others.","As the access to quantum computers is almost exclusively in a shared environment through cloud-based services, it is possible that an adversary can exploit crosstalk noise to disrupt quantum computations on nearby qubits, even carefully designing quantum circuits to purposely lead to wrong answers.","In this paper, we analyze the extent and characteristics of crosstalk noise through tomography conducted on IBM Quantum computers, leading to an enhanced crosstalk simulation model.","Our results indicate that crosstalk noise is a significant source of errors on IBM quantum hardware, making crosstalk based attack a viable threat to quantum computing in a shared environment.","Based on our crosstalk simulator benchmarked against IBM hardware, we assess the impact of crosstalk attacks and develop strategies for mitigating crosstalk effects.","Through a systematic set of simulations, we assess the effectiveness of three crosstalk attack mitigation strategies, namely circuit separation, qubit allocation optimization via reinforcement learning, and the use of spectator qubits, and show that they all overcome crosstalk attacks with varying degrees of success and help to secure quantum computing in a shared platform."],"url":"http://arxiv.org/abs/2402.02753v1","category":"quant-ph"}
{"created":"2024-02-05 06:16:40","title":"Fast single pixel modal wavefront sensing using neural networks","abstract":"Dynamic wavefront aberrations negatively impact a wide range of optical applications including astronomy, optical free-space telecommunications and bio-imaging. Wavefront errors can be compensated by an adaptive optics system comprised of a deformable mirror and wavefront sensor connected by a control loop. For satellite optical communications (SatCom), wavefront sensing is particularly challenging due to the rapid wavefront fluctuations induced by strong turbulence and movement of the transmitting satellite across the sky. Existing wavefront sensing techniques require fast cameras (>kHz) that are not widely available at wavelengths suitable for SatCom (e.g., 1550nm and mid-to-long wave infrared). Here, we propose a new wavefront sensing technique that uses a single photodiode and a fast mirror to make phase-diverse intensity measurements of the incoming wavefront. We train neural networks to accurately estimate the input phase given this phase-diverse sub-millisecond intensity trace. Our simulations show that our technique is robust in cases of strong turbulence where previous modal wavefront sensors fail due to modal crosstalk, achieving 99% of the optimal Strehl ratio from a 50-mode correction at a sensing rate of 2kHz. We explore typical cases of turbulence magnitude, sensing speed and noise that might be encountered by such a system.","sentences":["Dynamic wavefront aberrations negatively impact a wide range of optical applications including astronomy, optical free-space telecommunications and bio-imaging.","Wavefront errors can be compensated by an adaptive optics system comprised of a deformable mirror and wavefront sensor connected by a control loop.","For satellite optical communications (SatCom), wavefront sensing is particularly challenging due to the rapid wavefront fluctuations induced by strong turbulence and movement of the transmitting satellite across the sky.","Existing wavefront sensing techniques require fast cameras (>kHz) that are not widely available at wavelengths suitable for SatCom (e.g., 1550nm and mid-to-long wave infrared).","Here, we propose a new wavefront sensing technique that uses a single photodiode and a fast mirror to make phase-diverse intensity measurements of the incoming wavefront.","We train neural networks to accurately estimate the input phase given this phase-diverse sub-millisecond intensity trace.","Our simulations show that our technique is robust in cases of strong turbulence where previous modal wavefront sensors fail due to modal crosstalk, achieving 99% of the optimal Strehl ratio from a 50-mode correction at a sensing rate of 2kHz.","We explore typical cases of turbulence magnitude, sensing speed and noise that might be encountered by such a system."],"url":"http://arxiv.org/abs/2402.02752v1","category":"physics.optics"}
{"created":"2024-02-05 05:59:34","title":"Optimal dynamic climate adaptation pathways: a case study of New York City","abstract":"Assessing climate risk and its potential impacts on our cities and economies is of fundamental importance. Extreme weather events, such as hurricanes, floods, and storm surges can lead to catastrophic damages. We propose a flexible approach based on real options analysis and extreme value theory, which enables the selection of optimal adaptation pathways for a portfolio of climate adaptation projects. We model the severity of extreme sea level events using the block maxima approach from extreme value theory, and then develop a real options framework, factoring in climate change, sea level rise uncertainty, and the growth in asset exposure. We then apply the proposed framework to a real-world problem, considering sea level data as well as different adaptation investment options for New York City. Our research can assist governments and policy makers in taking informed decisions about optimal adaptation pathways and more specifically about reducing flood and storm surge risk in a dynamic settings.","sentences":["Assessing climate risk and its potential impacts on our cities and economies is of fundamental importance.","Extreme weather events, such as hurricanes, floods, and storm surges can lead to catastrophic damages.","We propose a flexible approach based on real options analysis and extreme value theory, which enables the selection of optimal adaptation pathways for a portfolio of climate adaptation projects.","We model the severity of extreme sea level events using the block maxima approach from extreme value theory, and then develop a real options framework, factoring in climate change, sea level rise uncertainty, and the growth in asset exposure.","We then apply the proposed framework to a real-world problem, considering sea level data as well as different adaptation investment options for New York City.","Our research can assist governments and policy makers in taking informed decisions about optimal adaptation pathways and more specifically about reducing flood and storm surge risk in a dynamic settings."],"url":"http://arxiv.org/abs/2402.02745v1","category":"q-fin.RM"}
{"created":"2024-02-05 05:48:25","title":"Bifurcation to complex dynamics in largely modulated voltage-controlled parametric oscillator","abstract":"An experimental demonstration of a parametric oscillation of a magnetization in a ferromagnet was performed recently by applying a microwave voltage, indicating the potential to be applied in a switching method in non-volatile memories. In the previous works, the modulation of a perpendicular magnetic anisotropy field produced by the microwave voltage was small compared with an external magnetic field pointing in an in-plane direction. A recent trend is, however, opposite, where an efficiency of the voltage controlled magnetic anisotropy (VCMA) effect is increased significantly by material research and thus, the modulated magnetic anisotropy field can be larger than the external magnetic field. Here, we solved the Landau-Lifshitz-Gilbert equation numerically and investigated the magnetization dynamics driven under a wide range of the microwave VCMA effect. We evaluated bifurcation diagrams, which summarize local maxima of the magnetization dynamics. For low modulation amplitudes, the local maximum is a single point because the dynamics is the periodic parametric oscillation. The bifurcation diagrams show distributions of the local maxima when the microwave magnetic anisotropy field becomes larger than the external magnetic field. The appearance of this broadened distribution indicates complex dynamics such as chaotic and transient-chaotic behaviors, which were confirmed from an analysis of temporal dynamics.","sentences":["An experimental demonstration of a parametric oscillation of a magnetization in a ferromagnet was performed recently by applying a microwave voltage, indicating the potential to be applied in a switching method in non-volatile memories.","In the previous works, the modulation of a perpendicular magnetic anisotropy field produced by the microwave voltage was small compared with an external magnetic field pointing in an in-plane direction.","A recent trend is, however, opposite, where an efficiency of the voltage controlled magnetic anisotropy (VCMA) effect is increased significantly by material research and thus, the modulated magnetic anisotropy field can be larger than the external magnetic field.","Here, we solved the Landau-Lifshitz-Gilbert equation numerically and investigated the magnetization dynamics driven under a wide range of the microwave VCMA effect.","We evaluated bifurcation diagrams, which summarize local maxima of the magnetization dynamics.","For low modulation amplitudes, the local maximum is a single point because the dynamics is the periodic parametric oscillation.","The bifurcation diagrams show distributions of the local maxima when the microwave magnetic anisotropy field becomes larger than the external magnetic field.","The appearance of this broadened distribution indicates complex dynamics such as chaotic and transient-chaotic behaviors, which were confirmed from an analysis of temporal dynamics."],"url":"http://arxiv.org/abs/2402.02742v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 05:46:31","title":"DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models","abstract":"In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme.   Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100\\% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\\% detection pass rate with very high attack and benign performance for the backdoored diffusion models.","sentences":["In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical.","Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   ","In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works.","Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection.","Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise.","We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme.   ","Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy.","For trigger detection, our distribution discrepancy-based solution can achieve a 100\\% detection rate for the Trojan triggers used in the existing works.","For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\\% detection pass rate with very high attack and benign performance for the backdoored diffusion models."],"url":"http://arxiv.org/abs/2402.02739v1","category":"cs.CR"}
{"created":"2024-02-05 05:06:26","title":"How phonemes contribute to deep speaker models?","abstract":"Which phonemes convey more speaker traits is a long-standing question, and various perception experiments were conducted with human subjects. For speaker recognition, studies were conducted with the conventional statistical models and the drawn conclusions are more or less consistent with the perception results. However, which phonemes are more important with modern deep neural models is still unexplored, due to the opaqueness of the decision process. This paper conducts a novel study for the attribution of phonemes with two types of deep speaker models that are based on TDNN and CNN respectively, from the perspective of model explanation. Specifically, we conducted the study by two post-explanation methods: LayerCAM and Time Align Occlusion (TAO). Experimental results showed that: (1) At the population level, vowels are more important than consonants, confirming the human perception studies. However, fricatives are among the most unimportant phonemes, which contrasts with previous studies. (2) At the speaker level, a large between-speaker variation is observed regarding phoneme importance, indicating that whether a phoneme is important or not is largely speaker-dependent.","sentences":["Which phonemes convey more speaker traits is a long-standing question, and various perception experiments were conducted with human subjects.","For speaker recognition, studies were conducted with the conventional statistical models and the drawn conclusions are more or less consistent with the perception results.","However, which phonemes are more important with modern deep neural models is still unexplored, due to the opaqueness of the decision process.","This paper conducts a novel study for the attribution of phonemes with two types of deep speaker models that are based on TDNN and CNN respectively, from the perspective of model explanation.","Specifically, we conducted the study by two post-explanation methods: LayerCAM and Time Align Occlusion (TAO).","Experimental results showed that: (1) At the population level, vowels are more important than consonants, confirming the human perception studies.","However, fricatives are among the most unimportant phonemes, which contrasts with previous studies.","(2) At the speaker level, a large between-speaker variation is observed regarding phoneme importance, indicating that whether a phoneme is important or not is largely speaker-dependent."],"url":"http://arxiv.org/abs/2402.02730v1","category":"cs.SD"}
{"created":"2024-02-05 04:59:09","title":"A Local Projection Stabilised HHO Method for the Oseen Problem","abstract":"Fluid flow problems with high Reynolds number show spurious oscillations in their solution when solved using standard Galerkin finite element methods. These Oscillations can be eradicated using various stabilisation techniques. In this article, we use a local projection stabilisation for a Hybrid High-Order approximation of the Oseen problem. We prove an existence-uniqueness result under a SUPG-like norm. We derive an optimal order error estimate under this norm for equal order polynomial discretisation of velocity and pressure spaces.","sentences":["Fluid flow problems with high Reynolds number show spurious oscillations in their solution when solved using standard Galerkin finite element methods.","These Oscillations can be eradicated using various stabilisation techniques.","In this article, we use a local projection stabilisation for a Hybrid High-Order approximation of the Oseen problem.","We prove an existence-uniqueness result under a SUPG-like norm.","We derive an optimal order error estimate under this norm for equal order polynomial discretisation of velocity and pressure spaces."],"url":"http://arxiv.org/abs/2402.02727v1","category":"math.NA"}
{"created":"2024-02-05 04:55:15","title":"How do software practitioners perceive human-centric defects?","abstract":"Context: Human-centric software design and development focuses on how users want to carry out their tasks rather than making users accommodate their software. Software users can have different genders, ages, cultures, languages, disabilities, socioeconomic statuses, and educational backgrounds, among many other differences. Due to the inherently varied nature of these differences and their impact on software usage, preferences and issues of users can vary, resulting in user-specific defects that we term as `human-centric defects' (HCDs).   Objective: This research aims to understand the perception and current management practices of such human-centric defects by software practitioners, identify key challenges in reporting, understanding and fixing them, and provide recommendations to improve HCDs management in software engineering.   Method: We conducted a survey and interviews with software engineering practitioners to gauge their knowledge and experience on HCDs and the defect tracking process.   Results: We analysed fifty (50) survey- and ten (10) interview- responses from SE practitioners and identified that there are multiple gaps in the current management of HCDs in software engineering practice. There is a lack of awareness regarding human-centric aspects, causing them to be lost or under-appreciated during software development. Our results revealed that handling HCDs could be improved by following a better feedback process with end-users, a more descriptive taxonomy, and suitable automation.   Conclusion: HCDs present a major challenge to software practitioners, given their diverse end-user base. In the software engineering domain, research on HCDs has been limited and requires effort from the research and practice communities to create better awareness and support regarding human-centric aspects.","sentences":["Context: Human-centric software design and development focuses on how users want to carry out their tasks rather than making users accommodate their software.","Software users can have different genders, ages, cultures, languages, disabilities, socioeconomic statuses, and educational backgrounds, among many other differences.","Due to the inherently varied nature of these differences and their impact on software usage, preferences and issues of users can vary, resulting in user-specific defects that we term as `human-centric defects' (HCDs).   ","Objective:","This research aims to understand the perception and current management practices of such human-centric defects by software practitioners, identify key challenges in reporting, understanding and fixing them, and provide recommendations to improve HCDs management in software engineering.   ","Method: We conducted a survey and interviews with software engineering practitioners to gauge their knowledge and experience on HCDs and the defect tracking process.   ","Results:","We analysed fifty (50) survey- and ten (10) interview- responses from SE practitioners and identified that there are multiple gaps in the current management of HCDs in software engineering practice.","There is a lack of awareness regarding human-centric aspects, causing them to be lost or under-appreciated during software development.","Our results revealed that handling HCDs could be improved by following a better feedback process with end-users, a more descriptive taxonomy, and suitable automation.   ","Conclusion: HCDs present a major challenge to software practitioners, given their diverse end-user base.","In the software engineering domain, research on HCDs has been limited and requires effort from the research and practice communities to create better awareness and support regarding human-centric aspects."],"url":"http://arxiv.org/abs/2402.02726v1","category":"cs.SE"}
{"created":"2024-02-05 04:45:24","title":"FDNet: Frequency Domain Denoising Network For Cell Segmentation in Astrocytes Derived From Induced Pluripotent Stem Cells","abstract":"Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases. Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism. The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation. However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe. Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue. In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their corresponding pixel-level annotation masks. Moreover, a novel frequency domain denoising network, named FDNet, is proposed for astrocyte segmentation. In detail, our FDNet consists of a contextual information fusion module (CIF), an attention block (AB), and a Fourier transform block (FTB). CIF and AB fuse multi-scale feature embeddings to localize the astrocytes. FTB transforms feature embeddings into the frequency domain and conducts a high-pass filter to eliminate interference information. Experimental results demonstrate the superiority of our proposed FDNet over the state-of-the-art substitutes in astrocyte segmentation, shedding insights for iPSC differentiation progress prediction.","sentences":["Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases.","Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism.","The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation.","However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe.","Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue.","In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their corresponding pixel-level annotation masks.","Moreover, a novel frequency domain denoising network, named FDNet, is proposed for astrocyte segmentation.","In detail, our FDNet consists of a contextual information fusion module (CIF), an attention block (AB), and a Fourier transform block (FTB).","CIF and AB fuse multi-scale feature embeddings to localize the astrocytes.","FTB transforms feature embeddings into the frequency domain and conducts a high-pass filter to eliminate interference information.","Experimental results demonstrate the superiority of our proposed FDNet over the state-of-the-art substitutes in astrocyte segmentation, shedding insights for iPSC differentiation progress prediction."],"url":"http://arxiv.org/abs/2402.02724v1","category":"cs.CV"}
{"created":"2024-02-05 04:29:39","title":"Discounted Adaptive Online Prediction","abstract":"Online learning is not always about memorizing everything. Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.","sentences":["Online learning is not always about memorizing everything.","Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in.","To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning.","Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate.","In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning.","We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions."],"url":"http://arxiv.org/abs/2402.02720v1","category":"cs.LG"}
{"created":"2024-02-05 04:07:20","title":"Entangling two exciton modes using exciton optomechanics","abstract":"Exciton optomechanics, bridging cavity exciton polaritons and optomechanics, opens new opportunities for the study of light-matter strong interactions and nonlinearities, due to the rich nonlinear couplings among excitons, phonons, and photons. Here, we propose to entangle two exciton modes in an exciton-optomechanical system, which consists of a semiconductor optomechanical microcavity integrated with two quantum wells. The quantum wells support two exciton modes, which simultaneously couple to an optical cavity mode via a linear coupling, and the cavity mode also couples to a mechanical vibration mode via a dispersive optomechanical interaction, accounting for both the radiation pressure and the photoelastic effect. We show that by strongly driving the microcavity with a red-detuned laser field and when the two exciton modes are respectively resonant with the Stokes and anti-Stokes sidebands scattered by the mechanical motion, stationary entanglement between the two exciton modes can be established under currently available parameters. The entanglement is robust against various dissipations of the system and can be achieved at room temperature for a mechanical quality factor higher than $\\sim10^4$.","sentences":["Exciton optomechanics, bridging cavity exciton polaritons and optomechanics, opens new opportunities for the study of light-matter strong interactions and nonlinearities, due to the rich nonlinear couplings among excitons, phonons, and photons.","Here, we propose to entangle two exciton modes in an exciton-optomechanical system, which consists of a semiconductor optomechanical microcavity integrated with two quantum wells.","The quantum wells support two exciton modes, which simultaneously couple to an optical cavity mode via a linear coupling, and the cavity mode also couples to a mechanical vibration mode via a dispersive optomechanical interaction, accounting for both the radiation pressure and the photoelastic effect.","We show that by strongly driving the microcavity with a red-detuned laser field and when the two exciton modes are respectively resonant with the Stokes and anti-Stokes sidebands scattered by the mechanical motion, stationary entanglement between the two exciton modes can be established under currently available parameters.","The entanglement is robust against various dissipations of the system and can be achieved at room temperature for a mechanical quality factor higher than $\\sim10^4$."],"url":"http://arxiv.org/abs/2402.02710v1","category":"quant-ph"}
{"created":"2024-02-05 04:03:44","title":"Passive decoy-state quantum secure direct communication with heralded single-photon source","abstract":"Quantum secure direct communications (QSDC) can directly transmit secret messages through quantum channel without keys. The imperfect photon source is a major obstacle for QSDC's practical implementation. The unwanted vacuum state and multi-photon components emitted from imperfect photon source largely reduce QSDC's secrecy message capacity and even threaten its security. In the paper, we propose a high-efficient passive decoy-state QSDC protocol with the heralded single-photon source (HSPS). We adopt a spontaneous parametric down-conversion source to emit entangled photon pairs in two spatial modes. By detecting the photons in one of the two correlated spatial modes, we can infer the photon number distribution of the other spatial mode. Meanwhile, our protocol allows a simple passive preparation of the signal states and decoy state. The HSPS can effectively reduce the probability of vacuum state and increase QSDC's secrecy message capacity. Meanwhile, the passive decoy-state method can simplify the experimental operations and enhance QSDC's robustness against the third-party side-channel attacks. Under the communication distance of 10 km, the secrecy message capacity of our QSDC protocol can achieve 81.85 times (average photon number of 0.1) and 12.79 times (average photon number of 0.01) of that in the original single-photon-based QSDC protocol without the HSPS. Our QSDC protocol has longer maximal communication distance (about 17.975 km with average photon number of 0.01). Our work serves as a major step toward the further development of practical passive decoy-state QSDC systems.","sentences":["Quantum secure direct communications (QSDC) can directly transmit secret messages through quantum channel without keys.","The imperfect photon source is a major obstacle for QSDC's practical implementation.","The unwanted vacuum state and multi-photon components emitted from imperfect photon source largely reduce QSDC's secrecy message capacity and even threaten its security.","In the paper, we propose a high-efficient passive decoy-state QSDC protocol with the heralded single-photon source (HSPS).","We adopt a spontaneous parametric down-conversion source to emit entangled photon pairs in two spatial modes.","By detecting the photons in one of the two correlated spatial modes, we can infer the photon number distribution of the other spatial mode.","Meanwhile, our protocol allows a simple passive preparation of the signal states and decoy state.","The HSPS can effectively reduce the probability of vacuum state and increase QSDC's secrecy message capacity.","Meanwhile, the passive decoy-state method can simplify the experimental operations and enhance QSDC's robustness against the third-party side-channel attacks.","Under the communication distance of 10 km, the secrecy message capacity of our QSDC protocol can achieve 81.85 times (average photon number of 0.1) and 12.79 times (average photon number of 0.01) of that in the original single-photon-based QSDC protocol without the HSPS.","Our QSDC protocol has longer maximal communication distance (about 17.975 km with average photon number of 0.01).","Our work serves as a major step toward the further development of practical passive decoy-state QSDC systems."],"url":"http://arxiv.org/abs/2402.02709v1","category":"quant-ph"}
{"created":"2024-02-05 03:31:44","title":"Causal inference under transportability assumptions for conditional relative effect measures","abstract":"When extending inferences from a randomized trial to a new target population, an assumption of transportability of difference effect measures (e.g., conditional average treatment effects) -- or even stronger assumptions of transportability in expectation or distribution of potential outcomes -- is invoked to identify the marginal causal mean difference in the target population. However, many clinical investigators believe that relative effect measures conditional on covariates, such as conditional risk ratios and mean ratios, are more likely to be ``transportable'' across populations compared with difference effect measures. Here, we examine the identification and estimation of the marginal counterfactual mean difference and ratio under a transportability assumption for conditional relative effect measures. We obtain identification results for two scenarios that often arise in practice when individuals in the target population (1) only have access to the control treatment, or (2) have access to the control and other treatments but not necessarily the experimental treatment evaluated in the trial. We then propose multiply robust and nonparametric efficient estimators that allow for the use of data-adaptive methods (e.g., machine learning techniques) to model the nuisance parameters. We examine the performance of the methods in simulation studies and illustrate their use with data from two trials of paliperidone for patients with schizophrenia. We conclude that the proposed methods are attractive when background knowledge suggests that the transportability assumption for conditional relative effect measures is more plausible than alternative assumptions.","sentences":["When extending inferences from a randomized trial to a new target population, an assumption of transportability of difference effect measures (e.g., conditional average treatment effects) -- or even stronger assumptions of transportability in expectation or distribution of potential outcomes -- is invoked to identify the marginal causal mean difference in the target population.","However, many clinical investigators believe that relative effect measures conditional on covariates, such as conditional risk ratios and mean ratios, are more likely to be ``transportable'' across populations compared with difference effect measures.","Here, we examine the identification and estimation of the marginal counterfactual mean difference and ratio under a transportability assumption for conditional relative effect measures.","We obtain identification results for two scenarios that often arise in practice when individuals in the target population (1) only have access to the control treatment, or (2) have access to the control and other treatments but not necessarily the experimental treatment evaluated in the trial.","We then propose multiply robust and nonparametric efficient estimators that allow for the use of data-adaptive methods (e.g., machine learning techniques) to model the nuisance parameters.","We examine the performance of the methods in simulation studies and illustrate their use with data from two trials of paliperidone for patients with schizophrenia.","We conclude that the proposed methods are attractive when background knowledge suggests that the transportability assumption for conditional relative effect measures is more plausible than alternative assumptions."],"url":"http://arxiv.org/abs/2402.02702v1","category":"stat.ME"}
{"created":"2024-02-05 03:25:04","title":"Sample Complexity Characterization for Linear Contextual MDPs","abstract":"Contextual Markov decision processes (CMDPs) describe a class of reinforcement learning problems in which the transition kernels and reward functions can change over time with different MDPs indexed by a context variable. While CMDPs serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective. In this paper, we study CMDPs under two linear function approximation models: Model I with context-varying representations and common linear weights for all contexts; and Model II with common representations for all contexts and context-varying linear weights. For both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\\epsilon$-suboptimality gap with desired polynomial sample complexity. In particular, instantiating our result for the first model to the tabular CMDP improves the existing result by removing the reachability assumption. Our result for the second model is the first-known result for such a type of function approximation models. Comparison between our results for the two models further indicates that having context-varying features leads to much better sample efficiency than having common representations for all contexts under linear CMDPs.","sentences":["Contextual Markov decision processes (CMDPs) describe a class of reinforcement learning problems in which the transition kernels and reward functions can change over time with different MDPs indexed by a context variable.","While CMDPs serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective.","In this paper, we study CMDPs under two linear function approximation models: Model I with context-varying representations and common linear weights for all contexts; and Model II with common representations for all contexts and context-varying linear weights.","For both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\\epsilon$-suboptimality gap with desired polynomial sample complexity.","In particular, instantiating our result for the first model to the tabular CMDP improves the existing result by removing the reachability assumption.","Our result for the second model is the first-known result for such a type of function approximation models.","Comparison between our results for the two models further indicates that having context-varying features leads to much better sample efficiency than having common representations for all contexts under linear CMDPs."],"url":"http://arxiv.org/abs/2402.02700v1","category":"cs.LG"}
{"created":"2024-02-05 03:23:34","title":"Adversarial Data Augmentation for Robust Speaker Verification","abstract":"Data augmentation (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness. It enriches training data by simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization. However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation. To address this problem, this paper proposes a novel approach called adversarial data augmentation (A-DA) which combines DA with adversarial learning. Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in data augmentation. This adversarial learning empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of augmentation variations. Experiments conducted on VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms standard DA in both augmentation matched and mismatched test conditions, showcasing its superior robustness and generalization against acoustic variations.","sentences":["Data augmentation (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness.","It enriches training data by simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization.","However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation.","To address this problem, this paper proposes a novel approach called adversarial data augmentation (A-DA) which combines DA with adversarial learning.","Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in data augmentation.","This adversarial learning empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of augmentation variations.","Experiments conducted on VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms standard DA in both augmentation matched and mismatched test conditions, showcasing its superior robustness and generalization against acoustic variations."],"url":"http://arxiv.org/abs/2402.02699v1","category":"cs.SD"}
{"created":"2024-02-05 03:12:51","title":"Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift","abstract":"Acoustic scene classification (ASC) is a crucial research problem in computational auditory scene analysis, and it aims to recognize the unique acoustic characteristics of an environment. One of the challenges of the ASC task is domain shift caused by a distribution gap between training and testing data. Since 2018, ASC challenges have focused on the generalization of ASC models across different recording devices. Although this task in recent years has achieved substantial progress in device generalization, the challenge of domain shift between different regions, involving characteristics such as time, space, culture, and language, remains insufficiently explored at present. In addition, considering the abundance of unlabeled acoustic scene data in the real world, it is important to study the possible ways to utilize these unlabelled data. Therefore, we introduce the task Semi-supervised Acoustic Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We encourage participants to innovate with semi-supervised learning techniques, aiming to develop more robust ASC models under domain shift.","sentences":["Acoustic scene classification (ASC) is a crucial research problem in computational auditory scene analysis, and it aims to recognize the unique acoustic characteristics of an environment.","One of the challenges of the ASC task is domain shift caused by a distribution gap between training and testing data.","Since 2018, ASC challenges have focused on the generalization of ASC models across different recording devices.","Although this task in recent years has achieved substantial progress in device generalization, the challenge of domain shift between different regions, involving characteristics such as time, space, culture, and language, remains insufficiently explored at present.","In addition, considering the abundance of unlabeled acoustic scene data in the real world, it is important to study the possible ways to utilize these unlabelled data.","Therefore, we introduce the task Semi-supervised Acoustic Scene Classification under Domain Shift in the ICME 2024 Grand Challenge.","We encourage participants to innovate with semi-supervised learning techniques, aiming to develop more robust ASC models under domain shift."],"url":"http://arxiv.org/abs/2402.02694v1","category":"eess.AS"}
{"created":"2024-02-05 03:06:18","title":"On Recurrence Axioms","abstract":"The Recurrence Axiom for a class $\\mathcal{P}$ of \\pos\\ and a set $A$ of parameters is an axiom scheme in the language of ZFC asserting that if a statement with parameters from $A$ is forced by a poset in $\\mathcal{P}$, then there is a ground containing the parameters and satisfying the statement.   The tightly super-$C^{(\\infty)}$-$\\mathcal{P}$-Laver generic hyperhuge continuum implies the Recurrence Axiom for $\\mathcal{P}$ and $\\mathcal{H}(2^{\\aleph_0})$. The consistency strength of this assumption can be decided thanks to our main theorems asserting that the minimal ground (bedrock) exists under a tightly $\\mathcal{P}$-generic hyperhuge cardinal $\\kappa$, and that $\\kappa$ in the bedrock is genuinely hyperhuge, or even super $C^{(\\infty)}$ hyperhuge if $\\kappa$ is a tightly super-$C^{(\\infty)}$-$\\mathcal{P}$-Laver generic hyperhuge definable cardinal.   The Laver Generic Maximum (LGM), one of the strongest combinations of axioms in our context, integrates practically all known set-theoretic principles and axioms in itself, either as its consequences or as theorems holding in (many) grounds of the universe. For example, double plus version of Martin's Maximum is a consequence of LGM while Cicho\\'n's Maximum is a phenomenon in many grounds of the universe under LGM.","sentences":["The Recurrence Axiom for a class $\\mathcal{P}$ of \\pos\\ and a set $A$ of parameters is an axiom scheme in the language of ZFC asserting that if a statement with parameters from $A$ is forced by a poset in $\\mathcal{P}$, then there is a ground containing the parameters and satisfying the statement.   ","The tightly super-$C^{(\\infty)}$-$\\mathcal{P}$-Laver generic hyperhuge continuum implies the Recurrence Axiom for $\\mathcal{P}$ and $\\mathcal{H}(2^{\\aleph_0})$. The consistency strength of this assumption can be decided thanks to our main theorems asserting that the minimal ground (bedrock) exists under a tightly $\\mathcal{P}$-generic hyperhuge cardinal $\\kappa$, and that $\\kappa$ in the bedrock is genuinely hyperhuge, or even super $C^{(\\infty)}$ hyperhuge if $\\kappa$ is a tightly super-$C^{(\\infty)}$-$\\mathcal{P}$-Laver generic hyperhuge definable cardinal.   ","The Laver Generic Maximum (LGM), one of the strongest combinations of axioms in our context, integrates practically all known set-theoretic principles and axioms in itself, either as its consequences or as theorems holding in (many) grounds of the universe.","For example, double plus version of Martin's Maximum is a consequence of LGM while Cicho\\'n's Maximum is a phenomenon in many grounds of the universe under LGM."],"url":"http://arxiv.org/abs/2402.02693v1","category":"math.LO"}
{"created":"2024-02-05 02:58:51","title":"Competitive Equilibrium in Microgrids With Dynamic Loads","abstract":"In this paper, we consider microgrids that interconnect prosumers with distributed energy resources and dynamic loads. Prosumers are connected through the microgrid to trade energy and gain profit while respecting the network constraints. We establish a local energy market by defining a competitive equilibrium which balances energy and satisfies voltage constraints within the microgrid for all time. Using duality theory, we prove that under some convexity assumptions, a competitive equilibrium is equivalent to a social welfare maximization solution. Additionally, we show that a competitive equilibrium is equivalent to a Nash equilibrium of a standard game. In general, the energy price for each prosumer is different, leading to the concept of locational prices. We investigate a case under which all prosumers have the same locational prices. Additionally, we show that under some assumptions on the resource supply and network topology, locational prices decay to zero after a period of time, implying the available supply will be more than the demand required to stabilize the system. Finally, two numerical examples are provided to validate the results, one of which is a direct application of our results on electric vehicle charging control.","sentences":["In this paper, we consider microgrids that interconnect prosumers with distributed energy resources and dynamic loads.","Prosumers are connected through the microgrid to trade energy and gain profit while respecting the network constraints.","We establish a local energy market by defining a competitive equilibrium which balances energy and satisfies voltage constraints within the microgrid for all time.","Using duality theory, we prove that under some convexity assumptions, a competitive equilibrium is equivalent to a social welfare maximization solution.","Additionally, we show that a competitive equilibrium is equivalent to a Nash equilibrium of a standard game.","In general, the energy price for each prosumer is different, leading to the concept of locational prices.","We investigate a case under which all prosumers have the same locational prices.","Additionally, we show that under some assumptions on the resource supply and network topology, locational prices decay to zero after a period of time, implying the available supply will be more than the demand required to stabilize the system.","Finally, two numerical examples are provided to validate the results, one of which is a direct application of our results on electric vehicle charging control."],"url":"http://arxiv.org/abs/2402.02690v1","category":"eess.SY"}
{"created":"2024-02-05 02:54:55","title":"Successive Bayesian Reconstructor for FAS Channel Estimation","abstract":"Fluid antenna systems (FASs) can reconfigure their locations freely within a spatially continuous space. To keep favorable antenna positions, the channel state information (CSI) acquisition for FASs is essential. While some techniques have been proposed, most existing FAS channel estimators require several channel assumptions, such as slow variation and angular-domain sparsity. When these assumptions are not reasonable, the model mismatch may lead to unpredictable performance loss. In this paper, we propose the successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS channels. Unlike model-based estimators, the proposed S-BAR is prior-aided, which builds the experiential kernel for CSI acquisition. Inspired by Bayesian regression, the key idea of S-BAR is to model the FAS channels as a stochastic process, whose uncertainty can be successively eliminated by kernel-based sampling and regression. In this way, the predictive mean of the regressed stochastic process can be viewed as the maximum a posterior (MAP) estimator of FAS channels. Simulation results verify that, in both model-mismatched and model-matched cases, the proposed S-BAR can achieve higher estimation accuracy than the existing schemes.","sentences":["Fluid antenna systems (FASs) can reconfigure their locations freely within a spatially continuous space.","To keep favorable antenna positions, the channel state information (CSI) acquisition for FASs is essential.","While some techniques have been proposed, most existing FAS channel estimators require several channel assumptions, such as slow variation and angular-domain sparsity.","When these assumptions are not reasonable, the model mismatch may lead to unpredictable performance loss.","In this paper, we propose the successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS channels.","Unlike model-based estimators, the proposed S-BAR is prior-aided, which builds the experiential kernel for CSI acquisition.","Inspired by Bayesian regression, the key idea of S-BAR is to model the FAS channels as a stochastic process, whose uncertainty can be successively eliminated by kernel-based sampling and regression.","In this way, the predictive mean of the regressed stochastic process can be viewed as the maximum a posterior (MAP) estimator of FAS channels.","Simulation results verify that, in both model-mismatched and model-matched cases, the proposed S-BAR can achieve higher estimation accuracy than the existing schemes."],"url":"http://arxiv.org/abs/2402.02688v1","category":"cs.IT"}
{"created":"2024-02-05 02:48:01","title":"Intrinsic nonlinear Hall effect in two-dimensional honeycomb topological antiferromagnets","abstract":"Two-dimensional systems with honeycomb lattice are known to be a paradigmatic platform to explore the various types of Hall effects, owing to that the interplay of lattice geometry, spin-orbit coupling and magnetism can give rise to very rich features in the quantum geometry of wave functions. In this work, we consider honeycomb topological antiferromagets that are effectively described by a $\\mathcal{PT}$-symmetric antiferromagnetic Kane-Mele model, and explore the evolution of its nonlinear Hall response with respect to the change of lattice anisotropy, chemical potential, and the direction of the N\\'{e}el vector. Due to the $\\mathcal{PT}$-symmetry, the leading-order Hall effect of quantum geometric origin is the intrinsic nonlinear Hall effect, which is a second-order effect of electric fields and is independent of the scattering time. We investigate the behavior of the intrinsic nonlinear Hall conductivity tensor across topological phase transitions driven by antiferromagnetic exchange field and lattice anisotropy and find that its components do not change sign, which is different from the extrinsic nonlinear Hall effect. In the weakly doped regime, we find that the intrinsic nonlinear Hall effect is valley-polarized. By varying the chemical potential, we find that the nonlinear Hall conductivity tensors exhibit kinks when the Fermi surface undergoes Lifshitz transitions. Furthermore, we find that the existence of spin-orbit coupling to lift the spin-rotation symmetry is decisive for the use of intrinsic nonlinear Hall effect to detect the direction of the N\\'{e}el vector. Our work shows that the two-dimensional honeycomb topological antiferromagnets are an ideal class of material systems with rich properties for the study of intrinsic nonlinear Hall effect.","sentences":["Two-dimensional systems with honeycomb lattice are known to be a paradigmatic platform to explore the various types of Hall effects, owing to that the interplay of lattice geometry, spin-orbit coupling and magnetism can give rise to very rich features in the quantum geometry of wave functions.","In this work, we consider honeycomb topological antiferromagets that are effectively described by a $\\mathcal{PT}$-symmetric antiferromagnetic Kane-Mele model, and explore the evolution of its nonlinear Hall response with respect to the change of lattice anisotropy, chemical potential, and the direction of the N\\'{e}el vector.","Due to the $\\mathcal{PT}$-symmetry, the leading-order Hall effect of quantum geometric origin is the intrinsic nonlinear Hall effect, which is a second-order effect of electric fields and is independent of the scattering time.","We investigate the behavior of the intrinsic nonlinear Hall conductivity tensor across topological phase transitions driven by antiferromagnetic exchange field and lattice anisotropy and find that its components do not change sign, which is different from the extrinsic nonlinear Hall effect.","In the weakly doped regime, we find that the intrinsic nonlinear Hall effect is valley-polarized.","By varying the chemical potential, we find that the nonlinear Hall conductivity tensors exhibit kinks when the Fermi surface undergoes Lifshitz transitions.","Furthermore, we find that the existence of spin-orbit coupling to lift the spin-rotation symmetry is decisive for the use of intrinsic nonlinear Hall effect to detect the direction of the N\\'{e}el vector.","Our work shows that the two-dimensional honeycomb topological antiferromagnets are an ideal class of material systems with rich properties for the study of intrinsic nonlinear Hall effect."],"url":"http://arxiv.org/abs/2402.02685v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 02:43:18","title":"Efficient estimation of subgroup treatment effects using multi-source data","abstract":"Investigators often use multi-source data (e.g., multi-center trials, meta-analyses of randomized trials, pooled analyses of observational cohorts) to learn about the effects of interventions in subgroups of some well-defined target population. Such a target population can correspond to one of the data sources of the multi-source data or an external population in which the treatment and outcome information may not be available. We develop and evaluate methods for using multi-source data to estimate subgroup potential outcome means and treatment effects in a target population. We consider identifiability conditions and propose doubly robust estimators that, under mild conditions, are non-parametrically efficient and allow for nuisance functions to be estimated using flexible data-adaptive methods (e.g., machine learning techniques). We also show how to construct confidence intervals and simultaneous confidence bands for the estimated subgroup treatment effects. We examine the properties of the proposed estimators in simulation studies and compare performance against alternative estimators. We also conclude that our methods work well when the sample size of the target population is much larger than the sample size of the multi-source data. We illustrate the proposed methods in a meta-analysis of randomized trials for schizophrenia.","sentences":["Investigators often use multi-source data (e.g., multi-center trials, meta-analyses of randomized trials, pooled analyses of observational cohorts) to learn about the effects of interventions in subgroups of some well-defined target population.","Such a target population can correspond to one of the data sources of the multi-source data or an external population in which the treatment and outcome information may not be available.","We develop and evaluate methods for using multi-source data to estimate subgroup potential outcome means and treatment effects in a target population.","We consider identifiability conditions and propose doubly robust estimators that, under mild conditions, are non-parametrically efficient and allow for nuisance functions to be estimated using flexible data-adaptive methods (e.g., machine learning techniques).","We also show how to construct confidence intervals and simultaneous confidence bands for the estimated subgroup treatment effects.","We examine the properties of the proposed estimators in simulation studies and compare performance against alternative estimators.","We also conclude that our methods work well when the sample size of the target population is much larger than the sample size of the multi-source data.","We illustrate the proposed methods in a meta-analysis of randomized trials for schizophrenia."],"url":"http://arxiv.org/abs/2402.02684v1","category":"stat.ME"}
{"created":"2024-02-05 02:42:08","title":"Holographic Thermal Mapping Using Acoustic Lenses","abstract":"Acoustic holographic lenses (AHLs) show great potential for sound manipulation. These lenses store the phase and amplitude profile of the desired wavefront when illuminated by a single acoustic source to reconstruct focused ultrasound (FUS) pressure fields, induce localized heating, and achieve temporal and spatial thermal effects in acousto-thermal materials like polymers. The ultrasonic energy is transmitted and focused by AHL from a transducer into a particular focal volume. It is then converted to heat by internal friction in the polymer chains, causing the temperature of the polymer to rise at the focus locations while having little to no effect elsewhere. This one-of-a-kind capability is made possible by the development of AHLs to make use of the translation of attenuated pressure fields into programmable heat patterns. However, the acousto-thermal dynamics of AHLs are largely unexplored. We use a machine learning-assisted single inverse problem approach for rapid and efficient AHL designs. The process involves the conversion of thermal information into a holographic representation through the utilization of two latent functions; pressure phase and amplitude. Experimental verification is performed for pressure and thermal measurements. The volumetric acousto-thermal analysis of experimental samples is performed to offer knowledge of the obtained pattern dynamics, as well as the applicability of holographic FUS thermal mapping for precise temperature control in complex volumes of heterogeneous media. The proposed framework provides a solid foundation for anticipating and assessing thermal changes within materials using only outer surface measurements since it can correlate with surface temperature data alone.","sentences":["Acoustic holographic lenses (AHLs) show great potential for sound manipulation.","These lenses store the phase and amplitude profile of the desired wavefront when illuminated by a single acoustic source to reconstruct focused ultrasound (FUS) pressure fields, induce localized heating, and achieve temporal and spatial thermal effects in acousto-thermal materials like polymers.","The ultrasonic energy is transmitted and focused by AHL from a transducer into a particular focal volume.","It is then converted to heat by internal friction in the polymer chains, causing the temperature of the polymer to rise at the focus locations while having little to no effect elsewhere.","This one-of-a-kind capability is made possible by the development of AHLs to make use of the translation of attenuated pressure fields into programmable heat patterns.","However, the acousto-thermal dynamics of AHLs are largely unexplored.","We use a machine learning-assisted single inverse problem approach for rapid and efficient AHL designs.","The process involves the conversion of thermal information into a holographic representation through the utilization of two latent functions; pressure phase and amplitude.","Experimental verification is performed for pressure and thermal measurements.","The volumetric acousto-thermal analysis of experimental samples is performed to offer knowledge of the obtained pattern dynamics, as well as the applicability of holographic FUS thermal mapping for precise temperature control in complex volumes of heterogeneous media.","The proposed framework provides a solid foundation for anticipating and assessing thermal changes within materials using only outer surface measurements since it can correlate with surface temperature data alone."],"url":"http://arxiv.org/abs/2402.02682v1","category":"physics.app-ph"}
{"created":"2024-02-05 02:28:07","title":"Near-IR clumps and their properties in high-z galaxies with JWST/NIRCam","abstract":"Resolved stellar morphology of z>1 galaxies was inaccessible before JWST. This limitation, due to the impact of dust on rest-frame UV light, had withheld major observational conclusions required to understand the importance of clumps in galaxy evolution. Essentially independent of this issue, we use the rest-frame near-IR for a stellar-mass dependent clump detection method and determine reliable estimations of selection effects. We exploit publicly available JWST/NIRCam and HST/ACS imaging data from CEERS, to create a stellar-mass based picture of clumps in a mass-complete sample of 418 galaxies within a wide wavelength coverage of 0.5-4.6${\\mu}$m and a redshift window of 1<z<2. We find that a near-IR detection gives access to a larger set of clumps within galaxies, with those also detected in UV making up only 28%. Whereas, 85% of the UV clumps are found to have a near-IR counterpart. These near-IR clumps closely follow the UVJ classification of their respective host galaxies, with these hosts mainly populating the star-forming regime besides a fraction of them (16%) that can be considered quiescent. The mass of the detected clumps are found to be within the range of $10^{7.5-9.5}\\,\\rm M_{\\odot}$, therefore expected to drive gas into galaxy cores through tidal torques. However, there is likely contribution from blending of smaller unresolved structures. Furthermore, we observe a radial gradient of increasing clump mass towards the centre of galaxies. This trend could be an indication of clump migration, but accurate star-formation measurements would be required to confirm such a scenario.","sentences":["Resolved stellar morphology of z>1 galaxies was inaccessible before JWST.","This limitation, due to the impact of dust on rest-frame UV light, had withheld major observational conclusions required to understand the importance of clumps in galaxy evolution.","Essentially independent of this issue, we use the rest-frame near-IR for a stellar-mass dependent clump detection method and determine reliable estimations of selection effects.","We exploit publicly available JWST/NIRCam and HST/ACS imaging data from CEERS, to create a stellar-mass based picture of clumps in a mass-complete sample of 418 galaxies within a wide wavelength coverage of 0.5-4.6${\\mu}$m and a redshift window of 1<z<2.","We find that a near-IR detection gives access to a larger set of clumps within galaxies, with those also detected in UV making up only 28%.","Whereas, 85% of the UV clumps are found to have a near-IR counterpart.","These near-IR clumps closely follow the UVJ classification of their respective host galaxies, with these hosts mainly populating the star-forming regime besides a fraction of them (16%) that can be considered quiescent.","The mass of the detected clumps are found to be within the range of $10^{7.5-9.5}\\,\\rm M_{\\odot}$, therefore expected to drive gas into galaxy cores through tidal torques.","However, there is likely contribution from blending of smaller unresolved structures.","Furthermore, we observe a radial gradient of increasing clump mass towards the centre of galaxies.","This trend could be an indication of clump migration, but accurate star-formation measurements would be required to confirm such a scenario."],"url":"http://arxiv.org/abs/2402.02679v1","category":"astro-ph.GA"}
{"created":"2024-02-05 02:26:00","title":"The Gig's Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights","abstract":"Generative AI is changing the way in which humans seek to find answers to questions in different fields including on the gig economy and labour markets, but there is limited information available about closely ChatGPT simulated output matches that obtainable from existing question and answer platforms. This paper uses ChatGPT as a research assistant to explore how far ChatGPT can replicate Quora question and answers, using data from the gig economy as an indicative case study. The results from content analysis suggest that Quora is likely to be asked questions from users looking to make money and answers are likely to include personal experiences and examples. ChatGPT simulated versions are less personal and more concept-based, including considerations on employment implications and labour rights. It appears therefore that generative AI simulates only part of what a human would want in their answers relating to the gig economy. The paper proposes that a similar comparative methodology would also be useful across other research fields to help in establishing the best real world uses of generative AI.","sentences":["Generative AI is changing the way in which humans seek to find answers to questions in different fields including on the gig economy and labour markets, but there is limited information available about closely ChatGPT simulated output matches that obtainable from existing question and answer platforms.","This paper uses ChatGPT as a research assistant to explore how far ChatGPT can replicate Quora question and answers, using data from the gig economy as an indicative case study.","The results from content analysis suggest that Quora is likely to be asked questions from users looking to make money and answers are likely to include personal experiences and examples.","ChatGPT simulated versions are less personal and more concept-based, including considerations on employment implications and labour rights.","It appears therefore that generative AI simulates only part of what a human would want in their answers relating to the gig economy.","The paper proposes that a similar comparative methodology would also be useful across other research fields to help in establishing the best real world uses of generative AI."],"url":"http://arxiv.org/abs/2402.02676v1","category":"cs.CY"}
{"created":"2024-02-05 01:55:52","title":"3D NLTE Lithium abundances for late-type stars in GALAH DR3","abstract":"Lithium's susceptibility to burning in stellar interiors makes it an invaluable tracer for delineating the evolutionary pathways of stars, offering insights into the processes governing their development. Observationally, the complex Li production and depletion mechanisms in stars manifest themselves as Li plateaus, and as Li-enhanced and Li-depleted regions of the HR diagram. The Li-dip represents a narrow range in effective temperature close to the main-sequence turn-off, where stars have slightly super-solar masses and strongly depleted Li. To study the modification of Li through stellar evolution, we measure 3D non-local thermodynamic equilibrium (NLTE) Li abundance for 581 149 stars released in GALAH DR3. We describe a novel method that fits the observed spectra using a combination of 3D NLTE Li line profiles with blending metal line strength that are optimized on a star-by-star basis. Furthermore, realistic errors are determined by a Monte Carlo nested sampling algorithm which samples the posterior distribution of the fitted spectral parameters. The method is validated by recovering parameters from a synthetic spectrum and comparing to 26 stars in the Hypatia catalogue. We find 228 613 Li detections, and 352 536 Li upper limits. Our abundance measurements are generally lower than GALAH DR3, with a mean difference of 0.23 dex. For the first time, we trace the evolution of Li-dip stars beyond the main sequence turn-off and up the subgiant branch. This is the first 3D NLTE analysis of Li applied to a large spectroscopic survey, and opens up a new era of precision analysis of abundances for large surveys.","sentences":["Lithium's susceptibility to burning in stellar interiors makes it an invaluable tracer for delineating the evolutionary pathways of stars, offering insights into the processes governing their development.","Observationally, the complex Li production and depletion mechanisms in stars manifest themselves as Li plateaus, and as Li-enhanced and Li-depleted regions of the HR diagram.","The Li-dip represents a narrow range in effective temperature close to the main-sequence turn-off, where stars have slightly super-solar masses and strongly depleted Li.","To study the modification of Li through stellar evolution, we measure 3D non-local thermodynamic equilibrium (NLTE)","Li abundance for 581 149 stars released in GALAH DR3.","We describe a novel method that fits the observed spectra using a combination of 3D NLTE Li line profiles with blending metal line strength that are optimized on a star-by-star basis.","Furthermore, realistic errors are determined by a Monte Carlo nested sampling algorithm which samples the posterior distribution of the fitted spectral parameters.","The method is validated by recovering parameters from a synthetic spectrum and comparing to 26 stars in the Hypatia catalogue.","We find 228 613 Li detections, and 352 536 Li upper limits.","Our abundance measurements are generally lower than GALAH DR3, with a mean difference of 0.23 dex.","For the first time, we trace the evolution of Li-dip stars beyond the main sequence turn-off and up the subgiant branch.","This is the first 3D NLTE analysis of Li applied to a large spectroscopic survey, and opens up a new era of precision analysis of abundances for large surveys."],"url":"http://arxiv.org/abs/2402.02669v1","category":"astro-ph.SR"}
{"created":"2024-02-05 01:42:28","title":"Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning","abstract":"Research in multi-objective reinforcement learning (MORL) has introduced the utility-based paradigm, which makes use of both environmental rewards and a function that defines the utility derived by the user from those rewards. In this paper we extend this paradigm to the context of single-objective reinforcement learning (RL), and outline multiple potential benefits including the ability to perform multi-policy learning across tasks relating to uncertain objectives, risk-aware RL, discounting, and safe RL. We also examine the algorithmic implications of adopting a utility-based approach.","sentences":["Research in multi-objective reinforcement learning (MORL) has introduced the utility-based paradigm, which makes use of both environmental rewards and a function that defines the utility derived by the user from those rewards.","In this paper we extend this paradigm to the context of single-objective reinforcement learning (RL), and outline multiple potential benefits including the ability to perform multi-policy learning across tasks relating to uncertain objectives, risk-aware RL, discounting, and safe RL.","We also examine the algorithmic implications of adopting a utility-based approach."],"url":"http://arxiv.org/abs/2402.02665v1","category":"cs.LG"}
{"created":"2024-02-05 01:31:03","title":"Statistical Inference for Generalized Integer Autoregressive Processes","abstract":"A popular and flexible time series model for counts is the generalized integer autoregressive process of order $p$, GINAR($p$). These Markov processes are defined using thinning operators evaluated on past values of the process along with a discretely-valued innovation process. This class includes the commonly used INAR($p$) process, defined with binomial thinning and Poisson innovations. GINAR processes can be used in a variety of settings, including modeling time series with low counts, and allow for more general mean-variance relationships, capturing both over- or under-dispersion. While there are many thinning operators and innovation processes given in the literature, less focus has been spent on comparing statistical inference and forecasting procedures over different choices of GINAR process. We provide an extensive study of exact and approximate inference and forecasting methods that can be applied to a wide class of GINAR($p$) processes with general thinning and innovation parameters. We discuss the challenges of exact estimation when $p$ is larger. We summarize and extend asymptotic results for estimators of process parameters, and present simulations to compare small sample performance, highlighting how different methods compare. We illustrate this methodology by fitting GINAR processes to a disease surveillance series.","sentences":["A popular and flexible time series model for counts is the generalized integer autoregressive process of order $p$, GINAR($p$).","These Markov processes are defined using thinning operators evaluated on past values of the process along with a discretely-valued innovation process.","This class includes the commonly used INAR($p$) process, defined with binomial thinning and Poisson innovations.","GINAR processes can be used in a variety of settings, including modeling time series with low counts, and allow for more general mean-variance relationships, capturing both over- or under-dispersion.","While there are many thinning operators and innovation processes given in the literature, less focus has been spent on comparing statistical inference and forecasting procedures over different choices of GINAR process.","We provide an extensive study of exact and approximate inference and forecasting methods that can be applied to a wide class of GINAR($p$) processes with general thinning and innovation parameters.","We discuss the challenges of exact estimation when $p$ is larger.","We summarize and extend asymptotic results for estimators of process parameters, and present simulations to compare small sample performance, highlighting how different methods compare.","We illustrate this methodology by fitting GINAR processes to a disease surveillance series."],"url":"http://arxiv.org/abs/2402.02664v1","category":"stat.ME"}
{"created":"2024-02-05 00:57:12","title":"Two-dimensional topological effect in a transmon qubit array with tunable couplings","abstract":"We investigate a square-lattice architecture of superconducting transmon qubits with inter-qubit interactions mediated by inductive couplers. Therein, the inductive couling between the qubit and couplers is suggested to be designed into the gradiometer form to intigimate the flux noise orginating from the environment. Via periodically modulating the couplers,the Abelian gauge potential, termed effective magnetic flux, can be synthesized artificially, making the system an excellent platform for simulating two-dimensional topological physics. In the simplest two-dimensional model, the double (or three-leg) ladder, the staggered vortex-Meissner phase transition different from that in the two-leg ladder can be found in the single-particle ground state as the effective magnetic flux varies. Besides, the large coupling ratio between the interleg and intraleg coupling strengths also makes the chiral current resemble squeezed sinusoidal functions. If the row number is further increased, the topological band structure anticipated at massive rows begins to occur even for a relatively small number of rows (ten or so for the considered parameters). This heralds a small circuit scale to observe the topological band. The edge state in the band gap is determined by the topological Chern number and can be calculated through integrating the Berry curvature with respect to the first Brillouin zone. Besides, we present a systematic method on how to measure the topological band structure based on time- and space-domain Frourier transformation of the wave function after properly excited. The result offers an avenue for simulating two-dimensional topological physics on the state-of-the-art superconducting quantum chips.","sentences":["We investigate a square-lattice architecture of superconducting transmon qubits with inter-qubit interactions mediated by inductive couplers.","Therein, the inductive couling between the qubit and couplers is suggested to be designed into the gradiometer form to intigimate the flux noise orginating from the environment.","Via periodically modulating the couplers,the Abelian gauge potential, termed effective magnetic flux, can be synthesized artificially, making the system an excellent platform for simulating two-dimensional topological physics.","In the simplest two-dimensional model, the double (or three-leg) ladder, the staggered vortex-Meissner phase transition different from that in the two-leg ladder can be found in the single-particle ground state as the effective magnetic flux varies.","Besides, the large coupling ratio between the interleg and intraleg coupling strengths also makes the chiral current resemble squeezed sinusoidal functions.","If the row number is further increased, the topological band structure anticipated at massive rows begins to occur even for a relatively small number of rows (ten or so for the considered parameters).","This heralds a small circuit scale to observe the topological band.","The edge state in the band gap is determined by the topological Chern number and can be calculated through integrating the Berry curvature with respect to the first Brillouin zone.","Besides, we present a systematic method on how to measure the topological band structure based on time- and space-domain Frourier transformation of the wave function after properly excited.","The result offers an avenue for simulating two-dimensional topological physics on the state-of-the-art superconducting quantum chips."],"url":"http://arxiv.org/abs/2402.02657v1","category":"quant-ph"}
{"created":"2024-02-05 00:51:07","title":"Second-order charge and spin transport in LaO/STO system in the presence of cubic Rashba spin orbit couplings","abstract":"Certain non-centrosymmetric materials with broken time-reversal symmetry may exhibit non-reciprocal transport behavior under an applied electric field in which the charge and spin currents contain components that are second order in the electric field. In this study, we investigate the second-order spin accumulation and charge and spin responses in the LaAlO$_3$/SrTiO$_3$ (LaO/STO) system with magnetic dopants under the influence of linear and cubic Rashba spin-orbit coupling (RSOC) terms. We explain the physical origin of the second-order response and perform a symmetry analysis of the first and second-order responses for different dopant magnetization directions relative to the applied electric field. We then numerically solve the Boltzmann transport equation by extending the approach of Schliemann and Loss [Phys. Rev. B 68, 165311] to higher orders in the electric field. We show that the sign of the second-order responses can be switched by varying the magnetization direction of the magnetic dopants or relative strengths of the two cubic RSOC terms and explain these trends by considering the Fermi surfaces of the respective systems. These findings provide insights into the interplay of multiple SOC effects in a LaO/STO system and how the resulting first- and second-order charge and spin responses can be engineered by exploiting the symmetries of the system.","sentences":["Certain non-centrosymmetric materials with broken time-reversal symmetry may exhibit non-reciprocal transport behavior under an applied electric field in which the charge and spin currents contain components that are second order in the electric field.","In this study, we investigate the second-order spin accumulation and charge and spin responses in the LaAlO$_3$/SrTiO$_3$ (LaO/STO) system with magnetic dopants under the influence of linear and cubic Rashba spin-orbit coupling (RSOC) terms.","We explain the physical origin of the second-order response and perform a symmetry analysis of the first and second-order responses for different dopant magnetization directions relative to the applied electric field.","We then numerically solve the Boltzmann transport equation by extending the approach of Schliemann and Loss [Phys. Rev. B 68, 165311] to higher orders in the electric field.","We show that the sign of the second-order responses can be switched by varying the magnetization direction of the magnetic dopants or relative strengths of the two cubic RSOC terms and explain these trends by considering the Fermi surfaces of the respective systems.","These findings provide insights into the interplay of multiple SOC effects in a LaO/STO system and how the resulting first- and second-order charge and spin responses can be engineered by exploiting the symmetries of the system."],"url":"http://arxiv.org/abs/2402.02652v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 00:36:20","title":"A multi-objective optimization framework for reducing the impact of ship noise on marine mammals","abstract":"The underwater radiated noise (URN) emanating from ships presents a significant threat to marine mammals, given their heavy reliance on hearing for essential life activities. The intensity of URN from ships is directly correlated to the speed, making speed reduction a crucial operational mitigation strategy. This paper presents a new multi-objective optimization framework to optimize the ship speed for effective URN mitigation without compromising fuel consumption. The proposed framework addresses a fixed-path voyage scheduling problem, incorporating two objective functions namely (i) noise intensity levels and (ii) fuel consumption. The optimization is performed using the state-of-the-art non-dominated sorting genetic algorithm under voyage constraints. A 2D ocean acoustic environment, comprising randomly scattered marine mammals of diverse audiogram groups and realistic conditions, including sound speed profiles and bathymetry, is simulated. To estimate the objective functions, we consider empirical relations for fuel consumption and near-field noise modeling together with a ray-tracing approach for far-field noise propagation. The optimization problem is solved using the genetic algorithm to determine the Pareto solutions and subsequently the trade-off solution. The effectiveness of the optimization framework is demonstrated via both simplified tests and practical case studies involving a large container ship. A comparative analysis illustrates the adaptability of the optimization framework across different oceanic environments, affirming its potential as a robust tool for reducing the URN from shipping.","sentences":["The underwater radiated noise (URN) emanating from ships presents a significant threat to marine mammals, given their heavy reliance on hearing for essential life activities.","The intensity of URN from ships is directly correlated to the speed, making speed reduction a crucial operational mitigation strategy.","This paper presents a new multi-objective optimization framework to optimize the ship speed for effective URN mitigation without compromising fuel consumption.","The proposed framework addresses a fixed-path voyage scheduling problem, incorporating two objective functions namely (i) noise intensity levels and (ii) fuel consumption.","The optimization is performed using the state-of-the-art non-dominated sorting genetic algorithm under voyage constraints.","A 2D ocean acoustic environment, comprising randomly scattered marine mammals of diverse audiogram groups and realistic conditions, including sound speed profiles and bathymetry, is simulated.","To estimate the objective functions, we consider empirical relations for fuel consumption and near-field noise modeling together with a ray-tracing approach for far-field noise propagation.","The optimization problem is solved using the genetic algorithm to determine the Pareto solutions and subsequently the trade-off solution.","The effectiveness of the optimization framework is demonstrated via both simplified tests and practical case studies involving a large container ship.","A comparative analysis illustrates the adaptability of the optimization framework across different oceanic environments, affirming its potential as a robust tool for reducing the URN from shipping."],"url":"http://arxiv.org/abs/2402.02647v1","category":"math.OC"}
{"created":"2024-02-04 23:51:04","title":"Variational DAG Estimation via State Augmentation With Stochastic Permutations","abstract":"Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and real datasets.","sentences":["Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery.","Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues.","From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space.","We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations.","We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions.","We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and real datasets."],"url":"http://arxiv.org/abs/2402.02644v1","category":"cs.LG"}
{"created":"2024-02-04 23:23:51","title":"It's how you do things that matters\": Attending to Process to Better Serve Indigenous Communities with Language Technologies","abstract":"Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages. This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities. We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia. Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts.","sentences":["Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages.","This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities.","We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia.","Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts."],"url":"http://arxiv.org/abs/2402.02639v1","category":"cs.CL"}
{"created":"2024-02-04 23:00:24","title":"Key-Graph Transformer for Image Restoration","abstract":"While it is crucial to capture global information for effective image restoration (IR), integrating such cues into transformer-based methods becomes computationally expensive, especially with high input resolution. Furthermore, the self-attention mechanism in transformers is prone to considering unnecessary global cues from unrelated objects or regions, introducing computational inefficiencies. In response to these challenges, we introduce the Key-Graph Transformer (KGT) in this paper. Specifically, KGT views patch features as graph nodes. The proposed Key-Graph Constructor efficiently forms a sparse yet representative Key-Graph by selectively connecting essential nodes instead of all the nodes. Then the proposed Key-Graph Attention is conducted under the guidance of the Key-Graph only among selected nodes with linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed KGT's state-of-the-art performance, showcasing advancements both quantitatively and qualitatively.","sentences":["While it is crucial to capture global information for effective image restoration (IR), integrating such cues into transformer-based methods becomes computationally expensive, especially with high input resolution.","Furthermore, the self-attention mechanism in transformers is prone to considering unnecessary global cues from unrelated objects or regions, introducing computational inefficiencies.","In response to these challenges, we introduce the Key-Graph Transformer (KGT) in this paper.","Specifically, KGT views patch features as graph nodes.","The proposed Key-Graph Constructor efficiently forms a sparse yet representative Key-Graph by selectively connecting essential nodes instead of all the nodes.","Then the proposed Key-Graph Attention is conducted under the guidance of the Key-Graph only among selected nodes with linear computational complexity within each window.","Extensive experiments across 6 IR tasks confirm the proposed KGT's state-of-the-art performance, showcasing advancements both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2402.02634v1","category":"cs.CV"}
{"created":"2024-02-04 22:47:34","title":"Learning to Understand: Identifying Interactions via the Mobius Transform","abstract":"One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn. The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables. The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance. This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs. When there are $K = O(2^{n \\delta})$ with $\\delta \\leq \\frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \\rightarrow \\infty$, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group testing and the Mobius transform. In the case where all interactions are between at most $t = \\Theta(n^{\\alpha})$ inputs, for $\\alpha < 0.409$, we are able to leverage results from group testing to provide the first algorithm that computes the Mobius transform in $O(Kt\\log n)$ sample complexity and $O(K\\mathrm{poly}(n))$ time with vanishing error as $K \\rightarrow \\infty$. Finally, we present a robust version of this algorithm that achieves the same sample and time complexity under some assumptions, but with a factor depending on noise variance. Our work is deeply interdisciplinary, drawing from tools spanning across signal processing, algebra, information theory, learning theory and group testing to address this important problem at the forefront of machine learning.","sentences":["One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn.","The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables.","The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance.","This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs.","When there are $K = O(2^{n \\delta})$ with $\\delta \\leq \\frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \\rightarrow \\infty$, the first non-adaptive algorithm to do so.","We also uncover a surprising connection between group testing and the Mobius transform.","In the case where all interactions are between at most $t = \\Theta(n^{\\alpha})$ inputs, for $\\alpha < 0.409$, we are able to leverage results from group testing to provide the first algorithm that computes the Mobius transform in $O(Kt\\log n)$ sample complexity and $O(K\\mathrm{poly}(n))$ time with vanishing error as $K \\rightarrow \\infty$. Finally, we present a robust version of this algorithm that achieves the same sample and time complexity under some assumptions, but with a factor depending on noise variance.","Our work is deeply interdisciplinary, drawing from tools spanning across signal processing, algebra, information theory, learning theory and group testing to address this important problem at the forefront of machine learning."],"url":"http://arxiv.org/abs/2402.02631v1","category":"cs.LG"}
{"created":"2024-02-04 22:45:20","title":"PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial Attacks","abstract":"It is widely known that state-of-the-art machine learning models, including vision and language models, can be seriously compromised by adversarial perturbations. It is therefore increasingly relevant to develop capabilities to certify their performance in the presence of the most effective adversarial attacks. Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees. In particular, we introduce the notion of $(\\alpha,\\zeta)$ machine learning model safety. We propose a hypothesis testing procedure, based on the availability of a calibration set, to derive statistical guarantees providing that the probability of declaring that the adversarial (population) risk of a machine learning model is less than $\\alpha$ (i.e. the model is safe), while the model is in fact unsafe (i.e. the model adversarial population risk is higher than $\\alpha$), is less than $\\zeta$. We also propose Bayesian optimization algorithms to determine efficiently whether a machine learning model is $(\\alpha,\\zeta)$-safe in the presence of an adversarial attack, along with statistical guarantees. We apply our framework to a range of machine learning models including various sizes of vision Transformer (ViT) and ResNet models impaired by a variety of adversarial attacks, such as AutoAttack, SquareAttack and natural evolution strategy attack, to illustrate the operation of our approach. Importantly, we show that ViT's are generally more robust to adversarial attacks than ResNets, and ViT-large is more robust than smaller models. Our approach goes beyond existing empirical adversarial risk-based certification guarantees. It formulates rigorous (and provable) performance guarantees that can be used to satisfy regulatory requirements mandating the use of state-of-the-art technical tools.","sentences":["It is widely known that state-of-the-art machine learning models, including vision and language models, can be seriously compromised by adversarial perturbations.","It is therefore increasingly relevant to develop capabilities to certify their performance in the presence of the most effective adversarial attacks.","Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees.","In particular, we introduce the notion of $(\\alpha,\\zeta)$ machine learning model safety.","We propose a hypothesis testing procedure, based on the availability of a calibration set, to derive statistical guarantees providing that the probability of declaring that the adversarial (population) risk of a machine learning model is less than $\\alpha$ (i.e. the model is safe), while the model is in fact unsafe (i.e. the model adversarial population risk is higher than $\\alpha$), is less than $\\zeta$. We also propose Bayesian optimization algorithms to determine efficiently whether a machine learning model is $(\\alpha,\\zeta)$-safe in the presence of an adversarial attack, along with statistical guarantees.","We apply our framework to a range of machine learning models including various sizes of vision Transformer (ViT) and ResNet models impaired by a variety of adversarial attacks, such as AutoAttack, SquareAttack and natural evolution strategy attack, to illustrate the operation of our approach.","Importantly, we show that ViT's are generally more robust to adversarial attacks than ResNets, and ViT-large is more robust than smaller models.","Our approach goes beyond existing empirical adversarial risk-based certification guarantees.","It formulates rigorous (and provable) performance guarantees that can be used to satisfy regulatory requirements mandating the use of state-of-the-art technical tools."],"url":"http://arxiv.org/abs/2402.02629v1","category":"cs.LG"}
{"created":"2024-02-04 22:17:59","title":"Slip-induced odd viscous flow past a cylinder","abstract":"Odd viscosity is a transport coefficient that can occur when fluids experience breaking of parity and time-reversal symmetry. Previous knowledge indicates that cylinders in incompressible odd viscous fluids, under no-slip boundary conditions, do not exhibit lift force, a phenomenon that poses challenges for the experimental detection of odd viscosity. This study investigates the impact of slip in Stokes flow, employing the odd generalization of the Lorentz reciprocal theorem. Our findings reveal that, at linear order in slip length, lift does not manifest. Subsequently, we explore the scenario involving a thin sheet with momentum decay as well as that of a finite system size, demonstrating that for Stokes flow lift does occur for the second order slip length contribution. We address cylinder flow beyond the Stokes approximation by solving the Oseen equation to obtain a fluid profile that shows an interplay between odd viscosity and inertia, and acquire an explicit expression for the leading order slip length contribution to Oseen lift at low Reynolds number.","sentences":["Odd viscosity is a transport coefficient that can occur when fluids experience breaking of parity and time-reversal symmetry.","Previous knowledge indicates that cylinders in incompressible odd viscous fluids, under no-slip boundary conditions, do not exhibit lift force, a phenomenon that poses challenges for the experimental detection of odd viscosity.","This study investigates the impact of slip in Stokes flow, employing the odd generalization of the Lorentz reciprocal theorem.","Our findings reveal that, at linear order in slip length, lift does not manifest.","Subsequently, we explore the scenario involving a thin sheet with momentum decay as well as that of a finite system size, demonstrating that for Stokes flow lift does occur for the second order slip length contribution.","We address cylinder flow beyond the Stokes approximation by solving the Oseen equation to obtain a fluid profile that shows an interplay between odd viscosity and inertia, and acquire an explicit expression for the leading order slip length contribution to Oseen lift at low Reynolds number."],"url":"http://arxiv.org/abs/2402.02628v1","category":"physics.flu-dyn"}
{"created":"2024-02-04 22:15:30","title":"Position bias in features","abstract":"The purpose of modeling document relevance for search engines is to rank better in subsequent searches. Document-specific historical click-through rates can be important features in a dynamic ranking system which updates as we accumulate more sample. This paper describes the properties of several such features, and tests them in controlled experiments. Extending the inverse propensity weighting method to documents creates an unbiased estimate of document relevance. This feature can approximate relevance accurately, leading to near-optimal ranking in ideal circumstances. However, it has high variance that is increasing with respect to the degree of position bias. Furthermore, inaccurate position bias estimation leads to poor performance. Under several scenarios this feature can perform worse than biased click-through rates. This paper underscores the need for accurate position bias estimation, and is unique in suggesting simultaneous use of biased and unbiased position bias features.","sentences":["The purpose of modeling document relevance for search engines is to rank better in subsequent searches.","Document-specific historical click-through rates can be important features in a dynamic ranking system which updates as we accumulate more sample.","This paper describes the properties of several such features, and tests them in controlled experiments.","Extending the inverse propensity weighting method to documents creates an unbiased estimate of document relevance.","This feature can approximate relevance accurately, leading to near-optimal ranking in ideal circumstances.","However, it has high variance that is increasing with respect to the degree of position bias.","Furthermore, inaccurate position bias estimation leads to poor performance.","Under several scenarios this feature can perform worse than biased click-through rates.","This paper underscores the need for accurate position bias estimation, and is unique in suggesting simultaneous use of biased and unbiased position bias features."],"url":"http://arxiv.org/abs/2402.02626v1","category":"cs.IR"}
{"created":"2024-02-04 21:29:14","title":"Schr\u00f6dinger's Cheshire Cat: A tabletop experiment to measure the Di\u00f3si-Penrose collapse time and demonstrate Objective Reduction (OR)","abstract":"For nearly 100 years, the paradox of Schr\\\"odinger's Cat has remained unresolved. Why does the world we live in appear classical despite being composed of quantum particles governed by the Schr\\\"odinger wave equation? Lajos Di\\'osi and Roger Penrose propose the wavefunction collapses because it describes two incompatible space-times, demonstrating an inconsistency between quantum mechanics and general relativity. To avoid this paradox, collapse must occur within Heisenberg's time-energy uncertainty limit. Subatomic particles with low mass, and correspondingly low energy, collapse in years, while superposed cats would collapse almost instantaneously. We propose a table-top experiment to put two small mirrors into superposition and observe them collapse in a time consistent with the Di\\'osi-Penrose model. We employ two techniques to perform this experiment in ambient laboratory conditions. Most experiments separate a small mass by a large distance. In contrast, we displace a large mass by a small distance where the self-energy follows an inverse square law with correspondingly high collapse times. We further use a symmetrical apparatus, where a break in symmetry indicates collapse independent of decoherence.","sentences":["For nearly 100 years, the paradox of Schr\\\"odinger's Cat has remained unresolved.","Why does the world we live in appear classical despite being composed of quantum particles governed by the Schr\\\"odinger wave equation?","Lajos Di\\'osi and Roger Penrose propose the wavefunction collapses because it describes two incompatible space-times, demonstrating an inconsistency between quantum mechanics and general relativity.","To avoid this paradox, collapse must occur within Heisenberg's time-energy uncertainty limit.","Subatomic particles with low mass, and correspondingly low energy, collapse in years, while superposed cats would collapse almost instantaneously.","We propose a table-top experiment to put two small mirrors into superposition and observe them collapse in a time consistent with the Di\\'osi-Penrose model.","We employ two techniques to perform this experiment in ambient laboratory conditions.","Most experiments separate a small mass by a large distance.","In contrast, we displace a large mass by a small distance where the self-energy follows an inverse square law with correspondingly high collapse times.","We further use a symmetrical apparatus, where a break in symmetry indicates collapse independent of decoherence."],"url":"http://arxiv.org/abs/2402.02618v1","category":"quant-ph"}
{"created":"2024-02-04 21:12:04","title":"High-fugacity expansion and crystallization in non-sliding hard-core lattice particle models without a tiling constraint","abstract":"In this paper, we prove the existence of a crystallization transition for a family of hard-core particle models on periodic graphs in arbitrary dimensions. We establish a criterion under which crystallization occurs at sufficiently high densities. The criterion is more general than that in [Jauslin, Lebowitz, Comm. Math. Phys. 364:2, 2018], as it allows models in which particles do not tile the space in the close-packing configurations, such as discrete hard-disk models. To prove crystallization, we prove that the pressure is analytic in the inverse of the fugacity for large enough complex fugacities, using Pirogov-Sinai theory. One of the main tools used for this result is the definition of a local density, based on a discrete generalization of Voronoi cells. We illustrate the criterion by proving that it applies to two examples: staircase models and the radius 2.5 hard-disk model on the square lattice.","sentences":["In this paper, we prove the existence of a crystallization transition for a family of hard-core particle models on periodic graphs in arbitrary dimensions.","We establish a criterion under which crystallization occurs at sufficiently high densities.","The criterion is more general than that in [Jauslin, Lebowitz, Comm.","Math.","Phys.","364:2, 2018], as it allows models in which particles do not tile the space in the close-packing configurations, such as discrete hard-disk models.","To prove crystallization, we prove that the pressure is analytic in the inverse of the fugacity for large enough complex fugacities, using Pirogov-Sinai theory.","One of the main tools used for this result is the definition of a local density, based on a discrete generalization of Voronoi cells.","We illustrate the criterion by proving that it applies to two examples: staircase models and the radius 2.5 hard-disk model on the square lattice."],"url":"http://arxiv.org/abs/2402.02615v1","category":"math-ph"}
{"created":"2024-02-04 21:00:16","title":"Advanced monitoring of rail breakage in double-track railway lines by means of PCA techniques","abstract":"This work describes a classifier designed to identify rail breakages in double-track railway lines, completing the electronic equipment carried out by authors. The main objective of this proposal is to guarantee the integrity of tracks before the railway traffic starts working. In addition, it facilitates maintenance tasks providing information about possible breakages. The detection of breakages is based on the analysis of eight currents provided by the electronic equipment, one per rail, at the ends of the section (emitting and receiving nodes). The imbalance that occurs among the value of these currents implies that there is at least a breakage in the track section under analysis. This analysis is conducted according to three phases. The first one identifies whether there is a breakage, and, in that case, the damaged track is identified. The second phase provides information about which rail is broken (internal, external or both of them) in the previously identified track. Finally, if there is only one breakage, the third phase estimates its most likely zone along the track section. This situation is considered as a classification problem, and solved by means of the Principal Component Analysis technique. This means that a significant number of measurements is required for every breakage pattern (types of breakages) to be considered. Due to the difficulty of having real data, the proposal has been validated using an 8km-long double-track hardware simulator specially designed by the authors, with specific localizations for breakages.","sentences":["This work describes a classifier designed to identify rail breakages in double-track railway lines, completing the electronic equipment carried out by authors.","The main objective of this proposal is to guarantee the integrity of tracks before the railway traffic starts working.","In addition, it facilitates maintenance tasks providing information about possible breakages.","The detection of breakages is based on the analysis of eight currents provided by the electronic equipment, one per rail, at the ends of the section (emitting and receiving nodes).","The imbalance that occurs among the value of these currents implies that there is at least a breakage in the track section under analysis.","This analysis is conducted according to three phases.","The first one identifies whether there is a breakage, and, in that case, the damaged track is identified.","The second phase provides information about which rail is broken (internal, external or both of them) in the previously identified track.","Finally, if there is only one breakage, the third phase estimates its most likely zone along the track section.","This situation is considered as a classification problem, and solved by means of the Principal Component Analysis technique.","This means that a significant number of measurements is required for every breakage pattern (types of breakages) to be considered.","Due to the difficulty of having real data, the proposal has been validated using an 8km-long double-track hardware simulator specially designed by the authors, with specific localizations for breakages."],"url":"http://arxiv.org/abs/2402.02613v1","category":"eess.SP"}
{"created":"2024-02-04 20:23:48","title":"Equivalence transformations and conservation laws for a generalized variable-coefficient Gardner equation","abstract":"In this paper we study the generalized variable-coefficient Gardner equations of the form $u_t + A(t)u^n\\,u_x+ C(t)\\,u^{2n}u_x + B(t)\\,u_{xxx} + Q(t)\\,u =0$. This class broadens out many other equations previously considered: Johnpillai and Khalique (2010), Molati and Ramollo (2012) and Vaneeva, Kuriksha and Sophocleous (2015). Equivalence group of the class under consideration has been constructed which permit an exhaustive study and a simple and clear formulation of the results. Some conservation laws are derived for the nonlinearly self-adjoint equations, based on differential substitutions, and by using the direct method of the multipliers.","sentences":["In this paper we study the generalized variable-coefficient Gardner equations of the form $u_t","+ A(t)u^n\\,u_x+ C(t)\\,u^{2n}u_x +","B(t)\\,u_{xxx} + Q(t)\\,u =0$.","This class broadens out many other equations previously considered: Johnpillai and Khalique (2010), Molati and Ramollo (2012) and Vaneeva, Kuriksha and Sophocleous (2015).","Equivalence group of the class under consideration has been constructed which permit an exhaustive study and a simple and clear formulation of the results.","Some conservation laws are derived for the nonlinearly self-adjoint equations, based on differential substitutions, and by using the direct method of the multipliers."],"url":"http://arxiv.org/abs/2402.02601v1","category":"math.AP"}
{"created":"2024-02-04 20:06:20","title":"Dual Interior-Point Optimization Learning","abstract":"This paper introduces Dual Interior Point Learning (DIPL) and Dual Supergradient Learning (DSL) to learn dual feasible solutions to parametric linear programs with bounded variables, which are pervasive across many industries. DIPL mimics a novel dual interior point algorithm while DSL mimics classical dual supergradient ascent. DIPL and DSL ensure dual feasibility by predicting dual variables associated with the constraints then exploiting the flexibility of the duals of the bound constraints. DIPL and DSL complement existing primal learning methods by providing a certificate of quality. They are shown to produce high-fidelity dual-feasible solutions to large-scale optimal power flow problems providing valid dual bounds under 0.5% optimality gap.","sentences":["This paper introduces Dual Interior Point Learning (DIPL) and Dual Supergradient Learning (DSL) to learn dual feasible solutions to parametric linear programs with bounded variables, which are pervasive across many industries.","DIPL mimics a novel dual interior point algorithm while DSL mimics classical dual supergradient ascent.","DIPL and DSL ensure dual feasibility by predicting dual variables associated with the constraints then exploiting the flexibility of the duals of the bound constraints.","DIPL and DSL complement existing primal learning methods by providing a certificate of quality.","They are shown to produce high-fidelity dual-feasible solutions to large-scale optimal power flow problems providing valid dual bounds under 0.5% optimality gap."],"url":"http://arxiv.org/abs/2402.02596v1","category":"cs.LG"}
{"created":"2024-02-04 19:40:17","title":"Prospective Prediction of Body Mass Index Trajectories using Multi-task Gaussian Processes","abstract":"Clinicians often investigate the body mass index (BMI) trajectories of children to assess their growth with respect to their peers, as well as to anticipate future growth and disease risk. While retrospective modelling of BMI trajectories has been an active area of research, prospective prediction of continuous BMI trajectories from historical growth data has not been well investigated. Using weight and height measurements from birth to age 10 years from a longitudinal mother-offspring cohort, we leveraged a multi-task Gaussian processes model, called MagmaClust, to derive probabilistic predictions for BMI trajectories over various forecasting periods. Experiments were conducted to evaluate the accuracy, sensitivity to missing values, and number of clusters. The results were compared with cubic B-spline regression and a parametric Jenss-Bayley mixed effects model. A downstream tool computing individual overweight probabilities was also proposed and evaluated. In all experiments, MagmaClust outperformed conventional models in prediction accuracy while correctly calibrating uncertainty regardless of the missing data amount (up to 90\\% missing) or the forecasting period (from 2 to 8 years in the future). Moreover, the overweight probabilities computed from MagmaClust's uncertainty quantification exhibited high specificity ($0.94$ to $0.96$) and accuracy ($0.86$ to $0.94$) in predicting the 10-year overweight status even from age 2 years. MagmaClust provides a probabilistic non-parametric framework to prospectively predict BMI trajectories, which is robust to missing values and outperforms conventional BMI trajectory modelling approaches. It also clusters individuals to identify typical BMI patterns (early peak, adiposity rebounds) during childhood. Overall, we demonstrated its potential to anticipate BMI evolution throughout childhood, allowing clinicians to implement prevention strategies.","sentences":["Clinicians often investigate the body mass index (BMI) trajectories of children to assess their growth with respect to their peers, as well as to anticipate future growth and disease risk.","While retrospective modelling of BMI trajectories has been an active area of research, prospective prediction of continuous BMI trajectories from historical growth data has not been well investigated.","Using weight and height measurements from birth to age 10 years from a longitudinal mother-offspring cohort, we leveraged a multi-task Gaussian processes model, called MagmaClust, to derive probabilistic predictions for BMI trajectories over various forecasting periods.","Experiments were conducted to evaluate the accuracy, sensitivity to missing values, and number of clusters.","The results were compared with cubic B-spline regression and a parametric Jenss-Bayley mixed effects model.","A downstream tool computing individual overweight probabilities was also proposed and evaluated.","In all experiments, MagmaClust outperformed conventional models in prediction accuracy while correctly calibrating uncertainty regardless of the missing data amount (up to 90\\% missing) or the forecasting period (from 2 to 8 years in the future).","Moreover, the overweight probabilities computed from MagmaClust's uncertainty quantification exhibited high specificity ($0.94$ to $0.96$) and accuracy ($0.86$ to $0.94$) in predicting the 10-year overweight status even from age 2 years.","MagmaClust provides a probabilistic non-parametric framework to prospectively predict BMI trajectories, which is robust to missing values and outperforms conventional BMI trajectory modelling approaches.","It also clusters individuals to identify typical BMI patterns (early peak, adiposity rebounds) during childhood.","Overall, we demonstrated its potential to anticipate BMI evolution throughout childhood, allowing clinicians to implement prevention strategies."],"url":"http://arxiv.org/abs/2402.02589v1","category":"stat.AP"}
{"created":"2024-02-04 19:14:49","title":"Quantum scalar fields interacting with quantum black hole asymptotic regions","abstract":"We continue our work on the study of spherically symmetric loop quantum gravity coupled to two spherically symmetric scalar fields, one that acts as a clock. As a consequence of the presence of the latter, we can define a true Hamiltonian for the theory. In previous papers we have studied the theory for large values of the radial coordinate, that is, far away from any black hole or star that may be present. This makes the calculations considerably more tractable. We have shown that in the asymptotic region the theory admits a large family of quantum vacua for quantum matter fields coupled to quantum gravity, as is expected from the well-known results of quantum field theory on classical curved space-time. Here, we study perturbative corrections involving terms that we neglected in our previous work. Using time-dependent perturbation theory, we show that the states that represent different possible vacua are essentially stable. This ensures that one recovers from a totally quantized gravitational theory coupled to matter the standard behavior of a matter quantum field theory plus low probability transitions due to gravity between particles that differ at most by a small amount of energy.","sentences":["We continue our work on the study of spherically symmetric loop quantum gravity coupled to two spherically symmetric scalar fields, one that acts as a clock.","As a consequence of the presence of the latter, we can define a true Hamiltonian for the theory.","In previous papers we have studied the theory for large values of the radial coordinate, that is, far away from any black hole or star that may be present.","This makes the calculations considerably more tractable.","We have shown that in the asymptotic region the theory admits a large family of quantum vacua for quantum matter fields coupled to quantum gravity, as is expected from the well-known results of quantum field theory on classical curved space-time.","Here, we study perturbative corrections involving terms that we neglected in our previous work.","Using time-dependent perturbation theory, we show that the states that represent different possible vacua are essentially stable.","This ensures that one recovers from a totally quantized gravitational theory coupled to matter the standard behavior of a matter quantum field theory plus low probability transitions due to gravity between particles that differ at most by a small amount of energy."],"url":"http://arxiv.org/abs/2402.02587v1","category":"gr-qc"}
{"created":"2024-02-04 18:45:33","title":"On the development of an application for the compilation of global sea level changes","abstract":"There is a lot of data about mean sea level variation from studies conducted around the globe. This data is dispersed, lacks organization along with standardization, and in most cases, it is not available online. In some instances, when it is available, it is often in unpractical ways and different formats. Analyzing it would be inefficient and very time-consuming. In addition to all of that, to successfully process spatial-temporal data, the user has to be equipped with particular skills and tools used for geographic data like PostGIS, PostgreSQL and GeoAlchemy. The presented solution is to develop a web application that solves some of the issues faced by researchers. The web application allows the user to add data, be it through forms in a browser or automated with the help of an API. The application also assists with data querying, processing and visualization by making tables, showing maps and drawing graphs. Comparing data points from different areas and publications is also made possible. The implemented web application permits the query and storage of spatial-temporal data about mean sea level variation in a simplified, easily accessible and user-friendly manner. It will also allow the realization of more global studies.","sentences":["There is a lot of data about mean sea level variation from studies conducted around the globe.","This data is dispersed, lacks organization along with standardization, and in most cases, it is not available online.","In some instances, when it is available, it is often in unpractical ways and different formats.","Analyzing it would be inefficient and very time-consuming.","In addition to all of that, to successfully process spatial-temporal data, the user has to be equipped with particular skills and tools used for geographic data like PostGIS, PostgreSQL and GeoAlchemy.","The presented solution is to develop a web application that solves some of the issues faced by researchers.","The web application allows the user to add data, be it through forms in a browser or automated with the help of an API.","The application also assists with data querying, processing and visualization by making tables, showing maps and drawing graphs.","Comparing data points from different areas and publications is also made possible.","The implemented web application permits the query and storage of spatial-temporal data about mean sea level variation in a simplified, easily accessible and user-friendly manner.","It will also allow the realization of more global studies."],"url":"http://arxiv.org/abs/2402.02582v1","category":"cs.DB"}
{"created":"2024-02-04 18:44:56","title":"Non-Fermi Liquid and Hund Correlation in La$_4$Ni$_3$O$_{10}$ under High Pressure","abstract":"High temperature superconductivity was recently found in the bilayer nickelate $\\rm{La}_3 \\rm{Ni}_2 \\rm{O}_7$ (La327), followed by the discovery of superconductivity in the trilayer $\\rm{La}_4 \\rm{Ni}_3 \\rm{O}_{10}$ (La4310), under high pressure. Through studying the electronic correlation of La4310 with DFT+DMFT, and further comparing it with that of La327, we find that the $e_g$ orbitals of the outer-layer Ni cations in La4310 have a similar (but slightly weaker) electronic correlation to those in La327, in which the electrons behave as a non-Fermi liquid with Hund correlation and linear-in-temperature scattering rate. Our results suggest that the experimentally observed ``strange metal'' behavior may be explained by the Hund spin correlation featuring high spin states and spin-orbital separation. In contrast, the electrons in the inner-layer Ni cations in La4310 behave as a Fermi liquid. The weaker electronic correlation in La4310 is attributed to more hole-doping, which may explain its lower superconducting transition temperature.","sentences":["High temperature superconductivity was recently found in the bilayer nickelate $\\rm{La}_3 \\rm{Ni}_2 \\rm{O}_7$ (La327), followed by the discovery of superconductivity in the trilayer $\\rm{La}_4 \\rm{Ni}_3 \\rm{O}_{10}$ (La4310), under high pressure.","Through studying the electronic correlation of La4310 with DFT+DMFT, and further comparing it with that of La327, we find that the $e_g$ orbitals of the outer-layer Ni cations in La4310 have a similar (but slightly weaker) electronic correlation to those in La327, in which the electrons behave as a non-Fermi liquid with Hund correlation and linear-in-temperature scattering rate.","Our results suggest that the experimentally observed ``strange metal'' behavior may be explained by the Hund spin correlation featuring high spin states and spin-orbital separation.","In contrast, the electrons in the inner-layer Ni cations in La4310 behave as a Fermi liquid.","The weaker electronic correlation in La4310 is attributed to more hole-doping, which may explain its lower superconducting transition temperature."],"url":"http://arxiv.org/abs/2402.02581v1","category":"cond-mat.str-el"}
{"created":"2024-02-04 18:06:39","title":"Impact of PSF misestimation and galaxy population bias on precision shear measurement using a CNN","abstract":"Weak gravitational lensing of distant galaxies provides a powerful probe of dark energy. The aim of this study is to investigate the application of convolutional neural networks (CNNs) to precision shear estimation. In particular, using a shallow CNN, we explore the impact of point spread function (PSF) misestimation and `galaxy population bias' (including `distribution bias' and `morphology bias'), focusing on the accuracy requirements of next generation surveys. We simulate a population of noisy disk and elliptical galaxies and adopt a PSF that is representative of a Euclid-like survey. We quantify the accuracy achieved by the CNN assuming a linear relationship between the estimated and true shears and measure the multiplicative ($m$) and additive ($c$) biases. We make use of an unconventional loss function to mitigate the effects of noise bias and measure $m$ and $c$ when we use either: (i) an incorrect galaxy ellipticity distribution or size-magnitude relation, or the wrong ratio of morphological types, to describe the population of galaxies (distribution bias); (ii) an incorrect galaxy light profile (morphology bias); or (iii) a PSF with size or ellipticity offset from its true value (PSF misestimation). We compare our results to the Euclid requirements on the knowledge of the PSF model shape and size. Finally, we outline further work to build on the promising potential of CNNs in precision shear estimation.","sentences":["Weak gravitational lensing of distant galaxies provides a powerful probe of dark energy.","The aim of this study is to investigate the application of convolutional neural networks (CNNs) to precision shear estimation.","In particular, using a shallow CNN, we explore the impact of point spread function (PSF) misestimation and `galaxy population bias' (including `distribution bias' and `morphology bias'), focusing on the accuracy requirements of next generation surveys.","We simulate a population of noisy disk and elliptical galaxies and adopt a PSF that is representative of a Euclid-like survey.","We quantify the accuracy achieved by the CNN assuming a linear relationship between the estimated and true shears and measure the multiplicative ($m$) and additive ($c$) biases.","We make use of an unconventional loss function to mitigate the effects of noise bias and measure $m$ and $c$ when we use either: (i) an incorrect galaxy ellipticity distribution or size-magnitude relation, or the wrong ratio of morphological types, to describe the population of galaxies (distribution bias); (ii) an incorrect galaxy light profile (morphology bias); or (iii) a PSF with size or ellipticity offset from its true value (PSF misestimation).","We compare our results to the Euclid requirements on the knowledge of the PSF model shape and size.","Finally, we outline further work to build on the promising potential of CNNs in precision shear estimation."],"url":"http://arxiv.org/abs/2402.02578v1","category":"astro-ph.CO"}
{"created":"2024-02-04 18:05:40","title":"Anomalous $U(1)$ extension of the Standard Model","abstract":"We present a set of example models in which the Standard Model (SM) symmetry group is extended by a new abelian symmetry. This additional symmetry appears anomalous in the effective low-energy theory; however, the anomalies cancel out when massive chiral fermions not present in the effective low-energy theory are taken into account. These chiral fermions under the new abelian gauge group, are chosen to be vector-like under the SM symmetries, and reside in the same representations as quarks and leptons. This allows us to quantitatively determine the magnitude of tree-level interactions between three vector bosons induced in low-energy effective field theory by the integration of chiral heavy fermions. We also examine the perturbativity constraints of the theory and the ultraviolet cut-off. We conclude by highlighting possible extensions of our work.","sentences":["We present a set of example models in which the Standard Model (SM) symmetry group is extended by a new abelian symmetry.","This additional symmetry appears anomalous in the effective low-energy theory; however, the anomalies cancel out when massive chiral fermions not present in the effective low-energy theory are taken into account.","These chiral fermions under the new abelian gauge group, are chosen to be vector-like under the SM symmetries, and reside in the same representations as quarks and leptons.","This allows us to quantitatively determine the magnitude of tree-level interactions between three vector bosons induced in low-energy effective field theory by the integration of chiral heavy fermions.","We also examine the perturbativity constraints of the theory and the ultraviolet cut-off.","We conclude by highlighting possible extensions of our work."],"url":"http://arxiv.org/abs/2402.02577v1","category":"hep-ph"}
{"created":"2024-02-04 17:52:04","title":"Spatio-temporal Prompting Network for Robust Video Feature Extraction","abstract":"Frame quality deterioration is one of the main challenges in the field of video understanding. To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks. In this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Network (STPN). It can efficiently extract robust and accurate video features by dynamically adjusting the input features in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal information of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. Moreover, STPN is easy to generalise to various video tasks because it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used datasets for different video understanding tasks, i.e., ImageNetVID for video object detection, YouTubeVIS for video instance segmentation, and GOT-10k for visual object tracking. Code is available at https://github.com/guanxiongsun/vfe.pytorch.","sentences":["Frame quality deterioration is one of the main challenges in the field of video understanding.","To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information.","However, these integration modules are heavy and complex.","Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks.","In this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Network (STPN).","It can efficiently extract robust and accurate video features by dynamically adjusting the input features in the backbone network.","Specifically, STPN predicts several video prompts containing spatio-temporal information of neighbour frames.","Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction.","Moreover, STPN is easy to generalise to various video tasks because it does not contain task-specific modules.","Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used datasets for different video understanding tasks, i.e., ImageNetVID for video object detection, YouTubeVIS for video instance segmentation, and GOT-10k for visual object tracking.","Code is available at https://github.com/guanxiongsun/vfe.pytorch."],"url":"http://arxiv.org/abs/2402.02574v1","category":"cs.CV"}
{"created":"2024-02-04 17:31:39","title":"Simple Stochastic Stopping Games: A Generator and Benchmark Library","abstract":"Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the simplest version of Stochastic Games for which there is no known polynomial-time algorithm. Condon showed that Stochastic Games are polynomial-time reducible to SSGs, which in turn are polynomial-time reducible to Stopping Games. SSGs are games where all decisions are binary and every move has a random outcome with a known probability distribution. Stopping Games are SSGs that are guaranteed to terminate. There are many algorithms for SSGs, most of which are fast in practice, but they all lack theoretical guarantees for polynomial-time convergence. The pursuit of a polynomial-time algorithm for SSGs is an active area of research. This paper is intended to support such research by making it easier to study the graphical structure of SSGs. Our contributions are: (1) a generating algorithm for Stopping Games, (2) a proof that the algorithm can generate any game, (3) a list of additional polynomial-time reductions that can be made to Stopping Games, (4) an open source generator for generating fully reduced instances of Stopping Games that comes with instructions and is fully documented, (5) a benchmark set of such instances, (6) and an analysis of how two main algorithm types perform on our benchmark set.","sentences":["Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the simplest version of Stochastic Games for which there is no known polynomial-time algorithm.","Condon showed that Stochastic Games are polynomial-time reducible to SSGs, which in turn are polynomial-time reducible to Stopping Games.","SSGs are games where all decisions are binary and every move has a random outcome with a known probability distribution.","Stopping Games are SSGs that are guaranteed to terminate.","There are many algorithms for SSGs, most of which are fast in practice, but they all lack theoretical guarantees for polynomial-time convergence.","The pursuit of a polynomial-time algorithm for SSGs is an active area of research.","This paper is intended to support such research by making it easier to study the graphical structure of SSGs.","Our contributions are: (1) a generating algorithm for Stopping Games, (2) a proof that the algorithm can generate any game, (3) a list of additional polynomial-time reductions that can be made to Stopping Games, (4) an open source generator for generating fully reduced instances of Stopping Games that comes with instructions and is fully documented, (5) a benchmark set of such instances, (6) and an analysis of how two main algorithm types perform on our benchmark set."],"url":"http://arxiv.org/abs/2402.02571v1","category":"cs.CC"}
{"created":"2024-02-04 17:14:53","title":"On the Complexity of Finite-Sum Smooth Optimization under the Polyak-\u0141ojasiewicz Condition","abstract":"This paper considers the optimization problem of the form $\\min_{{\\bf x}\\in{\\mathbb R}^d} f({\\bf x})\\triangleq \\frac{1}{n}\\sum_{i=1}^n f_i({\\bf x})$, where $f(\\cdot)$ satisfies the Polyak--{\\L}ojasiewicz (PL) condition with parameter $\\mu$ and $\\{f_i(\\cdot)\\}_{i=1}^n$ is $L$-mean-squared smooth. We show that any gradient method requires at least $\\Omega(n+\\kappa\\sqrt{n}\\log(1/\\epsilon))$ incremental first-order oracle (IFO) calls to find an $\\epsilon$-suboptimal solution, where $\\kappa\\triangleq L/\\mu$ is the condition number of the problem. This result nearly matches upper bounds of IFO complexity for best-known first-order methods. We also study the problem of minimizing the PL function in the distributed setting such that the individuals $f_1(\\cdot),\\dots,f_n(\\cdot)$ are located on a connected network of $n$ agents. We provide lower bounds of $\\Omega(\\kappa/\\sqrt{\\gamma}\\,\\log(1/\\epsilon))$, $\\Omega((\\kappa+\\tau\\kappa/\\sqrt{\\gamma}\\,)\\log(1/\\epsilon))$ and $\\Omega\\big(n+\\kappa\\sqrt{n}\\log(1/\\epsilon)\\big)$ for communication rounds, time cost and local first-order oracle calls respectively, where $\\gamma\\in(0,1]$ is the spectral gap of the mixing matrix associated with the network and~$\\tau>0$ is the time cost of per communication round. Furthermore, we propose a decentralized first-order method that nearly matches above lower bounds in expectation.","sentences":["This paper considers the optimization problem of the form $\\min_{{\\bf x}\\in{\\mathbb R}^d} f({\\bf x})\\triangleq \\frac{1}{n}\\sum_{i=1}^n f_i({\\bf x})$, where $f(\\cdot)$ satisfies the Polyak--{\\L}ojasiewicz (PL) condition with parameter $\\mu$ and $\\{f_i(\\cdot)\\}_{i=1}^n$ is $L$-mean-squared smooth.","We show that any gradient method requires at least $\\Omega(n+\\kappa\\sqrt{n}\\log(1/\\epsilon))$ incremental first-order oracle (IFO) calls to find an $\\epsilon$-suboptimal solution, where $\\kappa\\triangleq L/\\mu$ is the condition number of the problem.","This result nearly matches upper bounds of IFO complexity for best-known first-order methods.","We also study the problem of minimizing the PL function in the distributed setting such that the individuals $f_1(\\cdot),\\dots,f_n(\\cdot)$ are located on a connected network of $n$ agents.","We provide lower bounds of $\\Omega(\\kappa/\\sqrt{\\gamma}\\,\\log(1/\\epsilon))$, $\\Omega((\\kappa+\\tau\\kappa/\\sqrt{\\gamma}\\,)\\log(1/\\epsilon))$ and $\\Omega\\big(n+\\kappa\\sqrt{n}\\log(1/\\epsilon)\\big)$ for communication rounds, time cost and local first-order oracle calls respectively, where $\\gamma\\in(0,1]$ is the spectral gap of the mixing matrix associated with the network and~$\\tau>0$ is the time cost of per communication round.","Furthermore, we propose a decentralized first-order method that nearly matches above lower bounds in expectation."],"url":"http://arxiv.org/abs/2402.02569v1","category":"math.OC"}
{"created":"2024-02-04 17:05:27","title":"STAGE: Scalable and Traversability-Aware Graph based Exploration Planner for Dynamically Varying Environments","abstract":"In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas. The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively. The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning. Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs. Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer. Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable. This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions. As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph. Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor.","sentences":["In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas.","The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively.","The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning.","Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs.","Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer.","Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable.","This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions.","As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph.","Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor."],"url":"http://arxiv.org/abs/2402.02566v1","category":"cs.RO"}
{"created":"2024-02-04 16:27:37","title":"Foundation Model Makes Clustering a Better Initialization for Active Learning","abstract":"Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as pixel values, clustering quickly converges and identifies better initial samples. For a comprehensive comparison, we included a classic ImageNet-supervised model to acquire embeddings. Experiments on two clinical tasks of image classification and segmentation demonstrated that foundation model-based clustering efficiently pinpointed informative initial samples, leading to models showcasing enhanced performance than the baseline methods. We envisage that this study provides an effective paradigm for future active learning.","sentences":["Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget.","While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization.","Most of the previous studies resort to random sampling or naive clustering.","However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data.","In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization.","Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks.","Leveraging these embeddings to replace raw features such as pixel values, clustering quickly converges and identifies better initial samples.","For a comprehensive comparison, we included a classic ImageNet-supervised model to acquire embeddings.","Experiments on two clinical tasks of image classification and segmentation demonstrated that foundation model-based clustering efficiently pinpointed informative initial samples, leading to models showcasing enhanced performance than the baseline methods.","We envisage that this study provides an effective paradigm for future active learning."],"url":"http://arxiv.org/abs/2402.02561v1","category":"cs.LG"}
{"created":"2024-02-04 16:11:45","title":"Analog model of Kerr black holes in scalar-tensor theories using binary Bose-Einstein condensates: Quasibound states and tachyonic instabilities","abstract":"We propose an analog model of the Kerr black hole in scalar-tensor theories using 2-component Bose-Einstein Condensates. We focus on two types of gapped excitations induced by the modes of two-component condensates with a relative phase of $0$ and $\\pi$, analogous to the massive scalar field with positive and negative mass squared, respectively. The inclusion of space-dependent Rabi coupling can induce an effective space-dependent mass term. We study superradiant instabilities resulting from the quasibound states corresponding to positive mass squared and the tachyonic instabilities arising from negative mass squared in both frequency and time domains. These instabilities resemble two possible mechanisms that make Kerr black holes unstable in scalar-tensor gravity with the presence of matter around the black hole. Our proposed analog gravity model could potentially be implemented in future experiments, drawing from the success of recent analog rotating black hole implementations.","sentences":["We propose an analog model of the Kerr black hole in scalar-tensor theories using 2-component Bose-Einstein Condensates.","We focus on two types of gapped excitations induced by the modes of two-component condensates with a relative phase of $0$ and $\\pi$, analogous to the massive scalar field with positive and negative mass squared, respectively.","The inclusion of space-dependent Rabi coupling can induce an effective space-dependent mass term.","We study superradiant instabilities resulting from the quasibound states corresponding to positive mass squared and the tachyonic instabilities arising from negative mass squared in both frequency and time domains.","These instabilities resemble two possible mechanisms that make Kerr black holes unstable in scalar-tensor gravity with the presence of matter around the black hole.","Our proposed analog gravity model could potentially be implemented in future experiments, drawing from the success of recent analog rotating black hole implementations."],"url":"http://arxiv.org/abs/2402.02557v1","category":"gr-qc"}
{"created":"2024-02-04 16:09:04","title":"A new approach for imprecise probabilities","abstract":"This paper introduces a novel concept of interval probability measures that enables the representation of imprecise probabilities, or uncertainty, in a natural and coherent manner. Within an algebra of sets, we introduce a notion of weak complementation denoted as $\\psi$. The interval probability measure of an event $H$ is defined with respect to the set of indecisive eventualities $(\\psi(H))^c$, which is included in the standard complement $H^c$.   We characterize a broad class of interval probability measures and define their properties. Additionally, we establish an updating rule with respect to $H$, incorporating concepts of statistical independence and dependence. The interval distribution of a random variable is formulated, and a corresponding definition of stochastic dominance between two random variables is introduced. As a byproduct, a formal solution to the century-old Keynes-Ramsey controversy is presented.","sentences":["This paper introduces a novel concept of interval probability measures that enables the representation of imprecise probabilities, or uncertainty, in a natural and coherent manner.","Within an algebra of sets, we introduce a notion of weak complementation denoted as $\\psi$. The interval probability measure of an event $H$ is defined with respect to the set of indecisive eventualities $(\\psi(H))^c$, which is included in the standard complement $H^c$.   We characterize a broad class of interval probability measures and define their properties.","Additionally, we establish an updating rule with respect to $H$, incorporating concepts of statistical independence and dependence.","The interval distribution of a random variable is formulated, and a corresponding definition of stochastic dominance between two random variables is introduced.","As a byproduct, a formal solution to the century-old Keynes-Ramsey controversy is presented."],"url":"http://arxiv.org/abs/2402.02556v1","category":"math.ST"}
{"created":"2024-02-04 15:59:35","title":"DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers","abstract":"Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our evaluation demonstrates the attack's effectiveness on three token sparsification techniques and examines the attack's transferability between them and its effect on the GPU resources. To mitigate the impact of the attack, we propose various countermeasures.","sentences":["Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection).","However, their high computational requirements grow quadratically with the number of tokens used.","Token sparsification techniques have been proposed to address this issue.","These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency.","However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance.","In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms.","The attack aims to exhaust the operating system's resources, while maintaining its stealthiness.","Our evaluation demonstrates the attack's effectiveness on three token sparsification techniques and examines the attack's transferability between them and its effect on the GPU resources.","To mitigate the impact of the attack, we propose various countermeasures."],"url":"http://arxiv.org/abs/2402.02554v1","category":"cs.CV"}
{"created":"2024-02-04 15:53:14","title":"Compensation of the multipolar polarizability shift in optical lattice clocks","abstract":"In neutral atom optical clocks, the higher-order atomic polarizability terms lead to the clock transition frequency shift which is motion-state dependent and nonlinear with the optical lattice depth. We propose to use an auxiliary optical lattice to compensate the influence of the E2-M1 differential polarizability or tune the associated coefficient to a favorable value. We show that by applying this method to Sr and Hg optical lattice clocks, the low or even sub-10-19 clock transition frequency uncertainty from the optical lattice becomes feasible. Finally, the proposed scheme is simple for experimental realization and can be implemented and tested in the existing setups.","sentences":["In neutral atom optical clocks, the higher-order atomic polarizability terms lead to the clock transition frequency shift which is motion-state dependent and nonlinear with the optical lattice depth.","We propose to use an auxiliary optical lattice to compensate the influence of the E2-M1 differential polarizability or tune the associated coefficient to a favorable value.","We show that by applying this method to Sr and Hg optical lattice clocks, the low or even sub-10-19 clock transition frequency uncertainty from the optical lattice becomes feasible.","Finally, the proposed scheme is simple for experimental realization and can be implemented and tested in the existing setups."],"url":"http://arxiv.org/abs/2402.02550v1","category":"physics.atom-ph"}
{"created":"2024-02-04 15:52:59","title":"Are Large Language Models Table-based Fact-Checkers?","abstract":"Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which is beneficial to further research of table reasoning.","sentences":["Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables.","Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability.","Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields.","They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown.","In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers.","In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability.","Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs.","Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly.","We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples.","Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which is beneficial to further research of table reasoning."],"url":"http://arxiv.org/abs/2402.02549v1","category":"cs.CL"}
{"created":"2024-02-04 15:39:18","title":"Embedding Non-Distortive Cancelable Face Template Generation","abstract":"Biometric authentication systems are crucial for security, but developing them involves various complexities, including privacy, security, and achieving high accuracy without directly storing pure biometric data in storage. We introduce an innovative image distortion technique that makes facial images unrecognizable to the eye but still identifiable by any custom embedding neural network model. Using the proposed approach, we test the reliability of biometric recognition networks by determining the maximum image distortion that does not change the predicted identity. Through experiments on MNIST and LFW datasets, we assess its effectiveness and compare it based on the traditional comparison metrics.","sentences":["Biometric authentication systems are crucial for security, but developing them involves various complexities, including privacy, security, and achieving high accuracy without directly storing pure biometric data in storage.","We introduce an innovative image distortion technique that makes facial images unrecognizable to the eye but still identifiable by any custom embedding neural network model.","Using the proposed approach, we test the reliability of biometric recognition networks by determining the maximum image distortion that does not change the predicted identity.","Through experiments on MNIST and LFW datasets, we assess its effectiveness and compare it based on the traditional comparison metrics."],"url":"http://arxiv.org/abs/2402.02540v1","category":"cs.CV"}
{"created":"2024-02-04 15:35:55","title":"$a_0(1710)$-$f_0(1710)$ mixing effect in the $D_{s}^{+} \\rightarrow K_S^{0} K_S^{0} \u03c0^{+}$ decay","abstract":"With the measurements of the decay $D^+_s \\rightarrow K^0_S K^0_S \\pi^+$ by the BESIII Collaboration, we investigate this three-body weak decay via the chiral unitary approach for the final state interaction, where the resonances $S(980)$ and $S(1710)$ are dynamically reproduced with the interaction of eleven coupled channels, and the $W$-external and -internal emission mechanisms are considered at the quark level. Besides, we also take into account the contribution from the $P$-wave resonance $K^*(892)^+$ and make a combined fit of the $K^0_S K^0_S$ and $K^0_S \\pi^+$ invariant mass spectra measured by the BESIII Collaboration. The fitted results show that the enhancement around 1.7 GeV in $K^0_S K^0_S$ mass spectrum is overlapped with two visible peaks, indicating the mixing signal originated from the resonances $a_0(1710)$ and $f_0(1710)$ due to their different poles (masses). Thus, the decay $D^+_s \\rightarrow K^0_S K^0_S \\pi^+$ is helpful to reveal their molecular nature with the mixing signal, which can be more precisely measured in the future.","sentences":["With the measurements of the decay $D^+_s \\rightarrow K^0_S K^0_S \\pi^+$ by the BESIII Collaboration, we investigate this three-body weak decay via the chiral unitary approach for the final state interaction, where the resonances $S(980)$ and $S(1710)$ are dynamically reproduced with the interaction of eleven coupled channels, and the $W$-external and -internal emission mechanisms are considered at the quark level.","Besides, we also take into account the contribution from the $P$-wave resonance $K^*(892)^+$ and make a combined fit of the $K^0_S K^0_S$ and $K^0_S \\pi^+$ invariant mass spectra measured by the BESIII Collaboration.","The fitted results show that the enhancement around 1.7 GeV in $K^0_S K^0_S$ mass spectrum is overlapped with two visible peaks, indicating the mixing signal originated from the resonances $a_0(1710)$ and $f_0(1710)$ due to their different poles (masses).","Thus, the decay $D^+_s \\rightarrow K^0_S K^0_S","\\pi^+$ is helpful to reveal their molecular nature with the mixing signal, which can be more precisely measured in the future."],"url":"http://arxiv.org/abs/2402.02539v1","category":"hep-ph"}
{"created":"2024-02-04 15:30:05","title":"Data-driven Policy Learning for a Continuous Treatment","abstract":"This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable. Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare. We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension. This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension. Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias. This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation. To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques. This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficiency. By utilizing precise maximal and concentration inequalities, we derive sharper regret bounds than those currently available in the literature. In instances where the propensity score is unknown, we adopt the doubly robust (DR) moment condition tailored to the continuous treatment setting. In alignment with the binary-treatment case, the DR welfare regret closely parallels the IPW welfare regret, given the fast convergence of nuisance estimators.","sentences":["This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable.","Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare.","We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension.","This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension.","Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias.","This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation.","To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques.","This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficiency.","By utilizing precise maximal and concentration inequalities, we derive sharper regret bounds than those currently available in the literature.","In instances where the propensity score is unknown, we adopt the doubly robust (DR) moment condition tailored to the continuous treatment setting.","In alignment with the binary-treatment case, the DR welfare regret closely parallels the IPW welfare regret, given the fast convergence of nuisance estimators."],"url":"http://arxiv.org/abs/2402.02535v1","category":"econ.EM"}
{"created":"2024-02-05 18:59:52","title":"Test-Time Adaptation for Depth Completion","abstract":"It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.","sentences":["It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them.","Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data.","We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass.","We first present a study on how the domain shift in each data modality affects model performance.","Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth.","During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain.","We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%."],"url":"http://arxiv.org/abs/2402.03312v1","category":"cs.CV"}
{"created":"2024-02-05 17:53:16","title":"Declipping and the recovery of vectors from saturated measurements","abstract":"A frame $(x_j)_{j\\in J}$ for a Hilbert space $H$ allows for a linear and stable reconstruction of any vector $x\\in H$ from the linear measurements $(\\langle x,x_j\\rangle)_{j\\in J}$. However, there are many situations where some information in the frame coefficients is lost. In applications where one is using sensors with a fixed dynamic range, any measurement above that range is registered as the maximum, and any measurement below that range is registered as the minimum. Depending on the context, recovering a vector from such measurements is called either declipping or saturation recovery. We initiate a frame theoretic approach to saturation recovery in a similar way to what [BCE06] did for phase retrieval. We characterize when saturation recovery is possible, show optimal frames for use with saturation recovery correspond to minimal multi-fold packings in projective space, and prove that the classical frame algorithm may be adapted to this non-linear problem to provide a reconstruction algorithm.","sentences":["A frame $(x_j)_{j\\in J}$ for a Hilbert space $H$ allows for a linear and stable reconstruction of any vector $x\\in H$ from the linear measurements $(\\langle x,x_j\\rangle)_{j\\in J}$.","However, there are many situations where some information in the frame coefficients is lost.","In applications where one is using sensors with a fixed dynamic range, any measurement above that range is registered as the maximum, and any measurement below that range is registered as the minimum.","Depending on the context, recovering a vector from such measurements is called either declipping or saturation recovery.","We initiate a frame theoretic approach to saturation recovery in a similar way to what [BCE06] did for phase retrieval.","We characterize when saturation recovery is possible, show optimal frames for use with saturation recovery correspond to minimal multi-fold packings in projective space, and prove that the classical frame algorithm may be adapted to this non-linear problem to provide a reconstruction algorithm."],"url":"http://arxiv.org/abs/2402.03237v1","category":"math.FA"}
{"created":"2024-02-05 17:22:34","title":"Universal Gradient Methods for Stochastic Convex Optimization","abstract":"We develop universal gradient methods for Stochastic Convex Optimization (SCO). Our algorithms automatically adapt not only to the oracle's noise but also to the H\\\"older smoothness of the objective function without a priori knowledge of the particular setting. The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD). Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences. The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire H\\\"older class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient. We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates.","sentences":["We develop universal gradient methods for Stochastic Convex Optimization (SCO).","Our algorithms automatically adapt not only to the oracle's noise but also to the H\\\"older smoothness of the objective function without a priori knowledge of the particular setting.","The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD).","Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences.","The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire H\\\"older class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient.","We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates."],"url":"http://arxiv.org/abs/2402.03210v1","category":"math.OC"}
{"created":"2024-02-05 17:17:57","title":"Light and Optimal Schr\u00f6dinger Bridge Matching","abstract":"Schr\\\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \\textbf{optimal Schr\\\"odinger bridge matching}. It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \\textbf{(a)} with a single bridge matching step and \\textbf{(b)} with arbitrary transport plan as the input. Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\\\"odinger potential. We experimentally showcase the performance of our solver in a range of practical tasks. The code for the LightSB-M solver can be found at \\url{https://github.com/SKholkin/LightSB-Matching}.","sentences":["Schr\\\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT).","Recent solvers for SB exploit the pervasive bridge matching procedures.","Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them.","In particular, given the EOT plan, these procedures can be adapted to solve SB.","This fact is heavily exploited by recent works giving rives to matching-based SB solvers.","The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training.","We address these limitations and propose a novel procedure to learn SB which we call the \\textbf{optimal Schr\\\"odinger bridge matching}.","It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \\textbf{(a)} with a single bridge matching step and \\textbf{(b)} with arbitrary transport plan as the input.","Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB.","Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\\\"odinger potential.","We experimentally showcase the performance of our solver in a range of practical tasks.","The code for the LightSB-M solver can be found at \\url{https://github.com/SKholkin/LightSB-Matching}."],"url":"http://arxiv.org/abs/2402.03207v1","category":"cs.LG"}
{"created":"2024-02-05 17:13:31","title":"Right-censored models by the expectile method","abstract":"Based on the expectile loss function and the adaptive LASSO penalty, the paper proposes and studies the estimation methods for the accelerated failure time (AFT) model. In this approach, we need to estimate the survival function of the censoring variable by the Kaplan-Meier estimator. The AFT model parameters are first estimated by the expectile method and afterwards, when the number of explanatory variables can be large, by the adaptive LASSO expectile method which directly carries out the automatic selection of variables. We also obtain the convergence rate and asymptotic normality for the two estimators, while showing the sparsity property for the censored adaptive LASSO expectile estimator. A numerical study using Monte Carlo simulations confirms the theoretical results and demonstrates the competitive performance of the two proposed estimators. The usefulness of these estimators is illustrated by applying them to three survival data sets.","sentences":["Based on the expectile loss function and the adaptive LASSO penalty, the paper proposes and studies the estimation methods for the accelerated failure time (AFT) model.","In this approach, we need to estimate the survival function of the censoring variable by the Kaplan-Meier estimator.","The AFT model parameters are first estimated by the expectile method and afterwards, when the number of explanatory variables can be large, by the adaptive LASSO expectile method which directly carries out the automatic selection of variables.","We also obtain the convergence rate and asymptotic normality for the two estimators, while showing the sparsity property for the censored adaptive LASSO expectile estimator.","A numerical study using Monte Carlo simulations confirms the theoretical results and demonstrates the competitive performance of the two proposed estimators.","The usefulness of these estimators is illustrated by applying them to three survival data sets."],"url":"http://arxiv.org/abs/2402.03203v1","category":"math.ST"}
{"created":"2024-02-05 16:58:22","title":"Multiple testing using uniform filtering of ordered p-values","abstract":"We investigate the multiplicity model with m values of some test statistic independently drawn from a mixture of no effect (null) and positive effect (alternative), where we seek to identify, the alternative test results with a controlled error rate. We are interested in the case where the alternatives are rare. A number of multiple testing procedures filter the set of ordered p-values in order to eliminate the nulls. Such an approach can only work if the p-values originating from the alternatives form one or several identifiable clusters. The Benjamini and Hochberg (BH) method, for example, assumes that this cluster occurs in a small interval $(0,\\Delta)$ and filters out all or most of the ordered p-values $p_{(r)}$ above a linear threshold $s \\times r$. In repeated applications this filter controls the false discovery rate via the slope s. We propose a new adaptive filter that deletes the p-values from regions of uniform distribution. In cases where a single cluster remains, the p-values in an interval are declared alternatives, with the mid-point and the length of the interval chosen by controlling the data-dependent FDR at a desired level.","sentences":["We investigate the multiplicity model with m values of some test statistic independently drawn from a mixture of no effect (null) and positive effect (alternative), where we seek to identify, the alternative test results with a controlled error rate.","We are interested in the case where the alternatives are rare.","A number of multiple testing procedures filter the set of ordered p-values in order to eliminate the nulls.","Such an approach can only work if the p-values originating from the alternatives form one or several identifiable clusters.","The Benjamini and Hochberg (BH) method, for example, assumes that this cluster occurs in a small interval $(0,\\Delta)$ and filters out all or most of the ordered p-values $p_{(r)}$ above a linear threshold $s \\times r$.","In repeated applications this filter controls the false discovery rate via the slope s.","We propose a new adaptive filter that deletes the p-values from regions of uniform distribution.","In cases where a single cluster remains, the p-values in an interval are declared alternatives, with the mid-point and the length of the interval chosen by controlling the data-dependent FDR at a desired level."],"url":"http://arxiv.org/abs/2402.03192v1","category":"stat.ME"}
{"created":"2024-02-05 15:32:10","title":"High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy","abstract":"Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions. We evaluate our proposed method on various benchmark synthetic and real-world problems. The results demonstrate that our method outperforms existing state-of-the-art techniques.","sentences":["Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions.","However, it is well known that applying BO to high-dimensional optimization problems is challenging.","To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions.","In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy.","Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function.","Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum.","Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions.","We evaluate our proposed method on various benchmark synthetic and real-world problems.","The results demonstrate that our method outperforms existing state-of-the-art techniques."],"url":"http://arxiv.org/abs/2402.03104v1","category":"stat.ML"}
{"created":"2024-02-05 14:15:22","title":"Superconducting Qubits Above 20 GHz Operating over 200 mK","abstract":"Current state-of-the-art superconducting microwave qubits are cooled to extremely low temperatures to avoid sources of decoherence. Higher qubit operating temperatures would significantly increase the cooling power available, which is desirable for scaling up the number of qubits in quantum computing architectures and integrating qubits in experiments requiring increased heat dissipation. To operate superconducting qubits at higher temperatures, it is necessary to address both quasiparticle decoherence (which becomes significant for aluminum junctions above 160 mK) and dephasing from thermal microwave photons (which are problematic above 50 mK). Using low-loss niobium trilayer junctions, which have reduced sensitivity to quasiparticles due to niobium's higher superconducting transition temperature, we fabricate transmons with higher frequencies than previously studied, up to 24 GHz. We measure decoherence and dephasing times of about 1 us, corresponding to average qubit quality factors of approximately $10^5$, and find that decoherence is unaffected by quasiparticles up to 1 K. Without relaxation from quasiparticles, we are able to explore dephasing from purely thermal sources, finding that our qubits can operate up to approximately 250 mK while maintaining similar performance. The thermal resilience of these qubits creates new options for scaling up quantum processors, enables hybrid quantum experiments with high heat dissipation budgets, and introduces a material platform for even higher-frequency qubits.","sentences":["Current state-of-the-art superconducting microwave qubits are cooled to extremely low temperatures to avoid sources of decoherence.","Higher qubit operating temperatures would significantly increase the cooling power available, which is desirable for scaling up the number of qubits in quantum computing architectures and integrating qubits in experiments requiring increased heat dissipation.","To operate superconducting qubits at higher temperatures, it is necessary to address both quasiparticle decoherence (which becomes significant for aluminum junctions above 160 mK) and dephasing from thermal microwave photons (which are problematic above 50 mK).","Using low-loss niobium trilayer junctions, which have reduced sensitivity to quasiparticles due to niobium's higher superconducting transition temperature, we fabricate transmons with higher frequencies than previously studied, up to 24 GHz.","We measure decoherence and dephasing times of about 1 us, corresponding to average qubit quality factors of approximately $10^5$, and find that decoherence is unaffected by quasiparticles up to 1 K. Without relaxation from quasiparticles, we are able to explore dephasing from purely thermal sources, finding that our qubits can operate up to approximately 250 mK while maintaining similar performance.","The thermal resilience of these qubits creates new options for scaling up quantum processors, enables hybrid quantum experiments with high heat dissipation budgets, and introduces a material platform for even higher-frequency qubits."],"url":"http://arxiv.org/abs/2402.03031v1","category":"quant-ph"}
{"created":"2024-02-05 12:26:01","title":"Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers","abstract":"Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\\textsf{DynaBRO}$ -- a new method capable of withstanding $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentage of Byzantine workers.","sentences":["Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework.","However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process.","This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks.","Addressing this limitation, we propose $\\textsf{DynaBRO}$ -- a new method capable of withstanding $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting.","Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies.","Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentage of Byzantine workers."],"url":"http://arxiv.org/abs/2402.02951v1","category":"cs.LG"}
{"created":"2024-02-05 10:55:47","title":"Time-, Memory- and Parameter-Efficient Visual Adaptation","abstract":"As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly. We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.","sentences":["As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks.","Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained.","They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.","We propose an adaptation method which does not backpropagate gradients through the backbone.","We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone.","As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage.","Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too.","We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism.","Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time."],"url":"http://arxiv.org/abs/2402.02887v1","category":"cs.CV"}
{"created":"2024-02-05 10:17:36","title":"Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation","abstract":"Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.","sentences":["Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks.","Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods.","This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions.","Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator.","In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case.","Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning."],"url":"http://arxiv.org/abs/2402.02857v1","category":"stat.ML"}
{"created":"2024-02-05 08:25:24","title":"Time-velocity decay of solutions to the non-cutoff Boltzmann equation in the whole space","abstract":"In this paper, we consider the perturbed solutions with polynomial tail in large velocities for the non-cutoff Boltzmann equation near global Maxwellians in the whole space. The global in time existence is proved in the weighted Sobolev spaces and the almost optimal time decay is obtained in Fourier transform based low-regularity spaces. The result shows a time-velocity decay structure of solutions that can be decomposed into two parts. One part allows the slow polynomial tail in large velocities, carries the initial data and enjoys the exponential or arbitrarily large polynomial time decay. The other part, with zero initial data, is dominated by the non-negative definite symmetric dissipation and has the exponential velocity decay but only the slow polynomial time decay.","sentences":["In this paper, we consider the perturbed solutions with polynomial tail in large velocities for the non-cutoff Boltzmann equation near global Maxwellians in the whole space.","The global in time existence is proved in the weighted Sobolev spaces and the almost optimal time decay is obtained in Fourier transform based low-regularity spaces.","The result shows a time-velocity decay structure of solutions that can be decomposed into two parts.","One part allows the slow polynomial tail in large velocities, carries the initial data and enjoys the exponential or arbitrarily large polynomial time decay.","The other part, with zero initial data, is dominated by the non-negative definite symmetric dissipation and has the exponential velocity decay but only the slow polynomial time decay."],"url":"http://arxiv.org/abs/2402.02804v1","category":"math.AP"}
{"created":"2024-02-05 06:58:58","title":"Multiple Neuronal Specializations Elicited By Socially Driven Recognition Of Food Odors","abstract":"This study investigates the dynamics of non-spatial specializations in hippocampal place cells during exposure to novel environments. Hippocampal place cells, known for their role in spatial mapping, exhibit multi-modal responses to sensory cues. The research focuses on understanding how these cells adapt their specialization in response to novel stimuli, specifically examining non-spatial determinants such as odors and social interactions. Using a social-driven food odor recognition model in mice, the study records CA1 hippocampal neuron activity through miniscope imaging. The experimental design involves demonstrations of novel odors to mice, followed by observation sessions with food options. The analysis employs deep neural network tools for behavior tracking and the custom-developed INTENS software package for identifying neural specializations. Results indicate multiple specializations, particularly those related to odor, with differences observed between training and testing sessions. The findings suggest a temporal aspect to the formation of these specializations in novel conditions, necessitating further investigation for precise tracking.","sentences":["This study investigates the dynamics of non-spatial specializations in hippocampal place cells during exposure to novel environments.","Hippocampal place cells, known for their role in spatial mapping, exhibit multi-modal responses to sensory cues.","The research focuses on understanding how these cells adapt their specialization in response to novel stimuli, specifically examining non-spatial determinants such as odors and social interactions.","Using a social-driven food odor recognition model in mice, the study records CA1 hippocampal neuron activity through miniscope imaging.","The experimental design involves demonstrations of novel odors to mice, followed by observation sessions with food options.","The analysis employs deep neural network tools for behavior tracking and the custom-developed INTENS software package for identifying neural specializations.","Results indicate multiple specializations, particularly those related to odor, with differences observed between training and testing sessions.","The findings suggest a temporal aspect to the formation of these specializations in novel conditions, necessitating further investigation for precise tracking."],"url":"http://arxiv.org/abs/2402.02766v1","category":"q-bio.NC"}
{"created":"2024-02-05 06:30:47","title":"Exploring the Effects of Shared Autonomy on Cognitive Load and Trust in Human-Robot Interaction","abstract":"Teleoperation is increasingly recognized as a viable solution for deploying robots in hazardous environments. Controlling a robot to perform a complex or demanding task may overload operators resulting in poor performance. To design a robot controller to assist the human in executing such challenging tasks, a comprehensive understanding of the interplay between the robot's autonomous behavior and the operator's internal state is essential. In this paper, we investigate the relationships between robot autonomy and both the human user's cognitive load and trust levels, and the potential existence of three-way interactions in the robot-assisted execution of the task. Our user study (N=24) results indicate that while autonomy level influences the teleoperator's perceived cognitive load and trust, there is no clear interaction between these factors. Instead, these elements appear to operate independently, thus highlighting the need to consider both cognitive load and trust as distinct but interrelated factors in varying the robot autonomy level in shared-control settings. This insight is crucial for the development of more effective and adaptable assistive robotic systems.","sentences":["Teleoperation is increasingly recognized as a viable solution for deploying robots in hazardous environments.","Controlling a robot to perform a complex or demanding task may overload operators resulting in poor performance.","To design a robot controller to assist the human in executing such challenging tasks, a comprehensive understanding of the interplay between the robot's autonomous behavior and the operator's internal state is essential.","In this paper, we investigate the relationships between robot autonomy and both the human user's cognitive load and trust levels, and the potential existence of three-way interactions in the robot-assisted execution of the task.","Our user study (N=24) results indicate that while autonomy level influences the teleoperator's perceived cognitive load and trust, there is no clear interaction between these factors.","Instead, these elements appear to operate independently, thus highlighting the need to consider both cognitive load and trust as distinct but interrelated factors in varying the robot autonomy level in shared-control settings.","This insight is crucial for the development of more effective and adaptable assistive robotic systems."],"url":"http://arxiv.org/abs/2402.02758v1","category":"cs.RO"}
{"created":"2024-02-05 06:24:58","title":"Heterogeneous Solvent Dissipation Coupled with Particle Rearrangement and in Shear Thinning Non-Brownian Suspensions","abstract":"Dense non-Brownian suspensions exhibit significant shear thinning, although a comprehensive understanding of the full scope of this phenomenon remains elusive. This study numerically reveals intimate heterogenous coupled dynamics between many-body particle motions and solvent hydrodynamics in shear-thinning non-Brownian suspensions. We demonstrate the spatially correlated viscous dissipation and particle motions; they share the same characteristic length, which decreases with increasing shear rate. We further show that, at lower shear rates, significant particle density changes are induced against the incompressibility of the solvent, suggesting the cooperative creation and annihilation of gaps and flow channels. We discuss that hydrodynamic interactions may substantially restrict particle rearrangements even in highly dense suspensions, influencing the quantitative aspects of macroscopic rheology.","sentences":["Dense non-Brownian suspensions exhibit significant shear thinning, although a comprehensive understanding of the full scope of this phenomenon remains elusive.","This study numerically reveals intimate heterogenous coupled dynamics between many-body particle motions and solvent hydrodynamics in shear-thinning non-Brownian suspensions.","We demonstrate the spatially correlated viscous dissipation and particle motions; they share the same characteristic length, which decreases with increasing shear rate.","We further show that, at lower shear rates, significant particle density changes are induced against the incompressibility of the solvent, suggesting the cooperative creation and annihilation of gaps and flow channels.","We discuss that hydrodynamic interactions may substantially restrict particle rearrangements even in highly dense suspensions, influencing the quantitative aspects of macroscopic rheology."],"url":"http://arxiv.org/abs/2402.02756v1","category":"cond-mat.soft"}
{"created":"2024-02-05 05:38:50","title":"Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective","abstract":"In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.","sentences":["In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving.","However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored.","In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset.","Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios.","Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach."],"url":"http://arxiv.org/abs/2402.02738v1","category":"cs.CV"}
{"created":"2024-02-05 05:31:03","title":"Timed-Elastic-Band Based Variable Splitting for Autonomous Trajectory Planning","abstract":"Existing trajectory planning methods are struggling to handle the issue of autonomous track swinging during navigation, resulting in significant errors when reaching the destination. In this article, we address autonomous trajectory planning problems, which aims at developing innovative solutions to enhance the adaptability and robustness of unmanned systems in navigating complex and dynamic environments. We first introduce the variable splitting (VS) method as a constrained optimization method to reimagine the renowned Timed-Elastic-Band (TEB) algorithm, resulting in a novel collision avoidance approach named Timed-Elastic-Band based variable splitting (TEB-VS). The proposed TEB-VS demonstrates superior navigation stability, while maintaining nearly identical resource consumption to TEB. We then analyze the convergence of the proposed TEB-VS method. To evaluate the effectiveness and efficiency of TEB-VS, extensive experiments have been conducted using TurtleBot2 in both simulated environments and real-world datasets.","sentences":["Existing trajectory planning methods are struggling to handle the issue of autonomous track swinging during navigation, resulting in significant errors when reaching the destination.","In this article, we address autonomous trajectory planning problems, which aims at developing innovative solutions to enhance the adaptability and robustness of unmanned systems in navigating complex and dynamic environments.","We first introduce the variable splitting (VS) method as a constrained optimization method to reimagine the renowned Timed-Elastic-Band (TEB) algorithm, resulting in a novel collision avoidance approach named Timed-Elastic-Band based variable splitting (TEB-VS).","The proposed TEB-VS demonstrates superior navigation stability, while maintaining nearly identical resource consumption to TEB.","We then analyze the convergence of the proposed TEB-VS method.","To evaluate the effectiveness and efficiency of TEB-VS, extensive experiments have been conducted using TurtleBot2 in both simulated environments and real-world datasets."],"url":"http://arxiv.org/abs/2402.02735v1","category":"eess.SY"}
{"created":"2024-02-05 04:16:55","title":"Unconditionally energy stable IEQ-FEMs for the Cahn-Hilliard equation and Allen-Cahn equation","abstract":"In this paper, we present several unconditionally energy-stable invariant energy quadratization (IEQ) finite element methods (FEMs) with linear, first- and second-order accuracy for solving both the Cahn-Hilliard equation and the Allen-Cahn equation. For time discretization, we compare three distinct IEQ-FEM schemes that position the intermediate function introduced by the IEQ approach in different function spaces: finite element space, continuous function space, or a combination of these spaces. Rigorous proofs establishing the existence and uniqueness of the numerical solution, along with analyses of energy dissipation for both equations and mass conservation for the Cahn-Hilliard equation, are provided. The proposed schemes' accuracy, efficiency, and solution properties are demonstrated through numerical experiments.","sentences":["In this paper, we present several unconditionally energy-stable invariant energy quadratization (IEQ) finite element methods (FEMs) with linear, first- and second-order accuracy for solving both the Cahn-Hilliard equation and the Allen-Cahn equation.","For time discretization, we compare three distinct IEQ-FEM schemes that position the intermediate function introduced by the IEQ approach in different function spaces: finite element space, continuous function space, or a combination of these spaces.","Rigorous proofs establishing the existence and uniqueness of the numerical solution, along with analyses of energy dissipation for both equations and mass conservation for the Cahn-Hilliard equation, are provided.","The proposed schemes' accuracy, efficiency, and solution properties are demonstrated through numerical experiments."],"url":"http://arxiv.org/abs/2402.02712v1","category":"math.NA"}
{"created":"2024-02-05 03:41:03","title":"Dielectrics for Two-Dimensional Transition Metal Dichalcogenide Applications","abstract":"Despite over a decade of intense research efforts, the full potential of two-dimensional transition metal dichalcogenides continues to be limited by major challenges. The lack of compatible and scalable dielectric materials and integration techniques restrict device performances and their commercial applications Conventional dielectric integration techniques for bulk semiconductors are difficult to adapt for atomically thin two-dimensional materials. This review provides a brief introduction into various common and emerging dielectric synthesis and integration techniques and discusses their applicability for 2D transition metal dichalcogenides. Dielectric integration for various applications is reviewed in subsequent sections including nanoelectronics, optoelectronics, flexible electronics, valleytronics, biosensing, quantum information processing, and quantum sensing. For each application, we introduce basic device working principles, discuss the specific dielectric requirements, review current progress, present key challenges, and offer insights into future prospects and opportunities.","sentences":["Despite over a decade of intense research efforts, the full potential of two-dimensional transition metal dichalcogenides continues to be limited by major challenges.","The lack of compatible and scalable dielectric materials and integration techniques restrict device performances and their commercial applications Conventional dielectric integration techniques for bulk semiconductors are difficult to adapt for atomically thin two-dimensional materials.","This review provides a brief introduction into various common and emerging dielectric synthesis and integration techniques and discusses their applicability for 2D transition metal dichalcogenides.","Dielectric integration for various applications is reviewed in subsequent sections including nanoelectronics, optoelectronics, flexible electronics, valleytronics, biosensing, quantum information processing, and quantum sensing.","For each application, we introduce basic device working principles, discuss the specific dielectric requirements, review current progress, present key challenges, and offer insights into future prospects and opportunities."],"url":"http://arxiv.org/abs/2402.02707v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 02:02:57","title":"On the integrability of spinning-body dynamics around black holes","abstract":"In general relativity, the trajectory of a celestial body in a given spacetime is influenced by its proper rotation, or \\textit{spin}. We present a covariant and physically self-consistent Hamiltonian framework to study this motion, at linear order in the body's spin and in an arbitrary fixed spacetime. The choice of center-of-mass and degeneracies coming from Lorentz invariance are treated rigorously with adapted tools from Hamiltonian mechanics. Applying the formalism to a background space-time described by the Kerr metric, we prove that the motion of a spinning body around a generic rotating black hole is an \\textit{integrable} Hamiltonian system. In particular, linear-in-spin effects do not break the integrability of Kerr geodesics, and induce no \\textit{chaos} within the associated phase space. Our findings suggest a natural way to improve current gravitational waveform modelling for asymmetric binary systems, and provide a mean to extend classical features of Kerr geodesics to linear-in-spin trajectories.","sentences":["In general relativity, the trajectory of a celestial body in a given spacetime is influenced by its proper rotation, or \\textit{spin}.","We present a covariant and physically self-consistent Hamiltonian framework to study this motion, at linear order in the body's spin and in an arbitrary fixed spacetime.","The choice of center-of-mass and degeneracies coming from Lorentz invariance are treated rigorously with adapted tools from Hamiltonian mechanics.","Applying the formalism to a background space-time described by the Kerr metric, we prove that the motion of a spinning body around a generic rotating black hole is an \\textit{integrable} Hamiltonian system.","In particular, linear-in-spin effects do not break the integrability of Kerr geodesics, and induce no \\textit{chaos} within the associated phase space.","Our findings suggest a natural way to improve current gravitational waveform modelling for asymmetric binary systems, and provide a mean to extend classical features of Kerr geodesics to linear-in-spin trajectories."],"url":"http://arxiv.org/abs/2402.02670v1","category":"gr-qc"}
{"created":"2024-02-05 01:50:04","title":"A Priori Error Estimation of Physics-Informed Neural Networks Solving Allen--Cahn and Cahn--Hilliard Equations","abstract":"This paper aims to analyze errors in the implementation of the Physics-Informed Neural Network (PINN) for solving the Allen--Cahn (AC) and Cahn--Hilliard (CH) partial differential equations (PDEs). The accuracy of PINN is still challenged when dealing with strongly non-linear and higher-order time-varying PDEs. To address this issue, we introduce a stable and bounded self-adaptive weighting scheme known as Residuals-RAE, which ensures fair training and effectively captures the solution. By incorporating this new training loss function, we conduct numerical experiments on 1D and 2D AC and CH systems to validate our theoretical findings. Our theoretical analysis demonstrates that feedforward neural networks with two hidden layers and tanh activation function effectively bound the PINN approximation errors for the solution field, temporal derivative, and nonlinear term of the AC and CH equations by the training loss and number of collocation points.","sentences":["This paper aims to analyze errors in the implementation of the Physics-Informed Neural Network (PINN) for solving the Allen--Cahn (AC) and Cahn--Hilliard (CH) partial differential equations (PDEs).","The accuracy of PINN is still challenged when dealing with strongly non-linear and higher-order time-varying PDEs.","To address this issue, we introduce a stable and bounded self-adaptive weighting scheme known as Residuals-RAE, which ensures fair training and effectively captures the solution.","By incorporating this new training loss function, we conduct numerical experiments on 1D and 2D AC and CH systems to validate our theoretical findings.","Our theoretical analysis demonstrates that feedforward neural networks with two hidden layers and tanh activation function effectively bound the PINN approximation errors for the solution field, temporal derivative, and nonlinear term of the AC and CH equations by the training loss and number of collocation points."],"url":"http://arxiv.org/abs/2402.02667v1","category":"math.NA"}
{"created":"2024-02-05 00:44:57","title":"Densely Decoded Networks with Adaptive Deep Supervision for Medical Image Segmentation","abstract":"Medical image segmentation using deep neural networks has been highly successful. However, the effectiveness of these networks is often limited by inadequate dense prediction and inability to extract robust features. To achieve refined dense prediction, we propose densely decoded networks (ddn), by selectively introducing 'crutch' network connections. Such 'crutch' connections in each upsampling stage of the network decoder (1) enhance target localization by incorporating high resolution features from the encoder, and (2) improve segmentation by facilitating multi-stage contextual information flow. Further, we present a training strategy based on adaptive deep supervision (ads), which exploits and adapts specific attributes of input dataset, for robust feature extraction. In particular, ads strategically locates and deploys auxiliary supervision, by matching the average input object size with the layer-wise effective receptive fields (lerf) of a network, resulting in a class of ddns. Such inclusion of 'companion objective' from a specific hidden layer, helps the model pay close attention to some distinct input-dependent features, which the network might otherwise 'ignore' during training. Our new networks and training strategy are validated on 4 diverse datasets of different modalities, demonstrating their effectiveness.","sentences":["Medical image segmentation using deep neural networks has been highly successful.","However, the effectiveness of these networks is often limited by inadequate dense prediction and inability to extract robust features.","To achieve refined dense prediction, we propose densely decoded networks (ddn), by selectively introducing 'crutch' network connections.","Such 'crutch' connections in each upsampling stage of the network decoder (1) enhance target localization by incorporating high resolution features from the encoder, and (2) improve segmentation by facilitating multi-stage contextual information flow.","Further, we present a training strategy based on adaptive deep supervision (ads), which exploits and adapts specific attributes of input dataset, for robust feature extraction.","In particular, ads strategically locates and deploys auxiliary supervision, by matching the average input object size with the layer-wise effective receptive fields (lerf) of a network, resulting in a class of ddns.","Such inclusion of 'companion objective' from a specific hidden layer, helps the model pay close attention to some distinct input-dependent features, which the network might otherwise 'ignore' during training.","Our new networks and training strategy are validated on 4 diverse datasets of different modalities, demonstrating their effectiveness."],"url":"http://arxiv.org/abs/2402.02649v1","category":"cs.CV"}
{"created":"2024-02-04 20:29:43","title":"A Review of Full-Sized Autonomous Racing Vehicle Sensor Architecture","abstract":"In the landscape of technological innovation, autonomous racing is a dynamic and challenging domain that not only pushes the limits of technology, but also plays a crucial role in advancing and fostering a greater acceptance of autonomous systems. This paper thoroughly explores challenges and advances in autonomous racing vehicle design and performance, focusing on Roborace and the Indy Autonomous Challenge (IAC). This review provides a detailed analysis of sensor setups, architectural nuances, and test metrics on these cutting-edge platforms. In Roborace, the evolution from Devbot 1.0 to Robocar and Devbot 2.0 is detailed, revealing insights into sensor configurations and performance outcomes. The examination extends to the IAC, which is dedicated to high-speed self-driving vehicles, emphasizing developmental trajectories and sensor adaptations. By reviewing these platforms, the analysis provides valuable insight into autonomous driving racing, contributing to a broader understanding of sensor architectures and the challenges faced. This review supports future advances in full-scale autonomous racing technology.","sentences":["In the landscape of technological innovation, autonomous racing is a dynamic and challenging domain that not only pushes the limits of technology, but also plays a crucial role in advancing and fostering a greater acceptance of autonomous systems.","This paper thoroughly explores challenges and advances in autonomous racing vehicle design and performance, focusing on Roborace and the Indy Autonomous Challenge (IAC).","This review provides a detailed analysis of sensor setups, architectural nuances, and test metrics on these cutting-edge platforms.","In Roborace, the evolution from Devbot 1.0 to Robocar and Devbot 2.0 is detailed, revealing insights into sensor configurations and performance outcomes.","The examination extends to the IAC, which is dedicated to high-speed self-driving vehicles, emphasizing developmental trajectories and sensor adaptations.","By reviewing these platforms, the analysis provides valuable insight into autonomous driving racing, contributing to a broader understanding of sensor architectures and the challenges faced.","This review supports future advances in full-scale autonomous racing technology."],"url":"http://arxiv.org/abs/2402.02603v1","category":"cs.RO"}
{"created":"2024-02-04 19:01:27","title":"Grover-QAOA for 3-SAT: Quadratic Speedup, Fair-Sampling, and Parameter Clustering","abstract":"The SAT problem is a prototypical NP-complete problem of fundamental importance in computational complexity theory with many applications in science and engineering; as such, it has long served as an essential benchmark for classical and quantum algorithms. This study shows numerical evidence for a quadratic speedup of the Grover Quantum Approximate Optimization Algorithm (G-QAOA) over random sampling for finding all solutions to 3-SAT problems (All-SAT). G-QAOA is less resource-intensive and more adaptable for 3-SAT and Max-SAT than Grover's algorithm, and it surpasses conventional QAOA in its ability to sample all solutions. We show these benefits by classical simulations of many-round G-QAOA on thousands of random 3-SAT instances. We also observe G-QAOA advantages on the IonQ Aria quantum computer for small instances, finding that current hardware suffices to determine and sample all solutions. Interestingly, a single-angle-pair constraint that uses the same pair of angles at each G-QAOA round greatly reduces the classical computational overhead of optimizing the G-QAOA angles while preserving its quadratic speedup. We also find parameter clustering of the angles. The single-angle-pair protocol and parameter clustering significantly reduce obstacles to classical optimization of the G-QAOA angles.","sentences":["The SAT problem is a prototypical NP-complete problem of fundamental importance in computational complexity theory with many applications in science and engineering; as such, it has long served as an essential benchmark for classical and quantum algorithms.","This study shows numerical evidence for a quadratic speedup of the Grover Quantum Approximate Optimization Algorithm (G-QAOA) over random sampling for finding all solutions to 3-SAT problems (All-SAT).","G-QAOA is less resource-intensive and more adaptable for 3-SAT and Max-SAT than Grover's algorithm, and it surpasses conventional QAOA in its ability to sample all solutions.","We show these benefits by classical simulations of many-round G-QAOA on thousands of random 3-SAT instances.","We also observe G-QAOA advantages on the IonQ Aria quantum computer for small instances, finding that current hardware suffices to determine and sample all solutions.","Interestingly, a single-angle-pair constraint that uses the same pair of angles at each G-QAOA round greatly reduces the classical computational overhead of optimizing the G-QAOA angles while preserving its quadratic speedup.","We also find parameter clustering of the angles.","The single-angle-pair protocol and parameter clustering significantly reduce obstacles to classical optimization of the G-QAOA angles."],"url":"http://arxiv.org/abs/2402.02585v1","category":"quant-ph"}
{"created":"2024-02-04 18:53:11","title":"SYK Correlators from 2D Liouville-de Sitter Gravity","abstract":"We introduce and study a candidate gravity dual to the double scaled SYK model in the form of an exactly soluble 2D de Sitter gravity model consisting of two spacelike Liouville CFTs with complex central charge adding up to $c_+ + c_- = 26$. In [1] it was shown that the two-point function of physical operators in a doubled SYK model matches in the semi-classical limit with the Green's function of a massive scalar field in 3D de Sitter space. As further evidence of the duality, we adapt a result from Zamolodchikov to compute the boundary two-point function of the 2D Liouville-de Sitter gravity model on a disk and find that it reproduces the exact DSSYK two-point function to all orders in $\\lambda=p^2/N$. We describe how the 2D Liouville-de Sitter gravity model arises from quantizing 3D de Sitter gravity.","sentences":["We introduce and study a candidate gravity dual to the double scaled SYK model in the form of an exactly soluble 2D de Sitter gravity model consisting of two spacelike Liouville CFTs with complex central charge adding up to $c_+ + c_- = 26$.","In [1] it was shown that the two-point function of physical operators in a doubled SYK model matches in the semi-classical limit with the Green's function of a massive scalar field in 3D de Sitter space.","As further evidence of the duality, we adapt a result from Zamolodchikov to compute the boundary two-point function of the 2D Liouville-de Sitter gravity model on a disk and find that it reproduces the exact DSSYK two-point function to all orders in $\\lambda=p^2/N$. We describe how the 2D Liouville-de Sitter gravity model arises from quantizing 3D de Sitter gravity."],"url":"http://arxiv.org/abs/2402.02584v1","category":"hep-th"}
{"created":"2024-02-04 15:54:03","title":"Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators","abstract":"In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimize the time required to reach, time taken to stabilize, the maximum deviation from the desired value, and persistent tracking error in the steady state. This approach guarantees that position and velocity errors exponentially converge to zero, accounting for any initial and end-point variations, unknown modeling errors, and external disturbances. Theoretical assertions are validated through the presentation of simulation outcomes.","sentences":["In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety.","To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low-","and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment.","This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks.","First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning.","This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimize the time required to reach, time taken to stabilize, the maximum deviation from the desired value, and persistent tracking error in the steady state.","This approach guarantees that position and velocity errors exponentially converge to zero, accounting for any initial and end-point variations, unknown modeling errors, and external disturbances.","Theoretical assertions are validated through the presentation of simulation outcomes."],"url":"http://arxiv.org/abs/2402.02551v1","category":"cs.RO"}
{"created":"2024-02-04 15:27:46","title":"Identifying and Extracting Pedestrian Behavior in Critical Traffic Situations","abstract":"A better understanding of interactive pedestrian behavior in critical traffic situations is essential for the development of enhanced pedestrian safety systems. Real-world traffic observations play a decisive role in this, since they represent behavior in an unbiased way. In this work, we present an approach of how a subset of very considerable pedestrian-vehicle interactions can be derived from a camera-based observation system. For this purpose, we have examined road user trajectories automatically for establishing temporal and spatial relationships, using 110h hours of video recordings. In order to identify critical interactions, our approach combines the metric post-encroachment time with a newly introduced motion adaption metric. From more than 11,000 reconstructed pedestrian trajectories, 259 potential scenarios remained, using a post-encroachment time threshold of 2s. However, in 95% of cases, no adaptation of the pedestrian behavior was observed due to avoiding criticality. Applying the proposed motion adaption metric, only 21 critical scenarios remained. Manual investigations revealed that critical pedestrian vehicle interactions were present in 7 of those. They were further analyzed and made publicly available for developing pedestrian behavior models3. The results indicate that critical interactions in which the pedestrian perceives and reacts to the vehicle at a relatively late stage can be extracted using the proposed method.","sentences":["A better understanding of interactive pedestrian behavior in critical traffic situations is essential for the development of enhanced pedestrian safety systems.","Real-world traffic observations play a decisive role in this, since they represent behavior in an unbiased way.","In this work, we present an approach of how a subset of very considerable pedestrian-vehicle interactions can be derived from a camera-based observation system.","For this purpose, we have examined road user trajectories automatically for establishing temporal and spatial relationships, using 110h hours of video recordings.","In order to identify critical interactions, our approach combines the metric post-encroachment time with a newly introduced motion adaption metric.","From more than 11,000 reconstructed pedestrian trajectories, 259 potential scenarios remained, using a post-encroachment time threshold of 2s.","However, in 95% of cases, no adaptation of the pedestrian behavior was observed due to avoiding criticality.","Applying the proposed motion adaption metric, only 21 critical scenarios remained.","Manual investigations revealed that critical pedestrian vehicle interactions were present in 7 of those.","They were further analyzed and made publicly available for developing pedestrian behavior models3.","The results indicate that critical interactions in which the pedestrian perceives and reacts to the vehicle at a relatively late stage can be extracted using the proposed method."],"url":"http://arxiv.org/abs/2402.02533v1","category":"cs.RO"}
{"created":"2024-02-04 15:08:02","title":"A minimal model of cognition based on oscillatory and reinforcement processes","abstract":"Building mathematical models of brains is difficult because of the sheer complexity of the problem. One potential approach is to start by identifying models of basal cognition, which give an abstract representation of a range organisms without central nervous systems, including fungi, slime moulds and bacteria. We propose one such model, demonstrating how a combination of oscillatory and current-based reinforcement processes can be used to couple resources in an efficient manner. We first show that our model connects resources in an efficient manner when the environment is constant. We then show that in an oscillatory environment our model builds efficient solutions, provided the environmental oscillations are sufficiently out of phase. We show that amplitude differences can promote efficient solutions and that the system is robust to frequency differences. We identify connections between our model and basal cognition in biological systems and slime moulds, in particular, showing how oscillatory and problem-solving properties of these systems are captured by our model.","sentences":["Building mathematical models of brains is difficult because of the sheer complexity of the problem.","One potential approach is to start by identifying models of basal cognition, which give an abstract representation of a range organisms without central nervous systems, including fungi, slime moulds and bacteria.","We propose one such model, demonstrating how a combination of oscillatory and current-based reinforcement processes can be used to couple resources in an efficient manner.","We first show that our model connects resources in an efficient manner when the environment is constant.","We then show that in an oscillatory environment our model builds efficient solutions, provided the environmental oscillations are sufficiently out of phase.","We show that amplitude differences can promote efficient solutions and that the system is robust to frequency differences.","We identify connections between our model and basal cognition in biological systems and slime moulds, in particular, showing how oscillatory and problem-solving properties of these systems are captured by our model."],"url":"http://arxiv.org/abs/2402.02520v1","category":"q-bio.NC"}
{"created":"2024-02-04 13:46:03","title":"Indicator Random Processes and its Application for Modeling Open Stochastic Systems","abstract":"The authors present a method of indicator random processes, applicable to constructing models of jump processes associated with diffusion process. Indicator random processes are processes that take only two values: 1 and 0, in accordance with some probabilistic laws. It is shown that the indicator random process is invariant when reduced to an arbitrary positive degree. Equations with random coefficients used in modeling dynamic systems, when applying the method of indicator random processes, can take into account the possibility of adaptation to external changes, including random ones, in order to preserve indicators important for the existence of the system, which can be continuous or discrete. In the case of indicator random processes, defined as functions of the Poisson process, equations for dynamic processes in a media with abruptly changing properties are constructed and studied. To study the capabilities of the proposed method, dynamic models of the diffusion process in media with delay centers and diffusion processes during transitions by switching from one subspace to another were studied. For these models, equations for characteristic functions are constructed. Using the method of indicator random processes, a characteristic function for the Kac model was constructed. It is shown that in the case of dependence of the indicator random process on the Poisson process, the equation for the characteristic function corresponds to the telegraph equation. This result coincides with the result of Kac.","sentences":["The authors present a method of indicator random processes, applicable to constructing models of jump processes associated with diffusion process.","Indicator random processes are processes that take only two values: 1 and 0, in accordance with some probabilistic laws.","It is shown that the indicator random process is invariant when reduced to an arbitrary positive degree.","Equations with random coefficients used in modeling dynamic systems, when applying the method of indicator random processes, can take into account the possibility of adaptation to external changes, including random ones, in order to preserve indicators important for the existence of the system, which can be continuous or discrete.","In the case of indicator random processes, defined as functions of the Poisson process, equations for dynamic processes in a media with abruptly changing properties are constructed and studied.","To study the capabilities of the proposed method, dynamic models of the diffusion process in media with delay centers and diffusion processes during transitions by switching from one subspace to another were studied.","For these models, equations for characteristic functions are constructed.","Using the method of indicator random processes, a characteristic function for the Kac model was constructed.","It is shown that in the case of dependence of the indicator random process on the Poisson process, the equation for the characteristic function corresponds to the telegraph equation.","This result coincides with the result of Kac."],"url":"http://arxiv.org/abs/2402.02493v1","category":"math.DS"}
{"created":"2024-02-04 13:07:19","title":"Adaptive Downlink Localization in Near-Field and Far-Field","abstract":"This paper considers the problem of downlink localization of user equipment devices (UEs) that are either in the near-field (NF) or in the far-field (FF) of the array of the serving base station (BS). We propose a dual signaling scheme, which can be implemented at the BS, for localizing such UEs. The first scheme assumes FF, while the other assumes NF conditions. Both schemes comprise a beam-sweeping technique, employed by the BS, and a localization algorithm, employed by the UEs. The FF-based scheme enables beam-steering with a low signaling overhead, which is utilized for the proposed localization algorithm, while the NF-based scheme operates with a higher complexity. Specifically, our proposed localization scheme takes advantage of the relaxed structure of the FF, which yields low computational complexity, but is not suitable for operating in the NF. Since the compatibility and the performance of the FF- based scheme depends on the BS-to-UE distance, we study the limitations of FF-based procedure, explore the trade-off in terms of performance and resource requirements for the two schemes, and propose a triggering condition for operating the component schemes of the dual scheme. Also, we study the performance of an iterative localization algorithm that takes into account the accuracy-complexity trade-off and adapts to the actual position of the UE. We find that the conventional Fraunhofer distance is not sufficient for adapting localization algorithms in the mixed NF and FF environment.","sentences":["This paper considers the problem of downlink localization of user equipment devices (UEs) that are either in the near-field (NF) or in the far-field (FF) of the array of the serving base station (BS).","We propose a dual signaling scheme, which can be implemented at the BS, for localizing such UEs.","The first scheme assumes FF, while the other assumes NF conditions.","Both schemes comprise a beam-sweeping technique, employed by the BS, and a localization algorithm, employed by the UEs.","The FF-based scheme enables beam-steering with a low signaling overhead, which is utilized for the proposed localization algorithm, while the NF-based scheme operates with a higher complexity.","Specifically, our proposed localization scheme takes advantage of the relaxed structure of the FF, which yields low computational complexity, but is not suitable for operating in the NF.","Since the compatibility and the performance of the FF- based scheme depends on the BS-to-UE distance, we study the limitations of FF-based procedure, explore the trade-off in terms of performance and resource requirements for the two schemes, and propose a triggering condition for operating the component schemes of the dual scheme.","Also, we study the performance of an iterative localization algorithm that takes into account the accuracy-complexity trade-off and adapts to the actual position of the UE.","We find that the conventional Fraunhofer distance is not sufficient for adapting localization algorithms in the mixed NF and FF environment."],"url":"http://arxiv.org/abs/2402.02473v1","category":"cs.IT"}
{"created":"2024-02-04 13:06:40","title":"Current Induced Hidden States in Josephson Junctions","abstract":"Josephson junctions enable dissipation-less electrical current through metals and insulators below a critical current. Despite being central to quantum technology based on superconducting quantum bits and fundamental research into self-conjugate quasiparticles, the spatial distribution of super current flow at the junction and its predicted evolution with current bias and external magnetic field remain experimentally elusive. Revealing the hidden current flow, featureless in electrical resistance, helps understanding unconventional phenomena such as the nonreciprocal critical current, i.e., Josephson diode effect. Here we introduce a platform for nanoscale, quantitative, and nonperturbative visualization of the super current flow, overcoming the long-standing challenge. Utilizing a scanning magnetometer based on nitrogen vacancy centers in diamond, we uncover competing ground states electrically switchable within the zero-resistance regime. The competition results from the superconducting phase re-configuration induced by the Josephson current and kinetic inductance of thin-film superconductors. We further identify a new mechanism for the Josephson diode effect involving the Josephson current induced phase. The nanoscale super current flow emerges as a new experimental observable for elucidating unconventional superconductivity, and optimizing quantum computation and energy-efficient devices.","sentences":["Josephson junctions enable dissipation-less electrical current through metals and insulators below a critical current.","Despite being central to quantum technology based on superconducting quantum bits and fundamental research into self-conjugate quasiparticles, the spatial distribution of super current flow at the junction and its predicted evolution with current bias and external magnetic field remain experimentally elusive.","Revealing the hidden current flow, featureless in electrical resistance, helps understanding unconventional phenomena such as the nonreciprocal critical current, i.e., Josephson diode effect.","Here we introduce a platform for nanoscale, quantitative, and nonperturbative visualization of the super current flow, overcoming the long-standing challenge.","Utilizing a scanning magnetometer based on nitrogen vacancy centers in diamond, we uncover competing ground states electrically switchable within the zero-resistance regime.","The competition results from the superconducting phase re-configuration induced by the Josephson current and kinetic inductance of thin-film superconductors.","We further identify a new mechanism for the Josephson diode effect involving the Josephson current induced phase.","The nanoscale super current flow emerges as a new experimental observable for elucidating unconventional superconductivity, and optimizing quantum computation and energy-efficient devices."],"url":"http://arxiv.org/abs/2402.02472v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-04 10:00:00","title":"Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition","abstract":"Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities. Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations. Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods.","sentences":["Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction.","Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities.","To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers.","Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them.","Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities.","Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations.","Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods."],"url":"http://arxiv.org/abs/2402.02431v1","category":"cs.CV"}
{"created":"2024-02-04 09:58:42","title":"Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning","abstract":"As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\\boldsymbol{M}$ and its latent representation $\\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures, attaining the new state-of-the-art. We believe that our framework could open up avenues for new optimality bounds and COMRL algorithms.","sentences":["As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely.","Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations.","In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework.","Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\\boldsymbol{M}$ and its latent representation $\\boldsymbol{Z}$ by implementing various approximate bounds.","Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures, attaining the new state-of-the-art.","We believe that our framework could open up avenues for new optimality bounds and COMRL algorithms."],"url":"http://arxiv.org/abs/2402.02429v1","category":"cs.LG"}
{"created":"2024-02-04 09:45:35","title":"EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics","abstract":"Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into Eulerian fields to guide future prediction. Tracking key particles not only provides a clear and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency. Experimentally, EuLagNet excels in three challenging fluid prediction tasks, covering both 2D and 3D, simulated and real-world fluids.","sentences":["Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics.","However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction.","This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics.","Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time.","Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into Eulerian fields to guide future prediction.","Tracking key particles not only provides a clear and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency.","Experimentally, EuLagNet excels in three challenging fluid prediction tasks, covering both 2D and 3D, simulated and real-world fluids."],"url":"http://arxiv.org/abs/2402.02425v1","category":"cs.LG"}
{"created":"2024-02-04 09:40:52","title":"Influence of $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ Gravity on Cylindrical Collapse","abstract":"This article examines the dynamics of gravitational collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ gravity, where $\\mathcal{Q}=\\mathcal{R}_{\\mathrm{ab}}\\mathcal{T}^{\\mathrm{ab}}$. We consider self-gravitating anisotropic cylindrical geometry whose interior is filled with dissipative matter configuration and match it with exterior cylindrically symmetric spacetime at the hypersurface through junction conditions. We employ the Misner-Sharp and M\\\"{u}ler-Israel Stewart formalisms to derive the dynamical as well as transport equations corresponding to the model $\\mathcal{R}+\\Phi\\sqrt{\\mathcal{T}}+\\Psi\\mathcal{Q}$, where $\\Phi$ and $\\Psi$ are arbitrary coupling constants. We then establish some relations between these equations through which the impact of effective matter variables, heat dissipation and the bulk viscosity on the collapse rate is studied. Further, we express the Weyl scalar in terms of the effective matter sector. We also obtain the conformal flatness by applying some restrictions on the considered model and taking dust configuration into the account. Finally, we investigate various cases to check whether the modified corrections increase or decrease the collapse rate.","sentences":["This article examines the dynamics of gravitational collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ gravity, where $\\mathcal{Q}=\\mathcal{R}_{\\mathrm{ab}}\\mathcal{T}^{\\mathrm{ab}}$.","We consider self-gravitating anisotropic cylindrical geometry whose interior is filled with dissipative matter configuration and match it with exterior cylindrically symmetric spacetime at the hypersurface through junction conditions.","We employ the Misner-Sharp and M\\\"{u}ler-Israel Stewart formalisms to derive the dynamical as well as transport equations corresponding to the model $\\mathcal{R}+\\Phi\\sqrt{\\mathcal{T}}+\\Psi\\mathcal{Q}$, where $\\Phi$ and $\\Psi$ are arbitrary coupling constants.","We then establish some relations between these equations through which the impact of effective matter variables, heat dissipation and the bulk viscosity on the collapse rate is studied.","Further, we express the Weyl scalar in terms of the effective matter sector.","We also obtain the conformal flatness by applying some restrictions on the considered model and taking dust configuration into the account.","Finally, we investigate various cases to check whether the modified corrections increase or decrease the collapse rate."],"url":"http://arxiv.org/abs/2402.02424v1","category":"gr-qc"}
{"created":"2024-02-04 07:57:52","title":"Brain-Body-Task Co-Adaptation can Improve Autonomous Learning and Speed of Bipedal Walking","abstract":"Inspired by animals that co-adapt their brain and body to interact with the environment, we present a tendon-driven and over-actuated (i.e., n joint, n+1 actuators) bipedal robot that (i) exploits its backdrivable mechanical properties to manage body-environment interactions without explicit control, and (ii) uses a simple 3-layer neural network to learn to walk after only 2 minutes of 'natural' motor babbling (i.e., an exploration strategy that is compatible with leg and task dynamics; akin to childsplay). This brain-body collaboration first learns to produce feet cyclical movements 'in air' and, without further tuning, can produce locomotion when the biped is lowered to be in slight contact with the ground. In contrast, training with 2 minutes of 'naive' motor babbling (i.e., an exploration strategy that ignores leg task dynamics), does not produce consistent cyclical movements 'in air', and produces erratic movements and no locomotion when in slight contact with the ground. When further lowering the biped and making the desired leg trajectories reach 1cm below ground (causing the desired-vs-obtained trajectories error to be unavoidable), cyclical movements based on either natural or naive babbling presented almost equally persistent trends, and locomotion emerged with naive babbling. Therefore, we show how continual learning of walking in unforeseen circumstances can be driven by continual physical adaptation rooted in the backdrivable properties of the plant and enhanced by exploration strategies that exploit plant dynamics. Our studies also demonstrate that the bio-inspired codesign and co-adaptations of limbs and control strategies can produce locomotion without explicit control of trajectory errors.","sentences":["Inspired by animals that co-adapt their brain and body to interact with the environment, we present a tendon-driven and over-actuated (i.e., n joint, n+1 actuators) bipedal robot that (i) exploits its backdrivable mechanical properties to manage body-environment interactions without explicit control, and (ii) uses a simple 3-layer neural network to learn to walk after only 2 minutes of 'natural' motor babbling (i.e., an exploration strategy that is compatible with leg and task dynamics; akin to childsplay).","This brain-body collaboration first learns to produce feet cyclical movements 'in air' and, without further tuning, can produce locomotion when the biped is lowered to be in slight contact with the ground.","In contrast, training with 2 minutes of 'naive' motor babbling (i.e., an exploration strategy that ignores leg task dynamics), does not produce consistent cyclical movements 'in air', and produces erratic movements and no locomotion when in slight contact with the ground.","When further lowering the biped and making the desired leg trajectories reach 1cm below ground (causing the desired-vs-obtained trajectories error to be unavoidable), cyclical movements based on either natural or naive babbling presented almost equally persistent trends, and locomotion emerged with naive babbling.","Therefore, we show how continual learning of walking in unforeseen circumstances can be driven by continual physical adaptation rooted in the backdrivable properties of the plant and enhanced by exploration strategies that exploit plant dynamics.","Our studies also demonstrate that the bio-inspired codesign and co-adaptations of limbs and control strategies can produce locomotion without explicit control of trajectory errors."],"url":"http://arxiv.org/abs/2402.02387v1","category":"cs.RO"}
{"created":"2024-02-04 07:51:39","title":"Acoustic Local Positioning With Encoded Emission Beacons","abstract":"Acoustic local positioning systems (ALPSs) are an interesting alternative for indoor positioning due to certain advantages over other approaches, including their relatively high accuracy, low cost, and room-level signal propagation. Centimeter-level or fine-grained indoor positioning can be an asset for robot navigation, guiding a person to, for instance, a particular piece in a museum or to a specific product in a shop, targeted advertising, or augmented reality. In airborne system applications, acoustic positioning can be based on using opportunistic signals or sounds produced by the person or object to be located (e.g., noise from appliances or the speech from a speaker) or from encoded emission beacons (or anchors) specifically designed for this purpose. This work presents a review of the different challenges that designers of systems based on encoded emission beacons must address in order to achieve suitable performance. At low-level processing, the waveform design (coding and modulation) and the processing of the received signal are key factors to address such drawbacks as multipath propagation, multiple-access interference, nearfar effect, or Doppler shifting. With regards to high-level system design, the issues to be addressed are related to the distribution of beacons, ease of deployment, and calibration and positioning algorithms, including the possible fusion of information. Apart from theoretical discussions, this work also includes the description of an ALPS that was implemented, installed in a large area and tested for mobile robot navigation. In addition to practical interest for real applications, airborne ALPSs can also be used as an excellent platform to test complex algorithms, which can be subsequently adapted for other positioning systems, such as underwater acoustic systems or ultrawideband radiofrequency (UWB RF) systems.","sentences":["Acoustic local positioning systems (ALPSs) are an interesting alternative for indoor positioning due to certain advantages over other approaches, including their relatively high accuracy, low cost, and room-level signal propagation.","Centimeter-level or fine-grained indoor positioning can be an asset for robot navigation, guiding a person to, for instance, a particular piece in a museum or to a specific product in a shop, targeted advertising, or augmented reality.","In airborne system applications, acoustic positioning can be based on using opportunistic signals or sounds produced by the person or object to be located (e.g., noise from appliances or the speech from a speaker) or from encoded emission beacons (or anchors) specifically designed for this purpose.","This work presents a review of the different challenges that designers of systems based on encoded emission beacons must address in order to achieve suitable performance.","At low-level processing, the waveform design (coding and modulation) and the processing of the received signal are key factors to address such drawbacks as multipath propagation, multiple-access interference, nearfar effect, or Doppler shifting.","With regards to high-level system design, the issues to be addressed are related to the distribution of beacons, ease of deployment, and calibration and positioning algorithms, including the possible fusion of information.","Apart from theoretical discussions, this work also includes the description of an ALPS that was implemented, installed in a large area and tested for mobile robot navigation.","In addition to practical interest for real applications, airborne ALPSs can also be used as an excellent platform to test complex algorithms, which can be subsequently adapted for other positioning systems, such as underwater acoustic systems or ultrawideband radiofrequency (UWB RF) systems."],"url":"http://arxiv.org/abs/2402.02384v1","category":"eess.SP"}
{"created":"2024-02-04 07:49:02","title":"Revisiting the Power of Prompt for Visual Tuning","abstract":"Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outperforms existing methods by a remarkable margin. For instance, it surpasses full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable parameters on the FGVC and VTAB-1K benchmarks. Notably, our method significantly advances the adaptation for self-supervised pretraining, achieving impressive task performance gains of at least 10% to 30%. Besides, the experimental results demonstrate the proposed SPT is robust to prompt lengths and scales well with model capacity and training data size. We finally provide an insightful exploration into the amount of target data facilitating the adaptation of pre-trained models to downstream tasks.","sentences":["Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks.","However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation.","This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training.","Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes.","The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning.","To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT.","Exhaustive experiments show our proposed approach outperforms existing methods by a remarkable margin.","For instance, it surpasses full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable parameters on the FGVC and VTAB-1K benchmarks.","Notably, our method significantly advances the adaptation for self-supervised pretraining, achieving impressive task performance gains of at least 10% to 30%.","Besides, the experimental results demonstrate the proposed SPT is robust to prompt lengths and scales well with model capacity and training data size.","We finally provide an insightful exploration into the amount of target data facilitating the adaptation of pre-trained models to downstream tasks."],"url":"http://arxiv.org/abs/2402.02382v1","category":"cs.CV"}
{"created":"2024-02-04 07:18:44","title":"Variational Quantum AdaBoost with Supervised Learning Guarantee","abstract":"Although variational quantum algorithms based on parameterized quantum circuits promise to achieve quantum advantages, in the noisy intermediate-scale quantum (NISQ) era, their capabilities are greatly constrained due to limited number of qubits and depth of quantum circuits. Therefore, we may view these variational quantum algorithms as weak learners in supervised learning. Ensemble methods are a general technique in machine learning for combining weak learners to construct a more accurate one. In this paper, we theoretically prove and numerically verify a learning guarantee for variational quantum adaptive boosting (AdaBoost). To be specific, we theoretically depict how the prediction error of variational quantum AdaBoost on binary classification decreases with the increase of the number of boosting rounds and sample size. By employing quantum convolutional neural networks, we further demonstrate that variational quantum AdaBoost can not only achieve much higher accuracy in prediction, but also help mitigate the impact of noise. Our work indicates that in the current NISQ era, introducing appropriate ensemble methods is particularly valuable in improving the performance of quantum machine learning algorithms.","sentences":["Although variational quantum algorithms based on parameterized quantum circuits promise to achieve quantum advantages, in the noisy intermediate-scale quantum (NISQ) era, their capabilities are greatly constrained due to limited number of qubits and depth of quantum circuits.","Therefore, we may view these variational quantum algorithms as weak learners in supervised learning.","Ensemble methods are a general technique in machine learning for combining weak learners to construct a more accurate one.","In this paper, we theoretically prove and numerically verify a learning guarantee for variational quantum adaptive boosting (AdaBoost).","To be specific, we theoretically depict how the prediction error of variational quantum AdaBoost on binary classification decreases with the increase of the number of boosting rounds and sample size.","By employing quantum convolutional neural networks, we further demonstrate that variational quantum AdaBoost can not only achieve much higher accuracy in prediction, but also help mitigate the impact of noise.","Our work indicates that in the current NISQ era, introducing appropriate ensemble methods is particularly valuable in improving the performance of quantum machine learning algorithms."],"url":"http://arxiv.org/abs/2402.02376v1","category":"quant-ph"}
{"created":"2024-02-04 06:37:38","title":"Transolver: A Fast Transformer Solver for PDEs on General Geometries","abstract":"Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-general modeling capacity and can be efficiently computed in linear complexity. Transolver achieves consistent state-of-the-art with 22\\% relative gain across six standard benchmarks and also excels in large-scale industrial simulations, including car and airfoil designs.","sentences":["Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs).","However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points.","Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries.","Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice.","By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-general modeling capacity and can be efficiently computed in linear complexity.","Transolver achieves consistent state-of-the-art with 22\\% relative gain across six standard benchmarks and also excels in large-scale industrial simulations, including car and airfoil designs."],"url":"http://arxiv.org/abs/2402.02366v1","category":"cs.LG"}
{"created":"2024-02-04 06:11:12","title":"Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness","abstract":"Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\\textbf{PSA}$) and a Pattern-aware Cost Model ($\\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\\textbf{MTL}$) strategy using a Siamese network, which establishes a bidirectional feedback mechanism to improve the robustness of the pre-trained cost model. The extensive experimental results demonstrate the effectiveness and advancement of the proposed Pruner in various tensor program tuning tasks across both online and offline scenarios, with low resource overhead. The code is available at https://github.com/qiaolian9/Pruner.","sentences":["Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment.","Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability.","In this paper, we propose $\\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization.","Pruner comprises two primary components: a Parameterized Static Analyzer ($\\textbf{PSA}$) and a Pattern-aware Cost Model ($\\textbf{PaCM}$).","The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns.","Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\\textbf{MTL}$) strategy using a Siamese network, which establishes a bidirectional feedback mechanism to improve the robustness of the pre-trained cost model.","The extensive experimental results demonstrate the effectiveness and advancement of the proposed Pruner in various tensor program tuning tasks across both online and offline scenarios, with low resource overhead.","The code is available at https://github.com/qiaolian9/Pruner."],"url":"http://arxiv.org/abs/2402.02361v1","category":"cs.LG"}
{"created":"2024-02-04 05:05:43","title":"Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models","abstract":"In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is introduced to deep learning tasks for the first time in our work. We release our code at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.","sentences":["In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step.","Specifically, we introduce an $r\\times r$ preconditioner in each gradient step where $r$ is the LoRA rank.","This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead.","Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced.","Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate.","Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix.","This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is introduced to deep learning tasks for the first time in our work.","We release our code at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA."],"url":"http://arxiv.org/abs/2402.02347v1","category":"cs.LG"}
{"created":"2024-02-04 05:03:22","title":"Closed-Loop Unsupervised Representation Disentanglement with $\u03b2$-VAE Distillation and Diffusion Probabilistic Feedback","abstract":"Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \\textbf{C}losed-\\textbf{L}oop unsupervised representation \\textbf{Dis}entanglement approach dubbed \\textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \\textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.","sentences":["Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks.","It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data.","To address these challenges, we propose a \\textbf{C}losed-\\textbf{L}oop unsupervised representation \\textbf{Dis}entanglement approach dubbed \\textbf{CL-Dis}.","Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\\beta$-VAE as a co-pilot to extract semantically disentangled representations.","The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary.","To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion.","Then, a self-supervised \\textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space.","Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect.","Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis."],"url":"http://arxiv.org/abs/2402.02346v1","category":"cs.CV"}
{"created":"2024-02-04 04:42:05","title":"Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning","abstract":"Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models trained from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabilities, thereby improving metric learning performance. We conduct extensive experiments to demonstrate that our proposed framework is effective and efficient by evaluating popular DML benchmarks. In particular, we demonstrate that our fine-tuning method achieves comparable or even better performance than recent state-of-the-art full fine-tuning works of DML while tuning only a small percentage of total parameters.","sentences":["Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective.","Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets.","As a result of the success of recent pre-trained models trained from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge.","In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks.","In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT).","Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class.","We demonstrate that our new approximations with semantic information are superior to representative capabilities, thereby improving metric learning performance.","We conduct extensive experiments to demonstrate that our proposed framework is effective and efficient by evaluating popular DML benchmarks.","In particular, we demonstrate that our fine-tuning method achieves comparable or even better performance than recent state-of-the-art full fine-tuning works of DML while tuning only a small percentage of total parameters."],"url":"http://arxiv.org/abs/2402.02340v1","category":"cs.CV"}
{"created":"2024-02-04 04:21:34","title":"Large Language Model Adaptation for Networking","abstract":"Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve \"one model for all\" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in LLM adaptation, from how to process task-specific information with LLMs, to how to improve the efficiency of answer generation and acquiring domain knowledge for networking. Across three networking-related use cases - viewport prediction (VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we showcase the effectiveness of NetLLM in LLM adaptation for networking. Results show that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6% for VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior generalization performance.","sentences":["Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems.","However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks.","Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   ","Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy.","With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve \"one model for all\" with even better performance and stronger generalization for various tasks.","In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems.","NetLLM addresses many practical challenges in LLM adaptation, from how to process task-specific information with LLMs, to how to improve the efficiency of answer generation and acquiring domain knowledge for networking.","Across three networking-related use cases - viewport prediction (VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we showcase the effectiveness of NetLLM in LLM adaptation for networking.","Results show that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6% for VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior generalization performance."],"url":"http://arxiv.org/abs/2402.02338v1","category":"cs.NI"}
{"created":"2024-02-04 00:30:14","title":"A flexible Bayesian g-formula for causal survival analyses with time-dependent confounding","abstract":"In longitudinal observational studies with a time-to-event outcome, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios within the study cohort. The g-formula is a particularly useful tool for this analysis. To enhance the traditional parametric g-formula approach, we developed a more adaptable Bayesian g-formula estimator. This estimator facilitates both longitudinal predictive and causal inference. It incorporates Bayesian additive regression trees in the modeling of the time-evolving generative components, aiming to mitigate bias due to model misspecification. Specifically, we introduce a more general class of g-formulas for discrete survival data. These formulas can incorporate the longitudinal balancing scores, which serve as an effective method for dimension reduction and are vital when dealing with an expanding array of time-varying confounders. The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment regimes, whether static or dynamic. For each type of treatment regime, we provide posterior sampling algorithms, which are grounded in the Bayesian additive regression trees framework. We have conducted simulation studies to illustrate the empirical performance of our proposed Bayesian g-formula estimators, and to compare them with existing parametric estimators. We further demonstrate the practical utility of our methods in real-world scenarios using data from the Yale New Haven Health System's electronic health records.","sentences":["In longitudinal observational studies with a time-to-event outcome, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios within the study cohort.","The g-formula is a particularly useful tool for this analysis.","To enhance the traditional parametric g-formula approach, we developed a more adaptable Bayesian g-formula estimator.","This estimator facilitates both longitudinal predictive and causal inference.","It incorporates Bayesian additive regression trees in the modeling of the time-evolving generative components, aiming to mitigate bias due to model misspecification.","Specifically, we introduce a more general class of g-formulas for discrete survival data.","These formulas can incorporate the longitudinal balancing scores, which serve as an effective method for dimension reduction and are vital when dealing with an expanding array of time-varying confounders.","The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment regimes, whether static or dynamic.","For each type of treatment regime, we provide posterior sampling algorithms, which are grounded in the Bayesian additive regression trees framework.","We have conducted simulation studies to illustrate the empirical performance of our proposed Bayesian g-formula estimators, and to compare them with existing parametric estimators.","We further demonstrate the practical utility of our methods in real-world scenarios using data from the Yale New Haven Health System's electronic health records."],"url":"http://arxiv.org/abs/2402.02306v1","category":"stat.ME"}
{"created":"2024-02-03 21:29:28","title":"Perturbative QCD contribution to transverse single spin asymmetries in Drell-Yan and SIDIS","abstract":"In a previous publication [Beni\\' c et al., Phys. Rev. D104 (2021) 094027], we have computed the perturbative QCD contribution to transverse single spin asymmetries (SSAs) in semi-inclusive Deep Inelastic Scattering (SIDIS) involving the $g_T(x)$ distribution. In this paper, we first present a more efficient derivation of the asymmetries which is applicable to both transverse and longitudinal SSAs, and correct some inconsistencies in our previous calculation. We then adapt the method to compute transverse SSAs in Drell-Yan proportional to $g_T(x)$ and its gluonic counterpart, and discuss the crossing symmetry between the results for SIDIS and Drell-Yan. Finally, we present numerical results for various asymmetries measurable at the EIC, RHIC, COMPASS and Fermilab (SpinQuest), including also part of the genuine twist-three corrections to $g_T(x)$ from a recent global analysis. We find that the asymmetries can reach percent-level magnitude, if the kinematics predominantly probes the large-$x$ (valence) region of the polarized proton, but remain at sub-percent levels otherwise.","sentences":["In a previous publication [Beni\\' c et al., Phys.","Rev. D104 (2021) 094027], we have computed the perturbative QCD contribution to transverse single spin asymmetries (SSAs) in semi-inclusive Deep Inelastic Scattering (SIDIS) involving the $g_T(x)$ distribution.","In this paper, we first present a more efficient derivation of the asymmetries which is applicable to both transverse and longitudinal SSAs, and correct some inconsistencies in our previous calculation.","We then adapt the method to compute transverse SSAs in Drell-Yan proportional to $g_T(x)$ and its gluonic counterpart, and discuss the crossing symmetry between the results for SIDIS and Drell-Yan.","Finally, we present numerical results for various asymmetries measurable at the EIC, RHIC, COMPASS and Fermilab (SpinQuest), including also part of the genuine twist-three corrections to $g_T(x)$ from a recent global analysis.","We find that the asymmetries can reach percent-level magnitude, if the kinematics predominantly probes the large-$x$ (valence) region of the polarized proton, but remain at sub-percent levels otherwise."],"url":"http://arxiv.org/abs/2402.02267v1","category":"hep-ph"}
{"created":"2024-02-03 19:27:09","title":"Novel approaches for the reliable and efficient numerical evaluation of the Landau operator","abstract":"When applying Hamiltonian operator splitting methods for the time integration of multi-species Vlasov-Maxwell-Landau systems, the reliable and efficient numerical approximation of the Landau equation represents a fundamental component of the entire algorithm. Substantial computational issues arise from the treatment of the physically most relevant three-dimensional case with Coulomb interaction. This work is concerned with the introduction and numerical comparison of novel approaches for the evaluation of the Landau collision operator. In the spirit of collocation, common tools are the identification of fundamental integrals, series expansions of the integral kernel and the density function on the main part of the velocity domain, and interpolation as well as quadrature approximation nearby the singularity of the kernel. Focusing on the favourable choice of the Fourier spectral method, their practical implementation uses the reduction to basic integrals, fast Fourier techniques, and summations along certain directions. Moreover, an important observation is that a significant percentage of the overall computational effort can be transferred to precomputations which are independent of the density function. For the purpose of exposition and numerical validation, the cases of constant, regular, and singular integral kernels are distinguished, and the procedure is adapted accordingly to the increasing complexity of the problem. With regard to the time integration of the Landau equation, the most expedient approach is applied in such a manner that the conservation of mass is ensured.","sentences":["When applying Hamiltonian operator splitting methods for the time integration of multi-species Vlasov-Maxwell-Landau systems, the reliable and efficient numerical approximation of the Landau equation represents a fundamental component of the entire algorithm.","Substantial computational issues arise from the treatment of the physically most relevant three-dimensional case with Coulomb interaction.","This work is concerned with the introduction and numerical comparison of novel approaches for the evaluation of the Landau collision operator.","In the spirit of collocation, common tools are the identification of fundamental integrals, series expansions of the integral kernel and the density function on the main part of the velocity domain, and interpolation as well as quadrature approximation nearby the singularity of the kernel.","Focusing on the favourable choice of the Fourier spectral method, their practical implementation uses the reduction to basic integrals, fast Fourier techniques, and summations along certain directions.","Moreover, an important observation is that a significant percentage of the overall computational effort can be transferred to precomputations which are independent of the density function.","For the purpose of exposition and numerical validation, the cases of constant, regular, and singular integral kernels are distinguished, and the procedure is adapted accordingly to the increasing complexity of the problem.","With regard to the time integration of the Landau equation, the most expedient approach is applied in such a manner that the conservation of mass is ensured."],"url":"http://arxiv.org/abs/2402.02247v1","category":"math.NA"}
{"created":"2024-02-03 17:58:43","title":"Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training","abstract":"Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average performance and more equitable predictions.","sentences":["Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model.","Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL.","In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task.","The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task.","CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations.","Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average performance and more equitable predictions."],"url":"http://arxiv.org/abs/2402.02225v1","category":"cs.LG"}
{"created":"2024-02-03 16:51:04","title":"Wavelet-Decoupling Contrastive Enhancement Network for Fine-Grained Skeleton-Based Action Recognition","abstract":"Skeleton-based action recognition has attracted much attention, benefiting from its succinctness and robustness. However, the minimal inter-class variation in similar action sequences often leads to confusion. The inherent spatiotemporal coupling characteristics make it challenging to mine the subtle differences in joint motion trajectories, which is critical for distinguishing confusing fine-grained actions. To alleviate this problem, we propose a Wavelet-Attention Decoupling (WAD) module that utilizes discrete wavelet transform to effectively disentangle salient and subtle motion features in the time-frequency domain. Then, the decoupling attention adaptively recalibrates their temporal responses. To further amplify the discrepancies in these subtle motion features, we propose a Fine-grained Contrastive Enhancement (FCE) module to enhance attention towards trajectory features by contrastive learning. Extensive experiments are conducted on the coarse-grained dataset NTU RGB+D and the fine-grained dataset FineGYM. Our methods perform competitively compared to state-of-the-art methods and can discriminate confusing fine-grained actions well.","sentences":["Skeleton-based action recognition has attracted much attention, benefiting from its succinctness and robustness.","However, the minimal inter-class variation in similar action sequences often leads to confusion.","The inherent spatiotemporal coupling characteristics make it challenging to mine the subtle differences in joint motion trajectories, which is critical for distinguishing confusing fine-grained actions.","To alleviate this problem, we propose a Wavelet-Attention Decoupling (WAD) module that utilizes discrete wavelet transform to effectively disentangle salient and subtle motion features in the time-frequency domain.","Then, the decoupling attention adaptively recalibrates their temporal responses.","To further amplify the discrepancies in these subtle motion features, we propose a Fine-grained Contrastive Enhancement (FCE) module to enhance attention towards trajectory features by contrastive learning.","Extensive experiments are conducted on the coarse-grained dataset NTU RGB+D and the fine-grained dataset FineGYM.","Our methods perform competitively compared to state-of-the-art methods and can discriminate confusing fine-grained actions well."],"url":"http://arxiv.org/abs/2402.02210v1","category":"cs.CV"}
{"created":"2024-02-05 18:59:04","title":"4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes","abstract":"We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively.","sentences":["We consider the problem of novel view synthesis (NVS) for dynamic scenes.","Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial.","Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings.","In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes.","We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images.","As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions.","We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU.","Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2402.03307v1","category":"cs.CV"}
{"created":"2024-02-05 18:58:22","title":"Sharp $L^2$ estimates for the drift heat equation on shrinking Ricci solitons","abstract":"We prove an $L^2$ estimate for the drift heat equation on a complete gradient shrinking Ricci soliton. This estimate has a time-dependent weight which is Gaussian in its spatial asymptotics. When transferred and scaled to an estimate for the heat equation along the Ricci flow of the soliton, this estimate is uniform up to the singular time.","sentences":["We prove an $L^2$ estimate for the drift heat equation on a complete gradient shrinking Ricci soliton.","This estimate has a time-dependent weight which is Gaussian in its spatial asymptotics.","When transferred and scaled to an estimate for the heat equation along the Ricci flow of the soliton, this estimate is uniform up to the singular time."],"url":"http://arxiv.org/abs/2402.03304v1","category":"math.DG"}
{"created":"2024-02-05 18:51:10","title":"Observation of the fractional quantum spin Hall effect in moir\u00e9 MoTe2","abstract":"Quantum spin Hall (QSH) insulators are two-dimensional electronic materials that have a bulk band gap like an ordinary insulator but have topologically protected pairs of edge modes of opposite chiralities. To date, experimental studies have found only integer QSH insulators with counter-propagating up-spins and down-spins at each edge leading to a quantized conductance G0=e^2/h. Here we report transport evidence of a fractional QSH insulator in 2.1-degree-twisted bilayer MoTe2, which supports spin-Sz conservation and flat spin-contrasting Chern bands. At filling factor v = 3 of the moir\\'e valence bands, each edge contributes a conductance 3/2 G0 with zero anomalous Hall conductivity. The state is likely a time-reversal pair of the even-denominator 3/2-fractional Chern insulators. Further, at v = 2, 4 and 6, we observe a single, double and triple QSH insulator with each edge contributing a conductance G0, 2G0 and 3G0, respectively. Our results open up the possibility of realizing time reversal symmetric non-abelian anyons and other unexpected topological phases in highly tunable moir\\'e materials.","sentences":["Quantum spin Hall (QSH) insulators are two-dimensional electronic materials that have a bulk band gap like an ordinary insulator but have topologically protected pairs of edge modes of opposite chiralities.","To date, experimental studies have found only integer QSH insulators with counter-propagating up-spins and down-spins at each edge leading to a quantized conductance G0=e^2/h. Here we report transport evidence of a fractional QSH insulator in 2.1-degree-twisted bilayer MoTe2, which supports spin-Sz conservation and flat spin-contrasting Chern bands.","At filling factor v = 3 of the moir\\'e valence bands, each edge contributes a conductance 3/2 G0 with zero anomalous Hall conductivity.","The state is likely a time-reversal pair of the even-denominator 3/2-fractional Chern insulators.","Further, at v = 2, 4 and 6, we observe a single, double and triple QSH insulator with each edge contributing a conductance G0, 2G0 and 3G0, respectively.","Our results open up the possibility of realizing time reversal symmetric non-abelian anyons and other unexpected topological phases in highly tunable moir\\'e materials."],"url":"http://arxiv.org/abs/2402.03294v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 18:22:41","title":"Symmetries and conservation laws of a fifth-order KdV equation with time-dependent coefficients and linear damping","abstract":"A fifth-order KdV equation with time dependent coefficients and linear damping has been studied. Symmetry groups have several different applications in the context of nonlinear differential equations. For instance, they can be used to determine conservation laws. We obtain the symmetries of the model applying Lie's classical method. The choice of some arbitrary functions of the equation by the equivalence transformation enhances the study of Lie symmetries of the equation. We have determined the subclasses of the equation which are nonlinearly self-adjoint. This allow us to obtain conservation laws by using a theorem proved by Ibragimov which is based on the concept of adjoint equation for nonlinear differential equations.","sentences":["A fifth-order KdV equation with time dependent coefficients and linear damping has been studied.","Symmetry groups have several different applications in the context of nonlinear differential equations.","For instance, they can be used to determine conservation laws.","We obtain the symmetries of the model applying Lie's classical method.","The choice of some arbitrary functions of the equation by the equivalence transformation enhances the study of Lie symmetries of the equation.","We have determined the subclasses of the equation which are nonlinearly self-adjoint.","This allow us to obtain conservation laws by using a theorem proved by Ibragimov which is based on the concept of adjoint equation for nonlinear differential equations."],"url":"http://arxiv.org/abs/2402.03265v1","category":"math.AP"}
{"created":"2024-02-05 17:58:17","title":"PINN-BO: A Black-box Optimization Algorithm using Physics-Informed Neural Networks","abstract":"Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios. Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods. Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions. In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound. We perform several experiments on a variety of optimization tasks and show that our algorithm is more sample-efficient compared to existing methods.","sentences":["Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios.","Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods.","Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions.","In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization.","We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound.","We perform several experiments on a variety of optimization tasks and show that our algorithm is more sample-efficient compared to existing methods."],"url":"http://arxiv.org/abs/2402.03243v1","category":"cs.LG"}
{"created":"2024-02-05 17:30:42","title":"The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents","abstract":"We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamical process of the low-dimensional projections of the weights, and numerical experiments illustrating the theory.","sentences":["We investigate the training dynamics of two-layer neural networks when learning multi-index target functions.","We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent.","In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function.","We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021).","We characterize the (broad) class of functions efficiently learned in finite time.","The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT).","We further provide a closed-form description of the dynamical process of the low-dimensional projections of the weights, and numerical experiments illustrating the theory."],"url":"http://arxiv.org/abs/2402.03220v1","category":"stat.ML"}
{"created":"2024-02-05 17:28:35","title":"Determination of Schr\u00f6dinger nonlinearities from the scattering map","abstract":"We prove that the small-data scattering map uniquely determines the nonlinearity for a wide class of gauge-invariant, intercritical nonlinear Schr\\\"odinger equations. We use the Born approximation to reduce the analysis to a deconvolution problem involving the distribution function for linear Schr\\\"odinger solutions. We then solve this deconvolution problem using the Beurling--Lax Theorem.","sentences":["We prove that the small-data scattering map uniquely determines the nonlinearity for a wide class of gauge-invariant, intercritical nonlinear Schr\\\"odinger equations.","We use the Born approximation to reduce the analysis to a deconvolution problem involving the distribution function for linear Schr\\\"odinger solutions.","We then solve this deconvolution problem using the Beurling--Lax Theorem."],"url":"http://arxiv.org/abs/2402.03218v1","category":"math.AP"}
{"created":"2024-02-05 16:51:59","title":"How Good is a Single Basin?","abstract":"The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles. In this work, we probe this belief by constructing various \"connected\" ensembles which are restricted to lie in the same basin. Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance. However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin. Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins.","sentences":["The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles.","In this work, we probe this belief by constructing various \"connected\" ensembles which are restricted to lie in the same basin.","Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance.","However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin.","Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins."],"url":"http://arxiv.org/abs/2402.03187v1","category":"cs.LG"}
{"created":"2024-02-05 16:10:52","title":"Locally Lipschitz stability of solutions to a parametric parabolic optimal control problem with mixed pointwise constraints","abstract":"A class of parametric optimal control problems governed by semilinear parabolic equations with mixed pointwise constraints is investigated. The perturbations appear in the objective functional, the state equation and in mixed pointwise constraints. By analyzing regularity and establishing stability condition of Lagrange multipliers we prove that, if the strictly second-order sufficient condition for the unperturbed problem is valid, then the solutions of the problems as well as the associated Lagrange multipliers are locally Lipschitz continuous functions of parameters.","sentences":["A class of parametric optimal control problems governed by semilinear parabolic equations with mixed pointwise constraints is investigated.","The perturbations appear in the objective functional, the state equation and in mixed pointwise constraints.","By analyzing regularity and establishing stability condition of Lagrange multipliers we prove that, if the strictly second-order sufficient condition for the unperturbed problem is valid, then the solutions of the problems as well as the associated Lagrange multipliers are locally Lipschitz continuous functions of parameters."],"url":"http://arxiv.org/abs/2402.03140v1","category":"math.OC"}
{"created":"2024-02-05 16:09:35","title":"Enhancing Neural Subset Selection: Integrating Background Information into Set Representations","abstract":"Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \\textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.","sentences":["Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications.","The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets.","However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions.","In this work, we address this oversight by adopting a probabilistic perspective.","Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \\textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning.","This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated.","Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective.","Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts."],"url":"http://arxiv.org/abs/2402.03139v1","category":"cs.LG"}
{"created":"2024-02-05 15:55:34","title":"Clairaut anti-invariant Riemannian maps to Sasakian manifolds","abstract":"In this paper, we investigate the geometry of Clairaut anti-invariant Riemannnian maps whose base space are Sasakian manifolds. We obtain the necessary and sufficient conditions for a curve on a base manifold to be geodesic. We obtain conditions for an anti-invariant Riemannian map to be Clairaut. Further, we discuss the biharmonicity of such maps and construct some illustrative examples.","sentences":["In this paper, we investigate the geometry of Clairaut anti-invariant Riemannnian maps whose base space are Sasakian manifolds.","We obtain the necessary and sufficient conditions for a curve on a base manifold to be geodesic.","We obtain conditions for an anti-invariant Riemannian map to be Clairaut.","Further, we discuss the biharmonicity of such maps and construct some illustrative examples."],"url":"http://arxiv.org/abs/2402.03129v1","category":"math.DG"}
{"created":"2024-02-05 15:14:08","title":"Dual Lagrangian Learning for Conic Optimization","abstract":"This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the representation power of ML models. DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for parametric linear and nonlinear conic optimization problems. The paper introduces differentiable conic projection layers, a systematic dual completion procedure, and a self-supervised learning framework. The effectiveness of DLL is demonstrated on linear and nonlinear parametric optimization problems for which DLL provides valid dual bounds within 0.5% of optimality.","sentences":["This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the representation power of ML models.","DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for parametric linear and nonlinear conic optimization problems.","The paper introduces differentiable conic projection layers, a systematic dual completion procedure, and a self-supervised learning framework.","The effectiveness of DLL is demonstrated on linear and nonlinear parametric optimization problems for which DLL provides valid dual bounds within 0.5% of optimality."],"url":"http://arxiv.org/abs/2402.03086v1","category":"math.OC"}
{"created":"2024-02-05 14:52:01","title":"UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs without im2col","abstract":"Homomorphic encryption enables computations on encrypted data without decryption, which is crucial for privacy-preserving cloud services. However, deploying convolutional neural networks (CNNs) with homomorphic encryption encounters significant challenges, particularly in converting input data into a two-dimensional matrix for convolution, typically achieved using the im2col technique. While efficient, this method limits the variety of deployable CNN models due to compatibility constraints with the encrypted data structure. UniHENN, a homomorphic encryption-based CNN architecture, eliminates the need for im2col, ensuring compatibility with a diverse range of CNN models using homomorphic encryption. Our experiments demonstrate that UniHENN surpasses the leading 2D CNN inference architecture, PyCrCNN, in inference time, as evidenced by its performance on the LeNet-1 dataset, where it averages 30.090 seconds--significantly faster than PyCrCNN's 794.064 seconds. Furthermore, UniHENN outperforms TenSEAL, which employs im2col, in processing concurrent images, an essential feature for high-demand cloud applications. The versatility of UniHENN is proven across various CNN architectures, including 1D and six different 2D CNNs, highlighting its flexibility and efficiency. These qualities establish UniHENN as a promising solution for privacy-preserving, cloud-based CNN services, addressing the increasing demand for scalable, secure, and efficient deep learning in cloud computing environments.","sentences":["Homomorphic encryption enables computations on encrypted data without decryption, which is crucial for privacy-preserving cloud services.","However, deploying convolutional neural networks (CNNs) with homomorphic encryption encounters significant challenges, particularly in converting input data into a two-dimensional matrix for convolution, typically achieved using the im2col technique.","While efficient, this method limits the variety of deployable CNN models due to compatibility constraints with the encrypted data structure.","UniHENN, a homomorphic encryption-based CNN architecture, eliminates the need for im2col, ensuring compatibility with a diverse range of CNN models using homomorphic encryption.","Our experiments demonstrate that UniHENN surpasses the leading 2D CNN inference architecture, PyCrCNN, in inference time, as evidenced by its performance on the LeNet-1 dataset, where it averages 30.090 seconds--significantly faster than PyCrCNN's 794.064 seconds.","Furthermore, UniHENN outperforms TenSEAL, which employs im2col, in processing concurrent images, an essential feature for high-demand cloud applications.","The versatility of UniHENN is proven across various CNN architectures, including 1D and six different 2D CNNs, highlighting its flexibility and efficiency.","These qualities establish UniHENN as a promising solution for privacy-preserving, cloud-based CNN services, addressing the increasing demand for scalable, secure, and efficient deep learning in cloud computing environments."],"url":"http://arxiv.org/abs/2402.03060v1","category":"cs.CR"}
{"created":"2024-02-05 14:48:07","title":"Array Geometry-Robust Attention-Based Neural Beamformer for Moving Speakers","abstract":"Recently, a mask-based beamformer with attention-based spatial covariance matrix aggregator (ASA) was proposed, which was demonstrated to track moving sources accurately. However, the deep neural network model used in this algorithm is limited to a specific channel configuration, requiring a different model in case a different channel permutation, channel count, or microphone array geometry is considered. Addressing this limitation, in this paper, we investigate three approaches to improve the robustness of the ASA-based tracking method against such variations: incorporating random channel configurations during the training process, employing the transform-average-concatenate (TAC) method to process multi-channel input features (allowing for any channel count and enabling permutation invariance), and utilizing input features that are robust against variations of the channel configuration. Our experiments, conducted using the CHiME-3 and DEMAND datasets, demonstrate improved robustness against mismatches in channel permutations, channel counts, and microphone array geometries compared to the conventional ASA-based tracking method without compromising performance in matched conditions, suggesting that the mask-based beamformer with ASA integrating the proposed approaches has the potential to track moving sources for arbitrary microphone arrays.","sentences":["Recently, a mask-based beamformer with attention-based spatial covariance matrix aggregator (ASA) was proposed, which was demonstrated to track moving sources accurately.","However, the deep neural network model used in this algorithm is limited to a specific channel configuration, requiring a different model in case a different channel permutation, channel count, or microphone array geometry is considered.","Addressing this limitation, in this paper, we investigate three approaches to improve the robustness of the ASA-based tracking method against such variations: incorporating random channel configurations during the training process, employing the transform-average-concatenate (TAC) method to process multi-channel input features (allowing for any channel count and enabling permutation invariance), and utilizing input features that are robust against variations of the channel configuration.","Our experiments, conducted using the CHiME-3 and DEMAND datasets, demonstrate improved robustness against mismatches in channel permutations, channel counts, and microphone array geometries compared to the conventional ASA-based tracking method without compromising performance in matched conditions, suggesting that the mask-based beamformer with ASA integrating the proposed approaches has the potential to track moving sources for arbitrary microphone arrays."],"url":"http://arxiv.org/abs/2402.03058v1","category":"eess.AS"}
{"created":"2024-02-05 14:24:08","title":"Notes on Lagrangian continuum mechanics","abstract":"In Continuum Mechanic a simple material body $\\mathcal{B}$ is represeted by a three-dimensional differentiable manifold and the configuration space is given by the space of embeddings $Emb \\left( \\mathcal{B} , \\mathbb{R}^{n} \\right)$. We use the topology of infinite-dimensional manifold of this space, to present the first variation formula for Lagrangian mechanics.","sentences":["In Continuum Mechanic a simple material body $\\mathcal{B}$ is represeted by a three-dimensional differentiable manifold and the configuration space is given by the space of embeddings $Emb \\left( \\mathcal{B} , \\mathbb{R}^{n} \\right)$. We use the topology of infinite-dimensional manifold of this space, to present the first variation formula for Lagrangian mechanics."],"url":"http://arxiv.org/abs/2402.03039v1","category":"math-ph"}
{"created":"2024-02-05 13:50:08","title":"On the Impact of Output Perturbation on Fairness in Binary Linear Classification","abstract":"We theoretically study how differential privacy interacts with both individual and group fairness in binary linear classification. More precisely, we focus on the output perturbation mechanism, a classic approach in privacy-preserving machine learning. We derive high-probability bounds on the level of individual and group fairness that the perturbed models can achieve compared to the original model. Hence, for individual fairness, we prove that the impact of output perturbation on the level of fairness is bounded but grows with the dimension of the model. For group fairness, we show that this impact is determined by the distribution of so-called angular margins, that is signed margins of the non-private model re-scaled by the norm of each example.","sentences":["We theoretically study how differential privacy interacts with both individual and group fairness in binary linear classification.","More precisely, we focus on the output perturbation mechanism, a classic approach in privacy-preserving machine learning.","We derive high-probability bounds on the level of individual and group fairness that the perturbed models can achieve compared to the original model.","Hence, for individual fairness, we prove that the impact of output perturbation on the level of fairness is bounded but grows with the dimension of the model.","For group fairness, we show that this impact is determined by the distribution of so-called angular margins, that is signed margins of the non-private model re-scaled by the norm of each example."],"url":"http://arxiv.org/abs/2402.03011v1","category":"cs.LG"}
{"created":"2024-02-05 13:47:41","title":"Diffusive Gibbs Sampling","abstract":"The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.","sentences":["The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics.","Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes.","DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces.","Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering.","We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics."],"url":"http://arxiv.org/abs/2402.03008v1","category":"stat.ML"}
{"created":"2024-02-05 12:55:13","title":"Appearance of neutrino asymmetries in the process of expansion of the Universe, hierarchy of neutrino masses and CP violation","abstract":"In this work, we study the appearance of neutrino asymmetries during the expansion of the Universe. A mathematical model based on differential equations was used to describe the processes of neutrino oscillations considering CP violation and neutrino collisions. An analysis of the emergence of neutrino asymmetry due to collisions of neutrinos with each other and due to CP violation during neutrino oscillations is presented. It was discovered that the asymmetry bifurcates into two states with positive and negative asymmetry for the inverse hierarchy of neutrino masses. A state with a normal hierarchy of neutrino masses is unstable and is not realized. It is shown that the maximum CP violation is realized. The influence of this process on the dispersion of the primordial $^4\\text{He}$ mass fraction is calculated, which is confirmed by astrophysical observations. Thus, the inverse hierarchy of neutrino masses is realized. The CP violation in neutrino oscillations is maximum and the phase is $\\delta_{13}=270^\\circ (-\\pi/2)$","sentences":["In this work, we study the appearance of neutrino asymmetries during the expansion of the Universe.","A mathematical model based on differential equations was used to describe the processes of neutrino oscillations considering CP violation and neutrino collisions.","An analysis of the emergence of neutrino asymmetry due to collisions of neutrinos with each other and due to CP violation during neutrino oscillations is presented.","It was discovered that the asymmetry bifurcates into two states with positive and negative asymmetry for the inverse hierarchy of neutrino masses.","A state with a normal hierarchy of neutrino masses is unstable and is not realized.","It is shown that the maximum CP violation is realized.","The influence of this process on the dispersion of the primordial $^4\\text{He}$ mass fraction is calculated, which is confirmed by astrophysical observations.","Thus, the inverse hierarchy of neutrino masses is realized.","The CP violation in neutrino oscillations is maximum and the phase is $\\delta_{13}=270^\\circ (-\\pi/2)$"],"url":"http://arxiv.org/abs/2402.02974v1","category":"hep-ph"}
{"created":"2024-02-05 12:36:25","title":"Electrostatic Disturbances of Aerosol Atmospheric Plasma: Beaded Lightning","abstract":"Numerous sources produce the ionization impact on the planetary atmosphere. The tropospheric cloudiness therewith is originated at such altitudes, which coincide with places of the maximum of the atmospheric ionization that caused by space radiation. Components of cosmic radiation, penetrating down to the stratospheric and tropospheric altitudes, produce in the first place the ionization of aerosols that brings to subsequent amplifying the activity of atmospheric vortices. The ionized aerosol constituent is of primary importance both in the initiation of plasma vortices and in the concentration of energy-mass by air vortices due to the humidity condensation. The ionization process is cascading, that causes the non-linear impact of penetrating rays, which amplifying with growing the air pollution. Various heterogeneities inside plasma subsystems provoke the random intensification of aperiodic electrostatic disturbances, which can exert a significant action on the development of vortices. Sometimes beaded lightning arises during thunderstorm. Using the approach of the Boltzmann kinetic equation, the characteristics of electrostatic oscillations for non-uniform plasma, are analytically derived. The analytical expressions are obtained for non-magnetized plasma. In the limits of the Earth's atmosphere, these solutions are strictly applicable to the origination of electrostatic modulations along the lines of force of the geomagnetic field and approximately applicable in cases, when the impact of the Earth's magnetic field is negligible. The explicit expressions are deduced both in the hot plasma approximation and in the cold plasma approximation. It is demonstrated that the formation of chain elements of discharge in hot inhomogeneous plasma is defined by the nonmonotonic distribution of charges and fields. At such conditions, the irregularities breaks up into a cellular structure.","sentences":["Numerous sources produce the ionization impact on the planetary atmosphere.","The tropospheric cloudiness therewith is originated at such altitudes, which coincide with places of the maximum of the atmospheric ionization that caused by space radiation.","Components of cosmic radiation, penetrating down to the stratospheric and tropospheric altitudes, produce in the first place the ionization of aerosols that brings to subsequent amplifying the activity of atmospheric vortices.","The ionized aerosol constituent is of primary importance both in the initiation of plasma vortices and in the concentration of energy-mass by air vortices due to the humidity condensation.","The ionization process is cascading, that causes the non-linear impact of penetrating rays, which amplifying with growing the air pollution.","Various heterogeneities inside plasma subsystems provoke the random intensification of aperiodic electrostatic disturbances, which can exert a significant action on the development of vortices.","Sometimes beaded lightning arises during thunderstorm.","Using the approach of the Boltzmann kinetic equation, the characteristics of electrostatic oscillations for non-uniform plasma, are analytically derived.","The analytical expressions are obtained for non-magnetized plasma.","In the limits of the Earth's atmosphere, these solutions are strictly applicable to the origination of electrostatic modulations along the lines of force of the geomagnetic field and approximately applicable in cases, when the impact of the Earth's magnetic field is negligible.","The explicit expressions are deduced both in the hot plasma approximation and in the cold plasma approximation.","It is demonstrated that the formation of chain elements of discharge in hot inhomogeneous plasma is defined by the nonmonotonic distribution of charges and fields.","At such conditions, the irregularities breaks up into a cellular structure."],"url":"http://arxiv.org/abs/2402.02958v1","category":"physics.plasm-ph"}
{"created":"2024-02-05 11:36:16","title":"On bilinear Strichartz estimates on waveguides with applications","abstract":"We study local-in-time and global-in-time bilinear Strichartz estimates for the Schr\\\"odinger equation on waveguides. As applications, we apply those estimates to study global well-posedness of nonlinear Schr\\\"odinger equations on these waveguides.","sentences":["We study local-in-time and global-in-time bilinear Strichartz estimates for the Schr\\\"odinger equation on waveguides.","As applications, we apply those estimates to study global well-posedness of nonlinear Schr\\\"odinger equations on these waveguides."],"url":"http://arxiv.org/abs/2402.02916v1","category":"math.AP"}
{"created":"2024-02-05 11:23:32","title":"Additive-multiplicative stochastic heat equations, stationary solutions, and Cauchy statistics","abstract":"We study long-term behavior and stationary distributions for stochastic heat equations forced simultaneously by a multiplicative noise and an independent additive noise with the same distribution. We prove that nontrivial space-time translation-invariant measures exist for all values of the parameters. We also show that if the multiplicative noise is sufficiently strong, the invariant measure has Cauchy-distributed marginals. Using the same techniques, we prove a similar result on Cauchy-distributed marginals for a logarithmically attenuated version of the problem in two spatial dimensions. The proofs rely on stochastic analysis and elementary potential theory.","sentences":["We study long-term behavior and stationary distributions for stochastic heat equations forced simultaneously by a multiplicative noise and an independent additive noise with the same distribution.","We prove that nontrivial space-time translation-invariant measures exist for all values of the parameters.","We also show that if the multiplicative noise is sufficiently strong, the invariant measure has Cauchy-distributed marginals.","Using the same techniques, we prove a similar result on Cauchy-distributed marginals for a logarithmically attenuated version of the problem in two spatial dimensions.","The proofs rely on stochastic analysis and elementary potential theory."],"url":"http://arxiv.org/abs/2402.02907v1","category":"math.PR"}
{"created":"2024-02-05 11:21:49","title":"Variational discretizations of ideal magnetohydrodynamics in smooth regime using finite element exterior calculus","abstract":"We propose a new class of finite element approximations to ideal compressible magnetohydrodynamic equations in smooth regime. Following variational approximations developed for fluid models in the last decade, our discretizations are built via a discrete variational principle mimicking the continuous Euler-Poincare principle, and to further exploit the geometrical structure of the problem, vector fields are represented by their action as Lie derivatives on differential forms of any degree. The resulting semi-discrete approximations are shown to conserve the total mass, entropy and energy of the solutions for a wide class of finite element approximations. In addition, the divergence-free nature of the magnetic field is preserved in a pointwise sense and a time discretization is proposed, preserving those invariants and giving a reversible scheme at the fully discrete level. Numerical simulations are conducted to verify the accuracy of our approach and its ability to preserve the invariants for several test problems.","sentences":["We propose a new class of finite element approximations to ideal compressible magnetohydrodynamic equations in smooth regime.","Following variational approximations developed for fluid models in the last decade, our discretizations are built via a discrete variational principle mimicking the continuous Euler-Poincare principle, and to further exploit the geometrical structure of the problem, vector fields are represented by their action as Lie derivatives on differential forms of any degree.","The resulting semi-discrete approximations are shown to conserve the total mass, entropy and energy of the solutions for a wide class of finite element approximations.","In addition, the divergence-free nature of the magnetic field is preserved in a pointwise sense and a time discretization is proposed, preserving those invariants and giving a reversible scheme at the fully discrete level.","Numerical simulations are conducted to verify the accuracy of our approach and its ability to preserve the invariants for several test problems."],"url":"http://arxiv.org/abs/2402.02905v1","category":"math.NA"}
{"created":"2024-02-05 11:06:25","title":"Explaining the Lack of Mesh Convergence of Inviscid Adjoint Solutions Near Solid Walls for Subcritical Flows","abstract":"Numerical solutions to the adjoint Euler equations have been found to diverge with mesh refinement near walls for a variety of flow conditions and geometry configurations. The issue is reviewed and an explanation is provided by comparing a numerical incompressible adjoint solution with an analytic adjoint solution, showing that the anomaly observed in numerical computations is caused by a divergence of the analytic solution at the wall. The singularity causing this divergence is of the same type as the well-known singularity along the incoming stagnation streamline and both originate at the adjoint singularity at the trailing edge. The argument is extended to cover the fully compressible case, in subcritical flow conditions, by presenting an analytic solution that follows the same structure as the incompressible one.","sentences":["Numerical solutions to the adjoint Euler equations have been found to diverge with mesh refinement near walls for a variety of flow conditions and geometry configurations.","The issue is reviewed and an explanation is provided by comparing a numerical incompressible adjoint solution with an analytic adjoint solution, showing that the anomaly observed in numerical computations is caused by a divergence of the analytic solution at the wall.","The singularity causing this divergence is of the same type as the well-known singularity along the incoming stagnation streamline and both originate at the adjoint singularity at the trailing edge.","The argument is extended to cover the fully compressible case, in subcritical flow conditions, by presenting an analytic solution that follows the same structure as the incompressible one."],"url":"http://arxiv.org/abs/2402.02897v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 11:02:27","title":"Shock equations and jump conditions for the 2D Adjoint Euler equations","abstract":"This paper considers the formulation of the adjoint problem in two dimensions when there are shocks in the flow solution. For typical cost functions, the adjoint variables are continuous at shocks, where they have to obey an internal boundary condition, but their derivatives may be discontinuous. The derivation of the adjoint shock equations is reviewed and detailed predictions for the behavior of the gradients of the adjoint variables at shocks are obtained as jump conditions for the normal adjoint gradients in terms of the tangent gradients. Several numerical computations on a very fine mesh are used to illustrate the behavior of numerical adjoint solutions at shocks.","sentences":["This paper considers the formulation of the adjoint problem in two dimensions when there are shocks in the flow solution.","For typical cost functions, the adjoint variables are continuous at shocks, where they have to obey an internal boundary condition, but their derivatives may be discontinuous.","The derivation of the adjoint shock equations is reviewed and detailed predictions for the behavior of the gradients of the adjoint variables at shocks are obtained as jump conditions for the normal adjoint gradients in terms of the tangent gradients.","Several numerical computations on a very fine mesh are used to illustrate the behavior of numerical adjoint solutions at shocks."],"url":"http://arxiv.org/abs/2402.02894v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 10:42:16","title":"Low-energy $\u03b1$-harmonic maps into the round sphere","abstract":"We classify low-energy $\\alpha$-harmonic maps from a closed non-spherical Riemannian surface $\\Sigma$ of constant curvature to the round sphere via their bubble scales and centres. In particular we show that as $1<\\alpha\\downarrow 1$ and assuming $E_\\alpha$ is close to $|   \\Sigma|+4\\pi$ then degree-one $\\alpha$-harmonic maps blow a bubble based at a critical point $a_c$ of a an explicit function $\\mathcal{J}$ and at scale $\\sqrt{ |\\mathcal{J}(a_c)|^{-1}(\\alpha-1)}$. Up to a constant, $\\mathcal{J}$ is the sum of the squares of any $L^2$-orthonormal basis of holomorphic one-forms on the domain.","sentences":["We classify low-energy $\\alpha$-harmonic maps from a closed non-spherical Riemannian surface $\\Sigma$ of constant curvature to the round sphere via their bubble scales and centres.","In particular we show that as $1<\\alpha\\downarrow 1$ and assuming $E_\\alpha$ is close to $|   \\Sigma|+4\\pi$ then degree-one $\\alpha$-harmonic maps blow a bubble based at a critical point $a_c$ of a an explicit function $\\mathcal{J}$ and at scale $\\sqrt{ |\\mathcal{J}(a_c)|^{-1}(\\alpha-1)}$. Up to a constant, $\\mathcal{J}$ is the sum of the squares of any $L^2$-orthonormal basis of holomorphic one-forms on the domain."],"url":"http://arxiv.org/abs/2402.02875v1","category":"math.AP"}
{"created":"2024-02-05 10:30:47","title":"Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem","abstract":"Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the pre-trained capabilities. In particular, in NetHack, we achieve a new state-of-the-art for neural models, improving the previous best score from $5$K to over $10$K points in the Human Monk scenario.","sentences":["Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models.","However, fine-tuning reinforcement learning (RL) models remains a challenge.","This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities.","Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training.","This way, we lose the anticipated transfer benefits.","We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic.","Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the pre-trained capabilities.","In particular, in NetHack, we achieve a new state-of-the-art for neural models, improving the previous best score from $5$K to over $10$K points in the Human Monk scenario."],"url":"http://arxiv.org/abs/2402.02868v1","category":"cs.LG"}
{"created":"2024-02-05 10:22:15","title":"Graph Neural Machine: A New Model for Learning with Tabular Data","abstract":"In recent years, there has been a growing interest in mapping data from different domains to graph structures. Among others, neural network models such as the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can be represented as directed acyclic graphs. Graph neural networks (GNNs) have recently become the standard tool for performing machine learning tasks on graphs. In this work, we show that an MLP is equivalent to an asynchronous message passing GNN model which operates on the MLP's graph representation. We then propose a new machine learning model for tabular data, the so-called Graph Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a nearly complete graph and which employs a synchronous message passing scheme. We show that a single GNM model can simulate multiple MLP models. We evaluate the proposed model in several classification and regression datasets. In most cases, the GNM model outperforms the MLP architecture.","sentences":["In recent years, there has been a growing interest in mapping data from different domains to graph structures.","Among others, neural network models such as the multi-layer perceptron (MLP) can be modeled as graphs.","In fact, MLPs can be represented as directed acyclic graphs.","Graph neural networks (GNNs) have recently become the standard tool for performing machine learning tasks on graphs.","In this work, we show that an MLP is equivalent to an asynchronous message passing GNN model which operates on the MLP's graph representation.","We then propose a new machine learning model for tabular data, the so-called Graph Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a nearly complete graph and which employs a synchronous message passing scheme.","We show that a single GNM model can simulate multiple MLP models.","We evaluate the proposed model in several classification and regression datasets.","In most cases, the GNM model outperforms the MLP architecture."],"url":"http://arxiv.org/abs/2402.02862v1","category":"stat.ML"}
{"created":"2024-02-05 07:51:39","title":"Ab initio Investigation of Thermal Transport in Insulators: Unveiling the Roles of Phonon Renormalization and Higher-Order Anharmonicity","abstract":"The occurrence of thermal transport phenomena is widespread, exerting a pivotal influence on the functionality of diverse electronic and thermo-electric energy-conversion devices. The traditional first-principles theory governing the thermal and thermodynamic characteristics of insulators relies on the perturbative treatment of interatomic potential and ad-hoc displacement of atoms within supercells. However, the limitations of these approaches for highly anharmonic and weakly bonded materials, along with discrepancies arising from not considering explicit finite temperature effects, highlight the necessity for a well-defined quasiparticle approach to the lattice vibrations. To address these limitations, we present a comprehensive numerical framework in this study, designed to compute the thermal and thermodynamic characteristics of crystalline semiconductors and insulators. The self-consistent phonon renormalization method we have devised reveals phonons as quasiparticles, diverging from their conventional characterization as bare normal modes of lattice vibration. The extension of the renormalization impact to interatomic force constants (IFCs) of third and fourth orders is also integrated and demonstrated. For the comprehensive physical insights, we employed an iterative solution of the Peierls-Boltzmann transport equation (PBTE) to determine thermal conductivity and carry out Helmholtz free energy calculations, encompassing anharmonicity effects up to the fourth order. In this study, we utilize our numerical framework to showcase its applicability through an examination of phonon dispersion, phonon linewidth, anharmonic phonon scattering, and temperature-dependent lattice thermal conductivity in both highly anharmonic materials (NaCl and AgI) and weakly anharmonic materials (cBN and 3C-SiC).","sentences":["The occurrence of thermal transport phenomena is widespread, exerting a pivotal influence on the functionality of diverse electronic and thermo-electric energy-conversion devices.","The traditional first-principles theory governing the thermal and thermodynamic characteristics of insulators relies on the perturbative treatment of interatomic potential and ad-hoc displacement of atoms within supercells.","However, the limitations of these approaches for highly anharmonic and weakly bonded materials, along with discrepancies arising from not considering explicit finite temperature effects, highlight the necessity for a well-defined quasiparticle approach to the lattice vibrations.","To address these limitations, we present a comprehensive numerical framework in this study, designed to compute the thermal and thermodynamic characteristics of crystalline semiconductors and insulators.","The self-consistent phonon renormalization method we have devised reveals phonons as quasiparticles, diverging from their conventional characterization as bare normal modes of lattice vibration.","The extension of the renormalization impact to interatomic force constants (IFCs) of third and fourth orders is also integrated and demonstrated.","For the comprehensive physical insights, we employed an iterative solution of the Peierls-Boltzmann transport equation (PBTE) to determine thermal conductivity and carry out Helmholtz free energy calculations, encompassing anharmonicity effects up to the fourth order.","In this study, we utilize our numerical framework to showcase its applicability through an examination of phonon dispersion, phonon linewidth, anharmonic phonon scattering, and temperature-dependent lattice thermal conductivity in both highly anharmonic materials (NaCl and AgI) and weakly anharmonic materials (cBN and 3C-SiC)."],"url":"http://arxiv.org/abs/2402.02787v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 07:08:58","title":"TensoSDF: Roughness-aware Tensorial Representation for Robust Geometry and Material Reconstruction","abstract":"Reconstructing objects with realistic materials from multi-view images is problematic, since it is highly ill-posed. Although the neural reconstruction approaches have exhibited impressive reconstruction ability, they are designed for objects with specific materials (e.g., diffuse or specular materials). To this end, we propose a novel framework for robust geometry and material reconstruction, where the geometry is expressed with the implicit signed distance field (SDF) encoded by a tensorial representation, namely TensoSDF. At the core of our method is the roughness-aware incorporation of the radiance and reflectance fields, which enables a robust reconstruction of objects with arbitrary reflective materials. Furthermore, the tensorial representation enhances geometry details in the reconstructed surface and reduces the training time. Finally, we estimate the materials using an explicit mesh for efficient intersection computation and an implicit SDF for accurate representation. Consequently, our method can achieve more robust geometry reconstruction, outperform the previous works in terms of relighting quality, and reduce 50% training times and 70% inference time.","sentences":["Reconstructing objects with realistic materials from multi-view images is problematic, since it is highly ill-posed.","Although the neural reconstruction approaches have exhibited impressive reconstruction ability, they are designed for objects with specific materials (e.g., diffuse or specular materials).","To this end, we propose a novel framework for robust geometry and material reconstruction, where the geometry is expressed with the implicit signed distance field (SDF) encoded by a tensorial representation, namely TensoSDF.","At the core of our method is the roughness-aware incorporation of the radiance and reflectance fields, which enables a robust reconstruction of objects with arbitrary reflective materials.","Furthermore, the tensorial representation enhances geometry details in the reconstructed surface and reduces the training time.","Finally, we estimate the materials using an explicit mesh for efficient intersection computation and an implicit SDF for accurate representation.","Consequently, our method can achieve more robust geometry reconstruction, outperform the previous works in terms of relighting quality, and reduce 50% training times and 70% inference time."],"url":"http://arxiv.org/abs/2402.02771v1","category":"cs.GR"}
{"created":"2024-02-05 06:20:52","title":"Focal Modulation Networks for Interpretable Sound Classification","abstract":"The increasing success of deep neural networks has raised concerns about their inherent black-box nature, posing challenges related to interpretability and trust. While there has been extensive exploration of interpretation techniques in vision and language, interpretability in the audio domain has received limited attention, primarily focusing on post-hoc explanations. This paper addresses the problem of interpretability by-design in the audio domain by utilizing the recently proposed attention-free focal modulation networks (FocalNets). We apply FocalNets to the task of environmental sound classification for the first time and evaluate their interpretability properties on the popular ESC-50 dataset. Our method outperforms a similarly sized vision transformer both in terms of accuracy and interpretability. Furthermore, it is competitive against PIQ, a method specifically designed for post-hoc interpretation in the audio domain.","sentences":["The increasing success of deep neural networks has raised concerns about their inherent black-box nature, posing challenges related to interpretability and trust.","While there has been extensive exploration of interpretation techniques in vision and language, interpretability in the audio domain has received limited attention, primarily focusing on post-hoc explanations.","This paper addresses the problem of interpretability by-design in the audio domain by utilizing the recently proposed attention-free focal modulation networks (FocalNets).","We apply FocalNets to the task of environmental sound classification for the first time and evaluate their interpretability properties on the popular ESC-50 dataset.","Our method outperforms a similarly sized vision transformer both in terms of accuracy and interpretability.","Furthermore, it is competitive against PIQ, a method specifically designed for post-hoc interpretation in the audio domain."],"url":"http://arxiv.org/abs/2402.02754v1","category":"cs.SD"}
{"created":"2024-02-05 06:05:32","title":"Horizontality with infinite complexity in the twistor spaces on tori","abstract":"We study the complexity of horizontality in the twistor space $\\hat{E}$ associated with an oriented vector bundle $E$ of rank $4$ with a positive-definite metric over a torus. If the horizontality has finite complexity of degree $d>2$ for an element of a fiber of $\\hat{E}$, then the complexity is expressed in terms of a finite subgroup of $SO(3)$ ([1]). In the present paper, it is seen that if the horizontality has infinite complexity derived from one of the cases studied in [1], then the complexity is expressed by a dense subset of $S^2$.","sentences":["We study the complexity of horizontality in the twistor space $\\hat{E}$ associated with an oriented vector bundle $E$ of rank $4$ with a positive-definite metric over a torus.","If the horizontality has finite complexity of degree $d>2$ for an element of a fiber of $\\hat{E}$, then the complexity is expressed in terms of a finite subgroup of $SO(3)$ ([1]).","In the present paper, it is seen that if the horizontality has infinite complexity derived from one of the cases studied in [1], then the complexity is expressed by a dense subset of $S^2$."],"url":"http://arxiv.org/abs/2402.02748v1","category":"math.DG"}
{"created":"2024-02-05 05:26:17","title":"InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data","abstract":"There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \\texttt{InVA} over VAEs, which typically do not allow borrowing information between input images. The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI).","sentences":["There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities.","While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image.","Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image.","The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation.","Numerical results demonstrate substantial advantages of \\texttt{InVA} over VAEs, which typically do not allow borrowing information between input images.","The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI)."],"url":"http://arxiv.org/abs/2402.02734v1","category":"cs.CV"}
{"created":"2024-02-05 05:01:28","title":"Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN","abstract":"In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of generative AI. Simulation results showcase that the proposed GAN-CRME is even capable of coarse error-correction when the geographical map information is inaccurate.","sentences":["In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications.","This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map.","In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information.","The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity.","Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of generative AI.","Simulation results showcase that the proposed GAN-CRME is even capable of coarse error-correction when the geographical map information is inaccurate."],"url":"http://arxiv.org/abs/2402.02729v1","category":"cs.IT"}
{"created":"2024-02-05 04:36:56","title":"Dilatonic Geometrodynamics of a Two-Dimensional Curved Surface due to a Quantum Mechanically Confined Particle","abstract":"We provide a unique and novel extension of da Costa's calculation of a quantum mechanically constrained particle by analyzing the perturbative back reaction of the quantum confined particle's eigenstates and spectra upon the geometry of the curved surface itself. We do this by first formulating a two dimensional action principle of the quantum constrained particle, which upon wave function variation reproduces Schr\\\"odinger's equation including da Costa's surface curvature induced potentials. Given this action principle, we vary its functional with respect to the embedded two dimensional inverse-metric to obtain the respective geometrodynamical Einstein equation. We solve this resulting Einstein equation perturbatively by first solving the da Costa's Schr\\\"odinger equation to obtain an initial eigensystem, which is used as initial-input data for a perturbed metric inserted into the derived Einstein equation. As a proof of concept, we perform this calculation on a two-sphere and show its first iterative perturbed shape. We also include the back reaction of a constant external magnetic field in a separate calculation. The geometrodynamical analysis is performed within a two dimensional dilation gravity analog, due to several computational advantages.","sentences":["We provide a unique and novel extension of da Costa's calculation of a quantum mechanically constrained particle by analyzing the perturbative back reaction of the quantum confined particle's eigenstates and spectra upon the geometry of the curved surface itself.","We do this by first formulating a two dimensional action principle of the quantum constrained particle, which upon wave function variation reproduces Schr\\\"odinger's equation including da Costa's surface curvature induced potentials.","Given this action principle, we vary its functional with respect to the embedded two dimensional inverse-metric to obtain the respective geometrodynamical Einstein equation.","We solve this resulting Einstein equation perturbatively by first solving the da Costa's Schr\\\"odinger equation to obtain an initial eigensystem, which is used as initial-input data for a perturbed metric inserted into the derived Einstein equation.","As a proof of concept, we perform this calculation on a two-sphere and show its first iterative perturbed shape.","We also include the back reaction of a constant external magnetic field in a separate calculation.","The geometrodynamical analysis is performed within a two dimensional dilation gravity analog, due to several computational advantages."],"url":"http://arxiv.org/abs/2402.02722v1","category":"hep-th"}
{"created":"2024-02-05 04:21:30","title":"Neural option pricing for rough Bergomi model","abstract":"The rough Bergomi (rBergomi) model can accurately describe the historical and implied volatilities, and has gained much attention in the past few years. However, there are many hidden unknown parameters or even functions in the model. In this work, we investigate the potential of learning the forward variance curve in the rBergomi model using a neural SDE. To construct an efficient solver for the neural SDE, we propose a novel numerical scheme for simulating the volatility process using the modified summation of exponentials. Using the Wasserstein 1-distance to define the loss function, we show that the learned forward variance curve is capable of calibrating the price process of the underlying asset and the price of the European-style options simultaneously. Several numerical tests are provided to demonstrate its performance.","sentences":["The rough Bergomi (rBergomi) model can accurately describe the historical and implied volatilities, and has gained much attention in the past few years.","However, there are many hidden unknown parameters or even functions in the model.","In this work, we investigate the potential of learning the forward variance curve in the rBergomi model using a neural SDE.","To construct an efficient solver for the neural SDE, we propose a novel numerical scheme for simulating the volatility process using the modified summation of exponentials.","Using the Wasserstein 1-distance to define the loss function, we show that the learned forward variance curve is capable of calibrating the price process of the underlying asset and the price of the European-style options simultaneously.","Several numerical tests are provided to demonstrate its performance."],"url":"http://arxiv.org/abs/2402.02714v1","category":"q-fin.CP"}
{"created":"2024-02-05 04:15:31","title":"Architectural Strategies for the optimization of Physics-Informed Neural Networks","abstract":"Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles. Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs. In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs. Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process. Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature.","sentences":["Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles.","Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs.","In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective.","Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs.","Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process.","Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature."],"url":"http://arxiv.org/abs/2402.02711v1","category":"cs.LG"}
{"created":"2024-02-05 03:37:05","title":"Knowledge-driven deep learning for fast MR imaging: undersampled MR image reconstruction from supervised to un-supervised learning","abstract":"Deep learning (DL) has emerged as a leading approach in accelerating MR imaging. It employs deep neural networks to extract knowledge from available datasets and then applies the trained networks to reconstruct accurate images from limited measurements. Unlike natural image restoration problems, MR imaging involves physics-based imaging processes, unique data properties, and diverse imaging tasks. This domain knowledge needs to be integrated with data-driven approaches. Our review will introduce the significant challenges faced by such knowledge-driven DL approaches in the context of fast MR imaging along with several notable solutions, which include learning neural networks and addressing different imaging application scenarios. The traits and trends of these techniques have also been given which have shifted from supervised learning to semi-supervised learning, and finally, to unsupervised learning methods. In addition, MR vendors' choices of DL reconstruction have been provided along with some discussions on open questions and future directions, which are critical for the reliable imaging systems.","sentences":["Deep learning (DL) has emerged as a leading approach in accelerating MR imaging.","It employs deep neural networks to extract knowledge from available datasets and then applies the trained networks to reconstruct accurate images from limited measurements.","Unlike natural image restoration problems, MR imaging involves physics-based imaging processes, unique data properties, and diverse imaging tasks.","This domain knowledge needs to be integrated with data-driven approaches.","Our review will introduce the significant challenges faced by such knowledge-driven DL approaches in the context of fast MR imaging along with several notable solutions, which include learning neural networks and addressing different imaging application scenarios.","The traits and trends of these techniques have also been given which have shifted from supervised learning to semi-supervised learning, and finally, to unsupervised learning methods.","In addition, MR vendors' choices of DL reconstruction have been provided along with some discussions on open questions and future directions, which are critical for the reliable imaging systems."],"url":"http://arxiv.org/abs/2402.02704v1","category":"eess.IV"}
{"created":"2024-02-05 03:32:27","title":"Production of open-charm pentaquark molecules in decay $B^0 \\rightarrow \\bar{D}^0 p \\bar{p}$","abstract":"This study explores the production of open-charm pentaquark molecular states, specifically $N\\bar{D}^*$ and $\\bar{N}\\bar{D}^*$, within the $B^0 \\rightarrow \\bar{D}^0 p \\bar{p}$ decay process. We analyze the invariant mass spectrum of $p\\bar{D}^0$ and $\\bar{p}\\bar{D}^0$, incorporating the rescattering process calculated using a quasipotential Bethe-Salpeter equation approach. Our findings suggest the potential identification of the isoscalar $\\bar{N}\\bar{D}^*$ molecule with $3/2^+$, serving as the antiparticle partner of the $\\Lambda_c(2940)$, in the $\\bar{p}\\bar{D}^0$ mass distribution. Additionally, distinctive signals of the isovector $N\\bar{D}^*$ molecule with $1/2^-$ may emerge in the $p\\bar{D}^0$ invariant mass distribution. We highlight the significance of the three-body decay of the bottom meson as a valuable avenue for studying open-charm molecules and advocate for increased attention and more precise experimental measurements of the $B^0 \\rightarrow \\bar{D}^0 p \\bar{p}$ process.","sentences":["This study explores the production of open-charm pentaquark molecular states, specifically $N\\bar{D}^*$ and $\\bar{N}\\bar{D}^*$, within the $B^0 \\rightarrow \\bar{D}^0 p \\bar{p}$ decay process.","We analyze the invariant mass spectrum of $p\\bar{D}^0$ and $\\bar{p}\\bar{D}^0$, incorporating the rescattering process calculated using a quasipotential Bethe-Salpeter equation approach.","Our findings suggest the potential identification of the isoscalar $\\bar{N}\\bar{D}^*$ molecule with $3/2^+$, serving as the antiparticle partner of the $\\Lambda_c(2940)$, in the $\\bar{p}\\bar{D}^0$ mass distribution.","Additionally, distinctive signals of the isovector $N\\bar{D}^*$ molecule with $1/2^-$ may emerge in the $p\\bar{D}^0$ invariant mass distribution.","We highlight the significance of the three-body decay of the bottom meson as a valuable avenue for studying open-charm molecules and advocate for increased attention and more precise experimental measurements of the $B^0 \\rightarrow \\bar{D}^0 p \\bar{p}$ process."],"url":"http://arxiv.org/abs/2402.02703v1","category":"hep-ph"}
{"created":"2024-02-05 03:20:33","title":"Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures","abstract":"Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models. In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture. We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations. As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data, empirical results show the proposed theory and design principle also apply to popular real-world datasets.","sentences":["Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks.","There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models.","In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture.","We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations.","As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ.","Despite derived here for Gaussian mixture data, empirical results show the proposed theory and design principle also apply to popular real-world datasets."],"url":"http://arxiv.org/abs/2402.02697v1","category":"cs.LG"}
{"created":"2024-02-05 03:03:00","title":"Statistical Guarantees for Link Prediction using Graph Neural Networks","abstract":"This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense graphs. Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets.","sentences":["This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon.","We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities.","We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges.","Our guarantees hold for both sparse and dense graphs.","Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets."],"url":"http://arxiv.org/abs/2402.02692v1","category":"cs.LG"}
{"created":"2024-02-05 02:53:55","title":"Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions","abstract":"Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional representation, revealing communication directions across brain regions and separating oscillatory communications into different frequency bands.","sentences":["Studying the complex interactions between different brain regions is crucial in neuroscience.","Various statistical methods have explored the latent communication across multiple brain regions.","Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths.","The GP-based approach effectively discovers latent variables such as frequency bands and communication directions.","Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation.","In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP).","Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings.","Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional representation, revealing communication directions across brain regions and separating oscillatory communications into different frequency bands."],"url":"http://arxiv.org/abs/2402.02686v1","category":"q-bio.NC"}
{"created":"2024-02-05 02:42:35","title":"Nonlinear potential theoretic methods in nonuniformly ellliptic problems","abstract":"Nonuniform ellipticity is a classical topic in the theory of partial differential equations. While several results in regularity theory have been adding up over decades, many basic issues, as for instance the validity of Schauder theory and sharp dependence of regularity upon data, remained opened for a while. In these notes we give an overview of recent results and techniques about the topic, that, via a novel use of nonlinear potential theoretic methods, allow to answer several of the above questions.","sentences":["Nonuniform ellipticity is a classical topic in the theory of partial differential equations.","While several results in regularity theory have been adding up over decades, many basic issues, as for instance the validity of Schauder theory and sharp dependence of regularity upon data, remained opened for a while.","In these notes we give an overview of recent results and techniques about the topic, that, via a novel use of nonlinear potential theoretic methods, allow to answer several of the above questions."],"url":"http://arxiv.org/abs/2402.02683v1","category":"math.AP"}
{"created":"2024-02-05 00:20:01","title":"Log concavity of the Grothendieck class of $\\overline{\\mathcal M}_{0,n}$","abstract":"Using a known recursive formula for the Grothendieck classes of the moduli spaces $\\overline{\\mathcal M}_{0,n}$, we prove that they satisfy an asymptotic form of ultra-log-concavity as polynomials in the Lefschetz class. We also observe that these polynomials are $\\gamma$-positive. Both properties, along with numerical evidence, support the conjecture that these polynomials only have real zeros. This conjecture may be viewed as a particular case of a possible extension of a conjecture of J. Huh on Hilbert series of Chow rings of matroids.   We prove asymptotic ultra-log-concavity by studying differential equations obtained from the recursion, whose solutions are the generating functions of the individual betti numbers of $\\overline{\\mathcal M}_{0,n}$. We obtain a rather complete description of these generating functions, determining their asymptotic behavior; their dominant term is controlled by the coefficients of the Lambert W function. The $\\gamma$-positivity property follows directly from the recursion, extending the argument of Ferrari et al. proving $\\gamma$-positivity for the Hilbert series of the Chow ring of matroids.","sentences":["Using a known recursive formula for the Grothendieck classes of the moduli spaces $\\overline{\\mathcal M}_{0,n}$, we prove that they satisfy an asymptotic form of ultra-log-concavity as polynomials in the Lefschetz class.","We also observe that these polynomials are $\\gamma$-positive.","Both properties, along with numerical evidence, support the conjecture that these polynomials only have real zeros.","This conjecture may be viewed as a particular case of a possible extension of a conjecture of J. Huh on Hilbert series of Chow rings of matroids.   ","We prove asymptotic ultra-log-concavity by studying differential equations obtained from the recursion, whose solutions are the generating functions of the individual betti numbers of $\\overline{\\mathcal M}_{0,n}$. We obtain a rather complete description of these generating functions, determining their asymptotic behavior; their dominant term is controlled by the coefficients of the Lambert W function.","The $\\gamma$-positivity property follows directly from the recursion, extending the argument of Ferrari et al. proving $\\gamma$-positivity for the Hilbert series of the Chow ring of matroids."],"url":"http://arxiv.org/abs/2402.02646v1","category":"math.AG"}
{"created":"2024-02-04 23:15:47","title":"Representations of solutions of time-fractional multi-order systems of differential-operator equations","abstract":"This paper is devoted to the general theory of systems of time-fractional differential-operator equations. The representation formulas for solutions of systems of ordinary differential equations with single (commensurate) fractional order is known through the matrix valued Mittag-Leffler function. Multi-order (incommensurate) systems with rational components can be reduced to single order systems, and hence, representation formulas are also known. However, for arbitrary fractional multi-order (not necessarily with rational components) systems of differential equations this question remains open even in the case of ordinary differential equations. In this paper we obtain representation formulas for solutions of arbitrary fractional multi-order systems of differential-operator equations along with proving the existence and uniqueness theorems in appropriate topological-vector spaces. Moreover, we introduce vector-indexed Mittag-Leffler functions and prove some of their properties.","sentences":["This paper is devoted to the general theory of systems of time-fractional differential-operator equations.","The representation formulas for solutions of systems of ordinary differential equations with single (commensurate) fractional order is known through the matrix valued Mittag-Leffler function.","Multi-order (incommensurate) systems with rational components can be reduced to single order systems, and hence, representation formulas are also known.","However, for arbitrary fractional multi-order (not necessarily with rational components) systems of differential equations this question remains open even in the case of ordinary differential equations.","In this paper we obtain representation formulas for solutions of arbitrary fractional multi-order systems of differential-operator equations along with proving the existence and uniqueness theorems in appropriate topological-vector spaces.","Moreover, we introduce vector-indexed Mittag-Leffler functions and prove some of their properties."],"url":"http://arxiv.org/abs/2402.02638v1","category":"math.CA"}
{"created":"2024-02-04 23:11:19","title":"$C^*$-Algebraic Machine Learning: Moving in a New Direction","abstract":"Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications.","sentences":["Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra.","We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning.","The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers.","It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models.","We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks.","Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications."],"url":"http://arxiv.org/abs/2402.02637v1","category":"cs.LG"}
{"created":"2024-02-04 22:16:45","title":"Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network","abstract":"This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are perfectly trained. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with number of states very close to ground truth DFA. Among RNN cells, O2RNN produces stable DFA consistently compared to other cells. For Dyck Languages, we observe that although GRU outperforms other RNNs in network performance, the DFA extracted by O2RNN has higher performance and better stability. The stability is computed as the standard deviation of accuracy on test sets on networks trained across $10$ seeds. On Dyck Languages, quantization methods outperformed $L^{*}$ with better stability in accuracy and the number of states. $L^{*}$ often showed instability in accuracy in the order of $16\\% - 22\\%$ for GRU and MIRNN while deviation for quantization methods varied in $5\\% - 15\\%$. In many instances with LSTM and GRU, DFA's extracted by $L^{*}$ even failed to beat chance accuracy ($50\\%$), while those extracted by quantization method had standard deviation in the $7\\%-17\\%$ range. For O2RNN, both rule extraction methods had deviation in the $0.5\\% - 3\\%$ range.","sentences":["This paper analyzes two competing rule extraction methodologies: quantization and equivalence query.","We trained $3600$ RNN models, extracting $18000$ DFA with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds.","We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN.","The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others.","$L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are perfectly trained.","However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states.","In contrast, quantization methods result in rules with number of states very close to ground truth DFA.","Among RNN cells, O2RNN produces stable DFA consistently compared to other cells.","For Dyck Languages, we observe that although GRU outperforms other RNNs in network performance, the DFA extracted by O2RNN has higher performance and better stability.","The stability is computed as the standard deviation of accuracy on test sets on networks trained across $10$ seeds.","On Dyck Languages, quantization methods outperformed $L^{*}$ with better stability in accuracy and the number of states.","$L^{*}$ often showed instability in accuracy in the order of $16\\% - 22\\%$ for GRU and MIRNN while deviation for quantization methods varied in $5\\% - 15\\%$.","In many instances with LSTM and GRU, DFA's extracted by $L^{*}$ even failed to beat chance accuracy ($50\\%$), while those extracted by quantization method had standard deviation in the $7\\%-17\\%$ range.","For O2RNN, both rule extraction methods had deviation in the $0.5\\% - 3\\%$ range."],"url":"http://arxiv.org/abs/2402.02627v1","category":"cs.LG"}
{"created":"2024-02-04 20:53:45","title":"Orbital collapse and dual states of the $5g$ electrons in superheavy elements","abstract":"The problem of orbital collapse of the $5g$ and $6f$ electrons in atoms of superheavy elements (SHE) is considered. Previously, the presence of the orbital collapse was established for the $4f$ and $5f$ elements of the periodic table. Because of the large centrifugal term for the $f$ and $g$ electrons, the effective radial potential has two wells, one narrow and deep and the other wide but shallow. Depending on the external parameters, the electron can be either localized in the outer well with low binding energy and large average radius or in the inner one with higher energy and smaller radius. In this work, we demonstrate the existence of the orbital collapse for the $5g$ electrons when changing the total angular momentum $J$ of the atom. We also found that for some SHE elements, two different solutions of the same Dirac-Fock equations may coexist, with the $5g$ electron localized either in the inner or outer well. In both cases, the radial wave functions are nodeless. The problem of the dual-state coexistence is studied by the configuration-interaction method in the Dirac-Fock-Sturm orbital basis as well.","sentences":["The problem of orbital collapse of the $5g$ and $6f$ electrons in atoms of superheavy elements (SHE) is considered.","Previously, the presence of the orbital collapse was established for the $4f$ and $5f$ elements of the periodic table.","Because of the large centrifugal term for the $f$ and $g$ electrons, the effective radial potential has two wells, one narrow and deep and the other wide but shallow.","Depending on the external parameters, the electron can be either localized in the outer well with low binding energy and large average radius or in the inner one with higher energy and smaller radius.","In this work, we demonstrate the existence of the orbital collapse for the $5g$ electrons when changing the total angular momentum $J$ of the atom.","We also found that for some SHE elements, two different solutions of the same Dirac-Fock equations may coexist, with the $5g$ electron localized either in the inner or outer well.","In both cases, the radial wave functions are nodeless.","The problem of the dual-state coexistence is studied by the configuration-interaction method in the Dirac-Fock-Sturm orbital basis as well."],"url":"http://arxiv.org/abs/2402.02609v1","category":"physics.atom-ph"}
{"created":"2024-02-04 20:01:22","title":"Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments","abstract":"Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.","sentences":["Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models.","We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems.","We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise.","Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations.","As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero.","Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond."],"url":"http://arxiv.org/abs/2402.02593v1","category":"cs.LG"}
{"created":"2024-02-04 18:50:29","title":"DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing","abstract":"Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion.","sentences":["Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years.","Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging.","In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content.","In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content.","To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling.","In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality.","Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting).","Our source code is released at https://github.com/MC-E/DragonDiffusion."],"url":"http://arxiv.org/abs/2402.02583v1","category":"cs.CV"}
{"created":"2024-02-04 16:56:08","title":"A Truly Joint Neural Architecture for Segmentation and Parsing","abstract":"Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art performance on parsing, tagging and segmentation of the Hebrew section of UD, using a single model. This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.","sentences":["Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages.","The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance.","Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing.","However, neural state-of-the-art parsers to date use a strict pipeline.","In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once.","Our experiments on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art performance on parsing, tagging and segmentation of the Hebrew section of UD, using a single model.","This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages."],"url":"http://arxiv.org/abs/2402.02564v1","category":"cs.CL"}
{"created":"2024-02-04 15:50:27","title":"Applications of the icosahedral equation for the Rogers-Ramanujan continued fraction","abstract":"Let $R(q)$ denote the Rogers-Ramanujan continued fraction for $|q| < 1$. By applying the RootApproximant command in the Wolfram language to expressions involving the theta function $f(-q) := (q;q)_{\\infty}$ given in modular relations due to Yi, this provides a systematic way of obtaining experimentally discovered evaluations for $R\\big(e^{-\\pi\\sqrt{r}}\\big)$, for $r \\in \\mathbb{Q}_{> 0}$. We succeed in applying this approach to obtain explicit closed forms, in terms of radicals over $\\mathbb{Q}$, for the Rogers-Ramanujan continued fraction that have not previously been discovered or proved. We prove our closed forms using the icosahedral equation for $R$ together with closed forms for and modular relations associated with Ramanujan's $G$- and $g$-functions. An especially remarkable closed form that we introduce and prove is for $R\\big( e^{-\\pi \\sqrt{48/5} } \\big)$, in view of the computational difficulties surrounding the application of an order-25 modular relation in the evaluation of $G_{48/5}$.","sentences":["Let $R(q)$ denote the Rogers-Ramanujan continued fraction for $|q|","< 1$.","By applying the RootApproximant command in the Wolfram language to expressions involving the theta function $f(-q) :","= (q;q)_{\\infty}$ given in modular relations due to Yi, this provides a systematic way of obtaining experimentally discovered evaluations for $R\\big(e^{-\\pi\\sqrt{r}}\\big)$, for $r \\in \\mathbb{Q}_{> 0}$. We succeed in applying this approach to obtain explicit closed forms, in terms of radicals over $\\mathbb{Q}$, for the Rogers-Ramanujan continued fraction that have not previously been discovered or proved.","We prove our closed forms using the icosahedral equation for $R$ together with closed forms for and modular relations associated with Ramanujan's $G$- and $g$-functions.","An especially remarkable closed form that we introduce and prove is for $R\\big( e^{-\\pi \\sqrt{48/5} } \\big)$, in view of the computational difficulties surrounding the application of an order-25 modular relation in the evaluation of $G_{48/5}$."],"url":"http://arxiv.org/abs/2402.02546v1","category":"math.NT"}
{"created":"2024-02-04 15:32:33","title":"Bott-Chern formality and Massey products on strong K\u00e4hler with torsion and K\u00e4hler solvmanifolds","abstract":"We study the interplay between geometrically-Bott-Chern-formal metrics and SKT metrics. We prove that a $6$-dimensional nilmanifold endowed with a invariant complex structure admits an SKT metric if and only if it is geometrically-Bott-Chern-formal. We also provide some partial results in higher dimensions for nilmanifolds endowed with a class of suitable complex structures. Furthermore, we prove that any K\\\"ahler solvmanifold is geometrically formal. Finally, we explicitly construct lattices for a complex solvable Lie group in the list of Nakamura [23] on which we provide a non vanishing quadruple $ABC$-Massey product.","sentences":["We study the interplay between geometrically-Bott-Chern-formal metrics and SKT metrics.","We prove that a $6$-dimensional nilmanifold endowed with a invariant complex structure admits an SKT metric if and only if it is geometrically-Bott-Chern-formal.","We also provide some partial results in higher dimensions for nilmanifolds endowed with a class of suitable complex structures.","Furthermore, we prove that any K\\\"ahler solvmanifold is geometrically formal.","Finally, we explicitly construct lattices for a complex solvable Lie group in the list of Nakamura [23] on which we provide a non vanishing quadruple $ABC$-Massey product."],"url":"http://arxiv.org/abs/2402.02537v1","category":"math.DG"}
{"created":"2024-02-04 15:25:53","title":"Non-existence of classical solutions to a two-phase flow model with vacuum","abstract":"In this paper, we study the well-posedness of classical solutions to a two-phase flow model consisting of the pressureless Euler equations coupled with the isentropic compressible Navier-Stokes equations via a drag forcing term. We consider the case that the fluid densities may contain a vacuum, and the viscosities are density-dependent functions. Under suitable assumptions on the initial data, we show that the finite-energy (i.e., in the inhomogeneous Sobolev space) classical solutions to the Cauchy problem of this coupled system do not exist for any small time.","sentences":["In this paper, we study the well-posedness of classical solutions to a two-phase flow model consisting of the pressureless Euler equations coupled with the isentropic compressible Navier-Stokes equations via a drag forcing term.","We consider the case that the fluid densities may contain a vacuum, and the viscosities are density-dependent functions.","Under suitable assumptions on the initial data, we show that the finite-energy (i.e., in the inhomogeneous Sobolev space) classical solutions to the Cauchy problem of this coupled system do not exist for any small time."],"url":"http://arxiv.org/abs/2402.02531v1","category":"math.AP"}
{"created":"2024-02-04 15:24:23","title":"Polyhedral bounds on the joint spectrum and temperedness of locally symmetric spaces","abstract":"Given a real semisimple connected Lie group $G$ and a discrete torsion-free subgroup $\\Gamma < G$ we prove a precise connection between growth rates of the group $\\Gamma$, polyhedral bounds on the joint spectrum of the ring of invariant differential operators, and the decay of matrix coefficients. In particular, this allows us to completely characterize temperedness of $L^2(\\Gamma\\backslash G)$ in this general setting.","sentences":["Given a real semisimple connected Lie group $G$ and a discrete torsion-free subgroup $\\Gamma < G$ we prove a precise connection between growth rates of the group $\\Gamma$, polyhedral bounds on the joint spectrum of the ring of invariant differential operators, and the decay of matrix coefficients.","In particular, this allows us to completely characterize temperedness of $L^2(\\Gamma\\backslash G)$ in this general setting."],"url":"http://arxiv.org/abs/2402.02530v1","category":"math.RT"}
{"created":"2024-02-04 15:14:25","title":"A theta operator for the group $\\mathrm{GSp}_4$","abstract":"We construct a differential operator on sheaves of $p$-adic modular forms defined over the locus of $p$-rank $\\ge 1$ of the Siegel threefold, by applying a revisited version of the approach that Sean Howe recently introduced in his paper \"A unipotent circle action on $p$-adic modular forms\" (2020, Trans. Am. Math. Soc.) to construct the theta operator in the elliptic case.","sentences":["We construct a differential operator on sheaves of $p$-adic modular forms defined over the locus of $p$-rank $\\ge 1$ of the Siegel threefold, by applying a revisited version of the approach that Sean Howe recently introduced in his paper \"A unipotent circle action on $p$-adic modular forms\" (2020, Trans. Am.","Math.","Soc.) to construct the theta operator in the elliptic case."],"url":"http://arxiv.org/abs/2402.02524v1","category":"math.NT"}
{"created":"2024-02-04 14:48:21","title":"Mass-Radius relationship of Strongly Magnetized Super-Chandrasekhar Anisotropic Deformed White Dwarf Stars in presence of $\u03b3$-metric","abstract":"The masses and radii of strongly magnetized anisotropic deformed white dwarf stars are investigated using the stellar structure equations in the parameterized $\\gamma$-metric formalism. The Equation of State (EoS) of a completely degenerate relativistic electron gas in strong quantizing density-dependent magnetic field is developed. The fluid and field pressure anisotropy among the parallel and perpendicular components to the magnetic field is taken into consideration. This anisotropy in the EoS causes axisymmetric deformation of the star. We found stable solutions of deformed super-Chandrasekhar ultramassive white dwarfs. The masses of anisotropic magnetized white dwarfs at the same central density decrease monotonically with the increase in the strength of the central magnetic field, while the equatorial radii increase monotonically. This is in sharp contrast to the isotropic case where both the mass and radius increase monotonically. High magnetic field increases anisotropy and oblateness. We also see that the maximum mass and its corresponding equatorial radius both decrease as central magnetic field strength increases. We also notice that the maximum mass occurs at higher central density as the magnetic field increases. This shows that increasing magnetic field (hence increasing anisotropy) softens the EoS and makes the star more compact.","sentences":["The masses and radii of strongly magnetized anisotropic deformed white dwarf stars are investigated using the stellar structure equations in the parameterized $\\gamma$-metric formalism.","The Equation of State (EoS) of a completely degenerate relativistic electron gas in strong quantizing density-dependent magnetic field is developed.","The fluid and field pressure anisotropy among the parallel and perpendicular components to the magnetic field is taken into consideration.","This anisotropy in the EoS causes axisymmetric deformation of the star.","We found stable solutions of deformed super-Chandrasekhar ultramassive white dwarfs.","The masses of anisotropic magnetized white dwarfs at the same central density decrease monotonically with the increase in the strength of the central magnetic field, while the equatorial radii increase monotonically.","This is in sharp contrast to the isotropic case where both the mass and radius increase monotonically.","High magnetic field increases anisotropy and oblateness.","We also see that the maximum mass and its corresponding equatorial radius both decrease as central magnetic field strength increases.","We also notice that the maximum mass occurs at higher central density as the magnetic field increases.","This shows that increasing magnetic field (hence increasing anisotropy) softens the EoS and makes the star more compact."],"url":"http://arxiv.org/abs/2402.02509v1","category":"astro-ph.SR"}
{"created":"2024-02-04 14:11:53","title":"Gravitational wave energy momentum-tensor in reduced Horndeski theories","abstract":"We generalize, imposing the field equations only at dominant order, the Isaacson formula for the gravitational wave (GW) energy-momentum tensor (EMT) to the class of Horndeski theories in which the tensor modes travel at the speed of light (reduced Horndeski theories) and scalar waves are present. We discuss important particular cases such as: theories where scalar waves are also luminal and theories in which the transverse-traceless gauge can be achieved in an arbitrary open set. The vanishing of the trace of the gravitational wave energy-momentum tensor is obtained for theories in which all wave perturbations propagate at the speed of light. The trace is shown not to vanish trivially in other cases. We obtain, as a particular case of our general result, the GW EMTs, in a Brans-Dicke theory, both in the Einstein frame, recovering previous results in the literature, and in the Jordan frame, thereby showing the GW EMT is not conformally invariant. We further prove that there exists a subclass of reduced Horndeski theories where, in contrast to general relativity, the divergence of the GW EMT does not vanish even after the imposition of the full equations of motion, assuming an eikonal solution.","sentences":["We generalize, imposing the field equations only at dominant order, the Isaacson formula for the gravitational wave (GW) energy-momentum tensor (EMT) to the class of Horndeski theories in which the tensor modes travel at the speed of light (reduced Horndeski theories) and scalar waves are present.","We discuss important particular cases such as: theories where scalar waves are also luminal and theories in which the transverse-traceless gauge can be achieved in an arbitrary open set.","The vanishing of the trace of the gravitational wave energy-momentum tensor is obtained for theories in which all wave perturbations propagate at the speed of light.","The trace is shown not to vanish trivially in other cases.","We obtain, as a particular case of our general result, the GW EMTs, in a Brans-Dicke theory, both in the Einstein frame, recovering previous results in the literature, and in the Jordan frame, thereby showing the GW EMT is not conformally invariant.","We further prove that there exists a subclass of reduced Horndeski theories where, in contrast to general relativity, the divergence of the GW EMT does not vanish even after the imposition of the full equations of motion, assuming an eikonal solution."],"url":"http://arxiv.org/abs/2402.02497v1","category":"gr-qc"}
{"created":"2024-02-04 13:58:48","title":"Extended Dynamic Mode Decomposition: Sharp bounds on the sample efficiency","abstract":"We rigorously derive novel and sharp finite-data error bounds for highly sample-efficient Extended Dynamic Mode Decomposition (EDMD) for both i.i.d. and ergodic sampling. In particular, we show all results in a very general setting removing most of the typically imposed assumptions such that, among others, discrete- and continuous-time stochastic processes as well as nonlinear partial differential equations are contained in the considered system class. Besides showing an exponential rate for i.i.d. sampling, we prove, to the best of our knowledge, the first superlinear convergence rates for ergodic sampling of deterministic systems. We verify sharpness of the derived error bounds by conducting numerical simulations for highly-complex applications from molecular dynamics and chaotic flame propagation.","sentences":["We rigorously derive novel and sharp finite-data error bounds for highly sample-efficient Extended Dynamic Mode Decomposition (EDMD) for both i.i.d. and ergodic sampling.","In particular, we show all results in a very general setting removing most of the typically imposed assumptions such that, among others, discrete- and continuous-time stochastic processes as well as nonlinear partial differential equations are contained in the considered system class.","Besides showing an exponential rate for i.i.d. sampling, we prove, to the best of our knowledge, the first superlinear convergence rates for ergodic sampling of deterministic systems.","We verify sharpness of the derived error bounds by conducting numerical simulations for highly-complex applications from molecular dynamics and chaotic flame propagation."],"url":"http://arxiv.org/abs/2402.02494v1","category":"math.DS"}
{"created":"2024-02-04 13:27:27","title":"Interplay between tie strength and neighbourhood topology in complex networks: Granovetter's theory and beyond","abstract":"Granovetter's weak ties theory is a very important sociological theory according to which a correlation between edge weight and the network's topology should exist. More specifically, the neighbourhood overlap of two nodes connected by an edge should be positively correlated with edge weight (tie strength). However, some real social networks exhibit a negative correlation - the most prominent example is the scientific collaboration network, for which overlap decreases with edge weight. It has been demonstrated that the aforementioned inconsistency with Granovetter's theory can be alleviated in the scientific collaboration network through the use of asymmetric measures. In this paper, we explain that while asymmetric measures are often necessary to describe complex networks and to confirm Granovetter's theory, their interpretation is not simple, and there are pitfalls that one must be wary of. The definitions of asymmetric weights and overlaps introduce structural correlations that must be filtered out. We show that correlation profiles can be used to overcome this problem. Using this technique, not only do we confirm Granovetter's theory in various real and artificial social networks, but we also show that Granovetter-like weight-topology correlations are present in other complex networks (e.g. metabolic and neural networks). Our results suggest that Granovetter's theory is a sociological manifestation of more general principles governing various types of complex networks.","sentences":["Granovetter's weak ties theory is a very important sociological theory according to which a correlation between edge weight and the network's topology should exist.","More specifically, the neighbourhood overlap of two nodes connected by an edge should be positively correlated with edge weight (tie strength).","However, some real social networks exhibit a negative correlation - the most prominent example is the scientific collaboration network, for which overlap decreases with edge weight.","It has been demonstrated that the aforementioned inconsistency with Granovetter's theory can be alleviated in the scientific collaboration network through the use of asymmetric measures.","In this paper, we explain that while asymmetric measures are often necessary to describe complex networks and to confirm Granovetter's theory, their interpretation is not simple, and there are pitfalls that one must be wary of.","The definitions of asymmetric weights and overlaps introduce structural correlations that must be filtered out.","We show that correlation profiles can be used to overcome this problem.","Using this technique, not only do we confirm Granovetter's theory in various real and artificial social networks, but we also show that Granovetter-like weight-topology correlations are present in other complex networks (e.g. metabolic and neural networks).","Our results suggest that Granovetter's theory is a sociological manifestation of more general principles governing various types of complex networks."],"url":"http://arxiv.org/abs/2402.02487v1","category":"physics.soc-ph"}
{"created":"2024-02-04 13:25:18","title":"Weisfeiler Leman for Euclidean Equivariant Machine Learning","abstract":"The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds.   In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   Building on our results, we develop our WeLNet architecture, which can process position-velocity pairs, compute functions fully equivariant to permutations and rigid motions, and is provably complete and universal. Remarkably, WeLNet is provably complete precisely in the setting in which it is implemented in practice. Our theoretical results are complemented by experiments showing WeLNet sets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9 molecular conformation generation task.","sentences":["The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs).","Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\\mathrm{D}$ point cloud data.","Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds.","Yet, this result is limited to invariant continuous functions on point clouds.   ","In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications.","Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity.","Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   ","Building on our results, we develop our WeLNet architecture, which can process position-velocity pairs, compute functions fully equivariant to permutations and rigid motions, and is provably complete and universal.","Remarkably, WeLNet is provably complete precisely in the setting in which it is implemented in practice.","Our theoretical results are complemented by experiments showing WeLNet sets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9 molecular conformation generation task."],"url":"http://arxiv.org/abs/2402.02484v1","category":"cs.LG"}
{"created":"2024-02-04 13:18:25","title":"Linear Equations in Primes over $\\mathbb{F}_p[x]$","abstract":"We prove that the M\\\"obius function is orthogonal to polynomials over $\\mathbb{F}_q[x]$ (up to a characteristic condition). We use this orthogonality property to count prime solutions to affine-linear equations of bounded complexity in $\\mathbb{F}_p[x]$, with analog to a work of Green and Tao.","sentences":["We prove that the M\\\"obius function is orthogonal to polynomials over $\\mathbb{F}_q[x]$ (up to a characteristic condition).","We use this orthogonality property to count prime solutions to affine-linear equations of bounded complexity in $\\mathbb{F}_p[x]$, with analog to a work of Green and Tao."],"url":"http://arxiv.org/abs/2402.02480v1","category":"math.NT"}
{"created":"2024-02-04 13:15:52","title":"Topologically protected Casimir effect for lattice fermions","abstract":"The electromagnetic Casimir effect has a fermionic counterpart in topological insulators: Zero-point fluctuations of a massless Dirac fermion field mediate a force between magnetic scatterers. The Casimir force is insensitive to disorder that preserves the topological protection of an unpaired Dirac cone. The protection may be broken if the Dirac equation is discretized, and an exponential suppression of the Casimir effect will result if a gap opens at the Dirac point. Here we show how this lattice artefact may be avoided, by applying a recently developed local discretization of the Euclidean action that does not suffer from the fermion-doubling obstruction of local discretizations of the Hamiltonian.","sentences":["The electromagnetic Casimir effect has a fermionic counterpart in topological insulators: Zero-point fluctuations of a massless Dirac fermion field mediate a force between magnetic scatterers.","The Casimir force is insensitive to disorder that preserves the topological protection of an unpaired Dirac cone.","The protection may be broken if the Dirac equation is discretized, and an exponential suppression of the Casimir effect will result if a gap opens at the Dirac point.","Here we show how this lattice artefact may be avoided, by applying a recently developed local discretization of the Euclidean action that does not suffer from the fermion-doubling obstruction of local discretizations of the Hamiltonian."],"url":"http://arxiv.org/abs/2402.02477v1","category":"quant-ph"}
{"created":"2024-02-04 13:00:33","title":"Blow-up analysis of Large conformal metrics with prescribed Gaussian and geodesic curvatures","abstract":"Consider a compact Riemannian surface $(M,g)$ with nonempty boundary and negative Euler characteristic. Given two smooth non-constant functions $f$ in $M$ and $h$ in $\\partial M$ with $\\max f= \\max h= 0$, under a suitable condition on the maximum points of $f$ and $h$, we prove that for sufficiently small positive constants $\\lambda$ and $\\mu$, there exist at least two distinct conformal metrics $g_{\\lambda,\\mu}=e^{2u_{\\mu,\\lambda}}g$ and $g_{\\lambda,\\mu}=e^{2u^{\\mu,\\lambda}}g$ with prescribed sign-changing Gaussian and geodesic curvature equal to $f + \\mu$ and $h + \\lambda,$ respectively. Additionally, we employ the method used in Borer et al. (2015) to study the blowing up behavior of the large solution $u^{\\mu,\\lambda}$ when $\\mu\\downarrow 0$ and $\\lambda\\downarrow 0$. Finally, we derive a new Liouville-type result for the half-space, eliminating one of the potential blow-up profiles.","sentences":["Consider a compact Riemannian surface $(M,g)$ with nonempty boundary and negative Euler characteristic.","Given two smooth non-constant functions $f$ in $M$ and $h$ in $\\partial M$ with $\\max f= \\max h= 0$, under a suitable condition on the maximum points of $f$ and $h$, we prove that for sufficiently small positive constants $\\lambda$ and $\\mu$, there exist at least two distinct conformal metrics $g_{\\lambda,\\mu}=e^{2u_{\\mu,\\lambda}}g$ and $g_{\\lambda,\\mu}=e^{2u^{\\mu,\\lambda}}g$ with prescribed sign-changing Gaussian and geodesic curvature equal to $f + \\mu$ and $h + \\lambda,$","respectively.","Additionally, we employ the method used in Borer et al.","(2015) to study the blowing up behavior of the large solution $u^{\\mu,\\lambda}$ when $\\mu\\downarrow 0$ and $\\lambda\\downarrow 0$.","Finally, we derive a new Liouville-type result for the half-space, eliminating one of the potential blow-up profiles."],"url":"http://arxiv.org/abs/2402.02467v1","category":"math.DG"}
{"created":"2024-02-04 12:37:11","title":"Willmore-type inequality for closed hypersurfaces in complete manifolds with Ricci curvature bounded below","abstract":"In this paper, we establish a Willmore-type inequality for closed hypersurfaces in a complete Riemannian manifold of dimension $n+1$ with ${\\rm Ric}\\geq-ng$. It extends the classic result of Argostianiani, Fogagnolo, and Mazzieri in [1] to the Riemannian manifold of negative curvature. As an application, we construct a Willmore-type inequality for closed hypersurfaces in hyperbolic space and obtain the characterization of geodesic sphere.","sentences":["In this paper, we establish a Willmore-type inequality for closed hypersurfaces in a complete Riemannian manifold of dimension $n+1$ with ${\\rm Ric}\\geq-ng$. It extends the classic result of Argostianiani, Fogagnolo, and Mazzieri in [1] to the Riemannian manifold of negative curvature.","As an application, we construct a Willmore-type inequality for closed hypersurfaces in hyperbolic space and obtain the characterization of geodesic sphere."],"url":"http://arxiv.org/abs/2402.02465v1","category":"math.DG"}
{"created":"2024-02-04 11:54:07","title":"On the Role of Initialization on the Implicit Bias in Deep Linear Networks","abstract":"Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited. One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively. This study focuses on exploring this phenomenon attributed to the implicit bias at play. Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters. In this work, we focus on investigating the implicit bias originating from weight initialization. To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems. Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's performance characteristics.","sentences":["Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited.","One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively.","This study focuses on exploring this phenomenon attributed to the implicit bias at play.","Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters.","In this work, we focus on investigating the implicit bias originating from weight initialization.","To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems.","Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's performance characteristics."],"url":"http://arxiv.org/abs/2402.02454v1","category":"cs.LG"}
{"created":"2024-02-04 11:41:08","title":"On local well-posedness of 3D ideal Hall-MHD system with an azimuthal magnetic field","abstract":"In this paper, we study the local well-posedness of classical solutions to the ideal Hall-MHD equations whose magnetic field is supposed to be azimuthal in the $L^2$-based Sobolev spaces. By introducing a good unknown coupling with the original unknowns, we overcome difficulties arising from the lack of magnetic resistance, and establish a self-closed $H^m$ with $(3\\leq m\\in\\mathbb{N})$ local energy estimate of the system. Here, a key cancellation related to $\\theta$ derivatives is discovered. In order to apply this cancellation, part of the high-order energy estimates is performed in the cylindrical coordinate system, even though our solution is not assumed to be axially symmetric.   During the proof, high-order derivative tensors of unknowns in the cylindrical coordinates system are carefully calculated, which would be useful in further researches on related topics.","sentences":["In this paper, we study the local well-posedness of classical solutions to the ideal Hall-MHD equations whose magnetic field is supposed to be azimuthal in the $L^2$-based Sobolev spaces.","By introducing a good unknown coupling with the original unknowns, we overcome difficulties arising from the lack of magnetic resistance, and establish a self-closed $H^m$ with $(3\\leq m\\in\\mathbb{N})$ local energy estimate of the system.","Here, a key cancellation related to $\\theta$ derivatives is discovered.","In order to apply this cancellation, part of the high-order energy estimates is performed in the cylindrical coordinate system, even though our solution is not assumed to be axially symmetric.   ","During the proof, high-order derivative tensors of unknowns in the cylindrical coordinates system are carefully calculated, which would be useful in further researches on related topics."],"url":"http://arxiv.org/abs/2402.02451v1","category":"math.AP"}
{"created":"2024-02-04 11:19:43","title":"Fine boundary regularity for the singular fractional $p$-Laplacian","abstract":"We study the boundary weighted regularity of weak solutions $u$ to a $s$-fractional $p$-Laplacian equation in a bounded smooth domain $\\Omega$ with bounded reaction and nonlocal Dirichlet type boundary condition, in the singular case $p\\in(1,2)$ and with $s\\in(0,1)$. We prove that $u/\\ds$ has a $\\alpha$-H\\\"older continuous extension to the closure of $\\Omega$, ${\\rm d}_\\Omega(x)$ meaning the distance of $x$ from the complement of $\\Omega$. This result corresponds to that of ref. [25] for the degenerate case $p\\ge 2$.","sentences":["We study the boundary weighted regularity of weak solutions $u$ to a $s$-fractional $p$-Laplacian equation in a bounded smooth domain $\\Omega$ with bounded reaction and nonlocal Dirichlet type boundary condition, in the singular case $p\\in(1,2)$ and with $s\\in(0,1)$. We prove that $u/\\ds$ has a $\\alpha$-H\\\"older continuous extension to the closure of $\\Omega$, ${\\rm d}_\\Omega(x)$ meaning the distance of $x$ from the complement of $\\Omega$. This result corresponds to that of ref.","[25] for the degenerate case $p\\ge 2$."],"url":"http://arxiv.org/abs/2402.02448v1","category":"math.AP"}
{"created":"2024-02-04 10:43:35","title":"A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix Decomposition","abstract":"Recently, there has been a growing interest in the exploration of Nonlinear Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims to find a low-rank matrix from a sparse nonnegative matrix with a per-element nonlinear function. A typical choice is the Rectified Linear Unit (ReLU) activation function. To address over-fitting in the existing ReLU-based NMD model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for handling the ReLU-NMD-T model. A distinctive feature, setting our work apart from most existing studies, is the incorporation of both positive and negative momentum parameters in our algorithm. Our numerical experiments on real-world datasets show the effectiveness of the proposed model and algorithm. Moreover, the code is available at https://github.com/nothing2wang/NMD-TM.","sentences":["Recently, there has been a growing interest in the exploration of Nonlinear Matrix Decomposition (NMD) due to its close ties with neural networks.","NMD aims to find a low-rank matrix from a sparse nonnegative matrix with a per-element nonlinear function.","A typical choice is the Rectified Linear Unit (ReLU) activation function.","To address over-fitting in the existing ReLU-based NMD model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for handling the ReLU-NMD-T model.","A distinctive feature, setting our work apart from most existing studies, is the incorporation of both positive and negative momentum parameters in our algorithm.","Our numerical experiments on real-world datasets show the effectiveness of the proposed model and algorithm.","Moreover, the code is available at https://github.com/nothing2wang/NMD-TM."],"url":"http://arxiv.org/abs/2402.02442v1","category":"cs.LG"}
{"created":"2024-02-04 10:23:12","title":"Stability of Schur's iterates and fast solution of the discrete integrable NLS","abstract":"We prove a sharp stability estimate for Schur iterates of contractive analytic functions in the open unit disk. We then apply this result in the setting of the inverse scattering approach and obtain a fast algorithm for solving the discrete integrable nonlinear Schr\\\"odinger equation (Ablowitz-Ladik equation) on the integer lattice, $\\mathbb{Z}$. We also give a self-contained introduction to the theory of the nonlinear Fourier transform from the perspective of Schur functions and orthogonal polynomials on the unit circle.","sentences":["We prove a sharp stability estimate for Schur iterates of contractive analytic functions in the open unit disk.","We then apply this result in the setting of the inverse scattering approach and obtain a fast algorithm for solving the discrete integrable nonlinear Schr\\\"odinger equation (Ablowitz-Ladik equation) on the integer lattice, $\\mathbb{Z}$. We also give a self-contained introduction to the theory of the nonlinear Fourier transform from the perspective of Schur functions and orthogonal polynomials on the unit circle."],"url":"http://arxiv.org/abs/2402.02434v1","category":"math.SP"}
{"created":"2024-02-04 08:57:42","title":"Defining Neural Network Architecture through Polytope Structures of Dataset","abstract":"Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear. This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question. We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks. Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces.","sentences":["Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear.","This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question.","We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset.","Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks.","Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces."],"url":"http://arxiv.org/abs/2402.02407v1","category":"cs.LG"}
{"created":"2024-02-04 08:45:51","title":"Conformal vector fields on compact connected homogeneous Finsler manifolds","abstract":"Let $(M,F)$ be a compact connected homogeneous non-Riemannian Finsler manifold with $\\dim M>1$. We prove that any conformal vector field on $(M,F)$ is a Killing vector field. Further more, we prove that $\\rho F$ is a homogeneous Finsler metric on $M$ if and only if $\\rho$ is a positive constant function.","sentences":["Let $(M,F)$ be a compact connected homogeneous non-Riemannian Finsler manifold with $\\dim M>1$.","We prove that any conformal vector field on $(M,F)$ is a Killing vector field.","Further more, we prove that $\\rho F$ is a homogeneous Finsler metric on $M$ if and only if $\\rho$ is a positive constant function."],"url":"http://arxiv.org/abs/2402.02406v1","category":"math.DG"}
{"created":"2024-02-04 08:23:12","title":"Relative eta invariant and uniformly positive scalar curvature on non-compact manifolds","abstract":"On complete non-compact manifolds with bounded sectional curvature, we consider a class of self-adjoint Dirac-type operators called Dirac-Schr\\\"odinger operators. Assuming two Dirac-Schr\\\"odinger operators coincide at infinity, by previous work, one can define their relative eta invariant. A typical example of Dirac-Schr\\\"odinger operators is the (twisted) spin Dirac operators on spin manifolds which admit a Riemannian metric of uniformly positive scalar curvature. In this case, using the relative eta invariant, we get a geometric formula for the spectral flow on non-compact manifolds, which induces a new proof of Gromov-Lawson's result about compact area enlargeable manifolds in odd dimensions. When two such spin Dirac operators are the boundary restriction of an operator on a manifold with non-compact boundary, under certain conditions, we obtain an index formula involving the relative eta invariant. This generalizes the Atiyah-Patodi-Singer index theorem to non-compact boundary situation. As a result, we can use the relative eta invariant to study the space of uniformly positive scalar curvature metrics on some non-compact connected sums.","sentences":["On complete non-compact manifolds with bounded sectional curvature, we consider a class of self-adjoint Dirac-type operators called Dirac-Schr\\\"odinger operators.","Assuming two Dirac-Schr\\\"odinger operators coincide at infinity, by previous work, one can define their relative eta invariant.","A typical example of Dirac-Schr\\\"odinger operators is the (twisted) spin Dirac operators on spin manifolds which admit a Riemannian metric of uniformly positive scalar curvature.","In this case, using the relative eta invariant, we get a geometric formula for the spectral flow on non-compact manifolds, which induces a new proof of Gromov-Lawson's result about compact area enlargeable manifolds in odd dimensions.","When two such spin Dirac operators are the boundary restriction of an operator on a manifold with non-compact boundary, under certain conditions, we obtain an index formula involving the relative eta invariant.","This generalizes the Atiyah-Patodi-Singer index theorem to non-compact boundary situation.","As a result, we can use the relative eta invariant to study the space of uniformly positive scalar curvature metrics on some non-compact connected sums."],"url":"http://arxiv.org/abs/2402.02398v1","category":"math.DG"}
{"created":"2024-02-04 08:19:14","title":"Multiplexed all-optical permutation operations using a reconfigurable diffractive optical network","abstract":"Large-scale and high-dimensional permutation operations are important for various applications in e.g., telecommunications and encryption. Here, we demonstrate the use of all-optical diffractive computing to execute a set of high-dimensional permutation operations between an input and output field-of-view through layer rotations in a diffractive optical network. In this reconfigurable multiplexed material designed by deep learning, every diffractive layer has four orientations: 0, 90, 180, and 270 degrees. Each unique combination of these rotatable layers represents a distinct rotation state of the diffractive design tailored for a specific permutation operation. Therefore, a K-layer rotatable diffractive material is capable of all-optically performing up to 4^K independent permutation operations. The original input information can be decrypted by applying the specific inverse permutation matrix to output patterns, while applying other inverse operations will lead to loss of information. We demonstrated the feasibility of this reconfigurable multiplexed diffractive design by approximating 256 randomly selected permutation matrices using K=4 rotatable diffractive layers. We also experimentally validated this reconfigurable diffractive network using terahertz radiation and 3D-printed diffractive layers, providing a decent match to our numerical results. The presented rotation-multiplexed diffractive processor design is particularly useful due to its mechanical reconfigurability, offering multifunctional representation through a single fabrication process.","sentences":["Large-scale and high-dimensional permutation operations are important for various applications in e.g., telecommunications and encryption.","Here, we demonstrate the use of all-optical diffractive computing to execute a set of high-dimensional permutation operations between an input and output field-of-view through layer rotations in a diffractive optical network.","In this reconfigurable multiplexed material designed by deep learning, every diffractive layer has four orientations: 0, 90, 180, and 270 degrees.","Each unique combination of these rotatable layers represents a distinct rotation state of the diffractive design tailored for a specific permutation operation.","Therefore, a K-layer rotatable diffractive material is capable of all-optically performing up to 4^K independent permutation operations.","The original input information can be decrypted by applying the specific inverse permutation matrix to output patterns, while applying other inverse operations will lead to loss of information.","We demonstrated the feasibility of this reconfigurable multiplexed diffractive design by approximating 256 randomly selected permutation matrices using K=4 rotatable diffractive layers.","We also experimentally validated this reconfigurable diffractive network using terahertz radiation and 3D-printed diffractive layers, providing a decent match to our numerical results.","The presented rotation-multiplexed diffractive processor design is particularly useful due to its mechanical reconfigurability, offering multifunctional representation through a single fabrication process."],"url":"http://arxiv.org/abs/2402.02397v1","category":"physics.optics"}
{"created":"2024-02-04 08:16:09","title":"Frustration - no frustration crossover and phase transitions in 2D spin models with zig-zag structure","abstract":"Three 2D spin models made of frustrated zig-zag chains with competing interactions which by exact summation with respect to some degrees of freedom can be replaced by an effective temperature-dependent interaction were considered. The first model, exactly solvable Ising chains coupled by only four-spin interactions does not exhibit any finite temperature phase transition, nevertheless temperature can trigger a frustration - no frustration crossover accompanied by gigantic specific heat. A similar effect was observed in several two-leg ladder models [Yin Weiguo, arXiv:2006.08921v2 (2020), 2006.15087v1 (2020)]. The anisotropic Ising chains coupled by a direct interchain interaction and competing with it indirect interaction via spins located between chains are analyzed by using exact Onsager's equation and linear perturbation renormalization group (LPRG). Depending on the parameter set such a model exhibits one antiferromagnetic (AF) or ferromagnetic (FM) phase transition or three phase transitions with reentrant disordered phase between AF and FM ones. The LPRG method was also used to study coupled uniaxial $XXZ$ chains which, for example, can be a minimal model to describe the magnetic properties of compounds in which uranium and rare earth atoms form zig-zag chains. As with the Ising model for a certain set of parameters the model can undergo three phase transitions. However, both inchain and interchain plain interactions $s_{i,j}^x s_{k,l}^x+s_{i,j}^y s_{k,l}^y$ can eliminate the reentrant disordered phase and then only one transition takes place. Additionally, the $XXZ$ model can undergo temperature-induced metamagnetic transition.","sentences":["Three 2D spin models made of frustrated zig-zag chains with competing interactions which by exact summation with respect to some degrees of freedom can be replaced by an effective temperature-dependent interaction were considered.","The first model, exactly solvable Ising chains coupled by only four-spin interactions does not exhibit any finite temperature phase transition, nevertheless temperature can trigger a frustration - no frustration crossover accompanied by gigantic specific heat.","A similar effect was observed in several two-leg ladder models [Yin Weiguo, arXiv:2006.08921v2 (2020), 2006.15087v1 (2020)].","The anisotropic Ising chains coupled by a direct interchain interaction and competing with it indirect interaction via spins located between chains are analyzed by using exact Onsager's equation and linear perturbation renormalization group (LPRG).","Depending on the parameter set such a model exhibits one antiferromagnetic (AF) or ferromagnetic (FM) phase transition or three phase transitions with reentrant disordered phase between AF and FM ones.","The LPRG method was also used to study coupled uniaxial $XXZ$ chains which, for example, can be a minimal model to describe the magnetic properties of compounds in which uranium and rare earth atoms form zig-zag chains.","As with the Ising model for a certain set of parameters the model can undergo three phase transitions.","However, both inchain and interchain plain interactions $s_{i,j}^x s_{k,l}^x+s_{i,j}^y s_{k,l}^y$ can eliminate the reentrant disordered phase and then only one transition takes place.","Additionally, the $XXZ$ model can undergo temperature-induced metamagnetic transition."],"url":"http://arxiv.org/abs/2402.02394v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-04 07:22:01","title":"Maximizing the scientific return of Roman and Rubin with a joint wide-sky observing strategy","abstract":"This work presents the case for a single-band LSST-matched depth Roman Community Survey over the footprint of the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) Wide-Fast-Deep to enhance the key science programs of both missions. We propose to observe the ~18K sq deg LSST Wide-Fast-Deep footprint in the F146 filter to mAB~25; this will take approximately 5 months of Roman observing time. The combination of the multiwavelength nature of LSST and angular resolution of Roman would lead to enhanced scientific returns for both the Roman and LSST surveys. Galaxy deblending and crowded field photometry will be significantly improved. The extension of Rubin LSST six-band optical photometry to IR wavelengths would improve photometric redshift (photo-z) estimation, leading to improved cosmological parameter estimation, penetrate interstellar dust in the Galactic plane, improve differential chromatic refraction derived Spectral Energy Distributions, maximize galaxy-star separation and minimize crowding confusion through improved angular resolution. Conversely, the LSST survey will provide a time-domain extension of the Roman survey on the shared footprint and 6-band optical photometry with sensitivity extending all the way to ultraviolet wavelengths.","sentences":["This work presents the case for a single-band LSST-matched depth Roman Community Survey over the footprint of the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) Wide-Fast-Deep to enhance the key science programs of both missions.","We propose to observe the ~18K sq deg LSST Wide-Fast-Deep footprint in the F146 filter to mAB~25; this will take approximately 5 months of Roman observing time.","The combination of the multiwavelength nature of LSST and angular resolution of Roman would lead to enhanced scientific returns for both the Roman and LSST surveys.","Galaxy deblending and crowded field photometry will be significantly improved.","The extension of Rubin LSST six-band optical photometry to IR wavelengths would improve photometric redshift (photo-z) estimation, leading to improved cosmological parameter estimation, penetrate interstellar dust in the Galactic plane, improve differential chromatic refraction derived Spectral Energy Distributions, maximize galaxy-star separation and minimize crowding confusion through improved angular resolution.","Conversely, the LSST survey will provide a time-domain extension of the Roman survey on the shared footprint and 6-band optical photometry with sensitivity extending all the way to ultraviolet wavelengths."],"url":"http://arxiv.org/abs/2402.02378v1","category":"astro-ph.IM"}
{"created":"2024-02-04 07:19:40","title":"NOAH: Learning Pairwise Object Category Attentions for Image Classification","abstract":"A modern deep neural network (DNN) for image classification tasks typically consists of two parts: a backbone for feature extraction, and a head for feature encoding and class predication. We observe that the head structures of mainstream DNNs adopt a similar feature encoding pipeline, exploiting global feature dependencies while disregarding local ones. In this paper, we revisit the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that relies on a new form of dot-product attention called pairwise object category attention (POCA), efficiently exploiting spatially dense category-specific attentions to augment classification performance. NOAH introduces a neat combination of feature split, transform and merge operations to learn POCAs at local to global scales. As a drop-in design, NOAH can be easily used to replace existing heads of various types of DNNs, improving classification performance while maintaining similar model efficiency. We validate the effectiveness of NOAH on ImageNet classification benchmark with 25 DNN architectures spanning convolutional neural networks, vision transformers and multi-layer perceptrons. In general, NOAH is able to significantly improve the performance of lightweight DNNs, e.g., showing 3.14\\%|5.3\\%|1.9\\% top-1 accuracy improvement to MobileNetV2 (0.5x)|Deit-Tiny (0.5x)|gMLP-Tiny (0.5x). NOAH also generalizes well when applied to medium-size and large-size DNNs. We further show that NOAH retains its efficacy on other popular multi-class and multi-label image classification benchmarks as well as in different training regimes, e.g., showing 3.6\\%|1.1\\% mAP improvement to large ResNet101|ViT-Large on MS-COCO dataset. Project page: https://github.com/OSVAI/NOAH.","sentences":["A modern deep neural network (DNN) for image classification tasks typically consists of two parts: a backbone for feature extraction, and a head for feature encoding and class predication.","We observe that the head structures of mainstream DNNs adopt a similar feature encoding pipeline, exploiting global feature dependencies while disregarding local ones.","In this paper, we revisit the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that relies on a new form of dot-product attention called pairwise object category attention (POCA), efficiently exploiting spatially dense category-specific attentions to augment classification performance.","NOAH introduces a neat combination of feature split, transform and merge operations to learn POCAs at local to global scales.","As a drop-in design, NOAH can be easily used to replace existing heads of various types of DNNs, improving classification performance while maintaining similar model efficiency.","We validate the effectiveness of NOAH on ImageNet classification benchmark with 25 DNN architectures spanning convolutional neural networks, vision transformers and multi-layer perceptrons.","In general, NOAH is able to significantly improve the performance of lightweight DNNs, e.g., showing 3.14\\%|5.3\\%|1.9\\% top-1 accuracy improvement to MobileNetV2 (0.5x)|Deit-Tiny (0.5x)|gMLP-Tiny (0.5x).","NOAH also generalizes well when applied to medium-size and large-size DNNs.","We further show that NOAH retains its efficacy on other popular multi-class and multi-label image classification benchmarks as well as in different training regimes, e.g., showing 3.6\\%|1.1\\% mAP improvement to large ResNet101|ViT-Large on MS-COCO dataset.","Project page: https://github.com/OSVAI/NOAH."],"url":"http://arxiv.org/abs/2402.02377v1","category":"cs.CV"}
{"created":"2024-02-05 18:50:27","title":"Zero-shot Object-Level OOD Detection with Context-Aware Inpainting","abstract":"Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.","sentences":["Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data.","This motivates the problem of zero-shot out-of-distribution (OOD) detection.","Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects.","Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting.","RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain.","As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples.","Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings."],"url":"http://arxiv.org/abs/2402.03292v1","category":"cs.LG"}
{"created":"2024-02-05 18:17:15","title":"Meeting Bridges: Designing Information Artifacts that Bridge from Synchronous Meetings to Asynchronous Collaboration","abstract":"A recent surge in remote meetings has led to complaints of ``Zoom fatigue'' and ``collaboration overload,'' negatively impacting worker productivity and well-being. One way to alleviate the burden of meetings is to de-emphasize their synchronous participation by shifting work to and enabling sensemaking during post-meeting asynchronous activities. Towards this goal, we propose the design concept of meeting bridges, or information artifacts that can encapsulate meeting information towards bridging to and facilitating post-meeting activities. Through 13 interviews and a survey of 198 information workers, we learn how people use online meeting information after meetings are over, finding five main uses: as an archive, as task reminders, to onboard or support inclusion, for group sensemaking, and as a launching point for follow-on collaboration. However, we also find that current common meeting artifacts, such as notes and recordings, present challenges in serving as meeting bridges. After conducting co-design sessions with 16 participants, we distill key principles for the design of meeting bridges to optimally support asynchronous collaboration goals. Overall, our findings point to the opportunity of designing information artifacts that not only support users to access but also continue to transform and engage in meeting information post-meeting.","sentences":["A recent surge in remote meetings has led to complaints of ``Zoom fatigue'' and ``collaboration overload,'' negatively impacting worker productivity and well-being.","One way to alleviate the burden of meetings is to de-emphasize their synchronous participation by shifting work to and enabling sensemaking during post-meeting asynchronous activities.","Towards this goal, we propose the design concept of meeting bridges, or information artifacts that can encapsulate meeting information towards bridging to and facilitating post-meeting activities.","Through 13 interviews and a survey of 198 information workers, we learn how people use online meeting information after meetings are over, finding five main uses: as an archive, as task reminders, to onboard or support inclusion, for group sensemaking, and as a launching point for follow-on collaboration.","However, we also find that current common meeting artifacts, such as notes and recordings, present challenges in serving as meeting bridges.","After conducting co-design sessions with 16 participants, we distill key principles for the design of meeting bridges to optimally support asynchronous collaboration goals.","Overall, our findings point to the opportunity of designing information artifacts that not only support users to access but also continue to transform and engage in meeting information post-meeting."],"url":"http://arxiv.org/abs/2402.03259v1","category":"cs.HC"}
{"created":"2024-02-05 17:45:12","title":"Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications","abstract":"The paper presents the exact formula for the vector field that minimizes the loss for the standard flow. This formula depends analytically on a given distribution \\rho_0 and an unknown one \\rho_1. Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided. Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods. Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm.","sentences":["The paper presents the exact formula for the vector field that minimizes the loss for the standard flow.","This formula depends analytically on a given distribution \\rho_0 and an unknown one \\rho_1.","Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided.","Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods.","Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm."],"url":"http://arxiv.org/abs/2402.03232v1","category":"cs.LG"}
{"created":"2024-02-05 17:44:21","title":"Improved prediction of future user activity in online A/B testing","abstract":"In online randomized experiments or A/B tests, accurate predictions of participant inclusion rates are of paramount importance. These predictions not only guide experimenters in optimizing the experiment's duration but also enhance the precision of treatment effect estimates. In this paper we present a novel, straightforward, and scalable Bayesian nonparametric approach for predicting the rate at which individuals will be exposed to interventions within the realm of online A/B testing. Our approach stands out by offering dual prediction capabilities: it forecasts both the quantity of new customers expected in future time windows and, unlike available alternative methods, the number of times they will be observed. We derive closed-form expressions for the posterior distributions of the quantities needed to form predictions about future user activity, thereby bypassing the need for numerical algorithms such as Markov chain Monte Carlo. After a comprehensive exposition of our model, we test its performance on experiments on real and simulated data, where we show its superior performance with respect to existing alternatives in the literature.","sentences":["In online randomized experiments or A/B tests, accurate predictions of participant inclusion rates are of paramount importance.","These predictions not only guide experimenters in optimizing the experiment's duration but also enhance the precision of treatment effect estimates.","In this paper we present a novel, straightforward, and scalable Bayesian nonparametric approach for predicting the rate at which individuals will be exposed to interventions within the realm of online A/B testing.","Our approach stands out by offering dual prediction capabilities: it forecasts both the quantity of new customers expected in future time windows and, unlike available alternative methods, the number of times they will be observed.","We derive closed-form expressions for the posterior distributions of the quantities needed to form predictions about future user activity, thereby bypassing the need for numerical algorithms such as Markov chain Monte Carlo.","After a comprehensive exposition of our model, we test its performance on experiments on real and simulated data, where we show its superior performance with respect to existing alternatives in the literature."],"url":"http://arxiv.org/abs/2402.03231v1","category":"stat.ME"}
{"created":"2024-02-05 16:57:24","title":"Isotropy, Clusters, and Classifiers","abstract":"Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.","sentences":["Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion.","Evidence has been accrued both for and against enforcing isotropy in embedding spaces.","In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives.","We demonstrate this fact empirically and use it to shed light on previous results from the literature."],"url":"http://arxiv.org/abs/2402.03191v1","category":"cs.LG"}
{"created":"2024-02-05 16:44:17","title":"CIDAR: Culturally Relevant Instruction Dataset For Arabic","abstract":"Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at https://github.com/ARBML/CIDAR.","sentences":["Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions.","However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture.","This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region.","This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers.","CIDAR contains 10,000 instruction and output pairs that represent the Arab region.","We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets.","Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture.","All the code is available at https://github.com/ARBML/CIDAR."],"url":"http://arxiv.org/abs/2402.03177v1","category":"cs.CL"}
{"created":"2024-02-05 16:05:32","title":"Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification","abstract":"Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply.","sentences":["Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data.","Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages.","Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions.","To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset.","Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression.","Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce.","We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply."],"url":"http://arxiv.org/abs/2402.03137v1","category":"cs.CL"}
{"created":"2024-02-05 15:57:32","title":"Constrained Decoding for Cross-lingual Label Projection","abstract":"Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.","sentences":["Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data.","However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.","Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data.","However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model.","In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues.","Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies.","This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data.","We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages.","The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment."],"url":"http://arxiv.org/abs/2402.03131v1","category":"cs.CL"}
{"created":"2024-02-05 15:09:41","title":"Markov Persuasion Processes: Learning to Persuade from Scratch","abstract":"In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover, we provide a lower bound for our setting matching the guarantees of our algorithm.","sentences":["In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions.","Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially.","Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment.","The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards.","We fix such issues by addressing MPPs where the sender has no knowledge about the environment.","We design a learning algorithm for the sender, working with partial feedback.","We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning.","Moreover, we provide a lower bound for our setting matching the guarantees of our algorithm."],"url":"http://arxiv.org/abs/2402.03077v1","category":"cs.GT"}
{"created":"2024-02-05 14:00:13","title":"Taylor Videos for Action Recognition","abstract":"Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us dominant motion patterns, where static objects, small and unstable motions are removed. Experimentally we show that Taylor videos are effective inputs to popular architectures including 2D CNNs, 3D CNNs, and transformers. When used individually, Taylor videos yield competitive action recognition accuracy compared to RGB videos and optical flow. When fused with RGB or optical flow videos, further accuracy improvement is achieved.","sentences":["Effectively extracting motions from video is a critical and long-standing problem for action recognition.","This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels.","Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame.","Taylor video is named after Taylor series, which approximates a function at a given point using important terms.","In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block.","In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame.","We show the summation of the higher-order terms in the Taylor series gives us dominant motion patterns, where static objects, small and unstable motions are removed.","Experimentally we show that Taylor videos are effective inputs to popular architectures including 2D CNNs, 3D CNNs, and transformers.","When used individually, Taylor videos yield competitive action recognition accuracy compared to RGB videos and optical flow.","When fused with RGB or optical flow videos, further accuracy improvement is achieved."],"url":"http://arxiv.org/abs/2402.03019v1","category":"cs.CV"}
{"created":"2024-02-05 13:37:00","title":"Careful with that Scalpel: Improving Gradient Surgery with an EMA","abstract":"Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain this critical orthogonality property. We demonstrate that our method, Bloop, can lead to much better performances on NLP and vision experiments than other gradient surgery methods without EMA.","sentences":["Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior).","Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery.","We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss.","To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient.","In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain this critical orthogonality property.","We demonstrate that our method, Bloop, can lead to much better performances on NLP and vision experiments than other gradient surgery methods without EMA."],"url":"http://arxiv.org/abs/2402.02998v1","category":"cs.LG"}
{"created":"2024-02-05 12:42:21","title":"Mixed Noise and Posterior Estimation with Conditional DeepGEM","abstract":"Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.","sentences":["Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems.","We propose to solve the problem by an expectation maximization (EM) algorithm.","Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior.","In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas.","We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches."],"url":"http://arxiv.org/abs/2402.02964v1","category":"cs.LG"}
{"created":"2024-02-05 11:28:25","title":"Machine learning based event reconstruction for the MUonE experiment","abstract":"A proof-of-concept solution based on the machine learning techniques has been implemented and tested within the MUonE experiment designed to search for New Physics in the sector of anomalous magnetic moment of a muon. The results of the DNN based algorithm are comparable to the classical reconstruction, reducing enormously the execution time for the pattern recognition phase. The present implementation meets the conditions of classical reconstruction, providing an advantageous basis for further studies.","sentences":["A proof-of-concept solution based on the machine learning techniques has been implemented and tested within the MUonE experiment designed to search for New Physics in the sector of anomalous magnetic moment of a muon.","The results of the DNN based algorithm are comparable to the classical reconstruction, reducing enormously the execution time for the pattern recognition phase.","The present implementation meets the conditions of classical reconstruction, providing an advantageous basis for further studies."],"url":"http://arxiv.org/abs/2402.02913v1","category":"hep-ex"}
{"created":"2024-02-05 11:11:38","title":"Positive and negative sampling strategies for self-supervised learning on audio-video data","abstract":"In Self-Supervised Learning (SSL), Audio-Visual Correspondence (AVC) is a popular task to learn deep audio and video features from large unlabeled datasets. The key step in AVC is to randomly sample audio and video clips from the dataset and learn to minimize the feature distance between the positive pairs (corresponding audio-video pair) while maximizing the distance between the negative pairs (non-corresponding audio-video pairs). The learnt features are shown to be effective on various downstream tasks. However, these methods achieve subpar performance when the size of the dataset is rather small. In this paper, we investigate the effect of utilizing class label information in the AVC feature learning task. We modified various positive and negative data sampling techniques of SSL based on class label information to investigate the effect on the feature quality. We propose a new sampling approach which we call soft-positive sampling, where the positive pair for one audio sample is not from the exact corresponding video, but from a video of the same class. Experimental results suggest that when the dataset size is small in SSL setup, features learnt through the soft-positive sampling method significantly outperform those from the traditional SSL sampling approaches. This trend holds in both in-domain and out-of-domain downstream tasks, and even outperforms supervised classification. Finally, experiments show that class label information can easily be obtained using a publicly available classifier network and then can be used to boost the SSL performance without adding extra data annotation burden.","sentences":["In Self-Supervised Learning (SSL), Audio-Visual Correspondence (AVC) is a popular task to learn deep audio and video features from large unlabeled datasets.","The key step in AVC is to randomly sample audio and video clips from the dataset and learn to minimize the feature distance between the positive pairs (corresponding audio-video pair) while maximizing the distance between the negative pairs (non-corresponding audio-video pairs).","The learnt features are shown to be effective on various downstream tasks.","However, these methods achieve subpar performance when the size of the dataset is rather small.","In this paper, we investigate the effect of utilizing class label information in the AVC feature learning task.","We modified various positive and negative data sampling techniques of SSL based on class label information to investigate the effect on the feature quality.","We propose a new sampling approach which we call soft-positive sampling, where the positive pair for one audio sample is not from the exact corresponding video, but from a video of the same class.","Experimental results suggest that when the dataset size is small in SSL setup, features learnt through the soft-positive sampling method significantly outperform those from the traditional SSL sampling approaches.","This trend holds in both in-domain and out-of-domain downstream tasks, and even outperforms supervised classification.","Finally, experiments show that class label information can easily be obtained using a publicly available classifier network and then can be used to boost the SSL performance without adding extra data annotation burden."],"url":"http://arxiv.org/abs/2402.02899v1","category":"eess.AS"}
{"created":"2024-02-05 10:54:05","title":"Lossy Compression of Adjacency Matrices by Graph Filter Banks","abstract":"This paper proposes a compression framework for adjacency matrices of weighted graphs based on graph filter banks. Adjacency matrices are widely used mathematical representations of graphs and are used in various applications in signal processing, machine learning, and data mining. In many problems of interest, these adjacency matrices can be large, so efficient compression methods are crucial. In this paper, we propose a lossy compression of weighted adjacency matrices, where the binary adjacency information is encoded losslessly (so the topological information of the graph is preserved) while the edge weights are compressed lossily. For the edge weight compression, the target graph is converted into a line graph, whose nodes correspond to the edges of the original graph, and where the original edge weights are regarded as a graph signal on the line graph. We then transform the edge weights on the line graph with a graph filter bank for sparse representation. Experiments on synthetic data validate the effectiveness of the proposed method by comparing it with existing lossy matrix compression methods.","sentences":["This paper proposes a compression framework for adjacency matrices of weighted graphs based on graph filter banks.","Adjacency matrices are widely used mathematical representations of graphs and are used in various applications in signal processing, machine learning, and data mining.","In many problems of interest, these adjacency matrices can be large, so efficient compression methods are crucial.","In this paper, we propose a lossy compression of weighted adjacency matrices, where the binary adjacency information is encoded losslessly (so the topological information of the graph is preserved) while the edge weights are compressed lossily.","For the edge weight compression, the target graph is converted into a line graph, whose nodes correspond to the edges of the original graph, and where the original edge weights are regarded as a graph signal on the line graph.","We then transform the edge weights on the line graph with a graph filter bank for sparse representation.","Experiments on synthetic data validate the effectiveness of the proposed method by comparing it with existing lossy matrix compression methods."],"url":"http://arxiv.org/abs/2402.02884v1","category":"eess.SP"}
{"created":"2024-02-05 10:49:05","title":"Approximate Attributions for Off-the-Shelf Siamese Transformers","abstract":"Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\\\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically opposite adjectives, and find that they exhibit lexical bias.","sentences":["Siamese encoders such as sentence transformers are among the least understood deep models.","Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one.","To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\\\"oller et al., 2023).","However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models.","In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models.","We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects.","We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically opposite adjectives, and find that they exhibit lexical bias."],"url":"http://arxiv.org/abs/2402.02883v1","category":"cs.CL"}
{"created":"2024-02-05 10:28:20","title":"Quantum Normalizing Flows for Anomaly Detection","abstract":"A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution. Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned. In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets. Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer.","sentences":["A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution.","Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned.","In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets.","Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer."],"url":"http://arxiv.org/abs/2402.02866v1","category":"quant-ph"}
{"created":"2024-02-05 08:42:39","title":"State estimation of urban air pollution with statistical, physical, and super-learning graph models","abstract":"We consider the problem of real-time reconstruction of urban air pollution maps. The task is challenging due to the heterogeneous sources of available data, the scarcity of direct measurements, the presence of noise, and the large surfaces that need to be considered. In this work, we introduce different reconstruction methods based on posing the problem on city graphs. Our strategies can be classified as fully data-driven, physics-driven, or hybrid, and we combine them with super-learning models. The performance of the methods is tested in the case of the inner city of Paris, France.","sentences":["We consider the problem of real-time reconstruction of urban air pollution maps.","The task is challenging due to the heterogeneous sources of available data, the scarcity of direct measurements, the presence of noise, and the large surfaces that need to be considered.","In this work, we introduce different reconstruction methods based on posing the problem on city graphs.","Our strategies can be classified as fully data-driven, physics-driven, or hybrid, and we combine them with super-learning models.","The performance of the methods is tested in the case of the inner city of Paris, France."],"url":"http://arxiv.org/abs/2402.02812v1","category":"cs.LG"}
{"created":"2024-02-05 08:41:39","title":"Multi-scale fMRI time series analysis for understanding neurodegeneration in MCI","abstract":"In this study, we present a technique that spans multi-scale views (global scale -- meaning brain network-level and local scale -- examining each individual ROI that constitutes the network) applied to resting-state fMRI volumes. Deep learning based classification is utilized in understanding neurodegeneration. The novelty of the proposed approach lies in utilizing two extreme scales of analysis. One branch considers the entire network within graph-analysis framework. Concurrently, the second branch scrutinizes each ROI within a network independently, focusing on evolution of dynamics. For each subject, graph-based approach employs partial correlation to profile the subject in a single graph where each ROI is a node, providing insights into differences in levels of participation. In contrast, non-linear analysis employs recurrence plots to profile a subject as a multichannel 2D image, revealing distinctions in underlying dynamics. The proposed approach is employed for classification of a cohort of 50 healthy control (HC) and 50 Mild Cognitive Impairment (MCI), sourced from ADNI dataset. Results point to: (1) reduced activity in ROIs such as PCC in MCI (2) greater activity in occipital in MCI, which is not seen in HC (3) when analysed for dynamics, all ROIs in MCI show greater predictability in time-series.","sentences":["In this study, we present a technique that spans multi-scale views (global scale -- meaning brain network-level and local scale -- examining each individual ROI that constitutes the network) applied to resting-state fMRI volumes.","Deep learning based classification is utilized in understanding neurodegeneration.","The novelty of the proposed approach lies in utilizing two extreme scales of analysis.","One branch considers the entire network within graph-analysis framework.","Concurrently, the second branch scrutinizes each ROI within a network independently, focusing on evolution of dynamics.","For each subject, graph-based approach employs partial correlation to profile the subject in a single graph where each ROI is a node, providing insights into differences in levels of participation.","In contrast, non-linear analysis employs recurrence plots to profile a subject as a multichannel 2D image, revealing distinctions in underlying dynamics.","The proposed approach is employed for classification of a cohort of 50 healthy control (HC) and 50 Mild Cognitive Impairment (MCI), sourced from ADNI dataset.","Results point to: (1) reduced activity in ROIs such as PCC in MCI (2) greater activity in occipital in MCI, which is not seen in HC (3) when analysed for dynamics, all ROIs in MCI show greater predictability in time-series."],"url":"http://arxiv.org/abs/2402.02811v1","category":"cs.CV"}
{"created":"2024-02-05 08:18:47","title":"Extreme Two-View Geometry From Object Poses with Diffusion Models","abstract":"Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models.","sentences":["Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images.","This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching.","In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes.","In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem.","Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object.","The novel-view images are matched to determine the object pose and thus the two-view camera pose.","In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets.","Code will be available at https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models."],"url":"http://arxiv.org/abs/2402.02800v1","category":"cs.CV"}
{"created":"2024-02-05 18:38:15","title":"A note on the Winterbottom shape","abstract":"In this short note we review results on equilibrium shapes of minimizers to the sessile drop problem. More precisely, we study the Winterbottom problem and prove that the Winterbottom shape is indeed optimal. The arguments presented here are based on relaxation and the (anisotropic) isoperimetric inequality.","sentences":["In this short note we review results on equilibrium shapes of minimizers to the sessile drop problem.","More precisely, we study the Winterbottom problem and prove that the Winterbottom shape is indeed optimal.","The arguments presented here are based on relaxation and the (anisotropic) isoperimetric inequality."],"url":"http://arxiv.org/abs/2402.03281v1","category":"math.AP"}
{"created":"2024-02-05 16:49:30","title":"Stateless Quantum Structures and Extremal Graph Theory","abstract":"We study hypergraphs which represent finite quantum event structures. We contribute to results of graph theory, regarding bounds on the number of edges, given the number of vertices. We develop a missing one for 3-graphs of girth 4. As an application of the graph-theoretical approach to quantum structures, we show that the smallest orthoalgebra with an empty state space has 10 atoms. Optimized constructions of an orthomodular poset and an orthomodular lattice with no group-valued measures are given. We present also a handcrafted construction of an orthoalgebra with no group-valued measure; it is larger, but its properties can be verified without a computer","sentences":["We study hypergraphs which represent finite quantum event structures.","We contribute to results of graph theory, regarding bounds on the number of edges, given the number of vertices.","We develop a missing one for 3-graphs of girth 4.","As an application of the graph-theoretical approach to quantum structures, we show that the smallest orthoalgebra with an empty state space has 10 atoms.","Optimized constructions of an orthomodular poset and an orthomodular lattice with no group-valued measures are given.","We present also a handcrafted construction of an orthoalgebra with no group-valued measure; it is larger, but its properties can be verified without a computer"],"url":"http://arxiv.org/abs/2402.03185v1","category":"math.QA"}
{"created":"2024-02-05 16:45:56","title":"Nitrogen-polar growth of AlN on vicinal (0001) sapphire by MOVPE","abstract":"We report about metalorganic vapour phase epitaxy of smooth nitrogen-polar AlN templates on vicinal (0001) sapphire substrates. The influence of V/III ratio, growth temperature, growth rate, as well as sapphire-nitridation time and temperature were studied. With 4{\\deg} offcut sapphire, step-flow growth was possible only with V/III ratios below 2. However, optimal V/III ratio required precise adjustment, possibly dependent on reactor history and geometry. A rather narrow temperature window of less than 40{\\deg}C existed for smooth surface morphology. Reducing the temperature affected adatom mobility, eventually disrupting step-flow growth; increasing the temperature favoured the formation of high-aspect-ratio defects on the epilayer. A low thermal-budget nitridation step with a short nitridation time of 15 s proved to be effective in controlling polarity without inducing excessive surface damage on the sapphire substrate. Growth rate also influenced surface morphology, with an increase in RMS roughness and step-bouncing for faster growths; however, at growth rates of 1.4 ${\\mu}$m/h or higher step-flow growth could no longer form. Finally, we developed a V/III ratio fine-tuning procedure, whereby the reactor-specific value that induces optimal growth is inferred by growth-rate variations. With this method, N polar AlN templates with sub-nanometre RMS roughness were demonstrated for both 4{\\deg} and 2{\\deg} offcut sapphire substrates.","sentences":["We report about metalorganic vapour phase epitaxy of smooth nitrogen-polar AlN templates on vicinal (0001) sapphire substrates.","The influence of V/III ratio, growth temperature, growth rate, as well as sapphire-nitridation time and temperature were studied.","With 4{\\deg} offcut sapphire, step-flow growth was possible only with V/III ratios below 2.","However, optimal V/III ratio required precise adjustment, possibly dependent on reactor history and geometry.","A rather narrow temperature window of less than 40{\\deg}C existed for smooth surface morphology.","Reducing the temperature affected adatom mobility, eventually disrupting step-flow growth; increasing the temperature favoured the formation of high-aspect-ratio defects on the epilayer.","A low thermal-budget nitridation step with a short nitridation time of 15 s proved to be effective in controlling polarity without inducing excessive surface damage on the sapphire substrate.","Growth rate also influenced surface morphology, with an increase in RMS roughness and step-bouncing for faster growths; however, at growth rates of 1.4 ${\\mu}$m/h or higher step-flow growth could no longer form.","Finally, we developed a V/III ratio fine-tuning procedure, whereby the reactor-specific value that induces optimal growth is inferred by growth-rate variations.","With this method, N polar AlN templates with sub-nanometre RMS roughness were demonstrated for both 4{\\deg} and 2{\\deg} offcut sapphire substrates."],"url":"http://arxiv.org/abs/2402.03180v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 15:21:58","title":"Increasing TeraHertz spintronic emission with planar antennas","abstract":"Spintronic THz emitters, consisting of Ta/Co/Pt trilayers patterned into rectangles of lateral size in the 10 ${\\mu}$m range, have been integrated in planar electromagnetic antennas of various types (dipole, bow-tie, spiral). Antenna dimensions and shapes have been optimized with the help of electromagnetic simulations so as to maximize antenna efficiency in both narrow-band and broad-band geometries at/around 1 THz. The THz emission has been studied using a pump probe free space electro-optic sampling set up, both for a single emitter geometry and for arrays of emitters. Results show an increase of the detected THz signal for all antenna geometries, with enhancement ratios in the range of three to fifteen depending on antenna type and frequency range, together with changes of the emission bandwidth consistent with simulated characteristics.","sentences":["Spintronic THz emitters, consisting of Ta/Co/Pt trilayers patterned into rectangles of lateral size in the 10 ${\\mu}$m range, have been integrated in planar electromagnetic antennas of various types (dipole, bow-tie, spiral).","Antenna dimensions and shapes have been optimized with the help of electromagnetic simulations so as to maximize antenna efficiency in both narrow-band and broad-band geometries at/around 1 THz.","The THz emission has been studied using a pump probe free space electro-optic sampling set up, both for a single emitter geometry and for arrays of emitters.","Results show an increase of the detected THz signal for all antenna geometries, with enhancement ratios in the range of three to fifteen depending on antenna type and frequency range, together with changes of the emission bandwidth consistent with simulated characteristics."],"url":"http://arxiv.org/abs/2402.03089v1","category":"physics.app-ph"}
{"created":"2024-02-05 14:49:29","title":"Geometry controls diffusive target encounters and escape in tubular structures","abstract":"The endoplasmic reticulum (ER) is a network of sheet-like and tubular structures that spans much of a cell and contains molecules undergoing diffusive searches for targets, such as unfolded proteins searching for chaperones and recently-folded proteins searching for export sites. By applying a Brownian dynamics algorithm to simulate molecule diffusion, we describe how ER tube geometry influences whether a searcher will encounter a nearby target or instead diffuse away to a region near to a distinct target, as well as the timescale of successful searches. We find that targets are more likely to be found for longer and narrower tubes, and larger targets, and that search in the tube volume is more sensitive to the search geometry compared to search on the tube surface. Our results suggest ER proteins searching for low-density targets in the membrane and the lumen are very likely to encounter the nearest target before diffusing to the vicinity of another target. Our results have implications for the design of target search simulations and calculations and interpretation of molecular trajectories on the ER network, as well as other organelles with tubular geometry.","sentences":["The endoplasmic reticulum (ER) is a network of sheet-like and tubular structures that spans much of a cell and contains molecules undergoing diffusive searches for targets, such as unfolded proteins searching for chaperones and recently-folded proteins searching for export sites.","By applying a Brownian dynamics algorithm to simulate molecule diffusion, we describe how ER tube geometry influences whether a searcher will encounter a nearby target or instead diffuse away to a region near to a distinct target, as well as the timescale of successful searches.","We find that targets are more likely to be found for longer and narrower tubes, and larger targets, and that search in the tube volume is more sensitive to the search geometry compared to search on the tube surface.","Our results suggest ER proteins searching for low-density targets in the membrane and the lumen are very likely to encounter the nearest target before diffusing to the vicinity of another target.","Our results have implications for the design of target search simulations and calculations and interpretation of molecular trajectories on the ER network, as well as other organelles with tubular geometry."],"url":"http://arxiv.org/abs/2402.03059v1","category":"physics.bio-ph"}
{"created":"2024-02-05 14:25:08","title":"Demystifying Datapath Accelerator Enhanced Off-path SmartNIC","abstract":"Network speeds grow quickly in the modern cloud, so SmartNICs are introduced to offload network processing tasks, even application logic. However, typical multicore SmartNICs such as BlueFiled-2 are only capable of processing control-plane tasks with their embedded CPU that has limited memory bandwidth and computing power. On the other hand, hot cloud applications evolve, such that a limited number of fixed hardware engines in a SmartNIC cannot satisfy the requirements of cloud applications. Therefore, SmartNIC programmers call for a programmable datapath accelerator (DPA) to process network traffic at line rate. However, no existing work has unveiled the performance characteristics of the existing DPA.   To this end, we present the first architectural characterization of the latest DPA-enhanced BF3 SmartNIC. Our evaluation results indicate that BF3's DPA is much wimpier than the off-path Arm and the host CPU. However, we still identify that DPA has three unique architectural characteristics that unleash the performance potential of DPA. Specifically, we demonstrate how to take advantage of DPA's three architectural characteristics regarding computing, networking, and memory subsystems. Then we propose three important guidelines for programmers to fully unleash the potential of DPA. To demonstrate the effectiveness of our approach, we conduct detailed case studies regarding each guideline. Our case study on key-value aggregation service achieves up to 4.3$\\times$ higher throughput by using our guidelines to optimize memory combinations.","sentences":["Network speeds grow quickly in the modern cloud, so SmartNICs are introduced to offload network processing tasks, even application logic.","However, typical multicore SmartNICs such as BlueFiled-2 are only capable of processing control-plane tasks with their embedded CPU that has limited memory bandwidth and computing power.","On the other hand, hot cloud applications evolve, such that a limited number of fixed hardware engines in a SmartNIC cannot satisfy the requirements of cloud applications.","Therefore, SmartNIC programmers call for a programmable datapath accelerator (DPA) to process network traffic at line rate.","However, no existing work has unveiled the performance characteristics of the existing DPA.   ","To this end, we present the first architectural characterization of the latest DPA-enhanced BF3 SmartNIC.","Our evaluation results indicate that BF3's DPA is much wimpier than the off-path Arm and the host CPU.","However, we still identify that DPA has three unique architectural characteristics that unleash the performance potential of DPA.","Specifically, we demonstrate how to take advantage of DPA's three architectural characteristics regarding computing, networking, and memory subsystems.","Then we propose three important guidelines for programmers to fully unleash the potential of DPA.","To demonstrate the effectiveness of our approach, we conduct detailed case studies regarding each guideline.","Our case study on key-value aggregation service achieves up to 4.3$\\times$ higher throughput by using our guidelines to optimize memory combinations."],"url":"http://arxiv.org/abs/2402.03041v1","category":"cs.NI"}
{"created":"2024-02-05 14:21:06","title":"An optimality property of the Bayes-Kelly algorithm","abstract":"This note states a simple property of optimality of the Bayes-Kelly algorithm for conformal testing and poses a related open problem.","sentences":["This note states a simple property of optimality of the Bayes-Kelly algorithm for conformal testing and poses a related open problem."],"url":"http://arxiv.org/abs/2402.03035v1","category":"stat.ME"}
{"created":"2024-02-05 14:05:19","title":"Circular motion of non-collinear spin textures in Corbino disks: Dynamics of N\u00e9el- versus Bloch-type skyrmions and skyrmioniums","abstract":"Magnetic skyrmions are nano-scale magnetic whirls that can be driven by currents via spin torques. They are promising candidates for spintronic devices such as the racetrack memory, where a motion along the uniform current is typically desired. However, in Corbino disks, the goal is to achieve a circular motion, perpendicular to the radially applied current. As we show, based on analytical calculations and micromagnetic simulations, Bloch skyrmions engage in a circular motion with frequencies in the MHz range when driven by spin-orbit torques. In contrast, N\\'eel skyrmions get stuck at the edges of the disk. Our analysis reveals that the antagonistic dynamics between Bloch- and N\\'eel-type magnetic textures arise from their helicity. Furthermore, we find that skyrmioniums, which are topologically trivial variations of skyrmions, move even faster and allow an increase in the current density without being pushed toward the edges of the disk. When driven by spin-transfer torques instead, Bloch and N\\'eel skyrmions no longer exhibit different dynamics. Instead, they move along a circular trajectory due to the skyrmion Hall effect caused by their topological charge. Consequently, the topologically trivial skyrmioniums inevitably become trapped at the disk edge in this scenario. To provide a comprehensive understanding, our study also examines currents applied tangentially, further enriching our insights into skyrmion dynamics and appropriate current injection methods for skyrmion-based devices.","sentences":["Magnetic skyrmions are nano-scale magnetic whirls that can be driven by currents via spin torques.","They are promising candidates for spintronic devices such as the racetrack memory, where a motion along the uniform current is typically desired.","However, in Corbino disks, the goal is to achieve a circular motion, perpendicular to the radially applied current.","As we show, based on analytical calculations and micromagnetic simulations, Bloch skyrmions engage in a circular motion with frequencies in the MHz range when driven by spin-orbit torques.","In contrast, N\\'eel skyrmions get stuck at the edges of the disk.","Our analysis reveals that the antagonistic dynamics between Bloch- and N\\'eel-type magnetic textures arise from their helicity.","Furthermore, we find that skyrmioniums, which are topologically trivial variations of skyrmions, move even faster and allow an increase in the current density without being pushed toward the edges of the disk.","When driven by spin-transfer torques instead, Bloch and N\\'eel skyrmions no longer exhibit different dynamics.","Instead, they move along a circular trajectory due to the skyrmion Hall effect caused by their topological charge.","Consequently, the topologically trivial skyrmioniums inevitably become trapped at the disk edge in this scenario.","To provide a comprehensive understanding, our study also examines currents applied tangentially, further enriching our insights into skyrmion dynamics and appropriate current injection methods for skyrmion-based devices."],"url":"http://arxiv.org/abs/2402.03023v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 13:52:43","title":"DualBi: A dual bisection algorithm for non-convex problems with a scalar complicating constraint","abstract":"This paper addresses non-convex constrained optimization problems that are characterized by a scalar complicating constraint. We propose an iterative bisection method for the dual problem (DualBi Algorithm) that recovers a feasible primal solution with a performance that is progressively improving throughout iterations. Application to multi-agent problems with a scalar coupling constraint results in a decentralized resolution scheme where a central unit is in charge of the update of the (scalar) dual variable while agents compute their local primal variables. In the case of multi-agent MILPs, simulations showcase the performance of the proposed method compared with state-of-the-art duality-based approaches.","sentences":["This paper addresses non-convex constrained optimization problems that are characterized by a scalar complicating constraint.","We propose an iterative bisection method for the dual problem (DualBi Algorithm) that recovers a feasible primal solution with a performance that is progressively improving throughout iterations.","Application to multi-agent problems with a scalar coupling constraint results in a decentralized resolution scheme where a central unit is in charge of the update of the (scalar) dual variable while agents compute their local primal variables.","In the case of multi-agent MILPs, simulations showcase the performance of the proposed method compared with state-of-the-art duality-based approaches."],"url":"http://arxiv.org/abs/2402.03013v1","category":"math.OC"}
{"created":"2024-02-05 13:18:42","title":"Conversation Reconstruction Attack Against GPT Models","abstract":"In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous conversations, specifically the UNR attack and the PBU attack. Our experimental findings indicate that the PBU attack yields substantial performance across all models, achieving semantic similarity scores exceeding 0.60, while the UNR attack is effective solely on GPT-3.5. Our results reveal the concern about privacy risks associated with conversations involving GPT models and aim to draw the community's attention to prevent the potential misuse of these models' remarkable capabilities. We will responsibly disclose our findings to the suppliers of related large language models.","sentences":["In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models.","To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments.","These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud.","However, this operational paradigm introduces additional attack surfaces.","In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models.","Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations.","Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack.","However, GPT-4 demonstrates certain robustness to the proposed attacks.","We then introduce two advanced attacks aimed at better reconstructing previous conversations, specifically the UNR attack and the PBU attack.","Our experimental findings indicate that the PBU attack yields substantial performance across all models, achieving semantic similarity scores exceeding 0.60, while the UNR attack is effective solely on GPT-3.5.","Our results reveal the concern about privacy risks associated with conversations involving GPT models and aim to draw the community's attention to prevent the potential misuse of these models' remarkable capabilities.","We will responsibly disclose our findings to the suppliers of related large language models."],"url":"http://arxiv.org/abs/2402.02987v1","category":"cs.CR"}
{"created":"2024-02-05 11:46:14","title":"Dynamic Test Case Prioritization in Industrial Test Result Datasets","abstract":"Regression testing in software development checks if new software features affect existing ones. Regression testing is a key task in continuous development and integration, where software is built in small increments and new features are integrated as soon as possible. It is therefore important that developers are notified about possible faults quickly. In this article, we propose a test case prioritization schema that combines the use of a static and a dynamic prioritization algorithm. The dynamic prioritization algorithm rearranges the order of execution of tests on the fly, while the tests are being executed. We propose to use a conditional probability dynamic algorithm for this. We evaluate our solution on three industrial datasets and utilize Average Percentage of Fault Detection for that. The main findings are that our dynamic prioritization algorithm can: a) be applied with any static algorithm that assigns a priority score to each test case b) can improve the performance of the static algorithm if there are failure correlations between test cases c) can also reduce the performance of the static algorithm, but only when the static scheduling is performed at a near optimal level.","sentences":["Regression testing in software development checks if new software features affect existing ones.","Regression testing is a key task in continuous development and integration, where software is built in small increments and new features are integrated as soon as possible.","It is therefore important that developers are notified about possible faults quickly.","In this article, we propose a test case prioritization schema that combines the use of a static and a dynamic prioritization algorithm.","The dynamic prioritization algorithm rearranges the order of execution of tests on the fly, while the tests are being executed.","We propose to use a conditional probability dynamic algorithm for this.","We evaluate our solution on three industrial datasets and utilize Average Percentage of Fault Detection for that.","The main findings are that our dynamic prioritization algorithm can: a) be applied with any static algorithm that assigns a priority score to each test case b) can improve the performance of the static algorithm if there are failure correlations between test cases c) can also reduce the performance of the static algorithm, but only when the static scheduling is performed at a near optimal level."],"url":"http://arxiv.org/abs/2402.02925v1","category":"cs.SE"}
{"created":"2024-02-05 11:37:07","title":"Construction of Optimal Algorithms for Function Approximation in Gaussian Sobolev Spaces","abstract":"This paper studies function approximation in Gaussian Sobolev spaces over the real line and measures the error in a Gaussian-weighted $L^p$-norm. We construct two linear approximation algorithms using $n$ function evaluations that achieve the optimal or almost optimal rate of worst-case convergence in a Gaussian Sobolev space of order $\\alpha$. The first algorithm is based on scaled trigonometric interpolation and achieves the optimal rate $n^{-\\alpha}$ up to a logarithmic factor. This algorithm can be constructed in almost-linear time with the fast Fourier transform. The second algorithm is more complicated, being based on spline smoothing, but attains the optimal rate $n^{-\\alpha}$.","sentences":["This paper studies function approximation in Gaussian Sobolev spaces over the real line and measures the error in a Gaussian-weighted $L^p$-norm.","We construct two linear approximation algorithms using $n$ function evaluations that achieve the optimal or almost optimal rate of worst-case convergence in a Gaussian Sobolev space of order $\\alpha$.","The first algorithm is based on scaled trigonometric interpolation and achieves the optimal rate $n^{-\\alpha}$ up to a logarithmic factor.","This algorithm can be constructed in almost-linear time with the fast Fourier transform.","The second algorithm is more complicated, being based on spline smoothing, but attains the optimal rate $n^{-\\alpha}$."],"url":"http://arxiv.org/abs/2402.02917v1","category":"math.NA"}
{"created":"2024-02-05 07:12:26","title":"Series ridge regression for spatial data on $\\mathbb{R}^d$","abstract":"This paper develops a general asymptotic theory of series ridge estimators for spatial data observed at irregularly spaced locations in a sampling region $R_n \\subset \\mathbb{R}^d$. We adopt a stochastic sampling design that can generate irregularly spaced sampling sites in a flexible manner including both pure increasing and mixed increasing domain frameworks. Specifically, we consider a spatial trend regression model and a nonparametric regression model with spatially dependent covariates. For these models, we investigate the $L^2$-penalized series estimation of the trend and regression functions and establish (i) uniform and $L^2$ convergence rates and (ii) multivariate central limit theorems for general series estimators, (iii) optimal uniform and $L^2$ convergence rates for spline and wavelet series estimators, and (iv) show that our dependence structure conditions on the underlying spatial processes cover a wide class of random fields including L\\'evy-driven continuous autoregressive and moving average random fields.","sentences":["This paper develops a general asymptotic theory of series ridge estimators for spatial data observed at irregularly spaced locations in a sampling region $R_n \\subset","\\mathbb{R}^d$.","We adopt a stochastic sampling design that can generate irregularly spaced sampling sites in a flexible manner including both pure increasing and mixed increasing domain frameworks.","Specifically, we consider a spatial trend regression model and a nonparametric regression model with spatially dependent covariates.","For these models, we investigate the $L^2$-penalized series estimation of the trend and regression functions and establish (i) uniform and $L^2$ convergence rates and (ii) multivariate central limit theorems for general series estimators, (iii) optimal uniform and $L^2$ convergence rates for spline and wavelet series estimators, and (iv) show that our dependence structure conditions on the underlying spatial processes cover a wide class of random fields including L\\'evy-driven continuous autoregressive and moving average random fields."],"url":"http://arxiv.org/abs/2402.02773v1","category":"math.ST"}
{"created":"2024-02-05 07:12:02","title":"Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning","abstract":"Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return contrast mechanism to pull the states in generated trajectories towards high-return states while pushing them away from low-return states to improve the base distribution. Experiments on 14 commonly used D4RL benchmarks demonstrate the effectiveness of our proposed method. Our code is publicly available at https://anonymous.4open.science/r/ContrastiveDiffuser.","sentences":["Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently.","Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions.","These methods generate subsequent trajectories for planning and have demonstrated significant improvement.","However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns.","They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset.","As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution.","Even equipped with the guidance model, the performance is still suppressed.","To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return contrast mechanism to pull the states in generated trajectories towards high-return states while pushing them away from low-return states to improve the base distribution.","Experiments on 14 commonly used D4RL benchmarks demonstrate the effectiveness of our proposed method.","Our code is publicly available at https://anonymous.4open.science/r/ContrastiveDiffuser."],"url":"http://arxiv.org/abs/2402.02772v1","category":"cs.LG"}
{"created":"2024-02-05 06:00:54","title":"Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization","abstract":"There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO with standard GP not only excels in high-dimensional optimization but also proves robust in accommodating various structures within the target functions. Furthermore, with standard GP, achieving promising optimization performance is possible by only using maximum likelihood estimation, eliminating the need for expensive Markov-Chain Monte Carlo (MCMC) sampling that might be required by more complex surrogate models. We thus advocate for a re-evaluation and in-depth study of the potential of standard BO in addressing high-dimensional problems.","sentences":["There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems.","This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation.","While these concerns seem reasonable, empirical evidence supporting this belief is lacking.","In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization.","Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin.","Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions.","Without strong structural assumptions, BO with standard GP not only excels in high-dimensional optimization but also proves robust in accommodating various structures within the target functions.","Furthermore, with standard GP, achieving promising optimization performance is possible by only using maximum likelihood estimation, eliminating the need for expensive Markov-Chain Monte Carlo (MCMC) sampling that might be required by more complex surrogate models.","We thus advocate for a re-evaluation and in-depth study of the potential of standard BO in addressing high-dimensional problems."],"url":"http://arxiv.org/abs/2402.02746v1","category":"cs.LG"}
{"created":"2024-02-05 05:48:03","title":"Glocal Hypergradient Estimation with Koopman Operator","abstract":"Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending \"global\" quality with \"local\" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneously. Through numerical experiments of hyperparameter optimization, including optimization of optimizers, we demonstrate the effectiveness of the glocal hypergradient estimation.","sentences":["Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters.","Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates.","While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal.","In this paper, we propose glocal hypergradient estimation, blending \"global\" quality with \"local\" efficiency.","To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients.","Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneously.","Through numerical experiments of hyperparameter optimization, including optimization of optimizers, we demonstrate the effectiveness of the glocal hypergradient estimation."],"url":"http://arxiv.org/abs/2402.02741v1","category":"cs.LG"}
{"created":"2024-02-05 05:15:23","title":"Computing Augustin Information via Hybrid Geodesically Convex Optimization","abstract":"We propose a Riemannian gradient descent with the Poincar\\'e metric to compute the order-$\\alpha$ Augustin information, a widely used quantity for characterizing exponential error behaviors in information theory. We prove that the algorithm converges to the optimum at a rate of $\\mathcal{O}(1 / T)$. As far as we know, this is the first algorithm with a non-asymptotic optimization error guarantee for all positive orders. Numerical experimental results demonstrate the empirical efficiency of the algorithm. Our result is based on a novel hybrid analysis of Riemannian gradient descent for functions that are geodesically convex in a Riemannian metric and geodesically smooth in another.","sentences":["We propose a Riemannian gradient descent with the Poincar\\'e metric to compute the order-$\\alpha$ Augustin information, a widely used quantity for characterizing exponential error behaviors in information theory.","We prove that the algorithm converges to the optimum at a rate of $\\mathcal{O}(1 / T)$. As far as we know, this is the first algorithm with a non-asymptotic optimization error guarantee for all positive orders.","Numerical experimental results demonstrate the empirical efficiency of the algorithm.","Our result is based on a novel hybrid analysis of Riemannian gradient descent for functions that are geodesically convex in a Riemannian metric and geodesically smooth in another."],"url":"http://arxiv.org/abs/2402.02731v1","category":"cs.IT"}
{"created":"2024-02-05 04:34:48","title":"Quantum Switches for Gottesman-Kitaev-Preskill Qubit-based All-Photonic Quantum Networks","abstract":"The Gottesman-Kitaev-Preskill (GKP) code, being information theoretically near optimal for quantum communication over Gaussian thermal-loss optical channels, is likely to be the encoding of choice for advanced quantum networks of the future. Quantum repeaters based on GKP-encoded light have been shown to support high end-to-end entanglement rates across large distances despite realistic finite squeezing in GKP code preparation and homodyne detection inefficiencies. Here, we introduce a quantum switch for GKP-qubit-based quantum networks, whose architecture involves multiplexed GKP-qubit-based entanglement link generation with clients, and their all-photonic storage, together enabled by GKP-qubit graph state resources. For bipartite entanglement distribution between clients via entanglement swapping, the switch uses a multi-client generalization of a recently introduced $\\textit{entanglement-ranking-based link matching}$ protocol heuristic. Since generating the GKP-qubit graph state resource is hardware intensive, given a total resource budget and an arbitrary layout of clients, we address the question of their optimal allocation towards the different client-pair connections served by the switch such that the sum throughput of the switch is maximized while also being fair in terms of the individual entanglement rates. We illustrate our results for an exemplary data center network, where the data center is a client of a switch and all of its other clients aim to connect to the data center alone -- a scenario that also captures the general case of a gateway router connecting a local area network to a global network. Together with compatible quantum repeaters, our quantum switch provides a way to realize quantum networks of arbitrary topology.","sentences":["The Gottesman-Kitaev-Preskill (GKP) code, being information theoretically near optimal for quantum communication over Gaussian thermal-loss optical channels, is likely to be the encoding of choice for advanced quantum networks of the future.","Quantum repeaters based on GKP-encoded light have been shown to support high end-to-end entanglement rates across large distances despite realistic finite squeezing in GKP code preparation and homodyne detection inefficiencies.","Here, we introduce a quantum switch for GKP-qubit-based quantum networks, whose architecture involves multiplexed GKP-qubit-based entanglement link generation with clients, and their all-photonic storage, together enabled by GKP-qubit graph state resources.","For bipartite entanglement distribution between clients via entanglement swapping, the switch uses a multi-client generalization of a recently introduced $\\textit{entanglement-ranking-based link matching}$ protocol heuristic.","Since generating the GKP-qubit graph state resource is hardware intensive, given a total resource budget and an arbitrary layout of clients, we address the question of their optimal allocation towards the different client-pair connections served by the switch such that the sum throughput of the switch is maximized while also being fair in terms of the individual entanglement rates.","We illustrate our results for an exemplary data center network, where the data center is a client of a switch and all of its other clients aim to connect to the data center alone -- a scenario that also captures the general case of a gateway router connecting a local area network to a global network.","Together with compatible quantum repeaters, our quantum switch provides a way to realize quantum networks of arbitrary topology."],"url":"http://arxiv.org/abs/2402.02721v1","category":"quant-ph"}
{"created":"2024-02-05 04:28:38","title":"Budget-feasible Egalitarian Allocation of Conflicting Jobs","abstract":"Allocating conflicting jobs among individuals while respecting a budget constraint for each individual is an optimization problem that arises in various real-world scenarios. In this paper, we consider the situation where each individual derives some satisfaction from each job. We focus on finding a feasible allocation of conflicting jobs that maximize egalitarian cost, i.e. the satisfaction of the \\nc{individual who is worst-off}. To the best of our knowledge, this is the first paper to combine egalitarianism, budget-feasibility, and conflict-freeness in allocations. We provide a systematic study of the computational complexity of finding budget-feasible conflict-free egalitarian allocation and show that our problem generalizes a large number of classical optimization problems. Therefore, unsurprisingly, our problem is \\NPH even for two individuals and when there is no conflict between any jobs. We show that the problem admits algorithms when studied in the realm of approximation algorithms and parameterized algorithms with a host of natural parameters that match and in some cases improve upon the running time of known algorithms.","sentences":["Allocating conflicting jobs among individuals while respecting a budget constraint for each individual is an optimization problem that arises in various real-world scenarios.","In this paper, we consider the situation where each individual derives some satisfaction from each job.","We focus on finding a feasible allocation of conflicting jobs that maximize egalitarian cost, i.e. the satisfaction of the \\nc{individual who is worst-off}.","To the best of our knowledge, this is the first paper to combine egalitarianism, budget-feasibility, and conflict-freeness in allocations.","We provide a systematic study of the computational complexity of finding budget-feasible conflict-free egalitarian allocation and show that our problem generalizes a large number of classical optimization problems.","Therefore, unsurprisingly, our problem is \\NPH even for two individuals and when there is no conflict between any jobs.","We show that the problem admits algorithms when studied in the realm of approximation algorithms and parameterized algorithms with a host of natural parameters that match and in some cases improve upon the running time of known algorithms."],"url":"http://arxiv.org/abs/2402.02719v1","category":"cs.DS"}
{"created":"2024-02-05 04:24:02","title":"Table-Top Tunable Chiral Photonic Emitter","abstract":"The increasing interest in chiral light stems from its spiral trajectory along the propagation direction, facilitating the interaction between different polarization states of light and matter. Despite tremendous achievements in chiral light-related research, the generation and control of chiral pulse have presented enduring challenges, especially at the terahertz and ultraviolet spectral ranges, due to the lack of suitable optical elements for effective pulse manipulation. Conventionally, chiral light can be obtained from intricate optical systems, by an external magnetic field, or by metamaterials, which necessitate sophisticated optical configurations. Here, we propose a versatile tunable chiral emitter, composed of only two planar Weyl semimetals slabs, addressing the challenges in both spectral ranges. Our results open the way to a compact tunable chiral emitter platform in both terahertz and ultra-violet frequency ranges. This advancement holds the potential to serve as the cornerstone for integrated chiral photonics.","sentences":["The increasing interest in chiral light stems from its spiral trajectory along the propagation direction, facilitating the interaction between different polarization states of light and matter.","Despite tremendous achievements in chiral light-related research, the generation and control of chiral pulse have presented enduring challenges, especially at the terahertz and ultraviolet spectral ranges, due to the lack of suitable optical elements for effective pulse manipulation.","Conventionally, chiral light can be obtained from intricate optical systems, by an external magnetic field, or by metamaterials, which necessitate sophisticated optical configurations.","Here, we propose a versatile tunable chiral emitter, composed of only two planar Weyl semimetals slabs, addressing the challenges in both spectral ranges.","Our results open the way to a compact tunable chiral emitter platform in both terahertz and ultra-violet frequency ranges.","This advancement holds the potential to serve as the cornerstone for integrated chiral photonics."],"url":"http://arxiv.org/abs/2402.02715v1","category":"physics.optics"}
{"created":"2024-02-05 01:55:14","title":"Practical Rateless Set Reconciliation","abstract":"Set reconciliation, where two parties hold fixed-length bit strings and run a protocol to learn the strings they are missing from each other, is a fundamental task in many distributed systems. We present Rateless Invertible Bloom Lookup Tables (Rateless IBLT), the first set reconciliation protocol, to the best of our knowledge, that achieves low computation cost and near-optimal communication cost across a wide range of scenarios: set differences of one to millions, bit strings of a few bytes to megabytes, and workloads injected by potential adversaries. Rateless IBLT is based on a novel encoder that incrementally encodes the set difference into an infinite stream of coded symbols, resembling rateless error-correcting codes. We compare Rateless IBLT with state-of-the-art set reconciliation schemes and demonstrate significant improvements. Rateless IBLT achieves 3--4x lower communication cost than non-rateless schemes with similar computation cost, and 2--2000x lower computation cost than schemes with similar communication cost. We show the real-world benefits of Rateless IBLT by applying it to synchronize the state of the Ethereum blockchain, and demonstrate 5.6x lower end-to-end completion time and 4.4x lower communication cost compared to the system used in production.","sentences":["Set reconciliation, where two parties hold fixed-length bit strings and run a protocol to learn the strings they are missing from each other, is a fundamental task in many distributed systems.","We present Rateless Invertible Bloom Lookup Tables (Rateless IBLT), the first set reconciliation protocol, to the best of our knowledge, that achieves low computation cost and near-optimal communication cost across a wide range of scenarios: set differences of one to millions, bit strings of a few bytes to megabytes, and workloads injected by potential adversaries.","Rateless IBLT is based on a novel encoder that incrementally encodes the set difference into an infinite stream of coded symbols, resembling rateless error-correcting codes.","We compare Rateless IBLT with state-of-the-art set reconciliation schemes and demonstrate significant improvements.","Rateless IBLT achieves 3--4x lower communication cost than non-rateless schemes with similar computation cost, and 2--2000x lower computation cost than schemes with similar communication cost.","We show the real-world benefits of Rateless IBLT by applying it to synchronize the state of the Ethereum blockchain, and demonstrate 5.6x lower end-to-end completion time and 4.4x lower communication cost compared to the system used in production."],"url":"http://arxiv.org/abs/2402.02668v1","category":"cs.DC"}
{"created":"2024-02-05 00:52:50","title":"Learning with Mixture of Prototypes for Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at https://github.com/jeff024/PALM.","sentences":["Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world.","Distance-based OOD detection methods have emerged with enhanced deep representation learning.","They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes.","However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data.","Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance.","To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection.","Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights.","PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level.","Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data.","Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark.","Code is available at https://github.com/jeff024/PALM."],"url":"http://arxiv.org/abs/2402.02653v1","category":"cs.LG"}
{"created":"2024-02-04 21:40:23","title":"Perfect Multi-User Distributed Computing","abstract":"In this paper, we investigate the problem of multi-user linearly decomposable function computation, where $N$ servers help compute functions for $K$ users, and where each such function can be expressed as a linear combination of $L$ basis subfunctions. The process begins with each server computing some of the subfunctions, then broadcasting a linear combination of its computed outputs to a selected group of users, and finally having each user linearly combine its received data to recover its function. As it has become recently known, this problem can be translated into a matrix decomposition problem $\\mathbf{F}=\\mathbf{D}\\mathbf{E}$, where $\\mathbf{F} \\in \\mathbf{GF}(q)^{K \\times L}$ describes the coefficients that define the users' demands, where $\\mathbf{E} \\in \\mathbf{GF}(q)^{N \\times L}$ describes which subfunction each server computes and how it combines the computed outputs, and where $\\mathbf{D} \\in \\mathbf{GF}(q)^{K \\times N}$ describes which servers each user receives data from and how it combines this data. Our interest here is in reducing the total number of subfunction computations across the servers (cumulative computational cost), as well as the worst-case load which can be a measure of computational delay. Our contribution consists of novel bounds on the two computing costs, where these bounds are linked here to the covering and packing radius of classical codes. One of our findings is that in certain cases, our distributed computing problem -- and by extension our matrix decomposition problem -- is treated optimally when $\\mathbf{F}$ is decomposed into a parity check matrix $\\mathbf{D}$ of a perfect code, and a matrix $\\mathbf{E}$ which has as columns the coset leaders of this same code.","sentences":["In this paper, we investigate the problem of multi-user linearly decomposable function computation, where $N$ servers help compute functions for $K$ users, and where each such function can be expressed as a linear combination of $L$ basis subfunctions.","The process begins with each server computing some of the subfunctions, then broadcasting a linear combination of its computed outputs to a selected group of users, and finally having each user linearly combine its received data to recover its function.","As it has become recently known, this problem can be translated into a matrix decomposition problem $\\mathbf{F}=\\mathbf{D}\\mathbf{E}$, where $\\mathbf{F} \\in \\mathbf{GF}(q)^{K \\times L}$ describes the coefficients that define the users' demands, where $\\mathbf{E} \\in \\mathbf{GF}(q)^{N \\times L}$ describes which subfunction each server computes and how it combines the computed outputs, and where $\\mathbf{D} \\in \\mathbf{GF}(q)^{K \\times N}$ describes which servers each user receives data from and how it combines this data.","Our interest here is in reducing the total number of subfunction computations across the servers (cumulative computational cost), as well as the worst-case load which can be a measure of computational delay.","Our contribution consists of novel bounds on the two computing costs, where these bounds are linked here to the covering and packing radius of classical codes.","One of our findings is that in certain cases, our distributed computing problem -- and by extension our matrix decomposition problem -- is treated optimally when $\\mathbf{F}$ is decomposed into a parity check matrix $\\mathbf{D}$ of a perfect code, and a matrix $\\mathbf{E}$ which has as columns the coset leaders of this same code."],"url":"http://arxiv.org/abs/2402.02621v1","category":"cs.IT"}
{"created":"2024-02-04 21:24:54","title":"Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study on Speech Emotion Recognition","abstract":"The efficacy of self-supervised speech models has been validated, yet the optimal utilization of their representations remains challenging across diverse tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a fixed-length feature derived from continuous representations, to explore their advantages in specific tasks. AWEs have previously shown utility in capturing acoustic discriminability. In light of this, we propose measuring layer-wise similarity between AWEs and word embeddings, aiming to further investigate the inherent context within AWEs. Moreover, we evaluate the contribution of AWEs, in comparison to other types of speech features, in the context of Speech Emotion Recognition (SER). Through a comparative experiment and a layer-wise accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore differences between AWEs and raw self-supervised representations, as well as the proper utilization of AWEs alone and in combination with word embeddings. Our findings underscore the acoustic context conveyed by AWEs and showcase the highly competitive SER accuracies by appropriately employing AWEs.","sentences":["The efficacy of self-supervised speech models has been validated, yet the optimal utilization of their representations remains challenging across diverse tasks.","In this study, we delve into Acoustic Word Embeddings (AWEs), a fixed-length feature derived from continuous representations, to explore their advantages in specific tasks.","AWEs have previously shown utility in capturing acoustic discriminability.","In light of this, we propose measuring layer-wise similarity between AWEs and word embeddings, aiming to further investigate the inherent context within AWEs.","Moreover, we evaluate the contribution of AWEs, in comparison to other types of speech features, in the context of Speech Emotion Recognition (SER).","Through a comparative experiment and a layer-wise accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore differences between AWEs and raw self-supervised representations, as well as the proper utilization of AWEs alone and in combination with word embeddings.","Our findings underscore the acoustic context conveyed by AWEs and showcase the highly competitive SER accuracies by appropriately employing AWEs."],"url":"http://arxiv.org/abs/2402.02617v1","category":"cs.CL"}
{"created":"2024-02-04 20:56:39","title":"Fast Explicit-Input Assistance for Teleoperation in Clutter","abstract":"The performance of prediction-based assistance for robot teleoperation degrades in unseen or goal-rich environments due to incorrect or quickly-changing intent inferences. Poor predictions can confuse operators or cause them to change their control input to implicitly signal their goal, resulting in unnatural movement. We present a new assistance algorithm and interface for robotic manipulation where an operator can explicitly communicate a manipulation goal by pointing the end-effector. Rapid optimization and parallel collision checking in a local region around the pointing target enable direct, interactive control over grasp and place pose candidates. We compare the explicit pointing interface to an implicit inference-based assistance scheme in a within-subjects user study (N=20) where participants teleoperate a simulated robot to complete a multi-step singulation and stacking task in cluttered environments. We find that operators prefer the explicit interface, which improved completion time, pick and place success rates, and NASA TLX scores. Our code is available at https://github.com/NVlabs/fast-explicit-teleop","sentences":["The performance of prediction-based assistance for robot teleoperation degrades in unseen or goal-rich environments due to incorrect or quickly-changing intent inferences.","Poor predictions can confuse operators or cause them to change their control input to implicitly signal their goal, resulting in unnatural movement.","We present a new assistance algorithm and interface for robotic manipulation where an operator can explicitly communicate a manipulation goal by pointing the end-effector.","Rapid optimization and parallel collision checking in a local region around the pointing target enable direct, interactive control over grasp and place pose candidates.","We compare the explicit pointing interface to an implicit inference-based assistance scheme in a within-subjects user study (N=20) where participants teleoperate a simulated robot to complete a multi-step singulation and stacking task in cluttered environments.","We find that operators prefer the explicit interface, which improved completion time, pick and place success rates, and NASA TLX scores.","Our code is available at https://github.com/NVlabs/fast-explicit-teleop"],"url":"http://arxiv.org/abs/2402.02612v1","category":"cs.RO"}
{"created":"2024-02-04 20:21:09","title":"Modelling and cooling power control of a TES-backed-up vapour-compression refrigeration system","abstract":"This work addresses the modelling, power control, and optimization of a thermal energy storage (TES) system combined with a vapour-compression refrigeration facility based on phase change materials (PCM). Given a novel design of a PCM-based TES tank and its interconnection with an existing refrigeration system, the joint dynamic modelling is first studied, exploring the different time scales that coexist at the interconnected system. Diverse operating modes are defined, according to the intended use of the TES tank as a cold-energy buffer to decouple cooling demand and production, whereas the static characteristic and power limits are calculated and show the high coupling between the main cooling powers involved (TES charging/discharging power, and direct power production at the evaporator). In this light, a decoupling control strategy is proposed, where the low-level controllers are simply PI regulators and the refrigerant/secondary mass flows are considered as virtual manipulated variables, applying a feedforward-based cascade strategy. The control performance is evaluated through a thorough simulation that includes all operating modes, where the reference tracking is shown to be fast and reliable enough to address high-level scheduling strategies, where the references on the main cooling powers are intended to be imposed considering economic and efficiency criteria.","sentences":["This work addresses the modelling, power control, and optimization of a thermal energy storage (TES) system combined with a vapour-compression refrigeration facility based on phase change materials (PCM).","Given a novel design of a PCM-based TES tank and its interconnection with an existing refrigeration system, the joint dynamic modelling is first studied, exploring the different time scales that coexist at the interconnected system.","Diverse operating modes are defined, according to the intended use of the TES tank as a cold-energy buffer to decouple cooling demand and production, whereas the static characteristic and power limits are calculated and show the high coupling between the main cooling powers involved (TES charging/discharging power, and direct power production at the evaporator).","In this light, a decoupling control strategy is proposed, where the low-level controllers are simply PI regulators and the refrigerant/secondary mass flows are considered as virtual manipulated variables, applying a feedforward-based cascade strategy.","The control performance is evaluated through a thorough simulation that includes all operating modes, where the reference tracking is shown to be fast and reliable enough to address high-level scheduling strategies, where the references on the main cooling powers are intended to be imposed considering economic and efficiency criteria."],"url":"http://arxiv.org/abs/2402.02599v1","category":"eess.SY"}
{"created":"2024-02-04 15:44:37","title":"Characterization of Visible Range Gain in Praseodymium Doped Fiber Amplifier","abstract":"In the optical (380-700 nm) region, a simulation study was conducted to assess the low-signal gain, power conversion efficiency (PCE), and optical output power of a praseodymium-doped fiber optic amplifier (PDFA). The PDFA performance was assessed using the optimized Pr3+ fiber length, Pr3+ ion concentration, and pump power. Additionally, the effects of input signal voltage and amplifier gain on amplified spontaneous emission (ASE) were investigated. A lower peak signal of roughly 1 dB at 635 nm was obtained with a shorter 5 m Pr3 + doped fiber and a higher pump power of 300 mW. The impact of wavelength and pump power variations on the amplifier's optical output power's spontaneous enhanced emission (EEM) is examined. Finally, the ionic interaction (general-purpose conversion effect) in the low-power amplifier signal is analyzed by taking into account various values of the upconversion factor.","sentences":["In the optical (380-700 nm) region, a simulation study was conducted to assess the low-signal gain, power conversion efficiency (PCE), and optical output power of a praseodymium-doped fiber optic amplifier (PDFA).","The PDFA performance was assessed using the optimized Pr3+ fiber length, Pr3+ ion concentration, and pump power.","Additionally, the effects of input signal voltage and amplifier gain on amplified spontaneous emission (ASE) were investigated.","A lower peak signal of roughly 1 dB at 635 nm was obtained with a shorter 5 m Pr3 + doped fiber and a higher pump power of 300 mW. The impact of wavelength and pump power variations on the amplifier's optical output power's spontaneous enhanced emission (EEM) is examined.","Finally, the ionic interaction (general-purpose conversion effect) in the low-power amplifier signal is analyzed by taking into account various values of the upconversion factor."],"url":"http://arxiv.org/abs/2402.02542v1","category":"physics.optics"}
{"created":"2024-02-04 14:42:13","title":"Device Scheduling and Assignment in Hierarchical Federated Learning for Internet of Things","abstract":"Federated Learning (FL) is a promising machine learning approach for Internet of Things (IoT), but it has to address network congestion problems when the population of IoT devices grows. Hierarchical FL (HFL) alleviates this issue by distributing model aggregation to multiple edge servers. Nevertheless, the challenge of communication overhead remains, especially in scenarios where all IoT devices simultaneously join the training process. For scalability, practical HFL schemes select a subset of IoT devices to participate in the training, hence the notion of device scheduling. In this setting, only selected IoT devices are scheduled to participate in the global training, with each of them being assigned to one edge server. Existing HFL assignment methods are primarily based on search mechanisms, which suffer from high latency in finding the optimal assignment. This paper proposes an improved K-Center algorithm for device scheduling and introduces a deep reinforcement learning-based approach for assigning IoT devices to edge servers. Experiments show that scheduling 50% of IoT devices is generally adequate for achieving convergence in HFL with much lower time delay and energy consumption. In cases where reduction in energy consumption (such as in Green AI) and reduction of messages (to avoid burst traffic) are key objectives, scheduling 30% IoT devices allows a substantial reduction in energy and messages with similar model accuracy.","sentences":["Federated Learning (FL) is a promising machine learning approach for Internet of Things (IoT), but it has to address network congestion problems when the population of IoT devices grows.","Hierarchical FL (HFL) alleviates this issue by distributing model aggregation to multiple edge servers.","Nevertheless, the challenge of communication overhead remains, especially in scenarios where all IoT devices simultaneously join the training process.","For scalability, practical HFL schemes select a subset of IoT devices to participate in the training, hence the notion of device scheduling.","In this setting, only selected IoT devices are scheduled to participate in the global training, with each of them being assigned to one edge server.","Existing HFL assignment methods are primarily based on search mechanisms, which suffer from high latency in finding the optimal assignment.","This paper proposes an improved K-Center algorithm for device scheduling and introduces a deep reinforcement learning-based approach for assigning IoT devices to edge servers.","Experiments show that scheduling 50% of IoT devices is generally adequate for achieving convergence in HFL with much lower time delay and energy consumption.","In cases where reduction in energy consumption (such as in Green AI) and reduction of messages (to avoid burst traffic) are key objectives, scheduling 30% IoT devices allows a substantial reduction in energy and messages with similar model accuracy."],"url":"http://arxiv.org/abs/2402.02506v1","category":"cs.DC"}
{"created":"2024-02-04 14:18:20","title":"Robot Trajectron: Trajectory Prediction-based Shared Control for Robot Manipulation","abstract":"We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion's onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, easing the cognitive load of the operator by assisting them in their anticipated direction of motion. Our novel intent estimator, dubbed the \\emph{Robot Trajectron} (RT), produces a probabilistic representation of the robot's anticipated trajectory based on its recent position, velocity and acceleration history. Taking arm dynamics into account allows RT to capture the operator's intent better than other SOTA models that only use the arm's position, making it particularly well-suited to assist in tasks where the operator's intent is susceptible to change. We derive a novel shared-control solution that combines RT's predictive capacity to a representation of the locations of potential reaching targets. Our experiments demonstrate RT's effectiveness in both intent estimation and shared-control tasks. We will make the code and data supporting our experiments publicly available at https://github.com/mousecpn/Robot-Trajectron.git.","sentences":["We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion's onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, easing the cognitive load of the operator by assisting them in their anticipated direction of motion.","Our novel intent estimator, dubbed the \\emph{Robot Trajectron} (RT), produces a probabilistic representation of the robot's anticipated trajectory based on its recent position, velocity and acceleration history.","Taking arm dynamics into account allows RT to capture the operator's intent better than other SOTA models that only use the arm's position, making it particularly well-suited to assist in tasks where the operator's intent is susceptible to change.","We derive a novel shared-control solution that combines RT's predictive capacity to a representation of the locations of potential reaching targets.","Our experiments demonstrate RT's effectiveness in both intent estimation and shared-control tasks.","We will make the code and data supporting our experiments publicly available at https://github.com/mousecpn/Robot-Trajectron.git."],"url":"http://arxiv.org/abs/2402.02499v1","category":"cs.RO"}
{"created":"2024-02-04 13:33:56","title":"Decentralized Finite-Sum Optimization over Time-Varying Networks","abstract":"We consider decentralized time-varying stochastic optimization problems where each of the functions held by the nodes has a finite sum structure. Such problems can be efficiently solved using variance reduction techniques. Our aim is to explore the lower complexity bounds (for communication and number of stochastic oracle calls) and find optimal algorithms. The paper studies strongly convex and nonconvex scenarios. To the best of our knowledge, variance reduced schemes and lower bounds for time-varying graphs have not been studied in the literature. For nonconvex objectives, we obtain lower bounds and develop an optimal method GT-PAGE. For strongly convex objectives, we propose the first decentralized time-varying variance-reduction method ADOM+VR and establish lower bound in this scenario, highlighting the open question of matching the algorithms complexity and lower bounds even in static network case.","sentences":["We consider decentralized time-varying stochastic optimization problems where each of the functions held by the nodes has a finite sum structure.","Such problems can be efficiently solved using variance reduction techniques.","Our aim is to explore the lower complexity bounds (for communication and number of stochastic oracle calls) and find optimal algorithms.","The paper studies strongly convex and nonconvex scenarios.","To the best of our knowledge, variance reduced schemes and lower bounds for time-varying graphs have not been studied in the literature.","For nonconvex objectives, we obtain lower bounds and develop an optimal method GT-PAGE.","For strongly convex objectives, we propose the first decentralized time-varying variance-reduction method ADOM+VR and establish lower bound in this scenario, highlighting the open question of matching the algorithms complexity and lower bounds even in static network case."],"url":"http://arxiv.org/abs/2402.02490v1","category":"math.OC"}
{"created":"2024-02-04 12:23:04","title":"Zeroth-order Median Clipping for Non-Smooth Convex Optimization Problems with Heavy-tailed Symmetric Noise","abstract":"In this paper, we consider non-smooth convex optimization with a zeroth-order oracle corrupted by symmetric stochastic noise. Unlike the existing high-probability results requiring the noise to have bounded $\\kappa$-th moment with $\\kappa \\in (1,2]$, our results allow even heavier noise with any $\\kappa > 0$, e.g., the noise distribution can have unbounded $1$-st moment. Moreover, our results match the best-known ones for the case of the bounded variance. To achieve this, we use the mini-batched median estimate of the sampled gradient differences, apply gradient clipping to the result, and plug in the final estimate into the accelerated method. We apply this technique to the stochastic multi-armed bandit problem with heavy-tailed distribution of rewards and achieve $O(\\sqrt{dT})$ regret by incorporating the additional assumption of noise symmetry.","sentences":["In this paper, we consider non-smooth convex optimization with a zeroth-order oracle corrupted by symmetric stochastic noise.","Unlike the existing high-probability results requiring the noise to have bounded $\\kappa$-th moment with $\\kappa \\in (1,2]$, our results allow even heavier noise with any $\\kappa > 0$, e.g., the noise distribution can have unbounded $1$-st moment.","Moreover, our results match the best-known ones for the case of the bounded variance.","To achieve this, we use the mini-batched median estimate of the sampled gradient differences, apply gradient clipping to the result, and plug in the final estimate into the accelerated method.","We apply this technique to the stochastic multi-armed bandit problem with heavy-tailed distribution of rewards and achieve $O(\\sqrt{dT})$ regret by incorporating the additional assumption of noise symmetry."],"url":"http://arxiv.org/abs/2402.02461v1","category":"math.OC"}
{"created":"2024-02-04 11:12:17","title":"Breaking MLPerf Training: A Case Study on Optimizing BERT","abstract":"Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization as well as the fast training of gradient clipping before allreduce. We also re-evaluate existing optimizers via hyperparameter optimization and utilize ADAM, which also contributes to fast training via larger batches than existing methods. Our proposed methods, all combined, give the fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100 GPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one) submissions to MLPerf v1.1 (v2.0). Our implementation and evaluation results are available at MLPerf v1.1~v2.1.","sentences":["Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc.","We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance.","Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths.","Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation.","In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training.","We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization as well as the fast training of gradient clipping before allreduce.","We also re-evaluate existing optimizers via hyperparameter optimization and utilize ADAM, which also contributes to fast training via larger batches than existing methods.","Our proposed methods, all combined, give the fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100 GPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one) submissions to MLPerf v1.1 (v2.0).","Our implementation and evaluation results are available at MLPerf v1.1~v2.1."],"url":"http://arxiv.org/abs/2402.02447v1","category":"cs.LG"}
{"created":"2024-02-04 10:52:43","title":"BECLR: Batch Enhanced Contrastive Few-Shot Learning","abstract":"Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios where FSL approaches suffer the most from sample bias. We later on discuss that DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we coin as BECLR), constructively magnifying each other's impact. We then present a suite of extensive quantitative and qualitative experimentation to corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL benchmarks (to the best of our knowledge), and significantly outperforms the best of the current baselines (codebase available at: https://github.com/stypoumic/BECLR).","sentences":["Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning.","Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time.","Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages.","We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning.","We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage.","We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios where FSL approaches suffer the most from sample bias.","We later on discuss that DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we coin as BECLR), constructively magnifying each other's impact.","We then present a suite of extensive quantitative and qualitative experimentation to corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL benchmarks (to the best of our knowledge), and significantly outperforms the best of the current baselines (codebase available at: https://github.com/stypoumic/BECLR)."],"url":"http://arxiv.org/abs/2402.02444v1","category":"cs.CV"}
{"created":"2024-02-04 09:51:19","title":"Hybrid-Prediction Integrated Planning for Autonomous Driving","abstract":"Autonomous driving systems require the ability to fully understand and predict the surrounding environment to make informed decisions in complex scenarios. Recent advancements in learning-based systems have highlighted the importance of integrating prediction and planning modules. However, this integration has brought forth three major challenges: inherent trade-offs by sole prediction, consistency between prediction patterns, and social coherence in prediction and planning. To address these challenges, we introduce a hybrid-prediction integrated planning (HPP) system, which possesses three novelly designed modules. First, we introduce marginal-conditioned occupancy prediction to align joint occupancy with agent-wise perceptions. Our proposed MS-OccFormer module achieves multi-stage alignment per occupancy forecasting with consistent awareness from agent-wise motion predictions. Second, we propose a game-theoretic motion predictor, GTFormer, to model the interactive future among individual agents with their joint predictive awareness. Third, hybrid prediction patterns are concurrently integrated with Ego Planner and optimized by prediction guidance. HPP achieves state-of-the-art performance on the nuScenes dataset, demonstrating superior accuracy and consistency for end-to-end paradigms in prediction and planning. Moreover, we test the long-term open-loop and closed-loop performance of HPP on the Waymo Open Motion Dataset and CARLA benchmark, surpassing other integrated prediction and planning pipelines with enhanced accuracy and compatibility.","sentences":["Autonomous driving systems require the ability to fully understand and predict the surrounding environment to make informed decisions in complex scenarios.","Recent advancements in learning-based systems have highlighted the importance of integrating prediction and planning modules.","However, this integration has brought forth three major challenges: inherent trade-offs by sole prediction, consistency between prediction patterns, and social coherence in prediction and planning.","To address these challenges, we introduce a hybrid-prediction integrated planning (HPP) system, which possesses three novelly designed modules.","First, we introduce marginal-conditioned occupancy prediction to align joint occupancy with agent-wise perceptions.","Our proposed MS-OccFormer module achieves multi-stage alignment per occupancy forecasting with consistent awareness from agent-wise motion predictions.","Second, we propose a game-theoretic motion predictor, GTFormer, to model the interactive future among individual agents with their joint predictive awareness.","Third, hybrid prediction patterns are concurrently integrated with Ego Planner and optimized by prediction guidance.","HPP achieves state-of-the-art performance on the nuScenes dataset, demonstrating superior accuracy and consistency for end-to-end paradigms in prediction and planning.","Moreover, we test the long-term open-loop and closed-loop performance of HPP on the Waymo Open Motion Dataset and CARLA benchmark, surpassing other integrated prediction and planning pipelines with enhanced accuracy and compatibility."],"url":"http://arxiv.org/abs/2402.02426v1","category":"cs.RO"}
{"created":"2024-02-04 09:22:02","title":"A narrow-band parameterization for the stochastic gravitational wave background","abstract":"In light of the non-perturbative resonance effects that may occur during inflation, we introduce a parametrization for the power spectrum of the stochastic gravitational wave background (SGWB) characterized by narrow-band amplification. We utilize the universal $\\Omega_\\text{GW}\\propto k^3$ infrared limit, applicable to a wide array of gravitational wave sources, to devise a robust yet straightforward parameterization optimized for Markov Chain Monte Carlo (MCMC) analyses. This parameterization is demonstrated through select examples where its application is pertinent, and we discuss the advantages of this approach over traditional parametrizations for narrow-band scenarios. To evaluate the sensitivity of our proposed model parameters, we apply a mock likelihood based on the CMB-Stage4 data. Furthermore, we explicate the computational process for the mapping relationship between the foundational model parameters and our parameterized framework, using a two-field inflation model that resonantly amplifies gravitational waves (GWs) as an example.","sentences":["In light of the non-perturbative resonance effects that may occur during inflation, we introduce a parametrization for the power spectrum of the stochastic gravitational wave background (SGWB) characterized by narrow-band amplification.","We utilize the universal $\\Omega_\\text{GW}\\propto k^3$ infrared limit, applicable to a wide array of gravitational wave sources, to devise a robust yet straightforward parameterization optimized for Markov Chain Monte Carlo (MCMC) analyses.","This parameterization is demonstrated through select examples where its application is pertinent, and we discuss the advantages of this approach over traditional parametrizations for narrow-band scenarios.","To evaluate the sensitivity of our proposed model parameters, we apply a mock likelihood based on the CMB-Stage4 data.","Furthermore, we explicate the computational process for the mapping relationship between the foundational model parameters and our parameterized framework, using a two-field inflation model that resonantly amplifies gravitational waves (GWs) as an example."],"url":"http://arxiv.org/abs/2402.02415v1","category":"astro-ph.CO"}
{"created":"2024-02-04 09:07:28","title":"Physics-Inspired Degradation Models for Hyperspectral Image Fusion","abstract":"The fusion of a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) has garnered increasing research interest. However, most fusion methods solely focus on the fusion algorithm itself and overlook the degradation models, which results in unsatisfactory performance in practical scenarios. To fill this gap, we propose physics-inspired degradation models (PIDM) to model the degradation of LR-HSI and HR-MSI, which comprises a spatial degradation network (SpaDN) and a spectral degradation network (SpeDN). SpaDN and SpeDN are designed based on two insights. First, we employ spatial warping and spectral modulation operations to simulate lens aberrations, thereby introducing non-uniformity into the spatial and spectral degradation processes. Second, we utilize asymmetric downsampling and parallel downsampling operations to separately reduce the spatial and spectral resolutions of the images, thus ensuring the matching of spatial and spectral degradation processes with specific physical characteristics. Once SpaDN and SpeDN are established, we adopt a self-supervised training strategy to optimize the network parameters and provide a plug-and-play solution for fusion methods. Comprehensive experiments demonstrate that our proposed PIDM can boost the fusion performance of existing fusion methods in practical scenarios.","sentences":["The fusion of a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) has garnered increasing research interest.","However, most fusion methods solely focus on the fusion algorithm itself and overlook the degradation models, which results in unsatisfactory performance in practical scenarios.","To fill this gap, we propose physics-inspired degradation models (PIDM) to model the degradation of LR-HSI and HR-MSI, which comprises a spatial degradation network (SpaDN) and a spectral degradation network (SpeDN).","SpaDN and SpeDN are designed based on two insights.","First, we employ spatial warping and spectral modulation operations to simulate lens aberrations, thereby introducing non-uniformity into the spatial and spectral degradation processes.","Second, we utilize asymmetric downsampling and parallel downsampling operations to separately reduce the spatial and spectral resolutions of the images, thus ensuring the matching of spatial and spectral degradation processes with specific physical characteristics.","Once SpaDN and SpeDN are established, we adopt a self-supervised training strategy to optimize the network parameters and provide a plug-and-play solution for fusion methods.","Comprehensive experiments demonstrate that our proposed PIDM can boost the fusion performance of existing fusion methods in practical scenarios."],"url":"http://arxiv.org/abs/2402.02411v1","category":"cs.CV"}
{"created":"2024-02-04 09:01:11","title":"Block-Sparse Tensor Recovery","abstract":"This work explores the fundamental problem of the recoverability of a sparse tensor being reconstructed from its compressed embodiment. We present a generalized model of block-sparse tensor recovery as a theoretical foundation, where concepts measuring holistic mutual incoherence property (MIP) of the measurement matrix set are defined. A representative algorithm based on the orthogonal matching pursuit (OMP) framework, called tensor generalized block OMP (T-GBOMP), is applied to the theoretical framework elaborated for analyzing both noiseless and noisy recovery conditions. Specifically, we present the exact recovery condition (ERC) and sufficient conditions for establishing it with consideration of different degrees of restriction. Reliable reconstruction conditions, in terms of the residual convergence, the estimated error and the signal-to-noise ratio bound, are established to reveal the computable theoretical interpretability based on the newly defined MIP, which we introduce. The flexibility of tensor recovery is highlighted, i.e., the reliable recovery can be guaranteed by optimizing MIP of the measurement matrix set. Analytical comparisons demonstrate that the theoretical results developed are tighter and less restrictive than the existing ones (if any). Further discussions provide tensor extensions for several classic greedy algorithms, indicating that the sophisticated results derived are universal and applicable to all these tensorized variants.","sentences":["This work explores the fundamental problem of the recoverability of a sparse tensor being reconstructed from its compressed embodiment.","We present a generalized model of block-sparse tensor recovery as a theoretical foundation, where concepts measuring holistic mutual incoherence property (MIP) of the measurement matrix set are defined.","A representative algorithm based on the orthogonal matching pursuit (OMP) framework, called tensor generalized block OMP (T-GBOMP), is applied to the theoretical framework elaborated for analyzing both noiseless and noisy recovery conditions.","Specifically, we present the exact recovery condition (ERC) and sufficient conditions for establishing it with consideration of different degrees of restriction.","Reliable reconstruction conditions, in terms of the residual convergence, the estimated error and the signal-to-noise ratio bound, are established to reveal the computable theoretical interpretability based on the newly defined MIP, which we introduce.","The flexibility of tensor recovery is highlighted, i.e., the reliable recovery can be guaranteed by optimizing MIP of the measurement matrix set.","Analytical comparisons demonstrate that the theoretical results developed are tighter and less restrictive than the existing ones (if any).","Further discussions provide tensor extensions for several classic greedy algorithms, indicating that the sophisticated results derived are universal and applicable to all these tensorized variants."],"url":"http://arxiv.org/abs/2402.02410v1","category":"eess.SP"}
{"created":"2024-02-04 08:57:54","title":"GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model","abstract":"Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt optimization yields effective prompts comparable to accuracy-based ones. The code is publicly available at https://github.com/thunderous77/GLaPE.","sentences":["Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design.","Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy.","However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality.","To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels.","Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score.","Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent.","Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels.","Moreover, on six popular reasoning tasks, our GLaPE-based prompt optimization yields effective prompts comparable to accuracy-based ones.","The code is publicly available at https://github.com/thunderous77/GLaPE."],"url":"http://arxiv.org/abs/2402.02408v1","category":"cs.CL"}
{"created":"2024-02-04 08:29:41","title":"Full Characterization of the Depth Overhead for Quantum Circuit Compilation with Arbitrary Qubit Connectivity Constraint","abstract":"In some physical implementations of quantum computers, 2-qubit operations can be applied only on certain pairs of qubits. Compilation of a quantum circuit into one compliant to such qubit connectivity constraint results in an increase of circuit depth. Various compilation algorithms were studied, yet what this depth overhead is remains elusive. In this paper, we fully characterize the depth overhead by the routing number of the underlying constraint graph, a graph-theoretic measure which has been studied for 3 decades. We also give reduction algorithms between different graphs, which allow compilation for one graph to be transferred to one for another. These results, when combined with existing routing algorithms, give asymptotically optimal compilation for all commonly seen connectivity graphs in quantum computing.","sentences":["In some physical implementations of quantum computers, 2-qubit operations can be applied only on certain pairs of qubits.","Compilation of a quantum circuit into one compliant to such qubit connectivity constraint results in an increase of circuit depth.","Various compilation algorithms were studied, yet what this depth overhead is remains elusive.","In this paper, we fully characterize the depth overhead by the routing number of the underlying constraint graph, a graph-theoretic measure which has been studied for 3 decades.","We also give reduction algorithms between different graphs, which allow compilation for one graph to be transferred to one for another.","These results, when combined with existing routing algorithms, give asymptotically optimal compilation for all commonly seen connectivity graphs in quantum computing."],"url":"http://arxiv.org/abs/2402.02403v1","category":"quant-ph"}
{"created":"2024-02-04 07:49:11","title":"On payload architecture and pointing control strategies for TianQin","abstract":"TianQin is a proposed mission for space-based gravitational-wave detection that features a triangular constellation in circular high Earth orbits. The mission entails three drag-free controlled satellites and long-range laser interferometry with stringent beam pointing requirements at remote satellites. For the payload architecture and pointing control strategies, having two test masses per satellite, one for each laser arm, and rotating entire opto-mechanical assemblies (each consisting of a telescope, an optical bench, an inertial sensor, etc.) for constellation breathing angle compensation represent an important option for TianQin. In this paper, we examine its applicability from the perspectives of test mass and satellite control in the science mode, taking into account of perturbed orbits and orbital gravity gradients. First, based on the orbit-attitude coupling relationship, the required electrostatic forces and torques for the test mass suspension control are estimated and found to be sufficiently small for the acceleration noise budget. Further optimization favors configuring the centers of masses of the two test masses collinear and equidistant with the center of mass of the satellite, and slightly offsetting the assembly pivots from the electrode housing centers forward along the sensitive axes. Second, the required control forces and torques on the satellites are calculated, and thrust allocation solutions are found under the constraint of having a flat-top sunshield on the satellite with varying solar angles. The findings give a green light to adopting the two test masses and telescope pointing scheme for TianQin.","sentences":["TianQin is a proposed mission for space-based gravitational-wave detection that features a triangular constellation in circular high Earth orbits.","The mission entails three drag-free controlled satellites and long-range laser interferometry with stringent beam pointing requirements at remote satellites.","For the payload architecture and pointing control strategies, having two test masses per satellite, one for each laser arm, and rotating entire opto-mechanical assemblies (each consisting of a telescope, an optical bench, an inertial sensor, etc.) for constellation breathing angle compensation represent an important option for TianQin.","In this paper, we examine its applicability from the perspectives of test mass and satellite control in the science mode, taking into account of perturbed orbits and orbital gravity gradients.","First, based on the orbit-attitude coupling relationship, the required electrostatic forces and torques for the test mass suspension control are estimated and found to be sufficiently small for the acceleration noise budget.","Further optimization favors configuring the centers of masses of the two test masses collinear and equidistant with the center of mass of the satellite, and slightly offsetting the assembly pivots from the electrode housing centers forward along the sensitive axes.","Second, the required control forces and torques on the satellites are calculated, and thrust allocation solutions are found under the constraint of having a flat-top sunshield on the satellite with varying solar angles.","The findings give a green light to adopting the two test masses and telescope pointing scheme for TianQin."],"url":"http://arxiv.org/abs/2402.02383v1","category":"gr-qc"}
{"created":"2024-02-04 05:54:51","title":"Incremental Quasi-Newton Methods with Faster Superlinear Convergence Rates","abstract":"We consider the finite-sum optimization problem, where each component function is strongly convex and has Lipschitz continuous gradient and Hessian. The recently proposed incremental quasi-Newton method is based on BFGS update and achieves a local superlinear convergence rate that is dependent on the condition number of the problem. This paper proposes a more efficient quasi-Newton method by incorporating the symmetric rank-1 update into the incremental framework, which results in the condition-number-free local superlinear convergence rate. Furthermore, we can boost our method by applying the block update on the Hessian approximation, which leads to an even faster local convergence rate. The numerical experiments show the proposed methods significantly outperform the baseline methods.","sentences":["We consider the finite-sum optimization problem, where each component function is strongly convex and has Lipschitz continuous gradient and Hessian.","The recently proposed incremental quasi-Newton method is based on BFGS update and achieves a local superlinear convergence rate that is dependent on the condition number of the problem.","This paper proposes a more efficient quasi-Newton method by incorporating the symmetric rank-1 update into the incremental framework, which results in the condition-number-free local superlinear convergence rate.","Furthermore, we can boost our method by applying the block update on the Hessian approximation, which leads to an even faster local convergence rate.","The numerical experiments show the proposed methods significantly outperform the baseline methods."],"url":"http://arxiv.org/abs/2402.02359v1","category":"math.OC"}
{"created":"2024-02-04 05:48:45","title":"Decentralized Sum-of-Nonconvex Optimization","abstract":"We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components. The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario. In this paper, we study the sum-of-nonconvex optimization in the decentralized setting. We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach. However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem. To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm. The convergence rate of the proposed method has a square-root dependency on the condition number. The numerical experiments validate the theoretical guarantee of our proposed algorithms on both synthetic and real-world datasets.","sentences":["We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components.","The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario.","In this paper, we study the sum-of-nonconvex optimization in the decentralized setting.","We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach.","However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem.","To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm.","The convergence rate of the proposed method has a square-root dependency on the condition number.","The numerical experiments validate the theoretical guarantee of our proposed algorithms on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2402.02356v1","category":"math.OC"}
{"created":"2024-02-04 05:41:27","title":"Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning","abstract":"Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present \\textsc{Symbol}, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within \\textsc{Symbol}, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by \\textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem dimensions, population sizes, and optimization horizons. Furthermore, we conduct in-depth analyses of our \\textsc{Symbol} framework and the optimization rules that it generates, underscoring its desirable flexibility and interpretability.","sentences":["Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers.","Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers.","In this paper, we present \\textsc{Symbol}, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning.","Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps.","Within \\textsc{Symbol}, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently.","Extensive experiments reveal that the optimizers generated by \\textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem dimensions, population sizes, and optimization horizons.","Furthermore, we conduct in-depth analyses of our \\textsc{Symbol} framework and the optimization rules that it generates, underscoring its desirable flexibility and interpretability."],"url":"http://arxiv.org/abs/2402.02355v1","category":"cs.LG"}
{"created":"2024-02-04 04:20:18","title":"Eigen Is All You Need: Efficient Lidar-Inertial Continuous-Time Odometry with Internal Association","abstract":"In this paper, we propose a continuous-time lidar-inertial odometry (CT-LIO) system named SLICT2, which promotes two main insights. One, contrary to conventional wisdom, CT-LIO algorithm can be optimized by linear solvers in only a few iterations, which is more efficient than commonly used nonlinear solvers. Two, CT-LIO benefits more from the correct association than the number of iterations. Based on these ideas, we implement our method with a customized solver where the feature association process is performed immediately after each incremental step, and the solution can converge within a few iterations. Our implementation can achieve real-time performance with a high density of control points while yielding competitive performance in highly dynamical motion scenarios. We demonstrate the advantages of our method by comparing with other existing state-of-the-art CT-LIO methods. The source code will be released for the benefit of the community.","sentences":["In this paper, we propose a continuous-time lidar-inertial odometry (CT-LIO) system named SLICT2, which promotes two main insights.","One, contrary to conventional wisdom, CT-LIO algorithm can be optimized by linear solvers in only a few iterations, which is more efficient than commonly used nonlinear solvers.","Two, CT-LIO benefits more from the correct association than the number of iterations.","Based on these ideas, we implement our method with a customized solver where the feature association process is performed immediately after each incremental step, and the solution can converge within a few iterations.","Our implementation can achieve real-time performance with a high density of control points while yielding competitive performance in highly dynamical motion scenarios.","We demonstrate the advantages of our method by comparing with other existing state-of-the-art CT-LIO methods.","The source code will be released for the benefit of the community."],"url":"http://arxiv.org/abs/2402.02337v1","category":"cs.RO"}
{"created":"2024-02-04 03:03:27","title":"Data-driven algorithm design using neural networks with applications to branch-and-cut","abstract":"Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\\em for that instance}. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization (e.g., which cut to add?). In other words, the neural network will take as input a mixed-integer optimization instance and output a decision that will result in a small branch-and-cut tree for that instance. Our computational results provide evidence that our particular way of using neural networks for cut selection can make a significant impact in reducing branch-and-cut tree sizes, compared to previous data-driven approaches.","sentences":["Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem.","We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved.","In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\\em for that instance}.","We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design.","We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization (e.g., which cut to add?).","In other words, the neural network will take as input a mixed-integer optimization instance and output a decision that will result in a small branch-and-cut tree for that instance.","Our computational results provide evidence that our particular way of using neural networks for cut selection can make a significant impact in reducing branch-and-cut tree sizes, compared to previous data-driven approaches."],"url":"http://arxiv.org/abs/2402.02328v1","category":"cs.LG"}
{"created":"2024-02-04 02:48:28","title":"Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization","abstract":"While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.","sentences":["While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking.","In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm.","This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor.","We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function."],"url":"http://arxiv.org/abs/2402.02325v1","category":"cs.LG"}
{"created":"2024-02-04 02:26:40","title":"Dynamic Incremental Optimization for Best Subset Selection","abstract":"Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.","sentences":["Best subset selection is considered the `gold standard' for many sparse learning problems.","A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem.","In this paper, we investigate the dual forms of a family of $\\ell_0$-regularized problems.","An efficient primal-dual algorithm is developed based on the primal and dual problem structures.","By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection.","Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions."],"url":"http://arxiv.org/abs/2402.02322v1","category":"cs.LG"}
{"created":"2024-02-04 02:12:15","title":"Spin: An Efficient Secure Computation Framework with GPU Acceleration","abstract":"Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.","sentences":["Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks.","Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup.","We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security.","At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration.","Comprehensive evaluations demonstrate that Spin can be up to $2\\times$ faster than the state-of-the-art for deep neural network training.","For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy."],"url":"http://arxiv.org/abs/2402.02320v1","category":"cs.CR"}
{"created":"2024-02-04 01:52:56","title":"CNS-Edit: 3D Shape Editing via Coupled Neural Shape Optimization","abstract":"This paper introduces a new approach based on a coupled representation and a neural volume optimization to implicitly perform 3D shape editing in latent space. This work has three innovations. First, we design the coupled neural shape (CNS) representation for supporting 3D shape editing. This representation includes a latent code, which captures high-level global semantics of the shape, and a 3D neural feature volume, which provides a spatial context to associate with the local shape changes given by the editing. Second, we formulate the coupled neural shape optimization procedure to co-optimize the two coupled components in the representation subject to the editing operation. Last, we offer various 3D shape editing operators, i.e., copy, resize, delete, and drag, and derive each into an objective for guiding the CNS optimization, such that we can iteratively co-optimize the latent code and neural feature volume to match the editing target. With our approach, we can achieve a rich variety of editing results that are not only aware of the shape semantics but are also not easy to achieve by existing approaches. Both quantitative and qualitative evaluations demonstrate the strong capabilities of our approach over the state-of-the-art solutions.","sentences":["This paper introduces a new approach based on a coupled representation and a neural volume optimization to implicitly perform 3D shape editing in latent space.","This work has three innovations.","First, we design the coupled neural shape (CNS) representation for supporting 3D shape editing.","This representation includes a latent code, which captures high-level global semantics of the shape, and a 3D neural feature volume, which provides a spatial context to associate with the local shape changes given by the editing.","Second, we formulate the coupled neural shape optimization procedure to co-optimize the two coupled components in the representation subject to the editing operation.","Last, we offer various 3D shape editing operators, i.e., copy, resize, delete, and drag, and derive each into an objective for guiding the CNS optimization, such that we can iteratively co-optimize the latent code and neural feature volume to match the editing target.","With our approach, we can achieve a rich variety of editing results that are not only aware of the shape semantics but are also not easy to achieve by existing approaches.","Both quantitative and qualitative evaluations demonstrate the strong capabilities of our approach over the state-of-the-art solutions."],"url":"http://arxiv.org/abs/2402.02313v1","category":"cs.CV"}
{"created":"2024-02-03 23:47:34","title":"An efficient third-order WENO scheme with unconditionally optimal accuracy","abstract":"A novel scheme, based on third-order Weighted Essentially Non-Oscillatory (WENO) reconstructions, is presented. It attains unconditionally optimal accuracy when the data is smooth enough, even in presence of critical points, and second-order accuracy if a discontinuity crosses the data. The key to attribute these properties to this scheme is the inclusion of an additional node in the data stencil, which is only used in the computation of the weights measuring the smoothness. The accuracy properties of this scheme are proven in detail and several numerical experiments are presented, which show that this scheme is more efficient in terms of the error reduction versus CPU time than its traditional third-order counterparts as well as several higher-order WENO schemes that are found in the literature.","sentences":["A novel scheme, based on third-order Weighted Essentially Non-Oscillatory (WENO) reconstructions, is presented.","It attains unconditionally optimal accuracy when the data is smooth enough, even in presence of critical points, and second-order accuracy if a discontinuity crosses the data.","The key to attribute these properties to this scheme is the inclusion of an additional node in the data stencil, which is only used in the computation of the weights measuring the smoothness.","The accuracy properties of this scheme are proven in detail and several numerical experiments are presented, which show that this scheme is more efficient in terms of the error reduction versus CPU time than its traditional third-order counterparts as well as several higher-order WENO schemes that are found in the literature."],"url":"http://arxiv.org/abs/2402.02300v1","category":"math.NA"}
{"created":"2024-02-03 23:19:26","title":"Denoising Diffusion-Based Control of Nonlinear Systems","abstract":"We propose a novel approach based on Denoising Diffusion Probabilistic Models (DDPMs) to control nonlinear dynamical systems. DDPMs are the state-of-art of generative models that have achieved success in a wide variety of sampling tasks. In our framework, we pose the feedback control problem as a generative task of drawing samples from a target set under control system constraints. The forward process of DDPMs constructs trajectories originating from a target set by adding noise. We learn to control a dynamical system in reverse such that the terminal state belongs to the target set. For control-affine systems without drift, we prove that the control system can exactly track the trajectory of the forward process in reverse, whenever the the Lie bracket based condition for controllability holds. We numerically study our approach on various nonlinear systems and verify our theoretical results. We also conduct numerical experiments for cases beyond our theoretical results on a physics-engine.","sentences":["We propose a novel approach based on Denoising Diffusion Probabilistic Models (DDPMs) to control nonlinear dynamical systems.","DDPMs are the state-of-art of generative models that have achieved success in a wide variety of sampling tasks.","In our framework, we pose the feedback control problem as a generative task of drawing samples from a target set under control system constraints.","The forward process of DDPMs constructs trajectories originating from a target set by adding noise.","We learn to control a dynamical system in reverse such that the terminal state belongs to the target set.","For control-affine systems without drift, we prove that the control system can exactly track the trajectory of the forward process in reverse, whenever the the Lie bracket based condition for controllability holds.","We numerically study our approach on various nonlinear systems and verify our theoretical results.","We also conduct numerical experiments for cases beyond our theoretical results on a physics-engine."],"url":"http://arxiv.org/abs/2402.02297v1","category":"math.OC"}
{"created":"2024-02-03 23:17:18","title":"WENO reconstructions of unconditionally optimal high order","abstract":"A modified Weighted Essentially Non-Oscillatory (WENO) reconstruction technique preventing accuracy loss near critical points (regardless of their order) of the underlying data is presented. This approach only uses local data from the reconstruction stencil and does not rely on any sort of scaling parameters. The key novel ingredient is a weight design based on a new smoothness indicator, which defines the first WENO reconstruction procedure that never loses accuracy on smooth data, regardless of the presence of critical points of any order, and is therefore addressed as optimal WENO (OWENO) method. The corresponding weights are non-dimensional and scale-independent. The weight designs are supported by theoretical results concerning the accuracy of the smoothness indicators. The method is validated by numerical tests related to algebraic equations, scalar conservation laws, and systems of conservation laws.","sentences":["A modified Weighted Essentially Non-Oscillatory (WENO) reconstruction technique preventing accuracy loss near critical points (regardless of their order) of the underlying data is presented.","This approach only uses local data from the reconstruction stencil and does not rely on any sort of scaling parameters.","The key novel ingredient is a weight design based on a new smoothness indicator, which defines the first WENO reconstruction procedure that never loses accuracy on smooth data, regardless of the presence of critical points of any order, and is therefore addressed as optimal WENO (OWENO) method.","The corresponding weights are non-dimensional and scale-independent.","The weight designs are supported by theoretical results concerning the accuracy of the smoothness indicators.","The method is validated by numerical tests related to algebraic equations, scalar conservation laws, and systems of conservation laws."],"url":"http://arxiv.org/abs/2402.02295v1","category":"math.NA"}
