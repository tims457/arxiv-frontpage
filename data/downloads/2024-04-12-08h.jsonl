{"created":"2024-04-11 17:59:56","title":"OpenBias: Open-set Bias Detection in Text-to-Image Generative Models","abstract":"Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.","sentences":["Text-to-image generative models are becoming increasingly popular and accessible to the general public.","As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases.","However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts.","In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set.","OpenBias has three stages.","In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions.","Secondly, the target generative model produces images using the same set of captions.","Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases.","We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before.","Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement."],"url":"http://arxiv.org/abs/2404.07990v1","category":"cs.CV"}
{"created":"2024-04-11 17:59:45","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding","abstract":"Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm. In this paper, we introduce Any2Point, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding. Given a frozen transformer from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality. This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D geometry loss caused by the true projection and better motivates the transformer for 3D learning with 1D/2D positional priors. Then, within each transformer block, we insert an any-to-3D guided adapter module for parameter-efficient fine-tuning. The adapter incorporates prior spatial knowledge from the source modality to guide the local feature aggregation of 3D tokens, compelling the semantic adaption of any-modality transformers. We conduct extensive experiments to showcase the effectiveness and efficiency of our method. Code and models are released at https://github.com/Ivan-Tang-3D/Any2Point.","sentences":["Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios.","Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains.","However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost.","More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm.","In this paper, we introduce Any2Point, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding.","Given a frozen transformer from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality.","This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D geometry loss caused by the true projection and better motivates the transformer for 3D learning with 1D/2D positional priors.","Then, within each transformer block, we insert an any-to-3D guided adapter module for parameter-efficient fine-tuning.","The adapter incorporates prior spatial knowledge from the source modality to guide the local feature aggregation of 3D tokens, compelling the semantic adaption of any-modality transformers.","We conduct extensive experiments to showcase the effectiveness and efficiency of our method.","Code and models are released at https://github.com/Ivan-Tang-3D/Any2Point."],"url":"http://arxiv.org/abs/2404.07989v1","category":"cs.CV"}
{"created":"2024-04-11 17:59:09","title":"ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback","abstract":"To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions.","sentences":["To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls.","In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls.","To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls.","Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition.","A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs.","To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning.","This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning.","Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls.","For example, it achieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions."],"url":"http://arxiv.org/abs/2404.07987v1","category":"cs.CV"}
{"created":"2024-04-11 17:58:44","title":"WaveMo: Learning Wavefront Modulations to See Through Scattering","abstract":"Imaging through scattering media is a fundamental and pervasive challenge in fields ranging from medical diagnostics to astronomy. A promising strategy to overcome this challenge is wavefront modulation, which induces measurement diversity during image acquisition. Despite its importance, designing optimal wavefront modulations to image through scattering remains under-explored. This paper introduces a novel learning-based framework to address the gap. Our approach jointly optimizes wavefront modulations and a computationally lightweight feedforward \"proxy\" reconstruction network. This network is trained to recover scenes obscured by scattering, using measurements that are modified by these modulations. The learned modulations produced by our framework generalize effectively to unseen scattering scenarios and exhibit remarkable versatility. During deployment, the learned modulations can be decoupled from the proxy network to augment other more computationally expensive restoration algorithms. Through extensive experiments, we demonstrate our approach significantly advances the state of the art in imaging through scattering media. Our project webpage is at https://wavemo-2024.github.io/.","sentences":["Imaging through scattering media is a fundamental and pervasive challenge in fields ranging from medical diagnostics to astronomy.","A promising strategy to overcome this challenge is wavefront modulation, which induces measurement diversity during image acquisition.","Despite its importance, designing optimal wavefront modulations to image through scattering remains under-explored.","This paper introduces a novel learning-based framework to address the gap.","Our approach jointly optimizes wavefront modulations and a computationally lightweight feedforward \"proxy\" reconstruction network.","This network is trained to recover scenes obscured by scattering, using measurements that are modified by these modulations.","The learned modulations produced by our framework generalize effectively to unseen scattering scenarios and exhibit remarkable versatility.","During deployment, the learned modulations can be decoupled from the proxy network to augment other more computationally expensive restoration algorithms.","Through extensive experiments, we demonstrate our approach significantly advances the state of the art in imaging through scattering media.","Our project webpage is at https://wavemo-2024.github.io/."],"url":"http://arxiv.org/abs/2404.07985v1","category":"cs.CV"}
{"created":"2024-04-11 17:58:11","title":"View Selection for 3D Captioning via Diffusion Ranking","abstract":"Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications. However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality. This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models. We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations. To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object's characteristics. By ranking all rendered views and feeding the top-ranked ones into GPT4-Vision, we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets. Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained text-to-image models for a Visual Question Answering task, where it outperforms the CLIP model.","sentences":["Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications.","However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality.","This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models.","We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations.","To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object's characteristics.","By ranking all rendered views and feeding the top-ranked ones into GPT4-Vision, we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets.","Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained text-to-image models for a Visual Question Answering task, where it outperforms the CLIP model."],"url":"http://arxiv.org/abs/2404.07984v1","category":"cs.CV"}
{"created":"2024-04-11 17:57:32","title":"Manipulating Large Language Models to Increase Product Visibility","abstract":"Large language models (LLMs) are increasingly being integrated into search engines to provide natural language responses tailored to user queries. Customers and end-users are also becoming more dependent on these models for quick and easy purchase decisions. In this work, we investigate whether recommendations from LLMs can be manipulated to enhance a product's visibility. We demonstrate that adding a strategic text sequence (STS) -- a carefully crafted message -- to a product's information page can significantly increase its likelihood of being listed as the LLM's top recommendation. To understand the impact of STS, we use a catalog of fictitious coffee machines and analyze its effect on two target products: one that seldom appears in the LLM's recommendations and another that usually ranks second. We observe that the strategic text sequence significantly enhances the visibility of both products by increasing their chances of appearing as the top recommendation. This ability to manipulate LLM-generated search responses provides vendors with a considerable competitive advantage and has the potential to disrupt fair market competition. Just as search engine optimization (SEO) revolutionized how webpages are customized to rank higher in search engine results, influencing LLM recommendations could profoundly impact content optimization for AI-driven search services. Code for our experiments is available at https://github.com/aounon/llm-rank-optimizer.","sentences":["Large language models (LLMs) are increasingly being integrated into search engines to provide natural language responses tailored to user queries.","Customers and end-users are also becoming more dependent on these models for quick and easy purchase decisions.","In this work, we investigate whether recommendations from LLMs can be manipulated to enhance a product's visibility.","We demonstrate that adding a strategic text sequence (STS) -- a carefully crafted message -- to a product's information page can significantly increase its likelihood of being listed as the LLM's top recommendation.","To understand the impact of STS, we use a catalog of fictitious coffee machines and analyze its effect on two target products: one that seldom appears in the LLM's recommendations and another that usually ranks second.","We observe that the strategic text sequence significantly enhances the visibility of both products by increasing their chances of appearing as the top recommendation.","This ability to manipulate LLM-generated search responses provides vendors with a considerable competitive advantage and has the potential to disrupt fair market competition.","Just as search engine optimization (SEO) revolutionized how webpages are customized to rank higher in search engine results, influencing LLM recommendations could profoundly impact content optimization for AI-driven search services.","Code for our experiments is available at https://github.com/aounon/llm-rank-optimizer."],"url":"http://arxiv.org/abs/2404.07981v1","category":"cs.IR"}
{"created":"2024-04-11 17:57:30","title":"ISCOs and the weak gravity conjecture bound in Gauss-Bonnet gravity","abstract":"We study circular orbits of charged particles in spherically symmetric AdS black holes in Gauss-Bonnet gravity in arbitrary dimensions, and their limiting ISCOs (innermost stable circular orbits). The dual interpretation is in terms of certain conformal field theory (CFT) operators, whose anomalous dimensions can be extracted from the binding energy of charged probes in the bulk, in a large radius limit. The charge to mass ratio of particles in the black hole background is found to increase with the Gauss-Bonnet coupling parameter $\\widetilde{\\alpha}$, consistent with the Weak Gravity Conjecture (WGC) bound known for self-repulsive particles. The radius of the ISCOs decreases with $\\widetilde{\\alpha}$, existing until the limit set by the WGC bound.","sentences":["We study circular orbits of charged particles in spherically symmetric AdS black holes in Gauss-Bonnet gravity in arbitrary dimensions, and their limiting ISCOs (innermost stable circular orbits).","The dual interpretation is in terms of certain conformal field theory (CFT) operators, whose anomalous dimensions can be extracted from the binding energy of charged probes in the bulk, in a large radius limit.","The charge to mass ratio of particles in the black hole background is found to increase with the Gauss-Bonnet coupling parameter $\\widetilde{\\alpha}$, consistent with the Weak Gravity Conjecture (WGC) bound known for self-repulsive particles.","The radius of the ISCOs decreases with $\\widetilde{\\alpha}$, existing until the limit set by the WGC bound."],"url":"http://arxiv.org/abs/2404.07980v1","category":"hep-th"}
{"created":"2024-04-11 17:57:22","title":"LLoCO: Learning Long Contexts Offline","abstract":"Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. We propose a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning. Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. We introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using $30\\times$ fewer tokens during inference. LLoCO achieves up to $7.62\\times$ speed-up and substantially reduces the cost of long document question answering, making it a promising solution for efficient long context processing. Our code is publicly available at https://github.com/jeffreysijuntan/lloco.","sentences":["Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation.","We propose a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning.","Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately.","We introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA.","Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens.","We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using $30\\times$ fewer tokens during inference.","LLoCO achieves up to $7.62\\times$ speed-up and substantially reduces the cost of long document question answering, making it a promising solution for efficient long context processing.","Our code is publicly available at https://github.com/jeffreysijuntan/lloco."],"url":"http://arxiv.org/abs/2404.07979v1","category":"cs.CL"}
{"created":"2024-04-11 17:57:20","title":"On average output entropy of a quantum channel","abstract":"We describe analytical properties of the average output entropy of a quantum channel as a function of a pair (channel, input ensemble). In particular, tight semicontinuity bounds for this function with the rank/energy constraints are obtained by using the modified semicontinuity bounds for the quantum conditional entropy of quantum-classical states and a special approximation technique.   Several applications are considered. New semicontinuity and continuity bounds for the output Holevo information of a channel as a function of a pair (channel, input ensemble) are obtained. The semicontinuity bound for the entanglement of formation with the rank constraint obtained in [1] is improved.   In the preliminary part, some results concerning ensembles of quantum states are presented. In particular, a new useful metric on the set of generalized ensembles is proposed and explored. The concept of passive energy of an ensemble introduced here plays an important role in the article.","sentences":["We describe analytical properties of the average output entropy of a quantum channel as a function of a pair (channel, input ensemble).","In particular, tight semicontinuity bounds for this function with the rank/energy constraints are obtained by using the modified semicontinuity bounds for the quantum conditional entropy of quantum-classical states and a special approximation technique.   ","Several applications are considered.","New semicontinuity and continuity bounds for the output Holevo information of a channel as a function of a pair (channel, input ensemble) are obtained.","The semicontinuity bound for the entanglement of formation with the rank constraint obtained in [1] is improved.   ","In the preliminary part, some results concerning ensembles of quantum states are presented.","In particular, a new useful metric on the set of generalized ensembles is proposed and explored.","The concept of passive energy of an ensemble introduced here plays an important role in the article."],"url":"http://arxiv.org/abs/2404.07978v1","category":"quant-ph"}
{"created":"2024-04-11 17:56:40","title":"Self-supervised Dataset Distillation: A Good Compression Is All You Need","abstract":"Dataset distillation aims to compress information from a large-scale original dataset to a new compact dataset while striving to preserve the utmost degree of the original data informational essence. Previous studies have predominantly concentrated on aligning the intermediate statistics between the original and distilled data, such as weight trajectory, features, gradient, BatchNorm, etc. In this work, we consider addressing this task through the new lens of model informativeness in the compression stage on the original dataset pretraining. We observe that with the prior state-of-the-art SRe$^2$L, as model sizes increase, it becomes increasingly challenging for supervised pretrained models to recover learned information during data synthesis, as the channel-wise mean and variance inside the model are flatting and less informative. We further notice that larger variances in BN statistics from self-supervised models enable larger loss signals to update the recovered data by gradients, enjoying more informativeness during synthesis. Building on this observation, we introduce SC-DD, a simple yet effective Self-supervised Compression framework for Dataset Distillation that facilitates diverse information compression and recovery compared to traditional supervised learning schemes, further reaps the potential of large pretrained models with enhanced capabilities. Extensive experiments are conducted on CIFAR-100, Tiny-ImageNet and ImageNet-1K datasets to demonstrate the superiority of our proposed approach. The proposed SC-DD outperforms all previous state-of-the-art supervised dataset distillation methods when employing larger models, such as SRe$^2$L, MTT, TESLA, DC, CAFE, etc., by large margins under the same recovery and post-training budgets. Code is available at https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/.","sentences":["Dataset distillation aims to compress information from a large-scale original dataset to a new compact dataset while striving to preserve the utmost degree of the original data informational essence.","Previous studies have predominantly concentrated on aligning the intermediate statistics between the original and distilled data, such as weight trajectory, features, gradient, BatchNorm, etc.","In this work, we consider addressing this task through the new lens of model informativeness in the compression stage on the original dataset pretraining.","We observe that with the prior state-of-the-art SRe$^2$L, as model sizes increase, it becomes increasingly challenging for supervised pretrained models to recover learned information during data synthesis, as the channel-wise mean and variance inside the model are flatting and less informative.","We further notice that larger variances in BN statistics from self-supervised models enable larger loss signals to update the recovered data by gradients, enjoying more informativeness during synthesis.","Building on this observation, we introduce SC-DD, a simple yet effective Self-supervised Compression framework for Dataset Distillation that facilitates diverse information compression and recovery compared to traditional supervised learning schemes, further reaps the potential of large pretrained models with enhanced capabilities.","Extensive experiments are conducted on CIFAR-100, Tiny-ImageNet and ImageNet-1K datasets to demonstrate the superiority of our proposed approach.","The proposed SC-DD outperforms all previous state-of-the-art supervised dataset distillation methods when employing larger models, such as SRe$^2$L, MTT, TESLA, DC, CAFE, etc., by large margins under the same recovery and post-training budgets.","Code is available at https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/."],"url":"http://arxiv.org/abs/2404.07976v1","category":"cs.CV"}
{"created":"2024-04-11 17:56:05","title":"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","abstract":"Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io.","sentences":["Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity.","However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability.","To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS.","OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications.","Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications.","Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation.","Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants.","While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge.","Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks.","Our code, environment, baseline models, and data are publicly available at https://os-world.github.io."],"url":"http://arxiv.org/abs/2404.07972v1","category":"cs.AI"}
{"created":"2024-04-11 17:52:01","title":"Rho-1: Not All Tokens Are What You Need","abstract":"Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that \"Not all tokens in a corpus are equally important for language model training\". Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.","sentences":["Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens.","Challenging this norm, we posit that \"Not all tokens in a corpus are equally important for language model training\".","Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens.","Leveraging these insights, we introduce a new language model called Rho-1.","Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution.","This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss.","When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks.","After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens.","Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training."],"url":"http://arxiv.org/abs/2404.07965v1","category":"cs.CL"}
{"created":"2024-04-11 17:49:15","title":"Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation for Efficient Synthesis and Verification","abstract":"Learning-based neural network (NN) control policies have shown impressive empirical performance in a wide range of tasks in robotics and control. However, formal (Lyapunov) stability guarantees over the region-of-attraction (ROA) for NN controllers with nonlinear dynamical systems are challenging to obtain, and most existing approaches rely on expensive solvers such as sums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability modulo theories (SMT). In this paper, we demonstrate a new framework for learning NN controllers together with Lyapunov certificates using fast empirical falsification and strategic regularizations. We propose a novel formulation that defines a larger verifiable region-of-attraction (ROA) than shown in the literature, and refines the conventional restrictive constraints on Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunov condition is rigorously verified post-hoc using branch-and-bound with scalable linear bound propagation-based NN verification techniques. The approach is efficient and flexible, and the full training and verification procedure is accelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT. The flexibility and efficiency of our framework allow us to demonstrate Lyapunov-stable output feedback control with synthesized NN-based controllers and NN-based observers with formal stability guarantees, for the first time in literature. Source code at https://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.","sentences":["Learning-based neural network (NN) control policies have shown impressive empirical performance in a wide range of tasks in robotics and control.","However, formal (Lyapunov) stability guarantees over the region-of-attraction (ROA) for NN controllers with nonlinear dynamical systems are challenging to obtain, and most existing approaches rely on expensive solvers such as sums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability modulo theories (SMT).","In this paper, we demonstrate a new framework for learning NN controllers together with Lyapunov certificates using fast empirical falsification and strategic regularizations.","We propose a novel formulation that defines a larger verifiable region-of-attraction (ROA) than shown in the literature, and refines the conventional restrictive constraints on Lyapunov derivatives to focus only on certifiable ROAs.","The Lyapunov condition is rigorously verified post-hoc using branch-and-bound with scalable linear bound propagation-based NN verification techniques.","The approach is efficient and flexible, and the full training and verification procedure is accelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT.","The flexibility and efficiency of our framework allow us to demonstrate Lyapunov-stable output feedback control with synthesized NN-based controllers and NN-based observers with formal stability guarantees, for the first time in literature.","Source code at https://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers."],"url":"http://arxiv.org/abs/2404.07956v1","category":"cs.LG"}
{"created":"2024-04-11 17:47:17","title":"Floer Homology with DG Coefficients. Applications to cotangent bundles","abstract":"We define Hamiltonian Floer homology with differential graded (DG) local coefficients for symplectically aspherical manifolds. The differential of the underlying complex involves chain representatives of the fundamental classes of the moduli spaces of Floer trajectories of arbitrary dimension. This setup allows in particular to define and compute Floer homology with coefficients in chains on fibers of fibrations over the free loop space of the underlying symplectic manifold. We develop the DG Floer toolset, including continuation maps and homotopies, and we also define and study symplectic homology groups with DG local coefficients. We define spectral invariants and establish general criteria for almost existence of contractible periodic orbits on regular energy levels of Hamiltonian systems inside Liouville domains.   In the case of cotangent bundles, we prove a Viterbo isomorphism theorem with DG local coefficients. This serves as a stepping stone for applications to the almost existence of contractible closed characteristics on closed smooth hypersurfaces. In this context, our methods allow to access for the first time the dichotomy between closed manifolds that are aspherical and those that are not.","sentences":["We define Hamiltonian Floer homology with differential graded (DG) local coefficients for symplectically aspherical manifolds.","The differential of the underlying complex involves chain representatives of the fundamental classes of the moduli spaces of Floer trajectories of arbitrary dimension.","This setup allows in particular to define and compute Floer homology with coefficients in chains on fibers of fibrations over the free loop space of the underlying symplectic manifold.","We develop the DG Floer toolset, including continuation maps and homotopies, and we also define and study symplectic homology groups with DG local coefficients.","We define spectral invariants and establish general criteria for almost existence of contractible periodic orbits on regular energy levels of Hamiltonian systems inside Liouville domains.   ","In the case of cotangent bundles, we prove a Viterbo isomorphism theorem with DG local coefficients.","This serves as a stepping stone for applications to the almost existence of contractible closed characteristics on closed smooth hypersurfaces.","In this context, our methods allow to access for the first time the dichotomy between closed manifolds that are aspherical and those that are not."],"url":"http://arxiv.org/abs/2404.07953v1","category":"math.SG"}
{"created":"2024-04-11 17:46:14","title":"Taming Stable Diffusion for Text to 360\u00b0 Panorama Image Generation","abstract":"Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at https://chengzhag.github.io/publication/panfusion.","sentences":["Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts.","Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images.","In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt.","We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation.","We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process.","Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs.","Code is available at https://chengzhag.github.io/publication/panfusion."],"url":"http://arxiv.org/abs/2404.07949v1","category":"cs.CV"}
{"created":"2024-04-11 17:41:28","title":"On Unified Prompt Tuning for Request Quality Assurance in Public Code Review","abstract":"Public Code Review (PCR) can be implemented through a Software Question Answering (SQA) community, which facilitates high knowledge dissemination. Current methods mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments. Our intuition is that satisfying review necessity requests can increase their visibility, which in turn is a prerequisite for better review responses. To this end, we propose a unified framework called UniPCR to complete developer-based request quality assurance (i.e., predicting request necessity and recommending tags subtask) under a Masked Language Model (MLM). Specifically, we reformulate both subtasks via 1) text prompt tuning, which converts two subtasks into MLM by constructing prompt templates using hard prompt; 2) code prefix tuning, which optimizes a small segment of generated continuous vectors as the prefix of the code representation using soft prompt. Experimental results on the Public Code Review dataset for the time span 2011-2022 demonstrate that our UniPCR framework adapts to the two subtasks and outperforms comparable accuracy-based results with state-of-the-art methods for request quality assurance. These conclusions highlight the effectiveness of our unified framework from the developer's perspective in public code review.","sentences":["Public Code Review (PCR) can be implemented through a Software Question Answering (SQA) community, which facilitates high knowledge dissemination.","Current methods mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments.","Our intuition is that satisfying review necessity requests can increase their visibility, which in turn is a prerequisite for better review responses.","To this end, we propose a unified framework called UniPCR to complete developer-based request quality assurance (i.e., predicting request necessity and recommending tags subtask) under a Masked Language Model (MLM).","Specifically, we reformulate both subtasks via 1) text prompt tuning, which converts two subtasks into MLM by constructing prompt templates using hard prompt; 2) code prefix tuning, which optimizes a small segment of generated continuous vectors as the prefix of the code representation using soft prompt.","Experimental results on the Public Code Review dataset for the time span 2011-2022 demonstrate that our UniPCR framework adapts to the two subtasks and outperforms comparable accuracy-based results with state-of-the-art methods for request quality assurance.","These conclusions highlight the effectiveness of our unified framework from the developer's perspective in public code review."],"url":"http://arxiv.org/abs/2404.07942v1","category":"cs.SE"}
{"created":"2024-04-11 17:38:48","title":"Toward ultra-efficient high fidelity predictions of wind turbine wakes: Augmenting the accuracy of engineering models via LES-trained machine learning","abstract":"This study proposes a novel machine learning (ML) methodology for the efficient and cost-effective prediction of high-fidelity three-dimensional velocity fields in the wake of utility-scale turbines. The model consists of an auto-encoder convolutional neural network with U-Net skipped connections, fine-tuned using high-fidelity data from large-eddy simulations (LES). The trained model takes the low-fidelity velocity field cost-effectively generated from the analytical engineering wake model as input and produces the high-fidelity velocity fields. The accuracy of the proposed ML model is demonstrated in a utility-scale wind farm for which datasets of wake flow fields were previously generated using LES under various wind speeds, wind directions, and yaw angles. Comparing the ML model results with those of LES, the ML model was shown to reduce the error in the prediction from 20% obtained from the GCH model to less than 5%. In addition, the ML model captured the non-symmetric wake deflection observed for opposing yaw angles for wake steering cases, demonstrating a greater accuracy than the GCH model. The computational cost of the ML model is on par with that of the analytical wake model while generating numerical outcomes nearly as accurate as those of the high-fidelity LES.","sentences":["This study proposes a novel machine learning (ML) methodology for the efficient and cost-effective prediction of high-fidelity three-dimensional velocity fields in the wake of utility-scale turbines.","The model consists of an auto-encoder convolutional neural network with U-Net skipped connections, fine-tuned using high-fidelity data from large-eddy simulations (LES).","The trained model takes the low-fidelity velocity field cost-effectively generated from the analytical engineering wake model as input and produces the high-fidelity velocity fields.","The accuracy of the proposed ML model is demonstrated in a utility-scale wind farm for which datasets of wake flow fields were previously generated using LES under various wind speeds, wind directions, and yaw angles.","Comparing the ML model results with those of LES, the ML model was shown to reduce the error in the prediction from 20% obtained from the GCH model to less than 5%.","In addition, the ML model captured the non-symmetric wake deflection observed for opposing yaw angles for wake steering cases, demonstrating a greater accuracy than the GCH model.","The computational cost of the ML model is on par with that of the analytical wake model while generating numerical outcomes nearly as accurate as those of the high-fidelity LES."],"url":"http://arxiv.org/abs/2404.07938v1","category":"physics.flu-dyn"}
{"created":"2024-04-11 17:36:28","title":"Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","abstract":"We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.","sentences":["We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition.","While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes.","By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes.","Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models."],"url":"http://arxiv.org/abs/2404.07937v1","category":"math.ST"}
{"created":"2024-04-11 17:35:54","title":"Astrophysics and Nuclear Physics Informed Interactions in Dense Matter: Insights from PSR J0437-4715","abstract":"We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains. We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework. We show that neutron-matter $\\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\\omega^2\\rho^2$ term. We also show the behavior of the model with respect to the conformal limit. We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter. We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state. We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission. Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship.","sentences":["We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains.","We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework.","We show that neutron-matter $\\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\\omega^2\\rho^2$ term.","We also show the behavior of the model with respect to the conformal limit.","We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter.","We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state.","We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission.","Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship."],"url":"http://arxiv.org/abs/2404.07936v1","category":"nucl-th"}
{"created":"2024-04-11 17:35:19","title":"Compositional Growth Models","abstract":"We review models of compositional growth, which were introduced to explain the growth statistics of various quantities ranging from firm sizes to GDP. In these models, entities are decomposed into units that grow independently. Thus, the growth rate of the entity is the addition of the growth rates of the composing units, with possibly heterogeneous weights. We review such models and show that they can be understood through a unifying theoretical framework, explaining the resulting growth rate distributions using mixtures of Gaussians.","sentences":["We review models of compositional growth, which were introduced to explain the growth statistics of various quantities ranging from firm sizes to GDP.","In these models, entities are decomposed into units that grow independently.","Thus, the growth rate of the entity is the addition of the growth rates of the composing units, with possibly heterogeneous weights.","We review such models and show that they can be understood through a unifying theoretical framework, explaining the resulting growth rate distributions using mixtures of Gaussians."],"url":"http://arxiv.org/abs/2404.07935v1","category":"econ.GN"}
{"created":"2024-04-11 17:34:35","title":"Goal Recognition via Linear Programming","abstract":"Goal Recognition is the task by which an observer aims to discern the goals that correspond to plans that comply with the perceived behavior of subject agents given as a sequence of observations. Research on Goal Recognition as Planning encompasses reasoning about the model of a planning task, the observations, and the goals using planning techniques, resulting in very efficient recognition approaches. In this article, we design novel recognition approaches that rely on the Operator-Counting framework, proposing new constraints, and analyze their constraints' properties both theoretically and empirically. The Operator-Counting framework is a technique that efficiently computes heuristic estimates of cost-to-goal using Integer/Linear Programming (IP/LP). In the realm of theory, we prove that the new constraints provide lower bounds on the cost of plans that comply with observations. We also provide an extensive empirical evaluation to assess how the new constraints improve the quality of the solution, and we found that they are especially informed in deciding which goals are unlikely to be part of the solution. Our novel recognition approaches have two pivotal advantages: first, they employ new IP/LP constraints for efficiently recognizing goals; second, we show how the new IP/LP constraints can improve the recognition of goals under both partial and noisy observability.","sentences":["Goal Recognition is the task by which an observer aims to discern the goals that correspond to plans that comply with the perceived behavior of subject agents given as a sequence of observations.","Research on Goal Recognition as Planning encompasses reasoning about the model of a planning task, the observations, and the goals using planning techniques, resulting in very efficient recognition approaches.","In this article, we design novel recognition approaches that rely on the Operator-Counting framework, proposing new constraints, and analyze their constraints' properties both theoretically and empirically.","The Operator-Counting framework is a technique that efficiently computes heuristic estimates of cost-to-goal using Integer/Linear Programming (IP/LP).","In the realm of theory, we prove that the new constraints provide lower bounds on the cost of plans that comply with observations.","We also provide an extensive empirical evaluation to assess how the new constraints improve the quality of the solution, and we found that they are especially informed in deciding which goals are unlikely to be part of the solution.","Our novel recognition approaches have two pivotal advantages: first, they employ new IP/LP constraints for efficiently recognizing goals; second, we show how the new IP/LP constraints can improve the recognition of goals under both partial and noisy observability."],"url":"http://arxiv.org/abs/2404.07934v1","category":"cs.AI"}
{"created":"2024-04-11 17:29:56","title":"FusionMamba: Efficient Image Fusion with State Space Model","abstract":"Image fusion aims to generate a high-resolution multi/hyper-spectral image by combining a high-resolution image with limited spectral information and a low-resolution image with abundant spectral data. Current deep learning (DL)-based methods for image fusion primarily rely on CNNs or Transformers to extract features and merge different types of data. While CNNs are efficient, their receptive fields are limited, restricting their capacity to capture global context. Conversely, Transformers excel at learning global information but are hindered by their quadratic complexity. Fortunately, recent advancements in the State Space Model (SSM), particularly Mamba, offer a promising solution to this issue by enabling global awareness with linear complexity. However, there have been few attempts to explore the potential of SSM in information fusion, which is a crucial ability in domains like image fusion. Therefore, we propose FusionMamba, an innovative method for efficient image fusion. Our contributions mainly focus on two aspects. Firstly, recognizing that images from different sources possess distinct properties, we incorporate Mamba blocks into two U-shaped networks, presenting a novel architecture that extracts spatial and spectral features in an efficient, independent, and hierarchical manner. Secondly, to effectively combine spatial and spectral information, we extend the Mamba block to accommodate dual inputs. This expansion leads to the creation of a new module called the FusionMamba block, which outperforms existing fusion techniques such as concatenation and cross-attention. To validate FusionMamba's effectiveness, we conduct a series of experiments on five datasets related to three image fusion tasks. The quantitative and qualitative evaluation results demonstrate that our method achieves state-of-the-art (SOTA) performance, underscoring the superiority of FusionMamba.","sentences":["Image fusion aims to generate a high-resolution multi/hyper-spectral image by combining a high-resolution image with limited spectral information and a low-resolution image with abundant spectral data.","Current deep learning (DL)-based methods for image fusion primarily rely on CNNs or Transformers to extract features and merge different types of data.","While CNNs are efficient, their receptive fields are limited, restricting their capacity to capture global context.","Conversely, Transformers excel at learning global information but are hindered by their quadratic complexity.","Fortunately, recent advancements in the State Space Model (SSM), particularly Mamba, offer a promising solution to this issue by enabling global awareness with linear complexity.","However, there have been few attempts to explore the potential of SSM in information fusion, which is a crucial ability in domains like image fusion.","Therefore, we propose FusionMamba, an innovative method for efficient image fusion.","Our contributions mainly focus on two aspects.","Firstly, recognizing that images from different sources possess distinct properties, we incorporate Mamba blocks into two U-shaped networks, presenting a novel architecture that extracts spatial and spectral features in an efficient, independent, and hierarchical manner.","Secondly, to effectively combine spatial and spectral information, we extend the Mamba block to accommodate dual inputs.","This expansion leads to the creation of a new module called the FusionMamba block, which outperforms existing fusion techniques such as concatenation and cross-attention.","To validate FusionMamba's effectiveness, we conduct a series of experiments on five datasets related to three image fusion tasks.","The quantitative and qualitative evaluation results demonstrate that our method achieves state-of-the-art (SOTA) performance, underscoring the superiority of FusionMamba."],"url":"http://arxiv.org/abs/2404.07932v1","category":"cs.CV"}
{"created":"2024-04-11 17:27:39","title":"Parameter Hierarchical Optimization for Visible-Infrared Person Re-Identification","abstract":"Visible-infrared person re-identification (VI-reID) aims at matching cross-modality pedestrian images captured by disjoint visible or infrared cameras. Existing methods alleviate the cross-modality discrepancies via designing different kinds of network architectures. Different from available methods, in this paper, we propose a novel parameter optimizing paradigm, parameter hierarchical optimization (PHO) method, for the task of VI-ReID. It allows part of parameters to be directly optimized without any training, which narrows the search space of parameters and makes the whole network more easier to be trained. Specifically, we first divide the parameters into different types, and then introduce a self-adaptive alignment strategy (SAS) to automatically align the visible and infrared images through transformation. Considering that features in different dimension have varying importance, we develop an auto-weighted alignment learning (AAL) module that can automatically weight features according to their importance. Importantly, in the alignment process of SAS and AAL, all the parameters are immediately optimized with optimization principles rather than training the whole network, which yields a better parameter training manner. Furthermore, we establish the cross-modality consistent learning (CCL) loss to extract discriminative person representations with translation consistency. We provide both theoretical justification and empirical evidence that our proposed PHO method outperform existing VI-reID approaches.","sentences":["Visible-infrared person re-identification (VI-reID) aims at matching cross-modality pedestrian images captured by disjoint visible or infrared cameras.","Existing methods alleviate the cross-modality discrepancies via designing different kinds of network architectures.","Different from available methods, in this paper, we propose a novel parameter optimizing paradigm, parameter hierarchical optimization (PHO) method, for the task of VI-ReID.","It allows part of parameters to be directly optimized without any training, which narrows the search space of parameters and makes the whole network more easier to be trained.","Specifically, we first divide the parameters into different types, and then introduce a self-adaptive alignment strategy (SAS) to automatically align the visible and infrared images through transformation.","Considering that features in different dimension have varying importance, we develop an auto-weighted alignment learning (AAL) module that can automatically weight features according to their importance.","Importantly, in the alignment process of SAS and AAL, all the parameters are immediately optimized with optimization principles rather than training the whole network, which yields a better parameter training manner.","Furthermore, we establish the cross-modality consistent learning (CCL) loss to extract discriminative person representations with translation consistency.","We provide both theoretical justification and empirical evidence that our proposed PHO method outperform existing VI-reID approaches."],"url":"http://arxiv.org/abs/2404.07930v1","category":"cs.CV"}
{"created":"2024-04-11 17:26:37","title":"The expected kinematic matter dipole is robust against source evolution","abstract":"Recent measurements using catalogues of quasars and radio galaxies have shown that the dipole anisotropy in the large-scale distribution of matter is about twice as large as is expected in the standard $\\Lambda$CDM model, indeed in any cosmology based on the Friedman-Lema\\^itre-Robertson-Walker (FLRW) metric. This expectation is based on the kinematic interpretation of the dipole anisotropy of the cosmic microwave background, i.e. as arising due to our local peculiar velocity. The effect of aberration and Doppler boosting on the projected number counts on the sky of cosmologically distant objects in a flux-limited catalogue can then be calculated and confronted with observations. This fundamental consistency test of FLRW models proposed by Ellis & Baldwin (1984) was revisited by Dalang & Bonvin (2022) [arXiv:2111.03616] who argued that redshift evolution of the sources can significantly affect the expected matter dipole. In this note we demonstrate that the Ellis & Baldwin test is in fact robust to such effects, hence the > 5$\\sigma$ dipole anomaly uncovered by Secrest et al. (2021, 2022) [arXiv:2009.14826, arXiv:2206.05624] remains an outstanding challenge to the $\\Lambda$CDM model.","sentences":["Recent measurements using catalogues of quasars and radio galaxies have shown that the dipole anisotropy in the large-scale distribution of matter is about twice as large as is expected in the standard $\\Lambda$CDM model, indeed in any cosmology based on the Friedman-Lema\\^itre-Robertson-Walker (FLRW) metric.","This expectation is based on the kinematic interpretation of the dipole anisotropy of the cosmic microwave background, i.e. as arising due to our local peculiar velocity.","The effect of aberration and Doppler boosting on the projected number counts on the sky of cosmologically distant objects in a flux-limited catalogue can then be calculated and confronted with observations.","This fundamental consistency test of FLRW models proposed by Ellis & Baldwin (1984) was revisited by Dalang & Bonvin (2022)","[arXiv:2111.03616] who argued that redshift evolution of the sources can significantly affect the expected matter dipole.","In this note we demonstrate that the Ellis & Baldwin test is in fact robust to such effects, hence the > 5$\\sigma$ dipole anomaly uncovered by Secrest et al.","(2021, 2022)","[arXiv:2009.14826, arXiv:2206.05624] remains an outstanding challenge to the $\\Lambda$CDM model."],"url":"http://arxiv.org/abs/2404.07929v1","category":"astro-ph.CO"}
{"created":"2024-04-11 17:20:57","title":"Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation","abstract":"In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized. Therefore, we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI community's methods for working with data.","sentences":["In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale.","Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks.","Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied.","This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized.","Therefore, we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation.","Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research.","We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI community's methods for working with data."],"url":"http://arxiv.org/abs/2404.07926v1","category":"cs.HC"}
{"created":"2024-04-11 17:15:15","title":"Non-perturbative quantum gravity denounces singular Black Holes","abstract":"Although General Relativity predicts the presence of a singularity inside of a Black Hole, it is not a complete theory of gravity. A real structure of a Black Hole interior near an expected singularity depends on the UV completion of gravity. In this paper, we establish that the question whether singular spherically symmetric solutions are absent is governed by the functional form of a non-perturbative graviton propagator. We explicitly show in a framework of a ghost-free infinite derivative gravity that for the graviton propagator of an exponential form favored by the unitarity a singularity is not possible unless an unphysical situation when the total mass of the Black Hole is infinite is considered.","sentences":["Although General Relativity predicts the presence of a singularity inside of a Black Hole, it is not a complete theory of gravity.","A real structure of a Black Hole interior near an expected singularity depends on the UV completion of gravity.","In this paper, we establish that the question whether singular spherically symmetric solutions are absent is governed by the functional form of a non-perturbative graviton propagator.","We explicitly show in a framework of a ghost-free infinite derivative gravity that for the graviton propagator of an exponential form favored by the unitarity a singularity is not possible unless an unphysical situation when the total mass of the Black Hole is infinite is considered."],"url":"http://arxiv.org/abs/2404.07925v1","category":"hep-th"}
{"created":"2024-04-11 17:10:08","title":"A Bayesian Estimator of Sample Size","abstract":"We consider a Bayesian estimator of sample size (BESS) and an application to oncology dose optimization clinical trials. BESS is built upon balancing a trio of Sample size, Evidence from observed data, and Confidence in posterior inference. It uses a simple logic of \"given the evidence from data, a specific sample size can achieve a degree of confidence in the posterior inference.\" The key distinction between BESS and standard sample size estimation (SSE) is that SSE, typically based on Frequentist inference, specifies the true parameters values in its calculation while BESS assumes a possible outcome from the observed data. As a result, the calibration of the sample size is not based on Type I or Type II error rates, but on posterior probabilities. We argue that BESS leads to a more interpretable statement for investigators, and can easily accommodates prior information as well as sample size re-estimation. We explore its performance in comparison to SSE and demonstrate its usage through a case study of oncology optimization trial. BESS can be applied to general hypothesis tests. R functions are available at https://ccte.uchicago.edu/bess.","sentences":["We consider a Bayesian estimator of sample size (BESS) and an application to oncology dose optimization clinical trials.","BESS is built upon balancing a trio of Sample size, Evidence from observed data, and Confidence in posterior inference.","It uses a simple logic of \"given the evidence from data, a specific sample size can achieve a degree of confidence in the posterior inference.\"","The key distinction between BESS and standard sample size estimation (SSE) is that SSE, typically based on Frequentist inference, specifies the true parameters values in its calculation while BESS assumes a possible outcome from the observed data.","As a result, the calibration of the sample size is not based on Type I or Type II error rates, but on posterior probabilities.","We argue that BESS leads to a more interpretable statement for investigators, and can easily accommodates prior information as well as sample size re-estimation.","We explore its performance in comparison to SSE and demonstrate its usage through a case study of oncology optimization trial.","BESS can be applied to general hypothesis tests.","R functions are available at https://ccte.uchicago.edu/bess."],"url":"http://arxiv.org/abs/2404.07923v1","category":"stat.ME"}
{"created":"2024-04-11 17:05:50","title":"AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs","abstract":"As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.","sentences":["As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative.","Despite significant strides toward safety alignment, recent work GCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs.","In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps.","Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds.","AmpleGCG achieves near 100\\% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines.","More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99\\% ASR on the latest GPT-3.5.","To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs.","In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend."],"url":"http://arxiv.org/abs/2404.07921v1","category":"cs.CL"}
{"created":"2024-04-11 17:04:55","title":"Low-rank Adaptation for Spatio-Temporal Forecasting","abstract":"Spatio-temporal forecasting is crucial in real-world dynamic systems, predicting future changes using historical data from diverse locations. Existing methods often prioritize the development of intricate neural networks to capture the complex dependencies of the data, yet their accuracy fails to show sustained improvement. Besides, these methods also overlook node heterogeneity, hindering customized prediction modules from handling diverse regional nodes effectively. In this paper, our goal is not to propose a new model but to present a novel low-rank adaptation framework as an off-the-shelf plugin for existing spatial-temporal prediction models, termed ST-LoRA, which alleviates the aforementioned problems through node-level adjustments. Specifically, we first tailor a node adaptive low-rank layer comprising multiple trainable low-rank matrices. Additionally, we devise a multi-layer residual fusion stacking module, injecting the low-rank adapters into predictor modules of various models. Across six real-world traffic datasets and six different types of spatio-temporal prediction models, our approach minimally increases the parameters and training time of the original models by less than 4%, still achieving consistent and sustained performance enhancement.","sentences":["Spatio-temporal forecasting is crucial in real-world dynamic systems, predicting future changes using historical data from diverse locations.","Existing methods often prioritize the development of intricate neural networks to capture the complex dependencies of the data, yet their accuracy fails to show sustained improvement.","Besides, these methods also overlook node heterogeneity, hindering customized prediction modules from handling diverse regional nodes effectively.","In this paper, our goal is not to propose a new model but to present a novel low-rank adaptation framework as an off-the-shelf plugin for existing spatial-temporal prediction models, termed ST-LoRA, which alleviates the aforementioned problems through node-level adjustments.","Specifically, we first tailor a node adaptive low-rank layer comprising multiple trainable low-rank matrices.","Additionally, we devise a multi-layer residual fusion stacking module, injecting the low-rank adapters into predictor modules of various models.","Across six real-world traffic datasets and six different types of spatio-temporal prediction models, our approach minimally increases the parameters and training time of the original models by less than 4%, still achieving consistent and sustained performance enhancement."],"url":"http://arxiv.org/abs/2404.07919v1","category":"cs.LG"}
{"created":"2024-04-11 16:59:54","title":"DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation","abstract":"This research introduces DesignQA, a novel benchmark aimed at evaluating the proficiency of multimodal large language models (MLLMs) in comprehending and applying engineering requirements in technical documentation. Developed with a focus on real-world engineering challenges, DesignQA uniquely combines multimodal data-including textual design requirements, CAD images, and engineering drawings-derived from the Formula SAE student competition. Different from many existing MLLM benchmarks, DesignQA contains document-grounded visual questions where the input image and input document come from different sources. The benchmark features automatic evaluation metrics and is divided into segments-Rule Comprehension, Rule Compliance, and Rule Extraction-based on tasks that engineers perform when designing according to requirements. We evaluate state-of-the-art models like GPT4 and LLaVA against the benchmark, and our study uncovers the existing gaps in MLLMs' abilities to interpret complex engineering documentation. Key findings suggest that while MLLMs demonstrate potential in navigating technical documents, substantial limitations exist, particularly in accurately extracting and applying detailed requirements to engineering designs. This benchmark sets a foundation for future advancements in AI-supported engineering design processes. DesignQA is publicly available at: https://github.com/anniedoris/design_qa/.","sentences":["This research introduces DesignQA, a novel benchmark aimed at evaluating the proficiency of multimodal large language models (MLLMs) in comprehending and applying engineering requirements in technical documentation.","Developed with a focus on real-world engineering challenges, DesignQA uniquely combines multimodal data-including textual design requirements, CAD images, and engineering drawings-derived from the Formula SAE student competition.","Different from many existing MLLM benchmarks, DesignQA contains document-grounded visual questions where the input image and input document come from different sources.","The benchmark features automatic evaluation metrics and is divided into segments-Rule Comprehension, Rule Compliance, and Rule Extraction-based on tasks that engineers perform when designing according to requirements.","We evaluate state-of-the-art models like GPT4 and LLaVA against the benchmark, and our study uncovers the existing gaps in MLLMs' abilities to interpret complex engineering documentation.","Key findings suggest that while MLLMs demonstrate potential in navigating technical documents, substantial limitations exist, particularly in accurately extracting and applying detailed requirements to engineering designs.","This benchmark sets a foundation for future advancements in AI-supported engineering design processes.","DesignQA is publicly available at: https://github.com/anniedoris/design_qa/."],"url":"http://arxiv.org/abs/2404.07917v1","category":"cs.AI"}
{"created":"2024-04-11 16:59:31","title":"A Novel Optimization-Based Collision Avoidance For Autonomous On-Orbit Assembly","abstract":"The collision avoidance constraints are prominent as non-convex, non-differentiable, and challenging when defined in optimization-based motion planning problems. To overcome these issues, this paper presents a novel non-conservative collision avoidance technique using the notion of convex optimization to establish the distance between robotic spacecraft and space structures for autonomous on-orbit assembly operations. The proposed technique defines each ellipsoidal- and polyhedral-shaped object as the union of convex compact sets, each represented non-conservatively by a real-valued convex function. Then, the functions are introduced as a set of constraints to a convex optimization problem to produce a new set of differentiable constraints resulting from the optimality conditions. These new constraints are later fed into an optimal control problem to enforce collision avoidance where the motion planning for the autonomous on-orbit assembly takes place. Numerical experiments for two assembly scenarios in tight environments are presented to demonstrate the capability and effectiveness of the proposed technique. The results show that this framework leads to optimal non-conservative trajectories for robotic spacecraft in tight environments. Although developed for autonomous on-orbit assembly, this technique could be used for any generic motion planning problem where collision avoidance is crucial.","sentences":["The collision avoidance constraints are prominent as non-convex, non-differentiable, and challenging when defined in optimization-based motion planning problems.","To overcome these issues, this paper presents a novel non-conservative collision avoidance technique using the notion of convex optimization to establish the distance between robotic spacecraft and space structures for autonomous on-orbit assembly operations.","The proposed technique defines each ellipsoidal- and polyhedral-shaped object as the union of convex compact sets, each represented non-conservatively by a real-valued convex function.","Then, the functions are introduced as a set of constraints to a convex optimization problem to produce a new set of differentiable constraints resulting from the optimality conditions.","These new constraints are later fed into an optimal control problem to enforce collision avoidance where the motion planning for the autonomous on-orbit assembly takes place.","Numerical experiments for two assembly scenarios in tight environments are presented to demonstrate the capability and effectiveness of the proposed technique.","The results show that this framework leads to optimal non-conservative trajectories for robotic spacecraft in tight environments.","Although developed for autonomous on-orbit assembly, this technique could be used for any generic motion planning problem where collision avoidance is crucial."],"url":"http://arxiv.org/abs/2404.07916v1","category":"cs.RO"}
{"created":"2024-04-11 16:57:15","title":"Quantum computation in fermionic thermal field theories","abstract":"Thermal properties of quantum fields at finite temperature are crucial to understanding strongly interacting matter and recent development in quantum computing has provided an alternative and promising avenue of study. In this work, we study thermal field theories involving only fermions using quantum algorithms. We first delve into the presentations of fermion fields via qubits on digital quantum computers alongside the quantum algorithms such as quantum imaginary time evolutions employed to evaluate thermal properties of generic quantum field theories. Specifically, we show numerical results such as the thermal distribution and the energy density of thermal field theories for Majorana fermions in 1+1 dimensions using quantum simulators. In addition to free field theory, we also study the effects of interactions resulting from coupling with a spatially homogeneous Majorana field. In both cases, we show analytically that thermal properties of the system can be described using phase-space distributions, and the quantum simulation results agree with analytical and semiclassical expectations. Our work is an important step to understand thermal fixed points, preparing for quantum simulation of thermalization in real time.","sentences":["Thermal properties of quantum fields at finite temperature are crucial to understanding strongly interacting matter and recent development in quantum computing has provided an alternative and promising avenue of study.","In this work, we study thermal field theories involving only fermions using quantum algorithms.","We first delve into the presentations of fermion fields via qubits on digital quantum computers alongside the quantum algorithms such as quantum imaginary time evolutions employed to evaluate thermal properties of generic quantum field theories.","Specifically, we show numerical results such as the thermal distribution and the energy density of thermal field theories for Majorana fermions in 1+1 dimensions using quantum simulators.","In addition to free field theory, we also study the effects of interactions resulting from coupling with a spatially homogeneous Majorana field.","In both cases, we show analytically that thermal properties of the system can be described using phase-space distributions, and the quantum simulation results agree with analytical and semiclassical expectations.","Our work is an important step to understand thermal fixed points, preparing for quantum simulation of thermalization in real time."],"url":"http://arxiv.org/abs/2404.07912v1","category":"hep-ph"}
{"created":"2024-04-11 16:51:58","title":"Distributions and Collision Rates of ALP Stars in the Milky Way","abstract":"We apply current analytical knowledge on the characteristic mass and linear evolution of miniclusters down to redshift $z=0$ to the hypothetical minicluster distribution of the Milky Way. Using the mass-radius relation and a core-halo relation for stable soliton solutions composed of axion-like particles (ALPs), we connect the galactic minicluster mass distribution to that of their ALP star cores. We consider different temperature evolutions of the ALP field with masses in the range $10^{-12}\\,\\mathrm{eV} \\leq m_a \\leq 10^{-3}\\,$eV and infer the abundance and properties of QCD axion- and ALP stars in our galaxy. We re-evaluate detection prospects for collisions of neutron stars with both ALP stars and miniclusters as well as relativistic ALP bursts, so-called Bosenovae. Our analysis shows that the collision rates between miniclusters and neutron stars can become as large as $\\sim 10^5\\,$yr$^{-1}$ galaxy$^{-1}$, but that the fraction of encounters that can lead to resonance between ALP mass and magnetosphere plasma frequency is generally well below $\\sim 1\\,$yr$^{-1}$ galaxy$^{-1}$, depending on the ALP model. We confirm previous results that merger rates of ALP stars are extremely small $< 10^{-12}\\,$yr$^{-1}$ galaxy$^{-1}$, while their host miniclusters can merge much more frequently, up to $\\sim 10^3\\,$yr$^{-1}$ galaxy$^{-1}$ for the QCD axion. We find that Bosenovae and parametric resonance are much more likely to lead to observable signatures than neutron star encounters. We also suggest that a combination of accretion and parametric resonance can lead to observable radio lines for a wide range of ALP masses $m_a$ and photon-couplings $g_{a\\gamma\\gamma}$.","sentences":["We apply current analytical knowledge on the characteristic mass and linear evolution of miniclusters down to redshift $z=0$ to the hypothetical minicluster distribution of the Milky Way.","Using the mass-radius relation and a core-halo relation for stable soliton solutions composed of axion-like particles (ALPs), we connect the galactic minicluster mass distribution to that of their ALP star cores.","We consider different temperature evolutions of the ALP field with masses in the range $10^{-12}\\,\\mathrm{eV} \\leq m_a","\\leq 10^{-3}\\,$eV and infer the abundance and properties of QCD axion- and ALP stars in our galaxy.","We re-evaluate detection prospects for collisions of neutron stars with both ALP stars and miniclusters as well as relativistic ALP bursts, so-called Bosenovae.","Our analysis shows that the collision rates between miniclusters and neutron stars can become as large as $\\sim 10^5\\,$yr$^{-1}$ galaxy$^{-1}$, but that the fraction of encounters that can lead to resonance between ALP mass and magnetosphere plasma frequency is generally well below $\\sim 1\\,$yr$^{-1}$ galaxy$^{-1}$, depending on the ALP model.","We confirm previous results that merger rates of ALP stars are extremely small $< 10^{-12}\\,$yr$^{-1}$ galaxy$^{-1}$, while their host miniclusters can merge much more frequently, up to $\\sim 10^3\\,$yr$^{-1}$ galaxy$^{-1}$ for the QCD axion.","We find that Bosenovae and parametric resonance are much more likely to lead to observable signatures than neutron star encounters.","We also suggest that a combination of accretion and parametric resonance can lead to observable radio lines for a wide range of ALP masses $m_a$ and photon-couplings $g_{a\\gamma\\gamma}$."],"url":"http://arxiv.org/abs/2404.07908v1","category":"astro-ph.CO"}
{"created":"2024-04-11 16:45:52","title":"Poincar\u00e9 disk as a model of squeezed states of a harmonic oscillator","abstract":"Single-mode squeezed states exhibit a direct correspondence with points on the Poincar\\'e disk. In this study, we delve into this correspondence and describe the motions of the disk generated by a quadratic Hamiltonian. This provides a geometric representation of squeezed states and their evolution. We discuss applications in bang-bang and adiabatic control problems involving squeezed states.","sentences":["Single-mode squeezed states exhibit a direct correspondence with points on the Poincar\\'e disk.","In this study, we delve into this correspondence and describe the motions of the disk generated by a quadratic Hamiltonian.","This provides a geometric representation of squeezed states and their evolution.","We discuss applications in bang-bang and adiabatic control problems involving squeezed states."],"url":"http://arxiv.org/abs/2404.07905v1","category":"math-ph"}
{"created":"2024-04-11 16:39:00","title":"High-Dimension Human Value Representation in Large Language Models","abstract":"The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.","sentences":["The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences.","Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release.","There is also a need for model alignment without a costly large scale human annotation effort.","We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data.","Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources.","Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling."],"url":"http://arxiv.org/abs/2404.07900v1","category":"cs.CL"}
{"created":"2024-04-11 16:37:01","title":"Anomaly Detection in Power Grids via Context-Agnostic Learning","abstract":"An important tool grid operators use to safeguard against failures, whether naturally occurring or malicious, involves detecting anomalies in the power system SCADA data. In this paper, we aim to solve a real-time anomaly detection problem. Given time-series measurement values coming from a fixed set of sensors on the grid, can we identify anomalies in the network topology or measurement data? Existing methods, primarily optimization-based, mostly use only a single snapshot of the measurement values and do not scale well with the network size. Recent data-driven ML techniques have shown promise by using a combination of current and historical data for anomaly detection but generally do not consider physical attributes like the impact of topology or load/generation changes on sensor measurements and thus cannot accommodate regular context-variability in the historical data. To address this gap, we propose a novel context-aware anomaly detection algorithm, GridCAL, that considers the effect of regular topology and load/generation changes. This algorithm converts the real-time power flow measurements to context-agnostic values, which allows us to analyze measurement coming from different grid contexts in an aggregate fashion, enabling us to derive a unified statistical model that becomes the basis of anomaly detection. Through numerical simulations on networks up to 2383 nodes, we show that our approach is accurate, outperforming state-of-the-art approaches, and is computationally efficient.","sentences":["An important tool grid operators use to safeguard against failures, whether naturally occurring or malicious, involves detecting anomalies in the power system SCADA data.","In this paper, we aim to solve a real-time anomaly detection problem.","Given time-series measurement values coming from a fixed set of sensors on the grid, can we identify anomalies in the network topology or measurement data?","Existing methods, primarily optimization-based, mostly use only a single snapshot of the measurement values and do not scale well with the network size.","Recent data-driven ML techniques have shown promise by using a combination of current and historical data for anomaly detection but generally do not consider physical attributes like the impact of topology or load/generation changes on sensor measurements and thus cannot accommodate regular context-variability in the historical data.","To address this gap, we propose a novel context-aware anomaly detection algorithm, GridCAL, that considers the effect of regular topology and load/generation changes.","This algorithm converts the real-time power flow measurements to context-agnostic values, which allows us to analyze measurement coming from different grid contexts in an aggregate fashion, enabling us to derive a unified statistical model that becomes the basis of anomaly detection.","Through numerical simulations on networks up to 2383 nodes, we show that our approach is accurate, outperforming state-of-the-art approaches, and is computationally efficient."],"url":"http://arxiv.org/abs/2404.07898v1","category":"cs.LG"}
{"created":"2024-04-11 16:30:40","title":"Actionable forecasting as a determinant of function in noisy biological systems","abstract":"Continuous adaptation to variable environments is crucial for the survival of living organisms. Here, we analyze how adaptation, forecasting, and resource mobilization towards a target state, termed actionability, interact to determine biological function. We develop a general theory and show that it is possible for organisms to continuously track their optimal state in a dynamic environment by adapting towards an actionable target that incorporates just current information on the optimal state and its rate of change. If the environmental information is precise and readily actionable, it is possible to implement perfect tracking without anticipatory mechanisms, irrespective of the adaptation rate. In contrast, predictive functions, like those of circadian rhythms, are beneficial if sensing the environment is slow or unreliable, as they allow better adaptation with fewer resources. To explore potential actionable forecasting mechanisms, we develop a general approach that implements the adaptation dynamics with forecasting through a dynamics-informed neural network.","sentences":["Continuous adaptation to variable environments is crucial for the survival of living organisms.","Here, we analyze how adaptation, forecasting, and resource mobilization towards a target state, termed actionability, interact to determine biological function.","We develop a general theory and show that it is possible for organisms to continuously track their optimal state in a dynamic environment by adapting towards an actionable target that incorporates just current information on the optimal state and its rate of change.","If the environmental information is precise and readily actionable, it is possible to implement perfect tracking without anticipatory mechanisms, irrespective of the adaptation rate.","In contrast, predictive functions, like those of circadian rhythms, are beneficial if sensing the environment is slow or unreliable, as they allow better adaptation with fewer resources.","To explore potential actionable forecasting mechanisms, we develop a general approach that implements the adaptation dynamics with forecasting through a dynamics-informed neural network."],"url":"http://arxiv.org/abs/2404.07895v1","category":"q-bio.CB"}
{"created":"2024-04-11 16:27:50","title":"Typical blocks of the category $\\mathcal O$ and Whittaker modules for Takiff superalgebras","abstract":"We study the simplicity of Kac induced modules over the $\\ell$-th Takiff superalgebras $\\widetilde{\\mathfrak g}_\\ell:= \\widetilde{\\mathfrak g}\\otimes \\mathbb C[\\theta]/(\\theta^{\\ell+1})$, for $\\ell>0$, associated with the Lie superalgebras $\\widetilde{\\mathfrak g}$ of type I. We formulate a general notion of typical weights and typical Jordan blocks of the category $\\mathcal O$ for $\\widetilde{\\mathfrak g}_\\ell$ associated with Lie superalgebras $\\mathfrak{gl}(m|n)$, $\\mathfrak{osp}(2|2n)$ and $\\mathfrak{pe}(n)$. For Lie superalgebras $\\mathfrak{gl}(m|n)$ and $\\mathfrak{osp}(2|2n)$, we establish an equivalence from an arbitrary typical Jordan block of the category $\\mathcal O$ for $\\widetilde{\\mathfrak g}_\\ell$ to a Jordan block of the category $\\mathcal O$ for the even subalgebra of $\\widetilde{\\mathfrak g}_\\ell$. This provides a solution to the problem of determining the composition multiplicities of the Verma modules over $\\widetilde{\\mathfrak g}_\\ell$ with typical highest weights. We also investigate non-singular Whittaker modules over these Takiff superalgebras. In particular, we obtain a classification of non-singular simple Whittaker modules and a criterion for simplicity of non-singular standard Whittaker modules.","sentences":["We study the simplicity of Kac induced modules over the $\\ell$-th Takiff superalgebras $\\widetilde{\\mathfrak g}_\\ell:= \\widetilde{\\mathfrak g}\\otimes","\\mathbb C[\\theta]/(\\theta^{\\ell+1})$, for $\\ell>0$, associated with the Lie superalgebras $\\widetilde{\\mathfrak g}$ of type I.","We formulate a general notion of typical weights and typical Jordan blocks of the category $\\mathcal O$ for $\\widetilde{\\mathfrak g}_\\ell$ associated with Lie superalgebras $\\mathfrak{gl}(m|n)$, $\\mathfrak{osp}(2|2n)$ and $\\mathfrak{pe}(n)$. For Lie superalgebras $\\mathfrak{gl}(m|n)$ and $\\mathfrak{osp}(2|2n)$, we establish an equivalence from an arbitrary typical Jordan block of the category $\\mathcal O$ for $\\widetilde{\\mathfrak g}_\\ell$ to a Jordan block of the category $\\mathcal O$ for the even subalgebra of $\\widetilde{\\mathfrak g}_\\ell$. This provides a solution to the problem of determining the composition multiplicities of the Verma modules over $\\widetilde{\\mathfrak g}_\\ell$ with typical highest weights.","We also investigate non-singular Whittaker modules over these Takiff superalgebras.","In particular, we obtain a classification of non-singular simple Whittaker modules and a criterion for simplicity of non-singular standard Whittaker modules."],"url":"http://arxiv.org/abs/2404.07894v1","category":"math.RT"}
{"created":"2024-04-11 16:24:03","title":"A note on special cubic fourfolds of discriminant 14 and non-minimal K3 surfaces of degree 10","abstract":"We prove that a general cubic in the Hassett divisor $\\mathcal{C}_{14}$ of special cubic fourfolds of discriminant $14$ contains a non-minimal K3 surface of degree $10$ containing two skew $(-1)$-lines and contained in a smooth quadric hypersurface $Q^4\\subseteq \\mathbb{P}^5$, but not contained in any other (possibly of lower rank) quadric hypersurface.","sentences":["We prove that a general cubic in the Hassett divisor $\\mathcal{C}_{14}$ of special cubic fourfolds of discriminant $14$ contains a non-minimal K3 surface of degree $10$ containing two skew $(-1)$-lines and contained in a smooth quadric hypersurface $Q^4\\subseteq \\mathbb{P}^5$, but not contained in any other (possibly of lower rank) quadric hypersurface."],"url":"http://arxiv.org/abs/2404.07891v1","category":"math.AG"}
{"created":"2024-04-11 16:24:01","title":"Non-Markovian dynamics with a giant atom coupled to a semi-infinite photonic waveguide","abstract":"We study the non-Markovian dynamics of a two-level giant atom interacting with a one-dimensional semi-infinite waveguide through multiple coupling points, where a perfect mirror is located at the endpoint of the waveguide. The system enters a non-Markovian process when the travel time of the photon between adjacent coupling points is sufficiently large compared to the inverse of the bare relaxation rate of the giant atom. The photon released by the spontaneous emission of the atom transfers between multiple coupling points through the waveguide or is reabsorbed by the atom with the photon emitted via the atom having completed the round trip after reflection of the mirror, which leads to the photon being trapped and forming bound states. We find that three different types of bound states can be formed in the system, containing the static bound states with no inversion of population, the periodic equal amplitude oscillation with two bound states, and the periodic non-equal amplitude oscillation with three bound states. The physical origins of three bound states formation are revealed. Moreover, we consider the influences of the dissipation of unwanted modes and dephasing on the bound states. Finally, we extend the system to a more general case involving many giant atoms coupled into a one-dimensional semi-infinite waveguide. The obtained set of delay differential equations for the giant atoms might open a way to better understand the non-Markovian dynamics of many giant atoms coupled to a semi-infinite waveguide.","sentences":["We study the non-Markovian dynamics of a two-level giant atom interacting with a one-dimensional semi-infinite waveguide through multiple coupling points, where a perfect mirror is located at the endpoint of the waveguide.","The system enters a non-Markovian process when the travel time of the photon between adjacent coupling points is sufficiently large compared to the inverse of the bare relaxation rate of the giant atom.","The photon released by the spontaneous emission of the atom transfers between multiple coupling points through the waveguide or is reabsorbed by the atom with the photon emitted via the atom having completed the round trip after reflection of the mirror, which leads to the photon being trapped and forming bound states.","We find that three different types of bound states can be formed in the system, containing the static bound states with no inversion of population, the periodic equal amplitude oscillation with two bound states, and the periodic non-equal amplitude oscillation with three bound states.","The physical origins of three bound states formation are revealed.","Moreover, we consider the influences of the dissipation of unwanted modes and dephasing on the bound states.","Finally, we extend the system to a more general case involving many giant atoms coupled into a one-dimensional semi-infinite waveguide.","The obtained set of delay differential equations for the giant atoms might open a way to better understand the non-Markovian dynamics of many giant atoms coupled to a semi-infinite waveguide."],"url":"http://arxiv.org/abs/2404.07890v1","category":"quant-ph"}
{"created":"2024-04-11 16:18:37","title":"On the Performance of Jerk-Constrained Time-Optimal Trajectory Planning for Industrial Manipulators","abstract":"Jerk-constrained trajectories offer a wide range of advantages that collectively improve the performance of robotic systems, including increased energy efficiency, durability, and safety. In this paper, we present a novel approach to jerk-constrained time-optimal trajectory planning (TOTP), which follows a specified path while satisfying up to third-order constraints to ensure safety and smooth motion. One significant challenge in jerk-constrained TOTP is a non-convex formulation arising from the inclusion of third-order constraints. Approximating inequality constraints can be particularly challenging because the resulting solutions may violate the actual constraints. We address this problem by leveraging convexity within the proposed formulation to form conservative inequality constraints. We then obtain the desired trajectories by solving an $\\boldsymbol n$-dimensional Sequential Linear Program (SLP) iteratively until convergence. Lastly, we evaluate in a real robot the performance of trajectories generated with and without jerk limits in terms of peak power, torque efficiency, and tracking capability.","sentences":["Jerk-constrained trajectories offer a wide range of advantages that collectively improve the performance of robotic systems, including increased energy efficiency, durability, and safety.","In this paper, we present a novel approach to jerk-constrained time-optimal trajectory planning (TOTP), which follows a specified path while satisfying up to third-order constraints to ensure safety and smooth motion.","One significant challenge in jerk-constrained TOTP is a non-convex formulation arising from the inclusion of third-order constraints.","Approximating inequality constraints can be particularly challenging because the resulting solutions may violate the actual constraints.","We address this problem by leveraging convexity within the proposed formulation to form conservative inequality constraints.","We then obtain the desired trajectories by solving an $\\boldsymbol n$-dimensional Sequential Linear Program (SLP) iteratively until convergence.","Lastly, we evaluate in a real robot the performance of trajectories generated with and without jerk limits in terms of peak power, torque efficiency, and tracking capability."],"url":"http://arxiv.org/abs/2404.07889v1","category":"cs.RO"}
{"created":"2024-04-11 16:18:01","title":"Slowly rotating charges from Weyl double copy for Kerr black hole with Chern-Simons correction","abstract":"The Weyl double copy builds the relation between gauge theory and gravity theory, especially the correspondence between gauge solutions and gravity solutions. In this paper, we obtain the slowly rotating charge solutions from Weyl double copy for the Kerr black hole with small Chern-Simons correction. Based on the Weyl double copy relation, for the Petrov type D solution, we find the additional correction to the electromagnetic field strength tensor of rotating charge. For the Petrov type I solution, we find that the additional electromagnetic field strength tensors have the exogenous properties, while the total sources vanish at the leading order.","sentences":["The Weyl double copy builds the relation between gauge theory and gravity theory, especially the correspondence between gauge solutions and gravity solutions.","In this paper, we obtain the slowly rotating charge solutions from Weyl double copy for the Kerr black hole with small Chern-Simons correction.","Based on the Weyl double copy relation, for the Petrov type D solution, we find the additional correction to the electromagnetic field strength tensor of rotating charge.","For the Petrov type I solution, we find that the additional electromagnetic field strength tensors have the exogenous properties, while the total sources vanish at the leading order."],"url":"http://arxiv.org/abs/2404.07888v1","category":"gr-qc"}
{"created":"2024-04-11 16:17:36","title":"Context-aware Video Anomaly Detection in Long-Term Datasets","abstract":"Video anomaly detection research is generally evaluated on short, isolated benchmark videos only a few minutes long. However, in real-world environments, security cameras observe the same scene for months or years at a time, and the notion of anomalous behavior critically depends on context, such as the time of day, day of week, or schedule of events. Here, we propose a context-aware video anomaly detection algorithm, Trinity, specifically targeted to these scenarios. Trinity is especially well-suited to crowded scenes in which individuals cannot be easily tracked, and anomalies are due to speed, direction, or absence of group motion. Trinity is a contrastive learning framework that aims to learn alignments between context, appearance, and motion, and uses alignment quality to classify videos as normal or anomalous. We evaluate our algorithm on both conventional benchmarks and a public webcam-based dataset we collected that spans more than three months of activity.","sentences":["Video anomaly detection research is generally evaluated on short, isolated benchmark videos only a few minutes long.","However, in real-world environments, security cameras observe the same scene for months or years at a time, and the notion of anomalous behavior critically depends on context, such as the time of day, day of week, or schedule of events.","Here, we propose a context-aware video anomaly detection algorithm, Trinity, specifically targeted to these scenarios.","Trinity is especially well-suited to crowded scenes in which individuals cannot be easily tracked, and anomalies are due to speed, direction, or absence of group motion.","Trinity is a contrastive learning framework that aims to learn alignments between context, appearance, and motion, and uses alignment quality to classify videos as normal or anomalous.","We evaluate our algorithm on both conventional benchmarks and a public webcam-based dataset we collected that spans more than three months of activity."],"url":"http://arxiv.org/abs/2404.07887v1","category":"cs.CV"}
{"created":"2024-04-11 16:15:53","title":"Matroidal polynomials, their singularities, and applications to Feynman diagrams","abstract":"Given a matroid or flag of matroids we introduce several broad classes of polynomials satisfying Deletion-Contraction identities, and study their singularities.   There are three main families of polynomials captured by our approach: matroidal polynomials on a matroid (including matroid basis polynomials, configuration polynomials, Tutte polynomials); flag matroidal polynomials on a flag matroid; and Feynman integrands. The last class includes under general kinematics the inhomogeneous Feynman diagram polynomials which naturally arise in the Lee--Pomeransky form of the Feynman integral attached to a Feynman diagram.   Assuming that the primary underlying matroid is connected and of positive rank (and in the flag case, has rank at least two), we show: a) in positive characteristic, homogeneous matroidal polynomials are strongly $F$-regular; b) over an algebraically closed field of characteristic zero, the associated jet schemes of (flag) matroidal polynomials as well as those of Feynman integrands are irreducible. Consequently, all these polynomials have rational singularities (or are smooth).","sentences":["Given a matroid or flag of matroids we introduce several broad classes of polynomials satisfying Deletion-Contraction identities, and study their singularities.   ","There are three main families of polynomials captured by our approach: matroidal polynomials on a matroid (including matroid basis polynomials, configuration polynomials, Tutte polynomials); flag matroidal polynomials on a flag matroid; and Feynman integrands.","The last class includes under general kinematics the inhomogeneous Feynman diagram polynomials which naturally arise in the Lee--Pomeransky form of the Feynman integral attached to a Feynman diagram.   ","Assuming that the primary underlying matroid is connected and of positive rank (and in the flag case, has rank at least two), we show: a) in positive characteristic, homogeneous matroidal polynomials are strongly $F$-regular; b) over an algebraically closed field of characteristic zero, the associated jet schemes of (flag) matroidal polynomials as well as those of Feynman integrands are irreducible.","Consequently, all these polynomials have rational singularities (or are smooth)."],"url":"http://arxiv.org/abs/2404.07885v1","category":"math.AG"}
{"created":"2024-04-11 16:15:33","title":"New horizons for inhomogeneous quenches and Floquet CFT","abstract":"A fruitful avenue in investigating out-of-equilibrium quantum many-body systems is to abruptly change their Hamiltonian and study the subsequent evolution of their quantum state. If this is done once, the setup is called a quench, while if it is done periodically, it is called Floquet driving. We consider the solvable setup of a two-dimensional CFT driven by Hamiltonians built out of conformal symmetry generators: in this case, the quantum dynamics can be understood using two-dimensional geometry. We investigate how the dynamics is reflected in the holographic dual three-dimensional spacetime and find new horizons. We argue that bulk operators behind the new horizons are reconstructable by virtue of modular flow.","sentences":["A fruitful avenue in investigating out-of-equilibrium quantum many-body systems is to abruptly change their Hamiltonian and study the subsequent evolution of their quantum state.","If this is done once, the setup is called a quench, while if it is done periodically, it is called Floquet driving.","We consider the solvable setup of a two-dimensional CFT driven by Hamiltonians built out of conformal symmetry generators: in this case, the quantum dynamics can be understood using two-dimensional geometry.","We investigate how the dynamics is reflected in the holographic dual three-dimensional spacetime and find new horizons.","We argue that bulk operators behind the new horizons are reconstructable by virtue of modular flow."],"url":"http://arxiv.org/abs/2404.07884v1","category":"hep-th"}
{"created":"2024-04-11 16:14:23","title":"Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors","abstract":"Intelligent tutoring systems (ITS) are effective for improving students' learning outcomes. However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization. We present the Apprentice Tutor Builder (ATB) , a platform that simplifies tutor creation and personalization. Instructors can utilize ATB's drag-and-drop tool to build tutor interfaces. Instructors can then interactively train the tutors' underlying AI agent to produce expert models that can solve problems. Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels. We conducted a user study with 14 instructors to evaluate the effectiveness of ATB's design with end users. We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features. With these insights, we identified a set of design recommendations for our platform and others that utilize interactive AI agents for tutor creation and customization.","sentences":["Intelligent tutoring systems (ITS) are effective for improving students' learning outcomes.","However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization.","We present the Apprentice Tutor Builder (ATB) , a platform that simplifies tutor creation and personalization.","Instructors can utilize ATB's drag-and-drop tool to build tutor interfaces.","Instructors can then interactively train the tutors' underlying AI agent to produce expert models that can solve problems.","Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels.","We conducted a user study with 14 instructors to evaluate the effectiveness of ATB's design with end users.","We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features.","With these insights, we identified a set of design recommendations for our platform and others that utilize interactive AI agents for tutor creation and customization."],"url":"http://arxiv.org/abs/2404.07883v1","category":"cs.HC"}
{"created":"2024-04-11 16:10:52","title":"Diagram Analysis of Iterative Algorithms","abstract":"We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.   We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.   The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by \"implementing\" heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.   These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.","sentences":["We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent.","When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams.","Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.   ","We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees.","The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees.","We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.   ","The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field.","We demonstrate the connection with the replica symmetric cavity method by \"implementing\" heuristic physics derivations into rigorous proofs.","We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.   ","These results apply when the iterative algorithm runs for constantly many iterations.","We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix.","We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\\Omega(1)}$ iterations.","We conjecture that this can be extended up to $n^{1/2}$ iterations but no further.","Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory."],"url":"http://arxiv.org/abs/2404.07881v1","category":"cs.CC"}
{"created":"2024-04-11 16:10:44","title":"Analyzing Toxicity in Deep Conversations: A Reddit Case Study","abstract":"Online social media has become increasingly popular in recent years due to its ease of access and ability to connect with others. One of social media's main draws is its anonymity, allowing users to share their thoughts and opinions without fear of judgment or retribution. This anonymity has also made social media prone to harmful content, which requires moderation to ensure responsible and productive use. Several methods using artificial intelligence have been employed to detect harmful content. However, conversation and contextual analysis of hate speech are still understudied. Most promising works only analyze a single text at a time rather than the conversation supporting it. In this work, we employ a tree-based approach to understand how users behave concerning toxicity in public conversation settings. To this end, we collect both the posts and the comment sections of the top 100 posts from 8 Reddit communities that allow profanity, totaling over 1 million responses. We find that toxic comments increase the likelihood of subsequent toxic comments being produced in online conversations. Our analysis also shows that immediate context plays a vital role in shaping a response rather than the original post. We also study the effect of consensual profanity and observe overlapping similarities with non-consensual profanity in terms of user behavior and patterns.","sentences":["Online social media has become increasingly popular in recent years due to its ease of access and ability to connect with others.","One of social media's main draws is its anonymity, allowing users to share their thoughts and opinions without fear of judgment or retribution.","This anonymity has also made social media prone to harmful content, which requires moderation to ensure responsible and productive use.","Several methods using artificial intelligence have been employed to detect harmful content.","However, conversation and contextual analysis of hate speech are still understudied.","Most promising works only analyze a single text at a time rather than the conversation supporting it.","In this work, we employ a tree-based approach to understand how users behave concerning toxicity in public conversation settings.","To this end, we collect both the posts and the comment sections of the top 100 posts from 8 Reddit communities that allow profanity, totaling over 1 million responses.","We find that toxic comments increase the likelihood of subsequent toxic comments being produced in online conversations.","Our analysis also shows that immediate context plays a vital role in shaping a response rather than the original post.","We also study the effect of consensual profanity and observe overlapping similarities with non-consensual profanity in terms of user behavior and patterns."],"url":"http://arxiv.org/abs/2404.07879v1","category":"cs.CL"}
{"created":"2024-04-11 16:10:14","title":"Definability of band structures on posets","abstract":"The idempotent semigroups (bands) that give rise to partial orders by defining $a \\leq b \\iff a \\cdot b = a$ are the \"right-regular\" bands (RRB), which are axiomatized by $x\\cdot y \\cdot x = y \\cdot x$. In this work we consider the class of \"associative posets\", which comprises all partial orders underlying right-regular bands, and study to what extent the ordering determines the possible \"compatible\" band structures and their canonicity.   We show that the class of associative posets in the signature $\\{ \\leq \\}$ is not first-order axiomatizable. We also show that the Axiom of Choice is equivalent over $\\mathit{ZF}$ to the fact that every tree with finite branches is associative. We also present an adjunction between the categories of RRBs and that of associative posets.   We study the smaller class of \"normal\" posets (corresponding to right-normal bands) and give a structural characterization.   As an application of the order-theoretic perspective on bands, we generalize results by the third author, obtaining \"inner\" direct product representations for RRBs having a central (commuting) element.","sentences":["The idempotent semigroups (bands) that give rise to partial orders by defining $a \\leq b \\iff a \\cdot b = a$ are the \"right-regular\" bands (RRB), which are axiomatized by $x\\cdot y \\cdot x = y \\cdot x$.","In this work we consider the class of \"associative posets\", which comprises all partial orders underlying right-regular bands, and study to what extent the ordering determines the possible \"compatible\" band structures and their canonicity.   ","We show that the class of associative posets in the signature $\\{ \\leq \\}$ is not first-order axiomatizable.","We also show that the Axiom of Choice is equivalent over $\\mathit{ZF}$ to the fact that every tree with finite branches is associative.","We also present an adjunction between the categories of RRBs and that of associative posets.   ","We study the smaller class of \"normal\" posets (corresponding to right-normal bands) and give a structural characterization.   ","As an application of the order-theoretic perspective on bands, we generalize results by the third author, obtaining \"inner\" direct product representations for RRBs having a central (commuting) element."],"url":"http://arxiv.org/abs/2404.07877v1","category":"math.LO"}
{"created":"2024-04-11 16:09:16","title":"Artificial Chemotaxis under Electrodiffusiophoresis","abstract":"Diffusiophoretic motion induced by gradients of dissolved species has enabled the manipulation of colloids over large distances, spanning hundreds of microns. Nonetheless, studies have primarily focused on simple geometries that feature 1D gradients of solutes generated by reactions or selective dissolution. Thus, our understanding of 3D diffusiophoresis remains elusive despite its importance in wide-ranging scenarios, such as cellular transport and nanofluidics. Herein, we present a strategy to generate 3D chemical gradients under electric fields. In this approach, faradaic reactions at electrodes induce global pH gradients that drive long-range transport through electrodiffusiophoresis. Simultaneously, the electric field induces local pH gradients by driving the particle's double layer far from equilibrium. As a result, while global pH gradients lead to 2D focusing away from electrodes, local pH gradients induce aggregation in the third dimension. Resulting interparticle interactions display a strong dependence on surface chemistry, and particle size. Furthermore, pH gradients can be readily tuned by adjusting the voltage and frequency of the electric field. For large P\\'eclet numbers, we observed a chemotactic-like collapse. Remarkably, such collapse occurs without reactions at a particle's surface. By mixing particles with different sizes, we also demonstrate the emergence of non-reciprocal interactions through experiments and Brownian dynamics simulations. These findings suggest a wide array of possibilities for the dynamic assembly of materials and the design of responsive matter.","sentences":["Diffusiophoretic motion induced by gradients of dissolved species has enabled the manipulation of colloids over large distances, spanning hundreds of microns.","Nonetheless, studies have primarily focused on simple geometries that feature 1D gradients of solutes generated by reactions or selective dissolution.","Thus, our understanding of 3D diffusiophoresis remains elusive despite its importance in wide-ranging scenarios, such as cellular transport and nanofluidics.","Herein, we present a strategy to generate 3D chemical gradients under electric fields.","In this approach, faradaic reactions at electrodes induce global pH gradients that drive long-range transport through electrodiffusiophoresis.","Simultaneously, the electric field induces local pH gradients by driving the particle's double layer far from equilibrium.","As a result, while global pH gradients lead to 2D focusing away from electrodes, local pH gradients induce aggregation in the third dimension.","Resulting interparticle interactions display a strong dependence on surface chemistry, and particle size.","Furthermore, pH gradients can be readily tuned by adjusting the voltage and frequency of the electric field.","For large P\\'eclet numbers, we observed a chemotactic-like collapse.","Remarkably, such collapse occurs without reactions at a particle's surface.","By mixing particles with different sizes, we also demonstrate the emergence of non-reciprocal interactions through experiments and Brownian dynamics simulations.","These findings suggest a wide array of possibilities for the dynamic assembly of materials and the design of responsive matter."],"url":"http://arxiv.org/abs/2404.07874v1","category":"cond-mat.soft"}
{"created":"2024-04-11 16:08:13","title":"Supernova remnants of red supergiants: from barrels to Cygnus loops","abstract":"Core-collapse supernova remnants are the nebular leftover of defunct massive stars which have died during a supernova explosion, mostly while undergoing the red supergiant phase of their evolution. The morphology and emission properties of those remnants are a function of the distribution of circumstellar material at the moment of the supernova, the intrisic properties of the explosion, as well as those of the ambient medium. By means of 2.5 dimensional numerical magnetohydrodynamics simulations, we model the long term evolution of supernova remnants generated by runaway rotating massive stars moving into a magnetised interstellar medium. Radiative transfer calculations reveal that the projected non-thermal emission of the supernova remnants decreases with time, i.e. older remnants are fainter than younger ones. Older (80 kyr) supernova remnants whose progenitors were moving with space velocity corresponding to a Mach number M = 1 (v_star = 20 km/s ) in the Galactic plane of the ISM (nISM = 1/cm3 ) are brighter in synchrotron than when moving with a Mach number M = 2 (v_star = 40 km/s ). We show that runaway red supergiant progenitors first induce an asymmetric non thermal 1.4 GHz barrel like synchrotron supernova remnants (at the age of about 8 kyr), before further evolving to adopt a Cygnus loop like shape (at about 80 kyr). It is conjectured that a significative fraction of supernova remnants are currently in this bilateral-to-Cygnus-loop evolutionary sequence, and that this should be taken into account in the data interpretation of the forthcoming Cherenkov Telescope Array (CTA) observatory.","sentences":["Core-collapse supernova remnants are the nebular leftover of defunct massive stars which have died during a supernova explosion, mostly while undergoing the red supergiant phase of their evolution.","The morphology and emission properties of those remnants are a function of the distribution of circumstellar material at the moment of the supernova, the intrisic properties of the explosion, as well as those of the ambient medium.","By means of 2.5 dimensional numerical magnetohydrodynamics simulations, we model the long term evolution of supernova remnants generated by runaway rotating massive stars moving into a magnetised interstellar medium.","Radiative transfer calculations reveal that the projected non-thermal emission of the supernova remnants decreases with time, i.e. older remnants are fainter than younger ones.","Older (80 kyr) supernova remnants whose progenitors were moving with space velocity corresponding to a Mach number M = 1 (v_star = 20 km/s ) in the Galactic plane of the ISM (nISM = 1/cm3 ) are brighter in synchrotron than when moving with a Mach number M = 2 (v_star = 40 km/s ).","We show that runaway red supergiant progenitors first induce an asymmetric non thermal 1.4 GHz barrel like synchrotron supernova remnants (at the age of about 8 kyr), before further evolving to adopt a Cygnus loop like shape (at about 80 kyr).","It is conjectured that a significative fraction of supernova remnants are currently in this bilateral-to-Cygnus-loop evolutionary sequence, and that this should be taken into account in the data interpretation of the forthcoming Cherenkov Telescope Array (CTA) observatory."],"url":"http://arxiv.org/abs/2404.07873v1","category":"astro-ph.HE"}
{"created":"2024-04-11 16:05:24","title":"Flexible-step MPC for Switched Linear Systems with No Quadratic Common Lyapunov Function","abstract":"In this paper, we develop a systematic method for constructing a generalized discrete-time control Lyapunov function for the flexible-step Model Predictive Control (MPC) scheme, recently introduced in [3], when restricted to the class of linear systems. Specifically, we show that a set of Linear Matrix Inequalities (LMIs) can be used for this purpose, demonstrating its tractability. The main consequence of this LMI formulation is that, when combined with flexible-step MPC, we can effectively stabilize switched control systems, for which no quadratic common Lyapunov function exists.","sentences":["In this paper, we develop a systematic method for constructing a generalized discrete-time control Lyapunov function for the flexible-step Model Predictive Control (MPC) scheme, recently introduced in [3], when restricted to the class of linear systems.","Specifically, we show that a set of Linear Matrix Inequalities (LMIs) can be used for this purpose, demonstrating its tractability.","The main consequence of this LMI formulation is that, when combined with flexible-step MPC, we can effectively stabilize switched control systems, for which no quadratic common Lyapunov function exists."],"url":"http://arxiv.org/abs/2404.07870v1","category":"math.OC"}
{"created":"2024-04-11 16:04:33","title":"Accurate neural quantum states for interacting lattice bosons","abstract":"In recent years, neural quantum states have emerged as a powerful variational approach, achieving state-of-the-art accuracy when representing the ground-state wave function of a great variety of quantum many-body systems, including spin lattices, interacting fermions or continuous-variable systems. However, accurate neural representations of the ground state of interacting bosons on a lattice have remained elusive. We introduce a neural backflow Jastrow Ansatz, in which occupation factors are dressed with translationally equivariant many-body features generated by a deep neural network. We show that this neural quantum state is able to faithfully represent the ground state of the 2D Bose-Hubbard Hamiltonian across all values of the interaction strength. We scale our simulations to lattices of dimension up to $20{\\times}20$ while achieving the best variational energies reported for this model. This enables us to investigate the scaling of the entanglement entropy across the superfluid-to-Mott quantum phase transition, a quantity hard to extract with non-variational approaches.","sentences":["In recent years, neural quantum states have emerged as a powerful variational approach, achieving state-of-the-art accuracy when representing the ground-state wave function of a great variety of quantum many-body systems, including spin lattices, interacting fermions or continuous-variable systems.","However, accurate neural representations of the ground state of interacting bosons on a lattice have remained elusive.","We introduce a neural backflow Jastrow Ansatz, in which occupation factors are dressed with translationally equivariant many-body features generated by a deep neural network.","We show that this neural quantum state is able to faithfully represent the ground state of the 2D Bose-Hubbard Hamiltonian across all values of the interaction strength.","We scale our simulations to lattices of dimension up to $20{\\times}20$ while achieving the best variational energies reported for this model.","This enables us to investigate the scaling of the entanglement entropy across the superfluid-to-Mott quantum phase transition, a quantity hard to extract with non-variational approaches."],"url":"http://arxiv.org/abs/2404.07869v1","category":"quant-ph"}
{"created":"2024-04-11 16:00:39","title":"Precise spiking motifs in neurobiological and neuromorphic data","abstract":"Why do neurons communicate through spikes? By definition, spikes are all-or-none neural events which occur at continuous times. In other words, spikes are on one side binary, existing or not without further details, and on the other can occur at any asynchronous time, without the need for a centralized clock. This stands in stark contrast to the analog representation of values and the discretized timing classically used in digital processing and at the base of modern-day neural networks. As neural systems almost systematically use this so-called event-based representation in the living world, a better understanding of this phenomenon remains a fundamental challenge in neurobiology in order to better interpret the profusion of recorded data. With the growing need for intelligent embedded systems, it also emerges as a new computing paradigm to enable the efficient operation of a new class of sensors and event-based computers, called neuromorphic, which could enable significant gains in computation time and energy consumption -- a major societal issue in the era of the digital economy and global warming. In this review paper, we provide evidence from biology, theory and engineering that the precise timing of spikes plays a crucial role in our understanding of the efficiency of neural networks.","sentences":["Why do neurons communicate through spikes?","By definition, spikes are all-or-none neural events which occur at continuous times.","In other words, spikes are on one side binary, existing or not without further details, and on the other can occur at any asynchronous time, without the need for a centralized clock.","This stands in stark contrast to the analog representation of values and the discretized timing classically used in digital processing and at the base of modern-day neural networks.","As neural systems almost systematically use this so-called event-based representation in the living world, a better understanding of this phenomenon remains a fundamental challenge in neurobiology in order to better interpret the profusion of recorded data.","With the growing need for intelligent embedded systems, it also emerges as a new computing paradigm to enable the efficient operation of a new class of sensors and event-based computers, called neuromorphic, which could enable significant gains in computation time and energy consumption -- a major societal issue in the era of the digital economy and global warming.","In this review paper, we provide evidence from biology, theory and engineering that the precise timing of spikes plays a crucial role in our understanding of the efficiency of neural networks."],"url":"http://arxiv.org/abs/2404.07866v1","category":"q-bio.NC"}
{"created":"2024-04-11 15:57:18","title":"The Dance of Logic and Unpredictability: Examining the Predictability of User Behavior on Visual Analytics Tasks","abstract":"The quest to develop intelligent visual analytics (VA) systems capable of collaborating and naturally interacting with humans presents a multifaceted and intriguing challenge. VA systems designed for collaboration must adeptly navigate a complex landscape filled with the subtleties and unpredictabilities that characterize human behavior. However, it is noteworthy that scenarios exist where human behavior manifests predictably. These scenarios typically involve routine actions or present a limited range of choices. This paper delves into the predictability of user behavior in the context of visual analytics tasks. It offers an evidence-based discussion on the circumstances under which predicting user behavior is feasible and those where it proves challenging. We conclude with a forward-looking discussion of the future work necessary to cultivate more synergistic and efficient partnerships between humans and the VA system. This exploration is not just about understanding our current capabilities and limitations in mirroring human behavior but also about envisioning and paving the way for a future where human-machine interaction is more intuitive and productive.","sentences":["The quest to develop intelligent visual analytics (VA) systems capable of collaborating and naturally interacting with humans presents a multifaceted and intriguing challenge.","VA systems designed for collaboration must adeptly navigate a complex landscape filled with the subtleties and unpredictabilities that characterize human behavior.","However, it is noteworthy that scenarios exist where human behavior manifests predictably.","These scenarios typically involve routine actions or present a limited range of choices.","This paper delves into the predictability of user behavior in the context of visual analytics tasks.","It offers an evidence-based discussion on the circumstances under which predicting user behavior is feasible and those where it proves challenging.","We conclude with a forward-looking discussion of the future work necessary to cultivate more synergistic and efficient partnerships between humans and the VA system.","This exploration is not just about understanding our current capabilities and limitations in mirroring human behavior but also about envisioning and paving the way for a future where human-machine interaction is more intuitive and productive."],"url":"http://arxiv.org/abs/2404.07865v1","category":"cs.HC"}
{"created":"2024-04-11 15:53:58","title":"Orbital and spin bilinear magnetotransport effect in Weyl/Dirac semimetal","abstract":"We theoretically investigate the bilinear current, scaling as $j\\sim EB$, in two- and three-dimensional systems. Based on the extended semiclassical theory, we develop a unified theory including both longitudinal and transverse currents. We classify all contributions according to their different scaling relations with the relaxation time. We reveal the distinct contributions to the ordinary Hall effect, planar Hall effect, and magnetoresistance. We further report an intrinsic ordinary Hall current, which has a geometric origin and has not been discussed previously. Our theory is explicitly applied to studying a massive Dirac model and a $\\mathcal{PT}$-symmetric system. Our work presents a general theory of electric transport under a magnetic field, potentially laying the groundwork for future experimental studies or device fabrications.","sentences":["We theoretically investigate the bilinear current, scaling as $j\\sim EB$, in two- and three-dimensional systems.","Based on the extended semiclassical theory, we develop a unified theory including both longitudinal and transverse currents.","We classify all contributions according to their different scaling relations with the relaxation time.","We reveal the distinct contributions to the ordinary Hall effect, planar Hall effect, and magnetoresistance.","We further report an intrinsic ordinary Hall current, which has a geometric origin and has not been discussed previously.","Our theory is explicitly applied to studying a massive Dirac model and a $\\mathcal{PT}$-symmetric system.","Our work presents a general theory of electric transport under a magnetic field, potentially laying the groundwork for future experimental studies or device fabrications."],"url":"http://arxiv.org/abs/2404.07858v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 15:53:58","title":"A monoidal category viewpoint for translation functors for finite $W$-algebras","abstract":"We re-interpret Goodwin's translation functors for a finite $W$-algebra $H_\\ell$ as an action of a monoidal subcategory of $U(\\mathfrak{g})$-mod on the category of finitely generated $H_\\ell$-modules. This action is obtained by transporting the tensor product of $U(\\mathfrak{g})$-modules through Skryabin's equivalence. We apply this interpretation to show that the Skryabin equivalence by stages introduced by Genra and Juillard is an equivalence of $U(\\mathfrak{g})$-module categories.","sentences":["We re-interpret Goodwin's translation functors for a finite $W$-algebra $H_\\ell$ as an action of a monoidal subcategory of $U(\\mathfrak{g})$-mod on the category of finitely generated $H_\\ell$-modules.","This action is obtained by transporting the tensor product of $U(\\mathfrak{g})$-modules through Skryabin's equivalence.","We apply this interpretation to show that the Skryabin equivalence by stages introduced by Genra and Juillard is an equivalence of $U(\\mathfrak{g})$-module categories."],"url":"http://arxiv.org/abs/2404.07859v1","category":"math.RT"}
{"created":"2024-04-11 15:53:39","title":"Optical next generation reservoir computing","abstract":"Artificial neural networks with dynamics exhibit remarkable capability in processing information. Reservoir computing (RC) is a canonical example that features rich computing expressivity and compatibility with physical implementations for enhanced efficiency. Recently, a new RC paradigm known as next generation reservoir computing (NGRC) further improves expressivity but compromises the physical openness, posing challenges for neuromorphic realizations. Here we demonstrate optical NGRC with large-scale computations performed by light scattering through disordered media. In contrast to conventional optical RC implementations, we drive our optical reservoir directly with time-delay inputs. We show that, much like digital NGRC that relies on polynomial features of delayed inputs, our optical reservoir also implicitly generates these polynomial features for desired functionalities. By leveraging the domain knowledge of the reservoir inputs, the optical NGRC not only predicts the short-term dynamics of the low-dimensional Lorenz63 and high-dimensional Kuramoto-Sivashinsky chaotic time series, but also replicates their long-term ergodic properties. Optical NGRC shows superiority in shorter training length, fewer hyperparameters and increased interpretability compared to conventional optical RC, while achieving state-of-the-art forecasting performance. Given its scalability and versatility, the optical NGRC framework also paves the way for next generation physical RC, new applications and architectures in a broad sense.","sentences":["Artificial neural networks with dynamics exhibit remarkable capability in processing information.","Reservoir computing (RC) is a canonical example that features rich computing expressivity and compatibility with physical implementations for enhanced efficiency.","Recently, a new RC paradigm known as next generation reservoir computing (NGRC) further improves expressivity but compromises the physical openness, posing challenges for neuromorphic realizations.","Here we demonstrate optical NGRC with large-scale computations performed by light scattering through disordered media.","In contrast to conventional optical RC implementations, we drive our optical reservoir directly with time-delay inputs.","We show that, much like digital NGRC that relies on polynomial features of delayed inputs, our optical reservoir also implicitly generates these polynomial features for desired functionalities.","By leveraging the domain knowledge of the reservoir inputs, the optical NGRC not only predicts the short-term dynamics of the low-dimensional Lorenz63 and high-dimensional Kuramoto-Sivashinsky chaotic time series, but also replicates their long-term ergodic properties.","Optical NGRC shows superiority in shorter training length, fewer hyperparameters and increased interpretability compared to conventional optical RC, while achieving state-of-the-art forecasting performance.","Given its scalability and versatility, the optical NGRC framework also paves the way for next generation physical RC, new applications and architectures in a broad sense."],"url":"http://arxiv.org/abs/2404.07857v1","category":"physics.optics"}
{"created":"2024-04-11 15:51:00","title":"Reflexive graph lenses in univalent foundations","abstract":"Martin-L\\\"of's identity types provide a generic (albeit opaque) notion of identification or \"equality\" between any two elements of the same type, embodied in a canonical reflexive graph structure $(=_A, \\mathbf{refl})$ on any type $A$. The miracle of Voevodsky's univalence principle is that it ensures, for essentially any naturally occurring structure in mathematics, that this the resultant notion of identification is equivalent to the type of isomorphisms in the category of such structures. Characterisations of this kind are not automatic and must be established one-by-one; to this end, several authors have employed reflexive graphs and displayed reflexive graphs to organise the characterisation of identity types.   We contribute reflexive graph lenses, a new family of intermediate abstractions lying between families of reflexive graphs and displayed reflexive graphs that simplifies the characterisation of identity types for complex structures. Every reflexive graph lens gives rise to a (more complicated) displayed reflexive graph, and our experience suggests that many naturally occurring displayed reflexive graphs arise in this way. Evidence for the utility of reflexive graph lenses is given by means of several case studies, including the theory of reflexive graphs itself as well as that of polynomial type operators. Finally, we exhibit an equivalence between the type of reflexive graph fibrations and the type of univalent reflexive graph lenses.","sentences":["Martin-L\\\"of's identity types provide a generic (albeit opaque) notion of identification or \"equality\" between any two elements of the same type, embodied in a canonical reflexive graph structure $(=_","A, \\mathbf{refl})$ on any type $A$.","The miracle of Voevodsky's univalence principle is that it ensures, for essentially any naturally occurring structure in mathematics, that this the resultant notion of identification is equivalent to the type of isomorphisms in the category of such structures.","Characterisations of this kind are not automatic and must be established one-by-one; to this end, several authors have employed reflexive graphs and displayed reflexive graphs to organise the characterisation of identity types.   ","We contribute reflexive graph lenses, a new family of intermediate abstractions lying between families of reflexive graphs and displayed reflexive graphs that simplifies the characterisation of identity types for complex structures.","Every reflexive graph lens gives rise to a (more complicated) displayed reflexive graph, and our experience suggests that many naturally occurring displayed reflexive graphs arise in this way.","Evidence for the utility of reflexive graph lenses is given by means of several case studies, including the theory of reflexive graphs itself as well as that of polynomial type operators.","Finally, we exhibit an equivalence between the type of reflexive graph fibrations and the type of univalent reflexive graph lenses."],"url":"http://arxiv.org/abs/2404.07854v1","category":"cs.LO"}
{"created":"2024-04-11 15:49:12","title":"Beyond recognizing well-covered graphs","abstract":"We prove a number of results related to the computational complexity of recognizing well-covered graphs. Let $k$ and $s$ be positive integers and let $G$ be a graph. Then $G$ is said   - $\\mathbf{W_k}$ if for any $k$ pairwise disjoint independent vertex sets $A_1, \\dots, A_k$ in $G$, there exist $k$ pairwise disjoint maximum independent sets $S_1, \\dots,S_k$ in $G$ such that $A_i \\subseteq S_i$ for $i \\in [k]$.   - $\\mathbf{E_s}$ if every independent set in $G$ of size at most $s$ is contained in a maximum independent set in $G$.   Chv\\'atal and Slater (1993) and Sankaranarayana and Stewart (1992) famously showed that recognizing $\\mathbf{W_1}$ graphs or, equivalently, well-covered graphs is coNP-complete. We extend this result by showing that recognizing $\\mathbf{W_{k+1}}$ graphs in either $\\mathbf{W_k}$ or $\\mathbf{E_s}$ graphs is coNP-complete. This answers a question of Levit and Tankus (2023) and strengthens a theorem of Feghali and Marin (2024). We also show that recognizing $\\mathbf{E_{s+1}}$ graphs is $\\Theta_2^p$-complete even in $\\mathbf{E_s}$ graphs, where $\\Theta_2^p = \\text{P}^{\\text{NP}[\\log]}$ is the class of problems solvable in polynomial time using a logarithmic number of calls to a SAT oracle. This strengthens a theorem of Berg\\'e, Busson, Feghali and Watrigant (2023). We also obtain the complete picture of the complexity of recognizing chordal $\\mathbf{W_k}$ and $\\mathbf{E_s}$ graphs which, in particular, simplifies and generalizes a result of Dettlaff, Henning and Topp (2023).","sentences":["We prove a number of results related to the computational complexity of recognizing well-covered graphs.","Let $k$ and $s$ be positive integers and let $G$ be a graph.","Then $G$ is said   - $\\mathbf{W_k}$ if for any $k$ pairwise disjoint independent vertex sets $A_1, \\dots, A_k$ in $G$, there exist $k$ pairwise disjoint maximum independent sets $S_1, \\dots,S_k$ in $G$ such that $A_i \\subseteq S_i$ for $i \\in [k]$.   - $\\mathbf{E_s}$ if every independent set in $G$ of size at most $s$ is contained in a maximum independent set in $G$.   Chv\\'atal and Slater (1993) and Sankaranarayana and Stewart (1992) famously showed that recognizing $\\mathbf{W_1}$ graphs or, equivalently, well-covered graphs is coNP-complete.","We extend this result by showing that recognizing $\\mathbf{W_{k+1}}$ graphs in either $\\mathbf{W_k}$ or $\\mathbf{E_s}$ graphs is coNP-complete.","This answers a question of Levit and Tankus (2023) and strengthens a theorem of Feghali and Marin (2024).","We also show that recognizing $\\mathbf{E_{s+1}}$ graphs is $\\Theta_2^p$-complete even in $\\mathbf{E_s}$ graphs, where $\\Theta_2^p = \\text{P}^{\\text{NP}[\\log]}$ is the class of problems solvable in polynomial time using a logarithmic number of calls to a SAT oracle.","This strengthens a theorem of Berg\\'e, Busson, Feghali and Watrigant (2023).","We also obtain the complete picture of the complexity of recognizing chordal $\\mathbf{W_k}$ and $\\mathbf{E_s}$ graphs which, in particular, simplifies and generalizes a result of Dettlaff, Henning and Topp (2023)."],"url":"http://arxiv.org/abs/2404.07853v1","category":"math.CO"}
{"created":"2024-04-11 15:47:10","title":"Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations","abstract":"Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.","sentences":["Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems.","This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations.","Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance.","Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear.","Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation."],"url":"http://arxiv.org/abs/2404.07851v1","category":"cs.CL"}
{"created":"2024-04-11 15:46:42","title":"MindBridge: A Cross-Subject Brain Decoding Framework","abstract":"Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI). Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained. This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited data availability for new subjects in real-world scenarios hampers the performance of decoding models. In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model. Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant representation learning. Notably, by cycle reconstruction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo data augmentation. Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject. Experimental results demonstrate MindBridge's ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models. Furthermore, with limited data for a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models. This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI data in real-world scenarios. Project page: https://littlepure2333.github.io/MindBridge","sentences":["Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI).","Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained.","This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited data availability for new subjects in real-world scenarios hampers the performance of decoding models.","In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model.","Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant representation learning.","Notably, by cycle reconstruction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo data augmentation.","Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject.","Experimental results demonstrate MindBridge's ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models.","Furthermore, with limited data for a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models.","This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI data in real-world scenarios.","Project page: https://littlepure2333.github.io/MindBridge"],"url":"http://arxiv.org/abs/2404.07850v1","category":"cs.CV"}
{"created":"2024-04-11 15:37:03","title":"Efficiency of dynamos from autonomous generation of chiral asymmetry","abstract":"At high energies, the dynamics of a plasma with charged fermions can be described in terms of chiral magnetohydrodynamics. Using direct numerical simulations, we demonstrate that chiral magnetic waves (CMWs) can produce a chiral asymmetry $\\mu_5 = \\mu_\\mathrm{L} - \\mu_\\mathrm{R}$ from a spatially fluctuating (inhomogeneous) chemical potential $\\mu = \\mu_\\mathrm{L} + \\mu_\\mathrm{R}$, where $\\mu_\\mathrm{L}$ and $\\mu_\\mathrm{R}$ are the chemical potentials of left- and right-handed electrically charged fermions, respectively. If the frequency of the CMW is less than or comparable to the characteristic growth rate of the chiral dynamo instability, the magnetic field can be amplified on small spatial scales. The growth rate of this small-scale chiral dynamo instability is determined by the spatial maximum value of $\\mu_5$ fluctuations. Therefore, the magnetic field amplification occurs during periods when $\\mu_5$ reaches temporal maxima during the CMW. If the small-scale chiral dynamo instability leads to a magnetic field strength that exceeds a critical value, which depends on the resistivity and the initial value of $\\mu$, magnetically-dominated turbulence is produced. Turbulence gives rise to a large-scale dynamo instability, which we find to be caused by the magnetic alpha effect. Our results have consequences for the dynamics of certain high-energy plasmas, such as the early Universe or proto-neutron stars.","sentences":["At high energies, the dynamics of a plasma with charged fermions can be described in terms of chiral magnetohydrodynamics.","Using direct numerical simulations, we demonstrate that chiral magnetic waves (CMWs) can produce a chiral asymmetry $\\mu_5 = \\mu_\\mathrm{L} - \\mu_\\mathrm{R}$ from a spatially fluctuating (inhomogeneous) chemical potential $\\mu = \\mu_\\mathrm{L} + \\mu_\\mathrm{R}$, where $\\mu_\\mathrm{L}$ and $\\mu_\\mathrm{R}$ are the chemical potentials of left- and right-handed electrically charged fermions, respectively.","If the frequency of the CMW is less than or comparable to the characteristic growth rate of the chiral dynamo instability, the magnetic field can be amplified on small spatial scales.","The growth rate of this small-scale chiral dynamo instability is determined by the spatial maximum value of $\\mu_5$ fluctuations.","Therefore, the magnetic field amplification occurs during periods when $\\mu_5$ reaches temporal maxima during the CMW.","If the small-scale chiral dynamo instability leads to a magnetic field strength that exceeds a critical value, which depends on the resistivity and the initial value of $\\mu$, magnetically-dominated turbulence is produced.","Turbulence gives rise to a large-scale dynamo instability, which we find to be caused by the magnetic alpha effect.","Our results have consequences for the dynamics of certain high-energy plasmas, such as the early Universe or proto-neutron stars."],"url":"http://arxiv.org/abs/2404.07845v1","category":"physics.plasm-ph"}
{"created":"2024-04-11 15:30:57","title":"Approaches to photon absorption in a Lorentz invariance violation scenario","abstract":"We examine the Universe's transparency to gamma rays within a Lorentz Invariance Violation (LIV) framework, focusing on photon subluminal quadratic corrections driven by a high-energy scale. Based on an explicit calculation, we provide a new expression for the cross section that overcomes the limitations of previous approaches and refines existing constraints for the LIV scale, while we introduce a new approximation that may be useful in LIV scenarios beyond effective field theory. These improvements appear essential for setting constraints on LIV effects with future observations at ultra-high energies, where previous approximations may fall short.","sentences":["We examine the Universe's transparency to gamma rays within a Lorentz Invariance Violation (LIV) framework, focusing on photon subluminal quadratic corrections driven by a high-energy scale.","Based on an explicit calculation, we provide a new expression for the cross section that overcomes the limitations of previous approaches and refines existing constraints for the LIV scale, while we introduce a new approximation that may be useful in LIV scenarios beyond effective field theory.","These improvements appear essential for setting constraints on LIV effects with future observations at ultra-high energies, where previous approximations may fall short."],"url":"http://arxiv.org/abs/2404.07842v1","category":"hep-ph"}
{"created":"2024-04-11 15:27:56","title":"On Training Data Influence of GPT Models","abstract":"Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We will make our code and data publicly available.","sentences":["Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging.","This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models.","Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks.","Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data.","This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation.","We will make our code and data publicly available."],"url":"http://arxiv.org/abs/2404.07840v1","category":"cs.CL"}
{"created":"2024-04-11 15:27:22","title":"RecurrentGemma: Moving Past Transformers for Efficient Open Language Models","abstract":"We introduce RecurrentGemma, an open language model which uses Google's novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.","sentences":["We introduce RecurrentGemma, an open language model which uses Google's novel Griffin architecture.","Griffin combines linear recurrences with local attention to achieve excellent performance on language.","It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences.","We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant.","Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens."],"url":"http://arxiv.org/abs/2404.07839v1","category":"cs.LG"}
{"created":"2024-04-11 15:24:50","title":"Question Generation in Knowledge-Driven Dialog: Explainability and Evaluation","abstract":"We explore question generation in the context of knowledge-grounded dialogs focusing on explainability and evaluation. Inspired by previous work on planning-based summarisation, we present a model which instead of directly generating a question, sequentially predicts first a fact then a question. We evaluate our approach on 37k test dialogs adapted from the KGConv dataset and we show that, although more demanding in terms of inference, our approach performs on par with a standard model which solely generates a question while allowing for a detailed referenceless evaluation of the model behaviour in terms of relevance, factuality and pronominalisation.","sentences":["We explore question generation in the context of knowledge-grounded dialogs focusing on explainability and evaluation.","Inspired by previous work on planning-based summarisation, we present a model which instead of directly generating a question, sequentially predicts first a fact then a question.","We evaluate our approach on 37k test dialogs adapted from the KGConv dataset and we show that, although more demanding in terms of inference, our approach performs on par with a standard model which solely generates a question while allowing for a detailed referenceless evaluation of the model behaviour in terms of relevance, factuality and pronominalisation."],"url":"http://arxiv.org/abs/2404.07836v1","category":"cs.CL"}
{"created":"2024-04-11 15:18:42","title":"Tensor models and group field theories: combinatorics, large $N$ and renormalization","abstract":"We provide a brief overview of tensor models and group field theories, focusing on their main common features. Both frameworks arose in the context of quantum gravity research, and can be understood as higher-dimensional generalizations of matrix models. We describe the common combinatorial structure underlying such models and review some of the key mathematical results that have been obtained in this area. Of central importance is the discovery of a new family of large $N$ expansions for random tensors. It has driven applications of tensor models to random geometry and non-perturbative local quantum field theory, and has also enabled the development of rigorous renormalization methods for group field theories and other non-local quantum field theories.","sentences":["We provide a brief overview of tensor models and group field theories, focusing on their main common features.","Both frameworks arose in the context of quantum gravity research, and can be understood as higher-dimensional generalizations of matrix models.","We describe the common combinatorial structure underlying such models and review some of the key mathematical results that have been obtained in this area.","Of central importance is the discovery of a new family of large $N$ expansions for random tensors.","It has driven applications of tensor models to random geometry and non-perturbative local quantum field theory, and has also enabled the development of rigorous renormalization methods for group field theories and other non-local quantum field theories."],"url":"http://arxiv.org/abs/2404.07834v1","category":"math-ph"}
{"created":"2024-04-11 15:18:13","title":"Zeros of $L$-Functions in Low-Lying Intervals and de Branges spaces","abstract":"We consider a variant of a problem first introduced by Hughes and Rudnick (2003) and generalized by Bernard (2015) concerning conditional bounds for small first zeros in a family of $L$-functions. Here we seek to estimate the size of the smallest intervals centered at a low-lying height for which we can guarantee the existence of a zero in a family of $L$-functions. This leads us to consider an extremal problem in analysis which we address by applying the framework of de Branges spaces, introduced in this context by Carneiro, Chirre, and Milinovich (2022).","sentences":["We consider a variant of a problem first introduced by Hughes and Rudnick (2003) and generalized by Bernard (2015) concerning conditional bounds for small first zeros in a family of $L$-functions.","Here we seek to estimate the size of the smallest intervals centered at a low-lying height for which we can guarantee the existence of a zero in a family of $L$-functions.","This leads us to consider an extremal problem in analysis which we address by applying the framework of de Branges spaces, introduced in this context by Carneiro, Chirre, and Milinovich (2022)."],"url":"http://arxiv.org/abs/2404.07832v1","category":"math.NT"}
{"created":"2024-04-11 15:16:26","title":"Protected QR Code-based Anti-counterfeit System for Pharmaceutical Manufacturing","abstract":"The pharmaceutical manufacturing faces critical challenges due to the global threat of counterfeit drugs. This paper proposes a new approach of protected QR codes to secure unique product information for safeguarding the pharmaceutical supply chain. The proposed solution integrates secure QR code generation and encrypted data transmission to establish a comprehensive anti-counterfeit ecosystem. The protected QR codes encapsulate product information that cannot be identified using traditional QR code scanners which protect the information against replication and tampering. The system is developed with scalability in mind, which can be easily implemented without introducing any additional modification in the traditional supply chain.","sentences":["The pharmaceutical manufacturing faces critical challenges due to the global threat of counterfeit drugs.","This paper proposes a new approach of protected QR codes to secure unique product information for safeguarding the pharmaceutical supply chain.","The proposed solution integrates secure QR code generation and encrypted data transmission to establish a comprehensive anti-counterfeit ecosystem.","The protected QR codes encapsulate product information that cannot be identified using traditional QR code scanners which protect the information against replication and tampering.","The system is developed with scalability in mind, which can be easily implemented without introducing any additional modification in the traditional supply chain."],"url":"http://arxiv.org/abs/2404.07831v1","category":"cs.CR"}
{"created":"2024-04-11 15:10:10","title":"iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models","abstract":"This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction.","sentences":["This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques.","This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process.","The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing.","Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves.","Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition.","This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction."],"url":"http://arxiv.org/abs/2404.07827v1","category":"eess.SY"}
{"created":"2024-04-11 15:09:49","title":"On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning","abstract":"The use of Potential Based Reward Shaping (PBRS) has shown great promise in the ongoing research effort to tackle sample inefficiency in Reinforcement Learning (RL). However, the choice of the potential function is critical for this technique to be effective. Additionally, RL techniques are usually constrained to use a finite horizon for computational limitations. This introduces a bias when using PBRS, thus adding an additional layer of complexity. In this paper, we leverage abstractions to automatically produce a \"good\" potential function. We analyse the bias induced by finite horizons in the context of PBRS producing novel insights. Finally, to asses sample efficiency and performance impact, we evaluate our approach on four environments including a goal-oriented navigation task and three Arcade Learning Environments (ALE) games demonstrating that we can reach the same level of performance as CNN-based solutions with a simple fully-connected network.","sentences":["The use of Potential Based Reward Shaping (PBRS) has shown great promise in the ongoing research effort to tackle sample inefficiency in Reinforcement Learning (RL).","However, the choice of the potential function is critical for this technique to be effective.","Additionally, RL techniques are usually constrained to use a finite horizon for computational limitations.","This introduces a bias when using PBRS, thus adding an additional layer of complexity.","In this paper, we leverage abstractions to automatically produce a \"good\" potential function.","We analyse the bias induced by finite horizons in the context of PBRS producing novel insights.","Finally, to asses sample efficiency and performance impact, we evaluate our approach on four environments including a goal-oriented navigation task and three Arcade Learning Environments (ALE) games demonstrating that we can reach the same level of performance as CNN-based solutions with a simple fully-connected network."],"url":"http://arxiv.org/abs/2404.07826v1","category":"cs.LG"}
{"created":"2024-04-11 15:09:25","title":"Hilbert space fragmentation from lattice geometry","abstract":"The eigenstate thermalization hypothesis describes how isolated many-body quantum systems reach thermal equilibrium. However, quantum many-body scars and Hilbert space fragmentation violate this hypothesis and cause nonthermal behavior. We demonstrate that Hilbert space fragmentation may arise from lattice geometry in a spin-1/2 model that conserves the number of domain walls. We generalize a known, one-dimensional, scarred model to larger dimensions and show that this model displays Hilbert space fragmentation on the Vicsek fractal lattice of arbitrary generation. Using Monte Carlo methods, the model is characterized as strongly fragmented on the Vicsek fractal lattice when the number of domain walls is either small or close to the maximal value. We show that the model displays signatures similar to Hilbert space fragmentation on the second-generation hexaflake fractal lattice, the two-dimensional lattice, and a modified two-dimensional lattice. We study the autocorrelation function of local observables and demonstrate that the model displays nonthermal dynamics.","sentences":["The eigenstate thermalization hypothesis describes how isolated many-body quantum systems reach thermal equilibrium.","However, quantum many-body scars and Hilbert space fragmentation violate this hypothesis and cause nonthermal behavior.","We demonstrate that Hilbert space fragmentation may arise from lattice geometry in a spin-1/2 model that conserves the number of domain walls.","We generalize a known, one-dimensional, scarred model to larger dimensions and show that this model displays Hilbert space fragmentation on the Vicsek fractal lattice of arbitrary generation.","Using Monte Carlo methods, the model is characterized as strongly fragmented on the Vicsek fractal lattice when the number of domain walls is either small or close to the maximal value.","We show that the model displays signatures similar to Hilbert space fragmentation on the second-generation hexaflake fractal lattice, the two-dimensional lattice, and a modified two-dimensional lattice.","We study the autocorrelation function of local observables and demonstrate that the model displays nonthermal dynamics."],"url":"http://arxiv.org/abs/2404.07825v1","category":"cond-mat.str-el"}
{"created":"2024-04-11 15:00:55","title":"Sparse Laneformer","abstract":"Lane detection is a fundamental task in autonomous driving, and has achieved great progress as deep learning emerges. Previous anchor-based methods often design dense anchors, which highly depend on the training dataset and remain fixed during inference. We analyze that dense anchors are not necessary for lane detection, and propose a transformer-based lane detection framework based on a sparse anchor mechanism. To this end, we generate sparse anchors with position-aware lane queries and angle queries instead of traditional explicit anchors. We adopt Horizontal Perceptual Attention (HPA) to aggregate the lane features along the horizontal direction, and adopt Lane-Angle Cross Attention (LACA) to perform interactions between lane queries and angle queries. We also propose Lane Perceptual Attention (LPA) based on deformable cross attention to further refine the lane predictions. Our method, named Sparse Laneformer, is easy-to-implement and end-to-end trainable. Extensive experiments demonstrate that Sparse Laneformer performs favorably against the state-of-the-art methods, e.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 score with fewer MACs on CULane with the same ResNet-34 backbone.","sentences":["Lane detection is a fundamental task in autonomous driving, and has achieved great progress as deep learning emerges.","Previous anchor-based methods often design dense anchors, which highly depend on the training dataset and remain fixed during inference.","We analyze that dense anchors are not necessary for lane detection, and propose a transformer-based lane detection framework based on a sparse anchor mechanism.","To this end, we generate sparse anchors with position-aware lane queries and angle queries instead of traditional explicit anchors.","We adopt Horizontal Perceptual Attention (HPA) to aggregate the lane features along the horizontal direction, and adopt Lane-Angle Cross Attention (LACA) to perform interactions between lane queries and angle queries.","We also propose Lane Perceptual Attention (LPA) based on deformable cross attention to further refine the lane predictions.","Our method, named Sparse Laneformer, is easy-to-implement and end-to-end trainable.","Extensive experiments demonstrate that Sparse Laneformer performs favorably against the state-of-the-art methods, e.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 score with fewer MACs on CULane with the same ResNet-34 backbone."],"url":"http://arxiv.org/abs/2404.07821v1","category":"cs.CV"}
{"created":"2024-04-11 15:00:10","title":"Generation of $3$-connected, planar line graphs","abstract":"We classify and construct all line graphs that are $3$-polytopes (planar and $3$-connected). Apart from a few special cases, they are all obtained starting from the medial graphs of cubic (i.e., $3$-regular) $3$-polytopes, by applying two types of graph transformations. This is similar to the generation of other subclasses of $3$-polytopes.","sentences":["We classify and construct all line graphs that are $3$-polytopes (planar and $3$-connected).","Apart from a few special cases, they are all obtained starting from the medial graphs of cubic (i.e., $3$-regular) $3$-polytopes, by applying two types of graph transformations.","This is similar to the generation of other subclasses of $3$-polytopes."],"url":"http://arxiv.org/abs/2404.07819v1","category":"math.CO"}
{"created":"2024-04-11 14:59:49","title":"Calibration of Continual Learning Models","abstract":"Continual Learning (CL) focuses on maximizing the predictive performance of a model across a non-stationary stream of data. Unfortunately, CL models tend to forget previous knowledge, thus often underperforming when compared with an offline model trained jointly on the entire data stream. Given that any CL model will eventually make mistakes, it is of crucial importance to build calibrated CL models: models that can reliably tell their confidence when making a prediction. Model calibration is an active research topic in machine learning, yet to be properly investigated in CL. We provide the first empirical study of the behavior of calibration approaches in CL, showing that CL strategies do not inherently learn calibrated models. To mitigate this issue, we design a continual calibration approach that improves the performance of post-processing calibration methods over a wide range of different benchmarks and CL strategies. CL does not necessarily need perfect predictive models, but rather it can benefit from reliable predictive models. We believe our study on continual calibration represents a first step towards this direction.","sentences":["Continual Learning (CL) focuses on maximizing the predictive performance of a model across a non-stationary stream of data.","Unfortunately, CL models tend to forget previous knowledge, thus often underperforming when compared with an offline model trained jointly on the entire data stream.","Given that any CL model will eventually make mistakes, it is of crucial importance to build calibrated CL models: models that can reliably tell their confidence when making a prediction.","Model calibration is an active research topic in machine learning, yet to be properly investigated in CL.","We provide the first empirical study of the behavior of calibration approaches in CL, showing that CL strategies do not inherently learn calibrated models.","To mitigate this issue, we design a continual calibration approach that improves the performance of post-processing calibration methods over a wide range of different benchmarks and CL strategies.","CL does not necessarily need perfect predictive models, but rather it can benefit from reliable predictive models.","We believe our study on continual calibration represents a first step towards this direction."],"url":"http://arxiv.org/abs/2404.07817v1","category":"cs.LG"}
{"created":"2024-04-11 14:59:49","title":"Robustness of voting mechanisms to external information in expectation","abstract":"Analyses of voting algorithms often overlook informational externalities shaping individual votes. For example, pre-polling information often skews voters towards candidates who may not be their top choice, but who they believe would be a worthwhile recipient of their vote. In this work, we aim to understand the role of external information in voting outcomes. We study this by analyzing (1) the probability that voting outcomes align with external information, and (2) the effect of external information on the total utility across voters, or social welfare. In practice, voting mechanisms elicit coarse information about voter utilities, such as ordinal preferences, which initially prevents us from directly analyzing the effect of informational externalities with standard voting mechanisms. To overcome this, we present an intermediary mechanism for learning how preferences change with external information which does not require eliciting full cardinal preferences. With this tool in hand, we find that voting mechanisms are generally more likely to select the alternative most favored by the external information, and when external information reflects the population's true preferences, social welfare increases in expectation.","sentences":["Analyses of voting algorithms often overlook informational externalities shaping individual votes.","For example, pre-polling information often skews voters towards candidates who may not be their top choice, but who they believe would be a worthwhile recipient of their vote.","In this work, we aim to understand the role of external information in voting outcomes.","We study this by analyzing (1) the probability that voting outcomes align with external information, and (2) the effect of external information on the total utility across voters, or social welfare.","In practice, voting mechanisms elicit coarse information about voter utilities, such as ordinal preferences, which initially prevents us from directly analyzing the effect of informational externalities with standard voting mechanisms.","To overcome this, we present an intermediary mechanism for learning how preferences change with external information which does not require eliciting full cardinal preferences.","With this tool in hand, we find that voting mechanisms are generally more likely to select the alternative most favored by the external information, and when external information reflects the population's true preferences, social welfare increases in expectation."],"url":"http://arxiv.org/abs/2404.07818v1","category":"cs.GT"}
{"created":"2024-04-11 14:58:55","title":"Obstructions to semiorthogonal decompositions for singular projective varieties II: Representation theory","abstract":"We show that odd-dimensional projective varieties with tilting objects and only ADE-singularities are nodal, i.e. they only have $A_1$-singularities. This is a very special case of more general obstructions to the existence of semiorthogonal decompositions for projective Gorenstein varieties. More precisely, for many isolated hypersurface singularities, we show that Kuznetsov-Shinder's categorical absorptions of singularities cannot contain tilting objects. The key idea is to compare singularity categories of projective varieties to singularity categories of finite-dimensional associative Gorenstein algebras. The former often contain special generators, called cluster-tilting objects, which typically have loops and $2$-cycles in their quivers. In contrast, quivers of cluster-tilting objects in the latter categories, can never have loops or $2$-cycles.","sentences":["We show that odd-dimensional projective varieties with tilting objects and only ADE-singularities are nodal, i.e. they only have $A_1$-singularities.","This is a very special case of more general obstructions to the existence of semiorthogonal decompositions for projective Gorenstein varieties.","More precisely, for many isolated hypersurface singularities, we show that Kuznetsov-Shinder's categorical absorptions of singularities cannot contain tilting objects.","The key idea is to compare singularity categories of projective varieties to singularity categories of finite-dimensional associative Gorenstein algebras.","The former often contain special generators, called cluster-tilting objects, which typically have loops and $2$-cycles in their quivers.","In contrast, quivers of cluster-tilting objects in the latter categories, can never have loops or $2$-cycles."],"url":"http://arxiv.org/abs/2404.07816v1","category":"math.AG"}
{"created":"2024-04-11 14:58:19","title":"Post-Hoc Reversal: Are We Selecting Models Prematurely?","abstract":"Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc. However, such transforms are typically applied only after the base models have already been finalized by standard means. In this paper, we challenge this practice with an extensive empirical study. In particular, we demonstrate a phenomenon that we call post-hoc reversal, where performance trends are reversed after applying these post-hoc transforms. This phenomenon is especially prominent in high-noise settings. For example, while base models overfit badly early in training, both conventional ensembling and SWA favor base models trained for more epochs. Post-hoc reversal can also suppress the appearance of double descent and mitigate mismatches between test loss and test error seen in base models. Based on our findings, we propose post-hoc selection, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices. Our experimental analyses span real-world vision, language, tabular and graph datasets from domains like satellite imaging, language modeling, census prediction and social network analysis. On an LLM instruction tuning dataset, post-hoc selection results in > 1.5x MMLU improvement compared to naive selection. Code is available at https://github.com/rishabh-ranjan/post-hoc-reversal.","sentences":["Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc.","However, such transforms are typically applied only after the base models have already been finalized by standard means.","In this paper, we challenge this practice with an extensive empirical study.","In particular, we demonstrate a phenomenon that we call post-hoc reversal, where performance trends are reversed after applying these post-hoc transforms.","This phenomenon is especially prominent in high-noise settings.","For example, while base models overfit badly early in training, both conventional ensembling and SWA favor base models trained for more epochs.","Post-hoc reversal can also suppress the appearance of double descent and mitigate mismatches between test loss and test error seen in base models.","Based on our findings, we propose post-hoc selection, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices.","Our experimental analyses span real-world vision, language, tabular and graph datasets from domains like satellite imaging, language modeling, census prediction and social network analysis.","On an LLM instruction tuning dataset, post-hoc selection results in > 1.5x MMLU improvement compared to naive selection.","Code is available at https://github.com/rishabh-ranjan/post-hoc-reversal."],"url":"http://arxiv.org/abs/2404.07815v1","category":"cs.LG"}
{"created":"2024-04-11 14:53:28","title":"Problem-Driven Scenario Reduction Framework for Power System Stochastic Operation","abstract":"Scenario reduction (SR) aims to identify a small yet representative scenario set to depict the underlying uncertainty, which is critical to scenario-based stochastic optimization (SBSO) of power systems. Existing SR techniques commonly aim to achieve statistical approximation to the original scenario set. However, SR and SBSO are commonly considered into two distinct and decoupled processes, which cannot guarantee a superior approximation of the original optimality. Instead, this paper incorporates the SBSO problem structure into the SR process and introduces a novel problem-driven scenario reduction framework. Specifically, we transform the original scenario set in distribution space into the decision applicability between scenarios in problem space. Subsequently, the SR process, embedded by a distinctive problem-driven distance metric, is rendered as a mixed-integer linear programming formulation to obtain the representative scenario set while minimizing the optimality gap. Furthermore, ex-ante and ex-post problem-driven evaluation indices are proposed to evaluate the performance of SR. A two-stage stochastic economic dispatch problem with renewable generation and energy storage validates the effectiveness of the proposed framework. Numerical experiments demonstrate that the proposed framework significantly outperforms existing SR methods by identifying salient (e.g., worst-case) scenarios, and achieving an optimality gap of less than 0.1% within acceptable computation time.","sentences":["Scenario reduction (SR) aims to identify a small yet representative scenario set to depict the underlying uncertainty, which is critical to scenario-based stochastic optimization (SBSO) of power systems.","Existing SR techniques commonly aim to achieve statistical approximation to the original scenario set.","However, SR and SBSO are commonly considered into two distinct and decoupled processes, which cannot guarantee a superior approximation of the original optimality.","Instead, this paper incorporates the SBSO problem structure into the SR process and introduces a novel problem-driven scenario reduction framework.","Specifically, we transform the original scenario set in distribution space into the decision applicability between scenarios in problem space.","Subsequently, the SR process, embedded by a distinctive problem-driven distance metric, is rendered as a mixed-integer linear programming formulation to obtain the representative scenario set while minimizing the optimality gap.","Furthermore, ex-ante and ex-post problem-driven evaluation indices are proposed to evaluate the performance of SR.","A two-stage stochastic economic dispatch problem with renewable generation and energy storage validates the effectiveness of the proposed framework.","Numerical experiments demonstrate that the proposed framework significantly outperforms existing SR methods by identifying salient (e.g., worst-case) scenarios, and achieving an optimality gap of less than 0.1% within acceptable computation time."],"url":"http://arxiv.org/abs/2404.07810v1","category":"math.OC"}
{"created":"2024-04-11 14:48:53","title":"Optimal EV Charging Scheduling at Electric Railway Stations Under Peak Load Constraints","abstract":"In this paper, a novel Energy Management System (EMS) algorithm to achieve optimal Electric Vehicle (EV) charging scheduling at the parking lots of electric railway stations is proposed. The proposed approach uncovers the potential of leveraging EV charging flexibility to prevent overloading in the combined EV charging and railway operation along with renewable generation, railway regenerative capabilities, and energy storage. Specifically, to realize end-user flexibility, each EV state of charge at departure time is introduced as an optimization variable. Peak load constraints are included in the railway EMS to properly adjust EV charging requirements during periods of high railway demand. A comprehensive numerical study using a scenario-tree approach on an actual railway line in Switzerland demonstrates the effectiveness and the feasibility of the proposed method in a practical setting under multiple scenarios.","sentences":["In this paper, a novel Energy Management System (EMS) algorithm to achieve optimal Electric Vehicle (EV) charging scheduling at the parking lots of electric railway stations is proposed.","The proposed approach uncovers the potential of leveraging EV charging flexibility to prevent overloading in the combined EV charging and railway operation along with renewable generation, railway regenerative capabilities, and energy storage.","Specifically, to realize end-user flexibility, each EV state of charge at departure time is introduced as an optimization variable.","Peak load constraints are included in the railway EMS to properly adjust EV charging requirements during periods of high railway demand.","A comprehensive numerical study using a scenario-tree approach on an actual railway line in Switzerland demonstrates the effectiveness and the feasibility of the proposed method in a practical setting under multiple scenarios."],"url":"http://arxiv.org/abs/2404.07804v1","category":"eess.SY"}
{"created":"2024-04-11 14:45:08","title":"An efficient uniqueness theorem for overcomplete tensor decomposition","abstract":"We give a new, constructive uniqueness theorem for tensor decomposition. It applies to order 3 tensors of format $n \\times n \\times p$ and can prove uniqueness of decomposition for generic tensors up to rank $r=4n/3$ as soon as $p \\geq 4$. One major advantage over Kruskal's uniqueness theorem is that our theorem has an algorithmic proof, and the resulting algorithm is efficient. Like the uniqueness theorem, it applies in the range $n \\leq r \\leq 4n/3$. As a result, we obtain the first efficient algorithm for overcomplete decomposition of generic tensors of order 3.   For instance, prior to this work it was not known how to efficiently decompose generic tensors of format $n \\times n \\times n$ and rank $r=1.01n$ (or rank $r \\leq (1+\\epsilon) n$, for some constant $\\epsilon >0$). Efficient overcomplete decomposition of generic tensors of format $n \\times n \\times 3$ remains an open problem.   Our results are based on the method of commuting extensions pioneered by Strassen for the proof of his $3n/2$ lower bound on tensor rank and border rank. In particular, we rely on an algorithm for the computation of commuting extensions recently proposed in a companion paper, and on the classical diagonalization-based \"Jennrich algorithm\" for undercomplete tensor decomposition.","sentences":["We give a new, constructive uniqueness theorem for tensor decomposition.","It applies to order 3 tensors of format $n \\times n \\times p$ and can prove uniqueness of decomposition for generic tensors up to rank $r=4n/3$ as soon as $p \\geq","4$. One major advantage over Kruskal's uniqueness theorem is that our theorem has an algorithmic proof, and the resulting algorithm is efficient.","Like the uniqueness theorem, it applies in the range $n \\leq r \\leq 4n/3$. As a result, we obtain the first efficient algorithm for overcomplete decomposition of generic tensors of order 3.   ","For instance, prior to this work it was not known how to efficiently decompose generic tensors of format $n \\times n \\times n$ and rank $r=1.01n$ (or rank $r \\leq (1+\\epsilon) n$, for some constant $\\epsilon >0$).","Efficient overcomplete decomposition of generic tensors of format $n \\times n \\times 3$ remains an open problem.   ","Our results are based on the method of commuting extensions pioneered by Strassen for the proof of his $3n/2$ lower bound on tensor rank and border rank.","In particular, we rely on an algorithm for the computation of commuting extensions recently proposed in a companion paper, and on the classical diagonalization-based \"Jennrich algorithm\" for undercomplete tensor decomposition."],"url":"http://arxiv.org/abs/2404.07801v1","category":"cs.DS"}
{"created":"2024-04-11 14:35:59","title":"DGMamba: Domain Generalization via Generalized State Space Model","abstract":"Domain generalization~(DG) aims at solving distribution shift problems in various scenes. Existing approaches are based on Convolution Neural Networks (CNNs) or Vision Transformers (ViTs), which suffer from limited receptive fields or quadratic complexities issues. Mamba, as an emerging state space model (SSM), possesses superior linear complexity and global receptive fields. Despite this, it can hardly be applied to DG to address distribution shifts, due to the hidden state issues and inappropriate scan mechanisms. In this paper, we propose a novel framework for DG, named DGMamba, that excels in strong generalizability toward unseen domains and meanwhile has the advantages of global receptive fields, and efficient linear complexity. Our DGMamba compromises two core components: Hidden State Suppressing~(HSS) and Semantic-aware Patch refining~(SPR). In particular, HSS is introduced to mitigate the influence of hidden states associated with domain-specific features during output prediction. SPR strives to encourage the model to concentrate more on objects rather than context, consisting of two designs: Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely, PFS aims to shuffle the non-semantic patches within images, creating more flexible and effective sequences from images, and DCI is designed to regularize Mamba with the combination of mismatched non-semantic and semantic information by fusing patches among domains. Extensive experiments on four commonly used DG benchmarks demonstrate that the proposed DGMamba achieves remarkably superior results to state-of-the-art models. The code will be made publicly available.","sentences":["Domain generalization~(DG) aims at solving distribution shift problems in various scenes.","Existing approaches are based on Convolution Neural Networks (CNNs) or Vision Transformers (ViTs), which suffer from limited receptive fields or quadratic complexities issues.","Mamba, as an emerging state space model (SSM), possesses superior linear complexity and global receptive fields.","Despite this, it can hardly be applied to DG to address distribution shifts, due to the hidden state issues and inappropriate scan mechanisms.","In this paper, we propose a novel framework for DG, named DGMamba, that excels in strong generalizability toward unseen domains and meanwhile has the advantages of global receptive fields, and efficient linear complexity.","Our DGMamba compromises two core components: Hidden State Suppressing~(HSS) and Semantic-aware Patch refining~(SPR).","In particular, HSS is introduced to mitigate the influence of hidden states associated with domain-specific features during output prediction.","SPR strives to encourage the model to concentrate more on objects rather than context, consisting of two designs: Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI).","Concretely, PFS aims to shuffle the non-semantic patches within images, creating more flexible and effective sequences from images, and DCI is designed to regularize Mamba with the combination of mismatched non-semantic and semantic information by fusing patches among domains.","Extensive experiments on four commonly used DG benchmarks demonstrate that the proposed DGMamba achieves remarkably superior results to state-of-the-art models.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2404.07794v1","category":"cs.CV"}
{"created":"2024-04-11 14:29:30","title":"AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation","abstract":"Scene graph generation (SGG) aims to understand the visual objects and their semantic relationships from one given image. Until now, lots of SGG datasets with the eyelevel view are released but the SGG dataset with the overhead view is scarcely studied. By contrast to the object occlusion problem in the eyelevel view, which impedes the SGG, the overhead view provides a new perspective that helps to promote the SGG by providing a clear perception of the spatial relationships of objects in the ground scene. To fill in the gap of the overhead view dataset, this paper constructs and releases an aerial image urban scene graph generation (AUG) dataset. Images from the AUG dataset are captured with the low-attitude overhead view. In the AUG dataset, 25,594 objects, 16,970 relationships, and 27,175 attributes are manually annotated. To avoid the local context being overwhelmed in the complex aerial urban scene, this paper proposes one new locality-preserving graph convolutional network (LPG). Different from the traditional graph convolutional network, which has the natural advantage of capturing the global context for SGG, the convolutional layer in the LPG integrates the non-destructive initial features of the objects with dynamically updated neighborhood information to preserve the local context under the premise of mining the global context. To address the problem that there exists an extra-large number of potential object relationship pairs but only a small part of them is meaningful in AUG, we propose the adaptive bounding box scaling factor for potential relationship detection (ABS-PRD) to intelligently prune the meaningless relationship pairs. Extensive experiments on the AUG dataset show that our LPG can significantly outperform the state-of-the-art methods and the effectiveness of the proposed locality-preserving strategy.","sentences":["Scene graph generation (SGG) aims to understand the visual objects and their semantic relationships from one given image.","Until now, lots of SGG datasets with the eyelevel view are released but the SGG dataset with the overhead view is scarcely studied.","By contrast to the object occlusion problem in the eyelevel view, which impedes the SGG, the overhead view provides a new perspective that helps to promote the SGG by providing a clear perception of the spatial relationships of objects in the ground scene.","To fill in the gap of the overhead view dataset, this paper constructs and releases an aerial image urban scene graph generation (AUG) dataset.","Images from the AUG dataset are captured with the low-attitude overhead view.","In the AUG dataset, 25,594 objects, 16,970 relationships, and 27,175 attributes are manually annotated.","To avoid the local context being overwhelmed in the complex aerial urban scene, this paper proposes one new locality-preserving graph convolutional network (LPG).","Different from the traditional graph convolutional network, which has the natural advantage of capturing the global context for SGG, the convolutional layer in the LPG integrates the non-destructive initial features of the objects with dynamically updated neighborhood information to preserve the local context under the premise of mining the global context.","To address the problem that there exists an extra-large number of potential object relationship pairs but only a small part of them is meaningful in AUG, we propose the adaptive bounding box scaling factor for potential relationship detection (ABS-PRD) to intelligently prune the meaningless relationship pairs.","Extensive experiments on the AUG dataset show that our LPG can significantly outperform the state-of-the-art methods and the effectiveness of the proposed locality-preserving strategy."],"url":"http://arxiv.org/abs/2404.07788v1","category":"cs.CV"}
{"created":"2024-04-11 14:24:31","title":"Synchronization of two non-identical Chialvo neurons","abstract":"We investigate the synchronization between two neurons using the stochastic version of the map-based Chialvo model. To simulate non-identical neurons, a mismatch is introduced in one of the main parameters of the model. Subsequently, the synchronization of the neurons is studied as a function of this mismatch, the noise introduced in the stochastic model, and the coupling strength between the neurons. We propose the simplest neuron network for study, as its analysis is more straightforward and does not compromise generality. Within this network, two nonidentical neuron maps are electrically coupled. In order to understand if specific behaviors affect the global behavior of the system, we consider different cases related to the behavior of the neurons (chaotic or periodic). Furthermore, we study how variations in model parameters affect the firing frequency in all cases. Additionally, we consider that the two neurons have both excitatory and inhibitory couplings. Consequently, we identify critical values of noise and mismatch for achieving satisfactory synchronization between the neurons in both cases. Finally, we conjecture that the results are of a general nature and are applicable to different neuron models.","sentences":["We investigate the synchronization between two neurons using the stochastic version of the map-based Chialvo model.","To simulate non-identical neurons, a mismatch is introduced in one of the main parameters of the model.","Subsequently, the synchronization of the neurons is studied as a function of this mismatch, the noise introduced in the stochastic model, and the coupling strength between the neurons.","We propose the simplest neuron network for study, as its analysis is more straightforward and does not compromise generality.","Within this network, two nonidentical neuron maps are electrically coupled.","In order to understand if specific behaviors affect the global behavior of the system, we consider different cases related to the behavior of the neurons (chaotic or periodic).","Furthermore, we study how variations in model parameters affect the firing frequency in all cases.","Additionally, we consider that the two neurons have both excitatory and inhibitory couplings.","Consequently, we identify critical values of noise and mismatch for achieving satisfactory synchronization between the neurons in both cases.","Finally, we conjecture that the results are of a general nature and are applicable to different neuron models."],"url":"http://arxiv.org/abs/2404.07783v1","category":"nlin.CD"}
{"created":"2024-04-11 14:21:47","title":"Gravitational-wave lensing in Einstein-aether theory","abstract":"Einstein-aether theory provides a model to test the validity of local Lorentz invariance in gravitational interactions. The speed of gravitational waves as measured from the binary neutron star event GW170817 sets stringent limits on Einstein-aether theory, but only on a combination of the theory's free parameters. For this reason, a significant part of the theory's parameter space remains unconstrained by observations. Motivated by this, we explore the propagation of gravitational waves in Einstein-aether theory over an inhomogeneous background (i.e., gravitational wave lensing) as a potential mechanism to break the degeneracies between the theory's free parameters, and hence enable new constraints on the theory to be obtained. By bringing the field equations into the form of the so-called kinetic matrix and applying a formalism known as the propagation eigenstate framework, we find that the speed of gravitational waves is modified by inhomogeneities in the aether field. However, the modification is common to both gravitational polarizations and vanishes in the limit in which gravitational waves propagate with luminal speed. This lens-dependent gravitational wave speed contrasts with the lens-induced birefringence observed in other theories beyond general relativity, like Horndeski's theory. While the potential to improve tests based on gravitational-wave speed is limited, our formalism sets the basis to fully describe signal propagation over inhomogeneous spacetimes in Einstein-aether theory and other extensions of general relativity.","sentences":["Einstein-aether theory provides a model to test the validity of local Lorentz invariance in gravitational interactions.","The speed of gravitational waves as measured from the binary neutron star event GW170817 sets stringent limits on Einstein-aether theory, but only on a combination of the theory's free parameters.","For this reason, a significant part of the theory's parameter space remains unconstrained by observations.","Motivated by this, we explore the propagation of gravitational waves in Einstein-aether theory over an inhomogeneous background (i.e., gravitational wave lensing) as a potential mechanism to break the degeneracies between the theory's free parameters, and hence enable new constraints on the theory to be obtained.","By bringing the field equations into the form of the so-called kinetic matrix and applying a formalism known as the propagation eigenstate framework, we find that the speed of gravitational waves is modified by inhomogeneities in the aether field.","However, the modification is common to both gravitational polarizations and vanishes in the limit in which gravitational waves propagate with luminal speed.","This lens-dependent gravitational wave speed contrasts with the lens-induced birefringence observed in other theories beyond general relativity, like Horndeski's theory.","While the potential to improve tests based on gravitational-wave speed is limited, our formalism sets the basis to fully describe signal propagation over inhomogeneous spacetimes in Einstein-aether theory and other extensions of general relativity."],"url":"http://arxiv.org/abs/2404.07782v1","category":"gr-qc"}
{"created":"2024-04-11 14:13:53","title":"Unsupervised Concept Drift Detection based on Parallel Activations of Neural Network","abstract":"Practical applications of artificial intelligence increasingly often have to deal with the streaming properties of real data, which, considering the time factor, are subject to phenomena such as periodicity and more or less chaotic degeneration - resulting directly in the concept drifts. The modern concept drift detectors almost always assume immediate access to labels, which due to their cost, limited availability and possible delay has been shown to be unrealistic. This work proposes an unsupervised Parallel Activations Drift Detector, utilizing the outputs of an untrained neural network, presenting its key design elements, intuitions about processing properties, and a pool of computer experiments demonstrating its competitiveness with state-of-the-art methods.","sentences":["Practical applications of artificial intelligence increasingly often have to deal with the streaming properties of real data, which, considering the time factor, are subject to phenomena such as periodicity and more or less chaotic degeneration - resulting directly in the concept drifts.","The modern concept drift detectors almost always assume immediate access to labels, which due to their cost, limited availability and possible delay has been shown to be unrealistic.","This work proposes an unsupervised Parallel Activations Drift Detector, utilizing the outputs of an untrained neural network, presenting its key design elements, intuitions about processing properties, and a pool of computer experiments demonstrating its competitiveness with state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.07776v1","category":"cs.LG"}
{"created":"2024-04-11 14:13:44","title":"Discourse-Aware In-Context Learning for Temporal Expression Normalization","abstract":"Temporal expression (TE) normalization is a well-studied problem. However, the predominately used rule-based systems are highly restricted to specific settings, and upcoming machine learning approaches suffer from a lack of labeled data. In this work, we explore the feasibility of proprietary and open-source large language models (LLMs) for TE normalization using in-context learning to inject task, document, and example information into the model. We explore various sample selection strategies to retrieve the most relevant set of examples. By using a window-based prompt design approach, we can perform TE normalization across sentences, while leveraging the LLM knowledge without training the model. Our experiments show competitive results to models designed for this task. In particular, our method achieves large performance improvements for non-standard settings by dynamically including relevant examples during inference.","sentences":["Temporal expression (TE) normalization is a well-studied problem.","However, the predominately used rule-based systems are highly restricted to specific settings, and upcoming machine learning approaches suffer from a lack of labeled data.","In this work, we explore the feasibility of proprietary and open-source large language models (LLMs) for TE normalization using in-context learning to inject task, document, and example information into the model.","We explore various sample selection strategies to retrieve the most relevant set of examples.","By using a window-based prompt design approach, we can perform TE normalization across sentences, while leveraging the LLM knowledge without training the model.","Our experiments show competitive results to models designed for this task.","In particular, our method achieves large performance improvements for non-standard settings by dynamically including relevant examples during inference."],"url":"http://arxiv.org/abs/2404.07775v1","category":"cs.CL"}
{"created":"2024-04-11 14:09:41","title":"Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts for Language-Guided Robot Manipulation","abstract":"Our goal is to build embodied agents that can learn inductively generalizable spatial concepts in a continual manner, e.g, constructing a tower of a given height. Existing work suffers from certain limitations (a) (Liang et al., 2023) and their multi-modal extensions, rely heavily on prior knowledge and are not grounded in the demonstrations (b) (Liu et al., 2023) lack the ability to generalize due to their purely neural approach. A key challenge is to achieve a fine balance between symbolic representations which have the capability to generalize, and neural representations that are physically grounded. In response, we propose a neuro-symbolic approach by expressing inductive concepts as symbolic compositions over grounded neural concepts. Our key insight is to decompose the concept learning problem into the following steps 1) Sketch: Getting a programmatic representation for the given instruction 2) Plan: Perform Model-Based RL over the sequence of grounded neural action concepts to learn a grounded plan 3) Generalize: Abstract out a generic (lifted) Python program to facilitate generalizability. Continual learning is achieved by interspersing learning of grounded neural concepts with higher level symbolic constructs. Our experiments demonstrate that our approach significantly outperforms existing baselines in terms of its ability to learn novel concepts and generalize inductively.","sentences":["Our goal is to build embodied agents that can learn inductively generalizable spatial concepts in a continual manner, e.g, constructing a tower of a given height.","Existing work suffers from certain limitations (a) (Liang et al., 2023) and their multi-modal extensions, rely heavily on prior knowledge and are not grounded in the demonstrations (b) (Liu et al., 2023) lack the ability to generalize due to their purely neural approach.","A key challenge is to achieve a fine balance between symbolic representations which have the capability to generalize, and neural representations that are physically grounded.","In response, we propose a neuro-symbolic approach by expressing inductive concepts as symbolic compositions over grounded neural concepts.","Our key insight is to decompose the concept learning problem into the following steps 1) Sketch: Getting a programmatic representation for the given instruction 2) Plan: Perform Model-Based RL over the sequence of grounded neural action concepts to learn a grounded plan 3) Generalize:","Abstract out a generic (lifted) Python program to facilitate generalizability.","Continual learning is achieved by interspersing learning of grounded neural concepts with higher level symbolic constructs.","Our experiments demonstrate that our approach significantly outperforms existing baselines in terms of its ability to learn novel concepts and generalize inductively."],"url":"http://arxiv.org/abs/2404.07774v1","category":"cs.LG"}
{"created":"2024-04-11 14:08:45","title":"ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model","abstract":"Object detection, a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology. In the present study, we introduce a novel framework designed to articulate object detection as a denoising diffusion process, which operates on perturbed bounding boxes of annotated entities. This framework, termed ConsistencyDet, leverages an innovative denoising concept known as the Consistency Model. The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any temporal stage back to its pristine state, thereby realizing a ``one-step denoising'' mechanism. Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional Diffusion Model. Throughout the training phase, ConsistencyDet initiates the diffusion sequence with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task. Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution. Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into the definitive detections. Comprehensive evaluations employing standard benchmarks, such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics.","sentences":["Object detection, a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology.","In the present study, we introduce a novel framework designed to articulate object detection as a denoising diffusion process, which operates on perturbed bounding boxes of annotated entities.","This framework, termed ConsistencyDet, leverages an innovative denoising concept known as the Consistency Model.","The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any temporal stage back to its pristine state, thereby realizing a ``one-step denoising'' mechanism.","Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional Diffusion Model.","Throughout the training phase, ConsistencyDet initiates the diffusion sequence with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task.","Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution.","Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into the definitive detections.","Comprehensive evaluations employing standard benchmarks, such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics."],"url":"http://arxiv.org/abs/2404.07773v1","category":"cs.CV"}
{"created":"2024-04-11 14:07:25","title":"An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization","abstract":"Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.","sentences":["Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology.","In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties.","Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models.","In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls.","Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities.","We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts.","Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models.","Lastly, we discuss future directions about diffusion models.","The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models."],"url":"http://arxiv.org/abs/2404.07771v1","category":"cs.LG"}
{"created":"2024-04-11 14:07:16","title":"Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations","abstract":"Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods.","sentences":["Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously.","Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component.","Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted.","To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed.","Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance.","To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features.","Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods."],"url":"http://arxiv.org/abs/2404.07770v1","category":"cs.CV"}
{"created":"2024-04-11 14:06:39","title":"Using Letter Positional Probabilities to Assess Word Complexity","abstract":"Word complexity is defined in a number of different ways. Psycholinguistic, morphological and lexical proxies are often used. Human ratings are also used. The problem here is that these proxies do not measure complexity directly, and human ratings are subject to subjective bias. In this study we contend that some form of 'latent complexity' can be approximated by using samples of simple and complex words. We use a sample of 'simple' words from primary school picture books and a sample of 'complex' words from high school and academic settings. In order to analyse the differences between these classes, we look at the letter positional probabilities (LPPs). We find a strong statistical association between simple and complex words on the basis of LPPs. For example, simple words are significantly (p<.001) more likely to start with w, b, s, h, g,k, j,t y or f, while complex words are significantly (p<.001) more likely to start with i, a, e, r, v, u or d. We find similar strong associations for subsequent letter positions, with 84 letter-position variables in the first 6 positions being significant at the p<.001 level. We then use LPPs as variables in creating a classifier which can classify the two classes with an 83% accuracy. We test these findings using a second data set, with 66 LPPs significant (p<.001) in the first 6 positions common to both datasets. We use these 66 variables to create a classifier that is able to classify a third dataset with an accuracy of 70%. Finally, we create a fourth sample by combining the extreme high and low scoring words generated by three classifiers built on the first three separate datasets and use this sample to build a classifier which has an accuracy of 97%. We use this to score the four levels of English word groups from an ESL program.","sentences":["Word complexity is defined in a number of different ways.","Psycholinguistic, morphological and lexical proxies are often used.","Human ratings are also used.","The problem here is that these proxies do not measure complexity directly, and human ratings are subject to subjective bias.","In this study we contend that some form of 'latent complexity' can be approximated by using samples of simple and complex words.","We use a sample of 'simple' words from primary school picture books and a sample of 'complex' words from high school and academic settings.","In order to analyse the differences between these classes, we look at the letter positional probabilities (LPPs).","We find a strong statistical association between simple and complex words on the basis of LPPs.","For example, simple words are significantly (p<.001) more likely to start with w, b, s, h, g,k, j,t y or f, while complex words are significantly (p<.001) more likely to start with i, a, e, r, v, u or d. We find similar strong associations for subsequent letter positions, with 84 letter-position variables in the first 6 positions being significant at the p<.001 level.","We then use LPPs as variables in creating a classifier which can classify the two classes with an 83% accuracy.","We test these findings using a second data set, with 66 LPPs significant (p<.001) in the first 6 positions common to both datasets.","We use these 66 variables to create a classifier that is able to classify a third dataset with an accuracy of 70%.","Finally, we create a fourth sample by combining the extreme high and low scoring words generated by three classifiers built on the first three separate datasets and use this sample to build a classifier which has an accuracy of 97%.","We use this to score the four levels of English word groups from an ESL program."],"url":"http://arxiv.org/abs/2404.07768v1","category":"cs.CL"}
{"created":"2024-04-11 14:04:36","title":"AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports","abstract":"Monitoring the threat landscape to be aware of actual or potential attacks is of utmost importance to cybersecurity professionals. Information about cyber threats is typically distributed using natural language reports. Natural language processing can help with managing this large amount of unstructured information, yet to date, the topic has received little attention. With this paper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat reports. The reports have been annotated by a domain expert with named entities, temporal expressions, and cybersecurity-specific concepts including implicitly mentioned techniques and tactics. Entities and concepts are linked to Wikipedia and the MITRE ATT&CK knowledge base, the most widely-used taxonomy for classifying types of attacks. Prior datasets linking to MITRE ATT&CK either provide a single label per document or annotate sentences out-of-context; our dataset annotates entire documents in a much finer-grained way. In an experimental study, we model the annotations of our dataset using state-of-the-art neural models. In our few-shot scenario, we find that for identifying the MITRE ATT&CK concepts that are mentioned explicitly or implicitly in a text, concept descriptions from MITRE ATT&CK are an effective source for training data augmentation.","sentences":["Monitoring the threat landscape to be aware of actual or potential attacks is of utmost importance to cybersecurity professionals.","Information about cyber threats is typically distributed using natural language reports.","Natural language processing can help with managing this large amount of unstructured information, yet to date, the topic has received little attention.","With this paper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat reports.","The reports have been annotated by a domain expert with named entities, temporal expressions, and cybersecurity-specific concepts including implicitly mentioned techniques and tactics.","Entities and concepts are linked to Wikipedia and the MITRE ATT&CK knowledge base, the most widely-used taxonomy for classifying types of attacks.","Prior datasets linking to MITRE ATT&CK either provide a single label per document or annotate sentences out-of-context; our dataset annotates entire documents in a much finer-grained way.","In an experimental study, we model the annotations of our dataset using state-of-the-art neural models.","In our few-shot scenario, we find that for identifying the MITRE ATT&CK concepts that are mentioned explicitly or implicitly in a text, concept descriptions from MITRE ATT&CK are an effective source for training data augmentation."],"url":"http://arxiv.org/abs/2404.07765v1","category":"cs.CL"}
{"created":"2024-04-11 14:03:14","title":"An Application Layer Multi-Hop Collective Perception Service for Vehicular Adhoc Networks","abstract":"Collective Perception will play a crucial role for ensuring vehicular safety in the near future, enabling the sharing of local perceived objects with other Intelligent Transport System Stations (ITS-Ss). However, at the beginning of the roll-out, low market penetration rates are expected. This paper proposes and evaluates an application layer multi-hop Collective Perception Service (CPS) for vehicular ad-hoc networks. The goal is to improve the environmental awareness ratio in scenarios with low CPS market penetration. In such scenarios, the CPS service without forwarding enabled struggles to achieve complete awareness. A decentralized application layer forwarding algorithm is presented that shares perceived object information across multiple hops while maintaining a low age of information. The proposed approach is compared against standard CPS with no forwarding and CPS with geographically-scoped (GBC) multi-hop forwarding. Simulations according to standards of the European Telecommunications Standards Institute (ETSI) demonstrate that the application layer forwarding achieves near 100% awareness at 10% penetration rate versus 92% for standard CPS. The awareness improvement comes with moderate channel load, unlike GBC forwarding which quickly saturates the channel. The median age of information remains below 80 ms for the proposed scheme, enabling real-time CPS operation. Our application layer multi-hop approach effectively improves environmental awareness during initial CPS deployment while aligning with latency and channel load requirements.","sentences":["Collective Perception will play a crucial role for ensuring vehicular safety in the near future, enabling the sharing of local perceived objects with other Intelligent Transport System Stations (ITS-Ss).","However, at the beginning of the roll-out, low market penetration rates are expected.","This paper proposes and evaluates an application layer multi-hop Collective Perception Service (CPS) for vehicular ad-hoc networks.","The goal is to improve the environmental awareness ratio in scenarios with low CPS market penetration.","In such scenarios, the CPS service without forwarding enabled struggles to achieve complete awareness.","A decentralized application layer forwarding algorithm is presented that shares perceived object information across multiple hops while maintaining a low age of information.","The proposed approach is compared against standard CPS with no forwarding and CPS with geographically-scoped (GBC) multi-hop forwarding.","Simulations according to standards of the European Telecommunications Standards Institute (ETSI) demonstrate that the application layer forwarding achieves near 100% awareness at 10% penetration rate versus 92% for standard CPS.","The awareness improvement comes with moderate channel load, unlike GBC forwarding which quickly saturates the channel.","The median age of information remains below 80 ms for the proposed scheme, enabling real-time CPS operation.","Our application layer multi-hop approach effectively improves environmental awareness during initial CPS deployment while aligning with latency and channel load requirements."],"url":"http://arxiv.org/abs/2404.07761v1","category":"cs.NI"}
{"created":"2024-04-11 14:02:55","title":"RIS-Assisted OTFS Communications: Phase Configuration via Received Energy Maximization","abstract":"In this paper, we explore the integration of two revolutionary technologies, reconfigurable intelligent surfaces (RISs) and orthogonal time frequency space (OTFS) modulation, to enhance high-speed wireless communications. We introduce a novel phase shift design algorithm for RIS-assisted OTFS, optimizing energy reception and channel gain in dynamic environments. The study evaluates the proposed approach in a downlink scenario, demonstrating significant performance improvements compared to benchmark schemes in the literature, particularly in terms of bit error rate (BER). Our results showcase the potential of RIS to enhance the system's performance. Specifically, our proposed phase shift design technique outperforms the benchmark solutions by over 4 dB. Furthermore, even greater gains can be obtained as the number of RIS elements increases.","sentences":["In this paper, we explore the integration of two revolutionary technologies, reconfigurable intelligent surfaces (RISs) and orthogonal time frequency space (OTFS) modulation, to enhance high-speed wireless communications.","We introduce a novel phase shift design algorithm for RIS-assisted OTFS, optimizing energy reception and channel gain in dynamic environments.","The study evaluates the proposed approach in a downlink scenario, demonstrating significant performance improvements compared to benchmark schemes in the literature, particularly in terms of bit error rate (BER).","Our results showcase the potential of RIS to enhance the system's performance.","Specifically, our proposed phase shift design technique outperforms the benchmark solutions by over 4 dB. Furthermore, even greater gains can be obtained as the number of RIS elements increases."],"url":"http://arxiv.org/abs/2404.07759v1","category":"cs.IT"}
{"created":"2024-04-11 14:02:23","title":"Spacetimes with homogeneous and isotropic expansion","abstract":"In this short note, we define and characterize all the spacetimes admitting observers to whom the cosmic expansion is homogeneous and isotropic and interpret their Einstein's equations.","sentences":["In this short note, we define and characterize all the spacetimes admitting observers to whom the cosmic expansion is homogeneous and isotropic and interpret their Einstein's equations."],"url":"http://arxiv.org/abs/2404.07757v1","category":"gr-qc"}
{"created":"2024-04-11 14:00:20","title":"Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification","abstract":"Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.","sentences":["Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data.","These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation.","Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images?","How easily can they be generated?","How useful are they for ML researchers, and what is their potential for Open Science?","In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms.","We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics.","Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing.","Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification."],"url":"http://arxiv.org/abs/2404.07754v1","category":"cs.CV"}
{"created":"2024-04-11 13:54:15","title":"Mitigating Vulnerable Road Users Occlusion Risk Via Collective Perception: An Empirical Analysis","abstract":"Recent reports from the World Health Organization highlight that Vulnerable Road Users (VRUs) have been involved in over half of the road fatalities in recent years, with occlusion risk - a scenario where VRUs are hidden from drivers' view by obstacles like parked vehicles - being a critical contributing factor. To address this, we present a novel algorithm that quantifies occlusion risk based on the dynamics of both vehicles and VRUs. This algorithm has undergone testing and evaluation using a real-world dataset from German intersections. Additionally, we introduce the concept of Maximum Tracking Loss (MTL), which measures the longest consecutive duration a VRU remains untracked by any vehicle in a given scenario. Our study extends to examining the role of the Collective Perception Service (CPS) in VRU safety. CPS enhances safety by enabling vehicles to share sensor information, thereby potentially reducing occlusion risks. Our analysis reveals that a 25% market penetration of CPS-equipped vehicles can substantially diminish occlusion risks and significantly curtail MTL. These findings demonstrate how various scenarios pose different levels of risk to VRUs and how the deployment of Collective Perception can markedly improve their safety. Furthermore, they underline the efficacy of our proposed metrics to capture occlusion risk as a safety factor.","sentences":["Recent reports from the World Health Organization highlight that Vulnerable Road Users (VRUs) have been involved in over half of the road fatalities in recent years, with occlusion risk - a scenario where VRUs are hidden from drivers' view by obstacles like parked vehicles - being a critical contributing factor.","To address this, we present a novel algorithm that quantifies occlusion risk based on the dynamics of both vehicles and VRUs.","This algorithm has undergone testing and evaluation using a real-world dataset from German intersections.","Additionally, we introduce the concept of Maximum Tracking Loss (MTL), which measures the longest consecutive duration a VRU remains untracked by any vehicle in a given scenario.","Our study extends to examining the role of the Collective Perception Service (CPS) in VRU safety.","CPS enhances safety by enabling vehicles to share sensor information, thereby potentially reducing occlusion risks.","Our analysis reveals that a 25% market penetration of CPS-equipped vehicles can substantially diminish occlusion risks and significantly curtail MTL.","These findings demonstrate how various scenarios pose different levels of risk to VRUs and how the deployment of Collective Perception can markedly improve their safety.","Furthermore, they underline the efficacy of our proposed metrics to capture occlusion risk as a safety factor."],"url":"http://arxiv.org/abs/2404.07753v1","category":"cs.NI"}
{"created":"2024-04-11 13:48:48","title":"Generating consistent PDDL domains with Large Language Models","abstract":"Large Language Models (LLMs) are capable of transforming natural language domain descriptions into plausibly looking PDDL markup. However, ensuring that actions are consistent within domains still remains a challenging task. In this paper we present a novel concept to significantly improve the quality of LLM-generated PDDL models by performing automated consistency checking during the generation process. Although the proposed consistency checking strategies still can't guarantee absolute correctness of generated models, they can serve as valuable source of feedback reducing the amount of correction efforts expected from a human in the loop. We demonstrate the capabilities of our error detection approach on a number of classical and custom planning domains (logistics, gripper, tyreworld, household, pizza).","sentences":["Large Language Models (LLMs) are capable of transforming natural language domain descriptions into plausibly looking PDDL markup.","However, ensuring that actions are consistent within domains still remains a challenging task.","In this paper we present a novel concept to significantly improve the quality of LLM-generated PDDL models by performing automated consistency checking during the generation process.","Although the proposed consistency checking strategies still can't guarantee absolute correctness of generated models, they can serve as valuable source of feedback reducing the amount of correction efforts expected from a human in the loop.","We demonstrate the capabilities of our error detection approach on a number of classical and custom planning domains (logistics, gripper, tyreworld, household, pizza)."],"url":"http://arxiv.org/abs/2404.07751v1","category":"cs.RO"}
{"created":"2024-04-11 13:47:53","title":"The Broken Rung: Gender and the Leadership Gap","abstract":"Addressing female underrepresentation in leadership positions has become a key policy objective. However, little is known about the extent to which leadership appeals differently to women. Collecting new data from a large firm, I document that women are substantially less likely to apply for early-career promotions. Realized application patterns and large-scale surveys reveal the role of an understudied feature of promotions -- having to assume responsibility over a team -- which is less appealing to women. This gender difference is not accounted for by standard explanations, such as success likelihood or confidence, but is rather a product of common design features of leadership positions.","sentences":["Addressing female underrepresentation in leadership positions has become a key policy objective.","However, little is known about the extent to which leadership appeals differently to women.","Collecting new data from a large firm, I document that women are substantially less likely to apply for early-career promotions.","Realized application patterns and large-scale surveys reveal the role of an understudied feature of promotions -- having to assume responsibility over a team -- which is less appealing to women.","This gender difference is not accounted for by standard explanations, such as success likelihood or confidence, but is rather a product of common design features of leadership positions."],"url":"http://arxiv.org/abs/2404.07750v1","category":"econ.GN"}
{"created":"2024-04-11 13:38:01","title":"The solar dynamo begins near the surface","abstract":"The Sun's magnetic dynamo cycle features a distinct pattern: a propagating region of sunspot emergence appears around 30 degrees latitude and vanishes near the equator every 11 years. Moreover, longitudinal flows called ``torsional oscillations\" closely shadow sunspot migration, undoubtedly sharing a common. Contrary to theories suggesting deep origins for these phenomena, helioseismology pinpoints low-latitude torsional oscillations to the Sun's outer 5-10 percent, the ``Near-Surface Shear Layer\". Within this zone, inwardly increasing differential rotation coupled with a poloidal magnetic field strongly implicates the Magneto-Rotational Instability renowned in accretion-disk theory and observed in laboratory experiments. Together, these two facts prompt the general question: Is it possible that the solar dynamo is a near-surface instability? Here, we report strong affirmative evidence in stark contrast to traditional paradigms focusing on the deeper tachocline. Simple analytic estimates show that the near-surface magneto-rotational instability better explains the spatiotemporal scales of the torsional oscillations and inferred subsurface magnetic field amplitudes. State-of-the-art numerical simulations corroborate these estimates and, strikingly, reproduce hemispherical magnetic current helicity laws. The dynamo resulting from a well-understood near-surface phenomenon improves prospects for accurate predictions of full magnetic cycles and space weather, impacting Earth's electromagnetic infrastructure.","sentences":["The Sun's magnetic dynamo cycle features a distinct pattern: a propagating region of sunspot emergence appears around 30 degrees latitude and vanishes near the equator every 11 years.","Moreover, longitudinal flows called ``torsional oscillations\" closely shadow sunspot migration, undoubtedly sharing a common.","Contrary to theories suggesting deep origins for these phenomena, helioseismology pinpoints low-latitude torsional oscillations to the Sun's outer 5-10 percent, the ``Near-Surface Shear Layer\".","Within this zone, inwardly increasing differential rotation coupled with a poloidal magnetic field strongly implicates the Magneto-Rotational Instability renowned in accretion-disk theory and observed in laboratory experiments.","Together, these two facts prompt the general question: Is it possible that the solar dynamo is a near-surface instability?","Here, we report strong affirmative evidence in stark contrast to traditional paradigms focusing on the deeper tachocline.","Simple analytic estimates show that the near-surface magneto-rotational instability better explains the spatiotemporal scales of the torsional oscillations and inferred subsurface magnetic field amplitudes.","State-of-the-art numerical simulations corroborate these estimates and, strikingly, reproduce hemispherical magnetic current helicity laws.","The dynamo resulting from a well-understood near-surface phenomenon improves prospects for accurate predictions of full magnetic cycles and space weather, impacting Earth's electromagnetic infrastructure."],"url":"http://arxiv.org/abs/2404.07740v1","category":"astro-ph.SR"}
{"created":"2024-04-11 13:36:29","title":"ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models","abstract":"Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts. To enhance its productivity, we propose a ResearchAgent, a large language model-powered research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature. Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic graph but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers. In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively. Further, they are instantiated with human preference-aligned large language models whose criteria for evaluation are derived from actual human judgments. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results.","sentences":["Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts.","To enhance its productivity, we propose a ResearchAgent, a large language model-powered research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature.","Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic graph but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers.","In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively.","Further, they are instantiated with human preference-aligned large language models whose criteria for evaluation are derived from actual human judgments.","We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results."],"url":"http://arxiv.org/abs/2404.07738v1","category":"cs.CL"}
{"created":"2024-04-11 13:30:03","title":"Diffusing in Someone Else's Shoes: Robotic Perspective Taking with Diffusion","abstract":"Humanoid robots can benefit from their similarity to the human shape by learning from humans. When humans teach other humans how to perform actions, they often demonstrate the actions and the learning human can try to imitate the demonstration. Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans. As this is a challenging task, it is often simplified for robots by creating a demonstration in the first-person perspective. Creating these demonstrations requires more effort but allows for an easier imitation. We introduce a novel diffusion model aimed at enabling the robot to directly learn from the third-person demonstrations. Our model is capable of learning and generating the first-person perspective from the third-person perspective by translating the size and rotations of objects and the environment between two perspectives. This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations. The model can either represent the first-person perspective in an RGB image or calculate the joint values. Our approach significantly outperforms other image-to-image models in this task.","sentences":["Humanoid robots can benefit from their similarity to the human shape by learning from humans.","When humans teach other humans how to perform actions, they often demonstrate the actions and the learning human can try to imitate the demonstration.","Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans.","As this is a challenging task, it is often simplified for robots by creating a demonstration in the first-person perspective.","Creating these demonstrations requires more effort but allows for an easier imitation.","We introduce a novel diffusion model aimed at enabling the robot to directly learn from the third-person demonstrations.","Our model is capable of learning and generating the first-person perspective from the third-person perspective by translating the size and rotations of objects and the environment between two perspectives.","This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations.","The model can either represent the first-person perspective in an RGB image or calculate the joint values.","Our approach significantly outperforms other image-to-image models in this task."],"url":"http://arxiv.org/abs/2404.07735v1","category":"cs.RO"}
{"created":"2024-04-11 13:25:35","title":"Monte Carlo Tree Search with Boltzmann Exploration","abstract":"Monte-Carlo Tree Search (MCTS) methods, such as Upper Confidence Bound applied to Trees (UCT), are instrumental to automated planning techniques. However, UCT can be slow to explore an optimal action when it initially appears inferior to other actions. Maximum ENtropy Tree-Search (MENTS) incorporates the maximum entropy principle into an MCTS approach, utilising Boltzmann policies to sample actions, naturally encouraging more exploration. In this paper, we highlight a major limitation of MENTS: optimal actions for the maximum entropy objective do not necessarily correspond to optimal actions for the original objective. We introduce two algorithms, Boltzmann Tree Search (BTS) and Decaying ENtropy Tree-Search (DENTS), that address these limitations and preserve the benefits of Boltzmann policies, such as allowing actions to be sampled faster by using the Alias method. Our empirical analysis shows that our algorithms show consistent high performance across several benchmark domains, including the game of Go.","sentences":["Monte-Carlo Tree Search (MCTS) methods, such as Upper Confidence Bound applied to Trees (UCT), are instrumental to automated planning techniques.","However, UCT can be slow to explore an optimal action when it initially appears inferior to other actions.","Maximum ENtropy Tree-Search (MENTS) incorporates the maximum entropy principle into an MCTS approach, utilising Boltzmann policies to sample actions, naturally encouraging more exploration.","In this paper, we highlight a major limitation of MENTS: optimal actions for the maximum entropy objective do not necessarily correspond to optimal actions for the original objective.","We introduce two algorithms, Boltzmann Tree Search (BTS) and Decaying ENtropy Tree-Search (DENTS), that address these limitations and preserve the benefits of Boltzmann policies, such as allowing actions to be sampled faster by using the Alias method.","Our empirical analysis shows that our algorithms show consistent high performance across several benchmark domains, including the game of Go."],"url":"http://arxiv.org/abs/2404.07732v1","category":"cs.AI"}
{"created":"2024-04-11 13:19:46","title":"Realistic Continual Learning Approach using Pre-trained Models","abstract":"Continual learning (CL) is crucial for evaluating adaptability in learning solutions to retain knowledge. Our research addresses the challenge of catastrophic forgetting, where models lose proficiency in previously learned tasks as they acquire new ones. While numerous solutions have been proposed, existing experimental setups often rely on idealized class-incremental learning scenarios. We introduce Realistic Continual Learning (RealCL), a novel CL paradigm where class distributions across tasks are random, departing from structured setups.   We also present CLARE (Continual Learning Approach with pRE-trained models for RealCL scenarios), a pre-trained model-based solution designed to integrate new knowledge while preserving past learning. Our contributions include pioneering RealCL as a generalization of traditional CL setups, proposing CLARE as an adaptable approach for RealCL tasks, and conducting extensive experiments demonstrating its effectiveness across various RealCL scenarios. Notably, CLARE outperforms existing models on RealCL benchmarks, highlighting its versatility and robustness in unpredictable learning environments.","sentences":["Continual learning (CL) is crucial for evaluating adaptability in learning solutions to retain knowledge.","Our research addresses the challenge of catastrophic forgetting, where models lose proficiency in previously learned tasks as they acquire new ones.","While numerous solutions have been proposed, existing experimental setups often rely on idealized class-incremental learning scenarios.","We introduce Realistic Continual Learning (RealCL), a novel CL paradigm where class distributions across tasks are random, departing from structured setups.   ","We also present CLARE (Continual Learning Approach with pRE-trained models for RealCL scenarios), a pre-trained model-based solution designed to integrate new knowledge while preserving past learning.","Our contributions include pioneering RealCL as a generalization of traditional CL setups, proposing CLARE as an adaptable approach for RealCL tasks, and conducting extensive experiments demonstrating its effectiveness across various RealCL scenarios.","Notably, CLARE outperforms existing models on RealCL benchmarks, highlighting its versatility and robustness in unpredictable learning environments."],"url":"http://arxiv.org/abs/2404.07729v1","category":"cs.LG"}
{"created":"2024-04-11 13:17:10","title":"A Giant Metrewave Radio Telescope Survey of Radio-loud Broad Absorption Line Quasars","abstract":"A substantial fraction of quasars display broad absorption lines (BALs) in their rest-frame ultraviolet spectra. While the origin of BALs is thought to be related to the accretion disc wind, it remains unclear whether the observed ratio of BAL to non-BAL quasars is due to orientation. We conducted observations of 48 BAL quasars and the same number of non-BAL quasars at 322 MHz using the Giant Metrewave Radio Telescope. Combined with previous flux measurements ranging from MHz to GHz frequencies, we compared continuum radio spectra between the two quasar groups. These data offer insights into low-frequency radio properties that have been difficult to investigate with previous observations only at GHz frequencies. Our results present that $73\\pm13$ per cent of the BAL quasars exhibit steep or peaked spectra, a higher proportion than $44 \\pm 14$ per cent observed in the non-BAL quasars. In contrast, there are no discernible differences between the two quasar groups in the radio luminosity, peak frequency, and spectral index distributions of sources with steep or peaked spectra and sources with flat or inverted spectra. Generally, as the jet axis and line of sight become closer to parallel, quasars exhibit flat or inverted spectra rather than steep or peaked ones. Therefore, these results suggest that BAL quasars are more frequently observed farther from the jet axis than non-BAL quasars. However, given that a certain proportion of BAL quasars exhibit flat or inverted spectra, more than the simple orientation scenario is required to elucidate the radio properties of BAL quasars.","sentences":["A substantial fraction of quasars display broad absorption lines (BALs) in their rest-frame ultraviolet spectra.","While the origin of BALs is thought to be related to the accretion disc wind, it remains unclear whether the observed ratio of BAL to non-BAL quasars is due to orientation.","We conducted observations of 48 BAL quasars and the same number of non-BAL quasars at 322 MHz using the Giant Metrewave Radio Telescope.","Combined with previous flux measurements ranging from MHz to GHz frequencies, we compared continuum radio spectra between the two quasar groups.","These data offer insights into low-frequency radio properties that have been difficult to investigate with previous observations only at GHz frequencies.","Our results present that $73\\pm13$ per cent of the BAL quasars exhibit steep or peaked spectra, a higher proportion than $44 \\pm 14$ per cent observed in the non-BAL quasars.","In contrast, there are no discernible differences between the two quasar groups in the radio luminosity, peak frequency, and spectral index distributions of sources with steep or peaked spectra and sources with flat or inverted spectra.","Generally, as the jet axis and line of sight become closer to parallel, quasars exhibit flat or inverted spectra rather than steep or peaked ones.","Therefore, these results suggest that BAL quasars are more frequently observed farther from the jet axis than non-BAL quasars.","However, given that a certain proportion of BAL quasars exhibit flat or inverted spectra, more than the simple orientation scenario is required to elucidate the radio properties of BAL quasars."],"url":"http://arxiv.org/abs/2404.07726v1","category":"astro-ph.HE"}
{"created":"2024-04-11 13:16:51","title":"Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models","abstract":"The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N=40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N=136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.","sentences":["The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL).","However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong.","In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N=40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N=136).","Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations.","This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions."],"url":"http://arxiv.org/abs/2404.07725v1","category":"cs.HC"}
{"created":"2024-04-11 13:16:47","title":"Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models","abstract":"Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.","sentences":["Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models.","Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image.","We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle.","We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality.","This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40.","We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL.","We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance."],"url":"http://arxiv.org/abs/2404.07724v1","category":"cs.CV"}
{"created":"2024-04-11 13:13:35","title":"The classical-quantum hybrid canonical dynamics and its difficulties with special and general relativity","abstract":"We discuss the Hamiltonian hybrid coupling between a classical and a quantum subsystem. If applicable to classical gravity coupled to quantized matter, this hybrid theory might realize a captivating `postquantum' alternative to full quantum-gravity. We summarize the nonrelativistic hybrid dynamics in improved formalism adequate to Hamiltonian systems. The mandatory decoherence and diffusion terms become divergent in special and general relativistic extensions. It is not yet known if any renormalization method might reconcile Markovian decoherence and diffusion with relativity. Postquantum gravity could previously only be realized in the Newtonian approximation. We argue that pending problems of the recently proposed general relativistic postquantum theory will not be solved if Markovian diffusion/decoherence are truly incompatible with relativity.","sentences":["We discuss the Hamiltonian hybrid coupling between a classical and a quantum subsystem.","If applicable to classical gravity coupled to quantized matter, this hybrid theory might realize a captivating `postquantum' alternative to full quantum-gravity.","We summarize the nonrelativistic hybrid dynamics in improved formalism adequate to Hamiltonian systems.","The mandatory decoherence and diffusion terms become divergent in special and general relativistic extensions.","It is not yet known if any renormalization method might reconcile Markovian decoherence and diffusion with relativity.","Postquantum gravity could previously only be realized in the Newtonian approximation.","We argue that pending problems of the recently proposed general relativistic postquantum theory will not be solved if Markovian diffusion/decoherence are truly incompatible with relativity."],"url":"http://arxiv.org/abs/2404.07723v1","category":"gr-qc"}
{"created":"2024-04-11 13:12:13","title":"Nonlinear Economic State Equilibria via van der Waals Modeling","abstract":"The renowned van der Waals (VDW) state equation quantifies the equilibrium relationship between pressure $P$, volume $V$ and temperature $k_{B}T$ of a real gas. We assign new variable interpretations adapted to the economic context: $P \\rightarrow Y$, representing price; $V \\rightarrow X$, representing demand; and $k_{B}T \\rightarrow \\kappa$, representing income, to describe an economic state equilibrium. With this reinterpretation, the price elasticity of demand (PED) and the income elasticity of demand (YED) are non-constant factors and may exhibit a singularity of the cusp-catastrophe type. Within this economic framework, the counterpart of VDW liquid-gas phase transition illustrates a substitution mechanism where one product or service is replaced by an alternative substitute. The conceptual relevance of this reinterpretation is discussed qualitatively and quantitatively via several illustrations ranging from transport (carpooling), medical context (generic versus original medication) and empirical data drawn from the electricity market in Germany.","sentences":["The renowned van der Waals (VDW) state equation quantifies the equilibrium relationship between pressure $P$, volume $V$ and temperature $k_{B}T$ of a real gas.","We assign new variable interpretations adapted to the economic context: $P \\rightarrow Y$, representing price; $V \\rightarrow X$, representing demand; and $k_{B}T \\rightarrow \\kappa$, representing income, to describe an economic state equilibrium.","With this reinterpretation, the price elasticity of demand (PED) and the income elasticity of demand (YED) are non-constant factors and may exhibit a singularity of the cusp-catastrophe type.","Within this economic framework, the counterpart of VDW liquid-gas phase transition illustrates a substitution mechanism where one product or service is replaced by an alternative substitute.","The conceptual relevance of this reinterpretation is discussed qualitatively and quantitatively via several illustrations ranging from transport (carpooling), medical context (generic versus original medication) and empirical data drawn from the electricity market in Germany."],"url":"http://arxiv.org/abs/2404.07722v1","category":"nlin.AO"}
{"created":"2024-04-11 13:11:21","title":"Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models","abstract":"Reading comprehension tests are used in a variety of applications, reaching from education to assessing the comprehensibility of simplified texts. However, creating such tests manually and ensuring their quality is difficult and time-consuming. In this paper, we explore how large language models (LLMs) can be used to generate and evaluate multiple-choice reading comprehension items. To this end, we compiled a dataset of German reading comprehension items and developed a new protocol for human and automatic evaluation, including a metric we call text informativity, which is based on guessability and answerability. We then used this protocol and the dataset to evaluate the quality of items generated by Llama 2 and GPT-4. Our results suggest that both models are capable of generating items of acceptable quality in a zero-shot setting, but GPT-4 clearly outperforms Llama 2. We also show that LLMs can be used for automatic evaluation by eliciting item reponses from them. In this scenario, evaluation results with GPT-4 were the most similar to human annotators. Overall, zero-shot generation with LLMs is a promising approach for generating and evaluating reading comprehension test items, in particular for languages without large amounts of available data.","sentences":["Reading comprehension tests are used in a variety of applications, reaching from education to assessing the comprehensibility of simplified texts.","However, creating such tests manually and ensuring their quality is difficult and time-consuming.","In this paper, we explore how large language models (LLMs) can be used to generate and evaluate multiple-choice reading comprehension items.","To this end, we compiled a dataset of German reading comprehension items and developed a new protocol for human and automatic evaluation, including a metric we call text informativity, which is based on guessability and answerability.","We then used this protocol and the dataset to evaluate the quality of items generated by Llama 2 and GPT-4.","Our results suggest that both models are capable of generating items of acceptable quality in a zero-shot setting, but GPT-4 clearly outperforms Llama 2.","We also show that LLMs can be used for automatic evaluation by eliciting item reponses from them.","In this scenario, evaluation results with GPT-4 were the most similar to human annotators.","Overall, zero-shot generation with LLMs is a promising approach for generating and evaluating reading comprehension test items, in particular for languages without large amounts of available data."],"url":"http://arxiv.org/abs/2404.07720v1","category":"cs.CL"}
{"created":"2024-04-11 13:11:13","title":"Reframing the Mind-Body Picture: Applying Formal Systems to the Relationship of Mind and Matter","abstract":"This paper aims to show that a simple framework, utilizing basic formalisms from set theory and category theory, can clarify and inform our theories of the relation between mind and matter.","sentences":["This paper aims to show that a simple framework, utilizing basic formalisms from set theory and category theory, can clarify and inform our theories of the relation between mind and matter."],"url":"http://arxiv.org/abs/2404.07719v1","category":"cs.AI"}
{"created":"2024-04-11 13:09:37","title":"Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics","abstract":"Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%. These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.","sentences":["Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored.","In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors.","We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation.","In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images.","Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet.","Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%.","These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language."],"url":"http://arxiv.org/abs/2404.07717v1","category":"cs.RO"}
{"created":"2024-04-11 13:04:14","title":"Self-interacting dark sectors in supernovae are fluid","abstract":"We revisit supernova (SN) bounds on a hidden sector consisting of millicharged particles $\\chi$ and a massless dark photon. Unless the self-coupling is fine-tuned to be small, rather than exiting the SN core as a gas, the particles form a relativistic fluid and subsequent dark QED fireball, streaming out against the drag due to the interaction with matter. Novel bounds due to excessive energy deposition in the mantle of low-energy SNe can be obtained. The cooling bounds from SN~1987A are unexpectedly not affected in the free-streaming regime. The inclusion of $\\chi\\bar{\\chi}\\rightarrow \\rm SM$ substantially modifies the constraints in the trapping regime, which can only be treated hydrodynamically. Our results can be adapted to generic sub-GeV self-interacting dark sectors.","sentences":["We revisit supernova (SN) bounds on a hidden sector consisting of millicharged particles $\\chi$ and a massless dark photon.","Unless the self-coupling is fine-tuned to be small, rather than exiting the SN core as a gas, the particles form a relativistic fluid and subsequent dark QED fireball, streaming out against the drag due to the interaction with matter.","Novel bounds due to excessive energy deposition in the mantle of low-energy SNe can be obtained.","The cooling bounds from SN~1987A are unexpectedly not affected in the free-streaming regime.","The inclusion of $\\chi\\bar{\\chi}\\rightarrow \\rm SM$ substantially modifies the constraints in the trapping regime, which can only be treated hydrodynamically.","Our results can be adapted to generic sub-GeV self-interacting dark sectors."],"url":"http://arxiv.org/abs/2404.07714v1","category":"hep-ph"}
{"created":"2024-04-11 12:58:12","title":"OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities","abstract":"Identifying and classifying underground utilities is an important task for efficient and effective urban planning and infrastructure maintenance. We present OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point cloud dataset, designed to advance research and development in underground utility surveying and mapping. OpenTrench3D covers a completely novel domain for public 3D point cloud datasets and is unique in its focus, scope, and cost-effective capturing method. The dataset consists of 310 point clouds collected across 7 distinct areas. These include 5 water utility areas and 2 district heating utility areas. The inclusion of different geographical areas and main utilities (water and district heating utilities) makes OpenTrench3D particularly valuable for inter-domain transfer learning experiments. We provide benchmark results for the dataset using three state-of-the-art semantic segmentation models, PointNeXt, PointVector and PointMetaBase. Benchmarks are conducted by training on data from water areas, fine-tuning on district heating area 1 and evaluating on district heating area 2. The dataset is publicly available. With OpenTrench3D, we seek to foster innovation and progress in the field of 3D semantic segmentation in applications related to detection and documentation of underground utilities as well as in transfer learning methods in general.","sentences":["Identifying and classifying underground utilities is an important task for efficient and effective urban planning and infrastructure maintenance.","We present OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point cloud dataset, designed to advance research and development in underground utility surveying and mapping.","OpenTrench3D covers a completely novel domain for public 3D point cloud datasets and is unique in its focus, scope, and cost-effective capturing method.","The dataset consists of 310 point clouds collected across 7 distinct areas.","These include 5 water utility areas and 2 district heating utility areas.","The inclusion of different geographical areas and main utilities (water and district heating utilities) makes OpenTrench3D particularly valuable for inter-domain transfer learning experiments.","We provide benchmark results for the dataset using three state-of-the-art semantic segmentation models, PointNeXt, PointVector and PointMetaBase.","Benchmarks are conducted by training on data from water areas, fine-tuning on district heating area 1 and evaluating on district heating area 2.","The dataset is publicly available.","With OpenTrench3D, we seek to foster innovation and progress in the field of 3D semantic segmentation in applications related to detection and documentation of underground utilities as well as in transfer learning methods in general."],"url":"http://arxiv.org/abs/2404.07711v1","category":"cs.CV"}
{"created":"2024-04-11 12:56:56","title":"An Explicit Primitive Conservative Solver for the Euler Equations with Arbitrary Equation of State","abstract":"This work presents a procedure to solve the Euler equations by explicitly updating, in a conservative manner, a generic thermodynamic variable such as temperature, pressure or entropy instead of the total energy. The presented procedure is valid for any equation of state and spatial discretization. When using complex equations of state such as Span-Wagner, choosing the temperature as the generic thermodynamic variable yields great reductions in the computational costs associated to thermodynamic evaluations. Results computed with a state of the art thermodynamic model are presented, and computational times are analyzed. Particular attention is dedicated to the conservation of total energy, the propagation speed of shock waves and jump conditions. The procedure is thoroughly tested using the Span-Wagner equation of state through the CoolProp thermodynamic library and the Van der Waals equation of state, both in the ideal and non-ideal compressible fluid-dynamics regimes, by comparing it to the standard total energy update and analytical solutions where available.","sentences":["This work presents a procedure to solve the Euler equations by explicitly updating, in a conservative manner, a generic thermodynamic variable such as temperature, pressure or entropy instead of the total energy.","The presented procedure is valid for any equation of state and spatial discretization.","When using complex equations of state such as Span-Wagner, choosing the temperature as the generic thermodynamic variable yields great reductions in the computational costs associated to thermodynamic evaluations.","Results computed with a state of the art thermodynamic model are presented, and computational times are analyzed.","Particular attention is dedicated to the conservation of total energy, the propagation speed of shock waves and jump conditions.","The procedure is thoroughly tested using the Span-Wagner equation of state through the CoolProp thermodynamic library and the Van der Waals equation of state, both in the ideal and non-ideal compressible fluid-dynamics regimes, by comparing it to the standard total energy update and analytical solutions where available."],"url":"http://arxiv.org/abs/2404.07710v1","category":"math.NA"}
{"created":"2024-04-11 12:51:21","title":"Tree Splitting Based Rounding Scheme for Weighted Proportional Allocations with Subsidy","abstract":"We consider the problem of allocating $m$ indivisible items to a set of $n$ heterogeneous agents, aiming at computing a proportional allocation by introducing subsidy (money). It has been shown by Wu et al. (WINE 2023) that when agents are unweighted a total subsidy of $n/4$ suffices (assuming that each item has value/cost at most $1$ to every agent) to ensure proportionality. When agents have general weights, they proposed an algorithm that guarantees a weighted proportional allocation requiring a total subsidy of $(n-1)/2$, by rounding the fractional bid-and-take algorithm. In this work, we revisit the problem and the fractional bid-and-take algorithm. We show that by formulating the fractional allocation returned by the algorithm as a directed tree connecting the agents and splitting the tree into canonical components, there is a rounding scheme that requires a total subsidy of at most $n/3 - 1/6$.","sentences":["We consider the problem of allocating $m$ indivisible items to a set of $n$ heterogeneous agents, aiming at computing a proportional allocation by introducing subsidy (money).","It has been shown by Wu et al.","(WINE 2023) that when agents are unweighted a total subsidy of $n/4$ suffices (assuming that each item has value/cost at most $1$ to every agent) to ensure proportionality.","When agents have general weights, they proposed an algorithm that guarantees a weighted proportional allocation requiring a total subsidy of $(n-1)/2$, by rounding the fractional bid-and-take algorithm.","In this work, we revisit the problem and the fractional bid-and-take algorithm.","We show that by formulating the fractional allocation returned by the algorithm as a directed tree connecting the agents and splitting the tree into canonical components, there is a rounding scheme that requires a total subsidy of at most $n/3 - 1/6$."],"url":"http://arxiv.org/abs/2404.07707v1","category":"cs.GT"}
{"created":"2024-04-11 12:47:50","title":"Poset Positional Games","abstract":"We propose a generalization of positional games, supplementing them with a restriction on the order in which the elements of the board are allowed to be claimed. We introduce poset positional games, which are positional games with an additional structure -- a poset on the elements of the board. Throughout the game play, based on this poset and the set of the board elements that are claimed up to that point, we reduce the set of available moves for the player whose turn it is -- an element of the board can only be claimed if all the smaller elements in the poset are already claimed.   We proceed to analyse these games in more detail, with a prime focus on the most studied convention, the Maker-Breaker games. First we build a general framework around poset positional games. Then, we perform a comprehensive study of the complexity of determining the game outcome, conditioned on the structure of the family of winning sets on the one side and the structure of the poset on the other.","sentences":["We propose a generalization of positional games, supplementing them with a restriction on the order in which the elements of the board are allowed to be claimed.","We introduce poset positional games, which are positional games with an additional structure -- a poset on the elements of the board.","Throughout the game play, based on this poset and the set of the board elements that are claimed up to that point, we reduce the set of available moves for the player whose turn it is -- an element of the board can only be claimed if all the smaller elements in the poset are already claimed.   ","We proceed to analyse these games in more detail, with a prime focus on the most studied convention, the Maker-Breaker games.","First we build a general framework around poset positional games.","Then, we perform a comprehensive study of the complexity of determining the game outcome, conditioned on the structure of the family of winning sets on the one side and the structure of the poset on the other."],"url":"http://arxiv.org/abs/2404.07700v1","category":"math.CO"}
{"created":"2024-04-11 12:44:40","title":"Time evolution as an optimization problem: The hydrogen atom in strong laser fields in a basis of time-dependent Gaussian wave packets","abstract":"Recent advances in attosecond science have made it increasingly important to develop stable, reliable and accurate algorithms and methods to model the time evolution of atoms and molecules in intense laser fields. A key process in attosecond science is high-harmonic generation, which is challenging to model with fixed Gaussian basis sets, as it produces high-energy electrons, with a resulting rapidly varying and highly oscillatory wave function that extends over dozens of {\\aa}ngstr\\\"om. Recently, Rothe's method, where time evolution is rephrased as an optimization problem, has been applied to the one-dimensional Schr\\\"odinger equation. Here, we apply Rothe's method to the hydrogen wave function and demonstrate that complex-valued Gaussian wave packets with time-dependent width, center, and momentum parameters are able to reproduce spectra obtained from essentially exact grid calculations for high-harmonic generation with only 50-181 Gaussians for field strengths up to $5\\times 10^{14}$W/cm$^2$. This paves the way for the inclusion of continuum contributions into real-time, time-dependent electronic-structure theory with Gaussian basis sets for strong fields, and eventually accurate simulations of the time evolution of molecules without the Born-Oppenheimer approximation.","sentences":["Recent advances in attosecond science have made it increasingly important to develop stable, reliable and accurate algorithms and methods to model the time evolution of atoms and molecules in intense laser fields.","A key process in attosecond science is high-harmonic generation, which is challenging to model with fixed Gaussian basis sets, as it produces high-energy electrons, with a resulting rapidly varying and highly oscillatory wave function that extends over dozens of {\\aa}ngstr\\\"om.","Recently, Rothe's method, where time evolution is rephrased as an optimization problem, has been applied to the one-dimensional Schr\\\"odinger equation.","Here, we apply Rothe's method to the hydrogen wave function and demonstrate that complex-valued Gaussian wave packets with time-dependent width, center, and momentum parameters are able to reproduce spectra obtained from essentially exact grid calculations for high-harmonic generation with only 50-181 Gaussians for field strengths up to $5\\times 10^{14}$W/cm$^2$.","This paves the way for the inclusion of continuum contributions into real-time, time-dependent electronic-structure theory with Gaussian basis sets for strong fields, and eventually accurate simulations of the time evolution of molecules without the Born-Oppenheimer approximation."],"url":"http://arxiv.org/abs/2404.07699v1","category":"physics.chem-ph"}
{"created":"2024-04-11 12:44:15","title":"Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents Probability Estimator","abstract":"The widespread usage of point clouds (PC) for immersive visual applications has resulted in the use of very heterogeneous receiving conditions and devices, notably in terms of network, hardware, and display capabilities. In this scenario, quality scalability, i.e., the ability to reconstruct a signal at different qualities by progressively decoding a single bitstream, is a major requirement that has yet to be conveniently addressed, notably in most learning-based PC coding solutions. This paper proposes a quality scalability scheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based static point cloud geometry codecs, which uses a Quality-conditioned Latents Probability Estimator (QuLPE) to decode a high-quality version of a PC learning-based representation, based on an available lower quality base layer. SQH is integrated in the future JPEG PC coding standard, allowing to create a layered bitstream that can be used to progressively decode the PC geometry with increasing quality and fidelity. Experimental results show that SQH offers the quality scalability feature with very limited or no compression performance penalty at all when compared with the corresponding non-scalable solution, thus preserving the significant compression gains over other state-of-the-art PC codecs.","sentences":["The widespread usage of point clouds (PC) for immersive visual applications has resulted in the use of very heterogeneous receiving conditions and devices, notably in terms of network, hardware, and display capabilities.","In this scenario, quality scalability, i.e., the ability to reconstruct a signal at different qualities by progressively decoding a single bitstream, is a major requirement that has yet to be conveniently addressed, notably in most learning-based PC coding solutions.","This paper proposes a quality scalability scheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based static point cloud geometry codecs, which uses a Quality-conditioned Latents Probability Estimator (QuLPE) to decode a high-quality version of a PC learning-based representation, based on an available lower quality base layer.","SQH is integrated in the future JPEG PC coding standard, allowing to create a layered bitstream that can be used to progressively decode the PC geometry with increasing quality and fidelity.","Experimental results show that SQH offers the quality scalability feature with very limited or no compression performance penalty at all when compared with the corresponding non-scalable solution, thus preserving the significant compression gains over other state-of-the-art PC codecs."],"url":"http://arxiv.org/abs/2404.07698v1","category":"cs.LG"}
{"created":"2024-04-11 12:34:07","title":"Transcendental nature of $p$-adic digamma values","abstract":"For a fixed prime $p$, Murty and Saradha (2008) studied the transcendental nature of special values of the $p$-adic digamma function, denoted as $\\psi_p(r/p)+ \\gamma_p$. This research was later extended by Chatterjee and Gun in 2014, who investigated the case of $\\psi_p(r/p^n)+ \\gamma_p$, for any integer $n>1$. In this article, we generalize their results for distinct prime powers and explore the transcendental nature of the $p$-adic digamma values, with at most one exception. Further, we investigate the multiplicative independence of cyclotomic numbers satisfying certain conditions. Using this, we prove the transcendental nature of $p$-adic digamma values corresponding to $\\psi_p(r/pq)+ \\gamma_p$, where $p, q$ are distinct primes.","sentences":["For a fixed prime $p$, Murty and Saradha (2008) studied the transcendental nature of special values of the $p$-adic digamma function, denoted as $\\psi_p(r/p)+ \\gamma_p$.","This research was later extended by Chatterjee and Gun in 2014, who investigated the case of $\\psi_p(r/p^n)+ \\gamma_p$, for any integer $n>1$. In this article, we generalize their results for distinct prime powers and explore the transcendental nature of the $p$-adic digamma values, with at most one exception.","Further, we investigate the multiplicative independence of cyclotomic numbers satisfying certain conditions.","Using this, we prove the transcendental nature of $p$-adic digamma values corresponding to $\\psi_p(r/pq)+ \\gamma_p$, where $p, q$ are distinct primes."],"url":"http://arxiv.org/abs/2404.07690v1","category":"math.NT"}
{"created":"2024-04-11 12:32:56","title":"Towards resolving bedload flux variability","abstract":"Bedload transport occurs when a bed composed of sedimentary grains becomes mobile in response to the shearing by a flow of liquid. It shapes the landscapes of Earth and other planetary bodies by promoting the formation and growth of various multiscale geological features. Estimating the rate at which such processes take place requires accurate bedload flux predictions. However, even for highly idealized conditions in the laboratory, study-to-study variability of reported bedload flux measurements borders an order of magnitude. This uncertainty stems from physically poorly supported, typically empirical methods of determining the transport-driving bed shear stress, especially for very narrow or shallow channel flows, and from study-to-study grain shape variations. Here, we derive a non-empirical method of bed shear stress determination and apply it to a number of independent grain-shape-controlled data sets, based on well-controlled experiments and CFD-DEM simulations, for a very diverse range of transport conditions. An existing physical bedload model, here generalized to account for grain shape variability, predicts almost all these data within a factor of 1.3, whereas a prominent alternative model (Deal et al., Nature 613, 298-302, 2023) seems falsified.","sentences":["Bedload transport occurs when a bed composed of sedimentary grains becomes mobile in response to the shearing by a flow of liquid.","It shapes the landscapes of Earth and other planetary bodies by promoting the formation and growth of various multiscale geological features.","Estimating the rate at which such processes take place requires accurate bedload flux predictions.","However, even for highly idealized conditions in the laboratory, study-to-study variability of reported bedload flux measurements borders an order of magnitude.","This uncertainty stems from physically poorly supported, typically empirical methods of determining the transport-driving bed shear stress, especially for very narrow or shallow channel flows, and from study-to-study grain shape variations.","Here, we derive a non-empirical method of bed shear stress determination and apply it to a number of independent grain-shape-controlled data sets, based on well-controlled experiments and CFD-DEM simulations, for a very diverse range of transport conditions.","An existing physical bedload model, here generalized to account for grain shape variability, predicts almost all these data within a factor of 1.3, whereas a prominent alternative model (Deal et al., Nature 613, 298-302, 2023) seems falsified."],"url":"http://arxiv.org/abs/2404.07689v1","category":"physics.geo-ph"}
{"created":"2024-04-11 12:26:10","title":"Chaos in Motion: Unveiling Robustness in Remote Heart Rate Measurement through Brain-Inspired Skin Tracking","abstract":"Heart rate is an important physiological indicator of human health status. Existing remote heart rate measurement methods typically involve facial detection followed by signal extraction from the region of interest (ROI). These SOTA methods have three serious problems: (a) inaccuracies even failures in detection caused by environmental influences or subject movement; (b) failures for special patients such as infants and burn victims; (c) privacy leakage issues resulting from collecting face video. To address these issues, we regard the remote heart rate measurement as the process of analyzing the spatiotemporal characteristics of the optical flow signal in the video. We apply chaos theory to computer vision tasks for the first time, thus designing a brain-inspired framework. Firstly, using an artificial primary visual cortex model to extract the skin in the videos, and then calculate heart rate by time-frequency analysis on all pixels. Our method achieves Robust Skin Tracking for Heart Rate measurement, called HR-RST. The experimental results show that HR-RST overcomes the difficulty of environmental influences and effectively tracks the subject movement. Moreover, the method could extend to other body parts. Consequently, the method can be applied to special patients and effectively protect individual privacy, offering an innovative solution.","sentences":["Heart rate is an important physiological indicator of human health status.","Existing remote heart rate measurement methods typically involve facial detection followed by signal extraction from the region of interest (ROI).","These SOTA methods have three serious problems: (a) inaccuracies even failures in detection caused by environmental influences or subject movement; (b) failures for special patients such as infants and burn victims; (c) privacy leakage issues resulting from collecting face video.","To address these issues, we regard the remote heart rate measurement as the process of analyzing the spatiotemporal characteristics of the optical flow signal in the video.","We apply chaos theory to computer vision tasks for the first time, thus designing a brain-inspired framework.","Firstly, using an artificial primary visual cortex model to extract the skin in the videos, and then calculate heart rate by time-frequency analysis on all pixels.","Our method achieves Robust Skin Tracking for Heart Rate measurement, called HR-RST.","The experimental results show that HR-RST overcomes the difficulty of environmental influences and effectively tracks the subject movement.","Moreover, the method could extend to other body parts.","Consequently, the method can be applied to special patients and effectively protect individual privacy, offering an innovative solution."],"url":"http://arxiv.org/abs/2404.07687v1","category":"cs.CV"}
{"created":"2024-04-11 12:25:54","title":"Depth Estimation using Weighted-loss and Transfer Learning","abstract":"Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.","sentences":["Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics.","The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics.","In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function.","The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM).","We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model.","We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2.","We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively.","We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed.","The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture."],"url":"http://arxiv.org/abs/2404.07686v1","category":"cs.CV"}
{"created":"2024-04-11 12:24:47","title":"Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns","abstract":"Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.","sentences":["Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety.","Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern.","State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone.","However, that may not sufficiently address the complexities and sparsity of data in 3D object detection.","To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors.","Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity.","To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance."],"url":"http://arxiv.org/abs/2404.07685v1","category":"cs.CV"}
{"created":"2024-04-11 12:20:55","title":"Saturation-Informed Current-Limiting Control for Grid-Forming Converters","abstract":"In this paper, we investigate the transient stability of a state-of-the-art grid-forming complex-droop control (i.e., dispatchable virtual oscillator control, dVOC) under current saturation. We quantify the saturation level of a converter by introducing the concept of degree of saturation (DoS), and we propose a provably stable current-limiting control with saturation-informed feedback, which feeds the degree of saturation back to the inner voltage-control loop and the outer grid-forming loop. As a result, although the output current is saturated, the voltage phase angle can still be generated from an internal virtual voltage-source node that is governed by an equivalent complex-droop control. We prove that the proposed control achieves transient stability during current saturation under grid faults. We also provide parametric stability conditions for multi-converter systems under grid-connected and islanded scenarios. The stability performance of the current-limiting control is validated with various case studies.","sentences":["In this paper, we investigate the transient stability of a state-of-the-art grid-forming complex-droop control (i.e., dispatchable virtual oscillator control, dVOC) under current saturation.","We quantify the saturation level of a converter by introducing the concept of degree of saturation (DoS), and we propose a provably stable current-limiting control with saturation-informed feedback, which feeds the degree of saturation back to the inner voltage-control loop and the outer grid-forming loop.","As a result, although the output current is saturated, the voltage phase angle can still be generated from an internal virtual voltage-source node that is governed by an equivalent complex-droop control.","We prove that the proposed control achieves transient stability during current saturation under grid faults.","We also provide parametric stability conditions for multi-converter systems under grid-connected and islanded scenarios.","The stability performance of the current-limiting control is validated with various case studies."],"url":"http://arxiv.org/abs/2404.07682v1","category":"eess.SY"}
{"created":"2024-04-11 12:18:01","title":"On the role of ethics and sustainability in business innovation","abstract":"For organizations to survive and flourish in the long term, innovation and novelty must be continually introduced, which is particularly true in today's rapidly changing world. This raises a variety of ethical and sustainability considerations that seldom receive the attention they deserve. Existing innovation adoption frameworks often focus on technological, organizational, environmental, and social factors impacting adoption. In this chapter, we explore the ethical and sustainability angles, particularly as they relate to emerging technologies, artificial intelligence (AI) being a prominent example. We consider how to facilitate the development and cultivation of innovation cultures in organizations, including budding startups as well as established enterprises, through approaches such as systems thinking.","sentences":["For organizations to survive and flourish in the long term, innovation and novelty must be continually introduced, which is particularly true in today's rapidly changing world.","This raises a variety of ethical and sustainability considerations that seldom receive the attention they deserve.","Existing innovation adoption frameworks often focus on technological, organizational, environmental, and social factors impacting adoption.","In this chapter, we explore the ethical and sustainability angles, particularly as they relate to emerging technologies, artificial intelligence (AI) being a prominent example.","We consider how to facilitate the development and cultivation of innovation cultures in organizations, including budding startups as well as established enterprises, through approaches such as systems thinking."],"url":"http://arxiv.org/abs/2404.07678v1","category":"cs.CY"}
{"created":"2024-04-11 12:16:16","title":"ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs","abstract":"The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.","sentences":["The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks.","However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs.","To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs.","ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection.","Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism.","Subsequently, we integrate the observed knowledge into the action and reflection modules.","Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%."],"url":"http://arxiv.org/abs/2404.07677v1","category":"cs.CL"}
{"created":"2024-04-11 12:14:48","title":"Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis","abstract":"The QUILT-1M dataset is the first openly available dataset containing images harvested from various online sources. While it provides a huge data variety, the image quality and composition is highly heterogeneous, impacting its utility for text-conditional image synthesis. We propose an automatic pipeline that provides predictions of the most common impurities within the images, e.g., visibility of narrators, desktop environment and pathology software, or text within the image. Additionally, we propose to use semantic alignment filtering of the image-text pairs. Our findings demonstrate that by rigorously filtering the dataset, there is a substantial enhancement of image fidelity in text-to-image tasks.","sentences":["The QUILT-1M dataset is the first openly available dataset containing images harvested from various online sources.","While it provides a huge data variety, the image quality and composition is highly heterogeneous, impacting its utility for text-conditional image synthesis.","We propose an automatic pipeline that provides predictions of the most common impurities within the images, e.g., visibility of narrators, desktop environment and pathology software, or text within the image.","Additionally, we propose to use semantic alignment filtering of the image-text pairs.","Our findings demonstrate that by rigorously filtering the dataset, there is a substantial enhancement of image fidelity in text-to-image tasks."],"url":"http://arxiv.org/abs/2404.07676v1","category":"cs.CV"}
{"created":"2024-04-11 12:08:06","title":"Safe haptic teleoperations of admittance controlled robots with virtualization of the force feedback","abstract":"Haptic teleoperations play a key role in extending human capabilities to perform complex tasks remotely, employing a robotic system. The impact of haptics is far-reaching and can improve the sensory awareness and motor accuracy of the operator. In this context, a key challenge is attaining a natural, stable and safe haptic human-robot interaction. Achieving these conflicting requirements is particularly crucial for complex procedures, e.g. medical ones. To address this challenge, in this work we develop a novel haptic bilateral teleoperation system (HBTS), featuring a virtualized force feedback, based on the motion error generated by an admittance controlled robot. This approach allows decoupling the force rendering system from the control of the interaction: the rendered force is assigned with the desired dynamics, while the admittance control parameters are separately tuned to maximize interaction performance. Furthermore, recognizing the necessity to limit the forces exerted by the robot on the environment, to ensure a safe interaction, we embed a saturation strategy of the motion references provided by the haptic device to admittance control. We validate the different aspects of the proposed HBTS, through a teleoperated blackboard writing experiment, against two other architectures. The results indicate that the proposed HBTS improves the naturalness of teleoperation, as well as safety and accuracy of the interaction.","sentences":["Haptic teleoperations play a key role in extending human capabilities to perform complex tasks remotely, employing a robotic system.","The impact of haptics is far-reaching and can improve the sensory awareness and motor accuracy of the operator.","In this context, a key challenge is attaining a natural, stable and safe haptic human-robot interaction.","Achieving these conflicting requirements is particularly crucial for complex procedures, e.g. medical ones.","To address this challenge, in this work we develop a novel haptic bilateral teleoperation system (HBTS), featuring a virtualized force feedback, based on the motion error generated by an admittance controlled robot.","This approach allows decoupling the force rendering system from the control of the interaction: the rendered force is assigned with the desired dynamics, while the admittance control parameters are separately tuned to maximize interaction performance.","Furthermore, recognizing the necessity to limit the forces exerted by the robot on the environment, to ensure a safe interaction, we embed a saturation strategy of the motion references provided by the haptic device to admittance control.","We validate the different aspects of the proposed HBTS, through a teleoperated blackboard writing experiment, against two other architectures.","The results indicate that the proposed HBTS improves the naturalness of teleoperation, as well as safety and accuracy of the interaction."],"url":"http://arxiv.org/abs/2404.07672v1","category":"cs.RO"}
{"created":"2024-04-11 12:04:41","title":"On Naisargik Images of Varshamov-Tenengolts and Helberg Codes","abstract":"The VT and Helberg codes, both in binary and non-binary forms, stand as elegant solutions for rectifying insertion and deletion errors. In this paper we consider the quaternary versions of these codes. It is well known that many optimal binary non-linear codes like Kerdock and Prepreta can be depicted as Gray images (isometry) of codes defined over $\\mathbb{Z}_4$. Thus a natural question arises: Can we find similar maps between quaternary and binary spaces which gives interesting properties when applied to the VT and Helberg codes. We found several such maps called Naisargik (natural) maps and we study the images of quaternary VT and Helberg codes under these maps. Naisargik and inverse Naisargik images gives interesting error-correcting properties for VT and Helberg codes. If two Naisargik images of VT code generates an intersecting one deletion sphere, then the images holds the same weights. A quaternary Helberg code designed to correct $s$ deletions can effectively rectify $s+1$ deletion errors when considering its Naisargik image, and $s$-deletion correcting binary Helberg code can corrects $\\lfloor\\frac{s}{2}\\rfloor$ errors with inverse Naisargik image.","sentences":["The VT and Helberg codes, both in binary and non-binary forms, stand as elegant solutions for rectifying insertion and deletion errors.","In this paper we consider the quaternary versions of these codes.","It is well known that many optimal binary non-linear codes like Kerdock and Prepreta can be depicted as Gray images (isometry) of codes defined over $\\mathbb{Z}_4$. Thus a natural question arises: Can we find similar maps between quaternary and binary spaces which gives interesting properties when applied to the VT and Helberg codes.","We found several such maps called Naisargik (natural) maps and we study the images of quaternary VT and Helberg codes under these maps.","Naisargik and inverse Naisargik images gives interesting error-correcting properties for VT and Helberg codes.","If two Naisargik images of VT code generates an intersecting one deletion sphere, then the images holds the same weights.","A quaternary Helberg code designed to correct $s$ deletions can effectively rectify $s+1$ deletion errors when considering its Naisargik image, and $s$-deletion correcting binary Helberg code can corrects $\\lfloor\\frac{s}{2}\\rfloor$ errors with inverse Naisargik image."],"url":"http://arxiv.org/abs/2404.07670v1","category":"cs.IT"}
{"created":"2024-04-11 12:00:13","title":"Shape Completion in the Dark: Completing Vertebrae Morphology from 3D Ultrasound","abstract":"Purpose: Ultrasound (US) imaging, while advantageous for its radiation-free nature, is challenging to interpret due to only partially visible organs and a lack of complete 3D information. While performing US-based diagnosis or investigation, medical professionals therefore create a mental map of the 3D anatomy. In this work, we aim to replicate this process and enhance the visual representation of anatomical structures.   Methods: We introduce a point-cloud-based probabilistic DL method to complete occluded anatomical structures through 3D shape completion and choose US-based spine examinations as our application. To enable training, we generate synthetic 3D representations of partially occluded spinal views by mimicking US physics and accounting for inherent artifacts.   Results: The proposed model performs consistently on synthetic and patient data, with mean and median differences of 2.02 and 0.03 in CD, respectively. Our ablation study demonstrates the importance of US physics-based data generation, reflected in the large mean and median difference of 11.8 CD and 9.55 CD, respectively. Additionally, we demonstrate that anatomic landmarks, such as the spinous process (with reconstruction CD of 4.73) and the facet joints (mean distance to GT of 4.96mm) are preserved in the 3D completion.   Conclusion: Our work establishes the feasibility of 3D shape completion for lumbar vertebrae, ensuring the preservation of level-wise characteristics and successful generalization from synthetic to real data. The incorporation of US physics contributes to more accurate patient data completions. Notably, our method preserves essential anatomic landmarks and reconstructs crucial injections sites at their correct locations. The generated data and source code will be made publicly available (https://github.com/miruna20/Shape-Completion-in-the-Dark).","sentences":["Purpose: Ultrasound (US) imaging, while advantageous for its radiation-free nature, is challenging to interpret due to only partially visible organs and a lack of complete 3D information.","While performing US-based diagnosis or investigation, medical professionals therefore create a mental map of the 3D anatomy.","In this work, we aim to replicate this process and enhance the visual representation of anatomical structures.   ","Methods: We introduce a point-cloud-based probabilistic DL method to complete occluded anatomical structures through 3D shape completion and choose US-based spine examinations as our application.","To enable training, we generate synthetic 3D representations of partially occluded spinal views by mimicking US physics and accounting for inherent artifacts.   ","Results:","The proposed model performs consistently on synthetic and patient data, with mean and median differences of 2.02 and 0.03 in CD, respectively.","Our ablation study demonstrates the importance of US physics-based data generation, reflected in the large mean and median difference of 11.8 CD and 9.55 CD, respectively.","Additionally, we demonstrate that anatomic landmarks, such as the spinous process (with reconstruction CD of 4.73) and the facet joints (mean distance to GT of 4.96mm) are preserved in the 3D completion.   ","Conclusion: Our work establishes the feasibility of 3D shape completion for lumbar vertebrae, ensuring the preservation of level-wise characteristics and successful generalization from synthetic to real data.","The incorporation of US physics contributes to more accurate patient data completions.","Notably, our method preserves essential anatomic landmarks and reconstructs crucial injections sites at their correct locations.","The generated data and source code will be made publicly available (https://github.com/miruna20/Shape-Completion-in-the-Dark)."],"url":"http://arxiv.org/abs/2404.07668v1","category":"eess.IV"}
{"created":"2024-04-11 11:55:42","title":"Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes","abstract":"Detecting and localising unknown or Out-of-distribution (OOD) objects in any scene can be a challenging task in vision. Particularly, in safety-critical cases involving autonomous systems like automated vehicles or trains. Supervised anomaly segmentation or open-world object detection models depend on training on exhaustively annotated datasets for every domain and still struggle in distinguishing between background and OOD objects. In this work, we present a plug-and-play generalised framework - PRototype-based zero-shot OOD detection Without Labels (PROWL). It is an inference-based method that does not require training on the domain dataset and relies on extracting relevant features from self-supervised pre-trained models. PROWL can be easily adapted to detect OOD objects in any operational design domain by specifying a list of known classes from this domain. PROWL, as an unsupervised method, outperforms other supervised methods trained without auxiliary OOD data on the RoadAnomaly and RoadObstacle datasets provided in SegmentMeIfYouCan (SMIYC) benchmark. We also demonstrate its suitability for other domains such as rail and maritime scenes.","sentences":["Detecting and localising unknown or Out-of-distribution (OOD) objects in any scene can be a challenging task in vision.","Particularly, in safety-critical cases involving autonomous systems like automated vehicles or trains.","Supervised anomaly segmentation or open-world object detection models depend on training on exhaustively annotated datasets for every domain and still struggle in distinguishing between background and OOD objects.","In this work, we present a plug-and-play generalised framework - PRototype-based zero-shot OOD detection Without Labels (PROWL).","It is an inference-based method that does not require training on the domain dataset and relies on extracting relevant features from self-supervised pre-trained models.","PROWL can be easily adapted to detect OOD objects in any operational design domain by specifying a list of known classes from this domain.","PROWL, as an unsupervised method, outperforms other supervised methods trained without auxiliary OOD data on the RoadAnomaly and RoadObstacle datasets provided in SegmentMeIfYouCan (SMIYC) benchmark.","We also demonstrate its suitability for other domains such as rail and maritime scenes."],"url":"http://arxiv.org/abs/2404.07664v1","category":"cs.CV"}
{"created":"2024-04-11 11:53:14","title":"Interactive Ontology Matching with Cost-Efficient Learning","abstract":"The creation of high-quality ontologies is crucial for data integration and knowledge-based reasoning, specifically in the context of the rising data economy. However, automatic ontology matchers are often bound to the heuristics they are based on, leaving many matches unidentified. Interactive ontology matching systems involving human experts have been introduced, but they do not solve the fundamental issue of flexibly finding additional matches outside the scope of the implemented heuristics, even though this is highly demanded in industrial settings. Active machine learning methods appear to be a promising path towards a flexible interactive ontology matcher. However, off-the-shelf active learning mechanisms suffer from low query efficiency due to extreme class imbalance, resulting in a last-mile problem where high human effort is required to identify the remaining matches.   To address the last-mile problem, this work introduces DualLoop, an active learning method tailored to ontology matching. DualLoop offers three main contributions: (1) an ensemble of tunable heuristic matchers, (2) a short-term learner with a novel query strategy adapted to highly imbalanced data, and (3) long-term learners to explore potential matches by creating and tuning new heuristics. We evaluated DualLoop on three datasets of varying sizes and domains. Compared to existing active learning methods, we consistently achieved better F1 scores and recall, reducing the expected query cost spent on finding 90% of all matches by over 50%. Compared to traditional interactive ontology matchers, we are able to find additional, last-mile matches. Finally, we detail the successful deployment of our approach within an actual product and report its operational performance results within the Architecture, Engineering, and Construction (AEC) industry sector, showcasing its practical value and efficiency.","sentences":["The creation of high-quality ontologies is crucial for data integration and knowledge-based reasoning, specifically in the context of the rising data economy.","However, automatic ontology matchers are often bound to the heuristics they are based on, leaving many matches unidentified.","Interactive ontology matching systems involving human experts have been introduced, but they do not solve the fundamental issue of flexibly finding additional matches outside the scope of the implemented heuristics, even though this is highly demanded in industrial settings.","Active machine learning methods appear to be a promising path towards a flexible interactive ontology matcher.","However, off-the-shelf active learning mechanisms suffer from low query efficiency due to extreme class imbalance, resulting in a last-mile problem where high human effort is required to identify the remaining matches.   ","To address the last-mile problem, this work introduces DualLoop, an active learning method tailored to ontology matching.","DualLoop offers three main contributions: (1) an ensemble of tunable heuristic matchers, (2) a short-term learner with a novel query strategy adapted to highly imbalanced data, and (3) long-term learners to explore potential matches by creating and tuning new heuristics.","We evaluated DualLoop on three datasets of varying sizes and domains.","Compared to existing active learning methods, we consistently achieved better F1 scores and recall, reducing the expected query cost spent on finding 90% of all matches by over 50%.","Compared to traditional interactive ontology matchers, we are able to find additional, last-mile matches.","Finally, we detail the successful deployment of our approach within an actual product and report its operational performance results within the Architecture, Engineering, and Construction (AEC) industry sector, showcasing its practical value and efficiency."],"url":"http://arxiv.org/abs/2404.07663v1","category":"cs.DB"}
{"created":"2024-04-11 11:51:46","title":"PINNACLE: PINN Adaptive ColLocation and Experimental points selection","abstract":"Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations. Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interaction among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems.","sentences":["Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations.","Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics.","Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses.","PINNACLE uses information on the interaction among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK).","We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems."],"url":"http://arxiv.org/abs/2404.07662v1","category":"cs.LG"}
{"created":"2024-04-11 11:47:24","title":"High-frequency solutions to the Einstein equations","abstract":"We review recent mathematical results concerning the high-frequency solutions to the Einstein vacuum equations and the limits of these solutions. In particular, we focus on two conjectures of Burnett, which attempt to give an exact characterization of high-frequency limits of vacuum spacetimes as solutions to the Einstein-massless Vlasov system. Some open problems and future directions are discussed.","sentences":["We review recent mathematical results concerning the high-frequency solutions to the Einstein vacuum equations and the limits of these solutions.","In particular, we focus on two conjectures of Burnett, which attempt to give an exact characterization of high-frequency limits of vacuum spacetimes as solutions to the Einstein-massless Vlasov system.","Some open problems and future directions are discussed."],"url":"http://arxiv.org/abs/2404.07659v1","category":"gr-qc"}
{"created":"2024-04-11 11:43:22","title":"Re-Interpreting the Step-Response Probability Curve to Extract Fundamental Physical Parameters of Event-based Vision Sensors","abstract":"Biologically inspired event-based vision sensors (EVS) are growing in popularity due to performance benefits including ultra-low power consumption, high dynamic range, data sparsity, and fast temporal response. They efficiently encode dynamic information from a visual scene through pixels that respond autonomously and asynchronously when the per-pixel illumination level changes by a user-selectable contrast threshold ratio, $\\theta$. Due to their unique sensing paradigm and complex analog pixel circuitry, characterizing Event-based Vision Sensor (EVS) is non-trivial. The step-response probability curve (S-curve) is a key measurement technique that has emerged as the standard for measuring $\\theta$. In this work, we detail the method for generating accurate S-curves by applying an appropriate stimulus and sensor configuration to decouple 2nd-order effects from the parameter being studied. We use an EVS pixel simulation to demonstrate how noise and other physical constraints can lead to error in the measurement, and develop two techniques that are robust enough to obtain accurate estimates. We then apply best practices derived from our simulation to generate S-curves for the latest generation Sony IMX636 and interpret the resulting family of curves to correct the apparent anomalous result of previous reports suggesting that $\\theta$ changes with illumination. Further, we demonstrate that with correct interpretation, fundamental physical parameters such as dark current and RMS noise can be accurately inferred from a collection of S-curves, leading to more accurate parameterization for high-fidelity EVS simulations.","sentences":["Biologically inspired event-based vision sensors (EVS) are growing in popularity due to performance benefits including ultra-low power consumption, high dynamic range, data sparsity, and fast temporal response.","They efficiently encode dynamic information from a visual scene through pixels that respond autonomously and asynchronously when the per-pixel illumination level changes by a user-selectable contrast threshold ratio, $\\theta$. Due to their unique sensing paradigm and complex analog pixel circuitry, characterizing Event-based Vision Sensor (EVS) is non-trivial.","The step-response probability curve (S-curve) is a key measurement technique that has emerged as the standard for measuring $\\theta$. In this work, we detail the method for generating accurate S-curves by applying an appropriate stimulus and sensor configuration to decouple 2nd-order effects from the parameter being studied.","We use an EVS pixel simulation to demonstrate how noise and other physical constraints can lead to error in the measurement, and develop two techniques that are robust enough to obtain accurate estimates.","We then apply best practices derived from our simulation to generate S-curves for the latest generation Sony IMX636 and interpret the resulting family of curves to correct the apparent anomalous result of previous reports suggesting that $\\theta$ changes with illumination.","Further, we demonstrate that with correct interpretation, fundamental physical parameters such as dark current and RMS noise can be accurately inferred from a collection of S-curves, leading to more accurate parameterization for high-fidelity EVS simulations."],"url":"http://arxiv.org/abs/2404.07656v1","category":"eess.IV"}
{"created":"2024-04-11 11:37:18","title":"rollama: An R package for using generative large language models through Ollama","abstract":"rollama is an R package that wraps the Ollama API, which allows you to run different Generative Large Language Models (GLLM) locally. The package and learning material focus on making it easy to use Ollama for annotating textual or imagine data with open-source models as well as use these models for document embedding. But users can use or extend rollama to do essentially anything else that is possible through OpenAI's API, yet more private, reproducible and for free.","sentences":["rollama is an R package that wraps the Ollama API, which allows you to run different Generative Large Language Models (GLLM) locally.","The package and learning material focus on making it easy to use Ollama for annotating textual or imagine data with open-source models as well as use these models for document embedding.","But users can use or extend rollama to do essentially anything else that is possible through OpenAI's API, yet more private, reproducible and for free."],"url":"http://arxiv.org/abs/2404.07654v1","category":"cs.CL"}
{"created":"2024-04-11 11:21:00","title":"TURB-Hel: an open-access database of helically forced homogeneous and isotropic turbulence","abstract":"We present TURB-Hel, a database formed by two datasets of incompressible homogeneous and isotropic turbulence, maintained in a statistically stationary state by fully helical forcing. The aim is to provide a dataset that clearly exhibits the phenomenon of the helicity cascade from the large to the small scales generated by a large-scale forcing that breaks the mirror symmetry. This database offers the possibility to realize a wide variety of analyses of fully developed turbulence from the sub-grid scale filtering up to the validation of an a posteriori LES. TURB-Hel is available for download using the SMART-Turb portal http://smart-turb.roma2.infn.it.","sentences":["We present TURB-Hel, a database formed by two datasets of incompressible homogeneous and isotropic turbulence, maintained in a statistically stationary state by fully helical forcing.","The aim is to provide a dataset that clearly exhibits the phenomenon of the helicity cascade from the large to the small scales generated by a large-scale forcing that breaks the mirror symmetry.","This database offers the possibility to realize a wide variety of analyses of fully developed turbulence from the sub-grid scale filtering up to the validation of an a posteriori LES.","TURB-Hel is available for download using the SMART-Turb portal http://smart-turb.roma2.infn.it."],"url":"http://arxiv.org/abs/2404.07653v1","category":"physics.flu-dyn"}
{"created":"2024-04-11 11:13:35","title":"Impacto Distributivo Potencial de Reformas na Tributacao Indireta no Brasil: Simulacoes Baseadas na PEC 45/2019","abstract":"This paper analyzes the redistributive impacts of indirect taxation reforms in Brazil inspired by PEC 45/2019, particularly in the version that led to EC 132/2023. Comparisons are made between the current system and the simulated reforms, considering the distribution of the tax burden among families in different income classes, as well as the impact on poverty and inequality indicators. The simulations are conducted based on the combination of effective tax rates that apply to goods and services consumed by households, using nationally representative microdata from family budgets.","sentences":["This paper analyzes the redistributive impacts of indirect taxation reforms in Brazil inspired by PEC 45/2019, particularly in the version that led to EC 132/2023.","Comparisons are made between the current system and the simulated reforms, considering the distribution of the tax burden among families in different income classes, as well as the impact on poverty and inequality indicators.","The simulations are conducted based on the combination of effective tax rates that apply to goods and services consumed by households, using nationally representative microdata from family budgets."],"url":"http://arxiv.org/abs/2404.07651v1","category":"econ.GN"}
{"created":"2024-04-11 11:08:28","title":"T-convexity, Weakly Immediate Types and $T$-$\u03bb$-Spherical Completions of o-minimal Structures","abstract":"Let $T$ be the theory of an o-minimal field, and $T_{\\mathrm{convex}}$ the theory of its expansion by a predicate $\\mathcal{O}$ for a non-trival $T$-convex valuation ring. For $\\lambda$ an uncountable cardinal, say that a unary type $p(x)$ over a model of $T_{\\mathrm{convex}}$ is \\emph{$\\lambda$-bounded weakly immediate} if its cut is defined by an empty intersection of fewer than $\\lambda$ many nested valuation balls. Call an elementary extension \\emph{$\\lambda$-bounded wim-constructible} if it is obtained as a transfinite composition of extensions each generated by one element whose type is $\\lambda$-bounded weakly immediate.   I show that $\\lambda$-bounded wim-constructible extensions do not extend the residue-field sort and that any two wim-constructible extensions can be amalgamated in an extension which is again $\\lambda$-bounded wim-constructible over both.   A consequence is that given a cardinal $\\lambda$, every model of $T_{\\mathrm{convex}}$ has a unique-up-to-non-unique-isomorphism $\\lambda$-spherically complete $\\lambda$-bounded wim-constructible extension. We call this extension the $T$-$\\lambda$-spherical completion.   In the case $T$ is power bounded, wim-constructible extensions are just the immediate extensions. I discuss the example of power bounded theories expanded by $\\exp$.","sentences":["Let $T$ be the theory of an o-minimal field, and $T_{\\mathrm{convex}}$ the theory of its expansion by a predicate $\\mathcal{O}$ for a non-trival $T$-convex valuation ring.","For $\\lambda$ an uncountable cardinal, say that a unary type $p(x)$ over a model of $T_{\\mathrm{convex}}$ is \\emph{$\\lambda$-bounded weakly immediate} if its cut is defined by an empty intersection of fewer than $\\lambda$ many nested valuation balls.","Call an elementary extension \\emph{$\\lambda$-bounded wim-constructible} if it is obtained as a transfinite composition of extensions each generated by one element whose type is $\\lambda$-bounded weakly immediate.   ","I show that $\\lambda$-bounded wim-constructible extensions do not extend the residue-field sort and that any two wim-constructible extensions can be amalgamated in an extension which is again $\\lambda$-bounded wim-constructible over both.   ","A consequence is that given a cardinal $\\lambda$, every model of $T_{\\mathrm{convex}}$ has a unique-up-to-non-unique-isomorphism $\\lambda$-spherically complete $\\lambda$-bounded wim-constructible extension.","We call this extension the $T$-$\\lambda$-spherical completion.   ","In the case $T$ is power bounded, wim-constructible extensions are just the immediate extensions.","I discuss the example of power bounded theories expanded by $\\exp$."],"url":"http://arxiv.org/abs/2404.07646v1","category":"math.LO"}
{"created":"2024-04-11 11:01:32","title":"Coherent sheaves on primitive multiple curves","abstract":"A primitive multiple scheme is a Cohen-Macaulay scheme $Y$ such that the associated reduced scheme $X=Y_{red}$ is smooth, irreducible, and that $Y$ can be locally embedded in a smooth variety of dimension $\\dim(X)+1$. If $I_X$ is the ideal sheaf of $X$ in $Y$ and $Y\\not=X$, then $L=I_X/I_X^2$ is a line bundle on $X$, called the associated line bundle of $Y$.   Even if $X$ is projective, $Y$ needs not to be quasi projective. We define in every case the reduced Hilbert polynomial $P_{red,O_X(1)}(E)$ of a coherent sheaf $E$ on $Y$, depending on the choice of an ample line bundle $O_X(1)$ on $X$. We conjecture that if $F$ is a flat family of sheaves on $Y$ parameterized by a smooth curve $C$, then $P_{red,O_X(1)}(F_c)$ does not depend on $c\\in C$. We prove that it is true in two important cases: the families of quasi locally free sheaves, and if $Y$ is of multiplicity 2, the families of balanced sheaves. Balanced sheaves are generalizations of vector bundles on $Y$, and could be used to expand already known moduli spaces of vector bundles on $Y$.   When $X$ is a smooth projective surface, and $Y$ is of multiplicity 2 we study the simplest examples of balanced sheaves: the sheaves $E$ such that there is an exact sequence \\[0\\longrightarrow I_P\\otimes L\\longrightarrow E\\longrightarrow I_P=E_{|X} \\longrightarrow 0 \\ , \\] where $I_P\\subset O_X$ is the ideal sheaf of a point $P\\in X$. They can also be described as the ideal sheaves $E$ of subschemes of $Y$ concentrated on $P$, and such that $E_P$ is generated by two elements whose images in $O_{X,P}$ generate the maximal ideal. There is a moduli space for such sheaves, which is an affine bundle on $X$ with associated vector bundle $T_X\\otimes L$ (where $T_X$ is the tangent bundle of $X$). The associated class in $H^1(X,T_X\\otimes L)$ can be determined.","sentences":["A primitive multiple scheme is a Cohen-Macaulay scheme $Y$ such that the associated reduced scheme $X=Y_{red}$ is smooth, irreducible, and that $Y$ can be locally embedded in a smooth variety of dimension $\\dim(X)+1$. If $I_X$ is the ideal sheaf of $X$ in $Y$ and $Y\\not=X$, then $L=I_X/I_X^2$ is a line bundle on $X$, called the associated line bundle of $Y$.   Even if $X$ is projective, $Y$ needs not to be quasi projective.","We define in every case the reduced Hilbert polynomial $P_{red,O_X(1)}(E)$ of a coherent sheaf $E$ on $Y$, depending on the choice of an ample line bundle $O_X(1)$ on $X$. We conjecture that if $F$ is a flat family of sheaves on $Y$ parameterized by a smooth curve $C$, then $P_{red,O_X(1)}(F_c)$ does not depend on $c\\in C$.","We prove that it is true in two important cases: the families of quasi locally free sheaves, and if $Y$ is of multiplicity 2, the families of balanced sheaves.","Balanced sheaves are generalizations of vector bundles on $Y$, and could be used to expand already known moduli spaces of vector bundles on $Y$.   When $X$ is a smooth projective surface, and $Y$ is of multiplicity 2 we study the simplest examples of balanced sheaves: the sheaves $E$ such that there is an exact sequence \\[0\\longrightarrow I_P\\otimes L\\longrightarrow E\\longrightarrow I_P=E_{|X} \\longrightarrow 0 \\ , \\] where $I_P\\subset O_X$ is the ideal sheaf of a point $P\\in X$.","They can also be described as the ideal sheaves $E$ of subschemes of $Y$ concentrated on $P$, and such that $E_P$ is generated by two elements whose images in $O_{X,P}$ generate the maximal ideal.","There is a moduli space for such sheaves, which is an affine bundle on $X$ with associated vector bundle $T_X\\otimes L$ (where $T_X$ is the tangent bundle of $X$).","The associated class in $H^1(X,T_X\\otimes L)$ can be determined."],"url":"http://arxiv.org/abs/2404.07639v1","category":"math.AG"}
{"created":"2024-04-11 10:57:35","title":"A solution to Itoh's conjecture for integral closure filtration","abstract":"Let $(A,\\mathfrak{m})$ be an analytically unramified Cohen-Macaulay local ring of dimension $d \\geq 3$ and let $\\mathfrak{a}$ be an $\\mathfrak{m}$-primary ideal in $A$. If $I$ is an ideal in $A$ then let $I^*$ be the integral closure of $I$ in $A$. Let $G_{\\mathfrak{a}}(A)^* = \\bigoplus_{n\\geq 0 }(\\mathfrak{a}^n)^*/(\\mathfrak{a}^{n+1})^*$ be the associated graded ring of the integral closure filtration of $\\mathfrak{a}$. Itoh conjectured in 1992 that if third Hilbert coefficient of $G_{\\mathfrak{a}}(A)^*$ , i.e., $e_3^{\\mathfrak{a}^*}(A) = 0$ and $A$ is Gorenstein then   $G_{\\mathfrak{a}}(A)^*$ is Cohen-Macaulay. In this paper we prove Itoh's conjecture (more generally for analytically unramified Cohen-Macaulay local rings).","sentences":["Let $(A,\\mathfrak{m})$ be an analytically unramified Cohen-Macaulay local ring of dimension $d \\geq 3$ and let $\\mathfrak{a}$ be an $\\mathfrak{m}$-primary ideal in $A$.","If $I$ is an ideal in $A$ then let $I^*$ be the integral closure of $I$ in $A$.","Let $G_{\\mathfrak{a}}(A)^* = \\bigoplus_{n\\geq 0 }(\\mathfrak{a}^n)^*/(\\mathfrak{a}^{n+1})^*$ be the associated graded ring of the integral closure filtration of $\\mathfrak{a}$. Itoh conjectured in 1992 that if third Hilbert coefficient of $G_{\\mathfrak{a}}(A)^*$ , i.e., $e_3^{\\mathfrak{a}^*}(A) = 0$ and $A$ is Gorenstein then   $G_{\\mathfrak{a}}(A)^*$ is Cohen-Macaulay.","In this paper we prove Itoh's conjecture (more generally for analytically unramified Cohen-Macaulay local rings)."],"url":"http://arxiv.org/abs/2404.07638v1","category":"math.AC"}
{"created":"2024-04-11 10:52:17","title":"Versatile Metamaterial: Exploring Symmetry-Protected Mode Resonances for Multi-Task Functionality","abstract":"In this article, we present an experimental study supported by numerical modeling showing the possibility of exciting Symmetry-Protected Bound states In the Continuum (SP-BICs) in a 1D silicon grating fabricated on a lithium niobate substrate in both transverse electric and transverse magnetic polarization states of the incident illumination. This leads to different resonances in the transmission spectra with large quality factors up $10^6$ and a significant electric/magnetic field enhancement up to $10^5$ opening the way to the exploitation of this structure for different sensing applications (biological, electromagnetic, thermal...) but also to nonlinear applications such as the generation of second harmonic, in addition to electro- and acousto-optic modulation.","sentences":["In this article, we present an experimental study supported by numerical modeling showing the possibility of exciting Symmetry-Protected Bound states In the Continuum (SP-BICs) in a 1D silicon grating fabricated on a lithium niobate substrate in both transverse electric and transverse magnetic polarization states of the incident illumination.","This leads to different resonances in the transmission spectra with large quality factors up $10^6$ and a significant electric/magnetic field enhancement up to $10^5$ opening the way to the exploitation of this structure for different sensing applications (biological, electromagnetic, thermal...) but also to nonlinear applications such as the generation of second harmonic, in addition to electro- and acousto-optic modulation."],"url":"http://arxiv.org/abs/2404.07634v1","category":"physics.optics"}
{"created":"2024-04-11 10:39:35","title":"Short, Disclose, and Distort","abstract":"We investigate the voluntary disclosure decision of activist speculators under uncertainty about information endowment (Dye 1985). In our model, a speculator first uncovers initial evidence about the target firm and then seeks additional information to help interpret the initial evidence. The speculator takes a position in the firm's stock, then voluntarily discloses some or all of their findings, and finally closes their position after the disclosure. We present three main findings. First, the speculator will always disclose the initial evidence, even though the market is uncertain about whether the speculator possesses such evidence. Second, the speculator's disclosure strategy of the additional information increases stock price volatility: they disclose extreme news and withhold moderate news. Third, this distortion in disclosure enables the speculator to engage in market manipulation, whereby they take a short (long) position despite having good (bad) news.   Keywords: activist speculators, short and distort, voluntary disclosure, complex information, market manipulation   JEL Classifications: D82, D83, G14, M41","sentences":["We investigate the voluntary disclosure decision of activist speculators under uncertainty about information endowment (Dye 1985).","In our model, a speculator first uncovers initial evidence about the target firm and then seeks additional information to help interpret the initial evidence.","The speculator takes a position in the firm's stock, then voluntarily discloses some or all of their findings, and finally closes their position after the disclosure.","We present three main findings.","First, the speculator will always disclose the initial evidence, even though the market is uncertain about whether the speculator possesses such evidence.","Second, the speculator's disclosure strategy of the additional information increases stock price volatility: they disclose extreme news and withhold moderate news.","Third, this distortion in disclosure enables the speculator to engage in market manipulation, whereby they take a short (long) position despite having good (bad) news.   ","Keywords: activist speculators, short and distort, voluntary disclosure, complex information, market manipulation   JEL Classifications: D82, D83, G14, M41"],"url":"http://arxiv.org/abs/2404.07630v1","category":"econ.GN"}
{"created":"2024-04-11 10:26:40","title":"Homography Guided Temporal Fusion for Road Line and Marking Segmentation","abstract":"Reliable segmentation of road lines and markings is critical to autonomous driving. Our work is motivated by the observations that road lines and markings are (1) frequently occluded in the presence of moving vehicles, shadow, and glare and (2) highly structured with low intra-class shape variance and overall high appearance consistency. To solve these issues, we propose a Homography Guided Fusion (HomoFusion) module to exploit temporally-adjacent video frames for complementary cues facilitating the correct classification of the partially occluded road lines or markings. To reduce computational complexity, a novel surface normal estimator is proposed to establish spatial correspondences between the sampled frames, allowing the HomoFusion module to perform a pixel-to-pixel attention mechanism in updating the representation of the occluded road lines or markings. Experiments on ApolloScape, a large-scale lane mark segmentation dataset, and ApolloScape Night with artificial simulated night-time road conditions, demonstrate that our method outperforms other existing SOTA lane mark segmentation models with less than 9\\% of their parameters and computational complexity. We show that exploiting available camera intrinsic data and ground plane assumption for cross-frame correspondence can lead to a light-weight network with significantly improved performances in speed and accuracy. We also prove the versatility of our HomoFusion approach by applying it to the problem of water puddle segmentation and achieving SOTA performance.","sentences":["Reliable segmentation of road lines and markings is critical to autonomous driving.","Our work is motivated by the observations that road lines and markings are (1) frequently occluded in the presence of moving vehicles, shadow, and glare and (2) highly structured with low intra-class shape variance and overall high appearance consistency.","To solve these issues, we propose a Homography Guided Fusion (HomoFusion) module to exploit temporally-adjacent video frames for complementary cues facilitating the correct classification of the partially occluded road lines or markings.","To reduce computational complexity, a novel surface normal estimator is proposed to establish spatial correspondences between the sampled frames, allowing the HomoFusion module to perform a pixel-to-pixel attention mechanism in updating the representation of the occluded road lines or markings.","Experiments on ApolloScape, a large-scale lane mark segmentation dataset, and ApolloScape Night with artificial simulated night-time road conditions, demonstrate that our method outperforms other existing SOTA lane mark segmentation models with less than 9\\% of their parameters and computational complexity.","We show that exploiting available camera intrinsic data and ground plane assumption for cross-frame correspondence can lead to a light-weight network with significantly improved performances in speed and accuracy.","We also prove the versatility of our HomoFusion approach by applying it to the problem of water puddle segmentation and achieving SOTA performance."],"url":"http://arxiv.org/abs/2404.07626v1","category":"cs.CV"}
{"created":"2024-04-11 10:19:04","title":"Semirings generated by idempotents","abstract":"We prove that a semiring multiplicatively generated by its idempotents is commutative and Boolean, if every idempotent in the semiring has an orthogonal complement. We prove that a semiring additively generated by its idempotents is commutative, if every idempotent in the semiring has an orthogonal complement and all the nilpotents in the semirings are central. We also provide examples that the assumptions on the existence of orthogonal complements of idempotents and the centrality of nilpotents cannot be omitted.","sentences":["We prove that a semiring multiplicatively generated by its idempotents is commutative and Boolean, if every idempotent in the semiring has an orthogonal complement.","We prove that a semiring additively generated by its idempotents is commutative, if every idempotent in the semiring has an orthogonal complement and all the nilpotents in the semirings are central.","We also provide examples that the assumptions on the existence of orthogonal complements of idempotents and the centrality of nilpotents cannot be omitted."],"url":"http://arxiv.org/abs/2404.07623v1","category":"math.RA"}
{"created":"2024-04-11 10:16:44","title":"Multi-Image Visual Question Answering for Unsupervised Anomaly Detection","abstract":"Unsupervised anomaly detection enables the identification of potential pathological areas by juxtaposing original images with their pseudo-healthy reconstructions generated by models trained exclusively on normal images. However, the clinical interpretation of resultant anomaly maps presents a challenge due to a lack of detailed, understandable explanations. Recent advancements in language models have shown the capability of mimicking human-like understanding and providing detailed descriptions. This raises an interesting question: \\textit{How can language models be employed to make the anomaly maps more explainable?} To the best of our knowledge, we are the first to leverage a language model for unsupervised anomaly detection, for which we construct a dataset with different questions and answers. Additionally, we present a novel multi-image visual question answering framework tailored for anomaly detection, incorporating diverse feature fusion strategies to enhance visual knowledge extraction. Our experiments reveal that the framework, augmented by our new Knowledge Q-Former module, adeptly answers questions on the anomaly detection dataset. Besides, integrating anomaly maps as inputs distinctly aids in improving the detection of unseen pathologies.","sentences":["Unsupervised anomaly detection enables the identification of potential pathological areas by juxtaposing original images with their pseudo-healthy reconstructions generated by models trained exclusively on normal images.","However, the clinical interpretation of resultant anomaly maps presents a challenge due to a lack of detailed, understandable explanations.","Recent advancements in language models have shown the capability of mimicking human-like understanding and providing detailed descriptions.","This raises an interesting question: \\textit{How can language models be employed to make the anomaly maps more explainable?}","To the best of our knowledge, we are the first to leverage a language model for unsupervised anomaly detection, for which we construct a dataset with different questions and answers.","Additionally, we present a novel multi-image visual question answering framework tailored for anomaly detection, incorporating diverse feature fusion strategies to enhance visual knowledge extraction.","Our experiments reveal that the framework, augmented by our new Knowledge Q-Former module, adeptly answers questions on the anomaly detection dataset.","Besides, integrating anomaly maps as inputs distinctly aids in improving the detection of unseen pathologies."],"url":"http://arxiv.org/abs/2404.07622v1","category":"cs.CV"}
{"created":"2024-04-11 10:10:08","title":"$\u03b1$-$z$-R\u00e9nyi divergences in von Neumann algebras: data-processing inequality, reversibility, and monotonicity properties in $\u03b1,z$","abstract":"We study the $\\alpha$-$z$-R\\'enyi divergences $D_{\\alpha,z}(\\psi\\|\\varphi)$ where $\\alpha,z>0$ ($\\alpha\\ne1$) for normal positive functionals $\\psi,\\varphi$ on general von Neumann algebras, introduced in [S.~Kato and Y.~Ueda, arXiv:2307.01790] and [S.~Kato, arXiv:2311.01748]. We prove the variational expressions and the data processing inequality (DPI) for the $\\alpha$-$z$-R\\'enyi divergences. We establish the sufficiency theorem for $D_{\\alpha,z}(\\psi\\|\\varphi)$, saying that for $(\\alpha,z)$ inside the DPI bounds, the equality $D_{\\alpha,z}(\\psi\\circ\\gamma\\|\\varphi\\circ\\gamma)=D_{\\alpha,z}(\\psi\\|\\varphi)<\\infty$ in the DPI under a quantum channel (or a normal $2$-positive unital map) $\\gamma$ implies the reversibility of $\\gamma$ with respect to $\\psi,\\varphi$. Moreover, we show the monotonicity properties of $D_{\\alpha,z}(\\psi\\|\\varphi)$ in the parameters $\\alpha,z$ and their limits to the normalized relative entropy as $\\alpha\\nearrow1$ and $\\alpha\\searrow1$.","sentences":["We study the $\\alpha$-$z$-R\\'enyi divergences $D_{\\alpha,z}(\\psi\\|\\varphi)$ where $\\alpha,z>0$ ($\\alpha\\ne1$) for normal positive functionals $\\psi,\\varphi$ on general von Neumann algebras, introduced in [S.~Kato and Y.~Ueda, arXiv:2307.01790] and [S.~Kato, arXiv:2311.01748].","We prove the variational expressions and the data processing inequality (DPI) for the $\\alpha$-$z$-R\\'enyi divergences.","We establish the sufficiency theorem for $D_{\\alpha,z}(\\psi\\|\\varphi)$, saying that for $(\\alpha,z)$ inside the DPI bounds, the equality $D_{\\alpha,z}(\\psi\\circ\\gamma\\|\\varphi\\circ\\gamma)=D_{\\alpha,z}(\\psi\\|\\varphi)<\\infty$ in the DPI under a quantum channel (or a normal $2$-positive unital map) $\\gamma$ implies the reversibility of $\\gamma$ with respect to $\\psi,\\varphi$. Moreover, we show the monotonicity properties of $D_{\\alpha,z}(\\psi\\|\\varphi)$ in the parameters $\\alpha,z$ and their limits to the normalized relative entropy as $\\alpha\\nearrow1$ and $\\alpha\\searrow1$."],"url":"http://arxiv.org/abs/2404.07617v1","category":"quant-ph"}
{"created":"2024-04-11 10:08:34","title":"Audio Dialogues: Dialogues dataset for audio and music understanding","abstract":"Existing datasets for audio understanding primarily focus on single-turn interactions (i.e. audio captioning, audio question answering) for describing audio in natural language, thus limiting understanding audio via interactive dialogue. To address this gap, we introduce Audio Dialogues: a multi-turn dialogue dataset containing 163.8k samples for general audio sounds and music. In addition to dialogues, Audio Dialogues also has question-answer pairs to understand and compare multiple input audios together. Audio Dialogues leverages a prompting-based approach and caption annotations from existing datasets to generate multi-turn dialogues using a Large Language Model (LLM). We evaluate existing audio-augmented large language models on our proposed dataset to demonstrate the complexity and applicability of Audio Dialogues. Our code for generating the dataset will be made publicly available. Detailed prompts and generated dialogues can be found on the demo website https://audiodialogues.github.io/.","sentences":["Existing datasets for audio understanding primarily focus on single-turn interactions (i.e. audio captioning, audio question answering) for describing audio in natural language, thus limiting understanding audio via interactive dialogue.","To address this gap, we introduce Audio Dialogues: a multi-turn dialogue dataset containing 163.8k samples for general audio sounds and music.","In addition to dialogues, Audio Dialogues also has question-answer pairs to understand and compare multiple input audios together.","Audio Dialogues leverages a prompting-based approach and caption annotations from existing datasets to generate multi-turn dialogues using a Large Language Model (LLM).","We evaluate existing audio-augmented large language models on our proposed dataset to demonstrate the complexity and applicability of Audio Dialogues.","Our code for generating the dataset will be made publicly available.","Detailed prompts and generated dialogues can be found on the demo website https://audiodialogues.github.io/."],"url":"http://arxiv.org/abs/2404.07616v1","category":"cs.CL"}
{"created":"2024-04-11 10:01:32","title":"Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain","abstract":"Research on language technology for the development of medical applications is currently a hot topic in Natural Language Understanding and Generation. Thus, a number of large language models (LLMs) have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction. While these LLMs display competitive performance on automated medical texts benchmarks, they have been pre-trained and evaluated with a focus on a single language (English mostly). This is particularly true of text-to-text models, which typically require large amounts of domain-specific pre-training data, often not easily accessible for many languages. In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish. This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we present two new evaluation benchmarks for all four languages with the aim of facilitating multilingual research in this domain. A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian benchmarks, while being competitive with current state-of-the-art LLMs in English.","sentences":["Research on language technology for the development of medical applications is currently a hot topic in Natural Language Understanding and Generation.","Thus, a number of large language models (LLMs) have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction.","While these LLMs display competitive performance on automated medical texts benchmarks, they have been pre-trained and evaluated with a focus on a single language (English mostly).","This is particularly true of text-to-text models, which typically require large amounts of domain-specific pre-training data, often not easily accessible for many languages.","In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish.","This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain.","Additionally, we present two new evaluation benchmarks for all four languages with the aim of facilitating multilingual research in this domain.","A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian benchmarks, while being competitive with current state-of-the-art LLMs in English."],"url":"http://arxiv.org/abs/2404.07613v1","category":"cs.CL"}
{"created":"2024-04-11 09:59:21","title":"Measuring Geographic Diversity of Foundation Models with a Natural Language--based Geo-guessing Experiment on GPT-4","abstract":"Generative AI based on foundation models provides a first glimpse into the world represented by machines trained on vast amounts of multimodal data ingested by these models during training. If we consider the resulting models as knowledge bases in their own right, this may open up new avenues for understanding places through the lens of machines. In this work, we adopt this thinking and select GPT-4, a state-of-the-art representative in the family of multimodal large language models, to study its geographic diversity regarding how well geographic features are represented. Using DBpedia abstracts as a ground-truth corpus for probing, our natural language--based geo-guessing experiment shows that GPT-4 may currently encode insufficient knowledge about several geographic feature types on a global level. On a local level, we observe not only this insufficiency but also inter-regional disparities in GPT-4's geo-guessing performance on UNESCO World Heritage Sites that carry significance to both local and global populations, and the inter-regional disparities may become smaller as the geographic scale increases. Morever, whether assessing the geo-guessing performance on a global or local level, we find inter-model disparities in GPT-4's geo-guessing performance when comparing its unimodal and multimodal variants. We hope this work can initiate a discussion on geographic diversity as an ethical principle within the GIScience community in the face of global socio-technical challenges.","sentences":["Generative AI based on foundation models provides a first glimpse into the world represented by machines trained on vast amounts of multimodal data ingested by these models during training.","If we consider the resulting models as knowledge bases in their own right, this may open up new avenues for understanding places through the lens of machines.","In this work, we adopt this thinking and select GPT-4, a state-of-the-art representative in the family of multimodal large language models, to study its geographic diversity regarding how well geographic features are represented.","Using DBpedia abstracts as a ground-truth corpus for probing, our natural language--based geo-guessing experiment shows that GPT-4 may currently encode insufficient knowledge about several geographic feature types on a global level.","On a local level, we observe not only this insufficiency but also inter-regional disparities in GPT-4's geo-guessing performance on UNESCO World Heritage Sites that carry significance to both local and global populations, and the inter-regional disparities may become smaller as the geographic scale increases.","Morever, whether assessing the geo-guessing performance on a global or local level, we find inter-model disparities in GPT-4's geo-guessing performance when comparing its unimodal and multimodal variants.","We hope this work can initiate a discussion on geographic diversity as an ethical principle within the GIScience community in the face of global socio-technical challenges."],"url":"http://arxiv.org/abs/2404.07612v1","category":"cs.CY"}
{"created":"2024-04-11 09:59:01","title":"NoticIA: A Clickbait Article Summarization Dataset in Spanish","abstract":"We present NoticIA, a dataset consisting of 850 Spanish news articles featuring prominent clickbait headlines, each paired with high-quality, single-sentence generative summarizations written by humans. This task demands advanced text understanding and summarization abilities, challenging the models' capacity to infer and connect diverse pieces of information to meet the user's informational needs generated by the clickbait headline. We evaluate the Spanish text comprehension capabilities of a wide range of state-of-the-art large language models. Additionally, we use the dataset to train ClickbaitFighter, a task-specific model that achieves near-human performance in this task.","sentences":["We present NoticIA, a dataset consisting of 850 Spanish news articles featuring prominent clickbait headlines, each paired with high-quality, single-sentence generative summarizations written by humans.","This task demands advanced text understanding and summarization abilities, challenging the models' capacity to infer and connect diverse pieces of information to meet the user's informational needs generated by the clickbait headline.","We evaluate the Spanish text comprehension capabilities of a wide range of state-of-the-art large language models.","Additionally, we use the dataset to train ClickbaitFighter, a task-specific model that achieves near-human performance in this task."],"url":"http://arxiv.org/abs/2404.07611v1","category":"cs.CL"}
{"created":"2024-04-11 09:53:07","title":"Reconfigurable Superdirective and Superabsorptive Aperiodic Metasurfaces","abstract":"In this paper, we present a general theory of aperiodic subwavelength arrays for controlling electromagnetic waves. The considered platform is formed by an array of electrically small loaded scatterers above a ground plane. While the array is geometrically periodic, all the loads can be in general different, so that the distributions of currents induced by plane waves are not periodic. To allow analytical solutions, we study arrays of thin wires or strips loaded by bulk loads. We demonstrate a practical way of creating tunable and reconfigurable multifunctional devices, on examples of superdirective beam splitters, focusing lenses establishing subdiffraction focusing, and absorbers going beyond perfect absorption. Contrary to the constraints imposed by the Floquet theorem in periodic counterparts like periodic metasurfaces or metagratings, where a fixed angle of incidence and period dictate the propagating directions of reflected waves, the proposed aperiodic designs allow controlling all propagating modes in any direction, which provides more freedom in manipulating electromagnetic waves. We hope that these results can be useful in multiple applications, such as telecommunications, radar techniques, signal processing, and energy harnessing.","sentences":["In this paper, we present a general theory of aperiodic subwavelength arrays for controlling electromagnetic waves.","The considered platform is formed by an array of electrically small loaded scatterers above a ground plane.","While the array is geometrically periodic, all the loads can be in general different, so that the distributions of currents induced by plane waves are not periodic.","To allow analytical solutions, we study arrays of thin wires or strips loaded by bulk loads.","We demonstrate a practical way of creating tunable and reconfigurable multifunctional devices, on examples of superdirective beam splitters, focusing lenses establishing subdiffraction focusing, and absorbers going beyond perfect absorption.","Contrary to the constraints imposed by the Floquet theorem in periodic counterparts like periodic metasurfaces or metagratings, where a fixed angle of incidence and period dictate the propagating directions of reflected waves, the proposed aperiodic designs allow controlling all propagating modes in any direction, which provides more freedom in manipulating electromagnetic waves.","We hope that these results can be useful in multiple applications, such as telecommunications, radar techniques, signal processing, and energy harnessing."],"url":"http://arxiv.org/abs/2404.07608v1","category":"physics.app-ph"}
{"created":"2024-04-11 09:48:03","title":"Lifting with Inner Functions of Polynomial Discrepancy","abstract":"Lifting theorems are theorems that bound the communication complexity of a composed function $f\\circ g^{n}$ in terms of the query complexity of $f$ and the communication complexity of $g$. Such theorems constitute a powerful generalization of direct-sum theorems for $g$, and have seen numerous applications in recent years. We prove a new lifting theorem that works for every two functions $f,g$ such that the discrepancy of $g$ is at most inverse polynomial in the input length of $f$. Our result is a significant generalization of the known direct-sum theorem for discrepancy, and extends the range of inner functions $g$ for which lifting theorems hold.","sentences":["Lifting theorems are theorems that bound the communication complexity of a composed function $f\\circ g^{n}$ in terms of the query complexity of $f$ and the communication complexity of $g$. Such theorems constitute a powerful generalization of direct-sum theorems for $g$, and have seen numerous applications in recent years.","We prove a new lifting theorem that works for every two functions $f,g$ such that the discrepancy of $g$ is at most inverse polynomial in the input length of $f$. Our result is a significant generalization of the known direct-sum theorem for discrepancy, and extends the range of inner functions $g$ for which lifting theorems hold."],"url":"http://arxiv.org/abs/2404.07606v1","category":"cs.CC"}
{"created":"2024-04-11 09:47:52","title":"Contrastive-Based Deep Embeddings for Label Noise-Resilient Histopathology Image Classification","abstract":"Recent advancements in deep learning have proven highly effective in medical image classification, notably within histopathology. However, noisy labels represent a critical challenge in histopathology image classification, where accurate annotations are vital for training robust deep learning models. Indeed, deep neural networks can easily overfit label noise, leading to severe degradations in model performance. While numerous public pathology foundation models have emerged recently, none have evaluated their resilience to label noise. Through thorough empirical analyses across multiple datasets, we exhibit the label noise resilience property of embeddings extracted from foundation models trained in a self-supervised contrastive manner. We demonstrate that training with such embeddings substantially enhances label noise robustness when compared to non-contrastive-based ones as well as commonly used noise-resilient methods. Our results unequivocally underline the superiority of contrastive learning in effectively mitigating the label noise challenge. Code is publicly available at https://github.com/LucasDedieu/NoiseResilientHistopathology.","sentences":["Recent advancements in deep learning have proven highly effective in medical image classification, notably within histopathology.","However, noisy labels represent a critical challenge in histopathology image classification, where accurate annotations are vital for training robust deep learning models.","Indeed, deep neural networks can easily overfit label noise, leading to severe degradations in model performance.","While numerous public pathology foundation models have emerged recently, none have evaluated their resilience to label noise.","Through thorough empirical analyses across multiple datasets, we exhibit the label noise resilience property of embeddings extracted from foundation models trained in a self-supervised contrastive manner.","We demonstrate that training with such embeddings substantially enhances label noise robustness when compared to non-contrastive-based ones as well as commonly used noise-resilient methods.","Our results unequivocally underline the superiority of contrastive learning in effectively mitigating the label noise challenge.","Code is publicly available at https://github.com/LucasDedieu/NoiseResilientHistopathology."],"url":"http://arxiv.org/abs/2404.07605v1","category":"cs.CV"}
{"created":"2024-04-11 09:39:58","title":"Implicit and Explicit Language Guidance for Diffusion-based Visual Perception","abstract":"Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%.","sentences":["Text-to-image diffusion models have shown powerful ability on conditional image synthesis.","With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts.","However, it is an open problem to adapt the pre-trained diffusion model for visual perception.","In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP.","Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch.","The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts.","The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model.","During training, we jointly train diffusion model by sharing the model weights of these two branches.","As a result, implicit and explicit branches can jointly guide feature learning.","During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels.","Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation.","Our IEDP achieves promising performance on both tasks.","For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%.","For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%."],"url":"http://arxiv.org/abs/2404.07600v1","category":"cs.CV"}
{"created":"2024-04-11 09:23:44","title":"Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization","abstract":"Although robot-assisted cardiovascular catheterization is commonly performed for intervention of cardiovascular diseases, more studies are needed to support the procedure with automated tool segmentation. This can aid surgeons on tool tracking and visualization during intervention. Learning-based segmentation has recently offered state-of-the-art segmentation performances however, generating ground-truth signals for fully-supervised methods is labor-intensive and time consuming for the interventionists. In this study, a weakly-supervised learning method with multi-lateral pseudo labeling is proposed for tool segmentation in cardiac angiograms. The method includes a modified U-Net model with one encoder and multiple lateral-branched decoders that produce pseudo labels as supervision signals under different perturbation. The pseudo labels are self-generated through a mixed loss function and shared consistency in the decoders. We trained the model end-to-end with weakly-annotated data obtained during robotic cardiac catheterization. Experiments with the proposed model shows weakly annotated data has closer performance to when fully annotated data is used. Compared to three existing weakly-supervised methods, our approach yielded higher segmentation performance across three different cardiac angiogram data. With ablation study, we showed consistent performance under different parameters. Thus, we offer a less expensive method for real-time tool segmentation and tracking during robot-assisted cardiac catheterization.","sentences":["Although robot-assisted cardiovascular catheterization is commonly performed for intervention of cardiovascular diseases, more studies are needed to support the procedure with automated tool segmentation.","This can aid surgeons on tool tracking and visualization during intervention.","Learning-based segmentation has recently offered state-of-the-art segmentation performances however, generating ground-truth signals for fully-supervised methods is labor-intensive and time consuming for the interventionists.","In this study, a weakly-supervised learning method with multi-lateral pseudo labeling is proposed for tool segmentation in cardiac angiograms.","The method includes a modified U-Net model with one encoder and multiple lateral-branched decoders that produce pseudo labels as supervision signals under different perturbation.","The pseudo labels are self-generated through a mixed loss function and shared consistency in the decoders.","We trained the model end-to-end with weakly-annotated data obtained during robotic cardiac catheterization.","Experiments with the proposed model shows weakly annotated data has closer performance to when fully annotated data is used.","Compared to three existing weakly-supervised methods, our approach yielded higher segmentation performance across three different cardiac angiogram data.","With ablation study, we showed consistent performance under different parameters.","Thus, we offer a less expensive method for real-time tool segmentation and tracking during robot-assisted cardiac catheterization."],"url":"http://arxiv.org/abs/2404.07594v1","category":"cs.CV"}
{"created":"2024-04-11 09:23:36","title":"Diffusion posterior sampling for simulation-based inference in tall data settings","abstract":"Determining which parameters of a non-linear model could best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators (a.k.a. black-box simulators). The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available and one wishes to leverage their shared information to better infer the parameters of the model. The method we propose is built upon recent developments from the flourishing score-based diffusion literature and allows us to estimate the tall data posterior distribution simply using information from the score network trained on individual observations. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.","sentences":["Determining which parameters of a non-linear model could best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators (a.k.a. black-box simulators).","The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used.","Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation.","In this work, we consider a tall data extension in which multiple observations are available and one wishes to leverage their shared information to better infer the parameters of the model.","The method we propose is built upon recent developments from the flourishing score-based diffusion literature and allows us to estimate the tall data posterior distribution simply using information from the score network trained on individual observations.","We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost."],"url":"http://arxiv.org/abs/2404.07593v1","category":"stat.ML"}
{"created":"2024-04-11 09:22:20","title":"Elliptic Virtual Structure Constants and Generalizations of BCOV-Zinger Formula to Projective Fano Hypersurfaces","abstract":"In this paper, we propose a recipe for B-model computation of genus 1 Gromov-Witten invariants of Calabi-Yau and Fano Projective Hypersurfaces. Our formalism can be applied equally to both Calabi-Yau and Fano cases. In Calabi-Yau case, drastic cancellation of terms used in our formalism occurs and it results in another representation of BCOV-Zinger formula for projective Calabi-Yau hypersurfaces.","sentences":["In this paper, we propose a recipe for B-model computation of genus 1 Gromov-Witten invariants of Calabi-Yau and Fano Projective Hypersurfaces.","Our formalism can be applied equally to both Calabi-Yau and Fano cases.","In Calabi-Yau case, drastic cancellation of terms used in our formalism occurs and it results in another representation of BCOV-Zinger formula for projective Calabi-Yau hypersurfaces."],"url":"http://arxiv.org/abs/2404.07591v1","category":"math.AG"}
{"created":"2024-04-11 09:19:32","title":"See the lightning, hear the thunder: quasinormal modes-shadow correspondence for rotating regular black holes","abstract":"Eikonal quasinormal modes (QNMs) of black holes (BHs) and parameters of null geodesics, ultimately tied to the appearance of BHs to external observers, are known to be related, and the eikonal QNM-BH shadow radii correspondence has been extensively studied for spherically symmetric BHs. The extension to rotating BHs is non-trivial, and has been worked out only for equatorial ($m=\\pm\\ell$) QNMs, or for general modes but limited to the Kerr metric. We extend the QNM-shadow radius correspondence to more general rotating space-times, and argue that the requirements for it to hold amount to conditions on the separability of the Hamilton-Jacobi equation for null geodesics and the Klein-Gordon equation. Metrics obtained by the Newman-Janis algorithm enjoy these conditions, provided certain mathematical requirements are imposed on the line element. We explicitly verify the correspondence for the rotating Bardeen and Hayward regular BHs, both of which satisfy the separability requirements. Our findings show that the QNM-shadow radius correspondence holds for a wide range of axisymmetric space-times beyond Kerr. This paves the way to potential strong-field multi-messenger tests of fundamental physics by hearing (via gravitational wave spectroscopy - the ``thunder'') and seeing (via VLBI imaging - the ``lightning'') BHs, although substantial improvements relative to the current observational sensitivity are required to make this possible.","sentences":["Eikonal quasinormal modes (QNMs) of black holes (BHs) and parameters of null geodesics, ultimately tied to the appearance of BHs to external observers, are known to be related, and the eikonal QNM-BH shadow radii correspondence has been extensively studied for spherically symmetric BHs.","The extension to rotating BHs is non-trivial, and has been worked out only for equatorial ($m=\\pm\\ell$) QNMs, or for general modes but limited to the Kerr metric.","We extend the QNM-shadow radius correspondence to more general rotating space-times, and argue that the requirements for it to hold amount to conditions on the separability of the Hamilton-Jacobi equation for null geodesics and the Klein-Gordon equation.","Metrics obtained by the Newman-Janis algorithm enjoy these conditions, provided certain mathematical requirements are imposed on the line element.","We explicitly verify the correspondence for the rotating Bardeen and Hayward regular BHs, both of which satisfy the separability requirements.","Our findings show that the QNM-shadow radius correspondence holds for a wide range of axisymmetric space-times beyond Kerr.","This paves the way to potential strong-field multi-messenger tests of fundamental physics by hearing (via gravitational wave spectroscopy - the ``thunder'') and seeing (via VLBI imaging - the ``lightning'') BHs, although substantial improvements relative to the current observational sensitivity are required to make this possible."],"url":"http://arxiv.org/abs/2404.07589v1","category":"gr-qc"}
{"created":"2024-04-11 09:18:52","title":"CP Violation in $B \\to K\\ell^+\\ell^-$ Decays: New Opportunities in the High-Precision Era","abstract":"Experimental data on rare $B$-meson decays indicate deviations from Standard Model predictions. In studies of these decays, possible new sources of CP violation are often neglected. We discuss CP violation in the rare $B$-meson decays $B\\to K\\ell^+\\ell^- (\\ell = \\mu,e)$ and point to two phenomena that arise when new sources of CP violation are included. First, the Wilson coefficients $C_{9\\ell}$ and $C_{10\\ell}$ become complex, and we show how we can extract their values from measurements of direct and mixing-induced CP asymmetries. Second, new sources of CP violation can generate nontrivial lepton flavour universality violation. Such a violation is usually measured through ratios like $R_K$ and $R_{K^*}$, but we show that measuring only these ratios leaves a large parameter space unexplored. These results bring exciting opportunities to reveal New Physics effects in the high-precision era.","sentences":["Experimental data on rare $B$-meson decays indicate deviations from Standard Model predictions.","In studies of these decays, possible new sources of CP violation are often neglected.","We discuss CP violation in the rare $B$-meson decays $B\\to K\\ell^+\\ell^- (\\ell = \\mu,e)$ and point to two phenomena that arise when new sources of CP violation are included.","First, the Wilson coefficients $C_{9\\ell}$ and $C_{10\\ell}$ become complex, and we show how we can extract their values from measurements of direct and mixing-induced CP asymmetries.","Second, new sources of CP violation can generate nontrivial lepton flavour universality violation.","Such a violation is usually measured through ratios like $R_K$ and $R_{K^*}$, but we show that measuring only these ratios leaves a large parameter space unexplored.","These results bring exciting opportunities to reveal New Physics effects in the high-precision era."],"url":"http://arxiv.org/abs/2404.07588v1","category":"hep-ph"}
{"created":"2024-04-11 09:11:55","title":"Revisiting Data Recovery Loops in 6G Networks","abstract":"Mechanisms for data recovery and packet reliability are essential components of the upcoming 6th generation (6G) communication system. In this paper, we evaluate the interaction between a fast hybrid automatic repeat request (HARQ) scheme, present in the physical and medium access control layers, and a higher layer automatic repeat request (ARQ) scheme which may be present in the radio link control layer. Through extensive system-level simulations, we show that despite its higher complexity, a fast HARQ scheme yields > 66 % downlink average user throughput gains over simpler solutions without energy combining gains and orders of magnitude larger gains for users in challenging radio conditions. We present results for the design trade-off between HARQ and higher-layer data recovery mechanisms in the presence of realistic control and data channel errors, network delays, and transport protocols. We derive that, with a suitable design of 6G control and data channels reaching residual errors at the medium access control layer of 5 E-5 or better, a higher layer data recovery mechanism can be disabled. We then derive design targets for 6G control channel design, as well as promising enhancements to 6G higher layer data recovery to extend support for latency-intolerant services.","sentences":["Mechanisms for data recovery and packet reliability are essential components of the upcoming 6th generation (6G) communication system.","In this paper, we evaluate the interaction between a fast hybrid automatic repeat request (HARQ) scheme, present in the physical and medium access control layers, and a higher layer automatic repeat request (ARQ) scheme which may be present in the radio link control layer.","Through extensive system-level simulations, we show that despite its higher complexity, a fast HARQ scheme yields > 66 % downlink average user throughput gains over simpler solutions without energy combining gains and orders of magnitude larger gains for users in challenging radio conditions.","We present results for the design trade-off between HARQ and higher-layer data recovery mechanisms in the presence of realistic control and data channel errors, network delays, and transport protocols.","We derive that, with a suitable design of 6G control and data channels reaching residual errors at the medium access control layer of 5 E-5 or better, a higher layer data recovery mechanism can be disabled.","We then derive design targets for 6G control channel design, as well as promising enhancements to 6G higher layer data recovery to extend support for latency-intolerant services."],"url":"http://arxiv.org/abs/2404.07579v1","category":"eess.SP"}
{"created":"2024-04-11 09:11:42","title":"Hyperpolarisation Dynamics: Asymptotic Polarisation","abstract":"For applications of solid state quantum computing and quantum simulations, high fidelity initialisation of thermally mixed electronic and nuclear spin qubits is essential. Whereas electronic spins can readily be initialised optically to high fidelity, initialisation of the nuclear spins requires alternative approaches, such as dynamic nuclear polarisation (DNP) via the electronic spin. Pulse-based DNP methods, such as PulsePol, are already widely utilised. By means of repeated application of PulsePol sequences, interspersed with re-initialisation of the electronic spin, high levels of nuclear polarisation -- termed hyperpolarisation -- have been achieved. From theoretical analysis of these protocols perfect nuclear initialisation is expected; however, in practice, saturation below $\\sim$ 95$\\%$ is seen in experiment. We develop an analytical model to describe hyperpolarisation dynamics, predicting non-maximal nuclear polarisation saturation in the asymptotic limit for realistic nuclear spin clusters. We argue that perfect initialisation of a typical nuclear cluster using this method may not, in general, be possible even with an arbitrarily large number of repetitions.","sentences":["For applications of solid state quantum computing and quantum simulations, high fidelity initialisation of thermally mixed electronic and nuclear spin qubits is essential.","Whereas electronic spins can readily be initialised optically to high fidelity, initialisation of the nuclear spins requires alternative approaches, such as dynamic nuclear polarisation (DNP) via the electronic spin.","Pulse-based DNP methods, such as PulsePol, are already widely utilised.","By means of repeated application of PulsePol sequences, interspersed with re-initialisation of the electronic spin, high levels of nuclear polarisation -- termed hyperpolarisation -- have been achieved.","From theoretical analysis of these protocols perfect nuclear initialisation is expected; however, in practice, saturation below $\\sim$ 95$\\%$ is seen in experiment.","We develop an analytical model to describe hyperpolarisation dynamics, predicting non-maximal nuclear polarisation saturation in the asymptotic limit for realistic nuclear spin clusters.","We argue that perfect initialisation of a typical nuclear cluster using this method may not, in general, be possible even with an arbitrarily large number of repetitions."],"url":"http://arxiv.org/abs/2404.07578v1","category":"quant-ph"}
{"created":"2024-04-11 09:08:45","title":"Generating Comprehensive Lithium Battery Charging Data with Generative AI","abstract":"In optimizing performance and extending the lifespan of lithium batteries, accurate state prediction is pivotal. Traditional regression and classification methods have achieved some success in battery state prediction. However, the efficacy of these data-driven approaches heavily relies on the availability and quality of public datasets. Additionally, generating electrochemical data predominantly through battery experiments is a lengthy and costly process, making it challenging to acquire high-quality electrochemical data. This difficulty, coupled with data incompleteness, significantly impacts prediction accuracy. Addressing these challenges, this study introduces the End of Life (EOL) and Equivalent Cycle Life (ECL) as conditions for generative AI models. By integrating an embedding layer into the CVAE model, we developed the Refined Conditional Variational Autoencoder (RCVAE). Through preprocessing data into a quasi-video format, our study achieves an integrated synthesis of electrochemical data, including voltage, current, temperature, and charging capacity, which is then processed by the RCVAE model. Coupled with customized training and inference algorithms, this model can generate specific electrochemical data for EOL and ECL under supervised conditions. This method provides users with a comprehensive electrochemical dataset, pioneering a new research domain for the artificial synthesis of lithium battery data. Furthermore, based on the detailed synthetic data, various battery state indicators can be calculated, offering new perspectives and possibilities for lithium battery performance prediction.","sentences":["In optimizing performance and extending the lifespan of lithium batteries, accurate state prediction is pivotal.","Traditional regression and classification methods have achieved some success in battery state prediction.","However, the efficacy of these data-driven approaches heavily relies on the availability and quality of public datasets.","Additionally, generating electrochemical data predominantly through battery experiments is a lengthy and costly process, making it challenging to acquire high-quality electrochemical data.","This difficulty, coupled with data incompleteness, significantly impacts prediction accuracy.","Addressing these challenges, this study introduces the End of Life (EOL) and Equivalent Cycle Life (ECL) as conditions for generative AI models.","By integrating an embedding layer into the CVAE model, we developed the Refined Conditional Variational Autoencoder (RCVAE).","Through preprocessing data into a quasi-video format, our study achieves an integrated synthesis of electrochemical data, including voltage, current, temperature, and charging capacity, which is then processed by the RCVAE model.","Coupled with customized training and inference algorithms, this model can generate specific electrochemical data for EOL and ECL under supervised conditions.","This method provides users with a comprehensive electrochemical dataset, pioneering a new research domain for the artificial synthesis of lithium battery data.","Furthermore, based on the detailed synthetic data, various battery state indicators can be calculated, offering new perspectives and possibilities for lithium battery performance prediction."],"url":"http://arxiv.org/abs/2404.07577v1","category":"cs.LG"}
{"created":"2024-04-11 09:07:57","title":"GAN-based iterative motion estimation in HASTE MRI","abstract":"Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation. In this paper we propose an iterative approach which uses GAN predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen.","sentences":["Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times.","Even during the acquisition of a single 2D slice, motion can severely corrupt the image.","Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data.","Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation.","In this paper we propose an iterative approach which uses GAN predictions for motion estimation.","The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability.","With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed.","We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen."],"url":"http://arxiv.org/abs/2404.07576v1","category":"math.NA"}
{"created":"2024-04-11 09:06:49","title":"An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution","abstract":"Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner's speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.","sentences":["Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner's speech.","Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods.","However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels.","To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features.","Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy."],"url":"http://arxiv.org/abs/2404.07575v1","category":"cs.SD"}
{"created":"2024-04-11 09:06:19","title":"International environmental treaties: An honest or a misguided effort","abstract":"Climate change and environmental concerns represent a global crisis accompanied by significant economic challenges. Regular international conferences held to address these issues, such as in the UK (2021) and Egypt (2022), spark debate about the effectiveness and practicality of international commitments. This study examines international treaties from a different perspective, emphasizing the need to understand the power dynamics and stakeholder interests that delay logical actions to mitigate anthropogenic contributions to climate change and their impacts. Environmental and social concerns tend to increase within nations as their economies develop, where they fight to keep acceptable standards of living while reducing emissions volume. So, nations play disproportionate roles in global decision-making based on the size of their economies. Addressing climate change requires a paradigm shift to emphasize acknowledging and adhering to global commitments through civil pressure, rather than relying on traditional yet biased systems of international political diplomacy. Here, climate-friendly actions are evaluated and ideas to promote such activities are proposed. We introduce a \"transition regime\" as a solution to this metastasis challenge which gradually infects all nations.","sentences":["Climate change and environmental concerns represent a global crisis accompanied by significant economic challenges.","Regular international conferences held to address these issues, such as in the UK (2021) and Egypt (2022), spark debate about the effectiveness and practicality of international commitments.","This study examines international treaties from a different perspective, emphasizing the need to understand the power dynamics and stakeholder interests that delay logical actions to mitigate anthropogenic contributions to climate change and their impacts.","Environmental and social concerns tend to increase within nations as their economies develop, where they fight to keep acceptable standards of living while reducing emissions volume.","So, nations play disproportionate roles in global decision-making based on the size of their economies.","Addressing climate change requires a paradigm shift to emphasize acknowledging and adhering to global commitments through civil pressure, rather than relying on traditional yet biased systems of international political diplomacy.","Here, climate-friendly actions are evaluated and ideas to promote such activities are proposed.","We introduce a \"transition regime\" as a solution to this metastasis challenge which gradually infects all nations."],"url":"http://arxiv.org/abs/2404.07574v1","category":"econ.GN"}
{"created":"2024-04-11 09:03:52","title":"Bayesian Inference with Gaussian Processes for the Determination of Parton Distribution Functions","abstract":"We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs). In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data. We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF. We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD. We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori.","sentences":["We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs).","In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data.","We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF.","We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD.","We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori."],"url":"http://arxiv.org/abs/2404.07573v1","category":"hep-ph"}
{"created":"2024-04-11 09:01:52","title":"Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing","abstract":"Neural networks have increasingly influenced people's lives. Ensuring the faithful deployment of neural networks as designed by their model owners is crucial, as they may be susceptible to various malicious or unintentional modifications, such as backdooring and poisoning attacks. Fragile model watermarks aim to prevent unexpected tampering that could lead DNN models to make incorrect decisions. They ensure the detection of any tampering with the model as sensitively as possible.However, prior watermarking methods suffered from inefficient sample generation and insufficient sensitivity, limiting their practical applicability. Our approach employs a sample-pairing technique, placing the model boundaries between pairs of samples, while simultaneously maximizing logits. This ensures that the model's decision results of sensitive samples change as much as possible and the Top-1 labels easily alter regardless of the direction it moves.","sentences":["Neural networks have increasingly influenced people's lives.","Ensuring the faithful deployment of neural networks as designed by their model owners is crucial, as they may be susceptible to various malicious or unintentional modifications, such as backdooring and poisoning attacks.","Fragile model watermarks aim to prevent unexpected tampering that could lead DNN models to make incorrect decisions.","They ensure the detection of any tampering with the model as sensitively as possible.","However, prior watermarking methods suffered from inefficient sample generation and insufficient sensitivity, limiting their practical applicability.","Our approach employs a sample-pairing technique, placing the model boundaries between pairs of samples, while simultaneously maximizing logits.","This ensures that the model's decision results of sensitive samples change as much as possible and the Top-1 labels easily alter regardless of the direction it moves."],"url":"http://arxiv.org/abs/2404.07572v1","category":"cs.CR"}
{"created":"2024-04-11 08:57:48","title":"Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?","abstract":"Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner's capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios.   A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization. We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark.","sentences":["Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios.","Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop).","In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios.","This makes it difficult to judge a planner's capabilities to generalize to rarely-seen situations.","Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios.","We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios.   ","A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization.","We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark."],"url":"http://arxiv.org/abs/2404.07569v1","category":"cs.RO"}
{"created":"2024-04-11 08:51:50","title":"Orbital dynamics in the GG Tau A system: investigating its enigmatic disc","abstract":"GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary. Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive. We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas. We test the dynamical evolution of the two scenarios on short and long timescales. We obtain synthetic flux emission from our simulations and we compare them with 1300 $\\mu$m ALMA dust continuum emission and 1.67 $\\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement. We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits. We found that the time-scale for the onset of the disc eccentricity growth, $\\tau_{ecc}$, is a fundamental time-scale for the morphology of the system. Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\\Delta\\theta= 30^\\circ$) on timescales shorter than $\\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring. However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\\tau_{ecc}$ and the models do not reproduce the observed morphology anymore. This implies that either the age of the system is shorter than $\\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated. This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs.","sentences":["GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary.","Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive.","We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas.","We test the dynamical evolution of the two scenarios on short and long timescales.","We obtain synthetic flux emission from our simulations and we compare them with 1300 $\\mu$m ALMA dust continuum emission and 1.67 $\\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement.","We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits.","We found that the time-scale for the onset of the disc eccentricity growth, $\\tau_{ecc}$, is a fundamental time-scale for the morphology of the system.","Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\\Delta\\theta= 30^\\circ$) on timescales shorter than $\\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring.","However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\\tau_{ecc}$ and the models do not reproduce the observed morphology anymore.","This implies that either the age of the system is shorter than $\\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated.","This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs."],"url":"http://arxiv.org/abs/2404.07565v1","category":"astro-ph.EP"}
{"created":"2024-04-11 08:50:12","title":"ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation","abstract":"We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets.","sentences":["We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels.","Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images.","This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images.","Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs.","Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms.","With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets."],"url":"http://arxiv.org/abs/2404.07564v1","category":"cs.CV"}
{"created":"2024-04-11 08:43:37","title":"Socially Pertinent Robots in Gerontological Healthcare","abstract":"Despite the many recent achievements in developing and deploying social robotics, there are still many underexplored environments and applications for which systematic evaluation of such systems by end-users is necessary. While several robotic platforms have been used in gerontological healthcare, the question of whether or not a social interactive robot with multi-modal conversational capabilities will be useful and accepted in real-life facilities is yet to be answered. This paper is an attempt to partially answer this question, via two waves of experiments with patients and companions in a day-care gerontological facility in Paris with a full-sized humanoid robot endowed with social and conversational interaction capabilities. The software architecture, developed during the H2020 SPRING project, together with the experimental protocol, allowed us to evaluate the acceptability (AES) and usability (SUS) with more than 60 end-users. Overall, the users are receptive to this technology, especially when the robot perception and action skills are robust to environmental clutter and flexible to handle a plethora of different interactions.","sentences":["Despite the many recent achievements in developing and deploying social robotics, there are still many underexplored environments and applications for which systematic evaluation of such systems by end-users is necessary.","While several robotic platforms have been used in gerontological healthcare, the question of whether or not a social interactive robot with multi-modal conversational capabilities will be useful and accepted in real-life facilities is yet to be answered.","This paper is an attempt to partially answer this question, via two waves of experiments with patients and companions in a day-care gerontological facility in Paris with a full-sized humanoid robot endowed with social and conversational interaction capabilities.","The software architecture, developed during the H2020 SPRING project, together with the experimental protocol, allowed us to evaluate the acceptability (AES) and usability (SUS) with more than 60 end-users.","Overall, the users are receptive to this technology, especially when the robot perception and action skills are robust to environmental clutter and flexible to handle a plethora of different interactions."],"url":"http://arxiv.org/abs/2404.07560v1","category":"cs.RO"}
{"created":"2024-04-11 08:42:51","title":"Differentially Private Reinforcement Learning with Self-Play","abstract":"We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first line of results towards understanding trajectory-wise privacy protection in multi-agent RL.","sentences":["We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints.","This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information.","We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection.","Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses.","The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms.","Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints.","To the best of our knowledge, these are the first line of results towards understanding trajectory-wise privacy protection in multi-agent RL."],"url":"http://arxiv.org/abs/2404.07559v1","category":"cs.LG"}
{"created":"2024-04-11 08:41:27","title":"Statistical Independence and the Brockwell Transform -- From an Integral Equation Perspective","abstract":"Statistical independence is a notion ubiquitous in various fields such as in statistics, probability, number theory and physics. We establish the stability of independence for any pair of random variables by their corresponding Brockwell transforms (Brockwell, 2007) beyond the non-atomic condition that is naturally imposed on their distributions, thereby generalizing the proposition originated by Cai et al. (2022). A central novelty in our work is to formulate the problem as a possibly new type of mathematical inverse problem, which aims to claim that an integral equation has a unique solution in terms of its stochastic kernel -- not under-determined contrary to the usual cases, and also to design a recursive constructive scheme, combined with the property of the quantile function, that solves the aforementioned integral equation in an iterative manner.","sentences":["Statistical independence is a notion ubiquitous in various fields such as in statistics, probability, number theory and physics.","We establish the stability of independence for any pair of random variables by their corresponding Brockwell transforms (Brockwell, 2007) beyond the non-atomic condition that is naturally imposed on their distributions, thereby generalizing the proposition originated by Cai et al. (2022).","A central novelty in our work is to formulate the problem as a possibly new type of mathematical inverse problem, which aims to claim that an integral equation has a unique solution in terms of its stochastic kernel -- not under-determined contrary to the usual cases, and also to design a recursive constructive scheme, combined with the property of the quantile function, that solves the aforementioned integral equation in an iterative manner."],"url":"http://arxiv.org/abs/2404.07558v1","category":"math.PR"}
{"created":"2024-04-11 08:36:13","title":"CAT: Contrastive Adapter Training for Personalized Image Generation","abstract":"The emergence of various adapters, including Low-Rank Adaptation (LoRA) applied from the field of natural language processing, has allowed diffusion models to personalize image generation at a low cost. However, due to the various challenges including limited datasets and shortage of regularization and computation resources, adapter training often results in unsatisfactory outcomes, leading to the corruption of the backbone model's prior knowledge. One of the well known phenomena is the loss of diversity in object generation, especially within the same class which leads to generating almost identical objects with minor variations. This poses challenges in generation capabilities. To solve this issue, we present Contrastive Adapter Training (CAT), a simple yet effective strategy to enhance adapter training through the application of CAT loss. Our approach facilitates the preservation of the base model's original knowledge when the model initiates adapters. Furthermore, we introduce the Knowledge Preservation Score (KPS) to evaluate CAT's ability to keep the former information. We qualitatively and quantitatively compare CAT's improvement. Finally, we mention the possibility of CAT in the aspects of multi-concept adapter and optimization.","sentences":["The emergence of various adapters, including Low-Rank Adaptation (LoRA) applied from the field of natural language processing, has allowed diffusion models to personalize image generation at a low cost.","However, due to the various challenges including limited datasets and shortage of regularization and computation resources, adapter training often results in unsatisfactory outcomes, leading to the corruption of the backbone model's prior knowledge.","One of the well known phenomena is the loss of diversity in object generation, especially within the same class which leads to generating almost identical objects with minor variations.","This poses challenges in generation capabilities.","To solve this issue, we present Contrastive Adapter Training (CAT), a simple yet effective strategy to enhance adapter training through the application of CAT loss.","Our approach facilitates the preservation of the base model's original knowledge when the model initiates adapters.","Furthermore, we introduce the Knowledge Preservation Score (KPS) to evaluate CAT's ability to keep the former information.","We qualitatively and quantitatively compare CAT's improvement.","Finally, we mention the possibility of CAT in the aspects of multi-concept adapter and optimization."],"url":"http://arxiv.org/abs/2404.07554v1","category":"cs.CV"}
{"created":"2024-04-11 08:30:46","title":"Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective","abstract":"Code generation aims to understand the problem description and generate corresponding code snippets, where existing works generally decompose such complex tasks into intermediate steps by prompting strategies, such as Chain-of-Thought and its variants. While these studies have achieved some success, their effectiveness is highly dependent on the capabilities of advanced Large Language Models (LLMs) such as GPT-4, particularly in terms of API calls, which significantly limits their practical applicability. Consequently, how to enhance the code generation capabilities of small and medium-scale code LLMs without significantly increasing training costs is an appealing challenge. In this paper, we suggest that code comments are the natural logic pivot between natural language and code language and propose using comments to boost the code generation ability of code LLMs. Concretely, we propose MANGO (comMents As Natural loGic pivOts), including a comment contrastive training strategy and a corresponding logical comment decoding strategy. Experiments are performed on HumanEval and MBPP, utilizing StarCoder and WizardCoder as backbone models, and encompassing model parameter sizes between 3B and 7B. The results indicate that MANGO significantly improves the code pass rate based on the strong baselines. Meanwhile, the robustness of the logical comment decoding strategy is notably higher than the Chain-of-thoughts prompting. The code is publicly available at \\url{https://github.com/pppa2019/Mango}.","sentences":["Code generation aims to understand the problem description and generate corresponding code snippets, where existing works generally decompose such complex tasks into intermediate steps by prompting strategies, such as Chain-of-Thought and its variants.","While these studies have achieved some success, their effectiveness is highly dependent on the capabilities of advanced Large Language Models (LLMs) such as GPT-4, particularly in terms of API calls, which significantly limits their practical applicability.","Consequently, how to enhance the code generation capabilities of small and medium-scale code LLMs without significantly increasing training costs is an appealing challenge.","In this paper, we suggest that code comments are the natural logic pivot between natural language and code language and propose using comments to boost the code generation ability of code LLMs.","Concretely, we propose MANGO (comMents As Natural loGic pivOts), including a comment contrastive training strategy and a corresponding logical comment decoding strategy.","Experiments are performed on HumanEval and MBPP, utilizing StarCoder and WizardCoder as backbone models, and encompassing model parameter sizes between 3B and 7B.","The results indicate that MANGO significantly improves the code pass rate based on the strong baselines.","Meanwhile, the robustness of the logical comment decoding strategy is notably higher than the Chain-of-thoughts prompting.","The code is publicly available at \\url{https://github.com/pppa2019/Mango}."],"url":"http://arxiv.org/abs/2404.07549v1","category":"cs.CL"}
{"created":"2024-04-11 08:27:23","title":"DeVAIC: A Tool for Security Assessment of AI-generated Code","abstract":"Context: AI code generators are revolutionizing code writing and software development, but their training on large datasets, including potentially untrusted source code, raises security concerns. Furthermore, these generators can produce incomplete code snippets that are challenging to evaluate using current solutions. Objective: This research work introduces DeVAIC (Detection of Vulnerabilities in AI-generated Code), a tool to evaluate the security of AI-generated Python code, which overcomes the challenge of examining incomplete code. Method: We followed a methodological approach that involved gathering vulnerable samples, extracting implementation patterns, and creating regular expressions to develop the proposed tool. The implementation of DeVAIC includes a set of detection rules based on regular expressions that cover 35 Common Weakness Enumerations (CWEs) falling under the OWASP Top 10 vulnerability categories. Results: We utilized four popular AI models to generate Python code, which we then used as a foundation to evaluate the effectiveness of our tool. DeVAIC demonstrated a statistically significant difference in its ability to detect security vulnerabilities compared to the state-of-the-art solutions, showing an F1 Score and Accuracy of 94% while maintaining a low computational cost of 0.14 seconds per code snippet, on average. Conclusions: The proposed tool provides a lightweight and efficient solution for vulnerability detection even on incomplete code.","sentences":["Context: AI code generators are revolutionizing code writing and software development, but their training on large datasets, including potentially untrusted source code, raises security concerns.","Furthermore, these generators can produce incomplete code snippets that are challenging to evaluate using current solutions.","Objective:","This research work introduces DeVAIC (Detection of Vulnerabilities in AI-generated Code), a tool to evaluate the security of AI-generated Python code, which overcomes the challenge of examining incomplete code.","Method: We followed a methodological approach that involved gathering vulnerable samples, extracting implementation patterns, and creating regular expressions to develop the proposed tool.","The implementation of DeVAIC includes a set of detection rules based on regular expressions that cover 35 Common Weakness Enumerations (CWEs) falling under the OWASP Top 10 vulnerability categories.","Results:","We utilized four popular AI models to generate Python code, which we then used as a foundation to evaluate the effectiveness of our tool.","DeVAIC demonstrated a statistically significant difference in its ability to detect security vulnerabilities compared to the state-of-the-art solutions, showing an F1 Score and Accuracy of 94% while maintaining a low computational cost of 0.14 seconds per code snippet, on average.","Conclusions: The proposed tool provides a lightweight and efficient solution for vulnerability detection even on incomplete code."],"url":"http://arxiv.org/abs/2404.07548v1","category":"cs.SE"}
{"created":"2024-04-11 08:21:03","title":"Leveraging Eclipse MOSAIC for Modeling and Analyzing Ride-Hailing Services","abstract":"Ride-hailing services enjoy a large popularity in the sector of individualized mobility. Due to broad availability, ease of use, and competitive pricing strategies, these services have established themselves throughout the last decades. With the increased popularity, ride-hailing providers aimed to consistently improve the efficiency of their services, leading to the inception of novel research questions. Many of which can be effectively tackled using simulation. In this paper, we present such a simulation-based approach using Eclipse MOSAIC in-hand with a large-scale traffic scenario of Berlin. We analyze real-world logbook data including detailed shifts of drivers and discuss how to integrate them with the simulation scenario. Moreover, we present extensions to MOSAIC required for the modeling of the ride-hailing services, utilizing the powerful Application Simulator. Accordingly, as the primary result of this paper, we managed to extend the Eclipse MOSAIC framework to be able to answer research questions in the domain of ride-hailing and ride-sharing. Additionally, in an initial exemplary study, we analyze the traffic and environmental impacts of different, yet basic, rebalancing strategies, finding non-negligible differences in mileages and pollutant emissions. We, furthermore, applied our findings to the entire ride-hailing fleet in the city of Berlin for one year, showcasing the impacts different rebalancing strategies could have on environment and general traffic. To our knowledge, the consideration of environmental factors on a city-wide scale is a novel contribution of this paper, not addressed in previous research.","sentences":["Ride-hailing services enjoy a large popularity in the sector of individualized mobility.","Due to broad availability, ease of use, and competitive pricing strategies, these services have established themselves throughout the last decades.","With the increased popularity, ride-hailing providers aimed to consistently improve the efficiency of their services, leading to the inception of novel research questions.","Many of which can be effectively tackled using simulation.","In this paper, we present such a simulation-based approach using Eclipse MOSAIC in-hand with a large-scale traffic scenario of Berlin.","We analyze real-world logbook data including detailed shifts of drivers and discuss how to integrate them with the simulation scenario.","Moreover, we present extensions to MOSAIC required for the modeling of the ride-hailing services, utilizing the powerful Application Simulator.","Accordingly, as the primary result of this paper, we managed to extend the Eclipse MOSAIC framework to be able to answer research questions in the domain of ride-hailing and ride-sharing.","Additionally, in an initial exemplary study, we analyze the traffic and environmental impacts of different, yet basic, rebalancing strategies, finding non-negligible differences in mileages and pollutant emissions.","We, furthermore, applied our findings to the entire ride-hailing fleet in the city of Berlin for one year, showcasing the impacts different rebalancing strategies could have on environment and general traffic.","To our knowledge, the consideration of environmental factors on a city-wide scale is a novel contribution of this paper, not addressed in previous research."],"url":"http://arxiv.org/abs/2404.07547v1","category":"eess.SY"}
{"created":"2024-04-11 08:20:10","title":"Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning","abstract":"In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format which helps LLMs to respond in desired label words. We then demonstrate this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL and find that retrieving the most semantically similar examples notably boosts model's discriminative capability.","sentences":["In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs).","By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters.","However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies.","In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks.","Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models.","However, ICL exhibits significant efficacy in regulating the label space and format which helps LLMs to respond in desired label words.","We then demonstrate this ability functions similar to detailed instructions for LLMs to follow.","We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL and find that retrieving the most semantically similar examples notably boosts model's discriminative capability."],"url":"http://arxiv.org/abs/2404.07546v1","category":"cs.CL"}
{"created":"2024-04-11 08:12:48","title":"Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion","abstract":"Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on benchmark tests show its superior performance. Our code is available at https://github.com/SJTU-ViSYS/SDG-Depth.","sentences":["Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception.","While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution.","To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth.","Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window.","These maps then guide cost aggregation in stereo matching.","To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module.","Our method is both accurate and efficient.","The experimental results on benchmark tests show its superior performance.","Our code is available at https://github.com/SJTU-ViSYS/SDG-Depth."],"url":"http://arxiv.org/abs/2404.07545v1","category":"cs.CV"}
{"created":"2024-04-11 08:12:43","title":"From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples","abstract":"We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of large language models scales with the number of in-context exemplars. We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret.","sentences":["We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates.","Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting.","For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting.","We then investigate how well the performance of large language models scales with the number of in-context exemplars.","We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret."],"url":"http://arxiv.org/abs/2404.07544v1","category":"cs.CL"}
{"created":"2024-04-11 08:03:53","title":"Impact of Training Instance Selection on Automated Algorithm Selection Models for Numerical Black-box Optimization","abstract":"The recently proposed MA-BBOB function generator provides a way to create numerical black-box benchmark problems based on the well-established BBOB suite. Initial studies on this generator highlighted its ability to smoothly transition between the component functions, both from a low-level landscape feature perspective, as well as with regard to algorithm performance. This suggests that MA-BBOB-generated functions can be an ideal testbed for automated machine learning methods, such as automated algorithm selection (AAS).   In this paper, we generate 11800 functions in dimensions $d=2$ and $d=5$, respectively, and analyze the potential gains from AAS by studying performance complementarity within a set of eight algorithms. We combine this performance data with exploratory landscape features to create an AAS pipeline that we use to investigate how to efficiently select training sets within this space. We show that simply using the BBOB component functions for training yields poor test performance, while the ranking between uniformly chosen and diversity-based training sets strongly depends on the distribution of the test set.","sentences":["The recently proposed MA-BBOB function generator provides a way to create numerical black-box benchmark problems based on the well-established BBOB suite.","Initial studies on this generator highlighted its ability to smoothly transition between the component functions, both from a low-level landscape feature perspective, as well as with regard to algorithm performance.","This suggests that MA-BBOB-generated functions can be an ideal testbed for automated machine learning methods, such as automated algorithm selection (AAS).   ","In this paper, we generate 11800 functions in dimensions $d=2$ and $d=5$, respectively, and analyze the potential gains from AAS by studying performance complementarity within a set of eight algorithms.","We combine this performance data with exploratory landscape features to create an AAS pipeline that we use to investigate how to efficiently select training sets within this space.","We show that simply using the BBOB component functions for training yields poor test performance, while the ranking between uniformly chosen and diversity-based training sets strongly depends on the distribution of the test set."],"url":"http://arxiv.org/abs/2404.07539v1","category":"cs.NE"}
{"created":"2024-04-11 08:03:23","title":"How is Visual Attention Influenced by Text Guidance? Database and Model","abstract":"The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: https://github.com/IntMeGroup/TGSal.","sentences":["The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing.","In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance.","In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives.","Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data.","Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention.","Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models.","Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions.","Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics.","The SJTU-TIS database and the code of the proposed TGSal model will be released at: https://github.com/IntMeGroup/TGSal."],"url":"http://arxiv.org/abs/2404.07537v1","category":"cs.CV"}
{"created":"2024-04-11 07:54:21","title":"Realizing Laser-driven Deuteron Acceleration with Low Energy Spread via In-situ D$_2$O-deposited Target","abstract":"Generation of quasi-monoenergetic ion pulse by laser-driven acceleration is one of the hot topics in laser plasma physics. In this study, we present a new method for the \\textit{In-situ} deposition of an ultra-thin D$_2$O layer on the surface of an aluminum foil target utilizing a spherical D$_2$O capsule. Employing a 10$^{19}$ W/cm$^2$ laser, we achieve the acceleration of 10.8 MeV deuterons with an energy spread of $\\Delta$E/E = 4.6% in the most favorable shot. The energy spread depends on the exposure time of the D$_2$O capsule in the vacuum chamber. This method is simple in setup and operation, and has the potential to extend its applicability to other ion species.","sentences":["Generation of quasi-monoenergetic ion pulse by laser-driven acceleration is one of the hot topics in laser plasma physics.","In this study, we present a new method for the \\textit{In-situ} deposition of an ultra-thin D$_2$O layer on the surface of an aluminum foil target utilizing a spherical D$_2$O capsule.","Employing a 10$^{19}$ W/cm$^2$ laser, we achieve the acceleration of 10.8 MeV deuterons with an energy spread of $\\Delta$E/E = 4.6% in the most favorable shot.","The energy spread depends on the exposure time of the D$_2$O capsule in the vacuum chamber.","This method is simple in setup and operation, and has the potential to extend its applicability to other ion species."],"url":"http://arxiv.org/abs/2404.07534v1","category":"physics.plasm-ph"}
{"created":"2024-04-11 07:54:14","title":"IITP-VDLand: A Comprehensive Dataset on Decentraland Parcels","abstract":"This paper presents IITP-VDLand, a comprehensive dataset of Decentraland parcels sourced from diverse platforms. Unlike existing datasets which have limited attributes and records, IITP-VDLand offers a rich array of attributes, encompassing parcel characteristics, trading history, past activities, transactions, and social media interactions. Alongside, we introduce a key attribute in the dataset, namely Rarity score, which measures the uniqueness of each parcel within the virtual world. Addressing the significant challenge posed by the dispersed nature of this data across various sources, we employ a systematic approach, utilizing both available APIs and custom scripts, to gather it. Subsequently, we meticulously curate and organize the information into four distinct segments: (1) Characteristics Data-Fragment, (2) OpenSea Trading History Data-Fragment, (3) Ethereum Activity Transactions Data-Fragment, and (4) Social Media Data-Fragment. We envisage that this dataset would serve as a robust resource for training machine- and deep-learning models specifically designed to address real-world challenges within the domain of Decentraland parcels. The performance benchmarking of more than 20 state-of-the-art price prediction models on our dataset yields promising results, achieving a maximum R2 score of 0.8251 and an accuracy of 74.23% in case of Extra Trees Regressor and Classifier. The key findings reveal that the ensemble models performs better than both deep learning and linear models for our dataset. We observe a significant impact of coordinates, geographical proximity, rarity score, and few other economic indicators on the prediction of parcel prices.","sentences":["This paper presents IITP-VDLand, a comprehensive dataset of Decentraland parcels sourced from diverse platforms.","Unlike existing datasets which have limited attributes and records, IITP-VDLand offers a rich array of attributes, encompassing parcel characteristics, trading history, past activities, transactions, and social media interactions.","Alongside, we introduce a key attribute in the dataset, namely Rarity score, which measures the uniqueness of each parcel within the virtual world.","Addressing the significant challenge posed by the dispersed nature of this data across various sources, we employ a systematic approach, utilizing both available APIs and custom scripts, to gather it.","Subsequently, we meticulously curate and organize the information into four distinct segments: (1) Characteristics Data-Fragment, (2) OpenSea Trading History Data-Fragment, (3) Ethereum Activity Transactions Data-Fragment, and (4) Social Media Data-Fragment.","We envisage that this dataset would serve as a robust resource for training machine- and deep-learning models specifically designed to address real-world challenges within the domain of Decentraland parcels.","The performance benchmarking of more than 20 state-of-the-art price prediction models on our dataset yields promising results, achieving a maximum R2 score of 0.8251 and an accuracy of 74.23% in case of Extra Trees Regressor and Classifier.","The key findings reveal that the ensemble models performs better than both deep learning and linear models for our dataset.","We observe a significant impact of coordinates, geographical proximity, rarity score, and few other economic indicators on the prediction of parcel prices."],"url":"http://arxiv.org/abs/2404.07533v1","category":"cs.LG"}
{"created":"2024-04-11 07:51:30","title":"Bayesian Federated Model Compression for Communication and Computation Efficiency","abstract":"In this paper, we investigate Bayesian model compression in federated learning (FL) to construct sparse models that can achieve both communication and computation efficiencies. We propose a decentralized Turbo variational Bayesian inference (D-Turbo-VBI) FL framework where we firstly propose a hierarchical sparse prior to promote a clustered sparse structure in the weight matrix. Then, by carefully integrating message passing and VBI with a decentralized turbo framework, we propose the D-Turbo-VBI algorithm which can (i) reduce both upstream and downstream communication overhead during federated training, and (ii) reduce the computational complexity during local inference. Additionally, we establish the convergence property for thr proposed D-Turbo-VBI algorithm. Simulation results show the significant gain of our proposed algorithm over the baselines in reducing communication overhead during federated training and computational complexity of final model.","sentences":["In this paper, we investigate Bayesian model compression in federated learning (FL) to construct sparse models that can achieve both communication and computation efficiencies.","We propose a decentralized Turbo variational Bayesian inference (D-Turbo-VBI) FL framework where we firstly propose a hierarchical sparse prior to promote a clustered sparse structure in the weight matrix.","Then, by carefully integrating message passing and VBI with a decentralized turbo framework, we propose the D-Turbo-VBI algorithm which can (i) reduce both upstream and downstream communication overhead during federated training, and (ii) reduce the computational complexity during local inference.","Additionally, we establish the convergence property for thr proposed D-Turbo-VBI algorithm.","Simulation results show the significant gain of our proposed algorithm over the baselines in reducing communication overhead during federated training and computational complexity of final model."],"url":"http://arxiv.org/abs/2404.07532v1","category":"cs.LG"}
{"created":"2024-04-11 07:45:48","title":"Promoting collective cooperation through temporal interactions","abstract":"Collective cooperation drives the dynamics of many natural, social, and economic phenomena, making understanding the evolution of cooperation with evolutionary game theory a central question of modern science. Although human interactions are best described as complex networks, current explorations are limited to static networks where interactions represented by network links are permanent and do not change over time. In reality, human activities often involve temporal interactions, where links are impermanent, and understanding the evolution of cooperation on such ubiquitous temporal networks is an open question. Here, we present a general framework for systematically analyzing how collective cooperation evolves on any temporal network, which unifies the study of evolutionary game dynamics with dynamic and static interactions. We show that the emergence of cooperation is facilitated by a simple rule of thumb: hubs (individuals with many social ties) should be temporally deprioritized in interactions. We further provide a quantitative metric capturing the priority of hubs, which we utilize to orchestrate the ordering of interactions to best promote cooperation on empirical temporal networks. Our findings unveil the fundamental advantages conferred by temporal interactions for promoting collective cooperation, which transcends the specific insights gleaned from studying traditional static snapshots.","sentences":["Collective cooperation drives the dynamics of many natural, social, and economic phenomena, making understanding the evolution of cooperation with evolutionary game theory a central question of modern science.","Although human interactions are best described as complex networks, current explorations are limited to static networks where interactions represented by network links are permanent and do not change over time.","In reality, human activities often involve temporal interactions, where links are impermanent, and understanding the evolution of cooperation on such ubiquitous temporal networks is an open question.","Here, we present a general framework for systematically analyzing how collective cooperation evolves on any temporal network, which unifies the study of evolutionary game dynamics with dynamic and static interactions.","We show that the emergence of cooperation is facilitated by a simple rule of thumb: hubs (individuals with many social ties) should be temporally deprioritized in interactions.","We further provide a quantitative metric capturing the priority of hubs, which we utilize to orchestrate the ordering of interactions to best promote cooperation on empirical temporal networks.","Our findings unveil the fundamental advantages conferred by temporal interactions for promoting collective cooperation, which transcends the specific insights gleaned from studying traditional static snapshots."],"url":"http://arxiv.org/abs/2404.07530v1","category":"physics.soc-ph"}
{"created":"2024-04-11 07:43:04","title":"The morphology of cell spheroids in simple shear flow","abstract":"Cell spheroids are a widely used model to investigate cell-cell and cell-matrix interactions in a 3D microenvironment in vitro. Most research on cell spheroids has been focused on their response to various stimuli under static conditions. Recently, the effect of flow on cell spheroids has been investigated in the context of tumor invasion in interstitial space. In particular, microfluidic perfusion of cell spheroids embedded in a collagen matrix has been shown to modulate cell-cell adhesion and to represent a possible mechanism promoting tumor invasion by interstitial flow. However, studies on the effects of well-defined flow fields on cell spheroids are lacking in the literature. Here, we apply simple shear flow to cell spheroids in a parallel plate apparatus while observing their morphology by optical microscopy. By using image analysis techniques, we show that cell spheroids rotate under flow as rigid particles. As time goes on, cells from the outer layer detach from the sheared cell spheroids and are carried away by the flow. Hence, the size of cell spheroids declines with time at a rate increasing with the external shear stress, which can be used to estimate cell-cell adhesion. The technique proposed in this work allows one to correlate flow-induced effects with microscopy imaging of cell spheroids in a well-established shear flow field, thus providing a method to obtain quantitative results which are relevant in the general field of mechanobiology.","sentences":["Cell spheroids are a widely used model to investigate cell-cell and cell-matrix interactions in a 3D microenvironment in vitro.","Most research on cell spheroids has been focused on their response to various stimuli under static conditions.","Recently, the effect of flow on cell spheroids has been investigated in the context of tumor invasion in interstitial space.","In particular, microfluidic perfusion of cell spheroids embedded in a collagen matrix has been shown to modulate cell-cell adhesion and to represent a possible mechanism promoting tumor invasion by interstitial flow.","However, studies on the effects of well-defined flow fields on cell spheroids are lacking in the literature.","Here, we apply simple shear flow to cell spheroids in a parallel plate apparatus while observing their morphology by optical microscopy.","By using image analysis techniques, we show that cell spheroids rotate under flow as rigid particles.","As time goes on, cells from the outer layer detach from the sheared cell spheroids and are carried away by the flow.","Hence, the size of cell spheroids declines with time at a rate increasing with the external shear stress, which can be used to estimate cell-cell adhesion.","The technique proposed in this work allows one to correlate flow-induced effects with microscopy imaging of cell spheroids in a well-established shear flow field, thus providing a method to obtain quantitative results which are relevant in the general field of mechanobiology."],"url":"http://arxiv.org/abs/2404.07528v1","category":"q-bio.CB"}
{"created":"2024-04-11 07:40:41","title":"On the convergence analysis of one-shot inversion methods","abstract":"When an inverse problem is solved by a gradient-based optimization algorithm, the corresponding forward and adjoint problems, which are introduced to compute the gradient, can be also solved iteratively. The idea of iterating at the same time on the inverse problem unknown and on the forward and adjoint problem solutions yields the concept of one-shot inversion methods. We are especially interested in the case where the inner iterations for the direct and adjoint problems are incomplete, that is, stopped before achieving a high accuracy on their solutions. Here, we focus on general linear inverse problems and generic fixed-point iterations for the associated forward problem. We analyze variants of the so-called multi-step one-shot methods, in particular semi-implicit schemes with a regularization parameter. We establish sufficient conditions on the descent step for convergence, by studying the eigenvalues of the block matrix of the coupled iterations. Several numerical experiments are provided to illustrate the convergence of these methods in comparison with the classical gradient descent, where the forward and adjoint problems are solved exactly by a direct solver instead. We observe that very few inner iterations are enough to guarantee good convergence of the inversion algorithm, even in the presence of noisy data.","sentences":["When an inverse problem is solved by a gradient-based optimization algorithm, the corresponding forward and adjoint problems, which are introduced to compute the gradient, can be also solved iteratively.","The idea of iterating at the same time on the inverse problem unknown and on the forward and adjoint problem solutions yields the concept of one-shot inversion methods.","We are especially interested in the case where the inner iterations for the direct and adjoint problems are incomplete, that is, stopped before achieving a high accuracy on their solutions.","Here, we focus on general linear inverse problems and generic fixed-point iterations for the associated forward problem.","We analyze variants of the so-called multi-step one-shot methods, in particular semi-implicit schemes with a regularization parameter.","We establish sufficient conditions on the descent step for convergence, by studying the eigenvalues of the block matrix of the coupled iterations.","Several numerical experiments are provided to illustrate the convergence of these methods in comparison with the classical gradient descent, where the forward and adjoint problems are solved exactly by a direct solver instead.","We observe that very few inner iterations are enough to guarantee good convergence of the inversion algorithm, even in the presence of noisy data."],"url":"http://arxiv.org/abs/2404.07526v1","category":"math.NA"}
{"created":"2024-04-11 07:38:36","title":"Photon Counting Interferometry to Detect Geontropic Space-Time Fluctuations with GQuEST","abstract":"The GQuEST (Gravity from the Quantum Entanglement of Space-Time) experiment uses tabletop-scale Michelson laser interferometers to probe for fluctuations in space-time. We present an interferometer design featuring a novel photon counting readout method that provides unprecedented sensitivity, as it is not subject to the interferometric standard quantum limit. We evaluate the potential of this design to measure space-time fluctuations motivated by recent `geontropic' quantum gravity models. The accelerated accrual of statistical power offered by the photon counting readout enables GQuEST to detect the predicted quantum gravity phenomena within measurement times at least 100 times shorter than equivalent conventional interferometers. The GQuEST design thus enables a fast and sensitive search for signatures of quantum gravity in a laboratory-scale experiment.","sentences":["The GQuEST (Gravity from the Quantum Entanglement of Space-Time) experiment uses tabletop-scale Michelson laser interferometers to probe for fluctuations in space-time.","We present an interferometer design featuring a novel photon counting readout method that provides unprecedented sensitivity, as it is not subject to the interferometric standard quantum limit.","We evaluate the potential of this design to measure space-time fluctuations motivated by recent `geontropic' quantum gravity models.","The accelerated accrual of statistical power offered by the photon counting readout enables GQuEST to detect the predicted quantum gravity phenomena within measurement times at least 100 times shorter than equivalent conventional interferometers.","The GQuEST design thus enables a fast and sensitive search for signatures of quantum gravity in a laboratory-scale experiment."],"url":"http://arxiv.org/abs/2404.07524v1","category":"gr-qc"}
{"created":"2024-04-11 07:36:00","title":"GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks","abstract":"Successful supply chain optimization must mitigate imbalances between supply and demand over time. While accurate demand prediction is essential for supply planning, it alone does not suffice. The key to successful supply planning for optimal and viable execution lies in maximizing predictability for both demand and supply throughout an execution horizon. Therefore, enhancing the accuracy of supply predictions is imperative to create an attainable supply plan that matches demand without overstocking or understocking. However, in complex supply chain networks with numerous nodes and edges, accurate supply predictions are challenging due to dynamic node interactions, cascading supply delays, resource availability, production and logistic capabilities. Consequently, supply executions often deviate from their initial plans. To address this, we present the Graph-based Supply Prediction (GSP) probabilistic model. Our attention-based graph neural network (GNN) model predicts supplies, inventory, and imbalances using graph-structured historical data, demand forecasting, and original supply plan inputs. The experiments, conducted using historical data from a global consumer goods company's large-scale supply chain, demonstrate that GSP significantly improves supply and inventory prediction accuracy, potentially offering supply plan corrections to optimize executions.","sentences":["Successful supply chain optimization must mitigate imbalances between supply and demand over time.","While accurate demand prediction is essential for supply planning, it alone does not suffice.","The key to successful supply planning for optimal and viable execution lies in maximizing predictability for both demand and supply throughout an execution horizon.","Therefore, enhancing the accuracy of supply predictions is imperative to create an attainable supply plan that matches demand without overstocking or understocking.","However, in complex supply chain networks with numerous nodes and edges, accurate supply predictions are challenging due to dynamic node interactions, cascading supply delays, resource availability, production and logistic capabilities.","Consequently, supply executions often deviate from their initial plans.","To address this, we present the Graph-based Supply Prediction (GSP) probabilistic model.","Our attention-based graph neural network (GNN) model predicts supplies, inventory, and imbalances using graph-structured historical data, demand forecasting, and original supply plan inputs.","The experiments, conducted using historical data from a global consumer goods company's large-scale supply chain, demonstrate that GSP significantly improves supply and inventory prediction accuracy, potentially offering supply plan corrections to optimize executions."],"url":"http://arxiv.org/abs/2404.07523v1","category":"cs.AI"}
{"created":"2024-04-11 07:29:43","title":"Thermodynamics of black holes featuring primary scalar hair","abstract":"In this work, we embark on the thermodynamic investigation concerning a family of primary charged black holes within the context of shift and parity symmetric Beyond Horndeski gravity. Employing the Euclidean approach, we derive the functional expression for the free energy and derive the first thermodynamic law, offering a methodology to address the challenge of extracting the thermal quantities in shift-symmetric scalar tensor theories characterized by linear time dependence in the scalar field. Following the formal analysis, we provide some illustrative examples focusing on the thermal evaporation of these fascinating objects.","sentences":["In this work, we embark on the thermodynamic investigation concerning a family of primary charged black holes within the context of shift and parity symmetric Beyond Horndeski gravity.","Employing the Euclidean approach, we derive the functional expression for the free energy and derive the first thermodynamic law, offering a methodology to address the challenge of extracting the thermal quantities in shift-symmetric scalar tensor theories characterized by linear time dependence in the scalar field.","Following the formal analysis, we provide some illustrative examples focusing on the thermal evaporation of these fascinating objects."],"url":"http://arxiv.org/abs/2404.07522v1","category":"hep-th"}
{"created":"2024-04-11 07:29:26","title":"Advancements in Secondary and Backscattered Electron Energy Spectra and Yields Analysis: from Theory to Applications","abstract":"Over the past decade, experimental microscopy and spectroscopy have made significant progress in the study of the morphological, optical, electronic and transport properties of materials. These developments include higher spatial resolution, shorter acquisition times, more efficient monochromators and electron analysers, improved contrast imaging and advancements in sample preparation techniques. These advances have driven the need for more accurate theoretical descriptions and predictions of material properties. Computer simulations based on first principles and Monte Carlo methods have emerged as a rapidly growing field for modeling the interaction of charged particles, such as electron, proton and ion beams, with various systems, such as slabs, nanostructures and crystals. This report delves into the theoretical and computational approaches to modeling the physico-chemical mechanisms that occur when charged beams interact with a medium. These mechanisms encompass single and collective electronic excitation, ionization of the target atoms and the generation of a secondary electron cascade that deposits energy into the irradiated material. We show that the combined application of ab initio methods, which are able to model the dynamics of interacting many-fermion systems, and Monte Carlo methods, which capture statistical fluctuations in energy loss mechanisms by random sampling, proves to be an optimal strategy for the accurate description of charge transport in solids. This joint quantitative approach enables the theoretical interpretation of excitation, loss and secondary electron spectra, the analysis of the chemical composition and dielectric properties of solids and contributes to our understanding of irradiation-induced damage in materials, including those of biological significance.","sentences":["Over the past decade, experimental microscopy and spectroscopy have made significant progress in the study of the morphological, optical, electronic and transport properties of materials.","These developments include higher spatial resolution, shorter acquisition times, more efficient monochromators and electron analysers, improved contrast imaging and advancements in sample preparation techniques.","These advances have driven the need for more accurate theoretical descriptions and predictions of material properties.","Computer simulations based on first principles and Monte Carlo methods have emerged as a rapidly growing field for modeling the interaction of charged particles, such as electron, proton and ion beams, with various systems, such as slabs, nanostructures and crystals.","This report delves into the theoretical and computational approaches to modeling the physico-chemical mechanisms that occur when charged beams interact with a medium.","These mechanisms encompass single and collective electronic excitation, ionization of the target atoms and the generation of a secondary electron cascade that deposits energy into the irradiated material.","We show that the combined application of ab initio methods, which are able to model the dynamics of interacting many-fermion systems, and Monte Carlo methods, which capture statistical fluctuations in energy loss mechanisms by random sampling, proves to be an optimal strategy for the accurate description of charge transport in solids.","This joint quantitative approach enables the theoretical interpretation of excitation, loss and secondary electron spectra, the analysis of the chemical composition and dielectric properties of solids and contributes to our understanding of irradiation-induced damage in materials, including those of biological significance."],"url":"http://arxiv.org/abs/2404.07521v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 07:26:00","title":"PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination","abstract":"The potential for zero-shot generalization in vision-language (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks. Previous methods have employed test-time prompt tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions. In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views. Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using contrastive learning. The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the prompt representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains. Our method, named PromptSync, synchronizes the prompts for each test sample on both the text and vision branches of the V-L model. In empirical evaluations on the domain generalization benchmark, our method outperforms previous best methods by 2.33\\% in overall performance, by 1\\% in base-to-novel generalization, and by 2.84\\% in cross-dataset transfer tasks.","sentences":["The potential for zero-shot generalization in vision-language (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks.","Previous methods have employed test-time prompt tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions.","In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views.","Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using contrastive learning.","The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the prompt representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains.","Our method, named PromptSync, synchronizes the prompts for each test sample on both the text and vision branches of the V-L model.","In empirical evaluations on the domain generalization benchmark, our method outperforms previous best methods by 2.33\\% in overall performance, by 1\\% in base-to-novel generalization, and by 2.84\\% in cross-dataset transfer tasks."],"url":"http://arxiv.org/abs/2404.07520v1","category":"cs.CV"}
{"created":"2024-04-11 07:23:19","title":"LATTE: Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer","abstract":"With the rise of Transformer models in NLP and CV domain, Multi-Head Attention has been proven to be a game-changer. However, its expensive computation poses challenges to the model throughput and efficiency, especially for the long sequence tasks. Exploiting the sparsity in attention has been proven to be an effective way to reduce computation. Nevertheless, prior works do not consider the various distributions among different heads and lack a systematic method to determine the threshold. To address these challenges, we propose Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer (LATTE). LATTE employs a headwise threshold-based filter with the low-precision dot product and computation reuse mechanism to reduce the computation of MHA. Moreover, the trainable threshold is introduced to provide a systematic method for adjusting the thresholds and enable end-to-end optimization. Experimental results indicate LATTE can smoothly adapt to both NLP and CV tasks, offering significant computation savings with only a minor compromise in performance. Also, the trainable threshold is shown to be essential for the leverage between the performance and the computation. As a result, LATTE filters up to 85.16% keys with only a 0.87% accuracy drop in the CV task and 89.91% keys with a 0.86 perplexity increase in the NLP task.","sentences":["With the rise of Transformer models in NLP and CV domain, Multi-Head Attention has been proven to be a game-changer.","However, its expensive computation poses challenges to the model throughput and efficiency, especially for the long sequence tasks.","Exploiting the sparsity in attention has been proven to be an effective way to reduce computation.","Nevertheless, prior works do not consider the various distributions among different heads and lack a systematic method to determine the threshold.","To address these challenges, we propose Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer (LATTE).","LATTE employs a headwise threshold-based filter with the low-precision dot product and computation reuse mechanism to reduce the computation of MHA.","Moreover, the trainable threshold is introduced to provide a systematic method for adjusting the thresholds and enable end-to-end optimization.","Experimental results indicate LATTE can smoothly adapt to both NLP and CV tasks, offering significant computation savings with only a minor compromise in performance.","Also, the trainable threshold is shown to be essential for the leverage between the performance and the computation.","As a result, LATTE filters up to 85.16% keys with only a 0.87% accuracy drop in the CV task and 89.91% keys with a 0.86 perplexity increase in the NLP task."],"url":"http://arxiv.org/abs/2404.07519v1","category":"eess.IV"}
{"created":"2024-04-11 07:22:14","title":"Remembering Transformer for Continual Learning","abstract":"Neural networks encounter the challenge of Catastrophic Forgetting (CF) in continual learning, where new task knowledge interferes with previously learned knowledge. We propose Remembering Transformer, inspired by the brain's Complementary Learning Systems (CLS), to tackle this issue. Remembering Transformer employs a mixture-of-adapters and a generative model-based routing mechanism to alleviate CF by dynamically routing task data to relevant adapters. Our approach demonstrated a new SOTA performance in various vision continual learning tasks and great parameter efficiency.","sentences":["Neural networks encounter the challenge of Catastrophic Forgetting (CF) in continual learning, where new task knowledge interferes with previously learned knowledge.","We propose Remembering Transformer, inspired by the brain's Complementary Learning Systems (CLS), to tackle this issue.","Remembering Transformer employs a mixture-of-adapters and a generative model-based routing mechanism to alleviate CF by dynamically routing task data to relevant adapters.","Our approach demonstrated a new SOTA performance in various vision continual learning tasks and great parameter efficiency."],"url":"http://arxiv.org/abs/2404.07518v1","category":"cs.LG"}
{"created":"2024-04-11 07:11:43","title":"Generalization Gap in Data Augmentation: Insights from Illumination","abstract":"In the field of computer vision, data augmentation is widely used to enrich the feature complexity of training datasets with deep learning techniques. However, regarding the generalization capabilities of models, the difference in artificial features generated by data augmentation and natural visual features has not been fully revealed. This study focuses on the visual representation variable 'illumination', by simulating its distribution degradation and examining how data augmentation techniques enhance model performance on a classification task. Our goal is to investigate the differences in generalization between models trained with augmented data and those trained under real-world illumination conditions. Results indicate that after undergoing various data augmentation methods, model performance has been significantly improved. Yet, a noticeable generalization gap still exists after utilizing various data augmentation methods, emphasizing the critical role of feature diversity in the training set for enhancing model generalization.","sentences":["In the field of computer vision, data augmentation is widely used to enrich the feature complexity of training datasets with deep learning techniques.","However, regarding the generalization capabilities of models, the difference in artificial features generated by data augmentation and natural visual features has not been fully revealed.","This study focuses on the visual representation variable 'illumination', by simulating its distribution degradation and examining how data augmentation techniques enhance model performance on a classification task.","Our goal is to investigate the differences in generalization between models trained with augmented data and those trained under real-world illumination conditions.","Results indicate that after undergoing various data augmentation methods, model performance has been significantly improved.","Yet, a noticeable generalization gap still exists after utilizing various data augmentation methods, emphasizing the critical role of feature diversity in the training set for enhancing model generalization."],"url":"http://arxiv.org/abs/2404.07514v1","category":"cs.CV"}
{"created":"2024-04-11 17:49:47","title":"Results on pattern avoidance in parking functions","abstract":"In this paper, we mainly study two notions of pattern avoidance in parking functions. First, for any collection of length 3 patterns, we compute the number of parking functions of size $n$ that avoid them under the first notion. This is motivated by recent work of Adeniran and Pudwell, who obtained analogous results using a second notion of pattern avoidance. Then, we provide new purely bijective proofs for two of their results, and improve the formula of another one. Finally, we apply similar enumeration techniques to the work of Novelli and Thibon on certain Hopf algebras of generalised parking functions, and compute their graded dimensions.","sentences":["In this paper, we mainly study two notions of pattern avoidance in parking functions.","First, for any collection of length 3 patterns, we compute the number of parking functions of size $n$ that avoid them under the first notion.","This is motivated by recent work of Adeniran and Pudwell, who obtained analogous results using a second notion of pattern avoidance.","Then, we provide new purely bijective proofs for two of their results, and improve the formula of another one.","Finally, we apply similar enumeration techniques to the work of Novelli and Thibon on certain Hopf algebras of generalised parking functions, and compute their graded dimensions."],"url":"http://arxiv.org/abs/2404.07958v1","category":"math.CO"}
{"created":"2024-04-11 16:41:08","title":"Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation with a Time Budget","abstract":"Complex multi-objective missions require the coordination of heterogeneous robots at multiple inter-connected levels, such as coalition formation, scheduling, and motion planning. The associated challenges are exacerbated when solutions to these interconnected problems need to both maximize task performance and respect practical constraints on time and resources. In this work, we formulate a new class of spatio-temporal heterogeneous task allocation problems that consider these complexities. We contribute a novel framework, named Quality-Optimized Incremental Task Allocation Graph Search (Q-ITAGS), to solve such problems. Q-ITAGS builds upon our prior work in trait-based coordination and offers a flexible interleaved framework that i) explicitly models and optimizes the effect of collective capabilities on task performance via learnable trait-quality maps, and ii) respects both resource constraints and spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). In addition to algorithmic contributions, we derive theoretical suboptimality bounds in terms of task performance that varies as a function of a single hyperparameter. Our detailed experiments involving a simulated emergency response task and a real-world video game dataset reveal that i) Q-ITAGS results in superior team performance compared to a state-of-the-art method, while also respecting complex spatio-temporal and resource constraints, ii) Q-ITAGS efficiently learns trait-quality maps to enable effective trade-off between task performance and resource constraints, and iii) Q-ITAGS' suboptimality bounds consistently hold in practice.","sentences":["Complex multi-objective missions require the coordination of heterogeneous robots at multiple inter-connected levels, such as coalition formation, scheduling, and motion planning.","The associated challenges are exacerbated when solutions to these interconnected problems need to both maximize task performance and respect practical constraints on time and resources.","In this work, we formulate a new class of spatio-temporal heterogeneous task allocation problems that consider these complexities.","We contribute a novel framework, named Quality-Optimized Incremental Task Allocation Graph Search (Q-ITAGS), to solve such problems.","Q-ITAGS builds upon our prior work in trait-based coordination and offers a flexible interleaved framework that i) explicitly models and optimizes the effect of collective capabilities on task performance via learnable trait-quality maps, and ii) respects both resource constraints and spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan).","In addition to algorithmic contributions, we derive theoretical suboptimality bounds in terms of task performance that varies as a function of a single hyperparameter.","Our detailed experiments involving a simulated emergency response task and a real-world video game dataset reveal that i) Q-ITAGS results in superior team performance compared to a state-of-the-art method, while also respecting complex spatio-temporal and resource constraints, ii) Q-ITAGS efficiently learns trait-quality maps to enable effective trade-off between task performance and resource constraints, and iii) Q-ITAGS' suboptimality bounds consistently hold in practice."],"url":"http://arxiv.org/abs/2404.07902v1","category":"cs.MA"}
{"created":"2024-04-11 15:55:44","title":"Emergent factorization of Hilbert space at large $N$ and black hole","abstract":"We investigate the emergent factorization of Hilbert space in the low-energy description of matrix models, addressing key aspects of the black hole information paradox. We examine the collective description for the low-energy sector of $SU(N)$ matrix model, characterized by a factorized Hilbert space composed of a finite number of boxes and anti-boxes. This factorization leads us to examine the emergence of thermofield dynamics (TFD) state in the low energy sector from a fine-tuned state. In addition, we study the collective Hamiltonian of the $U(N)$ matrix model for the semi-classical description of \"particle-hole\" fluctuations around a background Young tableau. Our investigation of these matrix models elucidates a concrete mechanism for constructing the truncated algebra of accessible observables, thereby facilitating an understanding of black hole complementarity. In the context of the black hole information paradox, we discuss the origin of the island appearing inside the black hole and provide a reinterpretation of the recent proposal -- the holography of information.","sentences":["We investigate the emergent factorization of Hilbert space in the low-energy description of matrix models, addressing key aspects of the black hole information paradox.","We examine the collective description for the low-energy sector of $SU(N)$ matrix model, characterized by a factorized Hilbert space composed of a finite number of boxes and anti-boxes.","This factorization leads us to examine the emergence of thermofield dynamics (TFD) state in the low energy sector from a fine-tuned state.","In addition, we study the collective Hamiltonian of the $U(N)$ matrix model for the semi-classical description of \"particle-hole\" fluctuations around a background Young tableau.","Our investigation of these matrix models elucidates a concrete mechanism for constructing the truncated algebra of accessible observables, thereby facilitating an understanding of black hole complementarity.","In the context of the black hole information paradox, we discuss the origin of the island appearing inside the black hole and provide a reinterpretation of the recent proposal -- the holography of information."],"url":"http://arxiv.org/abs/2404.07862v1","category":"hep-th"}
{"created":"2024-04-11 15:25:13","title":"Data-Driven System Identification of Quadrotors Subject to Motor Delays","abstract":"Recently non-linear control methods like Model Predictive Control (MPC) and Reinforcement Learning (RL) have attracted increased interest in the quadrotor control community. In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics. The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand. Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected. In this work, we introduce a data-driven method to identify a quadrotor's inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data. The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant. Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers. Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors. It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions.","sentences":["Recently non-linear control methods like Model Predictive Control (MPC) and Reinforcement Learning (RL) have attracted increased interest in the quadrotor control community.","In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics.","The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand.","Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected.","In this work, we introduce a data-driven method to identify a quadrotor's inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data.","The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured.","We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant.","Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers.","Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors.","It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions."],"url":"http://arxiv.org/abs/2404.07837v1","category":"cs.RO"}
{"created":"2024-04-11 15:14:26","title":"Social Dilemma of Non-Pharmaceutical Interventions","abstract":"In fighting infectious diseases posing a global health threat, ranging from influenza to Zika, non-pharmaceutical interventions (NPI), such as social distancing and face covering, remain mitigation measures public health can resort to. However, the success of NPI lies in sufficiently high levels of collective compliance, otherwise giving rise to waves of infection incidences that are not only driven by pathogen evolution but also changing vigilance in the population. Here we show that compliance with each NPI measure can be highly dynamic and context-dependent during an ongoing epidemic, where individuals may prefer one to another or even do nothing, leading to intricate temporal switching behavior of NPI adoptions. By characterizing dynamic regimes through the perceived costs of NPI measures and their effectiveness in particular regarding face covering and social distancing, our work offers new insights into overcoming barriers in NPI adoptions.","sentences":["In fighting infectious diseases posing a global health threat, ranging from influenza to Zika, non-pharmaceutical interventions (NPI), such as social distancing and face covering, remain mitigation measures public health can resort to.","However, the success of NPI lies in sufficiently high levels of collective compliance, otherwise giving rise to waves of infection incidences that are not only driven by pathogen evolution but also changing vigilance in the population.","Here we show that compliance with each NPI measure can be highly dynamic and context-dependent during an ongoing epidemic, where individuals may prefer one to another or even do nothing, leading to intricate temporal switching behavior of NPI adoptions.","By characterizing dynamic regimes through the perceived costs of NPI measures and their effectiveness in particular regarding face covering and social distancing, our work offers new insights into overcoming barriers in NPI adoptions."],"url":"http://arxiv.org/abs/2404.07829v1","category":"physics.soc-ph"}
{"created":"2024-04-11 14:44:01","title":"Optical and Transport Properties of Plasma Mixtures from Ab Initio Molecular Dynamics","abstract":"Predicting the charged particle transport properties of warm dense matter / hot dense plasma mixtures is a challenge for analytical models. High accuracy ab initio methods are more computationally expensive, but can provide critical insight by explicitly simulating mixtures. In this work, we investigate the transport properties and optical response of warm dense carbon-hydrogen mixtures at varying concentrations under either conserved electronic pressure or mass density at a constant temperature. We compare options for mixing the calculated pure species properties to estimate the results of the mixtures. We find that a combination of the Drude model with the Matthiessen's rule works well for DC electron transport and low frequency optical response. This breaks down at higher frequencies, where a volumetric mix of pure-species AC conductivities works better.","sentences":["Predicting the charged particle transport properties of warm dense matter / hot dense plasma mixtures is a challenge for analytical models.","High accuracy ab initio methods are more computationally expensive, but can provide critical insight by explicitly simulating mixtures.","In this work, we investigate the transport properties and optical response of warm dense carbon-hydrogen mixtures at varying concentrations under either conserved electronic pressure or mass density at a constant temperature.","We compare options for mixing the calculated pure species properties to estimate the results of the mixtures.","We find that a combination of the Drude model with the Matthiessen's rule works well for DC electron transport and low frequency optical response.","This breaks down at higher frequencies, where a volumetric mix of pure-species AC conductivities works better."],"url":"http://arxiv.org/abs/2404.07800v1","category":"physics.plasm-ph"}
{"created":"2024-04-11 13:46:05","title":"3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces","abstract":"The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.","sentences":["The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years.","The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics.","However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples.","To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data.","In the proposed framework, we transform an input sample into two sets of profiles along different directions.","Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components.","In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix.","Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices.","Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods."],"url":"http://arxiv.org/abs/2404.07748v1","category":"cs.CV"}
{"created":"2024-04-11 13:24:58","title":"Point cloud obstacle detection with the map filtration","abstract":"Obstacle detection is one of the basic tasks of a robot movement in an unknown environment. The use of a LiDAR (Light Detection And Ranging) sensor allows one to obtain a point cloud in the vicinity of the sensor. After processing this data, obstacles can be found and recorded on a map. For this task, I present a pipeline capable of detecting obstacles even on a computationally limited device. The pipeline was also tested on a real robot and qualitatively evaluated on a dataset, which was collected in Brno University of Technology lab. Time consumption was recorded and compared with 3D object detectors.","sentences":["Obstacle detection is one of the basic tasks of a robot movement in an unknown environment.","The use of a LiDAR (Light Detection And Ranging) sensor allows one to obtain a point cloud in the vicinity of the sensor.","After processing this data, obstacles can be found and recorded on a map.","For this task, I present a pipeline capable of detecting obstacles even on a computationally limited device.","The pipeline was also tested on a real robot and qualitatively evaluated on a dataset, which was collected in Brno University of Technology lab.","Time consumption was recorded and compared with 3D object detectors."],"url":"http://arxiv.org/abs/2404.07730v1","category":"cs.RO"}
{"created":"2024-04-11 12:14:04","title":"Opportunistic Sensor-Based Multi-Factor Authentication in and for the Internet of Things","abstract":"Communication between connected objects often requires secure and reliable authentication mechanisms. These mechanisms are essential for verifying the identities of objects and preventing unauthorized access. The IoT offers several advantages and opportunities that are not necessarily found in other domains. For instance, IoT sensors collect real-time data about their environment and other objects which contain valuable information that, if used, can reinforce authentication. In this paper, we propose a novel idea for building opportunistic sensor-based authentication factors between IoT objects by leveraging the sensors already present in the systems where they interact. We claim that sensors can be utilized to build factors that reinforce object-to-object authentication mechanisms. Through the integration of these opportunistic sensor-based authentication factors into multi-factor authentication mechanisms, authentication in IoT can achieve a higher level of security. We provide illustrative experiments on two types of vehicles : mobile robots and cars.","sentences":["Communication between connected objects often requires secure and reliable authentication mechanisms.","These mechanisms are essential for verifying the identities of objects and preventing unauthorized access.","The IoT offers several advantages and opportunities that are not necessarily found in other domains.","For instance, IoT sensors collect real-time data about their environment and other objects which contain valuable information that, if used, can reinforce authentication.","In this paper, we propose a novel idea for building opportunistic sensor-based authentication factors between IoT objects by leveraging the sensors already present in the systems where they interact.","We claim that sensors can be utilized to build factors that reinforce object-to-object authentication mechanisms.","Through the integration of these opportunistic sensor-based authentication factors into multi-factor authentication mechanisms, authentication in IoT can achieve a higher level of security.","We provide illustrative experiments on two types of vehicles : mobile robots and cars."],"url":"http://arxiv.org/abs/2404.07675v1","category":"cs.CR"}
{"created":"2024-04-11 11:12:11","title":"Coexistence of Pull and Push Communication in Wireless Access for IoT Devices","abstract":"We consider a setup with Internet of Things (IoT), where a base station (BS) collects data from nodes that use two different communication modes. The first is pull-based, where the BS retrieves the data from specific nodes through queries. In addition, the nodes that apply pull-based communication contain a wake-up receiver: upon a query, the BS sends wake-up signal (WuS) to activate the corresponding devices equipped with wake-up receiver (WuDs). The second one is push-based communication, in which the nodes decide when to send to the BS. Consider a time-slotted model, where the time slots in each frame are shared for both pull-based and push-based communications. Therein, this coexistence scenario gives rise to a new type of problem with fundamental trade-offs in sharing communication resources: the objective to serve a maximum number of queries, within a specified deadline, limits the transmission opportunities for push sensors, and vice versa. This work develops a mathematical model that characterizes these trade-offs, validates them through simulations, and optimizes the frame design to meet the objectives of both the pull- and push-based communications.","sentences":["We consider a setup with Internet of Things (IoT), where a base station (BS) collects data from nodes that use two different communication modes.","The first is pull-based, where the BS retrieves the data from specific nodes through queries.","In addition, the nodes that apply pull-based communication contain a wake-up receiver: upon a query, the BS sends wake-up signal (WuS) to activate the corresponding devices equipped with wake-up receiver (WuDs).","The second one is push-based communication, in which the nodes decide when to send to the BS. Consider a time-slotted model, where the time slots in each frame are shared for both pull-based and push-based communications.","Therein, this coexistence scenario gives rise to a new type of problem with fundamental trade-offs in sharing communication resources: the objective to serve a maximum number of queries, within a specified deadline, limits the transmission opportunities for push sensors, and vice versa.","This work develops a mathematical model that characterizes these trade-offs, validates them through simulations, and optimizes the frame design to meet the objectives of both the pull- and push-based communications."],"url":"http://arxiv.org/abs/2404.07650v1","category":"eess.SP"}
{"created":"2024-04-11 09:14:40","title":"Delocalized low-frequency magnetoplasmon in a two-dimensional electron fluid with cylindrical symmetry","abstract":"The properties of a two-dimensional (2D) electron system can be drastically altered by a magnetic field applied perpendicular to the 2D plane. In particular, the frequency of its bulk collective excitations becomes gaped at the cyclotron frequency, while the low-frequency localized excitations, the edge magnetoplasmon (EMP), appear near the system's edge. A new type of the delocalized low-frequency excitations, the gradient magnetoplasmon (GMP), was recently shown to exist in a 2D electron system with a linear density gradient that breaks the system's cylindrical symmetry [Phys. Rev. B \\textbf{103}, 075420 (2021)]. Like EMP, these new excitations are gapless and chiral, and originate from the classical Hall effect. Here we show that a similar magnetoplasmon mode can exist in a system with strongly-inhomogeneous radial distribution of the electron density that preserves the cylindrical symmetry. This is experimentally demonstrated in a pristine system of surface electrons on liquid helium and is confirmed by a numerical simulation. This result extends the variety of known collective excitations in a 2D charge system and presents electrons on helium as a promising model system for their study.","sentences":["The properties of a two-dimensional (2D) electron system can be drastically altered by a magnetic field applied perpendicular to the 2D plane.","In particular, the frequency of its bulk collective excitations becomes gaped at the cyclotron frequency, while the low-frequency localized excitations, the edge magnetoplasmon (EMP), appear near the system's edge.","A new type of the delocalized low-frequency excitations, the gradient magnetoplasmon (GMP), was recently shown to exist in a 2D electron system with a linear density gradient that breaks the system's cylindrical symmetry [Phys. Rev. B \\textbf{103}, 075420 (2021)].","Like EMP, these new excitations are gapless and chiral, and originate from the classical Hall effect.","Here we show that a similar magnetoplasmon mode can exist in a system with strongly-inhomogeneous radial distribution of the electron density that preserves the cylindrical symmetry.","This is experimentally demonstrated in a pristine system of surface electrons on liquid helium and is confirmed by a numerical simulation.","This result extends the variety of known collective excitations in a 2D charge system and presents electrons on helium as a promising model system for their study."],"url":"http://arxiv.org/abs/2404.07582v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 09:01:07","title":"A continuous-time violation-free multi-agent optimization algorithm and its applications to safe distributed control","abstract":"In this work, we propose a continuous-time distributed optimization algorithm with guaranteed zero coupling constraint violation and apply it to safe distributed control in the presence of multiple control barrier functions (CBF). The optimization problem is defined over a network that collectively minimizes a separable cost function with coupled linear constraints. An equivalent optimization problem with auxiliary decision variables and a decoupling structure is proposed. A sensitivity analysis demonstrates that the subgradient information can be computed using local information. This then leads to a subgradient algorithm for updating the auxiliary variables. A case with sparse coupling constraints is further considered, and it is shown to have better memory and communication efficiency. For the specific case of a CBF-induced time-varying quadratic program (QP), an update law is proposed that achieves finite-time convergence. Numerical results involving a static resource allocation problem and a safe coordination problem for a multi-agent system demonstrate the efficiency and effectiveness of our proposed algorithms.","sentences":["In this work, we propose a continuous-time distributed optimization algorithm with guaranteed zero coupling constraint violation and apply it to safe distributed control in the presence of multiple control barrier functions (CBF).","The optimization problem is defined over a network that collectively minimizes a separable cost function with coupled linear constraints.","An equivalent optimization problem with auxiliary decision variables and a decoupling structure is proposed.","A sensitivity analysis demonstrates that the subgradient information can be computed using local information.","This then leads to a subgradient algorithm for updating the auxiliary variables.","A case with sparse coupling constraints is further considered, and it is shown to have better memory and communication efficiency.","For the specific case of a CBF-induced time-varying quadratic program (QP), an update law is proposed that achieves finite-time convergence.","Numerical results involving a static resource allocation problem and a safe coordination problem for a multi-agent system demonstrate the efficiency and effectiveness of our proposed algorithms."],"url":"http://arxiv.org/abs/2404.07571v1","category":"math.OC"}
{"created":"2024-04-11 08:34:10","title":"Event-Enhanced Snapshot Compressive Videography at 10K FPS","abstract":"Video snapshot compressive imaging (SCI) encodes the target dynamic scene compactly into a snapshot and reconstructs its high-speed frame sequence afterward, greatly reducing the required data footprint and transmission bandwidth as well as enabling high-speed imaging with a low frame rate intensity camera. In implementation, high-speed dynamics are encoded via temporally varying patterns, and only frames at corresponding temporal intervals can be reconstructed, while the dynamics occurring between consecutive frames are lost. To unlock the potential of conventional snapshot compressive videography, we propose a novel hybrid \"intensity+event\" imaging scheme by incorporating an event camera into a video SCI setup. Our proposed system consists of a dual-path optical setup to record the coded intensity measurement and intermediate event signals simultaneously, which is compact and photon-efficient by collecting the half photons discarded in conventional video SCI. Correspondingly, we developed a dual-branch Transformer utilizing the reciprocal relationship between two data modes to decode dense video frames. Extensive experiments on both simulated and real-captured data demonstrate our superiority to state-of-the-art video SCI and video frame interpolation (VFI) methods. Benefiting from the new hybrid design leveraging both intrinsic redundancy in videos and the unique feature of event cameras, we achieve high-quality videography at 0.1ms time intervals with a low-cost CMOS image sensor working at 24 FPS.","sentences":["Video snapshot compressive imaging (SCI) encodes the target dynamic scene compactly into a snapshot and reconstructs its high-speed frame sequence afterward, greatly reducing the required data footprint and transmission bandwidth as well as enabling high-speed imaging with a low frame rate intensity camera.","In implementation, high-speed dynamics are encoded via temporally varying patterns, and only frames at corresponding temporal intervals can be reconstructed, while the dynamics occurring between consecutive frames are lost.","To unlock the potential of conventional snapshot compressive videography, we propose a novel hybrid \"intensity+event\" imaging scheme by incorporating an event camera into a video SCI setup.","Our proposed system consists of a dual-path optical setup to record the coded intensity measurement and intermediate event signals simultaneously, which is compact and photon-efficient by collecting the half photons discarded in conventional video SCI.","Correspondingly, we developed a dual-branch Transformer utilizing the reciprocal relationship between two data modes to decode dense video frames.","Extensive experiments on both simulated and real-captured data demonstrate our superiority to state-of-the-art video SCI and video frame interpolation (VFI) methods.","Benefiting from the new hybrid design leveraging both intrinsic redundancy in videos and the unique feature of event cameras, we achieve high-quality videography at 0.1ms time intervals with a low-cost CMOS image sensor working at 24 FPS."],"url":"http://arxiv.org/abs/2404.07551v1","category":"eess.IV"}
{"created":"2024-04-11 07:06:58","title":"Generative Probabilistic Planning for Optimizing Supply Chain Networks","abstract":"Supply chain networks in enterprises are typically composed of complex topological graphs involving various types of nodes and edges, accommodating numerous products with considerable demand and supply variability. However, as supply chain networks expand in size and complexity, traditional supply chain planning methods (e.g., those found in heuristic rule-based and operations research-based systems) tend to become locally optimal or lack computational scalability, resulting in substantial imbalances between supply and demand across nodes in the network. This paper introduces a novel Generative AI technique, which we call Generative Probabilistic Planning (GPP). GPP generates dynamic supply action plans that are globally optimized across all network nodes over the time horizon for changing objectives like maximizing profits or service levels, factoring in time-varying probabilistic demand, lead time, and production conditions. GPP leverages attention-based graph neural networks (GNN), offline deep reinforcement learning (Offline RL), and policy simulations to train generative policy models and create optimal plans through probabilistic simulations, effectively accounting for various uncertainties. Our experiments using historical data from a global consumer goods company with complex supply chain networks demonstrate that GPP accomplishes objective-adaptable, probabilistically resilient, and dynamic planning for supply chain networks, leading to significant improvements in performance and profitability for enterprises. Our work plays a pivotal role in shaping the trajectory of AI adoption within the supply chain domain.","sentences":["Supply chain networks in enterprises are typically composed of complex topological graphs involving various types of nodes and edges, accommodating numerous products with considerable demand and supply variability.","However, as supply chain networks expand in size and complexity, traditional supply chain planning methods (e.g., those found in heuristic rule-based and operations research-based systems) tend to become locally optimal or lack computational scalability, resulting in substantial imbalances between supply and demand across nodes in the network.","This paper introduces a novel Generative AI technique, which we call Generative Probabilistic Planning (GPP).","GPP generates dynamic supply action plans that are globally optimized across all network nodes over the time horizon for changing objectives like maximizing profits or service levels, factoring in time-varying probabilistic demand, lead time, and production conditions.","GPP leverages attention-based graph neural networks (GNN), offline deep reinforcement learning (Offline RL), and policy simulations to train generative policy models and create optimal plans through probabilistic simulations, effectively accounting for various uncertainties.","Our experiments using historical data from a global consumer goods company with complex supply chain networks demonstrate that GPP accomplishes objective-adaptable, probabilistically resilient, and dynamic planning for supply chain networks, leading to significant improvements in performance and profitability for enterprises.","Our work plays a pivotal role in shaping the trajectory of AI adoption within the supply chain domain."],"url":"http://arxiv.org/abs/2404.07511v1","category":"cs.AI"}
{"created":"2024-04-11 06:39:53","title":"Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange","abstract":"In the realm of point cloud scene understanding, particularly in indoor scenes, objects are arranged following human habits, resulting in objects of certain semantics being closely positioned and displaying notable inter-object correlations. This can create a tendency for neural networks to exploit these strong dependencies, bypassing the individual object patterns. To address this challenge, we introduce a novel self-supervised learning (SSL) strategy. Our approach leverages both object patterns and contextual cues to produce robust features. It begins with the formulation of an object-exchanging strategy, where pairs of objects with comparable sizes are exchanged across different scenes, effectively disentangling the strong contextual dependencies. Subsequently, we introduce a context-aware feature learning strategy, which encodes object patterns without relying on their specific context by aggregating object features across various scenes. Our extensive experiments demonstrate the superiority of our method over existing SSL techniques, further showing its better robustness to environmental changes. Moreover, we showcase the applicability of our approach by transferring pre-trained models to diverse point cloud datasets.","sentences":["In the realm of point cloud scene understanding, particularly in indoor scenes, objects are arranged following human habits, resulting in objects of certain semantics being closely positioned and displaying notable inter-object correlations.","This can create a tendency for neural networks to exploit these strong dependencies, bypassing the individual object patterns.","To address this challenge, we introduce a novel self-supervised learning (SSL) strategy.","Our approach leverages both object patterns and contextual cues to produce robust features.","It begins with the formulation of an object-exchanging strategy, where pairs of objects with comparable sizes are exchanged across different scenes, effectively disentangling the strong contextual dependencies.","Subsequently, we introduce a context-aware feature learning strategy, which encodes object patterns without relying on their specific context by aggregating object features across various scenes.","Our extensive experiments demonstrate the superiority of our method over existing SSL techniques, further showing its better robustness to environmental changes.","Moreover, we showcase the applicability of our approach by transferring pre-trained models to diverse point cloud datasets."],"url":"http://arxiv.org/abs/2404.07504v1","category":"cs.CV"}
{"created":"2024-04-11 06:33:19","title":"Generating Counterfactual Explanations Using Cardinality Constraints","abstract":"Providing explanations about how machine learning algorithms work and/or make particular predictions is one of the main tools that can be used to improve their trusworthiness, fairness and robustness. Among the most intuitive type of explanations are counterfactuals, which are examples that differ from a given point only in the prediction target and some set of features, presenting which features need to be changed in the original example to flip the prediction for that example. However, such counterfactuals can have many different features than the original example, making their interpretation difficult. In this paper, we propose to explicitly add a cardinality constraint to counterfactual generation limiting how many features can be different from the original example, thus providing more interpretable and easily understantable counterfactuals.","sentences":["Providing explanations about how machine learning algorithms work and/or make particular predictions is one of the main tools that can be used to improve their trusworthiness, fairness and robustness.","Among the most intuitive type of explanations are counterfactuals, which are examples that differ from a given point only in the prediction target and some set of features, presenting which features need to be changed in the original example to flip the prediction for that example.","However, such counterfactuals can have many different features than the original example, making their interpretation difficult.","In this paper, we propose to explicitly add a cardinality constraint to counterfactual generation limiting how many features can be different from the original example, thus providing more interpretable and easily understantable counterfactuals."],"url":"http://arxiv.org/abs/2404.07502v1","category":"cs.LG"}
{"created":"2024-04-11 06:22:56","title":"Interactive Prompt Debugging with Sequence Salience","abstract":"We present Sequence Salience, a visual tool for interactive prompt debugging with input salience methods. Sequence Salience builds on widely used salience methods for text classification and single-token prediction, and extends this to a system tailored for debugging complex LLM prompts. Our system is well-suited for long texts, and expands on previous work by 1) providing controllable aggregation of token-level salience to the word, sentence, or paragraph level, making salience over long inputs tractable; and 2) supporting rapid iteration where practitioners can act on salience results, refine prompts, and run salience on the new output. We include case studies showing how Sequence Salience can help practitioners work with several complex prompting strategies, including few-shot, chain-of-thought, and constitutional principles. Sequence Salience is built on the Learning Interpretability Tool, an open-source platform for ML model visualizations, and code, notebooks, and tutorials are available at http://goo.gle/sequence-salience.","sentences":["We present Sequence Salience, a visual tool for interactive prompt debugging with input salience methods.","Sequence Salience builds on widely used salience methods for text classification and single-token prediction, and extends this to a system tailored for debugging complex LLM prompts.","Our system is well-suited for long texts, and expands on previous work by 1) providing controllable aggregation of token-level salience to the word, sentence, or paragraph level, making salience over long inputs tractable; and 2) supporting rapid iteration where practitioners can act on salience results, refine prompts, and run salience on the new output.","We include case studies showing how Sequence Salience can help practitioners work with several complex prompting strategies, including few-shot, chain-of-thought, and constitutional principles.","Sequence Salience is built on the Learning Interpretability Tool, an open-source platform for ML model visualizations, and code, notebooks, and tutorials are available at http://goo.gle/sequence-salience."],"url":"http://arxiv.org/abs/2404.07498v1","category":"cs.CL"}
{"created":"2024-04-11 06:22:56","title":"Can Large Language Models Assess Serendipity in Recommender Systems?","abstract":"Serendipity-oriented recommender systems aim to counteract over-specialization in user preferences. However, evaluating a user's serendipitous response towards a recommended item can be challenging because of its emotional nature. In this study, we address this issue by leveraging the rich knowledge of large language models (LLMs), which can perform a variety of tasks. First, this study explored the alignment between serendipitous evaluations made by LLMs and those made by humans. In this investigation, a binary classification task was given to the LLMs to predict whether a user would find the recommended item serendipitously. The predictive performances of three LLMs on a benchmark dataset in which humans assigned the ground truth of serendipitous items were measured. The experimental findings reveal that LLM-based assessment methods did not have a very high agreement rate with human assessments. However, they performed as well as or better than the baseline methods. Further validation results indicate that the number of user rating histories provided to LLM prompts should be carefully chosen to avoid both insufficient and excessive inputs and that the output of LLMs that show high classification performance is difficult to interpret.","sentences":["Serendipity-oriented recommender systems aim to counteract over-specialization in user preferences.","However, evaluating a user's serendipitous response towards a recommended item can be challenging because of its emotional nature.","In this study, we address this issue by leveraging the rich knowledge of large language models (LLMs), which can perform a variety of tasks.","First, this study explored the alignment between serendipitous evaluations made by LLMs and those made by humans.","In this investigation, a binary classification task was given to the LLMs to predict whether a user would find the recommended item serendipitously.","The predictive performances of three LLMs on a benchmark dataset in which humans assigned the ground truth of serendipitous items were measured.","The experimental findings reveal that LLM-based assessment methods did not have a very high agreement rate with human assessments.","However, they performed as well as or better than the baseline methods.","Further validation results indicate that the number of user rating histories provided to LLM prompts should be carefully chosen to avoid both insufficient and excessive inputs and that the output of LLMs that show high classification performance is difficult to interpret."],"url":"http://arxiv.org/abs/2404.07499v1","category":"cs.IR"}
{"created":"2024-04-11 06:05:40","title":"Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment","abstract":"Personalized fairness in recommendations has been attracting increasing attention from researchers. The existing works often treat a fairness requirement, represented as a collection of sensitive attributes, as a hyper-parameter, and pursue extreme fairness by completely removing information of sensitive attributes from the learned fair embedding, which suffer from two challenges: huge training cost incurred by the explosion of attribute combinations, and the suboptimal trade-off between fairness and accuracy. In this paper, we propose a novel Adaptive Fair Representation Learning (AFRL) model, which achieves a real personalized fairness due to its advantage of training only one model to adaptively serve different fairness requirements during inference phase. Particularly, AFRL treats fairness requirements as inputs and can learn an attribute-specific embedding for each attribute from the unfair user embedding, which endows AFRL with the adaptability during inference phase to determine the non-sensitive attributes under the guidance of the user's unique fairness requirement. To achieve a better trade-off between fairness and accuracy in recommendations, AFRL conducts a novel Information Alignment to exactly preserve discriminative information of non-sensitive attributes and incorporate a debiased collaborative embedding into the fair embedding to capture attribute-independent collaborative signals, without loss of fairness. Finally, the extensive experiments conducted on real datasets together with the sound theoretical analysis demonstrate the superiority of AFRL.","sentences":["Personalized fairness in recommendations has been attracting increasing attention from researchers.","The existing works often treat a fairness requirement, represented as a collection of sensitive attributes, as a hyper-parameter, and pursue extreme fairness by completely removing information of sensitive attributes from the learned fair embedding, which suffer from two challenges: huge training cost incurred by the explosion of attribute combinations, and the suboptimal trade-off between fairness and accuracy.","In this paper, we propose a novel Adaptive Fair Representation Learning (AFRL) model, which achieves a real personalized fairness due to its advantage of training only one model to adaptively serve different fairness requirements during inference phase.","Particularly, AFRL treats fairness requirements as inputs and can learn an attribute-specific embedding for each attribute from the unfair user embedding, which endows AFRL with the adaptability during inference phase to determine the non-sensitive attributes under the guidance of the user's unique fairness requirement.","To achieve a better trade-off between fairness and accuracy in recommendations, AFRL conducts a novel Information Alignment to exactly preserve discriminative information of non-sensitive attributes and incorporate a debiased collaborative embedding into the fair embedding to capture attribute-independent collaborative signals, without loss of fairness.","Finally, the extensive experiments conducted on real datasets together with the sound theoretical analysis demonstrate the superiority of AFRL."],"url":"http://arxiv.org/abs/2404.07494v1","category":"cs.IR"}
{"created":"2024-04-11 06:04:06","title":"Characterizing the Influence of Topology on Graph Learning Tasks","abstract":"Graph neural networks (GNN) have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations. However, the fundamental problem of understanding and analyzing how graph topology influences the performance of learning models on downstream tasks has not yet been well understood. In this paper, we propose a metric, TopoInf, which characterizes the influence of graph topology by measuring the level of compatibility between the topological information of graph data and downstream task objectives. We provide analysis based on the decoupled GNNs on the contextual stochastic block model to demonstrate the effectiveness of the metric. Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance graph learning.","sentences":["Graph neural networks (GNN) have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations.","However, the fundamental problem of understanding and analyzing how graph topology influences the performance of learning models on downstream tasks has not yet been well understood.","In this paper, we propose a metric, TopoInf, which characterizes the influence of graph topology by measuring the level of compatibility between the topological information of graph data and downstream task objectives.","We provide analysis based on the decoupled GNNs on the contextual stochastic block model to demonstrate the effectiveness of the metric.","Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance graph learning."],"url":"http://arxiv.org/abs/2404.07493v1","category":"cs.LG"}
{"created":"2024-04-11 05:44:27","title":"Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios","abstract":"In the Massive Open Online Courses (MOOC) learning scenario, the semantic information of instructional videos has a crucial impact on learners' emotional state. Learners mainly acquire knowledge by watching instructional videos, and the semantic information in the videos directly affects learners' emotional states. However, few studies have paid attention to the potential influence of the semantic information of instructional videos on learners' emotional states. To deeply explore the impact of video semantic information on learners' emotions, this paper innovatively proposes a multimodal emotion recognition method by fusing video semantic information and physiological signals. We generate video descriptions through a pre-trained large language model (LLM) to obtain high-level semantic information about instructional videos. Using the cross-attention mechanism for modal interaction, the semantic information is fused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain the features containing the critical information of the three modes. The accurate recognition of learners' emotional states is realized through the emotion classifier. The experimental results show that our method has significantly improved emotion recognition performance, providing a new perspective and efficient method for emotion recognition research in MOOC learning scenarios. The method proposed in this paper not only contributes to a deeper understanding of the impact of instructional videos on learners' emotional states but also provides a beneficial reference for future research on emotion recognition in MOOC learning scenarios.","sentences":["In the Massive Open Online Courses (MOOC) learning scenario, the semantic information of instructional videos has a crucial impact on learners' emotional state.","Learners mainly acquire knowledge by watching instructional videos, and the semantic information in the videos directly affects learners' emotional states.","However, few studies have paid attention to the potential influence of the semantic information of instructional videos on learners' emotional states.","To deeply explore the impact of video semantic information on learners' emotions, this paper innovatively proposes a multimodal emotion recognition method by fusing video semantic information and physiological signals.","We generate video descriptions through a pre-trained large language model (LLM) to obtain high-level semantic information about instructional videos.","Using the cross-attention mechanism for modal interaction, the semantic information is fused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain the features containing the critical information of the three modes.","The accurate recognition of learners' emotional states is realized through the emotion classifier.","The experimental results show that our method has significantly improved emotion recognition performance, providing a new perspective and efficient method for emotion recognition research in MOOC learning scenarios.","The method proposed in this paper not only contributes to a deeper understanding of the impact of instructional videos on learners' emotional states but also provides a beneficial reference for future research on emotion recognition in MOOC learning scenarios."],"url":"http://arxiv.org/abs/2404.07484v1","category":"cs.MM"}
{"created":"2024-04-11 05:11:01","title":"Integrated Sensing and Communication Under DISCO Physical-Layer Jamming Attacks","abstract":"Integrated sensing and communication (ISAC) systems traditionally presuppose that sensing and communication (S&C) channels remain approximately constant during their coherence time. However, a \"DISCO\" reconfigurable intelligent surface (DRIS), i.e., an illegitimate RIS with random, time-varying reflection properties that acts like a \"disco ball,\" introduces a paradigm shift that enables active channel aging more rapidly during the channel coherence time. In this letter, we investigate the impact of DISCO jamming attacks launched by a DRISbased fully-passive jammer (FPJ) on an ISAC system. Specifically, an ISAC problem formulation and a corresponding waveform optimization are presented in which the ISAC waveform design considers the trade-off between the S&C performance and is formulated as a Pareto optimization problem. Moreover, a theoretical analysis is conducted to quantify the impact of DISCO jamming attacks. Numerical results are presented to evaluate the S&C performance under DISCO jamming attacks and to validate the derived theoretical analysis.","sentences":["Integrated sensing and communication (ISAC) systems traditionally presuppose that sensing and communication (S&C) channels remain approximately constant during their coherence time.","However, a \"DISCO\" reconfigurable intelligent surface (DRIS), i.e., an illegitimate RIS with random, time-varying reflection properties that acts like a \"disco ball,\" introduces a paradigm shift that enables active channel aging more rapidly during the channel coherence time.","In this letter, we investigate the impact of DISCO jamming attacks launched by a DRISbased fully-passive jammer (FPJ) on an ISAC system.","Specifically, an ISAC problem formulation and a corresponding waveform optimization are presented in which the ISAC waveform design considers the trade-off between the S&C performance and is formulated as a Pareto optimization problem.","Moreover, a theoretical analysis is conducted to quantify the impact of DISCO jamming attacks.","Numerical results are presented to evaluate the S&C performance under DISCO jamming attacks and to validate the derived theoretical analysis."],"url":"http://arxiv.org/abs/2404.07477v1","category":"eess.SP"}
{"created":"2024-04-11 05:09:03","title":"Laissez-Faire Harms: Algorithmic Biases in Generative Language Models","abstract":"The rapid deployment of generative language models (LMs) has raised concerns about social biases affecting the well-being of diverse consumers. The extant literature on generative LMs has primarily examined bias via explicit identity prompting. However, prior research on bias in earlier language-based technology platforms, including search engines, has shown that discrimination can occur even when identity terms are not specified explicitly. Studies of bias in LM responses to open-ended prompts (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms. Here, we advance studies of generative LM bias by considering a broader set of natural use cases via open-ended prompting. In this \"laissez-faire\" setting, we find that synthetically generated texts from five of the most pervasive LMs (ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of omission, subordination, and stereotyping for minoritized individuals with intersectional race, gender, and/or sexual orientation identities (AI/AN, Asian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find widespread evidence of bias to an extent that such individuals are hundreds to thousands of times more likely to encounter LM-generated outputs that portray their identities in a subordinated manner compared to representative or empowering portrayals. We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals. These include stereotype threat, which leads to impaired cognitive performance and increased negative self-perception. Our findings highlight the urgent need to protect consumers from discriminatory harms caused by language models and invest in critical AI education programs tailored towards empowering diverse consumers.","sentences":["The rapid deployment of generative language models (LMs) has raised concerns about social biases affecting the well-being of diverse consumers.","The extant literature on generative LMs has primarily examined bias via explicit identity prompting.","However, prior research on bias in earlier language-based technology platforms, including search engines, has shown that discrimination can occur even when identity terms are not specified explicitly.","Studies of bias in LM responses to open-ended prompts (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms.","Here, we advance studies of generative LM bias by considering a broader set of natural use cases via open-ended prompting.","In this \"laissez-faire\" setting, we find that synthetically generated texts from five of the most pervasive LMs (ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of omission, subordination, and stereotyping for minoritized individuals with intersectional race, gender, and/or sexual orientation identities (AI/AN, Asian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer).","We find widespread evidence of bias to an extent that such individuals are hundreds to thousands of times more likely to encounter LM-generated outputs that portray their identities in a subordinated manner compared to representative or empowering portrayals.","We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals.","These include stereotype threat, which leads to impaired cognitive performance and increased negative self-perception.","Our findings highlight the urgent need to protect consumers from discriminatory harms caused by language models and invest in critical AI education programs tailored towards empowering diverse consumers."],"url":"http://arxiv.org/abs/2404.07475v1","category":"cs.CL"}
{"created":"2024-04-11 04:58:18","title":"G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images","abstract":"Novel view synthesis aims to generate new view images of a given view image collection. Recent attempts address this problem relying on 3D geometry priors (e.g., shapes, sizes, and positions) learned from multi-view images. However, such methods encounter the following limitations: 1) they require a set of multi-view images as training data for a specific scene (e.g., face, car or chair), which is often unavailable in many real-world scenarios; 2) they fail to extract the geometry priors from single-view images due to the lack of multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF (G-NeRF), which seeks to enhance the geometry priors by a geometry-guided multi-view synthesis approach, followed by a depth-aware training. In the synthesis process, inspired that existing 3D GAN models can unconditionally synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D GAN models, such as EG3D, as a free source to provide geometry priors through synthesizing multi-view data. Simultaneously, to further improve the geometry quality of the synthetic data, we introduce a truncation method to effectively sample latent codes within 3D GAN models. To tackle the absence of multi-view supervision for single-view images, we design the depth-aware training approach, incorporating a depth-aware discriminator to guide geometry priors through depth maps. Experiments demonstrate the effectiveness of our method in terms of both qualitative and quantitative results.","sentences":["Novel view synthesis aims to generate new view images of a given view image collection.","Recent attempts address this problem relying on 3D geometry priors (e.g., shapes, sizes, and positions) learned from multi-view images.","However, such methods encounter the following limitations: 1) they require a set of multi-view images as training data for a specific scene (e.g., face, car or chair), which is often unavailable in many real-world scenarios; 2) they fail to extract the geometry priors from single-view images due to the lack of multi-view supervision.","In this paper, we propose a Geometry-enhanced NeRF (G-NeRF), which seeks to enhance the geometry priors by a geometry-guided multi-view synthesis approach, followed by a depth-aware training.","In the synthesis process, inspired that existing 3D GAN models can unconditionally synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D GAN models, such as EG3D, as a free source to provide geometry priors through synthesizing multi-view data.","Simultaneously, to further improve the geometry quality of the synthetic data, we introduce a truncation method to effectively sample latent codes within 3D GAN models.","To tackle the absence of multi-view supervision for single-view images, we design the depth-aware training approach, incorporating a depth-aware discriminator to guide geometry priors through depth maps.","Experiments demonstrate the effectiveness of our method in terms of both qualitative and quantitative results."],"url":"http://arxiv.org/abs/2404.07474v1","category":"cs.CV"}
{"created":"2024-04-11 04:24:48","title":"Structure-aware Fine-tuning for Code Pre-trained Models","abstract":"Over the past few years, we have witnessed remarkable advancements in Code Pre-trained Models (CodePTMs). These models achieved excellent representation capabilities by designing structure-based pre-training tasks for code. However, how to enhance the absorption of structural knowledge when fine-tuning CodePTMs still remains a significant challenge. To fill this gap, in this paper, we present Structure-aware Fine-tuning (SAT), a novel structure-enhanced and plug-and-play fine-tuning method for CodePTMs. We first propose a structure loss to quantify the difference between the information learned by CodePTMs and the knowledge extracted from code structure. Specifically, we use the attention scores extracted from Transformer layer as the learned structural information, and the shortest path length between leaves in abstract syntax trees as the structural knowledge. Subsequently, multi-task learning is introduced to improve the performance of fine-tuning. Experiments conducted on four pre-trained models and two generation tasks demonstrate the effectiveness of our proposed method as a plug-and-play solution. Furthermore, we observed that SAT can benefit CodePTMs more with limited training data.","sentences":["Over the past few years, we have witnessed remarkable advancements in Code Pre-trained Models (CodePTMs).","These models achieved excellent representation capabilities by designing structure-based pre-training tasks for code.","However, how to enhance the absorption of structural knowledge when fine-tuning CodePTMs still remains a significant challenge.","To fill this gap, in this paper, we present Structure-aware Fine-tuning (SAT), a novel structure-enhanced and plug-and-play fine-tuning method for CodePTMs.","We first propose a structure loss to quantify the difference between the information learned by CodePTMs and the knowledge extracted from code structure.","Specifically, we use the attention scores extracted from Transformer layer as the learned structural information, and the shortest path length between leaves in abstract syntax trees as the structural knowledge.","Subsequently, multi-task learning is introduced to improve the performance of fine-tuning.","Experiments conducted on four pre-trained models and two generation tasks demonstrate the effectiveness of our proposed method as a plug-and-play solution.","Furthermore, we observed that SAT can benefit CodePTMs more with limited training data."],"url":"http://arxiv.org/abs/2404.07471v1","category":"cs.SE"}
{"created":"2024-04-11 04:02:20","title":"Leveraging Domain-Unlabeled Data in Offline Reinforcement Learning across Two Domains","abstract":"In this paper, we investigate an offline reinforcement learning (RL) problem where datasets are collected from two domains. In this scenario, having datasets with domain labels facilitates efficient policy training. However, in practice, the task of assigning domain labels can be resource-intensive or infeasible at a large scale, leading to a prevalence of domain-unlabeled data. To formalize this challenge, we introduce a novel offline RL problem setting named Positive-Unlabeled Offline RL (PUORL), which incorporates domain-unlabeled data. To address PUORL, we develop an offline RL algorithm utilizing positive-unlabeled learning to predict the domain labels of domain-unlabeled data, enabling the integration of this data into policy training. Our experiments show the effectiveness of our method in accurately identifying domains and learning policies that outperform baselines in the PUORL setting, highlighting its capability to leverage domain-unlabeled data effectively.","sentences":["In this paper, we investigate an offline reinforcement learning (RL) problem where datasets are collected from two domains.","In this scenario, having datasets with domain labels facilitates efficient policy training.","However, in practice, the task of assigning domain labels can be resource-intensive or infeasible at a large scale, leading to a prevalence of domain-unlabeled data.","To formalize this challenge, we introduce a novel offline RL problem setting named Positive-Unlabeled Offline RL (PUORL), which incorporates domain-unlabeled data.","To address PUORL, we develop an offline RL algorithm utilizing positive-unlabeled learning to predict the domain labels of domain-unlabeled data, enabling the integration of this data into policy training.","Our experiments show the effectiveness of our method in accurately identifying domains and learning policies that outperform baselines in the PUORL setting, highlighting its capability to leverage domain-unlabeled data effectively."],"url":"http://arxiv.org/abs/2404.07465v1","category":"cs.LG"}
{"created":"2024-04-11 03:51:29","title":"\"Confidently Nonsensical?'': A Critical Survey on the Perspectives and Challenges of 'Hallucinations' in NLP","abstract":"We investigate how hallucination in large language models (LLM) is characterized in peer-reviewed literature using a critical examination of 103 publications across NLP research. Through a comprehensive review of sociological and technological literature, we identify a lack of agreement with the term `hallucination.' Additionally, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis underscores the necessity for explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.","sentences":["We investigate how hallucination in large language models (LLM) is characterized in peer-reviewed literature using a critical examination of 103 publications across NLP research.","Through a comprehensive review of sociological and technological literature, we identify a lack of agreement with the term `hallucination.'","Additionally, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination.","Our analysis underscores the necessity for explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society."],"url":"http://arxiv.org/abs/2404.07461v1","category":"cs.CL"}
{"created":"2024-04-11 03:31:54","title":"WESE: Weak Exploration to Strong Exploitation for LLM Agents","abstract":"Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.","sentences":["Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent.","However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation.","When addressing complex tasks within open-world interactive environments, these methods exhibit limitations.","Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions.","On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost.","This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks.","Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge.","A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task.","Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks."],"url":"http://arxiv.org/abs/2404.07456v1","category":"cs.AI"}
{"created":"2024-04-11 03:14:50","title":"RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data","abstract":"The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering (Q$\\&$A), and stock movement prediction (binary classification), with a notable gap in the application of LLMs for financial risk prediction. Addressing this gap, in this paper, we introduce \\textbf{RiskLabs}, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data surrounding ECC release dates. Our approach involves a multi-stage process: initially extracting and analyzing ECC data using LLMs, followed by gathering and processing time-series data before the ECC dates to model and understand risk over different timeframes. Using multimodal fusion techniques, RiskLabs amalgamates these varied data features for comprehensive multi-task financial risk prediction. Empirical experiment results demonstrate RiskLab's effectiveness in forecasting both volatility and variance in financial markets. Through comparative experiments, we demonstrate how different data sources contribute to financial risk assessment and discuss the critical role of LLMs in this context. Our findings not only contribute to the AI in finance application but also open new avenues for applying LLMs in financial risk assessment.","sentences":["The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention.","Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering (Q$\\&$A), and stock movement prediction (binary classification), with a notable gap in the application of LLMs for financial risk prediction.","Addressing this gap, in this paper, we introduce \\textbf{RiskLabs}, a novel framework that leverages LLMs to analyze and predict financial risks.","RiskLabs uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data surrounding ECC release dates.","Our approach involves a multi-stage process: initially extracting and analyzing ECC data using LLMs, followed by gathering and processing time-series data before the ECC dates to model and understand risk over different timeframes.","Using multimodal fusion techniques, RiskLabs amalgamates these varied data features for comprehensive multi-task financial risk prediction.","Empirical experiment results demonstrate RiskLab's effectiveness in forecasting both volatility and variance in financial markets.","Through comparative experiments, we demonstrate how different data sources contribute to financial risk assessment and discuss the critical role of LLMs in this context.","Our findings not only contribute to the AI in finance application but also open new avenues for applying LLMs in financial risk assessment."],"url":"http://arxiv.org/abs/2404.07452v1","category":"q-fin.RM"}
{"created":"2024-04-11 03:02:06","title":"Graph Attention Network for Lane-Wise and Topology-Invariant Intersection Traffic Simulation","abstract":"Traffic congestion has significant economic, environmental, and social ramifications. Intersection traffic flow dynamics are influenced by numerous factors. While microscopic traffic simulators are valuable tools, they are computationally intensive and challenging to calibrate. Moreover, existing machine-learning approaches struggle to provide lane-specific waveforms or adapt to intersection topology and traffic patterns. In this study, we propose two efficient and accurate \"Digital Twin\" models for intersections, leveraging Graph Attention Neural Networks (GAT). These attentional graph auto-encoder digital twins capture temporal, spatial, and contextual aspects of traffic within intersections, incorporating various influential factors such as high-resolution loop detector waveforms, signal state records, driving behaviors, and turning-movement counts. Trained on diverse counterfactual scenarios across multiple intersections, our models generalize well, enabling the estimation of detailed traffic waveforms for any intersection approach and exit lanes. Multi-scale error metrics demonstrate that our models perform comparably to microsimulations. The primary application of our study lies in traffic signal optimization, a pivotal area in transportation systems research. These lightweight digital twins can seamlessly integrate into corridor and network signal timing optimization frameworks. Furthermore, our study's applications extend to lane reconfiguration, driving behavior analysis, and facilitating informed decisions regarding intersection safety and efficiency enhancements. A promising avenue for future research involves extending this approach to urban freeway corridors and integrating it with measures of effectiveness metrics.","sentences":["Traffic congestion has significant economic, environmental, and social ramifications.","Intersection traffic flow dynamics are influenced by numerous factors.","While microscopic traffic simulators are valuable tools, they are computationally intensive and challenging to calibrate.","Moreover, existing machine-learning approaches struggle to provide lane-specific waveforms or adapt to intersection topology and traffic patterns.","In this study, we propose two efficient and accurate \"Digital Twin\" models for intersections, leveraging Graph Attention Neural Networks (GAT).","These attentional graph auto-encoder digital twins capture temporal, spatial, and contextual aspects of traffic within intersections, incorporating various influential factors such as high-resolution loop detector waveforms, signal state records, driving behaviors, and turning-movement counts.","Trained on diverse counterfactual scenarios across multiple intersections, our models generalize well, enabling the estimation of detailed traffic waveforms for any intersection approach and exit lanes.","Multi-scale error metrics demonstrate that our models perform comparably to microsimulations.","The primary application of our study lies in traffic signal optimization, a pivotal area in transportation systems research.","These lightweight digital twins can seamlessly integrate into corridor and network signal timing optimization frameworks.","Furthermore, our study's applications extend to lane reconfiguration, driving behavior analysis, and facilitating informed decisions regarding intersection safety and efficiency enhancements.","A promising avenue for future research involves extending this approach to urban freeway corridors and integrating it with measures of effectiveness metrics."],"url":"http://arxiv.org/abs/2404.07446v1","category":"cs.LG"}
{"created":"2024-04-11 02:59:17","title":"Two-Way Aerial Secure Communications via Distributed Collaborative Beamforming under Eavesdropper Collusion","abstract":"Unmanned aerial vehicles (UAVs)-enabled aerial communication provides a flexible, reliable, and cost-effective solution for a range of wireless applications. However, due to the high line-of-sight (LoS) probability, aerial communications between UAVs are vulnerable to eavesdropping attacks, particularly when multiple eavesdroppers collude. In this work, we aim to introduce distributed collaborative beamforming (DCB) into UAV swarms and handle the eavesdropper collusion by controlling the corresponding signal distributions. Specifically, we consider a two-way DCB-enabled aerial communication between two UAV swarms and construct these swarms as two UAV virtual antenna arrays. Then, we minimize the two-way known secrecy capacity and the maximum sidelobe level to avoid information leakage from the known and unknown eavesdroppers, respectively. Simultaneously, we also minimize the energy consumption of UAVs for constructing virtual antenna arrays. Due to the conflicting relationships between secure performance and energy efficiency, we consider these objectives as a multi-objective optimization problem. Following this, we propose an enhanced multi-objective swarm intelligence algorithm via the characterized properties of the problem. Simulation results show that our proposed algorithm can obtain a set of informative solutions and outperform other state-of-the-art baseline algorithms. Experimental tests demonstrate that our method can be deployed in limited computing power platforms of UAVs and is beneficial for saving computational resources.","sentences":["Unmanned aerial vehicles (UAVs)-enabled aerial communication provides a flexible, reliable, and cost-effective solution for a range of wireless applications.","However, due to the high line-of-sight (LoS) probability, aerial communications between UAVs are vulnerable to eavesdropping attacks, particularly when multiple eavesdroppers collude.","In this work, we aim to introduce distributed collaborative beamforming (DCB) into UAV swarms and handle the eavesdropper collusion by controlling the corresponding signal distributions.","Specifically, we consider a two-way DCB-enabled aerial communication between two UAV swarms and construct these swarms as two UAV virtual antenna arrays.","Then, we minimize the two-way known secrecy capacity and the maximum sidelobe level to avoid information leakage from the known and unknown eavesdroppers, respectively.","Simultaneously, we also minimize the energy consumption of UAVs for constructing virtual antenna arrays.","Due to the conflicting relationships between secure performance and energy efficiency, we consider these objectives as a multi-objective optimization problem.","Following this, we propose an enhanced multi-objective swarm intelligence algorithm via the characterized properties of the problem.","Simulation results show that our proposed algorithm can obtain a set of informative solutions and outperform other state-of-the-art baseline algorithms.","Experimental tests demonstrate that our method can be deployed in limited computing power platforms of UAVs and is beneficial for saving computational resources."],"url":"http://arxiv.org/abs/2404.07444v1","category":"cs.NI"}
{"created":"2024-04-11 02:44:13","title":"Behavior Trees Enable Structured Programming of Language Model Agents","abstract":"Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise \"language-model agents.\" In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.","sentences":["Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision.","However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise \"language-model agents.\"","In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming.","We introduce Dendron, a Python library for programming language model agents using behavior trees.","We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF."],"url":"http://arxiv.org/abs/2404.07439v1","category":"cs.AI"}
{"created":"2024-04-11 02:38:10","title":"Measurement of $e^{+}e^{-}\\to \u03c9\u03b7^{\\prime}$ cross sections at $\\sqrt{s}=$ 2.000 to 3.080 GeV","abstract":"The Born cross sections for the process $e^{+}e^{-}\\to \\omega\\eta^{\\prime}$ are measured at 22 center-of-mass energies from 2.000 to 3.080 GeV using data collected with the BESIII detector at the BEPCII collider. A resonant structure is observed with a statistical significance of 9.6$\\sigma$. A Breit-Wigner fit determines its mass to be $M_R=(2153\\pm30\\pm31)~{\\rm{MeV}}/c^{2}$ and its width to be $\\Gamma_{R}=(167\\pm77\\pm7)~\\rm{MeV}$, where the first uncertainties are statistical and the second are systematic.","sentences":["The Born cross sections for the process $e^{+}e^{-}\\to \\omega\\eta^{\\prime}$ are measured at 22 center-of-mass energies from 2.000 to 3.080 GeV using data collected with the BESIII detector at the BEPCII collider.","A resonant structure is observed with a statistical significance of 9.6$\\sigma$. A Breit-Wigner fit determines its mass to be $M_R=(2153\\pm30\\pm31)~{\\rm{MeV}}/c^{2}$ and its width to be $\\Gamma_{R}=(167\\pm77\\pm7)~\\rm{MeV}$, where the first uncertainties are statistical and the second are systematic."],"url":"http://arxiv.org/abs/2404.07436v1","category":"hep-ex"}
{"created":"2024-04-11 02:23:30","title":"Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert","abstract":"Portfolio management is one of the unresponded problems of the Motion Pictures Industry (MPI). To design an optimal portfolio for an MPI distributor, it is essential to predict the box office of each project. Moreover, for an accurate box office prediction, it is critical to consider the effect of the celebrities involved in each MPI project, which was impossible with any precedent expert-based method. Additionally, the asymmetric characteristic of MPI data decreases the performance of any predictive algorithm. In this paper, firstly, the fame score of the celebrities is determined using a large language model. Then, to tackle the asymmetric character of MPI's data, projects are classified. Furthermore, the box office prediction takes place for each class of projects. Finally, using a hybrid multi-attribute decision-making technique, the preferability of each project for the distributor is calculated, and benefiting from a bi-objective optimization model, the optimal portfolio is designed.","sentences":["Portfolio management is one of the unresponded problems of the Motion Pictures Industry (MPI).","To design an optimal portfolio for an MPI distributor, it is essential to predict the box office of each project.","Moreover, for an accurate box office prediction, it is critical to consider the effect of the celebrities involved in each MPI project, which was impossible with any precedent expert-based method.","Additionally, the asymmetric characteristic of MPI data decreases the performance of any predictive algorithm.","In this paper, firstly, the fame score of the celebrities is determined using a large language model.","Then, to tackle the asymmetric character of MPI's data, projects are classified.","Furthermore, the box office prediction takes place for each class of projects.","Finally, using a hybrid multi-attribute decision-making technique, the preferability of each project for the distributor is calculated, and benefiting from a bi-objective optimization model, the optimal portfolio is designed."],"url":"http://arxiv.org/abs/2404.07434v1","category":"cs.LG"}
{"created":"2024-04-11 01:59:29","title":"AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic Agent","abstract":"Encouraged by the remarkable achievements of language and vision foundation models, developing generalist robotic agents through imitation learning, using large demonstration datasets, has become a prominent area of interest in robot learning. The efficacy of imitation learning is heavily reliant on the quantity and quality of the demonstration datasets. In this study, we aim to scale up demonstrations in a data-efficient way to facilitate the learning of generalist robotic agents. We introduce AdaDemo (Adaptive Online Demonstration Expansion), a general framework designed to improve multi-task policy learning by actively and continually expanding the demonstration dataset. AdaDemo strategically collects new demonstrations to address the identified weakness in the existing policy, ensuring data efficiency is maximized. Through a comprehensive evaluation on a total of 22 tasks across two robotic manipulation benchmarks (RLBench and Adroit), we demonstrate AdaDemo's capability to progressively improve policy performance by guiding the generation of high-quality demonstration datasets in a data-efficient manner.","sentences":["Encouraged by the remarkable achievements of language and vision foundation models, developing generalist robotic agents through imitation learning, using large demonstration datasets, has become a prominent area of interest in robot learning.","The efficacy of imitation learning is heavily reliant on the quantity and quality of the demonstration datasets.","In this study, we aim to scale up demonstrations in a data-efficient way to facilitate the learning of generalist robotic agents.","We introduce AdaDemo (Adaptive Online Demonstration Expansion), a general framework designed to improve multi-task policy learning by actively and continually expanding the demonstration dataset.","AdaDemo strategically collects new demonstrations to address the identified weakness in the existing policy, ensuring data efficiency is maximized.","Through a comprehensive evaluation on a total of 22 tasks across two robotic manipulation benchmarks (RLBench and Adroit), we demonstrate AdaDemo's capability to progressively improve policy performance by guiding the generation of high-quality demonstration datasets in a data-efficient manner."],"url":"http://arxiv.org/abs/2404.07428v1","category":"cs.RO"}
{"created":"2024-04-11 00:52:39","title":"JetMoE: Reaching Llama2 Performance with 0.1M Dollars","abstract":"Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs. The model weights are publicly available at https://github.com/myshell-ai/JetMoE.","sentences":["Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence.","This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model.","These results suggest that LLM training can be much more cost-effective than generally thought.","JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts.","Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code.","All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models.","This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs.","The model weights are publicly available at https://github.com/myshell-ai/JetMoE."],"url":"http://arxiv.org/abs/2404.07413v1","category":"cs.CL"}
{"created":"2024-04-11 00:40:12","title":"SealMates: Supporting Communication in Video Conferencing using a Collective Behavior-Driven Avatar","abstract":"The limited nonverbal cues and spatially distributed nature of remote communication make it challenging for unacquainted members to be expressive during social interactions over video conferencing. Though it enables seeing others' facial expressions, the visual feedback can instead lead to unexpected self-focus, resulting in users missing cues for others to engage in the conversation equally. To support expressive communication and equal participation among unacquainted counterparts, we propose SealMates, a behavior-driven avatar in which the avatar infers the engagement level of the group based on collective gaze and speech patterns and then moves across interlocutors' windows in the video conferencing. By conducting a controlled experiment with 15 groups of triads, we found the avatar's movement encouraged people to experience more self-disclosure and made them perceive everyone was equally engaged in the conversation than when there was no behavior-driven avatar. We discuss how a behavior-driven avatar influences distributed members' perceptions and the implications of avatar-mediated communication for future platforms.","sentences":["The limited nonverbal cues and spatially distributed nature of remote communication make it challenging for unacquainted members to be expressive during social interactions over video conferencing.","Though it enables seeing others' facial expressions, the visual feedback can instead lead to unexpected self-focus, resulting in users missing cues for others to engage in the conversation equally.","To support expressive communication and equal participation among unacquainted counterparts, we propose SealMates, a behavior-driven avatar in which the avatar infers the engagement level of the group based on collective gaze and speech patterns and then moves across interlocutors' windows in the video conferencing.","By conducting a controlled experiment with 15 groups of triads, we found the avatar's movement encouraged people to experience more self-disclosure and made them perceive everyone was equally engaged in the conversation than when there was no behavior-driven avatar.","We discuss how a behavior-driven avatar influences distributed members' perceptions and the implications of avatar-mediated communication for future platforms."],"url":"http://arxiv.org/abs/2404.07403v1","category":"cs.HC"}
{"created":"2024-04-11 00:23:28","title":"Post-hurricane building damage assessment using street-view imagery and structured data: A multi-modal deep learning approach","abstract":"Accurately assessing building damage is critical for disaster response and recovery. However, many existing models for detecting building damage have poor prediction accuracy due to their limited capabilities of identifying detailed, comprehensive structural and/or non-structural damage from the street-view image. Additionally, these models mainly rely on the imagery data for damage classification, failing to account for other critical information, such as wind speed, building characteristics, evacuation zones, and distance of the building to the hurricane track. To address these limitations, in this study, we propose a novel multi-modal (i.e., imagery and structured data) approach for post-hurricane building damage classification, named the Multi-Modal Swin Transformer (MMST). We empirically train and evaluate the proposed MMST using data collected from the 2022 Hurricane Ian in Florida, USA. Results show that MMST outperforms all selected state-of-the-art benchmark models and can achieve an accuracy of 92.67%, which are 7.71% improvement in accuracy compared to Visual Geometry Group 16 (VGG-16). In addition to the street-view imagery data, building value, building age, and wind speed are the most important predictors for damage level classification. The proposed MMST can be deployed to assist in rapid damage assessment and guide reconnaissance efforts in future hurricanes.","sentences":["Accurately assessing building damage is critical for disaster response and recovery.","However, many existing models for detecting building damage have poor prediction accuracy due to their limited capabilities of identifying detailed, comprehensive structural and/or non-structural damage from the street-view image.","Additionally, these models mainly rely on the imagery data for damage classification, failing to account for other critical information, such as wind speed, building characteristics, evacuation zones, and distance of the building to the hurricane track.","To address these limitations, in this study, we propose a novel multi-modal (i.e., imagery and structured data) approach for post-hurricane building damage classification, named the Multi-Modal Swin Transformer (MMST).","We empirically train and evaluate the proposed MMST using data collected from the 2022 Hurricane Ian in Florida, USA.","Results show that MMST outperforms all selected state-of-the-art benchmark models and can achieve an accuracy of 92.67%, which are 7.71% improvement in accuracy compared to Visual Geometry Group 16 (VGG-16).","In addition to the street-view imagery data, building value, building age, and wind speed are the most important predictors for damage level classification.","The proposed MMST can be deployed to assist in rapid damage assessment and guide reconnaissance efforts in future hurricanes."],"url":"http://arxiv.org/abs/2404.07399v1","category":"cs.CV"}
{"created":"2024-04-11 00:03:03","title":"ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past","abstract":"This study investigates whether OpenAI's ChatGPT-3.5 and ChatGPT-4 can accurately forecast future events using two distinct prompting strategies. To evaluate the accuracy of the predictions, we take advantage of the fact that the training data at the time of experiment stopped at September 2021, and ask about events that happened in 2022 using ChatGPT-3.5 and ChatGPT-4. We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters that share events that have happened to them, but after ChatGPT's training data had been collected. Concentrating on events in 2022, we prompted ChatGPT to engage in storytelling, particularly within economic contexts. After analyzing 100 prompts, we discovered that future narrative prompts significantly enhanced ChatGPT-4's forecasting accuracy. This was especially evident in its predictions of major Academy Award winners as well as economic trends, the latter inferred from scenarios where the model impersonated public figures like the Federal Reserve Chair, Jerome Powell. These findings indicate that narrative prompts leverage the models' capacity for hallucinatory narrative construction, facilitating more effective data synthesis and extrapolation than straightforward predictions. Our research reveals new aspects of LLMs' predictive capabilities and suggests potential future applications in analytical contexts.","sentences":["This study investigates whether OpenAI's ChatGPT-3.5 and ChatGPT-4 can accurately forecast future events using two distinct prompting strategies.","To evaluate the accuracy of the predictions, we take advantage of the fact that the training data at the time of experiment stopped at September 2021, and ask about events that happened in 2022 using ChatGPT-3.5 and ChatGPT-4.","We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters that share events that have happened to them, but after ChatGPT's training data had been collected.","Concentrating on events in 2022, we prompted ChatGPT to engage in storytelling, particularly within economic contexts.","After analyzing 100 prompts, we discovered that future narrative prompts significantly enhanced ChatGPT-4's forecasting accuracy.","This was especially evident in its predictions of major Academy Award winners as well as economic trends, the latter inferred from scenarios where the model impersonated public figures like the Federal Reserve Chair, Jerome Powell.","These findings indicate that narrative prompts leverage the models' capacity for hallucinatory narrative construction, facilitating more effective data synthesis and extrapolation than straightforward predictions.","Our research reveals new aspects of LLMs' predictive capabilities and suggests potential future applications in analytical contexts."],"url":"http://arxiv.org/abs/2404.07396v1","category":"econ.GN"}
{"created":"2024-04-10 23:28:09","title":"BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks","abstract":"Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs). However, they encounter difficulties in understanding and working with code produced by LLMs. To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user prompts and code generation. We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code. Through 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas. We discuss the implications of our findings for UI-centric interactive paradigm in code generation LLMs.","sentences":["Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs).","However, they encounter difficulties in understanding and working with code produced by LLMs.","To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user prompts and code generation.","We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code.","Through 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas.","We discuss the implications of our findings for UI-centric interactive paradigm in code generation LLMs."],"url":"http://arxiv.org/abs/2404.07387v1","category":"cs.HC"}
{"created":"2024-04-10 23:02:13","title":"Incorporating Explanations into Human-Machine Interfaces for Trust and Situation Awareness in Autonomous Vehicles","abstract":"Autonomous vehicles often make complex decisions via machine learning-based predictive models applied to collected sensor data. While this combination of methods provides a foundation for real-time actions, self-driving behavior primarily remains opaque to end users. In this sense, explainability of real-time decisions is a crucial and natural requirement for building trust in autonomous vehicles. Moreover, as autonomous vehicles still cause serious traffic accidents for various reasons, timely conveyance of upcoming hazards to road users can help improve scene understanding and prevent potential risks. Hence, there is also a need to supply autonomous vehicles with user-friendly interfaces for effective human-machine teaming. Motivated by this problem, we study the role of explainable AI and human-machine interface jointly in building trust in vehicle autonomy. We first present a broad context of the explanatory human-machine systems with the \"3W1H\" (what, whom, when, how) approach. Based on these findings, we present a situation awareness framework for calibrating users' trust in self-driving behavior. Finally, we perform an experiment on our framework, conduct a user study on it, and validate the empirical findings with hypothesis testing.","sentences":["Autonomous vehicles often make complex decisions via machine learning-based predictive models applied to collected sensor data.","While this combination of methods provides a foundation for real-time actions, self-driving behavior primarily remains opaque to end users.","In this sense, explainability of real-time decisions is a crucial and natural requirement for building trust in autonomous vehicles.","Moreover, as autonomous vehicles still cause serious traffic accidents for various reasons, timely conveyance of upcoming hazards to road users can help improve scene understanding and prevent potential risks.","Hence, there is also a need to supply autonomous vehicles with user-friendly interfaces for effective human-machine teaming.","Motivated by this problem, we study the role of explainable AI and human-machine interface jointly in building trust in vehicle autonomy.","We first present a broad context of the explanatory human-machine systems with the \"3W1H\" (what, whom, when, how) approach.","Based on these findings, we present a situation awareness framework for calibrating users' trust in self-driving behavior.","Finally, we perform an experiment on our framework, conduct a user study on it, and validate the empirical findings with hypothesis testing."],"url":"http://arxiv.org/abs/2404.07383v1","category":"cs.RO"}
{"created":"2024-04-10 23:01:45","title":"Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving","abstract":"Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.","sentences":["Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states.","The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts.","Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials.","In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths.","Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs.","We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches."],"url":"http://arxiv.org/abs/2404.07382v1","category":"cs.AI"}
{"created":"2024-04-10 22:35:06","title":"Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI","abstract":"Building on the remarkable achievements in generative sampling of natural images, we propose an innovative challenge, potentially overly ambitious, which involves generating samples of entire multivariate time series that resemble images. However, the statistical challenge lies in the small sample size, sometimes consisting of a few hundred subjects. This issue is especially problematic for deep generative models that follow the conventional approach of generating samples from a canonical distribution and then decoding or denoising them to match the true data distribution. In contrast, our method is grounded in information theory and aims to implicitly characterize the distribution of images, particularly the (global and local) dependency structure between pixels. We achieve this by empirically estimating its KL-divergence in the dual form with respect to the respective marginal distribution. This enables us to perform generative sampling directly in the optimized 1-D dual divergence space. Specifically, in the dual space, training samples representing the data distribution are embedded in the form of various clusters between two end points. In theory, any sample embedded between those two end points is in-distribution w.r.t. the data distribution. Our key idea for generating novel samples of images is to interpolate between the clusters via a walk as per gradients of the dual function w.r.t. the data dimensions. In addition to the data efficiency gained from direct sampling, we propose an algorithm that offers a significant reduction in sample complexity for estimating the divergence of the data distribution with respect to the marginal distribution. We provide strong theoretical guarantees along with an extensive empirical evaluation using many real-world datasets from diverse domains, establishing the superiority of our approach w.r.t. state-of-the-art deep learning methods.","sentences":["Building on the remarkable achievements in generative sampling of natural images, we propose an innovative challenge, potentially overly ambitious, which involves generating samples of entire multivariate time series that resemble images.","However, the statistical challenge lies in the small sample size, sometimes consisting of a few hundred subjects.","This issue is especially problematic for deep generative models that follow the conventional approach of generating samples from a canonical distribution and then decoding or denoising them to match the true data distribution.","In contrast, our method is grounded in information theory and aims to implicitly characterize the distribution of images, particularly the (global and local) dependency structure between pixels.","We achieve this by empirically estimating its KL-divergence in the dual form with respect to the respective marginal distribution.","This enables us to perform generative sampling directly in the optimized 1-D dual divergence space.","Specifically, in the dual space, training samples representing the data distribution are embedded in the form of various clusters between two end points.","In theory, any sample embedded between those two end points is in-distribution w.r.t.","the data distribution.","Our key idea for generating novel samples of images is to interpolate between the clusters via a walk as per gradients of the dual function w.r.t.","the data dimensions.","In addition to the data efficiency gained from direct sampling, we propose an algorithm that offers a significant reduction in sample complexity for estimating the divergence of the data distribution with respect to the marginal distribution.","We provide strong theoretical guarantees along with an extensive empirical evaluation using many real-world datasets from diverse domains, establishing the superiority of our approach w.r.t.","state-of-the-art deep learning methods."],"url":"http://arxiv.org/abs/2404.07377v1","category":"cs.LG"}
{"created":"2024-04-10 21:43:27","title":"Differentially Private GANs for Generating Synthetic Indoor Location Data","abstract":"The advent of location-based services has led to the widespread adoption of indoor localization systems, which enable location tracking of individuals within enclosed spaces such as buildings. While these systems provide numerous benefits such as improved security and personalized services, they also raise concerns regarding privacy violations. As such, there is a growing need for privacy-preserving solutions that can protect users' sensitive location information while still enabling the functionality of indoor localization systems. In recent years, Differentially Private Generative Adversarial Networks (DPGANs) have emerged as a powerful methodology that aims to protect the privacy of individual data points while generating realistic synthetic data similar to original data. DPGANs combine the power of generative adversarial networks (GANs) with the privacy-preserving technique of differential privacy (DP). In this paper, we introduce an indoor localization framework employing DPGANs in order to generate privacy-preserving indoor location data. We evaluate the performance of our framework on a real-world indoor localization dataset and demonstrate its effectiveness in preserving privacy while maintaining the accuracy of the localization system.","sentences":["The advent of location-based services has led to the widespread adoption of indoor localization systems, which enable location tracking of individuals within enclosed spaces such as buildings.","While these systems provide numerous benefits such as improved security and personalized services, they also raise concerns regarding privacy violations.","As such, there is a growing need for privacy-preserving solutions that can protect users' sensitive location information while still enabling the functionality of indoor localization systems.","In recent years, Differentially Private Generative Adversarial Networks (DPGANs) have emerged as a powerful methodology that aims to protect the privacy of individual data points while generating realistic synthetic data similar to original data.","DPGANs combine the power of generative adversarial networks (GANs) with the privacy-preserving technique of differential privacy (DP).","In this paper, we introduce an indoor localization framework employing DPGANs in order to generate privacy-preserving indoor location data.","We evaluate the performance of our framework on a real-world indoor localization dataset and demonstrate its effectiveness in preserving privacy while maintaining the accuracy of the localization system."],"url":"http://arxiv.org/abs/2404.07366v1","category":"cs.CR"}
{"created":"2024-04-10 21:23:13","title":"GANsemble for Small and Imbalanced Data Sets: A Baseline for Synthetic Microplastics Data","abstract":"Microplastic particle ingestion or inhalation by humans is a problem of growing concern. Unfortunately, current research methods that use machine learning to understand their potential harms are obstructed by a lack of available data. Deep learning techniques in particular are challenged by such domains where only small or imbalanced data sets are available. Overcoming this challenge often involves oversampling underrepresented classes or augmenting the existing data to improve model performance. This paper proposes GANsemble: a two-module framework connecting data augmentation with conditional generative adversarial networks (cGANs) to generate class-conditioned synthetic data. First, the data chooser module automates augmentation strategy selection by searching for the best data augmentation strategy. Next, the cGAN module uses this strategy to train a cGAN for generating enhanced synthetic data. We experiment with the GANsemble framework on a small and imbalanced microplastics data set. A Microplastic-cGAN (MPcGAN) algorithm is introduced, and baselines for synthetic microplastics (SYMP) data are established in terms of Frechet Inception Distance (FID) and Inception Scores (IS). We also provide a synthetic microplastics filter (SYMP-Filter) algorithm to increase the quality of generated SYMP. Additionally, we show the best amount of oversampling with augmentation to fix class imbalance in small microplastics data sets. To our knowledge, this study is the first application of generative AI to synthetically create microplastics data.","sentences":["Microplastic particle ingestion or inhalation by humans is a problem of growing concern.","Unfortunately, current research methods that use machine learning to understand their potential harms are obstructed by a lack of available data.","Deep learning techniques in particular are challenged by such domains where only small or imbalanced data sets are available.","Overcoming this challenge often involves oversampling underrepresented classes or augmenting the existing data to improve model performance.","This paper proposes GANsemble: a two-module framework connecting data augmentation with conditional generative adversarial networks (cGANs) to generate class-conditioned synthetic data.","First, the data chooser module automates augmentation strategy selection by searching for the best data augmentation strategy.","Next, the cGAN module uses this strategy to train a cGAN for generating enhanced synthetic data.","We experiment with the GANsemble framework on a small and imbalanced microplastics data set.","A Microplastic-cGAN (MPcGAN) algorithm is introduced, and baselines for synthetic microplastics (SYMP) data are established in terms of Frechet Inception Distance (FID) and Inception Scores (IS).","We also provide a synthetic microplastics filter (SYMP-Filter) algorithm to increase the quality of generated SYMP.","Additionally, we show the best amount of oversampling with augmentation to fix class imbalance in small microplastics data sets.","To our knowledge, this study is the first application of generative AI to synthetically create microplastics data."],"url":"http://arxiv.org/abs/2404.07356v1","category":"cs.LG"}
{"created":"2024-04-10 21:16:59","title":"Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation","abstract":"This work presents code to procedurally generate examples for the ARC training tasks. For each of the 400 tasks, an example generator following the transformation logic of the original examples was created. In effect, the assumed underlying distribution of examples for any given task was reverse engineered by implementing a means to sample from it. An attempt was made to cover an as large as reasonable space of possible examples for each task. That is, whenever the original examples of a given task may be limited in their diversity e.g. by having the dimensions of the grids, the set of symbols or number of objects constant or within tight bounds, even though the transformation does not require it, such constraints were lifted. Having access to not just a few examples per task, as the case for ARC, but instead very many, should enable a wide range of experiments that may be important stepping stones towards making leaps on the benchmark.","sentences":["This work presents code to procedurally generate examples for the ARC training tasks.","For each of the 400 tasks, an example generator following the transformation logic of the original examples was created.","In effect, the assumed underlying distribution of examples for any given task was reverse engineered by implementing a means to sample from it.","An attempt was made to cover an as large as reasonable space of possible examples for each task.","That is, whenever the original examples of a given task may be limited in their diversity e.g. by having the dimensions of the grids, the set of symbols or number of objects constant or within tight bounds, even though the transformation does not require it, such constraints were lifted.","Having access to not just a few examples per task, as the case for ARC, but instead very many, should enable a wide range of experiments that may be important stepping stones towards making leaps on the benchmark."],"url":"http://arxiv.org/abs/2404.07353v1","category":"cs.LG"}
{"created":"2024-04-10 21:03:23","title":"Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention","abstract":"Humans utilize their gaze to concentrate on essential information while perceiving and interpreting intentions in videos. Incorporating human gaze into computational algorithms can significantly enhance model performance in video understanding tasks. In this work, we address a challenging and innovative task in video understanding: predicting the actions of an agent in a video based on a partial video. We introduce the Gaze-guided Action Anticipation algorithm, which establishes a visual-semantic graph from the video input. Our method utilizes a Graph Neural Network to recognize the agent's intention and predict the action sequence to fulfill this intention. To assess the efficiency of our approach, we collect a dataset containing household activities generated in the VirtualHome environment, accompanied by human gaze data of viewing videos. Our method outperforms state-of-the-art techniques, achieving a 7\\% improvement in accuracy for 18-class intention recognition. This highlights the efficiency of our method in learning important features from human gaze data.","sentences":["Humans utilize their gaze to concentrate on essential information while perceiving and interpreting intentions in videos.","Incorporating human gaze into computational algorithms can significantly enhance model performance in video understanding tasks.","In this work, we address a challenging and innovative task in video understanding: predicting the actions of an agent in a video based on a partial video.","We introduce the Gaze-guided Action Anticipation algorithm, which establishes a visual-semantic graph from the video input.","Our method utilizes a Graph Neural Network to recognize the agent's intention and predict the action sequence to fulfill this intention.","To assess the efficiency of our approach, we collect a dataset containing household activities generated in the VirtualHome environment, accompanied by human gaze data of viewing videos.","Our method outperforms state-of-the-art techniques, achieving a 7\\% improvement in accuracy for 18-class intention recognition.","This highlights the efficiency of our method in learning important features from human gaze data."],"url":"http://arxiv.org/abs/2404.07347v1","category":"cs.CV"}
{"created":"2024-04-10 20:59:59","title":"Interactive Learning of Physical Object Properties Through Robot Manipulation and Database of Object Measurements","abstract":"This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements. The framework involves exploratory action selection to maximize learning about objects on a table. A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions. The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference. Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned. The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance. The robot pipeline integrates with a logging module and an online database of objects, containing over 24,000 measurements of 63 objects with different grippers. All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations.","sentences":["This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements.","The framework involves exploratory action selection to maximize learning about objects on a table.","A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions.","The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference.","Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned.","The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance.","The robot pipeline integrates with a logging module and an online database of objects, containing over 24,000 measurements of 63 objects with different grippers.","All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations."],"url":"http://arxiv.org/abs/2404.07344v1","category":"cs.RO"}
{"created":"2024-04-10 20:39:24","title":"RIP Twitter API: A eulogy to its vast research contributions","abstract":"Since 2006, Twitter's Application Programming Interface (API) has been a treasure trove of high-quality data for researchers studying everything from the spread of misinformation, to social psychology and emergency management. However, in the spring of 2023, Twitter (now called X) began changing $42,000/month for its Enterprise access level, an essential death knell for researcher use. Lacking sufficient funds to pay this monthly fee, academics are now scrambling to continue their research without this important data source. This study collects and tabulates the number of studies, number of citations, dates, major disciplines, and major topic areas of studies that used Twitter data between 2006 and 2023. While we cannot know for certain what will be lost now that Twitter data is cost prohibitive, we can illustrate its research value during the time it was available. A search of 8 databases and 3 related APIs found that since 2006, a total of 27,453 studies have been published in 7,432 publication venues, with 1,303,142 citations, across 14 disciplines. Major disciplines include: computational social science, engineering, data science, social media studies, public health, and medicine. Major topics include: information dissemination, assessing the credibility of tweets, strategies for conducting data research, detecting and analyzing major events, and studying human behavior. Twitter data studies have increased every year since 2006, but following Twitter's decision to begin charging for data in the spring of 2023, the number of studies published in 2023 decreased by 13% compared to 2022. We assume that much of the data used for studies published in 2023 were collected prior to Twitter's shutdown, and thus the number of new studies are likely to decline further in subsequent years.","sentences":["Since 2006, Twitter's Application Programming Interface (API) has been a treasure trove of high-quality data for researchers studying everything from the spread of misinformation, to social psychology and emergency management.","However, in the spring of 2023, Twitter (now called X) began changing $42,000/month for its Enterprise access level, an essential death knell for researcher use.","Lacking sufficient funds to pay this monthly fee, academics are now scrambling to continue their research without this important data source.","This study collects and tabulates the number of studies, number of citations, dates, major disciplines, and major topic areas of studies that used Twitter data between 2006 and 2023.","While we cannot know for certain what will be lost now that Twitter data is cost prohibitive, we can illustrate its research value during the time it was available.","A search of 8 databases and 3 related APIs found that since 2006, a total of 27,453 studies have been published in 7,432 publication venues, with 1,303,142 citations, across 14 disciplines.","Major disciplines include: computational social science, engineering, data science, social media studies, public health, and medicine.","Major topics include: information dissemination, assessing the credibility of tweets, strategies for conducting data research, detecting and analyzing major events, and studying human behavior.","Twitter data studies have increased every year since 2006, but following Twitter's decision to begin charging for data in the spring of 2023, the number of studies published in 2023 decreased by 13% compared to 2022.","We assume that much of the data used for studies published in 2023 were collected prior to Twitter's shutdown, and thus the number of new studies are likely to decline further in subsequent years."],"url":"http://arxiv.org/abs/2404.07340v1","category":"cs.CY"}
{"created":"2024-04-10 20:10:58","title":"Assessing Engraftment Following Fecal Microbiota Transplant","abstract":"Fecal Microbiota Transplant (FMT) is an FDA approved treatment for recurrent Clostridium difficile infections, and is being explored for other clinical applications, from alleviating digestive and neurological disorders, to priming the microbiome for cancer treatment, and restoring microbiomes impacted by cancer treatment.   Quantifying the extent of engraftment following an FMT is important in determining if a recipient didn't respond because the engrafted microbiome didn't produce the desired outcomes (a successful FMT, but negative treatment outcome), or the microbiome didn't engraft (an unsuccessful FMT and negative treatment outcome). The lack of a consistent methodology for quantifying FMT engraftment extent hinders the assessment of FMT success and its relation to clinical outcomes, and presents challenges for comparing FMT results and protocols across studies.   Here we review 46 studies of FMT in humans and model organisms and group their approaches for assessing the extent to which an FMT engrafts into three criteria: 1) Chimeric Asymmetric Community Coalescence investigates microbiome shifts following FMT engraftment. 2) Donated Microbiome Indicator Features tracks donated microbiome features as a signal of engraftment with methods such as differential abundance testing based on the current sample collection, or tracking changes in feature abundances that have been previously identified. 3) Temporal Stability examines how resistant post-FMT recipient's microbiomes are to reverting back to their baseline microbiome. Investigated together, these criteria provide a clear assessment of microbiome engraftment.   We discuss the pros and cons of each of these criteria, providing illustrative examples of their application. We also introduce key terminology and recommendations on how FMT studies can be analyzed for rigorous engraftment extent assessment.","sentences":["Fecal Microbiota Transplant (FMT) is an FDA approved treatment for recurrent Clostridium difficile infections, and is being explored for other clinical applications, from alleviating digestive and neurological disorders, to priming the microbiome for cancer treatment, and restoring microbiomes impacted by cancer treatment.   ","Quantifying the extent of engraftment following an FMT is important in determining if a recipient didn't respond because the engrafted microbiome didn't produce the desired outcomes (a successful FMT, but negative treatment outcome), or the microbiome didn't engraft (an unsuccessful FMT and negative treatment outcome).","The lack of a consistent methodology for quantifying FMT engraftment extent hinders the assessment of FMT success and its relation to clinical outcomes, and presents challenges for comparing FMT results and protocols across studies.   ","Here we review 46 studies of FMT in humans and model organisms and group their approaches for assessing the extent to which an FMT engrafts into three criteria: 1) Chimeric Asymmetric Community Coalescence investigates microbiome shifts following FMT engraftment.","2) Donated Microbiome Indicator Features tracks donated microbiome features as a signal of engraftment with methods such as differential abundance testing based on the current sample collection, or tracking changes in feature abundances that have been previously identified.","3) Temporal Stability examines how resistant post-FMT recipient's microbiomes are to reverting back to their baseline microbiome.","Investigated together, these criteria provide a clear assessment of microbiome engraftment.   ","We discuss the pros and cons of each of these criteria, providing illustrative examples of their application.","We also introduce key terminology and recommendations on how FMT studies can be analyzed for rigorous engraftment extent assessment."],"url":"http://arxiv.org/abs/2404.07325v1","category":"q-bio.QM"}
{"created":"2024-04-10 19:25:51","title":"Structured Reinforcement Learning for Media Streaming at the Wireless Edge","abstract":"Media streaming is the dominant application over wireless edge (access) networks. The increasing softwarization of such networks has led to efforts at intelligent control, wherein application-specific actions may be dynamically taken to enhance the user experience. The goal of this work is to develop and demonstrate learning-based policies for optimal decision making to determine which clients to dynamically prioritize in a video streaming setting. We formulate the policy design question as a constrained Markov decision problem (CMDP), and observe that by using a Lagrangian relaxation we can decompose it into single-client problems. Further, the optimal policy takes a threshold form in the video buffer length, which enables us to design an efficient constrained reinforcement learning (CRL) algorithm to learn it. Specifically, we show that a natural policy gradient (NPG) based algorithm that is derived using the structure of our problem converges to the globally optimal policy. We then develop a simulation environment for training, and a real-world intelligent controller attached to a WiFi access point for evaluation. We empirically show that the structured learning approach enables fast learning. Furthermore, such a structured policy can be easily deployed due to low computational complexity, leading to policy execution taking only about 15$\\mu$s. Using YouTube streaming experiments in a resource constrained scenario, we demonstrate that the CRL approach can increase QoE by over 30%.","sentences":["Media streaming is the dominant application over wireless edge (access) networks.","The increasing softwarization of such networks has led to efforts at intelligent control, wherein application-specific actions may be dynamically taken to enhance the user experience.","The goal of this work is to develop and demonstrate learning-based policies for optimal decision making to determine which clients to dynamically prioritize in a video streaming setting.","We formulate the policy design question as a constrained Markov decision problem (CMDP), and observe that by using a Lagrangian relaxation we can decompose it into single-client problems.","Further, the optimal policy takes a threshold form in the video buffer length, which enables us to design an efficient constrained reinforcement learning (CRL) algorithm to learn it.","Specifically, we show that a natural policy gradient (NPG) based algorithm that is derived using the structure of our problem converges to the globally optimal policy.","We then develop a simulation environment for training, and a real-world intelligent controller attached to a WiFi access point for evaluation.","We empirically show that the structured learning approach enables fast learning.","Furthermore, such a structured policy can be easily deployed due to low computational complexity, leading to policy execution taking only about 15$\\mu$s.","Using YouTube streaming experiments in a resource constrained scenario, we demonstrate that the CRL approach can increase QoE by over 30%."],"url":"http://arxiv.org/abs/2404.07315v1","category":"eess.SY"}
{"created":"2024-04-10 18:58:05","title":"AI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth","abstract":"From a process development perspective, diamond growth via chemical vapor deposition has made significant strides. However, challenges persist in achieving high quality and large-area material production. These difficulties include controlling conditions to maintain uniform growth rates for the entire growth surface. As growth progresses, various factors or defect states emerge, altering the uniform conditions. These changes affect the growth rate and result in the formation of crystalline defects at the microscale. However, there is a distinct lack of methods to identify these defect states and their geometry using images taken during the growth process. This paper details seminal work on defect segmentation pipeline using in-situ optical images to identify features that indicate defective states that are visible at the macroscale. Using a semantic segmentation approach as applied in our previous work, these defect states and corresponding derivative features are isolated and classified by their pixel masks. Using an annotation focused human-in-the-loop software architecture to produce training datasets, with modules for selective data labeling using active learning, data augmentations, and model-assisted labeling, our approach achieves effective annotation accuracy and drastically reduces the time and cost of labeling by orders of magnitude. On the model development front, we found that deep learning-based algorithms are the most efficient. They can accurately learn complex representations from feature-rich datasets. Our best-performing model, based on the YOLOV3 and DeeplabV3plus architectures, achieved excellent accuracy for specific features of interest. Specifically, it reached 93.35% accuracy for center defects, 92.83% for polycrystalline defects, and 91.98% for edge defects.","sentences":["From a process development perspective, diamond growth via chemical vapor deposition has made significant strides.","However, challenges persist in achieving high quality and large-area material production.","These difficulties include controlling conditions to maintain uniform growth rates for the entire growth surface.","As growth progresses, various factors or defect states emerge, altering the uniform conditions.","These changes affect the growth rate and result in the formation of crystalline defects at the microscale.","However, there is a distinct lack of methods to identify these defect states and their geometry using images taken during the growth process.","This paper details seminal work on defect segmentation pipeline using in-situ optical images to identify features that indicate defective states that are visible at the macroscale.","Using a semantic segmentation approach as applied in our previous work, these defect states and corresponding derivative features are isolated and classified by their pixel masks.","Using an annotation focused human-in-the-loop software architecture to produce training datasets, with modules for selective data labeling using active learning, data augmentations, and model-assisted labeling, our approach achieves effective annotation accuracy and drastically reduces the time and cost of labeling by orders of magnitude.","On the model development front, we found that deep learning-based algorithms are the most efficient.","They can accurately learn complex representations from feature-rich datasets.","Our best-performing model, based on the YOLOV3 and DeeplabV3plus architectures, achieved excellent accuracy for specific features of interest.","Specifically, it reached 93.35% accuracy for center defects, 92.83% for polycrystalline defects, and 91.98% for edge defects."],"url":"http://arxiv.org/abs/2404.07306v1","category":"cs.CV"}
{"created":"2024-04-10 18:52:31","title":"Altruism Improves Congestion in Series-Parallel Nonatomic Congestion Games","abstract":"Self-interested routing polices from individual users in a system can collectively lead to poor aggregate congestion in routing networks. The introduction of altruistic agents, whose goal is to benefit other agents in the system, can seemingly improve aggregate congestion. However, it is known in that in some network routing problems, altruistic agents can actually worsen congestion compared to that which would arise in the presence of a homogeneously selfish population. This paper provides a thorough investigation into the necessary conditions for altruists to be guaranteed to improve total congestion. In particular, we study the class of series-parallel non-atomic congestion games, where one sub-population is altruistic and the other is selfish. We find that a game is guaranteed to have improved congestion in the presence of altruistic agents (even if only a small part of the total population) compared to the homogeneously selfish version of the game, provided the network is symmetric, where all agents are given access to all paths in the network, and the series-parallel network for the game does not have sub-networks which emulate Braess's paradox -- a phenomenon we refer to as a Braess-resistant network. Our results appear to be the most complete characterization of when behavior that is designed to improve total congestion (which we refer to as altruism) is actually guaranteed to do so.","sentences":["Self-interested routing polices from individual users in a system can collectively lead to poor aggregate congestion in routing networks.","The introduction of altruistic agents, whose goal is to benefit other agents in the system, can seemingly improve aggregate congestion.","However, it is known in that in some network routing problems, altruistic agents can actually worsen congestion compared to that which would arise in the presence of a homogeneously selfish population.","This paper provides a thorough investigation into the necessary conditions for altruists to be guaranteed to improve total congestion.","In particular, we study the class of series-parallel non-atomic congestion games, where one sub-population is altruistic and the other is selfish.","We find that a game is guaranteed to have improved congestion in the presence of altruistic agents (even if only a small part of the total population) compared to the homogeneously selfish version of the game, provided the network is symmetric, where all agents are given access to all paths in the network, and the series-parallel network for the game does not have sub-networks which emulate Braess's paradox -- a phenomenon we refer to as a Braess-resistant network.","Our results appear to be the most complete characterization of when behavior that is designed to improve total congestion (which we refer to as altruism) is actually guaranteed to do so."],"url":"http://arxiv.org/abs/2404.07302v1","category":"cs.GT"}
{"created":"2024-04-10 18:03:44","title":"Combinatorics of higher-categorical diagrams","abstract":"This is a book on higher-categorical diagrams, including pasting diagrams. It aims to provide a thorough and modern reference on the subject, collecting, revisiting and expanding results scattered across the literature, informed by recent advances and practical experience with higher-dimensional diagram rewriting.   We approach the subject as a kind of directed combinatorial topology: a diagram is a map from a \"directed cell complex\", encoded combinatorially as a face poset together with orientation data. Unlike previous expositions, we adopt from the beginning a functorial viewpoint, focussing on morphisms and categorical constructions. We do not tie ourselves to a specific model of higher categories, and instead treat diagrams as independent combinatorial structures that admit functorial interpretations in various contexts.   Topics covered include the theory of layerings of diagrams; acyclicity properties and their consequences; constructions including Gray products, suspensions, and joins; special shapes such as globes, oriented simplices, cubes, and positive opetopes; the interpretation of diagrams in strict omega-categories and their geometric realisation as simplicial and CW complexes; and Steiner's theory of directed chain complexes.","sentences":["This is a book on higher-categorical diagrams, including pasting diagrams.","It aims to provide a thorough and modern reference on the subject, collecting, revisiting and expanding results scattered across the literature, informed by recent advances and practical experience with higher-dimensional diagram rewriting.   ","We approach the subject as a kind of directed combinatorial topology: a diagram is a map from a \"directed cell complex\", encoded combinatorially as a face poset together with orientation data.","Unlike previous expositions, we adopt from the beginning a functorial viewpoint, focussing on morphisms and categorical constructions.","We do not tie ourselves to a specific model of higher categories, and instead treat diagrams as independent combinatorial structures that admit functorial interpretations in various contexts.   ","Topics covered include the theory of layerings of diagrams; acyclicity properties and their consequences; constructions including Gray products, suspensions, and joins; special shapes such as globes, oriented simplices, cubes, and positive opetopes; the interpretation of diagrams in strict omega-categories and their geometric realisation as simplicial and CW complexes; and Steiner's theory of directed chain complexes."],"url":"http://arxiv.org/abs/2404.07273v1","category":"math.CT"}
{"created":"2024-04-10 11:56:01","title":"MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models","abstract":"Hallucinations in large language models (LLMs) have recently become a significant problem. A recent effort in this direction is a shared task at Semeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. This paper describes our winning solution ranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks respectively. We propose a meta-regressor framework of LLMs for model evaluation and integration that achieves the highest scores on the leaderboard. We also experiment with various transformer-based models and black box methods like ChatGPT, Vectara, and others. In addition, we perform an error analysis comparing GPT4 against our best model which shows the limitations of the former.","sentences":["Hallucinations in large language models (LLMs) have recently become a significant problem.","A recent effort in this direction is a shared task at Semeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes.","This paper describes our winning solution ranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks respectively.","We propose a meta-regressor framework of LLMs for model evaluation and integration that achieves the highest scores on the leaderboard.","We also experiment with various transformer-based models and black box methods like ChatGPT, Vectara, and others.","In addition, we perform an error analysis comparing GPT4 against our best model which shows the limitations of the former."],"url":"http://arxiv.org/abs/2404.06948v2","category":"cs.CL"}
{"created":"2024-04-10 07:46:30","title":"Generative Resident Separation and Multi-label Classification for Multi-person Activity Recognition","abstract":"This paper presents two models to address the problem of multi-person activity recognition using ambient sensors in a home. The first model, Seq2Res, uses a sequence generation approach to separate sensor events from different residents. The second model, BiGRU+Q2L, uses a Query2Label multi-label classifier to predict multiple activities simultaneously. Performances of these models are compared to a state-of-the-art model in different experimental scenarios, using a state-of-the-art dataset of two residents in a home instrumented with ambient sensors. These results lead to a discussion on the advantages and drawbacks of resident separation and multi-label classification for multi-person activity recognition.","sentences":["This paper presents two models to address the problem of multi-person activity recognition using ambient sensors in a home.","The first model, Seq2Res, uses a sequence generation approach to separate sensor events from different residents.","The second model, BiGRU+Q2L, uses a Query2Label multi-label classifier to predict multiple activities simultaneously.","Performances of these models are compared to a state-of-the-art model in different experimental scenarios, using a state-of-the-art dataset of two residents in a home instrumented with ambient sensors.","These results lead to a discussion on the advantages and drawbacks of resident separation and multi-label classification for multi-person activity recognition."],"url":"http://arxiv.org/abs/2404.07245v1","category":"cs.LG"}
{"created":"2024-04-11 17:59:40","title":"QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer","abstract":"We explore the dexterous manipulation transfer problem by designing simulators. The task wishes to transfer human manipulations to dexterous robot hand simulations and is inherently difficult due to its intricate, highly-constrained, and discontinuous dynamics and the need to control a dexterous hand with a DoF to accurately replicate human manipulations. Previous approaches that optimize in high-fidelity black-box simulators or a modified one with relaxed constraints only demonstrate limited capabilities or are restricted by insufficient simulation fidelity. We introduce parameterized quasi-physical simulators and a physics curriculum to overcome these limitations. The key ideas are 1) balancing between fidelity and optimizability of the simulation via a curriculum of parameterized simulators, and 2) solving the problem in each of the simulators from the curriculum, with properties ranging from high task optimizability to high fidelity. We successfully enable a dexterous hand to track complex and diverse manipulations in high-fidelity simulated environments, boosting the success rate by 11\\%+ from the best-performed baseline. The project website is available at https://meowuu7.github.io/QuasiSim/.","sentences":["We explore the dexterous manipulation transfer problem by designing simulators.","The task wishes to transfer human manipulations to dexterous robot hand simulations and is inherently difficult due to its intricate, highly-constrained, and discontinuous dynamics and the need to control a dexterous hand with a DoF to accurately replicate human manipulations.","Previous approaches that optimize in high-fidelity black-box simulators or a modified one with relaxed constraints only demonstrate limited capabilities or are restricted by insufficient simulation fidelity.","We introduce parameterized quasi-physical simulators and a physics curriculum to overcome these limitations.","The key ideas are 1) balancing between fidelity and optimizability of the simulation via a curriculum of parameterized simulators, and 2) solving the problem in each of the simulators from the curriculum, with properties ranging from high task optimizability to high fidelity.","We successfully enable a dexterous hand to track complex and diverse manipulations in high-fidelity simulated environments, boosting the success rate by 11\\%+ from the best-performed baseline.","The project website is available at https://meowuu7.github.io/QuasiSim/."],"url":"http://arxiv.org/abs/2404.07988v1","category":"cs.RO"}
{"created":"2024-04-11 17:59:07","title":"Trading Determinism for Noncommutativity in Edmonds' Problem","abstract":"Let $X=X_1\\sqcup X_2\\sqcup\\ldots\\sqcup X_k$ be a partitioned set of variables such that the variables in each part $X_i$ are noncommuting but for any $i\\neq j$, the variables $x\\in X_i$ commute with the variables $x'\\in X_j$. Given as input a square matrix $T$ whose entries are linear forms over $\\mathbb{Q}\\langle{X}\\rangle$, we consider the problem of checking if $T$ is invertible or not over the universal skew field of fractions of the partially commutative polynomial ring $\\mathbb{Q}\\langle{X}\\rangle$ [Klep-Vinnikov-Volcic (2020)]. In this paper, we design a deterministic polynomial-time algorithm for this problem for constant $k$. The special case $k=1$ is the noncommutative Edmonds' problem (NSINGULAR) which has a deterministic polynomial-time algorithm by recent results [Garg-Gurvits-Oliveira-Wigderson (2016), Ivanyos-Qiao-Subrahmanyam (2018), Hamada-Hirai (2021)].   En-route, we obtain the first deterministic polynomial-time algorithm for the equivalence testing problem of $k$-tape \\emph{weighted} automata (for constant $k$) resolving a long-standing open problem [Harju and Karhum\"{a}ki(1991), Worrell (2013)]. Algebraically, the equivalence problem reduces to testing whether a partially commutative rational series over the partitioned set $X$ is zero or not [Worrell (2013)]. Decidability of this problem was established by Harju and Karhum\\\"{a}ki (1991). Prior to this work, a \\emph{randomized} polynomial-time algorithm for this problem was given by Worrell (2013) and, subsequently, a deterministic quasipolynomial-time algorithm was also developed [Arvind et al. (2021)].","sentences":["Let $X=X_1\\sqcup X_2\\sqcup\\ldots\\sqcup X_k$ be a partitioned set of variables such that the variables in each part $X_i$ are noncommuting but for any $i\\neq j$, the variables $x\\in X_i$ commute with the variables $x'\\in X_j$. Given as input a square matrix $T$ whose entries are linear forms over $\\mathbb{Q}\\langle{X}\\rangle$, we consider the problem of checking if $T$ is invertible or not over the universal skew field of fractions of the partially commutative polynomial ring $\\mathbb{Q}\\langle{X}\\rangle$","[Klep-Vinnikov-Volcic (2020)].","In this paper, we design a deterministic polynomial-time algorithm for this problem for constant $k$. The special case $k=1$ is the noncommutative Edmonds' problem (NSINGULAR) which has a deterministic polynomial-time algorithm by recent results","[Garg-Gurvits-Oliveira-Wigderson (2016), Ivanyos-Qiao-Subrahmanyam (2018), Hamada-Hirai (2021)].   ","En-route, we obtain the first deterministic polynomial-time algorithm for the equivalence testing problem of $k$-tape \\emph{weighted} automata (for constant $k$) resolving a long-standing open problem [Harju and Karhum\"{a}ki(1991), Worrell (2013)].","Algebraically, the equivalence problem reduces to testing whether a partially commutative rational series over the partitioned set $X$ is zero or not","[Worrell (2013)].","Decidability of this problem was established by Harju and Karhum\\\"{a}ki (1991).","Prior to this work, a \\emph{randomized} polynomial-time algorithm for this problem was given by Worrell (2013) and, subsequently, a deterministic quasipolynomial-time algorithm was also developed","[Arvind et al. (2021)]."],"url":"http://arxiv.org/abs/2404.07986v1","category":"cs.CC"}
{"created":"2024-04-11 17:56:21","title":"Spectral Multifractality and Emergent Energyscales Across the Many-Body Localisation Transition","abstract":"We present a scaling theory of the many-body localisation transition in terms of emergent, characteristic energyscales. The analysis is based on the decomposition of the eigenstates in the basis of trivially localised states, resolved in the energies of the latter, which we refer to as the spectral decomposition of the eigenstates. The characteristic energyscales emerge when the multifractal properties, or lack thereof, of the spectral decomposition are studied at different scales. These characteristic scales correspond to the ones, above which the spectral decompositions exhibit their global behaviour, namely full ergodicity in the ergodic phase and multifractality in the many-body localised phase. On the other hand, at scales below the characteristic ones, the decomposition in the ergodic phase shows finer (multi)fractal structures whereas in the localised phase, the decomposition picks out well-separated, localised resonant peaks. The scaling of these characteristic energyscales across the many-body localisation transition admits a scaling theory consistent with a Kosterlitz-Thouless type scenario and bears striking resemblances to that of inverse participation ratios of eigenstates.","sentences":["We present a scaling theory of the many-body localisation transition in terms of emergent, characteristic energyscales.","The analysis is based on the decomposition of the eigenstates in the basis of trivially localised states, resolved in the energies of the latter, which we refer to as the spectral decomposition of the eigenstates.","The characteristic energyscales emerge when the multifractal properties, or lack thereof, of the spectral decomposition are studied at different scales.","These characteristic scales correspond to the ones, above which the spectral decompositions exhibit their global behaviour, namely full ergodicity in the ergodic phase and multifractality in the many-body localised phase.","On the other hand, at scales below the characteristic ones, the decomposition in the ergodic phase shows finer (multi)fractal structures whereas in the localised phase, the decomposition picks out well-separated, localised resonant peaks.","The scaling of these characteristic energyscales across the many-body localisation transition admits a scaling theory consistent with a Kosterlitz-Thouless type scenario and bears striking resemblances to that of inverse participation ratios of eigenstates."],"url":"http://arxiv.org/abs/2404.07975v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-11 17:55:05","title":"Differentiable All-pole Filters for Time-varying Audio Systems","abstract":"Infinite impulse response filters are an essential building block of many time-varying audio systems, such as audio effects and synthesisers. However, their recursive structure impedes end-to-end training of these systems using automatic differentiation. Although non-recursive filter approximations like frequency sampling and frame-based processing have been proposed and widely used in previous works, they cannot accurately reflect the gradient of the original system. We alleviate this difficulty by re-expressing a time-varying all-pole filter to backpropagate the gradients through itself, so the filter implementation is not bound to the technical limitations of automatic differentiation frameworks. This implementation can be employed within any audio system containing filters with poles for efficient gradient evaluation. We demonstrate its training efficiency and expressive capabilities for modelling real-world dynamic audio systems on a phaser, time-varying subtractive synthesiser, and feed-forward compressor. We make our code available and provide the trained audio effect and synth models in a VST plugin at https://christhetree.github.io/all_pole_filters/.","sentences":["Infinite impulse response filters are an essential building block of many time-varying audio systems, such as audio effects and synthesisers.","However, their recursive structure impedes end-to-end training of these systems using automatic differentiation.","Although non-recursive filter approximations like frequency sampling and frame-based processing have been proposed and widely used in previous works, they cannot accurately reflect the gradient of the original system.","We alleviate this difficulty by re-expressing a time-varying all-pole filter to backpropagate the gradients through itself, so the filter implementation is not bound to the technical limitations of automatic differentiation frameworks.","This implementation can be employed within any audio system containing filters with poles for efficient gradient evaluation.","We demonstrate its training efficiency and expressive capabilities for modelling real-world dynamic audio systems on a phaser, time-varying subtractive synthesiser, and feed-forward compressor.","We make our code available and provide the trained audio effect and synth models in a VST plugin at https://christhetree.github.io/all_pole_filters/."],"url":"http://arxiv.org/abs/2404.07970v1","category":"eess.AS"}
{"created":"2024-04-11 17:53:20","title":"Spin-Energy Entanglement of a Time-Focused Neutron","abstract":"Intra-particle entanglement of individual particles such as neutrons could enable a new class of scattering probes that are sensitive to entanglement in quantum systems and materials. In this work, we present experimental results demonstrating quantum contextuality as a result of entanglement between the spin and energy modes (i.e., degrees of freedom) of single neutrons in a beam using a pair of resonant radio-frequency neutron spin flippers in the MIEZE configuration (Modulated IntEnsity with Zero Effort). We verified the mode-entanglement by measuring a Clauser-Horne-Shimony-Holt (CHSH) contextuality witness $S$ defined in the spin and energy subsystems, observing a clear breach of the classical bound of $|S| \\leq 2$, obtaining $S = 2.40 \\pm 0.02$. These entangled beams could enable novel approaches for directly probing dynamics and entanglement in quantum materials whose low-energy excitation scales match those of the incident entangled neutron.","sentences":["Intra-particle entanglement of individual particles such as neutrons could enable a new class of scattering probes that are sensitive to entanglement in quantum systems and materials.","In this work, we present experimental results demonstrating quantum contextuality as a result of entanglement between the spin and energy modes (i.e., degrees of freedom) of single neutrons in a beam using a pair of resonant radio-frequency neutron spin flippers in the MIEZE configuration (Modulated IntEnsity with Zero Effort).","We verified the mode-entanglement by measuring a Clauser-Horne-Shimony-Holt (CHSH) contextuality witness $S$ defined in the spin and energy subsystems, observing a clear breach of the classical bound of $|S| \\leq 2$, obtaining $S = 2.40 \\pm 0.02$. These entangled beams could enable novel approaches for directly probing dynamics and entanglement in quantum materials whose low-energy excitation scales match those of the incident entangled neutron."],"url":"http://arxiv.org/abs/2404.07967v1","category":"quant-ph"}
{"created":"2024-04-11 17:50:42","title":"Overcoming the chemical complexity bottleneck in on-the-fly machine learned molecular dynamics simulations","abstract":"We develop a framework for on-the-fly machine learned force field molecular dynamics simulations based on the multipole featurization scheme that overcomes the bottleneck with the number of chemical elements. Considering bulk systems with up to 6 elements, we demonstrate that the number of density functional theory calls remains approximately independent of the number of chemical elements, in contrast to the increase in the smooth overlap of atomic positions scheme.","sentences":["We develop a framework for on-the-fly machine learned force field molecular dynamics simulations based on the multipole featurization scheme that overcomes the bottleneck with the number of chemical elements.","Considering bulk systems with up to 6 elements, we demonstrate that the number of density functional theory calls remains approximately independent of the number of chemical elements, in contrast to the increase in the smooth overlap of atomic positions scheme."],"url":"http://arxiv.org/abs/2404.07961v1","category":"physics.comp-ph"}
{"created":"2024-04-11 17:09:28","title":"LaVy: Vietnamese Multimodal Large Language Model","abstract":"Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. All code and model weights are public at https://github.com/baochi0212/LaVy","sentences":["Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension.","Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs.","In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks.","All code and model weights are public at https://github.com/baochi0212/LaVy"],"url":"http://arxiv.org/abs/2404.07922v1","category":"cs.CL"}
{"created":"2024-04-11 17:02:58","title":"Controlling measurement induced phase transitions with tunable detector coupling","abstract":"We study the evolution of a quantum many-body system driven by two competing measurements, which induces a topological entanglement transition between two distinct area law phases. We employ a positive operator-valued measurement with variable coupling between the system and detector within free Fermion dynamics. This approach allows us to continuously track the universal properties of the transition between projective and continuous monitoring. Our findings suggest that the percolation universality of the transition in the projective limit is unstable when the system-detector coupling is reduced.","sentences":["We study the evolution of a quantum many-body system driven by two competing measurements, which induces a topological entanglement transition between two distinct area law phases.","We employ a positive operator-valued measurement with variable coupling between the system and detector within free Fermion dynamics.","This approach allows us to continuously track the universal properties of the transition between projective and continuous monitoring.","Our findings suggest that the percolation universality of the transition in the projective limit is unstable when the system-detector coupling is reduced."],"url":"http://arxiv.org/abs/2404.07918v1","category":"quant-ph"}
{"created":"2024-04-11 16:59:03","title":"Dynamical Reorientation of Spin Multipoles in Silicon Carbide by Transverse Magnetic Fields","abstract":"The long-lived and optically addressable high-spin state of the negatively charged silicon vacancy ($\\mathrm{V_{Si}}$) in silicon carbide makes it a promising system for applications in quantum technologies. Most studies of its spin dynamics have been performed in external magnetic fields applied along the symmetry axis. Here, we find that the application of weak magnetic fields perpendicular to the symmetry axis leads to nontrivial behavior caused by dynamical reorientation of the $\\mathrm{V_{Si}}$ spin multipole under optical excitation. Particularly, we observe the inversion of the quadrupole spin polarization in the excited state and appearance of the dipole spin polarization in the ground state. The latter is much higher than thermal polarization and cannot be induced solely by optical excitation. Our theoretical calculations reproduce well all sharp features in the spin resonance spectra, and shine light on the complex dynamics of spin multipoles in these kinds of solid-state systems.","sentences":["The long-lived and optically addressable high-spin state of the negatively charged silicon vacancy ($\\mathrm{V_{Si}}$) in silicon carbide makes it a promising system for applications in quantum technologies.","Most studies of its spin dynamics have been performed in external magnetic fields applied along the symmetry axis.","Here, we find that the application of weak magnetic fields perpendicular to the symmetry axis leads to nontrivial behavior caused by dynamical reorientation of the $\\mathrm{V_{Si}}$ spin multipole under optical excitation.","Particularly, we observe the inversion of the quadrupole spin polarization in the excited state and appearance of the dipole spin polarization in the ground state.","The latter is much higher than thermal polarization and cannot be induced solely by optical excitation.","Our theoretical calculations reproduce well all sharp features in the spin resonance spectra, and shine light on the complex dynamics of spin multipoles in these kinds of solid-state systems."],"url":"http://arxiv.org/abs/2404.07915v1","category":"quant-ph"}
{"created":"2024-04-11 16:57:30","title":"Optimal Control for Linear Systems with $L^1$-norm Cost","abstract":"We study $L^1$-optimal stabilization of linear systems with finite and infinite horizons. Main results concern the existence, uniqueness and structure of optimal solutions, and the robustness of optimal cost.","sentences":["We study $L^1$-optimal stabilization of linear systems with finite and infinite horizons.","Main results concern the existence, uniqueness and structure of optimal solutions, and the robustness of optimal cost."],"url":"http://arxiv.org/abs/2404.07913v1","category":"math.OC"}
{"created":"2024-04-11 16:56:12","title":"High-performance matrix-free unfitted finite element operator evaluation","abstract":"Unfitted finite element methods, like CutFEM, have traditionally been implemented in a matrix-based fashion, where a sparse matrix is assembled and later applied to vectors while solving the resulting linear system. With the goal of increasing performance and enabling algorithms with polynomial spaces of higher degrees, this contribution chooses a more abstract approach by matrix-free evaluation of the operator action on vectors instead. The proposed method loops over cells and locally evaluates the cell, face, and interface integrals, including the contributions from cut cells and the different means of stabilization. The main challenge is the efficient numerical evaluation of terms in the weak form with unstructured quadrature points arising from the unfitted discretization in cells cut by the interface. We present design choices and performance optimizations for tensor-product elements and demonstrate the performance by means of benchmarks and application examples. We demonstrate a speedup of more than one order of magnitude for the operator evaluation of a discontinuous Galerkin discretization with polynomial degree three compared to a sparse matrix-vector product and develop performance models to quantify the performance properties over a wide range of polynomial degrees.","sentences":["Unfitted finite element methods, like CutFEM, have traditionally been implemented in a matrix-based fashion, where a sparse matrix is assembled and later applied to vectors while solving the resulting linear system.","With the goal of increasing performance and enabling algorithms with polynomial spaces of higher degrees, this contribution chooses a more abstract approach by matrix-free evaluation of the operator action on vectors instead.","The proposed method loops over cells and locally evaluates the cell, face, and interface integrals, including the contributions from cut cells and the different means of stabilization.","The main challenge is the efficient numerical evaluation of terms in the weak form with unstructured quadrature points arising from the unfitted discretization in cells cut by the interface.","We present design choices and performance optimizations for tensor-product elements and demonstrate the performance by means of benchmarks and application examples.","We demonstrate a speedup of more than one order of magnitude for the operator evaluation of a discontinuous Galerkin discretization with polynomial degree three compared to a sparse matrix-vector product and develop performance models to quantify the performance properties over a wide range of polynomial degrees."],"url":"http://arxiv.org/abs/2404.07911v1","category":"math.NA"}
{"created":"2024-04-11 16:49:48","title":"On orthogonality to uniquely ergodic systems","abstract":"We solve Boshernitzan's problem of characterization (in terms of so called Furstenberg systems) of bounded sequences that are orthogonal to all uniquely ergodic systems. Some variations of Boshernitzan's problem involving characteristic classes are considered. As an application, we characterize sequences orthogonal to all uniquely ergodic systems whose (unique) invariant measure yields a discrete spectrum automorphism as those satisfying an averaged Chowla property.","sentences":["We solve Boshernitzan's problem of characterization (in terms of so called Furstenberg systems) of bounded sequences that are orthogonal to all uniquely ergodic systems.","Some variations of Boshernitzan's problem involving characteristic classes are considered.","As an application, we characterize sequences orthogonal to all uniquely ergodic systems whose (unique) invariant measure yields a discrete spectrum automorphism as those satisfying an averaged Chowla property."],"url":"http://arxiv.org/abs/2404.07907v1","category":"math.DS"}
{"created":"2024-04-11 16:40:24","title":"Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games","abstract":"Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.","sentences":["Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play.","However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players.","As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals.","To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element.","To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants.","Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both.","Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming."],"url":"http://arxiv.org/abs/2404.07901v1","category":"cs.HC"}
{"created":"2024-04-11 16:32:02","title":"On the addition of a large scalar multiplet to the Standard Model","abstract":"We consider the addition of a single $SU(2)$ multiplet of complex scalar fields to the Standard Model (SM). We explicitly consider the various possible values of the weak isospin $J$ of that multiplet, up to and including $J = 7/2$. We allow the multiplet to have arbitrary weak hypercharge. The scalar fields of the multiplet are assumed to have no vacuum expectation value; the mass differences among the components of the multiplet originate in its coupling, present in the scalar potential (SP), to the Higgs doublet of the SM. We derive exact bounded-from-below and unitarity conditions on the SP, thereby constraining those mass differences. We compare those constraints to the ones that may be derived from the oblique parameters.","sentences":["We consider the addition of a single $SU(2)$ multiplet of complex scalar fields to the Standard Model (SM).","We explicitly consider the various possible values of the weak isospin $J$ of that multiplet, up to and including $J = 7/2$. We allow the multiplet to have arbitrary weak hypercharge.","The scalar fields of the multiplet are assumed to have no vacuum expectation value; the mass differences among the components of the multiplet originate in its coupling, present in the scalar potential (SP), to the Higgs doublet of the SM.","We derive exact bounded-from-below and unitarity conditions on the SP, thereby constraining those mass differences.","We compare those constraints to the ones that may be derived from the oblique parameters."],"url":"http://arxiv.org/abs/2404.07897v1","category":"hep-ph"}
{"created":"2024-04-11 16:31:35","title":"Auditing health-related recommendations in social media: A Case Study of Abortion on YouTube","abstract":"Recommendation algorithms (RS) used by social media, like YouTube, significantly shape our information consumption across various domains, especially in healthcare. Hence, algorithmic auditing becomes crucial to uncover their potential bias and misinformation, particularly in the context of controversial topics like abortion. We introduce a simple yet effective sock puppet auditing approach to investigate how YouTube recommends abortion-related videos to individuals with different backgrounds. Our framework allows for efficient auditing of RS, regardless of the complexity of the underlying algorithms","sentences":["Recommendation algorithms (RS) used by social media, like YouTube, significantly shape our information consumption across various domains, especially in healthcare.","Hence, algorithmic auditing becomes crucial to uncover their potential bias and misinformation, particularly in the context of controversial topics like abortion.","We introduce a simple yet effective sock puppet auditing approach to investigate how YouTube recommends abortion-related videos to individuals with different backgrounds.","Our framework allows for efficient auditing of RS, regardless of the complexity of the underlying algorithms"],"url":"http://arxiv.org/abs/2404.07896v1","category":"cs.SI"}
{"created":"2024-04-11 16:10:52","title":"Multi-Robot Target Tracking with Sensing and Communication Danger Zones","abstract":"Multi-robot target tracking finds extensive applications in different scenarios, such as environmental surveillance and wildfire management, which require the robustness of the practical deployment of multi-robot systems in uncertain and dangerous environments. Traditional approaches often focus on the performance of tracking accuracy with no modeling and assumption of the environments, neglecting potential environmental hazards which result in system failures in real-world deployments. To address this challenge, we investigate multi-robot target tracking in the adversarial environment considering sensing and communication attacks with uncertainty. We design specific strategies to avoid different danger zones and proposed a multi-agent tracking framework under the perilous environment. We approximate the probabilistic constraints and formulate practical optimization strategies to address computational challenges efficiently. We evaluate the performance of our proposed methods in simulations to demonstrate the ability of robots to adjust their risk-aware behaviors under different levels of environmental uncertainty and risk confidence. The proposed method is further validated via real-world robot experiments where a team of drones successfully track dynamic ground robots while being risk-aware of the sensing and/or communication danger zones.","sentences":["Multi-robot target tracking finds extensive applications in different scenarios, such as environmental surveillance and wildfire management, which require the robustness of the practical deployment of multi-robot systems in uncertain and dangerous environments.","Traditional approaches often focus on the performance of tracking accuracy with no modeling and assumption of the environments, neglecting potential environmental hazards which result in system failures in real-world deployments.","To address this challenge, we investigate multi-robot target tracking in the adversarial environment considering sensing and communication attacks with uncertainty.","We design specific strategies to avoid different danger zones and proposed a multi-agent tracking framework under the perilous environment.","We approximate the probabilistic constraints and formulate practical optimization strategies to address computational challenges efficiently.","We evaluate the performance of our proposed methods in simulations to demonstrate the ability of robots to adjust their risk-aware behaviors under different levels of environmental uncertainty and risk confidence.","The proposed method is further validated via real-world robot experiments where a team of drones successfully track dynamic ground robots while being risk-aware of the sensing and/or communication danger zones."],"url":"http://arxiv.org/abs/2404.07880v1","category":"cs.RO"}
{"created":"2024-04-11 16:10:13","title":"Joint transitivity for linear iterates","abstract":"We establish sufficient and necessary conditions for the joint transitivity of linear iterates in a minimal topological dynamical system with commuting transformations. This result provides the first topological analogue of the classical Berend and Bergelson joint ergodicity criterion in measure-preserving systems.","sentences":["We establish sufficient and necessary conditions for the joint transitivity of linear iterates in a minimal topological dynamical system with commuting transformations.","This result provides the first topological analogue of the classical Berend and Bergelson joint ergodicity criterion in measure-preserving systems."],"url":"http://arxiv.org/abs/2404.07876v1","category":"math.DS"}
{"created":"2024-04-11 16:05:33","title":"Chiro-Optical Structures with Magnetizable Plasmonic Elements for Modulating the Chiral Transmission of Light","abstract":"In this study, we introduce various chiro-optical structures comprising single, dimer, or trimer arrangements of plasmonic nanoantennas capable of actively modulating the chiral transmission of light through the application of an external magnetic field. Specifically, we explore monomers, dimers, and trimers of trapezoidal nanoantennas, as well as trimer configurations of triangular nanoantennas, with Au nanoparticles serving as the plasmonic elements. To enable active magnetic control over the chiro-optical response in these chiral plasmonic systems, Ni is incorporated as the active magneto-plasmonic element. Our findings not only confirm the chirality of all proposed structures but also demonstrate the feasibility of magnetically manipulating the chiro-optical response within these configurations. This study provides valuable insights into the field of chiral magneto-plasmonic nanophotonic systems.","sentences":["In this study, we introduce various chiro-optical structures comprising single, dimer, or trimer arrangements of plasmonic nanoantennas capable of actively modulating the chiral transmission of light through the application of an external magnetic field.","Specifically, we explore monomers, dimers, and trimers of trapezoidal nanoantennas, as well as trimer configurations of triangular nanoantennas, with Au nanoparticles serving as the plasmonic elements.","To enable active magnetic control over the chiro-optical response in these chiral plasmonic systems, Ni is incorporated as the active magneto-plasmonic element.","Our findings not only confirm the chirality of all proposed structures but also demonstrate the feasibility of magnetically manipulating the chiro-optical response within these configurations.","This study provides valuable insights into the field of chiral magneto-plasmonic nanophotonic systems."],"url":"http://arxiv.org/abs/2404.07871v1","category":"physics.optics"}
{"created":"2024-04-11 15:54:20","title":"Streaming detection of significant delay changes in public transport systems","abstract":"Public transport systems are expected to reduce pollution and contribute to sustainable development. However, disruptions in public transport such as delays may negatively affect mobility choices. To quantify delays, aggregated data from vehicle locations systems are frequently used. However, delays observed at individual stops are caused inter alia by fluctuations in running times and propagation of delays occurring in other locations. Hence, in this work, we propose both the method detecting significant delays and reference architecture, relying on stream processing engines, in which the method is implemented. The method can complement the calculation of delays defined as deviation from schedules. This provides both online rather than batch identification of significant and repetitive delays, and resilience to the limited quality of location data. The method we propose can be used with different change detectors, such as ADWIN, applied to location data stream shuffled to individual edges of a transport graph. It can detect in an online manner at which edges statistically significant delays are observed and at which edges delays arise and are reduced. Detections can be used to model mobility choices and quantify the impact of repetitive rather than random disruptions on feasible trips with multimodal trip modelling engines. The evaluation performed with the public transport data of over 2000 vehicles confirms the merits of the method and reveals that a limited-size subgraph of a transport system graph causes statistically significant delays","sentences":["Public transport systems are expected to reduce pollution and contribute to sustainable development.","However, disruptions in public transport such as delays may negatively affect mobility choices.","To quantify delays, aggregated data from vehicle locations systems are frequently used.","However, delays observed at individual stops are caused inter alia by fluctuations in running times and propagation of delays occurring in other locations.","Hence, in this work, we propose both the method detecting significant delays and reference architecture, relying on stream processing engines, in which the method is implemented.","The method can complement the calculation of delays defined as deviation from schedules.","This provides both online rather than batch identification of significant and repetitive delays, and resilience to the limited quality of location data.","The method we propose can be used with different change detectors, such as ADWIN, applied to location data stream shuffled to individual edges of a transport graph.","It can detect in an online manner at which edges statistically significant delays are observed and at which edges delays arise and are reduced.","Detections can be used to model mobility choices and quantify the impact of repetitive rather than random disruptions on feasible trips with multimodal trip modelling engines.","The evaluation performed with the public transport data of over 2000 vehicles confirms the merits of the method and reveals that a limited-size subgraph of a transport system graph causes statistically significant delays"],"url":"http://arxiv.org/abs/2404.07860v1","category":"cs.LG"}
{"created":"2024-04-11 15:53:03","title":"Active Carpets in floating viscous films","abstract":"Earth's aquatic environments are inherently stratified layered systems where interfaces between layers serve as ecological niches for microbial swimmers, forming colonies known as Active Carpet (AC). Previous theoretical studies have explored the hydrodynamic fluctuations exerted by ACs in semi-infinite fluid media, demonstrating their capability to enhance thermal diffusion and mass transport in aquatic systems. Yet, little is understood about the fluid dynamics and impact of ACs residing in confined layered environments, like slicks floating on water bodies. In this study, we report novel solutions for the hydrodynamic fluctuations induced by ACs geometrically confined between a free surface and a fluid-fluid interface characterized by a jump in fluid viscosity. Combining theory and numerical experiments, we investigate the topology of the biogenic hydrodynamic fluctuations in a confined, thin fluid environment. We reveal that within this thin layer, ACs gives shape to three characteristic regions: Region I is the closest zone to the AC and the fluid-fluid interface, where hydrodynamic fluctuations are dominantly vertical; Region II is further up from the AC and is characterized by isotropic hydrodynamic fluctuations; Region III is the furthest region, near the free surface and is dominated by horizontal flow fluctuations. We demonstrate that the extent of these regions depends strongly on the degree of confinement, i.e. the layer thickness and the strength of the viscosity jump. Lastly, we show that confinement fosters the emergence of large-scale flow structures within the layer housing the ACs--not previously reported. Our findings shed light on the complex interplay between confinement and hydrodynamics in floating viscous film biological systems, providing valuable insights with implications spanning from ecological conservation to bio-inspired engineering.","sentences":["Earth's aquatic environments are inherently stratified layered systems where interfaces between layers serve as ecological niches for microbial swimmers, forming colonies known as Active Carpet (AC).","Previous theoretical studies have explored the hydrodynamic fluctuations exerted by ACs in semi-infinite fluid media, demonstrating their capability to enhance thermal diffusion and mass transport in aquatic systems.","Yet, little is understood about the fluid dynamics and impact of ACs residing in confined layered environments, like slicks floating on water bodies.","In this study, we report novel solutions for the hydrodynamic fluctuations induced by ACs geometrically confined between a free surface and a fluid-fluid interface characterized by a jump in fluid viscosity.","Combining theory and numerical experiments, we investigate the topology of the biogenic hydrodynamic fluctuations in a confined, thin fluid environment.","We reveal that within this thin layer, ACs gives shape to three characteristic regions: Region I is the closest zone to the AC and the fluid-fluid interface, where hydrodynamic fluctuations are dominantly vertical; Region II is further up from the AC and is characterized by isotropic hydrodynamic fluctuations; Region III is the furthest region, near the free surface and is dominated by horizontal flow fluctuations.","We demonstrate that the extent of these regions depends strongly on the degree of confinement, i.e. the layer thickness and the strength of the viscosity jump.","Lastly, we show that confinement fosters the emergence of large-scale flow structures within the layer housing the ACs--not previously reported.","Our findings shed light on the complex interplay between confinement and hydrodynamics in floating viscous film biological systems, providing valuable insights with implications spanning from ecological conservation to bio-inspired engineering."],"url":"http://arxiv.org/abs/2404.07856v1","category":"physics.flu-dyn"}
{"created":"2024-04-11 15:47:17","title":"A New Route for the Determination of Protein Structure and Function","abstract":"Understanding complex biological macromolecules, especially proteins, is vital for grasping their diverse chemical functions with direct impact in biology and pharmacology. While techniques like X-ray crystallography and cryo-electron microscopy have been valuable, they face limitations such as radiation damage and difficulties in crystallizing certain proteins. X-ray free-electron lasers (XFELs) offer promising solutions with their ultrafast, high-intensity pulses, potentially enabling structural determination before radiation damage occurs. However, challenges like low signal-to-noise ratio persist, particularly for single protein molecules. To address this, we propose a new method involving engineered protein scaffolds to create ordered arrays of proteins with controlled orientations, aiming at enhancing the signal at the detector. This innovative strategy has the potential to address signal limitations and protein crystallization issues, opening avenues for determining protein structures under physiological conditions. Moreover, it holds promise for studying conformational changes resulting from photo-induced changes, protein-drug and/or protein-protein interactions. Indeed, the prediction of protein-protein interactions, fundamental to numerous biochemical and cellular processes, and the time-dependent conformational changes they undergo, continue to pose a considerable challenge in biology and biochemistry.","sentences":["Understanding complex biological macromolecules, especially proteins, is vital for grasping their diverse chemical functions with direct impact in biology and pharmacology.","While techniques like X-ray crystallography and cryo-electron microscopy have been valuable, they face limitations such as radiation damage and difficulties in crystallizing certain proteins.","X-ray free-electron lasers (XFELs) offer promising solutions with their ultrafast, high-intensity pulses, potentially enabling structural determination before radiation damage occurs.","However, challenges like low signal-to-noise ratio persist, particularly for single protein molecules.","To address this, we propose a new method involving engineered protein scaffolds to create ordered arrays of proteins with controlled orientations, aiming at enhancing the signal at the detector.","This innovative strategy has the potential to address signal limitations and protein crystallization issues, opening avenues for determining protein structures under physiological conditions.","Moreover, it holds promise for studying conformational changes resulting from photo-induced changes, protein-drug and/or protein-protein interactions.","Indeed, the prediction of protein-protein interactions, fundamental to numerous biochemical and cellular processes, and the time-dependent conformational changes they undergo, continue to pose a considerable challenge in biology and biochemistry."],"url":"http://arxiv.org/abs/2404.07852v1","category":"q-bio.BM"}
{"created":"2024-04-11 15:42:55","title":"Accounting for the Quantum Capacitance of Graphite in Constant Potential Molecular Dynamics Simulations","abstract":"Molecular dynamics simulations at a constant electric potential are an essential tool to study electrochemical processes, providing microscopic information on the structural, thermodynamic, and dynamical properties. Despite the numerous advances in the simulation of electrodes, they fail to accurately represent the electronic structure of materials such as graphite. In this work, we introduce a simple parameterization method that allows to tune the metallicity of the electrode based on a quantum chemistry calculation of the density of states. As a first illustration, we study the interface between graphite electrodes and two different liquid electrolytes, an aqueous solution of NaCl and a pure ionic liquid, at different applied potentials. We show that the simulations reproduce qualitatively the experimentally-measured capacitance; in particular, they yield a minimum of capacitance at the point of zero charge, which is due to the quantum capacitance contribution. An analysis of the structure of the adsorbed liquids allows to understand why the ionic liquid displays a lower capacitance despite its large ionic concentration. In addition to its relevance for the important class of carbonaceous electrodes, this method can be applied to any electrode materials (e.g. 2D materials, conducting polymers, etc), thus enabling molecular simulation studies of complex electrochemical devices in the future.","sentences":["Molecular dynamics simulations at a constant electric potential are an essential tool to study electrochemical processes, providing microscopic information on the structural, thermodynamic, and dynamical properties.","Despite the numerous advances in the simulation of electrodes, they fail to accurately represent the electronic structure of materials such as graphite.","In this work, we introduce a simple parameterization method that allows to tune the metallicity of the electrode based on a quantum chemistry calculation of the density of states.","As a first illustration, we study the interface between graphite electrodes and two different liquid electrolytes, an aqueous solution of NaCl and a pure ionic liquid, at different applied potentials.","We show that the simulations reproduce qualitatively the experimentally-measured capacitance; in particular, they yield a minimum of capacitance at the point of zero charge, which is due to the quantum capacitance contribution.","An analysis of the structure of the adsorbed liquids allows to understand why the ionic liquid displays a lower capacitance despite its large ionic concentration.","In addition to its relevance for the important class of carbonaceous electrodes, this method can be applied to any electrode materials (e.g. 2D materials, conducting polymers, etc), thus enabling molecular simulation studies of complex electrochemical devices in the future."],"url":"http://arxiv.org/abs/2404.07848v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 15:42:53","title":"Fuss-Free Network: A Simplified and Efficient Neural Network for Crowd Counting","abstract":"In the field of crowd-counting research, many recent deep learning based methods have demonstrated robust capabilities for accurately estimating crowd sizes. However, the enhancement in their performance often arises from an increase in the complexity of the model structure. This paper introduces the Fuss-Free Network (FFNet), a crowd counting deep learning model that is characterized by its simplicity and efficiency in terms of its structure. The model comprises only a backbone of a neural network and a multi-scale feature fusion structure.The multi-scale feature fusion structure is a simple architecture consisting of three branches, each only equipped with a focus transition module, and combines the features from these branches through the concatenation operation.Our proposed crowd counting model is trained and evaluated on four widely used public datasets, and it achieves accuracy that is comparable to that of existing complex models.The experimental results further indicate that excellent performance in crowd counting tasks can also be achieved by utilizing a simple, low-parameter, and computationally efficient neural network structure.","sentences":["In the field of crowd-counting research, many recent deep learning based methods have demonstrated robust capabilities for accurately estimating crowd sizes.","However, the enhancement in their performance often arises from an increase in the complexity of the model structure.","This paper introduces the Fuss-Free Network (FFNet), a crowd counting deep learning model that is characterized by its simplicity and efficiency in terms of its structure.","The model comprises only a backbone of a neural network and a multi-scale feature fusion structure.","The multi-scale feature fusion structure is a simple architecture consisting of three branches, each only equipped with a focus transition module, and combines the features from these branches through the concatenation operation.","Our proposed crowd counting model is trained and evaluated on four widely used public datasets, and it achieves accuracy that is comparable to that of existing complex models.","The experimental results further indicate that excellent performance in crowd counting tasks can also be achieved by utilizing a simple, low-parameter, and computationally efficient neural network structure."],"url":"http://arxiv.org/abs/2404.07847v1","category":"cs.CV"}
{"created":"2024-04-11 15:33:08","title":"The search for NLS ground states on a hybrid domain: Motivations, methods, and results","abstract":"We discuss the problem of establishing the existence of the Ground States for the subcritical focusing Nonlinear Schr\\\"odinger energy on a domain made of a line and a plane intersecting at a point. The problem is physically motivated by the experimental realization of hybrid traps for Bose-Einstein Condensates, that are able to concentrate the system on structures close to the domain we consider. In fact, such a domain approximates the trap as the temperature approaches the absolute zero. The spirit of the paper is mainly pedagogical, so we focus on the formulation of the problem and on the explanation of the result, giving references for the technical points and for the proofs.","sentences":["We discuss the problem of establishing the existence of the Ground States for the subcritical focusing Nonlinear Schr\\\"odinger energy on a domain made of a line and a plane intersecting at a point.","The problem is physically motivated by the experimental realization of hybrid traps for Bose-Einstein Condensates, that are able to concentrate the system on structures close to the domain we consider.","In fact, such a domain approximates the trap as the temperature approaches the absolute zero.","The spirit of the paper is mainly pedagogical, so we focus on the formulation of the problem and on the explanation of the result, giving references for the technical points and for the proofs."],"url":"http://arxiv.org/abs/2404.07843v1","category":"math.AP"}
{"created":"2024-04-11 15:27:14","title":"The Role of Confidence for Trust-based Resilient Consensus (Extended Version)","abstract":"We consider a multi-agent system where agents aim to achieve a consensus despite interactions with malicious agents that communicate misleading information. Physical channels supporting communication in cyberphysical systems offer attractive opportunities to detect malicious agents, nevertheless, trustworthiness indications coming from the channel are subject to uncertainty and need to be treated with this in mind. We propose a resilient consensus protocol that incorporates trust observations from the channel and weighs them with a parameter that accounts for how confident an agent is regarding its understanding of the legitimacy of other agents in the network, with no need for the initial observation window $T_0$ that has been utilized in previous works. Analytical and numerical results show that (i) our protocol achieves a resilient consensus in the presence of malicious agents and (ii) the steady-state deviation from nominal consensus can be minimized by a suitable choice of the confidence parameter that depends on the statistics of trust observations.","sentences":["We consider a multi-agent system where agents aim to achieve a consensus despite interactions with malicious agents that communicate misleading information.","Physical channels supporting communication in cyberphysical systems offer attractive opportunities to detect malicious agents, nevertheless, trustworthiness indications coming from the channel are subject to uncertainty and need to be treated with this in mind.","We propose a resilient consensus protocol that incorporates trust observations from the channel and weighs them with a parameter that accounts for how confident an agent is regarding its understanding of the legitimacy of other agents in the network, with no need for the initial observation window $T_0$ that has been utilized in previous works.","Analytical and numerical results show that (i) our protocol achieves a resilient consensus in the presence of malicious agents and (ii) the steady-state deviation from nominal consensus can be minimized by a suitable choice of the confidence parameter that depends on the statistics of trust observations."],"url":"http://arxiv.org/abs/2404.07838v1","category":"cs.MA"}
{"created":"2024-04-11 15:12:41","title":"Scalable spider nests (...or how to graphically grok transversal non-Clifford gates)","abstract":"This is the second in a series of \"graphical grokking\" papers in which we study how stabiliser codes can be understood using the ZX calculus. In this paper we show that certain complex rules involving ZX diagrams, called spider nest identities, can be captured succinctly using the scalable ZX calculus, and all such identities can be proved inductively from a single new rule using the Clifford ZX calculus. This can be combined with the ZX picture of CSS codes, developed in the first \"grokking\" paper, to give a simple characterisation of the set of all transversal diagonal gates at the third level of the Clifford hierarchy implementable in an arbitrary CSS code.","sentences":["This is the second in a series of \"graphical grokking\" papers in which we study how stabiliser codes can be understood using the ZX calculus.","In this paper we show that certain complex rules involving ZX diagrams, called spider nest identities, can be captured succinctly using the scalable ZX calculus, and all such identities can be proved inductively from a single new rule using the Clifford ZX calculus.","This can be combined with the ZX picture of CSS codes, developed in the first \"grokking\" paper, to give a simple characterisation of the set of all transversal diagonal gates at the third level of the Clifford hierarchy implementable in an arbitrary CSS code."],"url":"http://arxiv.org/abs/2404.07828v1","category":"quant-ph"}
{"created":"2024-04-11 15:08:11","title":"Learning Deterministic Multi-Clock Timed Automata","abstract":"We present an algorithm for active learning of deterministic timed automata with multiple clocks. The algorithm is within the querying framework of Angluin's $L^*$ algorithm and follows the idea proposed in existing work on the active learning of deterministic one-clock timed automata. We introduce an equivalence relation over the reset-clocked language of a timed automaton and then transform the learning problem into learning the corresponding reset-clocked language of the target automaton. Since a reset-clocked language includes the clock reset information which is not observable, we first present the approach of learning from a powerful teacher who can provide reset information by answering reset information queries from the learner. Then we extend the algorithm in a normal teacher situation in which the learner can only ask standard membership query and equivalence query while the learner guesses the reset information. We prove that the learning algorithm terminates and returns a correct deterministic timed automaton. Due to the need of guessing whether the clocks reset at the transitions, the algorithm is of exponential complexity in the size of the target automaton.","sentences":["We present an algorithm for active learning of deterministic timed automata with multiple clocks.","The algorithm is within the querying framework of Angluin's $L^*$ algorithm and follows the idea proposed in existing work on the active learning of deterministic one-clock timed automata.","We introduce an equivalence relation over the reset-clocked language of a timed automaton and then transform the learning problem into learning the corresponding reset-clocked language of the target automaton.","Since a reset-clocked language includes the clock reset information which is not observable, we first present the approach of learning from a powerful teacher who can provide reset information by answering reset information queries from the learner.","Then we extend the algorithm in a normal teacher situation in which the learner can only ask standard membership query and equivalence query while the learner guesses the reset information.","We prove that the learning algorithm terminates and returns a correct deterministic timed automaton.","Due to the need of guessing whether the clocks reset at the transitions, the algorithm is of exponential complexity in the size of the target automaton."],"url":"http://arxiv.org/abs/2404.07823v1","category":"cs.FL"}
{"created":"2024-04-11 14:57:19","title":"MultiLS-SP/CA: Lexical Complexity Prediction and Lexical Simplification Resources for Catalan and Spanish","abstract":"Automatic lexical simplification is a task to substitute lexical items that may be unfamiliar and difficult to understand with easier and more common words. This paper presents MultiLS-SP/CA, a novel dataset for lexical simplification in Spanish and Catalan. This dataset represents the first of its kind in Catalan and a substantial addition to the sparse data on automatic lexical simplification which is available for Spanish. Specifically, MultiLS-SP is the first dataset for Spanish which includes scalar ratings of the understanding difficulty of lexical items. In addition, we describe experiments with this dataset, which can serve as a baseline for future work on the same data.","sentences":["Automatic lexical simplification is a task to substitute lexical items that may be unfamiliar and difficult to understand with easier and more common words.","This paper presents MultiLS-SP/CA, a novel dataset for lexical simplification in Spanish and Catalan.","This dataset represents the first of its kind in Catalan and a substantial addition to the sparse data on automatic lexical simplification which is available for Spanish.","Specifically, MultiLS-SP is the first dataset for Spanish which includes scalar ratings of the understanding difficulty of lexical items.","In addition, we describe experiments with this dataset, which can serve as a baseline for future work on the same data."],"url":"http://arxiv.org/abs/2404.07814v1","category":"cs.CL"}
{"created":"2024-04-11 14:54:29","title":"M-dwarf flares in the Zwicky Transient Facility data and what we can learn from them","abstract":"In this paper, we explore the possibility of detecting M-dwarf flares using data from the Zwicky Transient Facility data releases (ZTF DRs). We employ two different approaches: the traditional method of parametric fit search and a machine learning algorithm originally developed for anomaly detection. We analyzed over 35 million ZTF light curves and visually scrutinized 1168 candidates suggested by the algorithms to filter out artifacts, occultations of a star by an asteroid, and known variable objects of other types. Our final sample comprises 134 flares with amplitude ranging from 0.2 to 4.6 magnitudes, including repeated flares and complex flares with multiple components. Using Pan-STARRS DR2 colors, we also assigned a corresponding spectral subclass to each object in the sample. For 13 flares with well-sampled light curves, we estimated the bolometric energy. Our results show that the ZTF's cadence strategy is suitable for identifying M-dwarf flares and other fast transients, allowing for the extraction of significant astrophysical information from their light curves.","sentences":["In this paper, we explore the possibility of detecting M-dwarf flares using data from the Zwicky Transient Facility data releases (ZTF DRs).","We employ two different approaches: the traditional method of parametric fit search and a machine learning algorithm originally developed for anomaly detection.","We analyzed over 35 million ZTF light curves and visually scrutinized 1168 candidates suggested by the algorithms to filter out artifacts, occultations of a star by an asteroid, and known variable objects of other types.","Our final sample comprises 134 flares with amplitude ranging from 0.2 to 4.6 magnitudes, including repeated flares and complex flares with multiple components.","Using Pan-STARRS DR2 colors, we also assigned a corresponding spectral subclass to each object in the sample.","For 13 flares with well-sampled light curves, we estimated the bolometric energy.","Our results show that the ZTF's cadence strategy is suitable for identifying M-dwarf flares and other fast transients, allowing for the extraction of significant astrophysical information from their light curves."],"url":"http://arxiv.org/abs/2404.07812v1","category":"astro-ph.SR"}
{"created":"2024-04-11 14:52:25","title":"The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids","abstract":"We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\\mathbb{R}^d$ ($d\\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \\textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.   The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \\textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.","sentences":["We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\\mathbb{R}^d$ ($d\\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system.","Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \\textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts.","In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF).","To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.   ","The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms.","To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory.","More precisely, our analysis relies on partitioning the frequency space into \\textit{three} distinct regimes: low, medium and high frequencies.","Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures."],"url":"http://arxiv.org/abs/2404.07809v1","category":"math.AP"}
{"created":"2024-04-11 14:51:12","title":"Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network","abstract":"Traffic signs are important in communicating information to drivers. Thus, comprehension of traffic signs is essential for road safety and ignorance may result in road accidents. Traffic sign detection has been a research spotlight over the past few decades. Real-time and accurate detections are the preliminaries of robust traffic sign detection system which is yet to be achieved. This study presents a voice-assisted real-time traffic sign recognition system which is capable of assisting drivers. This system functions under two subsystems. Initially, the detection and recognition of the traffic signs are carried out using a trained Convolutional Neural Network (CNN). After recognizing the specific traffic sign, it is narrated to the driver as a voice message using a text-to-speech engine. An efficient CNN model for a benchmark dataset is developed for real-time detection and recognition using Deep Learning techniques. The advantage of this system is that even if the driver misses a traffic sign, or does not look at the traffic sign, or is unable to comprehend the sign, the system detects it and narrates it to the driver. A system of this type is also important in the development of autonomous vehicles.","sentences":["Traffic signs are important in communicating information to drivers.","Thus, comprehension of traffic signs is essential for road safety and ignorance may result in road accidents.","Traffic sign detection has been a research spotlight over the past few decades.","Real-time and accurate detections are the preliminaries of robust traffic sign detection system which is yet to be achieved.","This study presents a voice-assisted real-time traffic sign recognition system which is capable of assisting drivers.","This system functions under two subsystems.","Initially, the detection and recognition of the traffic signs are carried out using a trained Convolutional Neural Network (CNN).","After recognizing the specific traffic sign, it is narrated to the driver as a voice message using a text-to-speech engine.","An efficient CNN model for a benchmark dataset is developed for real-time detection and recognition using Deep Learning techniques.","The advantage of this system is that even if the driver misses a traffic sign, or does not look at the traffic sign, or is unable to comprehend the sign, the system detects it and narrates it to the driver.","A system of this type is also important in the development of autonomous vehicles."],"url":"http://arxiv.org/abs/2404.07807v1","category":"cs.CV"}
{"created":"2024-04-11 14:47:18","title":"Synergy between noisy quantum computers and scalable classical deep learning","abstract":"We investigate the potential of combining the computational power of noisy quantum computers and of classical scalable convolutional neural networks (CNNs). The goal is to accurately predict exact expectation values of parameterized quantum circuits representing the Trotter-decomposed dynamics of quantum Ising models. By incorporating (simulated) noisy expectation values alongside circuit structure information, our CNNs effectively capture the underlying relationships between circuit architecture and output behaviour, enabling predictions for circuits with more qubits than those included in the training set. Notably, thanks to the quantum information, our CNNs succeed even when supervised learning based only on classical descriptors fails. Furthermore, they outperform a popular error mitigation scheme, namely, zero-noise extrapolation, demonstrating that the synergy between quantum and classical computational tools leads to higher accuracy compared with quantum-only or classical-only approaches. By tuning the noise strength, we explore the crossover from a computationally powerful classical CNN assisted by quantum noisy data, towards rather precise quantum computations, further error-mitigated via classical deep learning.","sentences":["We investigate the potential of combining the computational power of noisy quantum computers and of classical scalable convolutional neural networks (CNNs).","The goal is to accurately predict exact expectation values of parameterized quantum circuits representing the Trotter-decomposed dynamics of quantum Ising models.","By incorporating (simulated) noisy expectation values alongside circuit structure information, our CNNs effectively capture the underlying relationships between circuit architecture and output behaviour, enabling predictions for circuits with more qubits than those included in the training set.","Notably, thanks to the quantum information, our CNNs succeed even when supervised learning based only on classical descriptors fails.","Furthermore, they outperform a popular error mitigation scheme, namely, zero-noise extrapolation, demonstrating that the synergy between quantum and classical computational tools leads to higher accuracy compared with quantum-only or classical-only approaches.","By tuning the noise strength, we explore the crossover from a computationally powerful classical CNN assisted by quantum noisy data, towards rather precise quantum computations, further error-mitigated via classical deep learning."],"url":"http://arxiv.org/abs/2404.07802v1","category":"quant-ph"}
{"created":"2024-04-11 14:39:55","title":"Terahertz imaging super-resolution for documental heritage diagnostics","abstract":"Terahertz imaging provides valuable insights into the composition and structure of objects or materials, with applications spanning security screening, medical imaging, materials science, and cultural heritage preservation. Despite its widespread utility, traditional terahertz imaging is limited in spatial resolution to approximately 1 mm according to Abbe's formula. In this paper, we propose a novel super-resolution method for terahertz time-domain spectroscopy systems. Our approach involves spatial filtering through scattering in the far-field of high spatial frequency components of the imaged sample. This method leverages evanescent wave filtering using a knife edge, akin to a standard structured illumination scheme. We demonstrate improved spatial resolution in slit diffraction, edge imaging, and reflection imaging of structures fabricated on a paper substrate using commonly encountered materials in works of art and documents. Furthermore, we present super-resolved images of an ancient document on parchment, showcasing the effectiveness of our proposed method.","sentences":["Terahertz imaging provides valuable insights into the composition and structure of objects or materials, with applications spanning security screening, medical imaging, materials science, and cultural heritage preservation.","Despite its widespread utility, traditional terahertz imaging is limited in spatial resolution to approximately 1 mm according to Abbe's formula.","In this paper, we propose a novel super-resolution method for terahertz time-domain spectroscopy systems.","Our approach involves spatial filtering through scattering in the far-field of high spatial frequency components of the imaged sample.","This method leverages evanescent wave filtering using a knife edge, akin to a standard structured illumination scheme.","We demonstrate improved spatial resolution in slit diffraction, edge imaging, and reflection imaging of structures fabricated on a paper substrate using commonly encountered materials in works of art and documents.","Furthermore, we present super-resolved images of an ancient document on parchment, showcasing the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2404.07798v1","category":"physics.optics"}
{"created":"2024-04-11 14:38:29","title":"Point defects in CdTe and CdTeSe alloy: a first principles investigation with DFT+U","abstract":"CdTe and its alloy CdTeSe are widely used in optoelectronic devices, such as radiation detectors and solar cells, due to their superior electrical properties. However, the formation of defects and defect complexes in these materials can significantly affect their performance. As a result, understanding the defect formation and recombination processes in CdTe and CdTeSe alloy is of great importance. In recent years, density functional theory (DFT) calculations have emerged as a powerful tool for investigating the properties of defects in semiconductors. In this paper, we use DFT+U calculations to comprehensively study the properties of intrinsic defects as well as extrinsic defects induced by commonly used dopants, such as Cu and group V elements, in CdTe and CdTeSe alloy. This work provides insights into the effects of these defects on the electrical and optical properties of the material.","sentences":["CdTe and its alloy CdTeSe are widely used in optoelectronic devices, such as radiation detectors and solar cells, due to their superior electrical properties.","However, the formation of defects and defect complexes in these materials can significantly affect their performance.","As a result, understanding the defect formation and recombination processes in CdTe and CdTeSe alloy is of great importance.","In recent years, density functional theory (DFT) calculations have emerged as a powerful tool for investigating the properties of defects in semiconductors.","In this paper, we use DFT+U calculations to comprehensively study the properties of intrinsic defects as well as extrinsic defects induced by commonly used dopants, such as Cu and group V elements, in CdTe and CdTeSe alloy.","This work provides insights into the effects of these defects on the electrical and optical properties of the material."],"url":"http://arxiv.org/abs/2404.07796v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 14:36:13","title":"From the Lab to the Theater: An Unconventional Field Robotics Journey","abstract":"Artistic performances involving robotic systems present unique technical challenges akin to those encountered in other field deployments. In this paper, we delve into the orchestration of robotic artistic performances, focusing on the complexities inherent in communication protocols and localization methods. Through our case studies and experimental insights, we demonstrate the breadth of technical requirements for this type of deployment, and, most importantly, the significant contributions of working closely with non-experts.","sentences":["Artistic performances involving robotic systems present unique technical challenges akin to those encountered in other field deployments.","In this paper, we delve into the orchestration of robotic artistic performances, focusing on the complexities inherent in communication protocols and localization methods.","Through our case studies and experimental insights, we demonstrate the breadth of technical requirements for this type of deployment, and, most importantly, the significant contributions of working closely with non-experts."],"url":"http://arxiv.org/abs/2404.07795v1","category":"cs.RO"}
{"created":"2024-04-11 14:35:23","title":"Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation","abstract":"This paper describes submissions from the team Nostra Domina to the EvaLatin 2024 shared task of emotion polarity detection. Given the low-resource environment of Latin and the complexity of sentiment in rhetorical genres like poetry, we augmented the available data through automatic polarity annotation. We present two methods for doing so on the basis of the $k$-means algorithm, and we employ a variety of Latin large language models (LLMs) in a neural architecture to better capture the underlying contextual sentiment representations. Our best approach achieved the second highest macro-averaged Macro-$F_1$ score on the shared task's test set.","sentences":["This paper describes submissions from the team Nostra Domina to the EvaLatin 2024 shared task of emotion polarity detection.","Given the low-resource environment of Latin and the complexity of sentiment in rhetorical genres like poetry, we augmented the available data through automatic polarity annotation.","We present two methods for doing so on the basis of the $k$-means algorithm, and we employ a variety of Latin large language models (LLMs) in a neural architecture to better capture the underlying contextual sentiment representations.","Our best approach achieved the second highest macro-averaged Macro-$F_1$ score on the shared task's test set."],"url":"http://arxiv.org/abs/2404.07792v1","category":"cs.CL"}
{"created":"2024-04-11 14:29:26","title":"Research on fine co-focus adjustment method for segmented solar telescope","abstract":"For segmented telescopes, achieving fine co-focus adjustment is essential for realizing co-phase adjustment and maintenance, which involves adjusting the millimeter-scale piston between segments to fall within the capture range of the co-phase detection system. CGST proposes using a SHWFS for piston detection during the co-focus adjustment stage. However, the residual piston after adjustment exceeds the capture range of the broadband PSF phasing algorithm$(\\pm 30 \\mu m) $, and the multi-wavelength PSF algorithm requires even higher precision in co-focus adjustment. To improve the co-focus adjustment accuracy of CGST, a fine co-focus adjustment based on cross-calibration is proposed. This method utilizes a high-precision detector to calibrate and fit the measurements from the SHWFS, thereby reducing the impact of atmospheric turbulence and systematic errors on piston measurement accuracy during co-focus adjustment. Simulation results using CGST demonstrate that the proposed method significantly enhances adjustment accuracy compared to the SHWFS detection method. Additionally, the residual piston after fine co-focus adjustment using this method falls within the capture range of the multi-wavelength PSF algorithm. To verify the feasibility of this method, experiments were conducted on an 800mm ring segmented mirror system, successfully achieving fine co-focus adjustment where the remaining piston of all segments fell within $\\pm 15 \\mu m$.","sentences":["For segmented telescopes, achieving fine co-focus adjustment is essential for realizing co-phase adjustment and maintenance, which involves adjusting the millimeter-scale piston between segments to fall within the capture range of the co-phase detection system.","CGST proposes using a SHWFS for piston detection during the co-focus adjustment stage.","However, the residual piston after adjustment exceeds the capture range of the broadband PSF phasing algorithm$(\\pm 30 \\mu m) $, and the multi-wavelength PSF algorithm requires even higher precision in co-focus adjustment.","To improve the co-focus adjustment accuracy of CGST, a fine co-focus adjustment based on cross-calibration is proposed.","This method utilizes a high-precision detector to calibrate and fit the measurements from the SHWFS, thereby reducing the impact of atmospheric turbulence and systematic errors on piston measurement accuracy during co-focus adjustment.","Simulation results using CGST demonstrate that the proposed method significantly enhances adjustment accuracy compared to the SHWFS detection method.","Additionally, the residual piston after fine co-focus adjustment using this method falls within the capture range of the multi-wavelength PSF algorithm.","To verify the feasibility of this method, experiments were conducted on an 800mm ring segmented mirror system, successfully achieving fine co-focus adjustment where the remaining piston of all segments fell within $\\pm 15 \\mu m$."],"url":"http://arxiv.org/abs/2404.07787v1","category":"astro-ph.IM"}
{"created":"2024-04-11 14:18:00","title":"Galmoss: A package for GPU-accelerated Galaxy Profile Fitting","abstract":"We introduce galmoss, a python-based, torch-powered tool for two-dimensional fitting of galaxy profiles. By seamlessly enabling GPU parallelization, galmoss meets the high computational demands of large-scale galaxy surveys, placing galaxy profile fitting in the LSST-era. It incorporates widely used profiles such as the S\\'ersic, Exponential disk, Ferrer, King, Gaussian, and Moffat profiles, and allows for the easy integration of more complex models. Tested on 8,289 galaxies from the Sloan Digital Sky Survey (SDSS) g-band with a single NVIDIA A100 GPU, galmoss completed classical S\\'ersic profile fitting in about 10 minutes. Benchmark tests show that galmoss achieves computational speeds that are 6 $\\times$ faster than those of default implementations.","sentences":["We introduce galmoss, a python-based, torch-powered tool for two-dimensional fitting of galaxy profiles.","By seamlessly enabling GPU parallelization, galmoss meets the high computational demands of large-scale galaxy surveys, placing galaxy profile fitting in the LSST-era.","It incorporates widely used profiles such as the S\\'ersic, Exponential disk, Ferrer, King, Gaussian, and Moffat profiles, and allows for the easy integration of more complex models.","Tested on 8,289 galaxies from the Sloan Digital Sky Survey (SDSS) g-band with a single NVIDIA A100 GPU, galmoss completed classical S\\'ersic profile fitting in about 10 minutes.","Benchmark tests show that galmoss achieves computational speeds that are 6 $\\times$ faster than those of default implementations."],"url":"http://arxiv.org/abs/2404.07780v1","category":"astro-ph.GA"}
{"created":"2024-04-11 14:07:03","title":"Coupling nanoscopic tomography and micromagnetic modelling to assess the stability of geomagnetic recorders","abstract":"The recording of planetary magnetic fields is often attributed to uniformly-magnetised nanoscopic iron oxides, called single-domain (SD). Yet, the main magnetic constituents of rocks are more complex, non-uniformly magnetised grains in single or multi-vortex states. We know little about their behaviour due to limitations in defining their precise shape and internal magnetic structure. We propose a novel approach combining non-destructive synchrotron-based ptychographic nanotomography with micromagnetic modelling to explore the magnetic stability of remanence-bearing minerals. Applied to a microscopic rock sample, we identified hundreds of nanoscopic grains of magnetite/maghemite with diverse morphologies. For some grains, shape irregularities near the transition from SD to the single-vortex state allow for multiple domain states, some unstable and unable to record the field for significant periods. Additionally, some other grains exhibit temperature-dependent occupancy probabilities, potentially hampering experiments to recover the intensity of past magnetic fields.","sentences":["The recording of planetary magnetic fields is often attributed to uniformly-magnetised nanoscopic iron oxides, called single-domain (SD).","Yet, the main magnetic constituents of rocks are more complex, non-uniformly magnetised grains in single or multi-vortex states.","We know little about their behaviour due to limitations in defining their precise shape and internal magnetic structure.","We propose a novel approach combining non-destructive synchrotron-based ptychographic nanotomography with micromagnetic modelling to explore the magnetic stability of remanence-bearing minerals.","Applied to a microscopic rock sample, we identified hundreds of nanoscopic grains of magnetite/maghemite with diverse morphologies.","For some grains, shape irregularities near the transition from SD to the single-vortex state allow for multiple domain states, some unstable and unable to record the field for significant periods.","Additionally, some other grains exhibit temperature-dependent occupancy probabilities, potentially hampering experiments to recover the intensity of past magnetic fields."],"url":"http://arxiv.org/abs/2404.07769v1","category":"physics.geo-ph"}
{"created":"2024-04-11 14:06:14","title":"Active particles knead three-dimensional gels into open crumbs","abstract":"Colloidal gels are prime examples of functional materials exhibiting disordered, amorphous, yet meta-stable forms. They maintain stability through short-range attractive forces and their material properties are tunable by external forces. Combining persistent homology analyses and simulations of three-dimensional colloidal gels doped with active particles, we reveal novel dynamically evolving structures of colloidal gels. Specifically, we show that the local injection of energy by active dopants can lead to highly porous, yet compact gel structures that can significantly affect the transport of active particles within the modified colloidal gel. We further show the substantially distinct structural behaviour between active doping of 2D and 3D systems by revealing how passive interfaces play a topologically different role in interacting with active particles in two and three dimensions. The results open the door to an unexplored prospect of forming a wide variety of compact but highly heterogeneous and percolated porous media through active doping of 3D passive matter, with diverse implications in designing new functional materials to active ground remediation.","sentences":["Colloidal gels are prime examples of functional materials exhibiting disordered, amorphous, yet meta-stable forms.","They maintain stability through short-range attractive forces and their material properties are tunable by external forces.","Combining persistent homology analyses and simulations of three-dimensional colloidal gels doped with active particles, we reveal novel dynamically evolving structures of colloidal gels.","Specifically, we show that the local injection of energy by active dopants can lead to highly porous, yet compact gel structures that can significantly affect the transport of active particles within the modified colloidal gel.","We further show the substantially distinct structural behaviour between active doping of 2D and 3D systems by revealing how passive interfaces play a topologically different role in interacting with active particles in two and three dimensions.","The results open the door to an unexplored prospect of forming a wide variety of compact but highly heterogeneous and percolated porous media through active doping of 3D passive matter, with diverse implications in designing new functional materials to active ground remediation."],"url":"http://arxiv.org/abs/2404.07767v1","category":"cond-mat.soft"}
{"created":"2024-04-11 14:05:37","title":"RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric Stereo Network","abstract":"Predicting accurate normal maps of objects from two-dimensional images in regions of complex structure and spatial material variations is challenging using photometric stereo methods due to the influence of surface reflection properties caused by variations in object geometry and surface materials. To address this issue, we propose a photometric stereo network called a RMAFF-PSN that uses residual multiscale attentional feature fusion to handle the ``difficult'' regions of the object. Unlike previous approaches that only use stacked convolutional layers to extract deep features from the input image, our method integrates feature information from different resolution stages and scales of the image. This approach preserves more physical information, such as texture and geometry of the object in complex regions, through shallow-deep stage feature extraction, double branching enhancement, and attention optimization. To test the network structure under real-world conditions, we propose a new real dataset called Simple PS data, which contains multiple objects with varying structures and materials. Experimental results on a publicly available benchmark dataset demonstrate that our method outperforms most existing calibrated photometric stereo methods for the same number of input images, especially in the case of highly non-convex object structures. Our method also obtains good results under sparse lighting conditions.","sentences":["Predicting accurate normal maps of objects from two-dimensional images in regions of complex structure and spatial material variations is challenging using photometric stereo methods due to the influence of surface reflection properties caused by variations in object geometry and surface materials.","To address this issue, we propose a photometric stereo network called a RMAFF-PSN that uses residual multiscale attentional feature fusion to handle the ``difficult'' regions of the object.","Unlike previous approaches that only use stacked convolutional layers to extract deep features from the input image, our method integrates feature information from different resolution stages and scales of the image.","This approach preserves more physical information, such as texture and geometry of the object in complex regions, through shallow-deep stage feature extraction, double branching enhancement, and attention optimization.","To test the network structure under real-world conditions, we propose a new real dataset called Simple PS data, which contains multiple objects with varying structures and materials.","Experimental results on a publicly available benchmark dataset demonstrate that our method outperforms most existing calibrated photometric stereo methods for the same number of input images, especially in the case of highly non-convex object structures.","Our method also obtains good results under sparse lighting conditions."],"url":"http://arxiv.org/abs/2404.07766v1","category":"cs.CV"}
{"created":"2024-04-11 14:03:16","title":"NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous Driving","abstract":"We present a versatile NeRF-based simulator for testing autonomous driving (AD) software systems, designed with a focus on sensor-realistic closed-loop evaluation and the creation of safety-critical scenarios. The simulator learns from sequences of real-world driving sensor data and enables reconfigurations and renderings of new, unseen scenarios. In this work, we use our simulator to test the responses of AD models to safety-critical scenarios inspired by the European New Car Assessment Programme (Euro NCAP). Our evaluation reveals that, while state-of-the-art end-to-end planners excel in nominal driving scenarios in an open-loop setting, they exhibit critical flaws when navigating our safety-critical scenarios in a closed-loop setting. This highlights the need for advancements in the safety and real-world usability of end-to-end planners. By publicly releasing our simulator and scenarios as an easy-to-run evaluation suite, we invite the research community to explore, refine, and validate their AD models in controlled, yet highly configurable and challenging sensor-realistic environments. Code and instructions can be found at https://github.com/wljungbergh/NeuroNCAP","sentences":["We present a versatile NeRF-based simulator for testing autonomous driving (AD) software systems, designed with a focus on sensor-realistic closed-loop evaluation and the creation of safety-critical scenarios.","The simulator learns from sequences of real-world driving sensor data and enables reconfigurations and renderings of new, unseen scenarios.","In this work, we use our simulator to test the responses of AD models to safety-critical scenarios inspired by the European New Car Assessment Programme (Euro NCAP).","Our evaluation reveals that, while state-of-the-art end-to-end planners excel in nominal driving scenarios in an open-loop setting, they exhibit critical flaws when navigating our safety-critical scenarios in a closed-loop setting.","This highlights the need for advancements in the safety and real-world usability of end-to-end planners.","By publicly releasing our simulator and scenarios as an easy-to-run evaluation suite, we invite the research community to explore, refine, and validate their AD models in controlled, yet highly configurable and challenging sensor-realistic environments.","Code and instructions can be found at https://github.com/wljungbergh/NeuroNCAP"],"url":"http://arxiv.org/abs/2404.07762v1","category":"cs.CV"}
{"created":"2024-04-11 14:03:01","title":"Inverse solving the Schr\u00f6dinger equation for precision alignment of a microcavity","abstract":"In paraxial approximation, the electromagnetic eigenmodes inside an optical microresonator can be derived from a Schr\\\"odinger-type eigenvalue problem. In this framework, tilting the cavity mirrors effectively introduces a linear potential to the system. In our work, we apply solution strategies for inverse problems to precisely determine and control the relative orientation of two mirrors forming an optical microcavity. Our approach employs the inversion of the Schr\\\"odinger equation to reconstruct the effective potential landscape, and thus mirror tilts, from observed mode patterns. We investigate regularization techniques to address the ill-posed nature of inverse problems and to improve the stability of solutions. Our method consistently achieves an angle resolution of order 100 nanoradians per measurement.","sentences":["In paraxial approximation, the electromagnetic eigenmodes inside an optical microresonator can be derived from a Schr\\\"odinger-type eigenvalue problem.","In this framework, tilting the cavity mirrors effectively introduces a linear potential to the system.","In our work, we apply solution strategies for inverse problems to precisely determine and control the relative orientation of two mirrors forming an optical microcavity.","Our approach employs the inversion of the Schr\\\"odinger equation to reconstruct the effective potential landscape, and thus mirror tilts, from observed mode patterns.","We investigate regularization techniques to address the ill-posed nature of inverse problems and to improve the stability of solutions.","Our method consistently achieves an angle resolution of order 100 nanoradians per measurement."],"url":"http://arxiv.org/abs/2404.07760v1","category":"physics.optics"}
{"created":"2024-04-11 13:51:58","title":"Singular linear forms over global function fields","abstract":"In this paper, we consider singular linear forms over global function fields of class number one and give an upper bound for the Hausdorff dimension of the set of singular linear forms by constructing an appropriate Margulis function over global function fields.","sentences":["In this paper, we consider singular linear forms over global function fields of class number one and give an upper bound for the Hausdorff dimension of the set of singular linear forms by constructing an appropriate Margulis function over global function fields."],"url":"http://arxiv.org/abs/2404.07752v1","category":"math.DS"}
{"created":"2024-04-11 13:43:42","title":"Scenario Reduction with Guarantees for Stochastic Optimal Control of Linear Systems","abstract":"Scenario reduction algorithms can be an effective means to provide a tractable description of the uncertainty in optimal control problems. However, they might significantly compromise the performance of the controlled system. In this paper, we propose a method to compensate for the effect of scenario reduction on stochastic optimal control problems for chance-constrained linear systems with additive uncertainty. We consider a setting in which the uncertainty has a discrete distribution, where the number of possible realizations is large. We then propose a reduction algorithm with a problem-dependent loss function, and we define sufficient conditions on the stochastic optimal control problem to ensure out-of-sample guarantees (i.e., against the original distribution of the uncertainty) for the controlled system in terms of performance and chance constraint satisfaction. Finally, we demonstrate the effectiveness of the approach on a numerical example.","sentences":["Scenario reduction algorithms can be an effective means to provide a tractable description of the uncertainty in optimal control problems.","However, they might significantly compromise the performance of the controlled system.","In this paper, we propose a method to compensate for the effect of scenario reduction on stochastic optimal control problems for chance-constrained linear systems with additive uncertainty.","We consider a setting in which the uncertainty has a discrete distribution, where the number of possible realizations is large.","We then propose a reduction algorithm with a problem-dependent loss function, and we define sufficient conditions on the stochastic optimal control problem to ensure out-of-sample guarantees (i.e., against the original distribution of the uncertainty) for the controlled system in terms of performance and chance constraint satisfaction.","Finally, we demonstrate the effectiveness of the approach on a numerical example."],"url":"http://arxiv.org/abs/2404.07746v1","category":"math.OC"}
{"created":"2024-04-11 13:40:01","title":"Orbital and Atmospheric Characterization of the 1RXS J034231.8+121622 System Using High-Resolution Spectroscopy Confirms That The Companion is a Low-Mass Star","abstract":"The 1RXS J034231.8+121622 system consists of an M dwarf primary and a directly imaged low-mass stellar companion. We use high resolution spectroscopic data from Keck/KPIC to estimate the objects' atmospheric parameters and radial velocities (RVs). Using PHOENIX stellar models, we find that the primary has a temperature of 3460 $\\pm$ 50 K a metallicity of 0.16 $\\pm$ 0.04, while the secondary has a temperature of 2510 $\\pm$ 50 K and a metallicity of $0.13\\substack{+0.12 \\\\ -0.11}$. Recent work suggests this system is associated with the Hyades, placing it an older age than previous estimates. Both metallicities agree with current $[Fe/H]$ Hyades measurements (0.11 -- 0.21). Using stellar evolutionary models, we obtain significantly higher masses for the objects, of 0.30 $\\pm$ 0.15 $M_\\odot$ and 0.08 $\\pm$ 0.01 $M_\\odot$ (84 $\\pm$ 11 $M_{Jup}$) respectively. Using the RVs and a new astrometry point from Keck/NIRC2, we find that the system is likely an edge-on, moderately eccentric ($0.41\\substack{+0.27 \\\\ -0.08}$) configuration. We also estimate the C/O ratio of both objects using custom grid models, obtaining 0.42 $\\pm$ 0.10 (primary) and 0.55 $\\pm$ 0.10 (companion). From these results, we confirm that this system most likely went through a binary star formation process in the Hyades. The significant changes in this system's parameters since its discovery highlight the importance of high resolution spectroscopy for both orbital and atmospheric characterization of directly imaged companions.","sentences":["The 1RXS J034231.8+121622 system consists of an M dwarf primary and a directly imaged low-mass stellar companion.","We use high resolution spectroscopic data from Keck/KPIC to estimate the objects' atmospheric parameters and radial velocities (RVs).","Using PHOENIX stellar models, we find that the primary has a temperature of 3460 $\\pm$ 50 K a metallicity of 0.16 $\\pm$ 0.04, while the secondary has a temperature of 2510 $\\pm$ 50 K and a metallicity of $0.13\\substack{+0.12 \\\\ -0.11}$.","Recent work suggests this system is associated with the Hyades, placing it an older age than previous estimates.","Both metallicities agree with current $[Fe/H]$ Hyades measurements (0.11 -- 0.21).","Using stellar evolutionary models, we obtain significantly higher masses for the objects, of 0.30 $\\pm$ 0.15 $M_\\odot$ and 0.08 $\\pm$ 0.01 $M_\\odot$ (84 $\\pm$ 11 $M_{Jup}$) respectively.","Using the RVs and a new astrometry point from Keck/NIRC2, we find that the system is likely an edge-on, moderately eccentric ($0.41\\substack{+0.27 \\\\ -0.08}$) configuration.","We also estimate the C/O ratio of both objects using custom grid models, obtaining 0.42 $\\pm$ 0.10 (primary) and 0.55 $\\pm$ 0.10 (companion).","From these results, we confirm that this system most likely went through a binary star formation process in the Hyades.","The significant changes in this system's parameters since its discovery highlight the importance of high resolution spectroscopy for both orbital and atmospheric characterization of directly imaged companions."],"url":"http://arxiv.org/abs/2404.07742v1","category":"astro-ph.SR"}
{"created":"2024-04-11 13:38:32","title":"Quantization of optical quasinormal modes for spatially separated cavity systems with finite retardation","abstract":"A multi-cavity quantization scheme is developed using quasinormal modes (QNMs) of optical cavities embedded in a homogeneous background medium for cases where retardation is significant in the inter-cavity coupling. Using quantities that can be calculated in computational optics with numerical Maxwell solvers, we extend previous QNM quantization schemes and define a quantitative measure to determine if a separate quantization of QNM cavities is justified or if a joint quantization of the system is necessary. We test this measure for the examples of two coupled one-dimensional dielectric slabs and a dimer of metal nanorods acting as QNM cavities. For sufficiently large separations, the new scheme allows for an efficient treatment of multi-cavity phenomena using parameters defined for the individual cavities. Formulating the Hamiltonian in a familiar system-bath form, the scheme connects the rigorous QNM theory and widespread phenomenological models of open cavities coupled to a shared photonic bath with parameters obtained directly from Maxwell calculations.","sentences":["A multi-cavity quantization scheme is developed using quasinormal modes (QNMs) of optical cavities embedded in a homogeneous background medium for cases where retardation is significant in the inter-cavity coupling.","Using quantities that can be calculated in computational optics with numerical Maxwell solvers, we extend previous QNM quantization schemes and define a quantitative measure to determine if a separate quantization of QNM cavities is justified or if a joint quantization of the system is necessary.","We test this measure for the examples of two coupled one-dimensional dielectric slabs and a dimer of metal nanorods acting as QNM cavities.","For sufficiently large separations, the new scheme allows for an efficient treatment of multi-cavity phenomena using parameters defined for the individual cavities.","Formulating the Hamiltonian in a familiar system-bath form, the scheme connects the rigorous QNM theory and widespread phenomenological models of open cavities coupled to a shared photonic bath with parameters obtained directly from Maxwell calculations."],"url":"http://arxiv.org/abs/2404.07741v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 13:35:57","title":"Global regularity of 2D Rayleigh-B\u00e9nard equations with logarithmic supercritical dissipation","abstract":"In this paper, we study the global regularity problem for the 2D Rayleigh-B\\'{e}nard equations with logarithmic supercritical dissipation. By exploiting a combined quantity of the system, the technique of Littlewood-Paley decomposition and Besov spaces, and some commutator estimates, we establish the global regularity of a strong solution to this equations in the Sobolev space $H^{s}(\\mathbb{R}^{2})$ for $s \\ge2$.","sentences":["In this paper, we study the global regularity problem for the 2D Rayleigh-B\\'{e}nard equations with logarithmic supercritical dissipation.","By exploiting a combined quantity of the system, the technique of Littlewood-Paley decomposition and Besov spaces, and some commutator estimates, we establish the global regularity of a strong solution to this equations in the Sobolev space $H^{s}(\\mathbb{R}^{2})$ for $s \\ge2$."],"url":"http://arxiv.org/abs/2404.07737v1","category":"math.AP"}
{"created":"2024-04-11 13:25:18","title":"High speed stars: III. Detailed abundances and binary nature of the extreme speed star GHS143","abstract":"The Gaia satellite has provided the community with three releases containing astrometrical and photometric data as well as by products, such as stellar parameters and variability indicators. By selecting in the Gaia database, one can select stars with the requested characteristics, such as high speed. At present any selection is based on available Gaia releases including a subset of the observations. This, for some stars, can show some limitations, for example there is still not a sufficient number of observations to detect binarity. We investigated a star selected in Gaia EDR3 for its high speed that appears unbound to the Galaxy. We requested high-quality spectra to derive more information on the star. {From the spectroscopic investigation we confirm the low metallicity content of the star, and we derive a detailed chemical composition. The star is poor in carbon and very rich in oxygen: [(C+N+O)/Fe]=+0.65. From the two spectra observed we conclude that the star is in a binary system and from the investigation of the ionisation balance we derive that the star is closer than implied by the Gaia DR3 parallax, and thus has a a lower intrinsic luminosity. The star is probably still unbound, but there is the possibility that it is bound to the Galaxy. Its low carbon abundance suggests that the star was formed in a dwarf galaxy.","sentences":["The Gaia satellite has provided the community with three releases containing astrometrical and photometric data as well as by products, such as stellar parameters and variability indicators.","By selecting in the Gaia database, one can select stars with the requested characteristics, such as high speed.","At present any selection is based on available Gaia releases including a subset of the observations.","This, for some stars, can show some limitations, for example there is still not a sufficient number of observations to detect binarity.","We investigated a star selected in Gaia EDR3 for its high speed that appears unbound to the Galaxy.","We requested high-quality spectra to derive more information on the star.","{From the spectroscopic investigation we confirm the low metallicity content of the star, and we derive a detailed chemical composition.","The star is poor in carbon and very rich in oxygen: [(C+N+O)/Fe]=+0.65.","From the two spectra observed we conclude that the star is in a binary system and from the investigation of the ionisation balance we derive that the star is closer than implied by the Gaia DR3 parallax, and thus has a a lower intrinsic luminosity.","The star is probably still unbound, but there is the possibility that it is bound to the Galaxy.","Its low carbon abundance suggests that the star was formed in a dwarf galaxy."],"url":"http://arxiv.org/abs/2404.07731v1","category":"astro-ph.SR"}
{"created":"2024-04-11 13:19:45","title":"$(\\log t)^\\frac{2}{3}$-superdiffusivity for the 2d stochastic Burgers equation","abstract":"The Stochastic Burgers equation was introduced in [H. van Beijeren, R. Kutner and H. Spohn, Excess noise for driven diffusive systems, PRL, 1985] as a continuous approximation of the fluctuations of the asymmetric simple exclusion process. It is formally given by $$\\partial_t\\eta =\\frac{1}{2}\\Delta\\eta+ \\mathfrak w\\cdot\\nabla(\\eta^2) + \\nabla\\cdot\\xi,$$ where $\\xi$ is $d$-dimensional space time white noise and $\\mathfrak w$ is a fixed non-zero vector. In the critical dimension $d=2$ at stationarity, we show that this system exhibits superdiffusve behaviour: more specifically, its bulk diffusion coefficient behaves like $(\\log t)^\\frac23$, in a Tauberian sense, up to $\\log\\log\\log t$ corrections. This confirms a prediction made in the physics literature and complements [G. Cannizzarro, M. Gubinelli, F. Toninelli, Gaussian Fluctuations for the stochastic Burgers equation in dimension $d\\geq 2$, CMP, 2024], where the same equation was studied in the weak-coupling regime. Furthermore this model can be seen as a continuous analogue to [H.T. Yau, $(\\log t)^\\frac{2}{3}$ law of the two dimensional asymmetric simple exclusion process, Annals of Mathematics, 2004].","sentences":["The Stochastic Burgers equation was introduced in [H. van Beijeren, R. Kutner and H. Spohn, Excess noise for driven diffusive systems, PRL, 1985] as a continuous approximation of the fluctuations of the asymmetric simple exclusion process.","It is formally given by $$\\partial_t\\eta =\\frac{1}{2}\\Delta\\eta+ \\mathfrak w\\cdot\\nabla(\\eta^2)","+ \\nabla\\cdot\\xi,$$ where $\\xi$ is $d$-dimensional space time white noise and $\\mathfrak w$ is a fixed non-zero vector.","In the critical dimension $d=2$ at stationarity, we show that this system exhibits superdiffusve behaviour: more specifically, its bulk diffusion coefficient behaves like $(\\log t)^\\frac23$, in a Tauberian sense, up to $\\log\\log\\log t$ corrections.","This confirms a prediction made in the physics literature and complements","[G. Cannizzarro, M. Gubinelli, F. Toninelli, Gaussian Fluctuations for the stochastic Burgers equation in dimension $d\\geq 2$, CMP, 2024], where the same equation was studied in the weak-coupling regime.","Furthermore this model can be seen as a continuous analogue to [H.T. Yau, $(\\log t)^\\frac{2}{3}$ law of the two dimensional asymmetric simple exclusion process, Annals of Mathematics, 2004]."],"url":"http://arxiv.org/abs/2404.07728v1","category":"math.PR"}
{"created":"2024-04-11 13:11:37","title":"Trainable Joint Channel Estimation, Detection and Decoding for MIMO URLLC Systems","abstract":"The receiver design for multi-input multi-output (MIMO) ultra-reliable and low-latency communication (URLLC) systems can be a tough task due to the use of short channel codes and few pilot symbols. Consequently, error propagation can occur in traditional turbo receivers, leading to performance degradation. Moreover, the processing delay induced by information exchange between different modules may also be undesirable for URLLC. To address the issues, we advocate to perform joint channel estimation, detection, and decoding (JCDD) for MIMO URLLC systems encoded by short low-density parity-check (LDPC) codes. Specifically, we develop two novel JCDD problem formulations based on the maximum a posteriori (MAP) criterion for Gaussian MIMO channels and sparse mmWave MIMO channels, respectively, which integrate the pilots, the bit-to-symbol mapping, the LDPC code constraints, as well as the channel statistical information. Both the challenging large-scale non-convex problems are then solved based on the alternating direction method of multipliers (ADMM) algorithms, where closed-form solutions are achieved in each ADMM iteration. Furthermore, two JCDD neural networks, called JCDDNet-G and JCDDNet-S, are built by unfolding the derived ADMM algorithms and introducing trainable parameters. It is interesting to find via simulations that the proposed trainable JCDD receivers can outperform the turbo receivers with affordable computational complexities.","sentences":["The receiver design for multi-input multi-output (MIMO) ultra-reliable and low-latency communication (URLLC) systems can be a tough task due to the use of short channel codes and few pilot symbols.","Consequently, error propagation can occur in traditional turbo receivers, leading to performance degradation.","Moreover, the processing delay induced by information exchange between different modules may also be undesirable for URLLC.","To address the issues, we advocate to perform joint channel estimation, detection, and decoding (JCDD) for MIMO URLLC systems encoded by short low-density parity-check (LDPC) codes.","Specifically, we develop two novel JCDD problem formulations based on the maximum a posteriori (MAP) criterion for Gaussian MIMO channels and sparse mmWave MIMO channels, respectively, which integrate the pilots, the bit-to-symbol mapping, the LDPC code constraints, as well as the channel statistical information.","Both the challenging large-scale non-convex problems are then solved based on the alternating direction method of multipliers (ADMM) algorithms, where closed-form solutions are achieved in each ADMM iteration.","Furthermore, two JCDD neural networks, called JCDDNet-G and JCDDNet-S, are built by unfolding the derived ADMM algorithms and introducing trainable parameters.","It is interesting to find via simulations that the proposed trainable JCDD receivers can outperform the turbo receivers with affordable computational complexities."],"url":"http://arxiv.org/abs/2404.07721v1","category":"eess.SP"}
{"created":"2024-04-11 13:09:30","title":"Two liquid states of distinguishable helium-4: the existence of another non-superfluid frozen by heating","abstract":"We demonstrate that there can exist two liquid states in distinguishable helium-4 ($^4$He) obeying Boltzmann statistics. This is an indication of quantum liquid polyamorphism induced by nuclear quantum effect. For 0.08-3.3 K and 1-500 bar, we extensively conducted the isothermal-isobaric path integral centroid molecular dynamics simulations to explore not only possible states and state diagram but the state characteristics. The distinguishable $^4$He below 25 bar does not freeze down to 0.08 K even though it includes no Bosonic exchange effect and therefore no Bose condensation. One liquid state, low quantum-dispersion liquid (LQDL), is nearly identical to normal liquid He-I of real $^4$He. The other is high quantum-dispersion liquid (HQDL) consisting of atoms with longer quantum wavelength. This is another non-superfluid existing below 0.5 K or the temperatures of LQDL. The HQDL is also a low-entropy and fragile liquid to exhibit, unlike conventional liquids, rather gas-like relaxation of velocity autocorrelation function, while there the atoms diffuse without noticeable contribution from quantum tunneling. The LQDL-HQDL transition is not a thermodynamic phase transition but a continuous crossover accompanied by the change of the expansion factor of quantum wavelength. Freezing of HQDL into the low quantum-dispersion amorphous solid occurs by heating from 0.2 to 0.3 K at 40-50 bar, while this $P$-$T$ condition coincides with the Kim-Chan normal-supersolid phase boundary of real $^4$He. It is suggested that HQDL has relevance to the non-superfluid states of confined subnano-scale $^4$He systems which are semi-Boltzmann-like owing to the suppression of Bosonic correlation.","sentences":["We demonstrate that there can exist two liquid states in distinguishable helium-4 ($^4$He) obeying Boltzmann statistics.","This is an indication of quantum liquid polyamorphism induced by nuclear quantum effect.","For 0.08-3.3 K and 1-500 bar, we extensively conducted the isothermal-isobaric path integral centroid molecular dynamics simulations to explore not only possible states and state diagram but the state characteristics.","The distinguishable $^4$He below 25 bar does not freeze down to 0.08 K even though it includes no Bosonic exchange effect and therefore no Bose condensation.","One liquid state, low quantum-dispersion liquid (LQDL), is nearly identical to normal liquid He-I of real $^4$He.","The other is high quantum-dispersion liquid (HQDL) consisting of atoms with longer quantum wavelength.","This is another non-superfluid existing below 0.5 K or the temperatures of LQDL.","The HQDL is also a low-entropy and fragile liquid to exhibit, unlike conventional liquids, rather gas-like relaxation of velocity autocorrelation function, while there the atoms diffuse without noticeable contribution from quantum tunneling.","The LQDL-HQDL transition is not a thermodynamic phase transition but a continuous crossover accompanied by the change of the expansion factor of quantum wavelength.","Freezing of HQDL into the low quantum-dispersion amorphous solid occurs by heating from 0.2 to 0.3 K at 40-50 bar, while this $P$-$T$ condition coincides with the Kim-Chan normal-supersolid phase boundary of real $^4$He.","It is suggested that HQDL has relevance to the non-superfluid states of confined subnano-scale $^4$He systems which are semi-Boltzmann-like owing to the suppression of Bosonic correlation."],"url":"http://arxiv.org/abs/2404.07716v1","category":"physics.chem-ph"}
{"created":"2024-04-11 12:49:30","title":"Learning Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces and Random Features","abstract":"A method for learning Hamiltonian dynamics from a limited and noisy dataset is proposed. The method learns a Hamiltonian vector field on a reproducing kernel Hilbert space (RKHS) of inherently Hamiltonian vector fields, and in particular, odd Hamiltonian vector fields. This is done with a symplectic kernel, and it is shown how the kernel can be modified to an odd symplectic kernel to impose the odd symmetry. A random feature approximation is developed for the proposed kernel to reduce the problem size. This includes random feature approximations for odd kernels. The performance of the method is validated in simulations for three Hamiltonian systems. It is demonstrated that the use of an odd symplectic kernel improves prediction accuracy, and that the learned vector fields are Hamiltonian and exhibit the imposed odd symmetry characteristics.","sentences":["A method for learning Hamiltonian dynamics from a limited and noisy dataset is proposed.","The method learns a Hamiltonian vector field on a reproducing kernel Hilbert space (RKHS) of inherently Hamiltonian vector fields, and in particular, odd Hamiltonian vector fields.","This is done with a symplectic kernel, and it is shown how the kernel can be modified to an odd symplectic kernel to impose the odd symmetry.","A random feature approximation is developed for the proposed kernel to reduce the problem size.","This includes random feature approximations for odd kernels.","The performance of the method is validated in simulations for three Hamiltonian systems.","It is demonstrated that the use of an odd symplectic kernel improves prediction accuracy, and that the learned vector fields are Hamiltonian and exhibit the imposed odd symmetry characteristics."],"url":"http://arxiv.org/abs/2404.07703v1","category":"cs.LG"}
{"created":"2024-04-11 12:48:15","title":"Correctness of Flow Migration Across Network Function Instances","abstract":"Network Functions (NFs) improve the safety and efficiency of networks. Flows traversing NFs may need to be migrated to balance load, conserve energy, etc. When NFs are stateful, the information stored on the NF per flow must be migrated before the flows are migrated, to avoid problems of consistency. We examine what it means to correctly migrate flows from a stateful NF instance. We define the property of Weak-O, where only the state information required for packets to be correctly forwarded is migrated first, while the remaining states are eventually migrated. Weak-O can be preserved without buffering or dropping packets, unlike existing algorithms. We propose an algorithm that preserves Weak-O and prove its correctness. Even though this may cause packet re-ordering, we experimentally demonstrate that the goodputs with and without migration are comparable when the old and new paths have the same delays and bandwidths, or when the new path has larger bandwidth or at most 5 times longer delays, thus making this practical, contrary to what was thought before. We also prove that no criterion stronger than Weak-O can be preserved in a flow migration system that requires no buffering or dropping of packets and eventually synchronizes its states.","sentences":["Network Functions (NFs) improve the safety and efficiency of networks.","Flows traversing NFs may need to be migrated to balance load, conserve energy, etc.","When NFs are stateful, the information stored on the NF per flow must be migrated before the flows are migrated, to avoid problems of consistency.","We examine what it means to correctly migrate flows from a stateful NF instance.","We define the property of Weak-O, where only the state information required for packets to be correctly forwarded is migrated first, while the remaining states are eventually migrated.","Weak-O can be preserved without buffering or dropping packets, unlike existing algorithms.","We propose an algorithm that preserves Weak-O and prove its correctness.","Even though this may cause packet re-ordering, we experimentally demonstrate that the goodputs with and without migration are comparable when the old and new paths have the same delays and bandwidths, or when the new path has larger bandwidth or at most 5 times longer delays, thus making this practical, contrary to what was thought before.","We also prove that no criterion stronger than Weak-O can be preserved in a flow migration system that requires no buffering or dropping of packets and eventually synchronizes its states."],"url":"http://arxiv.org/abs/2404.07701v1","category":"cs.NI"}
{"created":"2024-04-11 12:44:05","title":"Monge Amp\u00e8re gravity: from the large deviation principle to cosmological simulations through optimal transport","abstract":"We study Monge-Amp\\`ere gravity (MAG) as an effective theory of cosmological structure formation through optimal transport theory. MAG is based on the Monge-Amp\\`ere equation, a nonlinear version of the Poisson equation, that relates the Hessian determinant of the potential to the density field. We explain how MAG emerges from a conditioned system of independent and indistinguishable Brownian particles, through the large deviation principle, in the continuum limit. To numerically explore this highly non-linear theory, we develop a novel N-body simulation method based on semi-discrete optimal transport. Our results obtained from the very first N-body simulation of Monge-Amp\\`ere gravity with over 100 millions particles show that on large scales, Monge-Amp\\`ere gravity is similar to the Newtonian gravity but favours the formation of anisotropic structures such as filaments. At small scales, MAG has a weaker clustering and is screened in high-density regions. Although here we study the Monge-Amp\\`ere gravity as an effective rather than a fundamental theory, our novel highly-performant optimal transport algorithm can be used to run high-resolution simulations of a large class of modified theories of gravity, such as Galileons, in which the equations of motion are second-order and of Monge-Amp\\`ere type.","sentences":["We study Monge-Amp\\`ere gravity (MAG) as an effective theory of cosmological structure formation through optimal transport theory.","MAG is based on the Monge-Amp\\`ere equation, a nonlinear version of the Poisson equation, that relates the Hessian determinant of the potential to the density field.","We explain how MAG emerges from a conditioned system of independent and indistinguishable Brownian particles, through the large deviation principle, in the continuum limit.","To numerically explore this highly non-linear theory, we develop a novel N-body simulation method based on semi-discrete optimal transport.","Our results obtained from the very first N-body simulation of Monge-Amp\\`ere gravity with over 100 millions particles show that on large scales, Monge-Amp\\`ere gravity is similar to the Newtonian gravity but favours the formation of anisotropic structures such as filaments.","At small scales, MAG has a weaker clustering and is screened in high-density regions.","Although here we study the Monge-Amp\\`ere gravity as an effective rather than a fundamental theory, our novel highly-performant optimal transport algorithm can be used to run high-resolution simulations of a large class of modified theories of gravity, such as Galileons, in which the equations of motion are second-order and of Monge-Amp\\`ere type."],"url":"http://arxiv.org/abs/2404.07697v1","category":"astro-ph.CO"}
{"created":"2024-04-11 12:34:31","title":"SWI-FEED: Smart Water IoT Framework for Evaluation of Energy and Data in Massive Scenarios","abstract":"This paper presents a comprehensive framework designed to facilitate the widespread deployment of the Internet of Things (IoT) for enhanced monitoring and optimization of Water Distribution Systems (WDSs). The framework aims to investigate the utilization of massive IoT in monitoring and optimizing WDSs, with a particular focus on leakage detection, energy consumption and wireless network performance assessment in real-world water networks. The framework integrates simulation environments at both the application level (using EPANET) and the radio level (using NS-3) within the LoRaWAN network. The paper culminates with a practical use case, alongside evaluation results concerning power consumption in a large-scale LoRaWAN network and strategies for optimal gateway positioning.","sentences":["This paper presents a comprehensive framework designed to facilitate the widespread deployment of the Internet of Things (IoT) for enhanced monitoring and optimization of Water Distribution Systems (WDSs).","The framework aims to investigate the utilization of massive IoT in monitoring and optimizing WDSs, with a particular focus on leakage detection, energy consumption and wireless network performance assessment in real-world water networks.","The framework integrates simulation environments at both the application level (using EPANET) and the radio level (using NS-3) within the LoRaWAN network.","The paper culminates with a practical use case, alongside evaluation results concerning power consumption in a large-scale LoRaWAN network and strategies for optimal gateway positioning."],"url":"http://arxiv.org/abs/2404.07692v1","category":"cs.NI"}
{"created":"2024-04-11 12:34:10","title":"Integrating On-demand Ride-sharing with Mass Transit at-Scale","abstract":"We are in the midst of a technology-driven transformation of the urban mobility landscape. However, unfortunately these new innovations are still dominated by car-centric personal mobility, which leads to concerns such as environmental sustainability, congestion, and equity. On the other hand, mass transit provides a means to move large amounts of travelers very efficiently, but is not very versatile and depends on an adequate concentration of demand. In this context, our overarching goal is to explore opportunities for new technologies such as ride-sharing to integrate with mass transit and provide a better service. More specifically, we envision a hybrid system that uses on-demand shuttles in conjunction with mass transit to move passengers efficiently, and provide an algorithmic framework for operational optimization. Our approach extends a state-of-the-art trip-vehicle assignment model to the multi-modal setting, where we develop a new integer-linear programming formulation to solve the problem efficiently. A comprehensive study covering five major cities in the United States based on real-world data is carried out to verify the advantages of such a system and the effectiveness of our algorithms. We show that our hybrid system provides significant improvements in comparison to a purely on-demand model by exploiting the efficiencies of the mass transit system.","sentences":["We are in the midst of a technology-driven transformation of the urban mobility landscape.","However, unfortunately these new innovations are still dominated by car-centric personal mobility, which leads to concerns such as environmental sustainability, congestion, and equity.","On the other hand, mass transit provides a means to move large amounts of travelers very efficiently, but is not very versatile and depends on an adequate concentration of demand.","In this context, our overarching goal is to explore opportunities for new technologies such as ride-sharing to integrate with mass transit and provide a better service.","More specifically, we envision a hybrid system that uses on-demand shuttles in conjunction with mass transit to move passengers efficiently, and provide an algorithmic framework for operational optimization.","Our approach extends a state-of-the-art trip-vehicle assignment model to the multi-modal setting, where we develop a new integer-linear programming formulation to solve the problem efficiently.","A comprehensive study covering five major cities in the United States based on real-world data is carried out to verify the advantages of such a system and the effectiveness of our algorithms.","We show that our hybrid system provides significant improvements in comparison to a purely on-demand model by exploiting the efficiencies of the mass transit system."],"url":"http://arxiv.org/abs/2404.07691v1","category":"cs.DM"}
{"created":"2024-04-11 12:22:18","title":"Maximum and minimum causal effects of quantum processes","abstract":"We introduce two quantitative measures of the strength of causal relations. These two measures capture the maximum and minimum changes in a quantum system induced by changes in another system. We show that both measures possess important properties, such as continuity and faithfulness, and can be evaluated through optimization over orthogonal input states. For the maximum causal effect, we provide numerical lower bounds based on a variational algorithm, which can be used to estimate the strength of causal relations without performing a full quantum process tomography. To illustrate the application of our algorithm, we analyze two paradigmatic examples, the first involving a coherent superposition of direct cause and common cause and the second involving communication through a coherent superposition of two completely depolarizing channels.","sentences":["We introduce two quantitative measures of the strength of causal relations.","These two measures capture the maximum and minimum changes in a quantum system induced by changes in another system.","We show that both measures possess important properties, such as continuity and faithfulness, and can be evaluated through optimization over orthogonal input states.","For the maximum causal effect, we provide numerical lower bounds based on a variational algorithm, which can be used to estimate the strength of causal relations without performing a full quantum process tomography.","To illustrate the application of our algorithm, we analyze two paradigmatic examples, the first involving a coherent superposition of direct cause and common cause and the second involving communication through a coherent superposition of two completely depolarizing channels."],"url":"http://arxiv.org/abs/2404.07683v1","category":"quant-ph"}
{"created":"2024-04-11 12:11:12","title":"Dynamic modeling and simulation of a flash clay calciner","abstract":"We present a novel dynamic model of a flash clay calciner. The model consists of thermophysical properties, reaction kinetics and stoichiometry, transport, mass and energy balances, and algebraic constraints. This gives rise to a system of partial differential-algebraic equations (PDAE). Spatial discretization is performed to convert the PDAEs into a system of differential-algebraic equations (DAE). The model can be used, for example, to perform dynamic simulations with changing inputs, and process design and optimization. Moreover, it can be used to develop model-based control, which is relevant for flexible operation of a clay calcination plant for green cement production.","sentences":["We present a novel dynamic model of a flash clay calciner.","The model consists of thermophysical properties, reaction kinetics and stoichiometry, transport, mass and energy balances, and algebraic constraints.","This gives rise to a system of partial differential-algebraic equations (PDAE).","Spatial discretization is performed to convert the PDAEs into a system of differential-algebraic equations (DAE).","The model can be used, for example, to perform dynamic simulations with changing inputs, and process design and optimization.","Moreover, it can be used to develop model-based control, which is relevant for flexible operation of a clay calcination plant for green cement production."],"url":"http://arxiv.org/abs/2404.07674v1","category":"eess.SY"}
{"created":"2024-04-11 12:02:18","title":"Venus","abstract":"After decades of relative neglect, interest in Venus surges anew in the planetary science community and the public. New missions are planned and selected, and will pave the way to the decade of Venus, as new observations allow us to uncover some of the many mysteries our closest Solar System neighbor still harbors. Building on the legacy of past works, here, we discuss the state of our understanding of Venus, from both observation and modeling. We describe each of the envelopes of the planet, from its atmosphere to its interior with an eye for the most recent advances and current topics of interest. We then briefly discuss coupled modelling efforts to better constrain the evolution of the planet. Finally, we describe how the upcoming missions and concepts will further lift the veil on Venus' secrets.","sentences":["After decades of relative neglect, interest in Venus surges anew in the planetary science community and the public.","New missions are planned and selected, and will pave the way to the decade of Venus, as new observations allow us to uncover some of the many mysteries our closest Solar System neighbor still harbors.","Building on the legacy of past works, here, we discuss the state of our understanding of Venus, from both observation and modeling.","We describe each of the envelopes of the planet, from its atmosphere to its interior with an eye for the most recent advances and current topics of interest.","We then briefly discuss coupled modelling efforts to better constrain the evolution of the planet.","Finally, we describe how the upcoming missions and concepts will further lift the veil on Venus' secrets."],"url":"http://arxiv.org/abs/2404.07669v1","category":"astro-ph.EP"}
{"created":"2024-04-11 12:00:06","title":"Dealing with Subject Similarity in Differential Morphing Attack Detection","abstract":"The advent of morphing attacks has posed significant security concerns for automated Face Recognition systems, raising the pressing need for robust and effective Morphing Attack Detection (MAD) methods able to effectively address this issue. In this paper, we focus on Differential MAD (D-MAD), where a trusted live capture, usually representing the criminal, is compared with the document image to classify it as morphed or bona fide. We show these approaches based on identity features are effective when the morphed image and the live one are sufficiently diverse; unfortunately, the effectiveness is significantly reduced when the same approaches are applied to look-alike subjects or in all those cases when the similarity between the two compared images is high (e.g. comparison between the morphed image and the accomplice). Therefore, in this paper, we propose ACIdA, a modular D-MAD system, consisting of a module for the attempt type classification, and two modules for the identity and artifacts analysis on input images. Successfully addressing this task would allow broadening the D-MAD applications including, for instance, the document enrollment stage, which currently relies entirely on human evaluation, thus limiting the possibility of releasing ID documents with manipulated images, as well as the automated gates to detect both accomplices and criminals. An extensive cross-dataset experimental evaluation conducted on the introduced scenario shows that ACIdA achieves state-of-the-art results, outperforming literature competitors, while maintaining good performance in traditional D-MAD benchmarks.","sentences":["The advent of morphing attacks has posed significant security concerns for automated Face Recognition systems, raising the pressing need for robust and effective Morphing Attack Detection (MAD) methods able to effectively address this issue.","In this paper, we focus on Differential MAD (D-MAD), where a trusted live capture, usually representing the criminal, is compared with the document image to classify it as morphed or bona fide.","We show these approaches based on identity features are effective when the morphed image and the live one are sufficiently diverse; unfortunately, the effectiveness is significantly reduced when the same approaches are applied to look-alike subjects or in all those cases when the similarity between the two compared images is high (e.g. comparison between the morphed image and the accomplice).","Therefore, in this paper, we propose ACIdA, a modular D-MAD system, consisting of a module for the attempt type classification, and two modules for the identity and artifacts analysis on input images.","Successfully addressing this task would allow broadening the D-MAD applications including, for instance, the document enrollment stage, which currently relies entirely on human evaluation, thus limiting the possibility of releasing ID documents with manipulated images, as well as the automated gates to detect both accomplices and criminals.","An extensive cross-dataset experimental evaluation conducted on the introduced scenario shows that ACIdA achieves state-of-the-art results, outperforming literature competitors, while maintaining good performance in traditional D-MAD benchmarks."],"url":"http://arxiv.org/abs/2404.07667v1","category":"cs.CV"}
{"created":"2024-04-11 11:56:40","title":"On the elliptic harmonic mappings and sense-preserving harmonic mappings","abstract":"In this paper, we first establish two versions of Landau-Bloch type theorem for $(K,K')$-elliptic harmonic mappings with a bounded minimum distortion. Next, we provide several coefficient estimates and a conjecture for $(K,K')$-elliptic harmonic mappings. Then, we establish three new versions of Landau-Bloch type theorem for sense-preserving harmonic mappings. Finally, we establish two sharp versions of Landau-Bloch type theorem for certain harmonic mappings. These results are sharp in some given cases and improve the related results of different authors.","sentences":["In this paper, we first establish two versions of Landau-Bloch type theorem for $(K,K')$-elliptic harmonic mappings with a bounded minimum distortion.","Next, we provide several coefficient estimates and a conjecture for $(K,K')$-elliptic harmonic mappings.","Then, we establish three new versions of Landau-Bloch type theorem for sense-preserving harmonic mappings.","Finally, we establish two sharp versions of Landau-Bloch type theorem for certain harmonic mappings.","These results are sharp in some given cases and improve the related results of different authors."],"url":"http://arxiv.org/abs/2404.07666v1","category":"math.CV"}
{"created":"2024-04-11 11:44:20","title":"Enhancing Valuation of Variable Annuities in L\u00e9vy Models with Stochastic Interest Rate","abstract":"This paper extends the valuation and optimal surrender framework for variable annuities with guaranteed minimum benefits in a L\\'evy equity market environment by incorporating a stochastic interest rate described by the Hull-White model. This approach frames a more dynamic and realistic financial setting compared to previous literature. We exploit a robust valuation mechanism employing a hybrid numerical method that merges tree methods for interest rate modeling with finite difference techniques for the underlying asset price. This method is particularly effective for addressing the complexities of variable annuities, where periodic fees and mortality risks are significant factors. Our findings reveal the influence of stochastic interest rates on the strategic decision-making process concerning the surrender of these financial instruments. Through comprehensive numerical experiments, and by comparing our results with those obtained through the Longstaff-Schwartz Monte Carlo method, we illustrate how our refined model can guide insurers in designing contracts that equitably balance the interests of both parties. This is particularly relevant in discouraging premature surrenders while adapting to the realistic fluctuations of financial markets. Lastly, a comparative statics analysis with varying interest rate parameters underscores the impact of interest rates on the cost of the optimal surrender strategy, emphasizing the importance of accurately modeling stochastic interest rates.","sentences":["This paper extends the valuation and optimal surrender framework for variable annuities with guaranteed minimum benefits in a L\\'evy equity market environment by incorporating a stochastic interest rate described by the Hull-White model.","This approach frames a more dynamic and realistic financial setting compared to previous literature.","We exploit a robust valuation mechanism employing a hybrid numerical method that merges tree methods for interest rate modeling with finite difference techniques for the underlying asset price.","This method is particularly effective for addressing the complexities of variable annuities, where periodic fees and mortality risks are significant factors.","Our findings reveal the influence of stochastic interest rates on the strategic decision-making process concerning the surrender of these financial instruments.","Through comprehensive numerical experiments, and by comparing our results with those obtained through the Longstaff-Schwartz Monte Carlo method, we illustrate how our refined model can guide insurers in designing contracts that equitably balance the interests of both parties.","This is particularly relevant in discouraging premature surrenders while adapting to the realistic fluctuations of financial markets.","Lastly, a comparative statics analysis with varying interest rate parameters underscores the impact of interest rates on the cost of the optimal surrender strategy, emphasizing the importance of accurately modeling stochastic interest rates."],"url":"http://arxiv.org/abs/2404.07658v1","category":"q-fin.PR"}
{"created":"2024-04-11 11:43:39","title":"Krylov complexity and gluon cascades in the high energy limit","abstract":"We point out an interesting connection between the mathematical framework of the Krylov basis used for quantifying quantum complexity and entanglement entropy in high-energy QCD. In particular, we observe that the cascade equation of the dipole model is equivalent to the $SL(2,R)$ Schrodinger equation in the Krylov basis. Consequently, the Krylov complexity corresponds to the average distribution of partons and Krylov entropy matches entanglement entropy computations of \\cite{Kharzeev:2017qzs}. Our work not only brings new tools for exploring quantum information and complexity in QCD but also gives hope for experimental tests of some of the recent, physical probes of quantum complexity.","sentences":["We point out an interesting connection between the mathematical framework of the Krylov basis used for quantifying quantum complexity and entanglement entropy in high-energy QCD.","In particular, we observe that the cascade equation of the dipole model is equivalent to the $SL(2,R)$ Schrodinger equation in the Krylov basis.","Consequently, the Krylov complexity corresponds to the average distribution of partons and Krylov entropy matches entanglement entropy computations of \\cite{Kharzeev:2017qzs}.","Our work not only brings new tools for exploring quantum information and complexity in QCD but also gives hope for experimental tests of some of the recent, physical probes of quantum complexity."],"url":"http://arxiv.org/abs/2404.07657v1","category":"hep-ph"}
{"created":"2024-04-11 11:07:56","title":"2DLIW-SLAM:2D LiDAR-Inertial-Wheel Odometry with Real-Time Loop Closure","abstract":"Due to budgetary constraints, indoor navigation typically employs 2D LiDAR rather than 3D LiDAR. However, the utilization of 2D LiDAR in Simultaneous Localization And Mapping (SLAM) frequently encounters challenges related to motion degeneracy, particularly in geometrically similar environments. To address this problem, this paper proposes a robust, accurate, and multi-sensor-fused 2D LiDAR SLAM system specifically designed for indoor mobile robots. To commence, the original LiDAR data undergoes meticulous processing through point and line extraction. Leveraging the distinctive characteristics of indoor environments, line-line constraints are established to complement other sensor data effectively, thereby augmenting the overall robustness and precision of the system. Concurrently, a tightly-coupled front-end is created, integrating data from the 2D LiDAR, IMU, and wheel odometry, thus enabling real-time state estimation. Building upon this solid foundation, a novel global feature point matching-based loop closure detection algorithm is proposed. This algorithm proves highly effective in mitigating front-end accumulated errors and ultimately constructs a globally consistent map. The experimental results indicate that our system fully meets real-time requirements. When compared to Cartographer, our system not only exhibits lower trajectory errors but also demonstrates stronger robustness, particularly in degeneracy problem.","sentences":["Due to budgetary constraints, indoor navigation typically employs 2D LiDAR rather than 3D LiDAR.","However, the utilization of 2D LiDAR in Simultaneous Localization And Mapping (SLAM) frequently encounters challenges related to motion degeneracy, particularly in geometrically similar environments.","To address this problem, this paper proposes a robust, accurate, and multi-sensor-fused 2D LiDAR SLAM system specifically designed for indoor mobile robots.","To commence, the original LiDAR data undergoes meticulous processing through point and line extraction.","Leveraging the distinctive characteristics of indoor environments, line-line constraints are established to complement other sensor data effectively, thereby augmenting the overall robustness and precision of the system.","Concurrently, a tightly-coupled front-end is created, integrating data from the 2D LiDAR, IMU, and wheel odometry, thus enabling real-time state estimation.","Building upon this solid foundation, a novel global feature point matching-based loop closure detection algorithm is proposed.","This algorithm proves highly effective in mitigating front-end accumulated errors and ultimately constructs a globally consistent map.","The experimental results indicate that our system fully meets real-time requirements.","When compared to Cartographer, our system not only exhibits lower trajectory errors but also demonstrates stronger robustness, particularly in degeneracy problem."],"url":"http://arxiv.org/abs/2404.07644v1","category":"cs.RO"}
{"created":"2024-04-11 11:07:17","title":"Tuning Magnetic and Optical Properties in MnxZn1-xPS3 Single Crystals by the Alloying Composition","abstract":"The exploration of two-dimensional (2D) antiferromagnetic (AFM) materials has shown great promise and interest in tuning the magnetic and electronic properties as well as studying magneto-optical effects. The current work investigates the control of magneto-optical interactions in alloyed MnxZn1-xPS3 lamellar semiconductor single crystals, with the Mn/Zn ratio regulating the coupling strength. Magnetic susceptibility results show a retention of AFM order followed by a decrease in N\\'eel temperatures down to ~ 40% Mn concentration, below which a paramagnetic behavior is observed. Absorption measurements reveal an increase in bandgap energy with higher Zn(II) concentration, and the presence of Mn(II) d-d transition below the absorption edge. DFT+U approach qualitatively explained the origin and the position of the experimentally observed mid band-gap states in pure MnPS3, and corresponding peaks visible in the alloyed systems MnxZn1-xPS3. Accordingly, emission at 1.3 eV in all alloyed compounds results from recombination from a 4T1g Mn(II) excited state to a hybrid p-d state at the valence band. Most significant, temperature-dependent photoluminescence (PL) intensity trends demonstrate strong magneto-optical coupling in compositions with x > 0.65. This study underscores the potential of tailored alloy compositions as a means to control magnetic and optical properties in 2D materials, paving the way for advances in spin-based technologies.","sentences":["The exploration of two-dimensional (2D) antiferromagnetic (AFM) materials has shown great promise and interest in tuning the magnetic and electronic properties as well as studying magneto-optical effects.","The current work investigates the control of magneto-optical interactions in alloyed MnxZn1-xPS3 lamellar semiconductor single crystals, with the Mn/Zn ratio regulating the coupling strength.","Magnetic susceptibility results show a retention of AFM order followed by a decrease in N\\'eel temperatures down to ~ 40% Mn concentration, below which a paramagnetic behavior is observed.","Absorption measurements reveal an increase in bandgap energy with higher Zn(II) concentration, and the presence of Mn(II) d-d transition below the absorption edge.","DFT+U approach qualitatively explained the origin and the position of the experimentally observed mid band-gap states in pure MnPS3, and corresponding peaks visible in the alloyed systems MnxZn1-xPS3.","Accordingly, emission at 1.3 eV in all alloyed compounds results from recombination from a 4T1g Mn(II) excited state to a hybrid p-d state at the valence band.","Most significant, temperature-dependent photoluminescence (PL) intensity trends demonstrate strong magneto-optical coupling in compositions with x > 0.65.","This study underscores the potential of tailored alloy compositions as a means to control magnetic and optical properties in 2D materials, paving the way for advances in spin-based technologies."],"url":"http://arxiv.org/abs/2404.07643v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 11:03:11","title":"Kinematics $\\&$ Star Formation in the Hub-Filament System G6.55-0.1","abstract":"Hub-filament systems (HFSs) being the potential sites of formation of star clusters and high mass stars, provide a test bed for the current theories that attempt to explain star formation globally. It is thus important to study a large number of HFSs using both intensity and velocity information to constrain these objects better observationally. We present here a study of the hub-filament system associated with G6.55-0.1 using newly obtained observations of radio continuum and $J$=2--1 transition of CO, $^{13}$CO, and C$^{18}$O. The radio continuum maps show multiple peaks that coincide with far-infrared dust continuum peaks indicating the presence of more than one young massive stars in the hub of the HFS. We used the velocity information from the C$^{18}$O(2--1) map to (a) show that the source G6.55-0.1 is not physically associated with the SNR W28 and (b) disentangle and identify the velocity components genuinely associated with G6.55-0.1. Among the velocity-coherent structures identified, the two filaments at 13.8 and 17.3 km s$^{-1}$ contribute a total mass accretion rate of $\\sim$3000 M$_{\\odot}$ Myr$^{-1}$ to the hub. Both the filaments also show V-shaped structure, characteristic of gravitational collapse, in their velocity profile at the location of the hub. Estimated mass per unit length of the segments of the filaments are smaller than the critical line masses derived from virial equilibrium considerations. This suggests that while the filaments are not gravitationally collapsing as a whole, the spectra from the hub indicate that the inner parts are dynamically decoupled and collapsing to form stars.","sentences":["Hub-filament systems (HFSs) being the potential sites of formation of star clusters and high mass stars, provide a test bed for the current theories that attempt to explain star formation globally.","It is thus important to study a large number of HFSs using both intensity and velocity information to constrain these objects better observationally.","We present here a study of the hub-filament system associated with G6.55-0.1 using newly obtained observations of radio continuum and $J$=2--1 transition of CO, $^{13}$CO, and C$^{18}$O.","The radio continuum maps show multiple peaks that coincide with far-infrared dust continuum peaks indicating the presence of more than one young massive stars in the hub of the HFS.","We used the velocity information from the C$^{18}$O(2--1) map to (a) show that the source G6.55-0.1 is not physically associated with the SNR W28 and (b) disentangle and identify the velocity components genuinely associated with G6.55-0.1.","Among the velocity-coherent structures identified, the two filaments at 13.8 and 17.3 km s$^{-1}$ contribute a total mass accretion rate of $\\sim$3000 M$_{\\odot}$ Myr$^{-1}$ to the hub.","Both the filaments also show V-shaped structure, characteristic of gravitational collapse, in their velocity profile at the location of the hub.","Estimated mass per unit length of the segments of the filaments are smaller than the critical line masses derived from virial equilibrium considerations.","This suggests that while the filaments are not gravitationally collapsing as a whole, the spectra from the hub indicate that the inner parts are dynamically decoupled and collapsing to form stars."],"url":"http://arxiv.org/abs/2404.07640v1","category":"astro-ph.GA"}
{"created":"2024-04-11 10:53:37","title":"Preservation of scalar spin chirality across a metallic spacer in synthetic antiferromagnets with chiral interlayer interactions","abstract":"Chiral magnetic textures are highly attractive for the development of spintronic devices, due to their properties to store and transport information in an energy efficient manner. In multilayered systems, these are typically stabilized via the interfacial intralayer Dzyaloshinskii-Moriya interaction (DMI), which leads to configurations with both vector and scalar spin chirality. Additionally, it has been recently observed that DMI may also promote interlayer vector spin chirality, coupling spins in different magnetic layers via non-magnetic spacer layers, an effect referred to as interlayer DMI (IL-DMI).   Here, we present evidence of the preservation of interlayer scalar spin chirality across the non-magnetic metallic spacer of a synthetic antiferromagnet (SAF) with orthogonal anisotropy axes and IL-DMI. We draw this conclusion from a combined comprehensive experimental and computational study using scattering and imaging X-ray techniques. We find a direct relationship between the sign of the net out-of plane orientation of one of the layers, and the preferential in-plane vector spin chirality in the other layer after its demagnetization. Current IL-DMI models and other mechanisms which could potentially yield such chiral configurations fail in explaining these experimental results. This work thus uncovers a new flavor of chiral interlayer interactions, demanding the further development of theoretical models in order to describe the microscopic origin of the effect.","sentences":["Chiral magnetic textures are highly attractive for the development of spintronic devices, due to their properties to store and transport information in an energy efficient manner.","In multilayered systems, these are typically stabilized via the interfacial intralayer Dzyaloshinskii-Moriya interaction (DMI), which leads to configurations with both vector and scalar spin chirality.","Additionally, it has been recently observed that DMI may also promote interlayer vector spin chirality, coupling spins in different magnetic layers via non-magnetic spacer layers, an effect referred to as interlayer DMI (IL-DMI).   ","Here, we present evidence of the preservation of interlayer scalar spin chirality across the non-magnetic metallic spacer of a synthetic antiferromagnet (SAF) with orthogonal anisotropy axes and IL-DMI.","We draw this conclusion from a combined comprehensive experimental and computational study using scattering and imaging X-ray techniques.","We find a direct relationship between the sign of the net out-of plane orientation of one of the layers, and the preferential in-plane vector spin chirality in the other layer after its demagnetization.","Current IL-DMI models and other mechanisms which could potentially yield such chiral configurations fail in explaining these experimental results.","This work thus uncovers a new flavor of chiral interlayer interactions, demanding the further development of theoretical models in order to describe the microscopic origin of the effect."],"url":"http://arxiv.org/abs/2404.07637v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 10:52:21","title":"Dual Quaternion Control of UAVs with Cable-suspended Load","abstract":"Modeling the kinematics and dynamics of robotics systems with suspended loads using dual quaternions has not been explored so far. This paper introduces a new innovative control strategy using dual quaternions for UAVs with cable-suspended loads, focusing on the sling load lifting and tracking problems. By utilizing the mathematical efficiency and compactness of dual quaternions, a unified representation of the UAV and its suspended load's dynamics and kinematics is achieved, facilitating the realization of load lifting and trajectory tracking. The simulation results have tested the proposed strategy's accuracy, efficiency, and robustness. This study makes a substantial contribution to present this novel control strategy that harnesses the benefits of dual quaternions for cargo UAVs. Our work also holds promise for inspiring future innovations in under-actuated systems control using dual quaternions.","sentences":["Modeling the kinematics and dynamics of robotics systems with suspended loads using dual quaternions has not been explored so far.","This paper introduces a new innovative control strategy using dual quaternions for UAVs with cable-suspended loads, focusing on the sling load lifting and tracking problems.","By utilizing the mathematical efficiency and compactness of dual quaternions, a unified representation of the UAV and its suspended load's dynamics and kinematics is achieved, facilitating the realization of load lifting and trajectory tracking.","The simulation results have tested the proposed strategy's accuracy, efficiency, and robustness.","This study makes a substantial contribution to present this novel control strategy that harnesses the benefits of dual quaternions for cargo UAVs.","Our work also holds promise for inspiring future innovations in under-actuated systems control using dual quaternions."],"url":"http://arxiv.org/abs/2404.07635v1","category":"cs.RO"}
{"created":"2024-04-11 10:48:23","title":"Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models","abstract":"We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.","sentences":["We propose a family of tests of the validity of the assumptions underlying independent component analysis methods.","The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests.","Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies.","This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components.","A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives.","A real-data application demonstrates the practical utility and effectiveness of the method."],"url":"http://arxiv.org/abs/2404.07632v1","category":"stat.ME"}
{"created":"2024-04-11 10:33:51","title":"Higher order spin interactions mediated by the substrate","abstract":"Since the seminal works in the fifties it has been known that a bath of non-interacting electrons mediates an interaction between local moments (such as nuclear spins or magnetic impurities) coupled to distant sites of the lattice. Recent efforts in simultaneous control of multi-qubit arrays relies on defining quantum dots in environments which likewise contain electrons capable of mediating effective interactions. It is therefore interesting to analyze systems with more than two impurities in higher orders of perturbation theory. Here we derive and discuss the significance of the effective spin interactions between four impurities in fourth order of the coupling between the impurities and the substrate, mediated by conduction electrons of a two-dimensional square lattice. This effective interaction resembles the ring exchange - a coupling of four spins appearing in perturbative treatment of the Hubbard model. Despite it being a higher-order effect, it can dramatically influence the magnetic ordering. We show that as an effective interaction between impurity spins, the ring exchange can also compete with the more familiar two-spin interactions.","sentences":["Since the seminal works in the fifties it has been known that a bath of non-interacting electrons mediates an interaction between local moments (such as nuclear spins or magnetic impurities) coupled to distant sites of the lattice.","Recent efforts in simultaneous control of multi-qubit arrays relies on defining quantum dots in environments which likewise contain electrons capable of mediating effective interactions.","It is therefore interesting to analyze systems with more than two impurities in higher orders of perturbation theory.","Here we derive and discuss the significance of the effective spin interactions between four impurities in fourth order of the coupling between the impurities and the substrate, mediated by conduction electrons of a two-dimensional square lattice.","This effective interaction resembles the ring exchange - a coupling of four spins appearing in perturbative treatment of the Hubbard model.","Despite it being a higher-order effect, it can dramatically influence the magnetic ordering.","We show that as an effective interaction between impurity spins, the ring exchange can also compete with the more familiar two-spin interactions."],"url":"http://arxiv.org/abs/2404.07628v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 10:03:50","title":"On homotopy properties of solutions of some differential inclusions in the $W^{1,p}$-topology","abstract":"We consider a differential inclusion on a manifold, defined by a field of open half-spaces whose boundary in each tangent space is the kernel of a one-form. We make the assumption that the corank one distribution associated to the kernel is completely nonholonomic. We identify a subset of solutions of the differential inclusion, satisfying two endpoints and periodic boundary conditions, which are homotopically equivalent in the $W^{1,p}$-topology, for any $p\\in [1,+\\infty)$, to the based loop space and the free loop space respectively.","sentences":["We consider a differential inclusion on a manifold, defined by a field of open half-spaces whose boundary in each tangent space is the kernel of a one-form.","We make the assumption that the corank one distribution associated to the kernel is completely nonholonomic.","We identify a subset of solutions of the differential inclusion, satisfying two endpoints and periodic boundary conditions, which are homotopically equivalent in the $W^{1,p}$-topology, for any $p\\in [1,+\\infty)$, to the based loop space and the free loop space respectively."],"url":"http://arxiv.org/abs/2404.07614v1","category":"math.DS"}
{"created":"2024-04-11 09:56:51","title":"Achieving violation-free distributed optimization under coupling constraints","abstract":"Constraint satisfaction is a critical component in a wide range of engineering applications, including but not limited to safe multi-agent control and economic dispatch in power systems. This study explores violation-free distributed optimization techniques for problems characterized by separable objective functions and coupling constraints. First, we incorporate auxiliary decision variables together with a network-dependent linear mapping to each coupling constraint. For the reformulated problem, we show that the projection of its feasible set onto the space of primal variables is identical to that of the original problem, which is the key to achieving all-time constraint satisfaction. Upon treating the reformulated problem as a min-min optimization problem with respect to auxiliary and primal variables, we demonstrate that the gradients in the outer minimization problem have a locally computable closed-form. Then, two violation-free distributed optimization algorithms are developed and their convergence under reasonable assumptions is analyzed. Finally, the proposed algorithm is applied to implement a control barrier function based controller in a distributed manner, and the results verify its effectiveness.","sentences":["Constraint satisfaction is a critical component in a wide range of engineering applications, including but not limited to safe multi-agent control and economic dispatch in power systems.","This study explores violation-free distributed optimization techniques for problems characterized by separable objective functions and coupling constraints.","First, we incorporate auxiliary decision variables together with a network-dependent linear mapping to each coupling constraint.","For the reformulated problem, we show that the projection of its feasible set onto the space of primal variables is identical to that of the original problem, which is the key to achieving all-time constraint satisfaction.","Upon treating the reformulated problem as a min-min optimization problem with respect to auxiliary and primal variables, we demonstrate that the gradients in the outer minimization problem have a locally computable closed-form.","Then, two violation-free distributed optimization algorithms are developed and their convergence under reasonable assumptions is analyzed.","Finally, the proposed algorithm is applied to implement a control barrier function based controller in a distributed manner, and the results verify its effectiveness."],"url":"http://arxiv.org/abs/2404.07609v1","category":"math.OC"}
{"created":"2024-04-11 09:47:05","title":"Novel Active Sensing and Inference for mmWave Beam Alignment Using Single RF Chain Systems","abstract":"We propose a novel sensing approach for the beam alignment problem in millimeter wave systems using a single Radio Frequency (RF) chain. Conventionally, beam alignment using a single phased array involves comparing beamformer output power across different spatial regions. This incurs large training overhead due to the need to perform the beam scan operation. The proposed Synthesis of Virtual Array Manifold (SVAM) sensing methodology is inspired from synthetic aperture radar systems and realizes a virtual array geometry over temporal measurements. We demonstrate the benefits of SVAM using Cram\\'er-Rao bound (CRB) analysis over schemes that repeat beam pattern to boost signal-to-noise (SNR) ratio. We also showcase versatile applicability of the proposed SVAM sensing by incorporating it within existing beam alignment procedures that assume perfect knowledge of the small-scale fading coefficient. We further consider the practical scenario wherein we estimate the fading coefficient and propose a novel beam alignment procedure based on efficient computation of an approximate posterior density on dominant path angle. We provide numerical experiments to study the impact of parameters involved in the procedure. The performance of the proposed sensing and beam alignment algorithm is empirically observed to approach the fading coefficient-perfectly known performance, even at low SNR.","sentences":["We propose a novel sensing approach for the beam alignment problem in millimeter wave systems using a single Radio Frequency (RF) chain.","Conventionally, beam alignment using a single phased array involves comparing beamformer output power across different spatial regions.","This incurs large training overhead due to the need to perform the beam scan operation.","The proposed Synthesis of Virtual Array Manifold (SVAM) sensing methodology is inspired from synthetic aperture radar systems and realizes a virtual array geometry over temporal measurements.","We demonstrate the benefits of SVAM using Cram\\'er-Rao bound (CRB) analysis over schemes that repeat beam pattern to boost signal-to-noise (SNR) ratio.","We also showcase versatile applicability of the proposed SVAM sensing by incorporating it within existing beam alignment procedures that assume perfect knowledge of the small-scale fading coefficient.","We further consider the practical scenario wherein we estimate the fading coefficient and propose a novel beam alignment procedure based on efficient computation of an approximate posterior density on dominant path angle.","We provide numerical experiments to study the impact of parameters involved in the procedure.","The performance of the proposed sensing and beam alignment algorithm is empirically observed to approach the fading coefficient-perfectly known performance, even at low SNR."],"url":"http://arxiv.org/abs/2404.07604v1","category":"eess.SP"}
{"created":"2024-04-11 09:41:14","title":"Attention based End to end network for Offline Writer Identification on Word level data","abstract":"Writer identification due to its widespread application in various fields has gained popularity over the years. In scenarios where optimum handwriting samples are available, whether they be in the form of a single line, a sentence, or an entire page, writer identification algorithms have demonstrated noteworthy levels of accuracy. However, in scenarios where only a limited number of handwritten samples are available, particularly in the form of word images, there is a significant scope for improvement.   In this paper, we propose a writer identification system based on an attention-driven Convolutional Neural Network (CNN). The system is trained utilizing image segments, known as fragments, extracted from word images, employing a pyramid-based strategy. This methodology enables the system to capture a comprehensive representation of the data, encompassing both fine-grained details and coarse features across various levels of abstraction. These extracted fragments serve as the training data for the convolutional network, enabling it to learn a more robust representation compared to traditional convolution-based networks trained on word images. Additionally, the paper explores the integration of an attention mechanism to enhance the representational power of the learned features. The efficacy of the proposed algorithm is evaluated on three benchmark databases, demonstrating its proficiency in writer identification tasks, particularly in scenarios with limited access to handwriting data.","sentences":["Writer identification due to its widespread application in various fields has gained popularity over the years.","In scenarios where optimum handwriting samples are available, whether they be in the form of a single line, a sentence, or an entire page, writer identification algorithms have demonstrated noteworthy levels of accuracy.","However, in scenarios where only a limited number of handwritten samples are available, particularly in the form of word images, there is a significant scope for improvement.   ","In this paper, we propose a writer identification system based on an attention-driven Convolutional Neural Network (CNN).","The system is trained utilizing image segments, known as fragments, extracted from word images, employing a pyramid-based strategy.","This methodology enables the system to capture a comprehensive representation of the data, encompassing both fine-grained details and coarse features across various levels of abstraction.","These extracted fragments serve as the training data for the convolutional network, enabling it to learn a more robust representation compared to traditional convolution-based networks trained on word images.","Additionally, the paper explores the integration of an attention mechanism to enhance the representational power of the learned features.","The efficacy of the proposed algorithm is evaluated on three benchmark databases, demonstrating its proficiency in writer identification tasks, particularly in scenarios with limited access to handwriting data."],"url":"http://arxiv.org/abs/2404.07602v1","category":"cs.CV"}
{"created":"2024-04-11 09:34:09","title":"Optical cavity characterization with a mode-matched heterodyne sensing scheme","abstract":"We describe a technique for measuring the complex reflectivity of an optical cavity with a resonant local oscillator laser and an auxiliary probe laser, each coupled via opposite ends of the cavity. A heterodyne sensing scheme is then used to observe the phase and amplitude of the interference beat-note between the promptly reflected field and the cavity transmitted field injected through the far mirror. Since the local oscillator laser must pass through the cavity before interfering with the probe laser these measurements are not only independent of the spatial coupling of either laser to the cavity, but also obtained at the in-situ position of the cavity Eigenmode. This technique was demonstrated on a 19 m cavity to measure the individual transmissivities of each of the mirrors as well as the round trip optical losses to an accuracy of several parts per million.","sentences":["We describe a technique for measuring the complex reflectivity of an optical cavity with a resonant local oscillator laser and an auxiliary probe laser, each coupled via opposite ends of the cavity.","A heterodyne sensing scheme is then used to observe the phase and amplitude of the interference beat-note between the promptly reflected field and the cavity transmitted field injected through the far mirror.","Since the local oscillator laser must pass through the cavity before interfering with the probe laser these measurements are not only independent of the spatial coupling of either laser to the cavity, but also obtained at the in-situ position of the cavity Eigenmode.","This technique was demonstrated on a 19 m cavity to measure the individual transmissivities of each of the mirrors as well as the round trip optical losses to an accuracy of several parts per million."],"url":"http://arxiv.org/abs/2404.07597v1","category":"physics.optics"}
{"created":"2024-04-11 09:26:14","title":"The zeros of random sections of real vector bundles","abstract":"We define integral geometric analogues of the Chern classes for real vector bundle on a smooth real variety. More precisely, we define the Chern densities of a real bundle. These densities are analogues of the Chern forms of a complex vector bundle and inherit some of their properties.   \\noindent (The text is a summary of a report on the conference PCA'2024 in Euler International Mathematical Institute, St. Petersburg)","sentences":["We define integral geometric analogues of the Chern classes for real vector bundle on a smooth real variety.","More precisely, we define the Chern densities of a real bundle.","These densities are analogues of the Chern forms of a complex vector bundle and inherit some of their properties.   ","\\noindent (The text is a summary of a report on the conference PCA'2024 in Euler International Mathematical Institute, St. Petersburg)"],"url":"http://arxiv.org/abs/2404.07596v1","category":"math.AG"}
{"created":"2024-04-11 09:17:12","title":"UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs","abstract":"Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements. The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher's workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration. UltraEval is now available for researchers publicly~\\footnote{Website is at \\url{https://github.com/OpenBMB/UltraEval}}.","sentences":["Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements.","The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment.","However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy.","Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher's workflows.","This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency.","We identify and reimplement three core components of model evaluation (models, data, and metrics).","The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow.","Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration.","UltraEval is now available for researchers publicly~\\footnote{Website is at \\url{https://github.com/OpenBMB/UltraEval}}."],"url":"http://arxiv.org/abs/2404.07584v1","category":"cs.CL"}
{"created":"2024-04-11 08:57:15","title":"Abundance theorem for minimal projective varieties satisfying Miyaoka's equality","abstract":"In this paper, we solve the abundance conjecture for minimal projective klt varieties $X$ satisfying Miyaoka's equality $3c_2(X) = c_{1}(X)^{2}$. Specifically, we prove that the canonical divisor $K_{X}$ is semi-ample and the Kodaira dimension $\\kappa(K_{X})$ is either $0$, $1$, or $2$. Moreover, according to the Kodaira dimension, we reveal the structure of the Iitaka fibration of $X$ up to quasi-\\'etale covers. Additionally, we show similar results for projective klt varieties with nef anti-canonical divisor.","sentences":["In this paper, we solve the abundance conjecture for minimal projective klt varieties $X$ satisfying Miyaoka's equality $3c_2(X) =","c_{1}(X)^{2}$.","Specifically, we prove that the canonical divisor $K_{X}$ is semi-ample and the Kodaira dimension $\\kappa(K_{X})$ is either $0$, $1$, or $2$. Moreover, according to the Kodaira dimension, we reveal the structure of the Iitaka fibration of $X$ up to quasi-\\'etale covers.","Additionally, we show similar results for projective klt varieties with nef anti-canonical divisor."],"url":"http://arxiv.org/abs/2404.07568v1","category":"math.AG"}
{"created":"2024-04-11 08:37:22","title":"Towards Secure and Reliable Heterogeneous Real-time Telemetry Communication in Autonomous UAV Swarms","abstract":"In the era of cutting-edge autonomous systems, Unmanned Aerial Vehicles (UAVs) are becoming an essential part of the solutions for numerous complex challenges. This paper evaluates UAV peer-to-peer telemetry communication, highlighting its security vulnerabilities and explores a transition to a het-erogeneous multi-hop mesh all-to-all communication architecture to increase inter-swarm connectivity and reliability. Additionally, we suggest a symmetric key agreement and data encryption mechanism implementation for inter - swarm communication, to ensure data integrity and confidentiality without compromising performance.","sentences":["In the era of cutting-edge autonomous systems, Unmanned Aerial Vehicles (UAVs) are becoming an essential part of the solutions for numerous complex challenges.","This paper evaluates UAV peer-to-peer telemetry communication, highlighting its security vulnerabilities and explores a transition to a het-erogeneous multi-hop mesh all-to-all communication architecture to increase inter-swarm connectivity and reliability.","Additionally, we suggest a symmetric key agreement and data encryption mechanism implementation for inter - swarm communication, to ensure data integrity and confidentiality without compromising performance."],"url":"http://arxiv.org/abs/2404.07557v1","category":"cs.CR"}
{"created":"2024-04-11 08:35:24","title":"SFSORT: Scene Features-based Simple Online Real-Time Tracker","abstract":"This paper introduces SFSORT, the world's fastest multi-object tracking system based on experiments conducted on MOT Challenge datasets. To achieve an accurate and computationally efficient tracker, this paper employs a tracking-by-detection method, following the online real-time tracking approach established in prior literature. By introducing a novel cost function called the Bounding Box Similarity Index, this work eliminates the Kalman Filter, leading to reduced computational requirements. Additionally, this paper demonstrates the impact of scene features on enhancing object-track association and improving track post-processing. Using a 2.2 GHz Intel Xeon CPU, the proposed method achieves an HOTA of 61.7\\% with a processing speed of 2242 Hz on the MOT17 dataset and an HOTA of 60.9\\% with a processing speed of 304 Hz on the MOT20 dataset. The tracker's source code, fine-tuned object detection model, and tutorials are available at \\url{https://github.com/gitmehrdad/SFSORT}.","sentences":["This paper introduces SFSORT, the world's fastest multi-object tracking system based on experiments conducted on MOT Challenge datasets.","To achieve an accurate and computationally efficient tracker, this paper employs a tracking-by-detection method, following the online real-time tracking approach established in prior literature.","By introducing a novel cost function called the Bounding Box Similarity Index, this work eliminates the Kalman Filter, leading to reduced computational requirements.","Additionally, this paper demonstrates the impact of scene features on enhancing object-track association and improving track post-processing.","Using a 2.2 GHz Intel Xeon CPU, the proposed method achieves an HOTA of 61.7\\% with a processing speed of 2242 Hz on the MOT17 dataset and an HOTA of 60.9\\% with a processing speed of 304 Hz on the MOT20 dataset.","The tracker's source code, fine-tuned object detection model, and tutorials are available at \\url{https://github.com/gitmehrdad/SFSORT}."],"url":"http://arxiv.org/abs/2404.07553v1","category":"cs.CV"}
{"created":"2024-04-11 08:34:20","title":"Correspondence Research of the Most Probable Transition Paths between a Stochastic Interacting Particle System and its Mean Field Limit System","abstract":"This paper derived the indirect approximation theorem of the most probable transition pathway of a stochastic interacting particle system in the mean field sense. This paper studied the problem of indirect approximation of the most probable transition pathway of an interacting particle system (i.e., a high-dimensional stochastic dynamic system) and its mean field limit equation (McKean-Vlasov stochastic differential equation). This study is based on the Onsager-Machlup action functional, reformulated the problem as an optimal control problem. With the stochastic Pontryagin's Maximum Principle, this paper completed the derivation. This paper proved the existence and uniqueness theorem of the solution to the mean field optimal control problem of McKean-Vlasov stochastic differential equations, and also established a system of equations satisfying the control parameters $\\theta^{*}$ and $\\theta^{N}$ respectively. There are few studies on the most probable transition pathways of stochastic interacting particle systems, it is still a great challenge to solve the most probable transition pathways directly or to approximate it with the mean field limit system. Therefore, this paper first gave the proof of correspondence between the core equation of Pontryagin's Maximum Principle, that is, Hamiltonian extreme condition equation. That is to say, this correspondence indirectly explain the correspondence between the most probable transition pathways of stochastic interacting particle systems and the mean field systems.","sentences":["This paper derived the indirect approximation theorem of the most probable transition pathway of a stochastic interacting particle system in the mean field sense.","This paper studied the problem of indirect approximation of the most probable transition pathway of an interacting particle system (i.e., a high-dimensional stochastic dynamic system) and its mean field limit equation (McKean-Vlasov stochastic differential equation).","This study is based on the Onsager-Machlup action functional, reformulated the problem as an optimal control problem.","With the stochastic Pontryagin's Maximum Principle, this paper completed the derivation.","This paper proved the existence and uniqueness theorem of the solution to the mean field optimal control problem of McKean-Vlasov stochastic differential equations, and also established a system of equations satisfying the control parameters $\\theta^{*}$ and $\\theta^{N}$ respectively.","There are few studies on the most probable transition pathways of stochastic interacting particle systems, it is still a great challenge to solve the most probable transition pathways directly or to approximate it with the mean field limit system.","Therefore, this paper first gave the proof of correspondence between the core equation of Pontryagin's Maximum Principle, that is, Hamiltonian extreme condition equation.","That is to say, this correspondence indirectly explain the correspondence between the most probable transition pathways of stochastic interacting particle systems and the mean field systems."],"url":"http://arxiv.org/abs/2404.07552v1","category":"math.DS"}
{"created":"2024-04-11 07:59:12","title":"EKF-SINDy: Empowering the extended Kalman filter with sparse identification of nonlinear dynamics","abstract":"Observed data from a dynamic system can be assimilated into a predictive model by means of Kalman filters. Nonlinear extensions of the Kalman filter, such as the Extended Kalman Filter (EKF), are required to enable the joint estimation of (possibly nonlinear) system dynamics and of input parameters. To construct the evolution model used in the prediction phase of the EKF, we propose to rely on the Sparse Identification of Nonlinear Dynamics (SINDy). The numerical integration of a SINDy model leads to great computational savings compared to alternate strategies based on, e.g., finite elements. Indeed, SINDy allows for the immediate definition of the Jacobian matrices required by the EKF to identify system dynamics and properties, a derivation that is usually extremely involved with physical models. As a result, combining the EKF with SINDy provides a computationally efficient, easy-to-apply approach for the identification of nonlinear systems, capable of robust operation even outside the range of training of SINDy. To demonstrate the potential of the approach, we address the identification of a linear non-autonomous system consisting of a shear building model excited by real seismograms, and the identification of a partially observed nonlinear system. The challenge arising from applying SINDy when the system state is not accessible has been relieved by means of time-delay embedding. The great accuracy and the small uncertainty associated with the state identification, where the state has been augmented to include system properties, underscores the great potential of the proposed strategy, paving the way for the development of predictive digital twins in different fields.","sentences":["Observed data from a dynamic system can be assimilated into a predictive model by means of Kalman filters.","Nonlinear extensions of the Kalman filter, such as the Extended Kalman Filter (EKF), are required to enable the joint estimation of (possibly nonlinear) system dynamics and of input parameters.","To construct the evolution model used in the prediction phase of the EKF, we propose to rely on the Sparse Identification of Nonlinear Dynamics (SINDy).","The numerical integration of a SINDy model leads to great computational savings compared to alternate strategies based on, e.g., finite elements.","Indeed, SINDy allows for the immediate definition of the Jacobian matrices required by the EKF to identify system dynamics and properties, a derivation that is usually extremely involved with physical models.","As a result, combining the EKF with SINDy provides a computationally efficient, easy-to-apply approach for the identification of nonlinear systems, capable of robust operation even outside the range of training of SINDy.","To demonstrate the potential of the approach, we address the identification of a linear non-autonomous system consisting of a shear building model excited by real seismograms, and the identification of a partially observed nonlinear system.","The challenge arising from applying SINDy when the system state is not accessible has been relieved by means of time-delay embedding.","The great accuracy and the small uncertainty associated with the state identification, where the state has been augmented to include system properties, underscores the great potential of the proposed strategy, paving the way for the development of predictive digital twins in different fields."],"url":"http://arxiv.org/abs/2404.07536v1","category":"math.DS"}
{"created":"2024-04-11 07:41:36","title":"Security Modelling for Cyber-Physical Systems: A Systematic Literature Review","abstract":"Cyber-physical systems (CPS) are at the intersection of digital technology and engineering domains, rendering them high-value targets of sophisticated and well-funded cybersecurity threat actors. Prominent cybersecurity attacks on CPS have brought attention to the vulnerability of these systems, and the soft underbelly of critical infrastructure reliant on CPS. Security modelling for CPS is an important mechanism to systematically identify and assess vulnerabilities, threats, and risks throughout system lifecycles, and to ultimately ensure system resilience, safety, and reliability. This literature review delves into state-of-the-art research in CPS security modelling, encompassing both threat and attack modelling. While these terms are sometimes used interchangeably, they are different concepts. This article elaborates on the differences between threat and attack modelling, examining their implications for CPS security. A systematic search yielded 428 articles, from which 15 were selected and categorised into three clusters: those focused on threat modelling methods, attack modelling methods, and literature reviews. Specifically, we sought to examine what security modelling methods exist today, and how they address real-world cybersecurity threats and CPS-specific attacker capabilities throughout the lifecycle of CPS, which typically span longer durations compared to traditional IT systems. This article also highlights several limitations in existing research, wherein security models adopt simplistic approaches that do not adequately consider the dynamic, multi-layer, multi-path, and multi-agent characteristics of real-world cyber-physical attacks.","sentences":["Cyber-physical systems (CPS) are at the intersection of digital technology and engineering domains, rendering them high-value targets of sophisticated and well-funded cybersecurity threat actors.","Prominent cybersecurity attacks on CPS have brought attention to the vulnerability of these systems, and the soft underbelly of critical infrastructure reliant on CPS.","Security modelling for CPS is an important mechanism to systematically identify and assess vulnerabilities, threats, and risks throughout system lifecycles, and to ultimately ensure system resilience, safety, and reliability.","This literature review delves into state-of-the-art research in CPS security modelling, encompassing both threat and attack modelling.","While these terms are sometimes used interchangeably, they are different concepts.","This article elaborates on the differences between threat and attack modelling, examining their implications for CPS security.","A systematic search yielded 428 articles, from which 15 were selected and categorised into three clusters: those focused on threat modelling methods, attack modelling methods, and literature reviews.","Specifically, we sought to examine what security modelling methods exist today, and how they address real-world cybersecurity threats and CPS-specific attacker capabilities throughout the lifecycle of CPS, which typically span longer durations compared to traditional IT systems.","This article also highlights several limitations in existing research, wherein security models adopt simplistic approaches that do not adequately consider the dynamic, multi-layer, multi-path, and multi-agent characteristics of real-world cyber-physical attacks."],"url":"http://arxiv.org/abs/2404.07527v1","category":"cs.CR"}
{"created":"2024-04-11 07:14:11","title":"Parameterized Complexity of Submodular Minimization under Uncertainty","abstract":"This paper studies the computational complexity of a robust variant of a two-stage submodular minimization problem that we call Robust Submodular Minimizer. In this problem, we are given $k$ submodular functions $f_1,\\dots,f_k$ over a set family $2^V$, which represent $k$ possible scenarios in the future when we will need to find an optimal solution for one of these scenarios, i.e., a minimizer for one of the functions. The present task is to find a set $X \\subseteq V$ that is close to some optimal solution for each $f_i$ in the sense that some minimizer of $f_i$ can be obtained from $X$ by adding/removing at most $d$ elements for a given integer $d$. The main contribution of this paper is to provide a complete computational map of this problem with respect to parameters $k$ and $d$, which reveals a tight complexity threshold for both parameters: (1) Robust Submodular Minimizer can be solved in polynomial time when $k \\leq 2$, but is NP-hard if $k$ is a constant with $k \\geq 3$. (2) Robust Submodular Minimizer can be solved in polynomial time when $d=0$, but is NP-hard if $d$ is a constant with $d \\geq 1$. (3) Robust Submodular Minimizer is fixed-parameter tractable when parameterized by $(k,d)$. We also show that if some submodular function $f_i$ has a polynomial number of minimizers, then the problem becomes fixed-parameter tractable when parameterized by $d$. We remark that all our hardness results hold even if each submodular function is given by a cut function of a directed graph.","sentences":["This paper studies the computational complexity of a robust variant of a two-stage submodular minimization problem that we call Robust Submodular Minimizer.","In this problem, we are given $k$ submodular functions $f_1,\\dots,f_k$ over a set family $2^V$, which represent $k$ possible scenarios in the future when we will need to find an optimal solution for one of these scenarios, i.e., a minimizer for one of the functions.","The present task is to find a set $X \\subseteq V$ that is close to some optimal solution for each $f_i$ in the sense that some minimizer of $f_i$ can be obtained from $X$ by adding/removing at most $d$ elements for a given integer $d$.","The main contribution of this paper is to provide a complete computational map of this problem with respect to parameters $k$ and $d$, which reveals a tight complexity threshold for both parameters: (1) Robust Submodular Minimizer can be solved in polynomial time when $k \\leq 2$, but is NP-hard if $k$ is a constant with $k \\geq 3$.","(2) Robust Submodular Minimizer can be solved in polynomial time when $d=0$, but is NP-hard if $d$ is a constant with $d \\geq 1$. (3) Robust Submodular Minimizer is fixed-parameter tractable when parameterized by $(k,d)$.","We also show that if some submodular function $f_i$ has a polynomial number of minimizers, then the problem becomes fixed-parameter tractable when parameterized by $d$. We remark that all our hardness results hold even if each submodular function is given by a cut function of a directed graph."],"url":"http://arxiv.org/abs/2404.07516v1","category":"cs.DS"}
{"created":"2024-04-11 07:13:59","title":"Stability in Phase Retrieval: Characterizing Condition Numbers and the Optimal Vector Set","abstract":"In this paper, we primarily focus on analyzing the stability property of phase retrieval by examining the bi-Lipschitz property of the map $\\Phi_{\\boldsymbol{A}}(\\boldsymbol{x})=|\\boldsymbol{A}\\boldsymbol{x}|\\in \\mathbb{R}_+^m$, where $\\boldsymbol{x}\\in \\mathbb{H}^d$ and $\\boldsymbol{A}\\in \\mathbb{H}^{m\\times d}$ is the measurement matrix for $\\mathbb{H}\\in\\{\\mathbb{R},\\mathbb{C}\\}$. We define the condition number $\\beta_{\\boldsymbol{A}}:=\\frac{U_{\\boldsymbol{A}}}{L_{\\boldsymbol{A}}}$, where $L_{\\boldsymbol{A}}$ and $U_{\\boldsymbol{A}}$ represent the optimal lower and upper Lipschitz constants, respectively. We establish the first universal lower bound on $\\beta_{\\boldsymbol{A}}$ by demonstrating that for any ${\\boldsymbol{A}}\\in\\mathbb{H}^{m\\times d}$, \\begin{equation*} \\beta_{\\boldsymbol{A}}\\geq \\beta_0^{\\mathbb{H}}:=\\begin{cases} \\sqrt{\\frac{\\pi}{\\pi-2}}\\,\\,\\approx\\,\\, 1.659 & \\text{if $\\mathbb{H}=\\mathbb{R}$,}\\\\ \\sqrt{\\frac{4}{4-\\pi}}\\,\\,\\approx\\,\\, 2.159 & \\text{if $\\mathbb{H}=\\mathbb{C}$.} \\end{cases} \\end{equation*} We prove that the condition number of a standard Gaussian matrix in $\\mathbb{H}^{m\\times d}$ asymptotically matches the lower bound $\\beta_0^{\\mathbb{H}}$ for both real and complex cases. This result indicates that the constant lower bound $\\beta_0^{\\mathbb{H}}$ is asymptotically tight, holding true for both the real and complex scenarios. As an application of this result, we utilize it to investigate the performance of quadratic models for phase retrieval. Lastly, we establish that for any odd integer $m\\geq 3$, the harmonic frame $\\boldsymbol{A}\\in \\mathbb{R}^{m\\times 2}$ possesses the minimum condition number among all $\\boldsymbol{A}\\in \\mathbb{R}^{m\\times 2}$. We are confident that these findings carry substantial implications for enhancing our understanding of phase retrieval.","sentences":["In this paper, we primarily focus on analyzing the stability property of phase retrieval by examining the bi-Lipschitz property of the map $\\Phi_{\\boldsymbol{A}}(\\boldsymbol{x})=|\\boldsymbol{A}\\boldsymbol{x}|\\in \\mathbb{R}_+^m$, where $\\boldsymbol{x}\\in \\mathbb{H}^d$ and $\\boldsymbol{A}\\in \\mathbb{H}^{m\\times d}$ is the measurement matrix for $\\mathbb{H}\\in\\{\\mathbb{R},\\mathbb{C}\\}$. We define the condition number $\\beta_{\\boldsymbol{A}}:=\\frac{U_{\\boldsymbol{A}}}{L_{\\boldsymbol{A}}}$, where $L_{\\boldsymbol{A}}$ and $U_{\\boldsymbol{A}}$ represent the optimal lower and upper Lipschitz constants, respectively.","We establish the first universal lower bound on $\\beta_{\\boldsymbol{A}}$ by demonstrating that for any ${\\boldsymbol{A}}\\in\\mathbb{H}^{m\\times d}$, \\begin{equation*} \\beta_{\\boldsymbol{A}}\\geq \\beta_0^{\\mathbb{H}}:=\\begin{cases} \\sqrt{\\frac{\\pi}{\\pi-2}}\\,\\,\\approx\\,\\, 1.659 & \\text{if $\\mathbb{H}=\\mathbb{R}$,}\\\\ \\sqrt{\\frac{4}{4-\\pi}}\\,\\,\\approx\\,\\, 2.159 & \\text{if $\\mathbb{H}=\\mathbb{C}$.} \\end{cases} \\end{equation*} We prove that the condition number of a standard Gaussian matrix in $\\mathbb{H}^{m\\times d}$ asymptotically matches the lower bound $\\beta_0^{\\mathbb{H}}$ for both real and complex cases.","This result indicates that the constant lower bound $\\beta_0^{\\mathbb{H}}$ is asymptotically tight, holding true for both the real and complex scenarios.","As an application of this result, we utilize it to investigate the performance of quadratic models for phase retrieval.","Lastly, we establish that for any odd integer $m\\geq 3$, the harmonic frame $\\boldsymbol{A}\\in \\mathbb{R}^{m\\times 2}$ possesses the minimum condition number among all $\\boldsymbol{A}\\in \\mathbb{R}^{m\\times","2}$.","We are confident that these findings carry substantial implications for enhancing our understanding of phase retrieval."],"url":"http://arxiv.org/abs/2404.07515v1","category":"cs.IT"}
{"created":"2024-04-11 07:08:11","title":"Numerical investigation of the quantum inverse algorithm on small molecules","abstract":"We evaluate the accuracy of the quantum inverse (Q-Inv) algorithm in which the multiplication of $\\hat{H}^{-k}$ to the reference wavefunction is replaced by the Fourier Transformed multiplication of $e^{-i\\lambda \\hat{H}}$, as a function of the integration parameters ($\\lambda$) and the power $k$ for various systems, including H$_2$, LiH, BeH$_2$ and the notorious H$_4$ molecule at single point. We further consider the possibility of employing the Gaussian-quadrature rule as an alternate integration method and compared it to the results employing trapezoidal integration. The Q-Inv algorithm is compared to the inverse iteration method using the $\\hat{H}^{-1}$ inverse (I-Iter) and the exact inverse by lower-upper decomposition (LU). Energy values are evaluated as the expectation values of the Hamiltonian. Results suggest that the Q-Inv method provides lower energy results than the I-Iter method up to a certain $k$, after which the energy increases due to errors in the numerical integration that are dependent of the integration interval. A combined Gaussian-quadrature and trapezoidal integration method proved to be more effective at reaching convergence while decreasing the number of operations. For systems like H$_4$, in which the Q-Inv can not reach the expected error threshold, we propose a combination of Q-Inv and I-Iter methods to further decrease the error with $k$ at lower computational cost. Finally, we summarize the recommended procedure when treating unknown systems.","sentences":["We evaluate the accuracy of the quantum inverse (Q-Inv) algorithm in which the multiplication of $\\hat{H}^{-k}$ to the reference wavefunction is replaced by the Fourier Transformed multiplication of $e^{-i\\lambda \\hat{H}}$, as a function of the integration parameters ($\\lambda$) and the power $k$ for various systems, including H$_2$, LiH, BeH$_2$ and the notorious H$_4$ molecule at single point.","We further consider the possibility of employing the Gaussian-quadrature rule as an alternate integration method and compared it to the results employing trapezoidal integration.","The Q-Inv algorithm is compared to the inverse iteration method using the $\\hat{H}^{-1}$ inverse (I-Iter) and the exact inverse by lower-upper decomposition (LU).","Energy values are evaluated as the expectation values of the Hamiltonian.","Results suggest that the Q-Inv method provides lower energy results than the I-Iter method up to a certain $k$, after which the energy increases due to errors in the numerical integration that are dependent of the integration interval.","A combined Gaussian-quadrature and trapezoidal integration method proved to be more effective at reaching convergence while decreasing the number of operations.","For systems like H$_4$, in which the Q-Inv can not reach the expected error threshold, we propose a combination of Q-Inv and I-Iter methods to further decrease the error with $k$ at lower computational cost.","Finally, we summarize the recommended procedure when treating unknown systems."],"url":"http://arxiv.org/abs/2404.07512v1","category":"physics.chem-ph"}
{"created":"2024-04-11 06:59:45","title":"Dynamic Suffix Array in Optimal Compressed Space","abstract":"Big data, encompassing extensive datasets, has seen rapid expansion, notably with a considerable portion being textual data, including strings and texts. Simple compression methods and standard data structures prove inadequate for processing these datasets, as they require decompression for usage or consume extensive memory resources. Consequently, this motivation has led to the development of compressed data structures that support various queries for a given string, typically operating in polylogarithmic time and utilizing compressed space proportional to the string's length. Notably, the suffix array (SA) query is a critical component in implementing a suffix tree, which has a broad spectrum of applications.   A line of research has been conducted on (especially, static) compressed data structures that support the SA query. A common finding from most of the studies is the suboptimal space efficiency of existing compressed data structures. Kociumaka, Navarro, and Prezza have made a significant contribution by introducing an asymptotically minimal space requirement, $O\\left(\\delta \\log\\frac{n\\log\\sigma}{\\delta\\log n} \\log n \\right)$ bits, sufficient to represent any string of length $n$, with an alphabet size of $\\sigma$, and substring complexity $\\delta$, serving as a measure of repetitiveness. The space is referred to as $\\delta$-optimal space. More recently, Kempa and Kociumaka presented $\\delta$-SA, a compressed data structure supporting SA queries in $\\delta$-optimal space. However, the data structures introduced thus far are static.   We present the first dynamic compressed data structure that supports the SA query and update in polylogarithmic time and $\\delta$-optimal space. More precisely, it can answer SA queries and perform updates in $O(\\log^7 n)$ and expected $O(\\log^8 n)$ time, respectively, using an expected $\\delta$-optimal space.","sentences":["Big data, encompassing extensive datasets, has seen rapid expansion, notably with a considerable portion being textual data, including strings and texts.","Simple compression methods and standard data structures prove inadequate for processing these datasets, as they require decompression for usage or consume extensive memory resources.","Consequently, this motivation has led to the development of compressed data structures that support various queries for a given string, typically operating in polylogarithmic time and utilizing compressed space proportional to the string's length.","Notably, the suffix array (SA) query is a critical component in implementing a suffix tree, which has a broad spectrum of applications.   ","A line of research has been conducted on (especially, static) compressed data structures that support the SA query.","A common finding from most of the studies is the suboptimal space efficiency of existing compressed data structures.","Kociumaka, Navarro, and Prezza have made a significant contribution by introducing an asymptotically minimal space requirement, $O\\left(\\delta \\log\\frac{n\\log\\sigma}{\\delta\\log n} \\log n \\right)$ bits, sufficient to represent any string of length $n$, with an alphabet size of $\\sigma$, and substring complexity $\\delta$, serving as a measure of repetitiveness.","The space is referred to as $\\delta$-optimal space.","More recently, Kempa and Kociumaka presented $\\delta$-SA, a compressed data structure supporting SA queries in $\\delta$-optimal space.","However, the data structures introduced thus far are static.   ","We present the first dynamic compressed data structure that supports the SA query and update in polylogarithmic time and $\\delta$-optimal space.","More precisely, it can answer SA queries and perform updates in $O(\\log^7 n)$ and expected $O(\\log^8 n)$ time, respectively, using an expected $\\delta$-optimal space."],"url":"http://arxiv.org/abs/2404.07510v1","category":"cs.DS"}
{"created":"2024-04-11 06:55:48","title":"An advanced 1D physics-based model for PEM hydrogen fuel cells with enhanced overvoltage prediction","abstract":"A one-dimensional, dynamic, two-phase, isothermal and finite-difference model of proton exchange membrane fuel cell (PEMFC) systems has been developed. It is distinct from most existing models which are either fast but imprecise, such as lumped-parameter models, or detailed but computationally intensive, such as computational fluid dynamics models. This model, partially validated using experimental polarisation curves, provides a comprehensive description of cell internal states while maintaining a low computational burden. Additionally, a new physical quantity, named the limit liquid water saturation coefficient ($s_{lim}$), is introduced in the overvoltage calculation equation. This quantity replaces the limit current density coefficient ($i_{lim}$) and establishes a connection between the voltage drop at high current densities, the amount of liquid water present in the catalyst layers of the cell, and the operating conditions. At high current densities, a significant amount of liquid water is generated, which limits the accessibility of reactants to certain triple point zones within the catalyst layers by covering them. This, in turn, increases overpotential. It has also been observed that $s_{lim}$ is influenced, at minimum, by the gas pressure imposed by the operator.","sentences":["A one-dimensional, dynamic, two-phase, isothermal and finite-difference model of proton exchange membrane fuel cell (PEMFC) systems has been developed.","It is distinct from most existing models which are either fast but imprecise, such as lumped-parameter models, or detailed but computationally intensive, such as computational fluid dynamics models.","This model, partially validated using experimental polarisation curves, provides a comprehensive description of cell internal states while maintaining a low computational burden.","Additionally, a new physical quantity, named the limit liquid water saturation coefficient ($s_{lim}$), is introduced in the overvoltage calculation equation.","This quantity replaces the limit current density coefficient ($i_{lim}$) and establishes a connection between the voltage drop at high current densities, the amount of liquid water present in the catalyst layers of the cell, and the operating conditions.","At high current densities, a significant amount of liquid water is generated, which limits the accessibility of reactants to certain triple point zones within the catalyst layers by covering them.","This, in turn, increases overpotential.","It has also been observed that $s_{lim}$ is influenced, at minimum, by the gas pressure imposed by the operator."],"url":"http://arxiv.org/abs/2404.07508v1","category":"eess.SY"}
{"created":"2024-04-11 06:55:44","title":"Learning to Classify New Foods Incrementally Via Compressed Exemplars","abstract":"Food image classification systems play a crucial role in health monitoring and diet tracking through image-based dietary assessment techniques. However, existing food recognition systems rely on static datasets characterized by a pre-defined fixed number of food classes. This contrasts drastically with the reality of food consumption, which features constantly changing data. Therefore, food image classification systems should adapt to and manage data that continuously evolves. This is where continual learning plays an important role. A challenge in continual learning is catastrophic forgetting, where ML models tend to discard old knowledge upon learning new information. While memory-replay algorithms have shown promise in mitigating this problem by storing old data as exemplars, they are hampered by the limited capacity of memory buffers, leading to an imbalance between new and previously learned data. To address this, our work explores the use of neural image compression to extend buffer size and enhance data diversity. We introduced the concept of continuously learning a neural compression model to adaptively improve the quality of compressed data and optimize the bitrates per pixel (bpp) to store more exemplars. Our extensive experiments, including evaluations on food-specific datasets including Food-101 and VFN-74, as well as the general dataset ImageNet-100, demonstrate improvements in classification accuracy. This progress is pivotal in advancing more realistic food recognition systems that are capable of adapting to continually evolving data. Moreover, the principles and methodologies we've developed hold promise for broader applications, extending their benefits to other domains of continual machine learning systems.","sentences":["Food image classification systems play a crucial role in health monitoring and diet tracking through image-based dietary assessment techniques.","However, existing food recognition systems rely on static datasets characterized by a pre-defined fixed number of food classes.","This contrasts drastically with the reality of food consumption, which features constantly changing data.","Therefore, food image classification systems should adapt to and manage data that continuously evolves.","This is where continual learning plays an important role.","A challenge in continual learning is catastrophic forgetting, where ML models tend to discard old knowledge upon learning new information.","While memory-replay algorithms have shown promise in mitigating this problem by storing old data as exemplars, they are hampered by the limited capacity of memory buffers, leading to an imbalance between new and previously learned data.","To address this, our work explores the use of neural image compression to extend buffer size and enhance data diversity.","We introduced the concept of continuously learning a neural compression model to adaptively improve the quality of compressed data and optimize the bitrates per pixel (bpp) to store more exemplars.","Our extensive experiments, including evaluations on food-specific datasets including Food-101 and VFN-74, as well as the general dataset ImageNet-100, demonstrate improvements in classification accuracy.","This progress is pivotal in advancing more realistic food recognition systems that are capable of adapting to continually evolving data.","Moreover, the principles and methodologies we've developed hold promise for broader applications, extending their benefits to other domains of continual machine learning systems."],"url":"http://arxiv.org/abs/2404.07507v1","category":"eess.IV"}
{"created":"2024-04-11 06:46:07","title":"Flexible Control of Chiral Superconductivity in Optically Driven Nodal Point Superconductors with Antiferromagnetism","abstract":"Recent studies have attracted widespread attention on magnet-superconductor hybrid systems with emergent topological superconductivity. Here, we present the Floquet engineering of realistic two-dimensional topological nodal-point superconductors that are composed of antiferromagnetic monolayers in proximity to an s-wave superconductor. We show that Floquet chiral topological superconductivity arises naturally due to light-induced breaking of the effective time-reversal symmetry. More strikingly, we find that the Floquet chiral topological superconducting phases can be flexibly controlled by irradiating elliptically polarized light, with the photon-dressed quasi-energy spectrum carrying different Chern numbers. Such optically switchable topological transition is attributed to the simultaneous creations (or annihilations) of valley pairs. Our findings provide a feasible approach for achieving the Floquet chiral topological superconductivity with flexible tunability, which would draw extensive attention in experiments.","sentences":["Recent studies have attracted widespread attention on magnet-superconductor hybrid systems with emergent topological superconductivity.","Here, we present the Floquet engineering of realistic two-dimensional topological nodal-point superconductors that are composed of antiferromagnetic monolayers in proximity to an s-wave superconductor.","We show that Floquet chiral topological superconductivity arises naturally due to light-induced breaking of the effective time-reversal symmetry.","More strikingly, we find that the Floquet chiral topological superconducting phases can be flexibly controlled by irradiating elliptically polarized light, with the photon-dressed quasi-energy spectrum carrying different Chern numbers.","Such optically switchable topological transition is attributed to the simultaneous creations (or annihilations) of valley pairs.","Our findings provide a feasible approach for achieving the Floquet chiral topological superconductivity with flexible tunability, which would draw extensive attention in experiments."],"url":"http://arxiv.org/abs/2404.07506v1","category":"cond-mat.supr-con"}
{"created":"2024-04-11 06:32:03","title":"Leveraging Data Augmentation for Process Information Extraction","abstract":"Business Process Modeling projects often require formal process models as a central component. High costs associated with the creation of such formal process models motivated many different fields of research aimed at automated generation of process models from readily available data. These include process mining on event logs, and generating business process models from natural language texts. Research in the latter field is regularly faced with the problem of limited data availability, hindering both evaluation and development of new techniques, especially learning-based ones.   To overcome this data scarcity issue, in this paper we investigate the application of data augmentation for natural language text data. Data augmentation methods are well established in machine learning for creating new, synthetic data without human assistance. We find that many of these methods are applicable to the task of business process information extraction, improving the accuracy of extraction. Our study shows, that data augmentation is an important component in enabling machine learning methods for the task of business process model generation from natural language text, where currently mostly rule-based systems are still state of the art. Simple data augmentation techniques improved the $F_1$ score of mention extraction by 2.9 percentage points, and the $F_1$ of relation extraction by $4.5$. To better understand how data augmentation alters human annotated texts, we analyze the resulting text, visualizing and discussing the properties of augmented textual data.   We make all code and experiments results publicly available.","sentences":["Business Process Modeling projects often require formal process models as a central component.","High costs associated with the creation of such formal process models motivated many different fields of research aimed at automated generation of process models from readily available data.","These include process mining on event logs, and generating business process models from natural language texts.","Research in the latter field is regularly faced with the problem of limited data availability, hindering both evaluation and development of new techniques, especially learning-based ones.   ","To overcome this data scarcity issue, in this paper we investigate the application of data augmentation for natural language text data.","Data augmentation methods are well established in machine learning for creating new, synthetic data without human assistance.","We find that many of these methods are applicable to the task of business process information extraction, improving the accuracy of extraction.","Our study shows, that data augmentation is an important component in enabling machine learning methods for the task of business process model generation from natural language text, where currently mostly rule-based systems are still state of the art.","Simple data augmentation techniques improved the $F_1$ score of mention extraction by 2.9 percentage points, and the $F_1$ of relation extraction by $4.5$. To better understand how data augmentation alters human annotated texts, we analyze the resulting text, visualizing and discussing the properties of augmented textual data.   ","We make all code and experiments results publicly available."],"url":"http://arxiv.org/abs/2404.07501v1","category":"cs.CL"}
{"created":"2024-04-11 06:11:26","title":"Multifractal Dimension Spectrum Analysis for Nuclear Density Distribution","abstract":"We present an integral density method for calculating the multifractal dimension spectrum for the nucleon distribution in atomic nuclei. This method is then applied to analyze the non-uniformity of the density distribution in several typical types of nuclear matter distributions, including the Woods-Saxon distribution, the halo structure and the tetrahedral $\\alpha$ clustering. The subsequent discussion provides a comprehensive and detailed exploration of the results obtained. The multifractal dimension spectrum shows remarkable sensitivity to the density distribution, establishing it as an effective tool for studying the distribution of nucleons in nuclear multibody systems.","sentences":["We present an integral density method for calculating the multifractal dimension spectrum for the nucleon distribution in atomic nuclei.","This method is then applied to analyze the non-uniformity of the density distribution in several typical types of nuclear matter distributions, including the Woods-Saxon distribution, the halo structure and the tetrahedral $\\alpha$ clustering.","The subsequent discussion provides a comprehensive and detailed exploration of the results obtained.","The multifractal dimension spectrum shows remarkable sensitivity to the density distribution, establishing it as an effective tool for studying the distribution of nucleons in nuclear multibody systems."],"url":"http://arxiv.org/abs/2404.07496v1","category":"nucl-th"}
{"created":"2024-04-11 06:04:05","title":"The Ideal Glass and the Ideal Disk Packing in Two Dimensions","abstract":"The ideal glass, a disordered system of particles with zero configurational entropy, cannot be realized through thermal processes. Nevertheless, we present a method for constructing ideal jammed packings of soft spheres, and thus the zero temperature ideal glass, in two dimensions. In line with the predicted properties, these critically jammed packings have high bulk and shear moduli as well as an anomalously high density. While the absence of pressure scaling in the shear moduli of crystalline materials is often attributed to the ordered nature of the particles, we show for the first time that disordered ideal packings also have this feature. We also find that the density of states avoids the low frequency power law scaling famously found in most amorphous materials. Finally, these configurations display hyperuniformity. In addition to resolving a long-standing mystery, this methodology represents a valuable shortcut in the generation of well-equilibrated glassy systems. The creation of such an ideal packing makes possible a complete exploration and explanation of two dimensional jammed and glassy systems.","sentences":["The ideal glass, a disordered system of particles with zero configurational entropy, cannot be realized through thermal processes.","Nevertheless, we present a method for constructing ideal jammed packings of soft spheres, and thus the zero temperature ideal glass, in two dimensions.","In line with the predicted properties, these critically jammed packings have high bulk and shear moduli as well as an anomalously high density.","While the absence of pressure scaling in the shear moduli of crystalline materials is often attributed to the ordered nature of the particles, we show for the first time that disordered ideal packings also have this feature.","We also find that the density of states avoids the low frequency power law scaling famously found in most amorphous materials.","Finally, these configurations display hyperuniformity.","In addition to resolving a long-standing mystery, this methodology represents a valuable shortcut in the generation of well-equilibrated glassy systems.","The creation of such an ideal packing makes possible a complete exploration and explanation of two dimensional jammed and glassy systems."],"url":"http://arxiv.org/abs/2404.07492v1","category":"cond-mat.soft"}
{"created":"2024-04-11 05:59:16","title":"Neural Fault Injection: Generating Software Faults from Natural Language","abstract":"Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise. This paper introduces a novel methodology that harnesses the capabilities of Large Language Models (LLMs) augmented with Reinforcement Learning from Human Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the LLM's fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks. This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems.","sentences":["Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise.","This paper introduces a novel methodology that harnesses the capabilities of Large Language Models (LLMs) augmented with Reinforcement Learning from Human Feedback (RLHF) to overcome these challenges.","The usage of RLHF emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the LLM's fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks.","This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems."],"url":"http://arxiv.org/abs/2404.07491v1","category":"cs.SE"}
{"created":"2024-04-11 05:58:25","title":"Low-energy spin dynamics in a Kitaev material Na3Ni2BiO6 investigated by NMR","abstract":"We performed 23Na NMR and magnetization measurements on an S = 1, quasi-2D honeycomb lattice antiferromagnet Na3Ni2BiO6. A large positive Curie-Weiss constant of 22.9 K is observed. The NMR spectra at low fields are consistent with a \"zigzag\" magnetic order, indicating a large easy-axis anisotropy. With field applied along the c* axis, the NMR spectra confirm the existence of a 1/3-magnetization plateau phase between 5.1 T and 7.1 T. The transition from the zigzag order to the 1/3-magnetization plateau phase is also found to be a first-order type. A monotonic decrease of the spin gap is revealed in the 1/3-magnetization plateau phase, which reaches zero at a quantum critical field Hc = 8.35 T before entering the fully polarized phase. These data suggest the existence of exchange frustration in the system along with strong ferromagnetic interactions, hosting the possibility for Kitaev physics. Besides, well below the ordered phase, the 1/T1 at high fields shows either a level off or an enhancement upon cooling below 3 K, which suggests the existence of low-energy fluctuations.","sentences":["We performed 23Na NMR and magnetization measurements on an S = 1, quasi-2D honeycomb lattice antiferromagnet Na3Ni2BiO6.","A large positive Curie-Weiss constant of 22.9 K is observed.","The NMR spectra at low fields are consistent with a \"zigzag\" magnetic order, indicating a large easy-axis anisotropy.","With field applied along the c* axis, the NMR spectra confirm the existence of a 1/3-magnetization plateau phase between 5.1 T and 7.1 T. The transition from the zigzag order to the 1/3-magnetization plateau phase is also found to be a first-order type.","A monotonic decrease of the spin gap is revealed in the 1/3-magnetization plateau phase, which reaches zero at a quantum critical field Hc = 8.35 T before entering the fully polarized phase.","These data suggest the existence of exchange frustration in the system along with strong ferromagnetic interactions, hosting the possibility for Kitaev physics.","Besides, well below the ordered phase, the 1/T1 at high fields shows either a level off or an enhancement upon cooling below 3 K, which suggests the existence of low-energy fluctuations."],"url":"http://arxiv.org/abs/2404.07490v1","category":"cond-mat.str-el"}
{"created":"2024-04-11 05:56:37","title":"Generalized Pitaevskii relation between rectifying and linear responses: its application to reciprocal magnetization induction","abstract":"Nonlinear optics has regained attention in recent years, especially in the context of optospintronics and topological materials. Nonlinear responses involved in various degrees of freedom manifest their intricacy more pronounced than linear responses. However, for a certain class of nonlinear responses, a connection can be established with linear-response coefficients, enabling the exploration of diverse nonlinear-response functionality in terms of the linear-response counterpart. Our study quantum-mechanically elucidates the relation between such nonlinear and linear responses we call the Pitevskii relation and identifies the condition for the relation to hold. Following the obtained general formulation, we systematically identify the Pitaevskii relations such as the inverse magnetoelectric effect and inverse natural optical activity unique to systems manifesting the space-inversion-symmetry breaking. These results provide a systematic understanding of intricate nonlinear responses and may offer further implications to ultrafast spintronics.","sentences":["Nonlinear optics has regained attention in recent years, especially in the context of optospintronics and topological materials.","Nonlinear responses involved in various degrees of freedom manifest their intricacy more pronounced than linear responses.","However, for a certain class of nonlinear responses, a connection can be established with linear-response coefficients, enabling the exploration of diverse nonlinear-response functionality in terms of the linear-response counterpart.","Our study quantum-mechanically elucidates the relation between such nonlinear and linear responses we call the Pitevskii relation and identifies the condition for the relation to hold.","Following the obtained general formulation, we systematically identify the Pitaevskii relations such as the inverse magnetoelectric effect and inverse natural optical activity unique to systems manifesting the space-inversion-symmetry breaking.","These results provide a systematic understanding of intricate nonlinear responses and may offer further implications to ultrafast spintronics."],"url":"http://arxiv.org/abs/2404.07489v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 05:55:50","title":"Approximation of non-linear SPDEs with additive noise via weighted interacting particles systems: the stochastic McKean-Vlasov equation","abstract":"This paper is devoted to the problem of approximating non-linear Stochastic Partial Differential Equations (SPDEs) via interacting particle systems. In particular, we consider the Stochastic McKean-Vlasov equation, which is the McKean-Vlasov (MKV) PDE, perturbed by additive trace class noise. As is well-known, the MKV PDE can be obtained as mean field limit of the empirical measure of a stochastic system of interacting particles, where particles are subject to independent sources of noise. There is now a natural question, which is the one we consider and answer in this paper: can we obtain the SMKV equation, i.e. additive perturbations of the MKV PDE, as limit of interacting particle systems? It turns out that, in order to obtain the SMKV equation, one needs to study weighted empirical measures of particles, where the particles evolve according to a system of SDEs with independent noise, while the weights are time evolving and subject to common noise. The work of this manuscript therefore complements and contributes to various streams of literature, in particular: i) much attention in the community is currently devoted to obtaining SPDEs as scaling limits of appropriate dynamics; this paper contributes to a complementary stream, which is devoted to obtaining representations of SPDE through limits of empirical measures of interacting particle systems; ii) since the literature on limits of weighted empirical measures is often constrained to the case of static (random or deterministic) weights, this paper contributes to further expanding this line of research to the case of time-evolving weights.","sentences":["This paper is devoted to the problem of approximating non-linear Stochastic Partial Differential Equations (SPDEs) via interacting particle systems.","In particular, we consider the Stochastic McKean-Vlasov equation, which is the McKean-Vlasov (MKV) PDE, perturbed by additive trace class noise.","As is well-known, the MKV PDE can be obtained as mean field limit of the empirical measure of a stochastic system of interacting particles, where particles are subject to independent sources of noise.","There is now a natural question, which is the one we consider and answer in this paper: can we obtain the SMKV equation, i.e. additive perturbations of the MKV PDE, as limit of interacting particle systems?","It turns out that, in order to obtain the SMKV equation, one needs to study weighted empirical measures of particles, where the particles evolve according to a system of SDEs with independent noise, while the weights are time evolving and subject to common noise.","The work of this manuscript therefore complements and contributes to various streams of literature, in particular: i) much attention in the community is currently devoted to obtaining SPDEs as scaling limits of appropriate dynamics; this paper contributes to a complementary stream, which is devoted to obtaining representations of SPDE through limits of empirical measures of interacting particle systems; ii) since the literature on limits of weighted empirical measures is often constrained to the case of static (random or deterministic) weights, this paper contributes to further expanding this line of research to the case of time-evolving weights."],"url":"http://arxiv.org/abs/2404.07488v1","category":"math.PR"}
{"created":"2024-04-11 05:31:39","title":"Color code decoder with improved scaling for correcting circuit-level noise","abstract":"Two-dimensional color codes are a promising candidate for fault-tolerant quantum computing, as they have high encoding rates, transversal implementation of logical Clifford gates, and high feasibility of magic state constructions. However, decoding color codes presents a significant challenge due to their structure, where elementary errors violate three checks instead of just two (a key feature in surface code decoding), and the complexity in extracting syndrome is greater. We introduce an efficient color-code decoder that tackles these issues by combining two matching decoders for each color, generalized to handle circuit-level noise by employing detector error models. We provide comprehensive analyses of the decoder, covering its threshold and sub-threshold scaling both for bit-flip noise with ideal measurements and for circuit-level noise. Our simulations reveal that this decoding strategy nearly reaches the best possible scaling of logical failure ($p_\\mathrm{fail} \\sim p^{d/2}$) for both noise models, where $p$ is the noise strength, in the regime of interest for fault-tolerant quantum computing. While its noise thresholds are comparable with other matching-based decoders for color codes (8.2% for bit-flip noise and 0.46% for circuit-level noise), the scaling of logical failure rates below threshold significantly outperforms the best matching-based decoders.","sentences":["Two-dimensional color codes are a promising candidate for fault-tolerant quantum computing, as they have high encoding rates, transversal implementation of logical Clifford gates, and high feasibility of magic state constructions.","However, decoding color codes presents a significant challenge due to their structure, where elementary errors violate three checks instead of just two (a key feature in surface code decoding), and the complexity in extracting syndrome is greater.","We introduce an efficient color-code decoder that tackles these issues by combining two matching decoders for each color, generalized to handle circuit-level noise by employing detector error models.","We provide comprehensive analyses of the decoder, covering its threshold and sub-threshold scaling both for bit-flip noise with ideal measurements and for circuit-level noise.","Our simulations reveal that this decoding strategy nearly reaches the best possible scaling of logical failure ($p_\\mathrm{fail} \\sim p^{d/2}$) for both noise models, where $p$ is the noise strength, in the regime of interest for fault-tolerant quantum computing.","While its noise thresholds are comparable with other matching-based decoders for color codes (8.2% for bit-flip noise and 0.46% for circuit-level noise), the scaling of logical failure rates below threshold significantly outperforms the best matching-based decoders."],"url":"http://arxiv.org/abs/2404.07482v1","category":"quant-ph"}
{"created":"2024-04-11 05:27:40","title":"Adiabatic State Preparation in a Quantum Ising Spin Chain","abstract":"We report on adiabatic state preparation in the one-dimensional quantum Ising model using ultracold bosons in a tilted optical lattice. We prepare many-body ground states of controllable system sizes and observe enhanced fluctuations around the transition between paramagnetic and antiferromagnetic states, marking the precursor of quantum critical behavior. Furthermore, we find evidence for superpositions of domain walls and study their effect on the many-body ground state by measuring the populations of each spin configuration across the transition. These results shed new light on the effect of boundary conditions in finite-size quantum systems.","sentences":["We report on adiabatic state preparation in the one-dimensional quantum Ising model using ultracold bosons in a tilted optical lattice.","We prepare many-body ground states of controllable system sizes and observe enhanced fluctuations around the transition between paramagnetic and antiferromagnetic states, marking the precursor of quantum critical behavior.","Furthermore, we find evidence for superpositions of domain walls and study their effect on the many-body ground state by measuring the populations of each spin configuration across the transition.","These results shed new light on the effect of boundary conditions in finite-size quantum systems."],"url":"http://arxiv.org/abs/2404.07481v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-11 05:21:13","title":"Geometric Aspects of Observability of Hypergraphs","abstract":"In this paper we consider aspects of geometric observability for hypergraphs, extending our earlier work from the uniform to the nonuniform case. Hypergraphs, a generalization of graphs, allow hyperedges to connect multiple nodes and unambiguously represent multi-way relationships which are ubiquitous in many real-world networks including those that arise in biology. We consider polynomial dynamical systems with linear outputs defined according to hypergraph structure, and we propose methods to evaluate local, weak observability.","sentences":["In this paper we consider aspects of geometric observability for hypergraphs, extending our earlier work from the uniform to the nonuniform case.","Hypergraphs, a generalization of graphs, allow hyperedges to connect multiple nodes and unambiguously represent multi-way relationships which are ubiquitous in many real-world networks including those that arise in biology.","We consider polynomial dynamical systems with linear outputs defined according to hypergraph structure, and we propose methods to evaluate local, weak observability."],"url":"http://arxiv.org/abs/2404.07480v1","category":"math.DS"}
{"created":"2024-04-11 05:09:32","title":"The Survey on Multi-Source Data Fusion in Cyber-Physical-Social Systems:Foundational Infrastructure for Industrial Metaverses and Industries 5.0","abstract":"As the concept of Industries 5.0 develops, industrial metaverses are expected to operate in parallel with the actual industrial processes to offer ``Human-Centric\" Safe, Secure, Sustainable, Sensitive, Service, and Smartness ``6S\" manufacturing solutions. Industrial metaverses not only visualize the process of productivity in a dynamic and evolutional way, but also provide an immersive laboratory experimental environment for optimizing and remodeling the process. Besides, the customized user needs that are hidden in social media data can be discovered by social computing technologies, which introduces an input channel for building the whole social manufacturing process including industrial metaverses. This makes the fusion of multi-source data cross Cyber-Physical-Social Systems (CPSS) the foundational and key challenge. This work firstly proposes a multi-source-data-fusion-driven operational architecture for industrial metaverses on the basis of conducting a comprehensive literature review on the state-of-the-art multi-source data fusion methods. The advantages and disadvantages of each type of method are analyzed by considering the fusion mechanisms and application scenarios. Especially, we combine the strengths of deep learning and knowledge graphs in scalability and parallel computation to enable our proposed framework the ability of prescriptive optimization and evolution. This integration can address the shortcomings of deep learning in terms of explainability and fact fabrication, as well as overcoming the incompleteness and the challenges of construction and maintenance inherent in knowledge graphs. The effectiveness of the proposed architecture is validated through a parallel weaving case study. In the end, we discuss the challenges and future directions of multi-source data fusion cross CPSS for industrial metaverses and social manufacturing in Industries 5.0.","sentences":["As the concept of Industries 5.0 develops, industrial metaverses are expected to operate in parallel with the actual industrial processes to offer ``Human-Centric\" Safe, Secure, Sustainable, Sensitive, Service, and Smartness ``6S\" manufacturing solutions.","Industrial metaverses not only visualize the process of productivity in a dynamic and evolutional way, but also provide an immersive laboratory experimental environment for optimizing and remodeling the process.","Besides, the customized user needs that are hidden in social media data can be discovered by social computing technologies, which introduces an input channel for building the whole social manufacturing process including industrial metaverses.","This makes the fusion of multi-source data cross Cyber-Physical-Social Systems (CPSS) the foundational and key challenge.","This work firstly proposes a multi-source-data-fusion-driven operational architecture for industrial metaverses on the basis of conducting a comprehensive literature review on the state-of-the-art multi-source data fusion methods.","The advantages and disadvantages of each type of method are analyzed by considering the fusion mechanisms and application scenarios.","Especially, we combine the strengths of deep learning and knowledge graphs in scalability and parallel computation to enable our proposed framework the ability of prescriptive optimization and evolution.","This integration can address the shortcomings of deep learning in terms of explainability and fact fabrication, as well as overcoming the incompleteness and the challenges of construction and maintenance inherent in knowledge graphs.","The effectiveness of the proposed architecture is validated through a parallel weaving case study.","In the end, we discuss the challenges and future directions of multi-source data fusion cross CPSS for industrial metaverses and social manufacturing in Industries 5.0."],"url":"http://arxiv.org/abs/2404.07476v1","category":"cs.CY"}
{"created":"2024-04-11 04:54:42","title":"LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation","abstract":"In this study, the performance of existing U-shaped neural network architectures was enhanced for medical image segmentation by adding Transformer. Although Transformer architectures are powerful at extracting global information, its ability to capture local information is limited due to its high complexity. To address this challenge, we proposed a new lightweight U-shaped cascade fusion network (LUCF-Net) for medical image segmentation. It utilized an asymmetrical structural design and incorporated both local and global modules to enhance its capacity for local and global modeling. Additionally, a multi-layer cascade fusion decoding network was designed to further bolster the network's information fusion capabilities. Validation results achieved on multi-organ datasets in CT format, cardiac segmentation datasets in MRI format, and dermatology datasets in image format demonstrated that the proposed model outperformed other state-of-the-art methods in handling local-global information, achieving an improvement of 1.54% in Dice coefficient and 2.6 mm in Hausdorff distance on multi-organ segmentation. Furthermore, as a network that combines Convolutional Neural Network and Transformer architectures, it achieves competitive segmentation performance with only 6.93 million parameters and 6.6 gigabytes of floating point operations, without the need of pre-training. In summary, the proposed method demonstrated enhanced performance while retaining a simpler model design compared to other Transformer-based segmentation networks.","sentences":["In this study, the performance of existing U-shaped neural network architectures was enhanced for medical image segmentation by adding Transformer.","Although Transformer architectures are powerful at extracting global information, its ability to capture local information is limited due to its high complexity.","To address this challenge, we proposed a new lightweight U-shaped cascade fusion network (LUCF-Net) for medical image segmentation.","It utilized an asymmetrical structural design and incorporated both local and global modules to enhance its capacity for local and global modeling.","Additionally, a multi-layer cascade fusion decoding network was designed to further bolster the network's information fusion capabilities.","Validation results achieved on multi-organ datasets in CT format, cardiac segmentation datasets in MRI format, and dermatology datasets in image format demonstrated that the proposed model outperformed other state-of-the-art methods in handling local-global information, achieving an improvement of 1.54% in Dice coefficient and 2.6 mm in Hausdorff distance on multi-organ segmentation.","Furthermore, as a network that combines Convolutional Neural Network and Transformer architectures, it achieves competitive segmentation performance with only 6.93 million parameters and 6.6 gigabytes of floating point operations, without the need of pre-training.","In summary, the proposed method demonstrated enhanced performance while retaining a simpler model design compared to other Transformer-based segmentation networks."],"url":"http://arxiv.org/abs/2404.07473v1","category":"eess.IV"}
{"created":"2024-04-11 04:42:01","title":"Cramer-Rao Bounds for Near-Field Sensing: A Generic Modular Architecture","abstract":"A generic modular array architecture is proposed, featuring uniform/non-uniform subarray layouts that allows for flexible deployment. The bistatic near-field sensing system is considered, where the target is located in the near-field of the whole modular array and the far-field of each subarray. Then, the closed-form expressions of Cramer-Rao bounds (CRBs) for range and angle estimations are derived based on the hybrid spherical and planar wave model (HSPM). Simulation results validate the accuracy of the derived closed-form CRBs and demonstrate that: i) The HSPM with varying angles of arrival (AoAs) between subarrays can reduce the CRB for range estimation compared to the traditional HSPM with shared AoA; and ii) The proposed generic modular architecture with subarrays positioned closer to the edges can significantly reduce the CRBs compared to the traditional modular architecture with uniform subarray layout, when the array aperture is fixed.","sentences":["A generic modular array architecture is proposed, featuring uniform/non-uniform subarray layouts that allows for flexible deployment.","The bistatic near-field sensing system is considered, where the target is located in the near-field of the whole modular array and the far-field of each subarray.","Then, the closed-form expressions of Cramer-Rao bounds (CRBs) for range and angle estimations are derived based on the hybrid spherical and planar wave model (HSPM).","Simulation results validate the accuracy of the derived closed-form CRBs and demonstrate that: i)","The HSPM with varying angles of arrival (AoAs) between subarrays can reduce the CRB for range estimation compared to the traditional HSPM with shared AoA; and ii) The proposed generic modular architecture with subarrays positioned closer to the edges can significantly reduce the CRBs compared to the traditional modular architecture with uniform subarray layout, when the array aperture is fixed."],"url":"http://arxiv.org/abs/2404.07472v1","category":"eess.SP"}
{"created":"2024-04-11 04:20:00","title":"On the stability of the spherically symmetric solution to an inflow problem for an isentropic model of compressible viscous fluid","abstract":"We investigate an inflow problem for the multi-dimensional isentropic compressible Navier-Stokes equations. The fluid under consideration occupies the exterior domain of unit ball, $\\Omega=\\{x\\in\\mathbb{R}^n\\,\\vert\\, |x|\\ge 1\\}$, and a constant stream of mass is flowing into the domain from the boundary $\\partial\\Omega=\\{|x|=1\\}$. The existence and uniqueness of a spherically symmetric stationary solution, denoted as $(\\tilde{\\rho},\\tilde{u})$, is first proved by I. Hashimoto and A. Matsumura in 2021. In this paper, we show that either $\\tilde{\\rho}$ is monotone increasing or $\\tilde{\\rho}$ attains a unique global minimum, and this is classified by the boundary condition of density. Moreover, we also derive a set of decay rates for $(\\tilde{\\rho},\\tilde{u})$ which allows us to prove the long time stability of $(\\tilde{\\rho},\\tilde{u})$ under small initial perturbations using the energy method. The main difficulty for this is the boundary terms that appears in the a-priori estimates. We resolve this issue by reformulating the problem in Lagrangian coordinate system.","sentences":["We investigate an inflow problem for the multi-dimensional isentropic compressible Navier-Stokes equations.","The fluid under consideration occupies the exterior domain of unit ball, $\\Omega=\\{x\\in\\mathbb{R}^n\\,\\vert\\, |x|\\ge 1\\}$, and a constant stream of mass is flowing into the domain from the boundary $\\partial\\Omega=\\{|x|=1\\}$. The existence and uniqueness of a spherically symmetric stationary solution, denoted as $(\\tilde{\\rho},\\tilde{u})$, is first proved by I. Hashimoto and A. Matsumura in 2021.","In this paper, we show that either $\\tilde{\\rho}$ is monotone increasing or $\\tilde{\\rho}$ attains a unique global minimum, and this is classified by the boundary condition of density.","Moreover, we also derive a set of decay rates for $(\\tilde{\\rho},\\tilde{u})$ which allows us to prove the long time stability of $(\\tilde{\\rho},\\tilde{u})$ under small initial perturbations using the energy method.","The main difficulty for this is the boundary terms that appears in the a-priori estimates.","We resolve this issue by reformulating the problem in Lagrangian coordinate system."],"url":"http://arxiv.org/abs/2404.07469v1","category":"math.AP"}
{"created":"2024-04-11 04:16:14","title":"One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting","abstract":"Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation. Code and additional details are available at stanford-tml.github.io/extrinsic-manipulation.","sentences":["Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper.","However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics.","We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed.","Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements.","We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive.","Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics.","The goals are then tracked by the primitive policies.","Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives.","Our experiments cover 10 objects and 6 environment configurations.","We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation.","Code and additional details are available at stanford-tml.github.io/extrinsic-manipulation."],"url":"http://arxiv.org/abs/2404.07468v1","category":"cs.RO"}
{"created":"2024-04-11 04:14:48","title":"Trashbusters: Deep Learning Approach for Litter Detection and Tracking","abstract":"The illegal disposal of trash is a major public health and environmental concern. Disposing of trash in unplanned places poses serious health and environmental risks. We should try to restrict public trash cans as much as possible. This research focuses on automating the penalization of litterbugs, addressing the persistent problem of littering in public places. Traditional approaches relying on manual intervention and witness reporting suffer from delays, inaccuracies, and anonymity issues. To overcome these challenges, this paper proposes a fully automated system that utilizes surveillance cameras and advanced computer vision algorithms for litter detection, object tracking, and face recognition. The system accurately identifies and tracks individuals engaged in littering activities, attaches their identities through face recognition, and enables efficient enforcement of anti-littering policies. By reducing reliance on manual intervention, minimizing human error, and providing prompt identification, the proposed system offers significant advantages in addressing littering incidents. The primary contribution of this research lies in the implementation of the proposed system, leveraging advanced technologies to enhance surveillance operations and automate the penalization of litterbugs.","sentences":["The illegal disposal of trash is a major public health and environmental concern.","Disposing of trash in unplanned places poses serious health and environmental risks.","We should try to restrict public trash cans as much as possible.","This research focuses on automating the penalization of litterbugs, addressing the persistent problem of littering in public places.","Traditional approaches relying on manual intervention and witness reporting suffer from delays, inaccuracies, and anonymity issues.","To overcome these challenges, this paper proposes a fully automated system that utilizes surveillance cameras and advanced computer vision algorithms for litter detection, object tracking, and face recognition.","The system accurately identifies and tracks individuals engaged in littering activities, attaches their identities through face recognition, and enables efficient enforcement of anti-littering policies.","By reducing reliance on manual intervention, minimizing human error, and providing prompt identification, the proposed system offers significant advantages in addressing littering incidents.","The primary contribution of this research lies in the implementation of the proposed system, leveraging advanced technologies to enhance surveillance operations and automate the penalization of litterbugs."],"url":"http://arxiv.org/abs/2404.07467v1","category":"cs.CV"}
{"created":"2024-04-11 04:08:24","title":"Quantum Multigrid Algorithm for Finite Element Problems","abstract":"Quantum linear system algorithms (QLSAs) can provide exponential speedups for the solution of linear systems, but the growth of the condition number for finite element problems can eliminate the exponential speedup. QLSAs are also incapable of using an initial guess of a solution to improve upon it. To circumvent these issues, we present a Quantum Multigrid Algorithm (qMG) for the iterative solution of linear systems by applying the sequence of multigrid operations on a quantum state. Given an initial guess with error e_0, qMG can produce a vector encoding the entire sequence of multigrid iterates with the final iterate having a relative error e'=e/e_0, as a subspace of the final quantum state, with exponential advantage in O( poly log (N/e') ) time using O( poly log (N/e') ) qubits. Although extracting the final iterate from the sequence is efficient, extracting the sequence of iterates from the final quantum state can be inefficient. We provide an analysis of the complexity of the method along with numerical analysis.","sentences":["Quantum linear system algorithms (QLSAs) can provide exponential speedups for the solution of linear systems, but the growth of the condition number for finite element problems can eliminate the exponential speedup.","QLSAs are also incapable of using an initial guess of a solution to improve upon it.","To circumvent these issues, we present a Quantum Multigrid Algorithm (qMG) for the iterative solution of linear systems by applying the sequence of multigrid operations on a quantum state.","Given an initial guess with error e_0, qMG can produce a vector encoding the entire sequence of multigrid iterates with the final iterate having a relative error e'=e/e_0, as a subspace of the final quantum state, with exponential advantage in O( poly log (N/e') )","time using O( poly log (N/e') ) qubits.","Although extracting the final iterate from the sequence is efficient, extracting the sequence of iterates from the final quantum state can be inefficient.","We provide an analysis of the complexity of the method along with numerical analysis."],"url":"http://arxiv.org/abs/2404.07466v1","category":"quant-ph"}
{"created":"2024-04-11 17:56:18","title":"A Lightweight Protocol for Matchgate Fidelity Estimation","abstract":"We present a low-depth randomised algorithm for the estimation of entanglement fidelity between an $n$-qubit matchgate circuit $\\mathcal{U}$ and its noisy implementation $\\mathcal{E}$. Our procedure makes use of a modified Pauli-Liouville representation of quantum channels, with Clifford algebra elements as a basis. We show that this choice of representation leads to a block-diagonal compound matrix structure of matchgate superoperators which enables construction of efficient protocols for estimating the fidelity, achieving a $1/\\sqrt{n}$ speedup over protocols of Flammia \\& Liu [PRL 106, 230501]. Finally, we offer simple extensions of our protocol which (without additional overhead) benchmark matchgate circuits intertwined by Clifford circuits, and circuits composed of exclusively nearest-neighbour $XY(\\theta)$ gates or Givens rotations - forming the first known method for direct benchmarking of matchgate subgroups.","sentences":["We present a low-depth randomised algorithm for the estimation of entanglement fidelity between an $n$-qubit matchgate circuit $\\mathcal{U}$ and its noisy implementation $\\mathcal{E}$. Our procedure makes use of a modified Pauli-Liouville representation of quantum channels, with Clifford algebra elements as a basis.","We show that this choice of representation leads to a block-diagonal compound matrix structure of matchgate superoperators which enables construction of efficient protocols for estimating the fidelity, achieving a $1/\\sqrt{n}$ speedup over protocols of Flammia \\& Liu","[PRL 106, 230501].","Finally, we offer simple extensions of our protocol which (without additional overhead) benchmark matchgate circuits intertwined by Clifford circuits, and circuits composed of exclusively nearest-neighbour $XY(\\theta)$ gates or Givens rotations - forming the first known method for direct benchmarking of matchgate subgroups."],"url":"http://arxiv.org/abs/2404.07974v1","category":"quant-ph"}
{"created":"2024-04-11 17:55:57","title":"The Newman algorithm for constructing polynomials with restricted coefficients and many real roots","abstract":"Under certain natural sufficient conditions on the sequence of uniformly bounded closed sets $E_k\\subset\\mathbb{R}$ of admissible coefficients, we construct a polynomial $P_n(x)=1+\\sum_{k=1}^n\\varepsilon_k x^k$, $\\varepsilon_k\\in E_k$, with at least $c\\sqrt{n}$ distinct roots in $[0,1]$, which matches the classical upper bound up to the value of the constant $c>0$. Our sufficient conditions cover the Littlewood ($E_k=\\{-1,1\\}$) and Newman ($E_k=\\{0,(-1)^k\\}$) polynomials and are also necessary for the existence of such polynomials with arbitrarily many roots in the case when the sequence $E_k$ is periodic.","sentences":["Under certain natural sufficient conditions on the sequence of uniformly bounded closed sets $E_k\\subset\\mathbb{R}$ of admissible coefficients, we construct a polynomial $P_n(x)=1+\\sum_{k=1}^n\\varepsilon_k x^k$, $\\varepsilon_k\\in E_k$, with at least $c\\sqrt{n}$ distinct roots in $[0,1]$, which matches the classical upper bound up to the value of the constant $c>0$. Our sufficient conditions cover the Littlewood ($E_k=\\{-1,1\\}$) and Newman ($E_k=\\{0,(-1)^k\\}$) polynomials and are also necessary for the existence of such polynomials with arbitrarily many roots in the case when the sequence $E_k$ is periodic."],"url":"http://arxiv.org/abs/2404.07971v1","category":"math.CA"}
{"created":"2024-04-11 17:25:23","title":"Robust elastic full-waveform inversion using an alternating direction method of multipliers with reconstructed wavefields","abstract":"Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics. The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information. The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems. Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations. The Hessian matrix is blocky with diagonal blocks, making model updates fast. Gradient ascent is used to update Lagrange multipliers by summing PDE violations. Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects.","sentences":["Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics.","The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information.","The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems.","Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations.","The Hessian matrix is blocky with diagonal blocks, making model updates fast.","Gradient ascent is used to update Lagrange multipliers by summing PDE violations.","Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects."],"url":"http://arxiv.org/abs/2404.07927v1","category":"math.NA"}
{"created":"2024-04-11 16:27:43","title":"Revisiting a drag partition model for canopy-like roughness elements","abstract":"Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow. The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92. The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V. The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes. To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition. This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another. This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here. The two approaches are shown to recover R92 under plausible conditions. Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S.","sentences":["Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow.","The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92.","The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V.","The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes.","To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition.","This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another.","This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here.","The two approaches are shown to recover R92 under plausible conditions.","Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S."],"url":"http://arxiv.org/abs/2404.07893v1","category":"physics.flu-dyn"}
{"created":"2024-04-11 16:24:49","title":"A Measurement of Genuine Tor Traces for Realistic Website Fingerprinting","abstract":"Website fingerprinting (WF) is a dangerous attack on web privacy because it enables an adversary to predict the website a user is visiting, despite the use of encryption, VPNs, or anonymizing networks such as Tor. Previous WF work almost exclusively uses synthetic datasets to evaluate the performance and estimate the feasibility of WF attacks despite evidence that synthetic data misrepresents the real world. In this paper we present GTT23, the first WF dataset of genuine Tor traces, which we obtain through a large-scale measurement of the Tor network. GTT23 represents real Tor user behavior better than any existing WF dataset, is larger than any existing WF dataset by at least an order of magnitude, and will help ground the future study of realistic WF attacks and defenses. In a detailed evaluation, we survey 25 WF datasets published over the last 15 years and compare their characteristics to those of GTT23. We discover common deficiencies of synthetic datasets that make them inferior to GTT23 for drawing meaningful conclusions about the effectiveness of WF attacks directed at real Tor users. We have made GTT23 available to promote reproducible research and to help inspire new directions for future work.","sentences":["Website fingerprinting (WF) is a dangerous attack on web privacy because it enables an adversary to predict the website a user is visiting, despite the use of encryption, VPNs, or anonymizing networks such as Tor.","Previous WF work almost exclusively uses synthetic datasets to evaluate the performance and estimate the feasibility of WF attacks despite evidence that synthetic data misrepresents the real world.","In this paper we present GTT23, the first WF dataset of genuine Tor traces, which we obtain through a large-scale measurement of the Tor network.","GTT23 represents real Tor user behavior better than any existing WF dataset, is larger than any existing WF dataset by at least an order of magnitude, and will help ground the future study of realistic WF attacks and defenses.","In a detailed evaluation, we survey 25 WF datasets published over the last 15 years and compare their characteristics to those of GTT23.","We discover common deficiencies of synthetic datasets that make them inferior to GTT23 for drawing meaningful conclusions about the effectiveness of WF attacks directed at real Tor users.","We have made GTT23 available to promote reproducible research and to help inspire new directions for future work."],"url":"http://arxiv.org/abs/2404.07892v1","category":"cs.CR"}
{"created":"2024-04-11 16:09:27","title":"Silicon-Photomultiplier (SiPM) Protection Against Over-Current and Over-Illumination","abstract":"SiPMs operate in Geiger mode, wherein photodiode cells are reverse-biased to the breakdown by even a single photon. Each cell is connected in series with a quenching resistor, which prevents cell damage and resets the cell after making a signal. All cells are arranged in parallel, making SiPMs and biasing circuits vulnerable to over-illumination, where the current passing through the SiPM can exceed the allowable value, leading to damage. In this study, we investigate over-current conditions in SiPMs and propose a protective method against over-illumination and over-current using a series resistor. Additionally, we ensure SiPM stability through the incorporation of a suitable capacitor.","sentences":["SiPMs operate in Geiger mode, wherein photodiode cells are reverse-biased to the breakdown by even a single photon.","Each cell is connected in series with a quenching resistor, which prevents cell damage and resets the cell after making a signal.","All cells are arranged in parallel, making SiPMs and biasing circuits vulnerable to over-illumination, where the current passing through the SiPM can exceed the allowable value, leading to damage.","In this study, we investigate over-current conditions in SiPMs and propose a protective method against over-illumination and over-current using a series resistor.","Additionally, we ensure SiPM stability through the incorporation of a suitable capacitor."],"url":"http://arxiv.org/abs/2404.07875v1","category":"hep-ex"}
{"created":"2024-04-11 16:07:11","title":"Video Compression Beyond VVC: Quantitative Analysis of Intra Coding Tools in Enhanced Compression Model (ECM)","abstract":"A quantitative analysis of post-VVC luma and chroma intra tools is presented, focusing on their statistical behaviors, in terms of block selection rate under different conditions. The aim is to provide insights to the standardization community, offering a clearer understanding of interactions between tools and assisting in the design of an optimal combination of these novel tools when the JVET enters the standardization phase. Specifically, this paper examines the selection rate of intra tools as function of 1) the version of the ECM, 2) video resolution, and 3) video bitrate. Additionally, tests have been conducted on sequences beyond the JVET CTC database. The statistics show several trends and interactions, with various strength, between coding tools of both luma and chroma.","sentences":["A quantitative analysis of post-VVC luma and chroma intra tools is presented, focusing on their statistical behaviors, in terms of block selection rate under different conditions.","The aim is to provide insights to the standardization community, offering a clearer understanding of interactions between tools and assisting in the design of an optimal combination of these novel tools when the JVET enters the standardization phase.","Specifically, this paper examines the selection rate of intra tools as function of 1) the version of the ECM, 2) video resolution, and 3) video bitrate.","Additionally, tests have been conducted on sequences beyond the JVET CTC database.","The statistics show several trends and interactions, with various strength, between coding tools of both luma and chroma."],"url":"http://arxiv.org/abs/2404.07872v1","category":"cs.MM"}
{"created":"2024-04-11 16:01:00","title":"The Power of Properties: Uncovering the Influential Factors in Emotion Classification","abstract":"Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.","sentences":["Facial expression-based human emotion recognition is a critical research area in psychology and medicine.","State-of-the-art classification performance is only reached by end-to-end trained neural networks.","Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions.","Analyzing single inputs alone fails to expose systematic learned biases.","These biases can be characterized as facial properties summarizing abstract information like age or medical conditions.","Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties.","We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties.","Among those are age, gender, and facial symmetry.","Furthermore, the medical usage of surface electromyography significantly influences emotion prediction.","We introduce a workflow to evaluate explicit properties and their impact.","These insights might help medical professionals select and apply classifiers regarding their specialized data and properties."],"url":"http://arxiv.org/abs/2404.07867v1","category":"cs.CV"}
{"created":"2024-04-11 15:57:12","title":"Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing","abstract":"We consider the problem of localizing change points in high-dimensional linear regression. We propose an Approximate Message Passing (AMP) algorithm for estimating both the signals and the change point locations. Assuming Gaussian covariates, we give an exact asymptotic characterization of its estimation performance in the limit where the number of samples grows proportionally to the signal dimension. Our algorithm can be tailored to exploit any prior information on the signal, noise, and change points. It also enables uncertainty quantification in the form of an efficiently computable approximate posterior distribution, whose asymptotic form we characterize exactly. We validate our theory via numerical experiments, and demonstrate the favorable performance of our estimators on both synthetic data and images.","sentences":["We consider the problem of localizing change points in high-dimensional linear regression.","We propose an Approximate Message Passing (AMP) algorithm for estimating both the signals and the change point locations.","Assuming Gaussian covariates, we give an exact asymptotic characterization of its estimation performance in the limit where the number of samples grows proportionally to the signal dimension.","Our algorithm can be tailored to exploit any prior information on the signal, noise, and change points.","It also enables uncertainty quantification in the form of an efficiently computable approximate posterior distribution, whose asymptotic form we characterize exactly.","We validate our theory via numerical experiments, and demonstrate the favorable performance of our estimators on both synthetic data and images."],"url":"http://arxiv.org/abs/2404.07864v1","category":"stat.ML"}
{"created":"2024-04-11 15:51:52","title":"Resolve Domain Conflicts for Generalizable Remote Physiological Measurement","abstract":"Remote photoplethysmography (rPPG) technology has become increasingly popular due to its non-invasive monitoring of various physiological indicators, making it widely applicable in multimedia interaction, healthcare, and emotion analysis. Existing rPPG methods utilize multiple datasets for training to enhance the generalizability of models. However, they often overlook the underlying conflict issues across different datasets, such as (1) label conflict resulting from different phase delays between physiological signal labels and face videos at the instance level, and (2) attribute conflict stemming from distribution shifts caused by head movements, illumination changes, skin types, etc. To address this, we introduce the DOmain-HArmonious framework (DOHA). Specifically, we first propose a harmonious phase strategy to eliminate uncertain phase delays and preserve the temporal variation of physiological signals. Next, we design a harmonious hyperplane optimization that reduces irrelevant attribute shifts and encourages the model's optimization towards a global solution that fits more valid scenarios. Our experiments demonstrate that DOHA significantly improves the performance of existing methods under multiple protocols. Our code is available at https://github.com/SWY666/rPPG-DOHA.","sentences":["Remote photoplethysmography (rPPG) technology has become increasingly popular due to its non-invasive monitoring of various physiological indicators, making it widely applicable in multimedia interaction, healthcare, and emotion analysis.","Existing rPPG methods utilize multiple datasets for training to enhance the generalizability of models.","However, they often overlook the underlying conflict issues across different datasets, such as (1) label conflict resulting from different phase delays between physiological signal labels and face videos at the instance level, and (2) attribute conflict stemming from distribution shifts caused by head movements, illumination changes, skin types, etc.","To address this, we introduce the DOmain-HArmonious framework (DOHA).","Specifically, we first propose a harmonious phase strategy to eliminate uncertain phase delays and preserve the temporal variation of physiological signals.","Next, we design a harmonious hyperplane optimization that reduces irrelevant attribute shifts and encourages the model's optimization towards a global solution that fits more valid scenarios.","Our experiments demonstrate that DOHA significantly improves the performance of existing methods under multiple protocols.","Our code is available at https://github.com/SWY666/rPPG-DOHA."],"url":"http://arxiv.org/abs/2404.07855v1","category":"cs.CV"}
{"created":"2024-04-11 15:18:34","title":"Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution","abstract":"Foundation models have rapidly evolved and have achieved significant accomplishments in computer vision tasks. Specifically, the prompt mechanism conveniently allows users to integrate image prior information into the model, making it possible to apply models without any training. Therefore, we propose a method based on foundation models and zero training to solve the tasks of photoacoustic (PA) image segmentation. We employed the segment anything model (SAM) by setting simple prompts and integrating the model's outputs with prior knowledge of the imaged objects to accomplish various tasks, including: (1) removing the skin signal in three-dimensional PA image rendering; (2) dual speed-of-sound reconstruction, and (3) segmentation of finger blood vessels. Through these demonstrations, we have concluded that deep learning can be directly applied in PA imaging without the requirement for network design and training. This potentially allows for a hands-on, convenient approach to achieving efficient and accurate segmentation of PA images. This letter serves as a comprehensive tutorial, facilitating the mastery of the technique through the provision of code and sample datasets.","sentences":["Foundation models have rapidly evolved and have achieved significant accomplishments in computer vision tasks.","Specifically, the prompt mechanism conveniently allows users to integrate image prior information into the model, making it possible to apply models without any training.","Therefore, we propose a method based on foundation models and zero training to solve the tasks of photoacoustic (PA) image segmentation.","We employed the segment anything model (SAM) by setting simple prompts and integrating the model's outputs with prior knowledge of the imaged objects to accomplish various tasks, including: (1) removing the skin signal in three-dimensional PA image rendering; (2) dual speed-of-sound reconstruction, and (3) segmentation of finger blood vessels.","Through these demonstrations, we have concluded that deep learning can be directly applied in PA imaging without the requirement for network design and training.","This potentially allows for a hands-on, convenient approach to achieving efficient and accurate segmentation of PA images.","This letter serves as a comprehensive tutorial, facilitating the mastery of the technique through the provision of code and sample datasets."],"url":"http://arxiv.org/abs/2404.07833v1","category":"cs.CV"}
{"created":"2024-04-11 15:15:35","title":"Global solution and singularity formation for the supersonic expanding wave of compressible Euler equations with radial symmetry","abstract":"In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry. Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions. On the other hand, singularity forms in finite time when the initial data include strong compression somewhere. Several useful invariant domains will be also given.","sentences":["In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry.","Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions.","On the other hand, singularity forms in finite time when the initial data include strong compression somewhere.","Several useful invariant domains will be also given."],"url":"http://arxiv.org/abs/2404.07830v1","category":"math.AP"}
{"created":"2024-04-11 15:06:15","title":"Trials Factor for Semi-Supervised NN Classifiers in Searches for Narrow Resonances at the LHC","abstract":"To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used. Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution. We perform a frequentist study to quantify this effect, in the form of a trials factor. As an example, we consider simulated $Z\\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers. The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control.","sentences":["To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used.","Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution.","We perform a frequentist study to quantify this effect, in the form of a trials factor.","As an example, we consider simulated $Z\\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers.","The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control."],"url":"http://arxiv.org/abs/2404.07822v1","category":"hep-ph"}
{"created":"2024-04-11 14:32:32","title":"Using rare event algorithms to understand the statistics and dynamics of extreme heatwave seasons in South Asia","abstract":"Computing the return times of extreme events and assessing the impact of climate change on such return times is fundamental to extreme event attribution studies. However, the rarity of such events in the observational record makes this task a challenging one, even more so for \"record-shattering\" events that have not been previously observed at all. While climate models could be used to simulate such extremely rare events, such an approach entails a huge computational cost: gathering robust statistics for events with return time of centuries would require a few thousand years of simulation.   In this study, we use an innovative tool, rare event algorithm, that allows to sample numerous extremely rare events at a much lower cost than direct simulations. We employ the algorithm to sample extreme heatwave seasons, corresponding to large anomalies of the seasonal average temperature, in a heatwave hotspot of South Asia using the global climate model Plasim. We show that the algorithm estimates the return levels of extremely rare events with much greater precision than traditional statistical fits. It also enables the computation of various composite statistics, whose accuracy is demonstrated through comparison with a very long control run. In particular, our results reveal that extreme heatwave seasons are associated with an anticyclonic anomaly embedded within a large-scale hemispheric quasi-stationary wave-pattern. Additionally, the algorithm accurately represents the intensity-duration-frequency statistics of sub-seasonal heatwaves, offering insights into both seasonal and sub-seasonal aspects of extreme heatwave seasons. This innovative approach could be used in extreme event attribution studies to better constrain the changes in event's probability and intensity with global warming, particularly for events with return times spanning centuries or millennia.","sentences":["Computing the return times of extreme events and assessing the impact of climate change on such return times is fundamental to extreme event attribution studies.","However, the rarity of such events in the observational record makes this task a challenging one, even more so for \"record-shattering\" events that have not been previously observed at all.","While climate models could be used to simulate such extremely rare events, such an approach entails a huge computational cost: gathering robust statistics for events with return time of centuries would require a few thousand years of simulation.   ","In this study, we use an innovative tool, rare event algorithm, that allows to sample numerous extremely rare events at a much lower cost than direct simulations.","We employ the algorithm to sample extreme heatwave seasons, corresponding to large anomalies of the seasonal average temperature, in a heatwave hotspot of South Asia using the global climate model Plasim.","We show that the algorithm estimates the return levels of extremely rare events with much greater precision than traditional statistical fits.","It also enables the computation of various composite statistics, whose accuracy is demonstrated through comparison with a very long control run.","In particular, our results reveal that extreme heatwave seasons are associated with an anticyclonic anomaly embedded within a large-scale hemispheric quasi-stationary wave-pattern.","Additionally, the algorithm accurately represents the intensity-duration-frequency statistics of sub-seasonal heatwaves, offering insights into both seasonal and sub-seasonal aspects of extreme heatwave seasons.","This innovative approach could be used in extreme event attribution studies to better constrain the changes in event's probability and intensity with global warming, particularly for events with return times spanning centuries or millennia."],"url":"http://arxiv.org/abs/2404.07791v1","category":"physics.ao-ph"}
{"created":"2024-04-11 14:28:04","title":"PRAM: Place Recognition Anywhere Model for Efficient Visual Localization","abstract":"Humans localize themselves efficiently in known environments by first recognizing landmarks defined on certain objects and their spatial relationships, and then verifying the location by aligning detailed structures of recognized objects with those in the memory. Inspired by this, we propose the place recognition anywhere model (PRAM) to perform visual localization as efficiently as humans do. PRAM consists of two main components - recognition and registration. In detail, first of all, a self-supervised map-centric landmark definition strategy is adopted, making places in either indoor or outdoor scenes act as unique landmarks. Then, sparse keypoints extracted from images, are utilized as the input to a transformer-based deep neural network for landmark recognition; these keypoints enable PRAM to recognize hundreds of landmarks with high time and memory efficiency. Keypoints along with recognized landmark labels are further used for registration between query images and the 3D landmark map. Different from previous hierarchical methods, PRAM discards global and local descriptors, and reduces over 90% storage. Since PRAM utilizes recognition and landmark-wise verification to replace global reference search and exhaustive matching respectively, it runs 2.4 times faster than prior state-of-the-art approaches. Moreover, PRAM opens new directions for visual localization including multi-modality localization, map-centric feature learning, and hierarchical scene coordinate regression.","sentences":["Humans localize themselves efficiently in known environments by first recognizing landmarks defined on certain objects and their spatial relationships, and then verifying the location by aligning detailed structures of recognized objects with those in the memory.","Inspired by this, we propose the place recognition anywhere model (PRAM) to perform visual localization as efficiently as humans do.","PRAM consists of two main components - recognition and registration.","In detail, first of all, a self-supervised map-centric landmark definition strategy is adopted, making places in either indoor or outdoor scenes act as unique landmarks.","Then, sparse keypoints extracted from images, are utilized as the input to a transformer-based deep neural network for landmark recognition; these keypoints enable PRAM to recognize hundreds of landmarks with high time and memory efficiency.","Keypoints along with recognized landmark labels are further used for registration between query images and the 3D landmark map.","Different from previous hierarchical methods, PRAM discards global and local descriptors, and reduces over 90% storage.","Since PRAM utilizes recognition and landmark-wise verification to replace global reference search and exhaustive matching respectively, it runs 2.4 times faster than prior state-of-the-art approaches.","Moreover, PRAM opens new directions for visual localization including multi-modality localization, map-centric feature learning, and hierarchical scene coordinate regression."],"url":"http://arxiv.org/abs/2404.07785v1","category":"cs.CV"}
{"created":"2024-04-11 14:15:29","title":"Improving Network Degree Correlation by Degree-preserving Rewiring","abstract":"Degree correlation is a crucial measure in networks, significantly impacting network topology and dynamical behavior. The degree sequence of a network is a significant characteristic, and altering network degree correlation through degree-preserving rewiring poses an interesting problem. In this paper, we define the problem of maximizing network degree correlation through a finite number of rewirings and use the assortativity coefficient to measure it. We analyze the changes in assortativity coefficient under degree-preserving rewiring and establish its relationship with the s-metric. Under our assumptions, we prove the problem to be monotonic and submodular, leading to the proposal of the GA method to enhance network degree correlation. By formulating an integer programming model, we demonstrate that the GA method can effectively approximate the optimal solution and validate its superiority over other baseline methods through experiments on three types of real-world networks. Additionally, we introduce three heuristic rewiring strategies, EDA, TA and PEA, and demonstrate their applicability to different types of networks. Furthermore, we extend our investigation to explore the impact of these rewiring strategies on several spectral robustness metrics based on the adjacency matrix. Finally, we examine the robustness of various centrality metrics in the network while enhancing network degree correlation using the GA method.","sentences":["Degree correlation is a crucial measure in networks, significantly impacting network topology and dynamical behavior.","The degree sequence of a network is a significant characteristic, and altering network degree correlation through degree-preserving rewiring poses an interesting problem.","In this paper, we define the problem of maximizing network degree correlation through a finite number of rewirings and use the assortativity coefficient to measure it.","We analyze the changes in assortativity coefficient under degree-preserving rewiring and establish its relationship with the s-metric.","Under our assumptions, we prove the problem to be monotonic and submodular, leading to the proposal of the GA method to enhance network degree correlation.","By formulating an integer programming model, we demonstrate that the GA method can effectively approximate the optimal solution and validate its superiority over other baseline methods through experiments on three types of real-world networks.","Additionally, we introduce three heuristic rewiring strategies, EDA, TA and PEA, and demonstrate their applicability to different types of networks.","Furthermore, we extend our investigation to explore the impact of these rewiring strategies on several spectral robustness metrics based on the adjacency matrix.","Finally, we examine the robustness of various centrality metrics in the network while enhancing network degree correlation using the GA method."],"url":"http://arxiv.org/abs/2404.07779v1","category":"cs.SI"}
{"created":"2024-04-11 14:14:58","title":"Quality check of a sample partition using multinomial distribution","abstract":"In this paper, we advocate a novel measure for the purpose of checking the quality of a cluster partition for a sample into several distinct classes, and thus, determine the unknown value for the true number of clusters prevailing the provided set of data. Our objective leads us to the development of an approach through applying the multinomial distribution to the distances of data members, clustered in a group, from their respective cluster representatives. This procedure is carried out independently for each of the clusters, and the concerned statistics are combined together to design our targeted measure. Individual clusters separately possess the category-wise probabilities which correspond to different positions of its members in the cluster with respect to a typical member, in the form of cluster-centroid, medoid or mode, referred to as the corresponding cluster representative. Our method is robust in the sense that it is distribution-free, since this is devised irrespective of the parent distribution of the underlying sample. It fulfills one of the rare coveted qualities, present in the existing cluster accuracy measures, of having the capability to investigate whether the assigned sample owns any inherent clusters other than a single group of all members or not. Our measure's simple concept, easy algorithm, fast runtime, good performance, and wide usefulness, demonstrated through extensive simulation and diverse case-studies, make it appealing.","sentences":["In this paper, we advocate a novel measure for the purpose of checking the quality of a cluster partition for a sample into several distinct classes, and thus, determine the unknown value for the true number of clusters prevailing the provided set of data.","Our objective leads us to the development of an approach through applying the multinomial distribution to the distances of data members, clustered in a group, from their respective cluster representatives.","This procedure is carried out independently for each of the clusters, and the concerned statistics are combined together to design our targeted measure.","Individual clusters separately possess the category-wise probabilities which correspond to different positions of its members in the cluster with respect to a typical member, in the form of cluster-centroid, medoid or mode, referred to as the corresponding cluster representative.","Our method is robust in the sense that it is distribution-free, since this is devised irrespective of the parent distribution of the underlying sample.","It fulfills one of the rare coveted qualities, present in the existing cluster accuracy measures, of having the capability to investigate whether the assigned sample owns any inherent clusters other than a single group of all members or not.","Our measure's simple concept, easy algorithm, fast runtime, good performance, and wide usefulness, demonstrated through extensive simulation and diverse case-studies, make it appealing."],"url":"http://arxiv.org/abs/2404.07778v1","category":"stat.AP"}
{"created":"2024-04-11 13:47:47","title":"Control of the Schr\u00f6dinger equation in $\\mathbb{R}^3$: The critical case","abstract":"This article deals with the $\\dot{H}^{1}$--level exact controllability for the defocusing critical nonlinear Schr\\\"{o}dinger equation in $\\mathbb{R}^3$. Firstly, we show the problem under consideration to be well-posed using Strichartz estimates. Moreover, through the Hilbert uniqueness method, we prove the linear Schr\\\"{o}dinger equation to be controllable. Finally, we use a perturbation argument and show local exact controllability for the critical nonlinear Schr\\\"{o}dinger equation.","sentences":["This article deals with the $\\dot{H}^{1}$--level exact controllability for the defocusing critical nonlinear Schr\\\"{o}dinger equation in $\\mathbb{R}^3$. Firstly, we show the problem under consideration to be well-posed using Strichartz estimates.","Moreover, through the Hilbert uniqueness method, we prove the linear Schr\\\"{o}dinger equation to be controllable.","Finally, we use a perturbation argument and show local exact controllability for the critical nonlinear Schr\\\"{o}dinger equation."],"url":"http://arxiv.org/abs/2404.07749v1","category":"math.AP"}
{"created":"2024-04-11 13:45:46","title":"Chromoelectric flux tubes within non-Abelian Proca theory","abstract":"Flux tube solutions within non-Abelian SU(3) Proca theory with external sources are obtained. It is shown that such tubes have a longitudinal chromoelectric field possessing two components (nonlinear and gradient), as well as a transverse chromomagnetic field whose force lines create concentric circles with the center on the axis of the tube. The scenario of a possible relationship between non-Abelian Proca theory and quantum chromodynamics is considered. In such scenario: (a)~the components of color fields have different behavior: those which are almost classical, and those which are purely quantum; (b)~the second components create a gluon condensate that is a source of the field for the almost classical components of the Proca field; (c)~Proca masses may appear as a result of an approximate description of the gluon condensate. It is shown that the results obtained are in good agreement with the results of lattice calculations. We make an assumption that an approximate description of a flux tube in quantum chromodynamics can be carried out using classical Proca equations but with a mandatory account of a gluon condensate.","sentences":["Flux tube solutions within non-Abelian SU(3)","Proca theory with external sources are obtained.","It is shown that such tubes have a longitudinal chromoelectric field possessing two components (nonlinear and gradient), as well as a transverse chromomagnetic field whose force lines create concentric circles with the center on the axis of the tube.","The scenario of a possible relationship between non-Abelian Proca theory and quantum chromodynamics is considered.","In such scenario: (a)~the components of color fields have different behavior: those which are almost classical, and those which are purely quantum; (b)~the second components create a gluon condensate that is a source of the field for the almost classical components of the Proca field; (c)~Proca masses may appear as a result of an approximate description of the gluon condensate.","It is shown that the results obtained are in good agreement with the results of lattice calculations.","We make an assumption that an approximate description of a flux tube in quantum chromodynamics can be carried out using classical Proca equations but with a mandatory account of a gluon condensate."],"url":"http://arxiv.org/abs/2404.07747v1","category":"hep-th"}
{"created":"2024-04-11 12:58:40","title":"Large-deviations approach to thermalization: the case of harmonic chains with conservative noise","abstract":"We investigate the possibility of characterizing the different thermalization pathways through a large-deviation approach. Specifically, we consider clean, disordered and quasi-periodic harmonic chains under energy and momentum-conserving noise. For their associated master equations, describing the dynamics of normal modes energies, we compute the fluctuations of activity and dynamical entropy in the corresponding biased ensembles. First-order dynamical phase transition are found that originates from different activity regions in action space. At the transitions, the steady-state in the biased ensembles changes from extended to localized, yielding a kind of condensation in normal-modes space. For the disordered and quasi-periodic models, we argue that the phase-diagram has a critical point at a finite value of the disorder or potential strength.","sentences":["We investigate the possibility of characterizing the different thermalization pathways through a large-deviation approach.","Specifically, we consider clean, disordered and quasi-periodic harmonic chains under energy and momentum-conserving noise.","For their associated master equations, describing the dynamics of normal modes energies, we compute the fluctuations of activity and dynamical entropy in the corresponding biased ensembles.","First-order dynamical phase transition are found that originates from different activity regions in action space.","At the transitions, the steady-state in the biased ensembles changes from extended to localized, yielding a kind of condensation in normal-modes space.","For the disordered and quasi-periodic models, we argue that the phase-diagram has a critical point at a finite value of the disorder or potential strength."],"url":"http://arxiv.org/abs/2404.07712v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-11 12:53:23","title":"A Geometrical Analysis of Kernel Ridge Regression and its Applications","abstract":"We obtain upper bounds for the estimation error of Kernel Ridge Regression (KRR) for all non-negative regularization parameters, offering a geometric perspective on various phenomena in KRR. As applications: 1. We address the multiple descent problem, unifying the proofs of arxiv:1908.10292 and arxiv:1904.12191 for polynomial kernels and we establish multiple descent for the upper bound of estimation error of KRR under sub-Gaussian design and non-asymptotic regimes. 2. For a sub-Gaussian design vector and for non-asymptotic scenario, we prove the Gaussian Equivalent Conjecture. 3. We offer a novel perspective on the linearization of kernel matrices of non-linear kernel, extending it to the power regime for polynomial kernels. 4. Our theory is applicable to data-dependent kernels, providing a convenient and accurate tool for the feature learning regime in deep learning theory. 5. Our theory extends the results in arxiv:2009.14286 under weak moment assumption.   Our proof is based on three mathematical tools developed in this paper that can be of independent interest: 1. Dvoretzky-Milman theorem for ellipsoids under (very) weak moment assumptions. 2. Restricted Isomorphic Property in Reproducing Kernel Hilbert Spaces with embedding index conditions. 3. A concentration inequality for finite-degree polynomial kernel functions.","sentences":["We obtain upper bounds for the estimation error of Kernel Ridge Regression (KRR) for all non-negative regularization parameters, offering a geometric perspective on various phenomena in KRR.","As applications: 1.","We address the multiple descent problem, unifying the proofs of arxiv:1908.10292 and arxiv:1904.12191 for polynomial kernels and we establish multiple descent for the upper bound of estimation error of KRR under sub-Gaussian design and non-asymptotic regimes.","2.","For a sub-Gaussian design vector and for non-asymptotic scenario, we prove the Gaussian Equivalent Conjecture.","3.","We offer a novel perspective on the linearization of kernel matrices of non-linear kernel, extending it to the power regime for polynomial kernels.","4.","Our theory is applicable to data-dependent kernels, providing a convenient and accurate tool for the feature learning regime in deep learning theory.","5.","Our theory extends the results in arxiv:2009.14286 under weak moment assumption.   ","Our proof is based on three mathematical tools developed in this paper that can be of independent interest: 1. Dvoretzky-Milman theorem for ellipsoids under (very) weak moment assumptions.","2. Restricted Isomorphic Property in Reproducing Kernel Hilbert Spaces with embedding index conditions.","3.","A concentration inequality for finite-degree polynomial kernel functions."],"url":"http://arxiv.org/abs/2404.07709v1","category":"math.ST"}
{"created":"2024-04-11 12:49:20","title":"Parton-shower effects in polarized deep inelastic scattering","abstract":"We present a Monte-Carlo program for the simulation of polarized deep inelastic scattering at next-to-leading order in QCD matched to parton shower programs building on an existing implementation of the unpolarized case in the POWHEG BOX package. We discuss extensions of the POWHEG BOX framework necessary to account for polarized initial states and validate the code by detailed comparisons to existing fixed-order results. We then use the new tool to make predictions for the upcoming Electron Ion Collider. We find that parton-shower effects do have an impact on experimentally accessible distributions and improve the agreement with the next-to-next-to-leading order results.","sentences":["We present a Monte-Carlo program for the simulation of polarized deep inelastic scattering at next-to-leading order in QCD matched to parton shower programs building on an existing implementation of the unpolarized case in the POWHEG BOX package.","We discuss extensions of the POWHEG BOX framework necessary to account for polarized initial states and validate the code by detailed comparisons to existing fixed-order results.","We then use the new tool to make predictions for the upcoming Electron Ion Collider.","We find that parton-shower effects do have an impact on experimentally accessible distributions and improve the agreement with the next-to-next-to-leading order results."],"url":"http://arxiv.org/abs/2404.07702v1","category":"hep-ph"}
{"created":"2024-04-11 12:40:03","title":"A martingale approach to Gaussian fluctuations and laws of iterated logarithm for Ewens-Pitman model","abstract":"The Ewens-Pitman model refers to a distribution for exchangeable random partitions of $[n]=\\{1,\\ldots,n\\}$, which is indexed by a pair of parameters $\\alpha\\in[0,1)$ and $\\theta>-\\alpha$, with $\\alpha=0$ corresponding to the celebrated Ewens model in population genetics. The large $n$ asymptotic properties of the Ewens-Pitman model have been the subject of numerous studies, with the focus being on the number $K_{n}$ of partition sets and the number $K_{r,n}$ of partition sets that appear $r$ times, for $r=1,\\ldots,n$. While for $\\alpha=0$ asymptotic results have been obtained in terms of almost-sure convergence and Gaussian fluctuations, for $\\alpha\\in(0,1)$ only almost-sure convergences are available, with the proof for $K_{r,m}$ being given only as a sketch. In this paper, we make use of martingales to develop a unified and comprehensive treatment of the large $n$ asymptotic behaviours of $K_{n}$ and $K_{r,n}$ for $\\alpha\\in(0,1)$, providing alternative, and rigorous, proofs of the almost-sure convergences of $K_{n}$ and $K_{r,n}$, and covering the gap of Gaussian fluctuations. We also obtain new laws of the iterated logarithm for $K_{n}$ and $K_{r,n}$.","sentences":["The Ewens-Pitman model refers to a distribution for exchangeable random partitions of $[n]=\\{1,\\ldots,n\\}$, which is indexed by a pair of parameters $\\alpha\\in[0,1)$ and $\\theta>-\\alpha$, with $\\alpha=0$ corresponding to the celebrated Ewens model in population genetics.","The large $n$ asymptotic properties of the Ewens-Pitman model have been the subject of numerous studies, with the focus being on the number $K_{n}$ of partition sets and the number $K_{r,n}$ of partition sets that appear $r$ times, for $r=1,\\ldots,n$. While for $\\alpha=0$ asymptotic results have been obtained in terms of almost-sure convergence and Gaussian fluctuations, for $\\alpha\\in(0,1)$ only almost-sure convergences are available, with the proof for $K_{r,m}$ being given only as a sketch.","In this paper, we make use of martingales to develop a unified and comprehensive treatment of the large $n$ asymptotic behaviours of $K_{n}$ and $K_{r,n}$ for $\\alpha\\in(0,1)$, providing alternative, and rigorous, proofs of the almost-sure convergences of $K_{n}$ and $K_{r,n}$, and covering the gap of Gaussian fluctuations.","We also obtain new laws of the iterated logarithm for $K_{n}$ and $K_{r,n}$."],"url":"http://arxiv.org/abs/2404.07694v1","category":"math.PR"}
{"created":"2024-04-11 12:39:31","title":"The photoinduced hidden metallic phase of monoclinic VO2 driven by local nucleation via a self-amplification process","abstract":"The insulator-to-metal transition (IMT) in vanadium dioxide (VO2) has garnered extensive attention for its potential applications in ultrafast switches, neuronal network architectures, and storage technologies. However, a significant controversy persists regarding the formation of the IMT, specifically concerning whether a complete structural phase transition from monoclinic (M1) to rutile (R) phase is necessary. Here we employ the real-time time-dependent density functional theory (rt-TDDFT) to track the dynamic evolution of atomic and electronic structures in photoexcited VO2, revealing the emergence of a long-lived monoclinic metal phase (MM) under low electronic excitation. The emergence of the metal phase in the monoclinic structure originates from the dissociation of the local V-V dimer, driven by the self-trapped and self-amplified dynamics of photoexcited holes, rather than by a pure electron-electron correction. On the other hand, the M1-to-R phase transition does appear at higher electronic excitation. Our findings validate the existence of MM phase and provide a comprehensive picture of the IMT in photoexcited VO2.","sentences":["The insulator-to-metal transition (IMT) in vanadium dioxide (VO2) has garnered extensive attention for its potential applications in ultrafast switches, neuronal network architectures, and storage technologies.","However, a significant controversy persists regarding the formation of the IMT, specifically concerning whether a complete structural phase transition from monoclinic (M1) to rutile (R) phase is necessary.","Here we employ the real-time time-dependent density functional theory (rt-TDDFT) to track the dynamic evolution of atomic and electronic structures in photoexcited VO2, revealing the emergence of a long-lived monoclinic metal phase (MM) under low electronic excitation.","The emergence of the metal phase in the monoclinic structure originates from the dissociation of the local V-V dimer, driven by the self-trapped and self-amplified dynamics of photoexcited holes, rather than by a pure electron-electron correction.","On the other hand, the M1-to-R phase transition does appear at higher electronic excitation.","Our findings validate the existence of MM phase and provide a comprehensive picture of the IMT in photoexcited VO2."],"url":"http://arxiv.org/abs/2404.07693v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 12:06:50","title":"Deep learning-driven pulmonary arteries and veins segmentation reveals demography-associated pulmonary vasculature anatomy","abstract":"Pulmonary artery-vein segmentation is crucial for diagnosing pulmonary diseases and surgical planning, and is traditionally achieved by Computed Tomography Pulmonary Angiography (CTPA). However, concerns regarding adverse health effects from contrast agents used in CTPA have constrained its clinical utility. In contrast, identifying arteries and veins using non-contrast CT, a conventional and low-cost clinical examination routine, has long been considered impossible. Here we propose a High-abundant Pulmonary Artery-vein Segmentation (HiPaS) framework achieving accurate artery-vein segmentation on both non-contrast CT and CTPA across various spatial resolutions. HiPaS first performs spatial normalization on raw CT scans via a super-resolution module, and then iteratively achieves segmentation results at different branch levels by utilizing the low-level vessel segmentation as a prior for high-level vessel segmentation. We trained and validated HiPaS on our established multi-centric dataset comprising 1,073 CT volumes with meticulous manual annotation. Both quantitative experiments and clinical evaluation demonstrated the superior performance of HiPaS, achieving a dice score of 91.8% and a sensitivity of 98.0%. Further experiments demonstrated the non-inferiority of HiPaS segmentation on non-contrast CT compared to segmentation on CTPA. Employing HiPaS, we have conducted an anatomical study of pulmonary vasculature on 10,613 participants in China (five sites), discovering a new association between pulmonary vessel abundance and sex and age: vessel abundance is significantly higher in females than in males, and slightly decreases with age, under the controlling of lung volumes (p < 0.0001). HiPaS realizing accurate artery-vein segmentation delineates a promising avenue for clinical diagnosis and understanding pulmonary physiology in a non-invasive manner.","sentences":["Pulmonary artery-vein segmentation is crucial for diagnosing pulmonary diseases and surgical planning, and is traditionally achieved by Computed Tomography Pulmonary Angiography (CTPA).","However, concerns regarding adverse health effects from contrast agents used in CTPA have constrained its clinical utility.","In contrast, identifying arteries and veins using non-contrast CT, a conventional and low-cost clinical examination routine, has long been considered impossible.","Here we propose a High-abundant Pulmonary Artery-vein Segmentation (HiPaS) framework achieving accurate artery-vein segmentation on both non-contrast CT and CTPA across various spatial resolutions.","HiPaS first performs spatial normalization on raw CT scans via a super-resolution module, and then iteratively achieves segmentation results at different branch levels by utilizing the low-level vessel segmentation as a prior for high-level vessel segmentation.","We trained and validated HiPaS on our established multi-centric dataset comprising 1,073 CT volumes with meticulous manual annotation.","Both quantitative experiments and clinical evaluation demonstrated the superior performance of HiPaS, achieving a dice score of 91.8% and a sensitivity of 98.0%.","Further experiments demonstrated the non-inferiority of HiPaS segmentation on non-contrast CT compared to segmentation on CTPA.","Employing HiPaS",", we have conducted an anatomical study of pulmonary vasculature on 10,613 participants in China (five sites), discovering a new association between pulmonary vessel abundance and sex and age: vessel abundance is significantly higher in females than in males, and slightly decreases with age, under the controlling of lung volumes (p < 0.0001).","HiPaS realizing accurate artery-vein segmentation delineates a promising avenue for clinical diagnosis and understanding pulmonary physiology in a non-invasive manner."],"url":"http://arxiv.org/abs/2404.07671v1","category":"cs.CV"}
{"created":"2024-04-11 11:50:05","title":"Robust performance metrics for imbalanced classification problems","abstract":"We show that established performance metrics in binary classification, such as the F-score, the Jaccard similarity coefficient or Matthews' correlation coefficient (MCC), are not robust to class imbalance in the sense that if the proportion of the minority class tends to $0$, the true positive rate (TPR) of the Bayes classifier under these metrics tends to $0$ as well. Thus, in imbalanced classification problems, these metrics favour classifiers which ignore the minority class. To alleviate this issue we introduce robust modifications of the F-score and the MCC for which, even in strongly imbalanced settings, the TPR is bounded away from $0$. We numerically illustrate the behaviour of the various performance metrics in simulations as well as on a credit default data set. We also discuss connections to the ROC and precision-recall curves and give recommendations on how to combine their usage with performance metrics.","sentences":["We show that established performance metrics in binary classification, such as the F-score, the Jaccard similarity coefficient or Matthews' correlation coefficient (MCC), are not robust to class imbalance in the sense that if the proportion of the minority class tends to $0$, the true positive rate (TPR) of the Bayes classifier under these metrics tends to $0$ as well.","Thus, in imbalanced classification problems, these metrics favour classifiers which ignore the minority class.","To alleviate this issue we introduce robust modifications of the F-score and the MCC for which, even in strongly imbalanced settings, the TPR is bounded away from $0$. We numerically illustrate the behaviour of the various performance metrics in simulations as well as on a credit default data set.","We also discuss connections to the ROC and precision-recall curves and give recommendations on how to combine their usage with performance metrics."],"url":"http://arxiv.org/abs/2404.07661v1","category":"stat.ML"}
{"created":"2024-04-11 11:48:10","title":"Approximation of Random Evolution Equations","abstract":"In this paper, we present an abstract framework to obtain convergence rates for the approximation of random evolution equations corresponding to a random family of forms determined by finite-dimensional noise. The full discretisation error in space, time, and randomness is considered, where polynomial chaos expansion (PCE) is used for the semi-discretisation in randomness. The main result are regularity conditions on the random forms under which convergence of polynomial order in randomness is obtained depending on the smoothness of the coefficients and the Sobolev regularity of the initial value. In space and time, the same convergence rates as in the deterministic setting are achieved. To this end, we derive error estimates for vector-valued PCE as well as a quantified version of the Trotter-Kato theorem for form-induced semigroups.","sentences":["In this paper, we present an abstract framework to obtain convergence rates for the approximation of random evolution equations corresponding to a random family of forms determined by finite-dimensional noise.","The full discretisation error in space, time, and randomness is considered, where polynomial chaos expansion (PCE) is used for the semi-discretisation in randomness.","The main result are regularity conditions on the random forms under which convergence of polynomial order in randomness is obtained depending on the smoothness of the coefficients and the Sobolev regularity of the initial value.","In space and time, the same convergence rates as in the deterministic setting are achieved.","To this end, we derive error estimates for vector-valued PCE as well as a quantified version of the Trotter-Kato theorem for form-induced semigroups."],"url":"http://arxiv.org/abs/2404.07660v1","category":"math.FA"}
{"created":"2024-04-11 11:41:03","title":"Topology of shallow-water waves on the rotating sphere","abstract":"Topological properties of the spectrum of shallow-water waves on a rotating spherical body are established. Particular attention is paid to its spectral flow, i.e. the modes whose frequencies transit between the Rossby and inertia-gravity wavebands as the zonal wave number is varied. Organising the modes according to the number of zeros of their meridional velocity, we conclude that the net number of modes transiting between the shallow-water wavebands on the sphere is null, in contrast with the Matsuno spectrum. This difference can be explained by a miscount of zeros under the $\\beta$-plane approximation. We corroborate this result with the analysis of Delplace et al (2017) by showing that the curved metric discloses a pair of degeneracy points in the Weyl symbol of the wave operator, non-existent under the $\\beta$-plane approximation, each of them bearing a Chern number $-1$.","sentences":["Topological properties of the spectrum of shallow-water waves on a rotating spherical body are established.","Particular attention is paid to its spectral flow, i.e. the modes whose frequencies transit between the Rossby and inertia-gravity wavebands as the zonal wave number is varied.","Organising the modes according to the number of zeros of their meridional velocity, we conclude that the net number of modes transiting between the shallow-water wavebands on the sphere is null, in contrast with the Matsuno spectrum.","This difference can be explained by a miscount of zeros under the $\\beta$-plane approximation.","We corroborate this result with the analysis of Delplace et al (2017) by showing that the curved metric discloses a pair of degeneracy points in the Weyl symbol of the wave operator, non-existent under the $\\beta$-plane approximation, each of them bearing a Chern number $-1$."],"url":"http://arxiv.org/abs/2404.07655v1","category":"physics.flu-dyn"}
{"created":"2024-04-11 11:12:06","title":"Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method","abstract":"In this paper we have present an improved Cycle GAN based model for under water image enhancement. We have utilized the cycle consistent learning technique of the state-of-the-art Cycle GAN model with modification in the loss function in terms of depth-oriented attention which enhance the contrast of the overall image, keeping global content, color, local texture, and style information intact. We trained the Cycle GAN model with the modified loss functions on the benchmarked Enhancing Underwater Visual Perception (EUPV) dataset a large dataset including paired and unpaired sets of underwater images (poor and good quality) taken with seven distinct cameras in a range of visibility situation during research on ocean exploration and human-robot cooperation. In addition, we perform qualitative and quantitative evaluation which supports the given technique applied and provided a better contrast enhancement model of underwater imagery. More significantly, the upgraded images provide better results from conventional models and further for under water navigation, pose estimation, saliency prediction, object detection and tracking. The results validate the appropriateness of the model for autonomous underwater vehicles (AUV) in visual navigation.","sentences":["In this paper we have present an improved Cycle GAN based model for under water image enhancement.","We have utilized the cycle consistent learning technique of the state-of-the-art Cycle GAN model with modification in the loss function in terms of depth-oriented attention which enhance the contrast of the overall image, keeping global content, color, local texture, and style information intact.","We trained the Cycle GAN model with the modified loss functions on the benchmarked Enhancing Underwater Visual Perception (EUPV) dataset a large dataset including paired and unpaired sets of underwater images (poor and good quality) taken with seven distinct cameras in a range of visibility situation during research on ocean exploration and human-robot cooperation.","In addition, we perform qualitative and quantitative evaluation which supports the given technique applied and provided a better contrast enhancement model of underwater imagery.","More significantly, the upgraded images provide better results from conventional models and further for under water navigation, pose estimation, saliency prediction, object detection and tracking.","The results validate the appropriateness of the model for autonomous underwater vehicles (AUV) in visual navigation."],"url":"http://arxiv.org/abs/2404.07649v1","category":"cs.CV"}
{"created":"2024-04-11 11:04:26","title":"Accessing the Free Expansion and Melting of a Crystalline Drop of Charged Colloidal Spheres in a Particle-Free Environment by Optical Experiments","abstract":"We address crystals of non-attractive colloidal spheres freely expanding into particle-free environ-ments and melting during dilution. This problem has been studied in two dimensions, both numeri-cally and in experiments on colloidal model crystals. Here, we place three-dimensional drops of aqueous colloidal charged sphere suspensions in a colloid-free, deionized aqueous environment. Initially in a shear-molten state, they rapidly crystallize to a fine-grained polycrystalline material of body centred cubic structure. They stabilize their spherical shape within a few seconds. We over-come the challenges provided by drop turbidity and use a combination of optical methods to follow the drop evolution. The crystal ball shows a nearly fourfold increase of the volume followed by slow shrinkage due to melting, which is nearly linear in time. Exploiting coherent multiple-scattering by (110) Bragg reflecting crystals, time-dependent density profiles were recorded within the drop interior. These show a continuously flattening radial density gradient. Our experimental situation is close to the isothermal three-dimensional expansion of a spherical crystallite as described by a theoretical model based on dynamical density functional theory. We obtain an overall good agree-ment of measured and calculated expansion curves at most probed densities. We anticipate that our study opens novel experimental and theoretical access to a long-standing condensed matter issue.","sentences":["We address crystals of non-attractive colloidal spheres freely expanding into particle-free environ-ments and melting during dilution.","This problem has been studied in two dimensions, both numeri-cally and in experiments on colloidal model crystals.","Here, we place three-dimensional drops of aqueous colloidal charged sphere suspensions in a colloid-free, deionized aqueous environment.","Initially in a shear-molten state, they rapidly crystallize to a fine-grained polycrystalline material of body centred cubic structure.","They stabilize their spherical shape within a few seconds.","We over-come the challenges provided by drop turbidity and use a combination of optical methods to follow the drop evolution.","The crystal ball shows a nearly fourfold increase of the volume followed by slow shrinkage due to melting, which is nearly linear in time.","Exploiting coherent multiple-scattering by (110) Bragg reflecting crystals, time-dependent density profiles were recorded within the drop interior.","These show a continuously flattening radial density gradient.","Our experimental situation is close to the isothermal three-dimensional expansion of a spherical crystallite as described by a theoretical model based on dynamical density functional theory.","We obtain an overall good agree-ment of measured and calculated expansion curves at most probed densities.","We anticipate that our study opens novel experimental and theoretical access to a long-standing condensed matter issue."],"url":"http://arxiv.org/abs/2404.07642v1","category":"cond-mat.soft"}
{"created":"2024-04-11 10:14:56","title":"Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation","abstract":"Accurately segmenting the pancreas remains a huge challenge. Traditional methods encounter difficulties in semantic localization due to the small volume and distorted structure of the pancreas, while deep learning methods encounter challenges in obtaining accurate edges because of low contrast and organ overlapping. To overcome these issues, we propose a multi-cue level set method based on the diffusion probabilistic model, namely Diff-mcs. Our method adopts a coarse-to-fine segmentation strategy. We use the diffusion probabilistic model in the coarse segmentation stage, with the obtained probability distribution serving as both the initial localization and prior cues for the level set method. In the fine segmentation stage, we combine the prior cues with grayscale cues and texture cues to refine the edge by maximizing the difference between probability distributions of the cues inside and outside the level set curve. The method is validated on three public datasets and achieves state-of-the-art performance, which can obtain more accurate segmentation results with lower uncertainty segmentation edges. In addition, we conduct ablation studies and uncertainty analysis to verify that the diffusion probability model provides a more appropriate initialization for the level set method. Furthermore, when combined with multiple cues, the level set method can better obtain edges and improve the overall accuracy. Our code is available at https://github.com/GOUYUEE/Diff-mcs.","sentences":["Accurately segmenting the pancreas remains a huge challenge.","Traditional methods encounter difficulties in semantic localization due to the small volume and distorted structure of the pancreas, while deep learning methods encounter challenges in obtaining accurate edges because of low contrast and organ overlapping.","To overcome these issues, we propose a multi-cue level set method based on the diffusion probabilistic model, namely Diff-mcs.","Our method adopts a coarse-to-fine segmentation strategy.","We use the diffusion probabilistic model in the coarse segmentation stage, with the obtained probability distribution serving as both the initial localization and prior cues for the level set method.","In the fine segmentation stage, we combine the prior cues with grayscale cues and texture cues to refine the edge by maximizing the difference between probability distributions of the cues inside and outside the level set curve.","The method is validated on three public datasets and achieves state-of-the-art performance, which can obtain more accurate segmentation results with lower uncertainty segmentation edges.","In addition, we conduct ablation studies and uncertainty analysis to verify that the diffusion probability model provides a more appropriate initialization for the level set method.","Furthermore, when combined with multiple cues, the level set method can better obtain edges and improve the overall accuracy.","Our code is available at https://github.com/GOUYUEE/Diff-mcs."],"url":"http://arxiv.org/abs/2404.07620v1","category":"eess.IV"}
{"created":"2024-04-11 09:13:52","title":"M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation","abstract":"We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.","sentences":["We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data.","Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios.","However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance.","Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models.","To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan).","This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario.","Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios.","Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models."],"url":"http://arxiv.org/abs/2404.07581v1","category":"cs.IR"}
{"created":"2024-04-11 09:13:50","title":"Multi-rater Prompting for Ambiguous Medical Image Segmentation","abstract":"Multi-rater annotations commonly occur when medical images are independently annotated by multiple experts (raters). In this paper, we tackle two challenges arisen in multi-rater annotations for medical image segmentation (called ambiguous medical image segmentation): (1) How to train a deep learning model when a group of raters produces a set of diverse but plausible annotations, and (2) how to fine-tune the model efficiently when computation resources are not available for re-training the entire model on a different dataset domain. We propose a multi-rater prompt-based approach to address these two challenges altogether. Specifically, we introduce a series of rater-aware prompts that can be plugged into the U-Net model for uncertainty estimation to handle multi-annotation cases. During the prompt-based fine-tuning process, only 0.3% of learnable parameters are required to be updated comparing to training the entire model. Further, in order to integrate expert consensus and disagreement, we explore different multi-rater incorporation strategies and design a mix-training strategy for comprehensive insight learning. Extensive experiments verify the effectiveness of our new approach for ambiguous medical image segmentation on two public datasets while alleviating the heavy burden of model re-training.","sentences":["Multi-rater annotations commonly occur when medical images are independently annotated by multiple experts (raters).","In this paper, we tackle two challenges arisen in multi-rater annotations for medical image segmentation (called ambiguous medical image segmentation): (1) How to train a deep learning model when a group of raters produces a set of diverse but plausible annotations, and (2) how to fine-tune the model efficiently when computation resources are not available for re-training the entire model on a different dataset domain.","We propose a multi-rater prompt-based approach to address these two challenges altogether.","Specifically, we introduce a series of rater-aware prompts that can be plugged into the U-Net model for uncertainty estimation to handle multi-annotation cases.","During the prompt-based fine-tuning process, only 0.3% of learnable parameters are required to be updated comparing to training the entire model.","Further, in order to integrate expert consensus and disagreement, we explore different multi-rater incorporation strategies and design a mix-training strategy for comprehensive insight learning.","Extensive experiments verify the effectiveness of our new approach for ambiguous medical image segmentation on two public datasets while alleviating the heavy burden of model re-training."],"url":"http://arxiv.org/abs/2404.07580v1","category":"cs.CV"}
{"created":"2024-04-11 08:58:08","title":"Evolution of rotating massive stars adopting a newer, self-consistent wind prescription at SMC metallicity","abstract":"We use Geneva-evolution-code to run evolutionary tracks for stellar masses ranging from $20$ to $85$ $M_\\odot$ at SMC metallicity ($Z=0.002$). We upgrade the recipe for stellar winds by adopting our self-consistent m-CAK prescription, which reduces the value of mass-loss rate by a factor between 2 and 6 depending on the mass range.   The impact of our new winds is wide, and it can be divided between direct and indirect impact. For the most massive models ($60$ and $85$ $M_\\odot$) with $\\dot M\\gtrsim2\\times10^{-7}$ $M_\\odot$ yr$^{-1}$, the impact is direct because lower mass loss make stars remove less envelope and therefore remain more massive and less chemically enriched at their surface at the end of their MS phase. For the less massive models ($20$ and $25$ $M_\\odot$) with $\\dot M\\lesssim2\\times10^{-8}$ $M_\\odot$ yr$^{-1}$, the impact is indirect because lower mass loss make the stars keep high rotational velocities for a longer period of time, then extending the H-core burning lifetime and reaching the end of the MS with higher surface enrichment. Given that the conditions at the H-depletion change, the stars will lose more mass during their He-core burning stages anyways. For $M_\\text{zams}=20$ to $40$ $M_\\odot$, our models predict stars will evolve through the Hertzsprung gap, from O-type supergiants to BSG and finally RSG, with larger mass fractions of helium compared to old evolution models. New models also set down to $M_\\text{zams}=85\\,M_\\odot$ the minimal initial mass required for a single star to become WR at metallicity $Z=0.002$.   New values for $\\dot M$ need to be complemented with upgrades in additional features such as convective core overshooting and distribution of rotational velocities, besides more detailed observations from projects such as XShootU, in order to provide a robust framework for the study of massive stars at low metallicity environments.","sentences":["We use Geneva-evolution-code to run evolutionary tracks for stellar masses ranging from $20$ to $85$ $M_\\odot$ at SMC metallicity ($Z=0.002$).","We upgrade the recipe for stellar winds by adopting our self-consistent m-CAK prescription, which reduces the value of mass-loss rate by a factor between 2 and 6 depending on the mass range.   ","The impact of our new winds is wide, and it can be divided between direct and indirect impact.","For the most massive models ($60$ and $85$ $M_\\odot$) with $\\dot M\\gtrsim2\\times10^{-7}$ $M_\\odot$ yr$^{-1}$, the impact is direct because lower mass loss make stars remove less envelope and therefore remain more massive and less chemically enriched at their surface at the end of their MS phase.","For the less massive models ($20$ and $25$ $M_\\odot$) with $\\dot M\\lesssim2\\times10^{-8}$ $M_\\odot$ yr$^{-1}$, the impact is indirect because lower mass loss make the stars keep high rotational velocities for a longer period of time, then extending the H-core burning lifetime and reaching the end of the MS with higher surface enrichment.","Given that the conditions at the H-depletion change, the stars will lose more mass during their He-core burning stages anyways.","For $M_\\text{zams}=20$ to $40$ $M_\\odot$, our models predict stars will evolve through the Hertzsprung gap, from O-type supergiants to BSG and finally RSG, with larger mass fractions of helium compared to old evolution models.","New models also set down to $M_\\text{zams}=85\\,M_\\odot$ the minimal initial mass required for a single star to become WR at metallicity $Z=0.002$.   New values for $\\dot M$ need to be complemented with upgrades in additional features such as convective core overshooting and distribution of rotational velocities, besides more detailed observations from projects such as XShootU, in order to provide a robust framework for the study of massive stars at low metallicity environments."],"url":"http://arxiv.org/abs/2404.07570v1","category":"astro-ph.SR"}
{"created":"2024-04-11 08:44:41","title":"Mode-resolved micromagnetics study of parametric spin wave excitation in thin-film disks","abstract":"We present a computational study of the parametric excitation of spin waves in thin film disks with a mode-resolved approach. The method involves projecting out the time-dependent magnetization, computed using micromagnetics simulations, onto the spatial profile of the eigenmodes that are obtained from the linearization of the equations of motion. Unlike spectral analysis in the frequency domain, the projection allows for the analysis of transient mode dynamics under parametric excitation. We apply this method to parallel pumping of quantized spin wave modes in in-plane magnetized thin-film disks, where phenomena like frequency pulling, mutual phase locking, and higher-order magnon scattering processes are identified.","sentences":["We present a computational study of the parametric excitation of spin waves in thin film disks with a mode-resolved approach.","The method involves projecting out the time-dependent magnetization, computed using micromagnetics simulations, onto the spatial profile of the eigenmodes that are obtained from the linearization of the equations of motion.","Unlike spectral analysis in the frequency domain, the projection allows for the analysis of transient mode dynamics under parametric excitation.","We apply this method to parallel pumping of quantized spin wave modes in in-plane magnetized thin-film disks, where phenomena like frequency pulling, mutual phase locking, and higher-order magnon scattering processes are identified."],"url":"http://arxiv.org/abs/2404.07561v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 08:08:36","title":"Poisson imbedding meets the Clark-Ocone formula","abstract":"In this paper we develop a representation formula of Clark-Ocone type for any integrable Poisson functionals, which extends the Poisson imbedding for point processes. This representation formula differs from the classical Clark-Ocone formula on three accounts. First the representation holds with respect to the Poisson measure instead of the compensated one; second the representation holds true in L1 and not in L2; and finally contrary to the classical Clark-Ocone formula the integrand is defined as a pathwise operator and not as a L2-limiting object. We make use of Malliavin's calculus and of the pseudo-chaotic decomposition with uncompensated iteraded integrals to establish this Pseudo-Clark-Ocone representation formula and to characterize the integrand, which turns out to be a predictable integrable process.","sentences":["In this paper we develop a representation formula of Clark-Ocone type for any integrable Poisson functionals, which extends the Poisson imbedding for point processes.","This representation formula differs from the classical Clark-Ocone formula on three accounts.","First the representation holds with respect to the Poisson measure instead of the compensated one; second the representation holds true in L1 and not in L2; and finally contrary to the classical Clark-Ocone formula the integrand is defined as a pathwise operator and not as a L2-limiting object.","We make use of Malliavin's calculus and of the pseudo-chaotic decomposition with uncompensated iteraded integrals to establish this Pseudo-Clark-Ocone representation formula and to characterize the integrand, which turns out to be a predictable integrable process."],"url":"http://arxiv.org/abs/2404.07541v1","category":"math.PR"}
{"created":"2024-04-11 08:04:13","title":"Second register production on the clarinet: nonlinear losses in the register hole as the decisive physical phenomenon","abstract":"This study investigates the role of localized nonlinear losses in the register hole on the production of second-register notes. First, an experiment is conducted to study the ability of a register hole to produce second register. A cylindrical tube is drilled with holes of increasing diameter. Five are at the same level as the register hole of a B-flat clarinet, and five are at the same level as the thumb hole. Participant clarinetists are then asked to play with constant control parameters. At the beginning of each measurement, all holes are closed. The operator then opens randomly one of the ten holes.The resulting register is noted. The experiment is replicated numerically by time integration of two different models. The first is the state-of-the-art model based on the modal decomposition of the input impedance of the resonator. The second accounts for localized nonlinear losses in the register hole, through the model from Dalmont and Nederveen (2002). These losses are handled through a variable modal coefficients method. For the first model, simulations never produce second register, for any of the open holes. For the second, the proportion of second-register production is close to the experiment for upstream holes, but remains at zero for downstream holes.","sentences":["This study investigates the role of localized nonlinear losses in the register hole on the production of second-register notes.","First, an experiment is conducted to study the ability of a register hole to produce second register.","A cylindrical tube is drilled with holes of increasing diameter.","Five are at the same level as the register hole of a B-flat clarinet, and five are at the same level as the thumb hole.","Participant clarinetists are then asked to play with constant control parameters.","At the beginning of each measurement, all holes are closed.","The operator then opens randomly one of the ten holes.","The resulting register is noted.","The experiment is replicated numerically by time integration of two different models.","The first is the state-of-the-art model based on the modal decomposition of the input impedance of the resonator.","The second accounts for localized nonlinear losses in the register hole, through the model from Dalmont and Nederveen (2002).","These losses are handled through a variable modal coefficients method.","For the first model, simulations never produce second register, for any of the open holes.","For the second, the proportion of second-register production is close to the experiment for upstream holes, but remains at zero for downstream holes."],"url":"http://arxiv.org/abs/2404.07540v1","category":"physics.class-ph"}
{"created":"2024-04-11 06:41:43","title":"Model Predictive Trajectory Planning for Human-Robot Handovers","abstract":"This work develops a novel trajectory planner for human-robot handovers. The handover requirements can naturally be handled by a path-following-based model predictive controller, where the path progress serves as a progress measure of the handover. Moreover, the deviations from the path are used to follow human motion by adapting the path deviation bounds with a handover location prediction. A Gaussian process regression model, which is trained on known handover trajectories, is employed for this prediction. Experiments with a collaborative 7-DoF robotic manipulator show the effectiveness and versatility of the proposed approach.","sentences":["This work develops a novel trajectory planner for human-robot handovers.","The handover requirements can naturally be handled by a path-following-based model predictive controller, where the path progress serves as a progress measure of the handover.","Moreover, the deviations from the path are used to follow human motion by adapting the path deviation bounds with a handover location prediction.","A Gaussian process regression model, which is trained on known handover trajectories, is employed for this prediction.","Experiments with a collaborative 7-DoF robotic manipulator show the effectiveness and versatility of the proposed approach."],"url":"http://arxiv.org/abs/2404.07505v1","category":"cs.RO"}
{"created":"2024-04-11 04:01:15","title":"Enhancing Network Intrusion Detection Performance using Generative Adversarial Networks","abstract":"Network intrusion detection systems (NIDS) play a pivotal role in safeguarding critical digital infrastructures against cyber threats. Machine learning-based detection models applied in NIDS are prevalent today. However, the effectiveness of these machine learning-based models is often limited by the evolving and sophisticated nature of intrusion techniques as well as the lack of diverse and updated training samples. In this research, a novel approach for enhancing the performance of an NIDS through the integration of Generative Adversarial Networks (GANs) is proposed. By harnessing the power of GANs in generating synthetic network traffic data that closely mimics real-world network behavior, we address a key challenge associated with NIDS training datasets, which is the data scarcity. Three distinct GAN models (Vanilla GAN, Wasserstein GAN and Conditional Tabular GAN) are implemented in this work to generate authentic network traffic patterns specifically tailored to represent the anomalous activity. We demonstrate how this synthetic data resampling technique can significantly improve the performance of the NIDS model for detecting such activity. By conducting comprehensive experiments using the CIC-IDS2017 benchmark dataset, augmented with GAN-generated data, we offer empirical evidence that shows the effectiveness of our proposed approach. Our findings show that the integration of GANs into NIDS can lead to enhancements in intrusion detection performance for attacks with limited training data, making it a promising avenue for bolstering the cybersecurity posture of organizations in an increasingly interconnected and vulnerable digital landscape.","sentences":["Network intrusion detection systems (NIDS) play a pivotal role in safeguarding critical digital infrastructures against cyber threats.","Machine learning-based detection models applied in NIDS are prevalent today.","However, the effectiveness of these machine learning-based models is often limited by the evolving and sophisticated nature of intrusion techniques as well as the lack of diverse and updated training samples.","In this research, a novel approach for enhancing the performance of an NIDS through the integration of Generative Adversarial Networks (GANs) is proposed.","By harnessing the power of GANs in generating synthetic network traffic data that closely mimics real-world network behavior, we address a key challenge associated with NIDS training datasets, which is the data scarcity.","Three distinct GAN models (Vanilla GAN, Wasserstein GAN and Conditional Tabular GAN) are implemented in this work to generate authentic network traffic patterns specifically tailored to represent the anomalous activity.","We demonstrate how this synthetic data resampling technique can significantly improve the performance of the NIDS model for detecting such activity.","By conducting comprehensive experiments using the CIC-IDS2017 benchmark dataset, augmented with GAN-generated data, we offer empirical evidence that shows the effectiveness of our proposed approach.","Our findings show that the integration of GANs into NIDS can lead to enhancements in intrusion detection performance for attacks with limited training data, making it a promising avenue for bolstering the cybersecurity posture of organizations in an increasingly interconnected and vulnerable digital landscape."],"url":"http://arxiv.org/abs/2404.07464v1","category":"cs.CR"}
{"created":"2024-04-11 03:41:23","title":"A Proximal-Gradient Method for Constrained Optimization","abstract":"We present a new algorithm for solving optimization problems with objective functions that are the sum of a smooth function and a (potentially) nonsmooth regularization function, and nonlinear equality constraints. The algorithm may be viewed as an extension of the well-known proximal-gradient method that is applicable when constraints are not present. To account for nonlinear equality constraints, we combine a decomposition procedure for computing trial steps with an exact merit function for determining trial step acceptance. Under common assumptions, we show that both the proximal parameter and merit function parameter eventually remain fixed, and then prove a worst-case complexity result for the maximum number of iterations before an iterate satisfying approximate first-order optimality conditions for a given tolerance is computed. Our preliminary numerical results indicate that our approach has great promise, especially in terms of returning approximate solutions that are structured (e.g., sparse solutions when a one-norm regularizer is used).","sentences":["We present a new algorithm for solving optimization problems with objective functions that are the sum of a smooth function and a (potentially) nonsmooth regularization function, and nonlinear equality constraints.","The algorithm may be viewed as an extension of the well-known proximal-gradient method that is applicable when constraints are not present.","To account for nonlinear equality constraints, we combine a decomposition procedure for computing trial steps with an exact merit function for determining trial step acceptance.","Under common assumptions, we show that both the proximal parameter and merit function parameter eventually remain fixed, and then prove a worst-case complexity result for the maximum number of iterations before an iterate satisfying approximate first-order optimality conditions for a given tolerance is computed.","Our preliminary numerical results indicate that our approach has great promise, especially in terms of returning approximate solutions that are structured (e.g., sparse solutions when a one-norm regularizer is used)."],"url":"http://arxiv.org/abs/2404.07460v1","category":"math.OC"}
{"created":"2024-04-11 03:39:08","title":"Safe subspace screening for the adaptive nuclear norm regularized trace regression","abstract":"Matrix form data sets arise in many areas, so there are lots of works about the matrix regression models. One special model of these models is the adaptive nuclear norm regularized trace regression, which has been proven have good statistical performances. In order to accelerate the computation of this model, we consider the technique called screening rule. According to matrix decomposition and optimal condition of the model, we develop a safe subspace screening rule that can be used to identify inactive subspace of the solution decomposition and reduce the dimension of the solution. To evaluate the efficiency of the safe subspace screening rule, we embed this result into the alternating direction method of multipliers algorithm under a sequence of the tuning parameters. Under this process, each solution under the tuning parameter provides a matrix decomposition space. Then, the safe subspace screening rule is applied to eliminate inactive subspace, reduce the solution dimension and accelerate the computation process. Some numerical experiments are implemented on simulation data sets and real data sets, which illustrate the efficiency of our screening rule.","sentences":["Matrix form data sets arise in many areas, so there are lots of works about the matrix regression models.","One special model of these models is the adaptive nuclear norm regularized trace regression, which has been proven have good statistical performances.","In order to accelerate the computation of this model, we consider the technique called screening rule.","According to matrix decomposition and optimal condition of the model, we develop a safe subspace screening rule that can be used to identify inactive subspace of the solution decomposition and reduce the dimension of the solution.","To evaluate the efficiency of the safe subspace screening rule, we embed this result into the alternating direction method of multipliers algorithm under a sequence of the tuning parameters.","Under this process, each solution under the tuning parameter provides a matrix decomposition space.","Then, the safe subspace screening rule is applied to eliminate inactive subspace, reduce the solution dimension and accelerate the computation process.","Some numerical experiments are implemented on simulation data sets and real data sets, which illustrate the efficiency of our screening rule."],"url":"http://arxiv.org/abs/2404.07459v1","category":"stat.ME"}
{"created":"2024-04-11 03:23:15","title":"Representation Learning of Tangled Key-Value Sequence Data for Early Classification","abstract":"Key-value sequence data has become ubiquitous and naturally appears in a variety of real-world applications, ranging from the user-product purchasing sequences in e-commerce, to network packet sequences forwarded by routers in networking. Classifying these key-value sequences is important in many scenarios such as user profiling and malicious applications identification. In many time-sensitive scenarios, besides the requirement of classifying a key-value sequence accurately, it is also desired to classify a key-value sequence early, in order to respond fast. However, these two goals are conflicting in nature, and it is challenging to achieve them simultaneously. In this work, we formulate a novel tangled key-value sequence early classification problem, where a tangled key-value sequence is a mixture of several concurrent key-value sequences with different keys. The goal is to classify each individual key-value sequence sharing a same key both accurately and early. To address this problem, we propose a novel method, i.e., Key-Value sequence Early Co-classification (KVEC), which leverages both inner- and inter-correlations of items in a tangled key-value sequence through key correlation and value correlation to learn a better sequence representation. Meanwhile, a time-aware halting policy decides when to stop the ongoing key-value sequence and classify it based on current sequence representation. Experiments on both real-world and synthetic datasets demonstrate that our method outperforms the state-of-the-art baselines significantly. KVEC improves the prediction accuracy by up to $4.7 - 17.5\\%$ under the same prediction earliness condition, and improves the harmonic mean of accuracy and earliness by up to $3.7 - 14.0\\%$.","sentences":["Key-value sequence data has become ubiquitous and naturally appears in a variety of real-world applications, ranging from the user-product purchasing sequences in e-commerce, to network packet sequences forwarded by routers in networking.","Classifying these key-value sequences is important in many scenarios such as user profiling and malicious applications identification.","In many time-sensitive scenarios, besides the requirement of classifying a key-value sequence accurately, it is also desired to classify a key-value sequence early, in order to respond fast.","However, these two goals are conflicting in nature, and it is challenging to achieve them simultaneously.","In this work, we formulate a novel tangled key-value sequence early classification problem, where a tangled key-value sequence is a mixture of several concurrent key-value sequences with different keys.","The goal is to classify each individual key-value sequence sharing a same key both accurately and early.","To address this problem, we propose a novel method, i.e., Key-Value sequence Early Co-classification (KVEC), which leverages both inner- and inter-correlations of items in a tangled key-value sequence through key correlation and value correlation to learn a better sequence representation.","Meanwhile, a time-aware halting policy decides when to stop the ongoing key-value sequence and classify it based on current sequence representation.","Experiments on both real-world and synthetic datasets demonstrate that our method outperforms the state-of-the-art baselines significantly.","KVEC improves the prediction accuracy by up to $4.7 - 17.5\\%$ under the same prediction earliness condition, and improves the harmonic mean of accuracy and earliness by up to $3.7 - 14.0\\%$."],"url":"http://arxiv.org/abs/2404.07454v1","category":"cs.LG"}
{"created":"2024-04-11 03:19:22","title":"UAV-enabled Collaborative Beamforming via Multi-Agent Deep Reinforcement Learning","abstract":"In this paper, we investigate an unmanned aerial vehicle (UAV)-assistant air-to-ground communication system, where multiple UAVs form a UAV-enabled virtual antenna array (UVAA) to communicate with remote base stations by utilizing collaborative beamforming. To improve the work efficiency of the UVAA, we formulate a UAV-enabled collaborative beamforming multi-objective optimization problem (UCBMOP) to simultaneously maximize the transmission rate of the UVAA and minimize the energy consumption of all UAVs by optimizing the positions and excitation current weights of all UAVs. This problem is challenging because these two optimization objectives conflict with each other, and they are non-concave to the optimization variables. Moreover, the system is dynamic, and the cooperation among UAVs is complex, making traditional methods take much time to compute the optimization solution for a single task. In addition, as the task changes, the previously obtained solution will become obsolete and invalid. To handle these issues, we leverage the multi-agent deep reinforcement learning (MADRL) to address the UCBMOP. Specifically, we use the heterogeneous-agent trust region policy optimization (HATRPO) as the basic framework, and then propose an improved HATRPO algorithm, namely HATRPO-UCB, where three techniques are introduced to enhance the performance. Simulation results demonstrate that the proposed algorithm can learn a better strategy compared with other methods. Moreover, extensive experiments also demonstrate the effectiveness of the proposed techniques.","sentences":["In this paper, we investigate an unmanned aerial vehicle (UAV)-assistant air-to-ground communication system, where multiple UAVs form a UAV-enabled virtual antenna array (UVAA) to communicate with remote base stations by utilizing collaborative beamforming.","To improve the work efficiency of the UVAA, we formulate a UAV-enabled collaborative beamforming multi-objective optimization problem (UCBMOP) to simultaneously maximize the transmission rate of the UVAA and minimize the energy consumption of all UAVs by optimizing the positions and excitation current weights of all UAVs.","This problem is challenging because these two optimization objectives conflict with each other, and they are non-concave to the optimization variables.","Moreover, the system is dynamic, and the cooperation among UAVs is complex, making traditional methods take much time to compute the optimization solution for a single task.","In addition, as the task changes, the previously obtained solution will become obsolete and invalid.","To handle these issues, we leverage the multi-agent deep reinforcement learning (MADRL) to address the UCBMOP.","Specifically, we use the heterogeneous-agent trust region policy optimization (HATRPO) as the basic framework, and then propose an improved HATRPO algorithm, namely HATRPO-UCB, where three techniques are introduced to enhance the performance.","Simulation results demonstrate that the proposed algorithm can learn a better strategy compared with other methods.","Moreover, extensive experiments also demonstrate the effectiveness of the proposed techniques."],"url":"http://arxiv.org/abs/2404.07453v1","category":"cs.NI"}
{"created":"2024-04-11 03:13:02","title":"Collaborative Ground-Space Communications via Evolutionary Multi-objective Deep Reinforcement Learning","abstract":"In this paper, we propose a distributed collaborative beamforming (DCB)-based uplink communication paradigm for enabling ground-space direct communications. Specifically, DCB treats the terminals that are unable to establish efficient direct connections with the low Earth orbit (LEO) satellites as distributed antennas, forming a virtual antenna array to enhance the terminal-to-satellite uplink achievable rates and durations. However, such systems need multiple trade-off policies that variously balance the terminal-satellite uplink achievable rate, energy consumption of terminals, and satellite switching frequency to satisfy the scenario requirement changes. Thus, we perform a multi-objective optimization analysis and formulate a long-term optimization problem. To address availability in different terminal cluster scales, we reformulate this problem into an action space-reduced and universal multi-objective Markov decision process. Then, we propose an evolutionary multi-objective deep reinforcement learning algorithm to obtain the desirable policies, in which the low-value actions are masked to speed up the training process. As such, the applicability of a one-time trained model can cover more changing terminal-satellite uplink scenarios. Simulation results show that the proposed algorithm outmatches various baselines, and draw some useful insights. Specifically, it is found that DCB enables terminals that cannot reach the uplink achievable threshold to achieve efficient direct uplink transmission, which thus reveals that DCB is an effective solution for enabling direct ground-space communications. Moreover, it reveals that the proposed algorithm achieves multiple policies favoring different objectives and achieving near-optimal uplink achievable rates with low switching frequency.","sentences":["In this paper, we propose a distributed collaborative beamforming (DCB)-based uplink communication paradigm for enabling ground-space direct communications.","Specifically, DCB treats the terminals that are unable to establish efficient direct connections with the low Earth orbit (LEO) satellites as distributed antennas, forming a virtual antenna array to enhance the terminal-to-satellite uplink achievable rates and durations.","However, such systems need multiple trade-off policies that variously balance the terminal-satellite uplink achievable rate, energy consumption of terminals, and satellite switching frequency to satisfy the scenario requirement changes.","Thus, we perform a multi-objective optimization analysis and formulate a long-term optimization problem.","To address availability in different terminal cluster scales, we reformulate this problem into an action space-reduced and universal multi-objective Markov decision process.","Then, we propose an evolutionary multi-objective deep reinforcement learning algorithm to obtain the desirable policies, in which the low-value actions are masked to speed up the training process.","As such, the applicability of a one-time trained model can cover more changing terminal-satellite uplink scenarios.","Simulation results show that the proposed algorithm outmatches various baselines, and draw some useful insights.","Specifically, it is found that DCB enables terminals that cannot reach the uplink achievable threshold to achieve efficient direct uplink transmission, which thus reveals that DCB is an effective solution for enabling direct ground-space communications.","Moreover, it reveals that the proposed algorithm achieves multiple policies favoring different objectives and achieving near-optimal uplink achievable rates with low switching frequency."],"url":"http://arxiv.org/abs/2404.07450v1","category":"cs.NI"}
{"created":"2024-04-11 03:08:53","title":"Transferable and Principled Efficiency for Open-Vocabulary Segmentation","abstract":"Recent success of pre-trained foundation vision-language models makes Open-Vocabulary Segmentation (OVS) possible. Despite the promising performance, this approach introduces heavy computational overheads for two challenges: 1) large model sizes of the backbone; 2) expensive costs during the fine-tuning. These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios. Although traditional methods such as model compression and efficient fine-tuning can address these challenges, they often rely on heuristics. This means that their solutions cannot be easily transferred and necessitate re-training on different models, which comes at a cost. In the context of efficient OVS, we target achieving performance that is comparable to or even better than prior OVS works based on large vision-language foundation models, by utilizing smaller models that incur lower training costs. The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization. Comprehensive experiments on diverse OVS benchmarks demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works. Our code is available on https://github.com/Xujxyang/OpenTrans","sentences":["Recent success of pre-trained foundation vision-language models makes Open-Vocabulary Segmentation (OVS) possible.","Despite the promising performance, this approach introduces heavy computational overheads for two challenges: 1) large model sizes of the backbone; 2) expensive costs during the fine-tuning.","These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios.","Although traditional methods such as model compression and efficient fine-tuning can address these challenges, they often rely on heuristics.","This means that their solutions cannot be easily transferred and necessitate re-training on different models, which comes at a cost.","In the context of efficient OVS, we target achieving performance that is comparable to or even better than prior OVS works based on large vision-language foundation models, by utilizing smaller models that incur lower training costs.","The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization.","Comprehensive experiments on diverse OVS benchmarks demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works.","Our code is available on https://github.com/Xujxyang/OpenTrans"],"url":"http://arxiv.org/abs/2404.07448v1","category":"cs.CV"}
{"created":"2024-04-11 02:51:35","title":"Near Optimal Alphabet-Soundness Tradeoff PCPs","abstract":"We show that for all $\\varepsilon>0$, for sufficiently large prime power $q$, for all $\\delta>0$, it is NP-hard to distinguish whether a 2-Prover-1-Round projection game with alphabet size $q$ has value at least $1-\\delta$, or value at most $1/q^{(1-\\epsilon)}$. This establishes a nearly optimal alphabet-to-soundness tradeoff for 2-query PCPs with alphabet size $q$, improving upon a result of [Chan 2016]. Our result has the following implications:   1) Near optimal hardness for Quadratic Programming: it is NP-hard to approximate the value of a given Boolean Quadratic Program within factor $(\\log n)^{(1 - o(1))}$ under quasi-polynomial time reductions. This result improves a result of [Khot-Safra 2013] and nearly matches the performance of the best known approximation algorithm [Megrestki 2001, Nemirovski-Roos-Terlaky 1999 Charikar-Wirth 2004] that achieves a factor of $O(\\log n)$.   2) Bounded degree 2-CSP's: under randomized reductions, for sufficiently large $d>0$, it is NP-hard to approximate the value of 2-CSPs in which each variable appears in at most d constraints within factor $(1-o(1))d/2$ improving upon a recent result of [Lee-Manurangsi 2023].   3) Improved hardness results for connectivity problems: using results of [Laekhanukit 2014] and [Manurangsi 2019], we deduce improved hardness results for the Rooted $k$-Connectivity Problem, the Vertex-Connectivity Survivable Network Design Problem and the Vertex-Connectivity $k$-Route Cut Problem.","sentences":["We show that for all $\\varepsilon>0$, for sufficiently large prime power $q$, for all $\\delta>0$, it is NP-hard to distinguish whether a 2-Prover-1-Round projection game with alphabet size $q$ has value at least $1-\\delta$, or value at most $1/q^{(1-\\epsilon)}$. This establishes a nearly optimal alphabet-to-soundness tradeoff for 2-query PCPs with alphabet size $q$, improving upon a result of [Chan 2016].","Our result has the following implications:   1) Near optimal hardness for Quadratic Programming: it is NP-hard to approximate the value of a given Boolean Quadratic Program within factor $(\\log n)^{(1 - o(1))}$ under quasi-polynomial time reductions.","This result improves a result of [Khot-Safra 2013] and nearly matches the performance of the best known approximation algorithm","[Megrestki 2001, Nemirovski-Roos-Terlaky 1999 Charikar-Wirth 2004] that achieves a factor of $O(\\log n)$.   2) Bounded degree 2-CSP's: under randomized reductions, for sufficiently large $d>0$, it is NP-hard to approximate the value of 2-CSPs in which each variable appears in at most d constraints within factor $(1-o(1))d/2$ improving upon a recent result of [Lee-Manurangsi 2023].   ","3) Improved hardness results for connectivity problems: using results of [Laekhanukit 2014] and [Manurangsi 2019], we deduce improved hardness results for the Rooted $k$-Connectivity Problem, the Vertex-Connectivity Survivable Network Design Problem and the Vertex-Connectivity $k$-Route Cut Problem."],"url":"http://arxiv.org/abs/2404.07441v1","category":"cs.CC"}
{"created":"2024-04-11 02:50:37","title":"Bayesian Penalized Transformation Models: Structured Additive Location-Scale Regression for Arbitrary Conditional Distributions","abstract":"Penalized transformation models (PTMs) are a novel form of location-scale regression. In PTMs, the shape of the response's conditional distribution is estimated directly from the data, and structured additive predictors are placed on its location and scale. The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution. The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference distribution. These models can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape. Markov chain Monte Carlo inference for PTMs can be conducted with the No-U-Turn sampler and offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects. A simulation study demonstrates the effectiveness of the approach. We apply the model to data from the Fourth Dutch Growth Study and the Framingham Heart Study. A full-featured implementation is available as a Python library.","sentences":["Penalized transformation models (PTMs) are a novel form of location-scale regression.","In PTMs, the shape of the response's conditional distribution is estimated directly from the data, and structured additive predictors are placed on its location and scale.","The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution.","The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference distribution.","These models can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape.","Markov chain Monte Carlo inference for PTMs can be conducted with the No-U-Turn sampler and offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects.","A simulation study demonstrates the effectiveness of the approach.","We apply the model to data from the Fourth Dutch Growth Study and the Framingham Heart Study.","A full-featured implementation is available as a Python library."],"url":"http://arxiv.org/abs/2404.07440v1","category":"stat.ME"}
{"created":"2024-04-11 02:20:12","title":"Floquet engineering Rydberg sub-THz frequency comb spectroscopy","abstract":"Engineering a Terahertz (THz) frequency comb spectroscopy at atomic level advances the precisely measurement in spectroscopy and sensing. Current progresses on THz frequency comb rely on difference-frequency generation, optical parametric oscillation, and other methods. Generating a THz frequency comb poses challenges in source stability and achieving a narrow bandwidth, which traditional THz devices are difficult to achieve. Furthermore, accurately measuring the generated THz frequency comb necessitates a high-performance THz detector. Rydberg atoms are well-suited for electric field sensing due to their ultra-wide radio frequency transition energy levels, making them especially sensitive to external electric fields in the DC to THz bandwidth. However, there have been no reports about generating THz frequency comb spectroscopy at the atomic level until now. This work presents a THz frequency comb spectroscopy with Rydberg atoms, in which a Floquet comb-like transition is engineered through a time-periodic drive field. Our approach simplifies the setup required for THz frequency comb spectroscopy while extending the working bandwidth for Rydberg atomic sensors. The THz frequency comb spectroscopy at the atomic level reported in this article shows great potential for various applications in astronomy, remote sensing, spectral detection of biological samples, and other related fields.","sentences":["Engineering a Terahertz (THz) frequency comb spectroscopy at atomic level advances the precisely measurement in spectroscopy and sensing.","Current progresses on THz frequency comb rely on difference-frequency generation, optical parametric oscillation, and other methods.","Generating a THz frequency comb poses challenges in source stability and achieving a narrow bandwidth, which traditional THz devices are difficult to achieve.","Furthermore, accurately measuring the generated THz frequency comb necessitates a high-performance THz detector.","Rydberg atoms are well-suited for electric field sensing due to their ultra-wide radio frequency transition energy levels, making them especially sensitive to external electric fields in the DC to THz bandwidth.","However, there have been no reports about generating THz frequency comb spectroscopy at the atomic level until now.","This work presents a THz frequency comb spectroscopy with Rydberg atoms, in which a Floquet comb-like transition is engineered through a time-periodic drive field.","Our approach simplifies the setup required for THz frequency comb spectroscopy while extending the working bandwidth for Rydberg atomic sensors.","The THz frequency comb spectroscopy at the atomic level reported in this article shows great potential for various applications in astronomy, remote sensing, spectral detection of biological samples, and other related fields."],"url":"http://arxiv.org/abs/2404.07433v1","category":"physics.atom-ph"}
{"created":"2024-04-11 02:03:41","title":"Neutral-current background induced by atmospheric neutrinos at large liquid-scintillator detectors: III. Quantitative calculations for reactor neutrinos","abstract":"Atmospheric neutrinos contribute significantly to irreducible backgrounds through their neutral-current (NC) interactions with $^{12}$C nuclei in liquid-scintillator detectors, impacting diffuse supernova neutrino background, nucleon decay, and reactor neutrinos. This paper extends our prior work by systematically studying the NC backgrounds towards the MeV region of reactor neutrinos. We employ contemporary neutrino generator models from GENIE and NuWro for calculations, with a focus on predicting NC background in experimental searches for inverse-beta-decay signals below 100 MeV visible energy. We estimate the systematic uncertainty to our estimation of the NC background using various data-driven neutrino generator models, addressing factors such as the initial neutrino-nucleon NC interaction, the nuclear model, the final-state interaction model, the nucleus deexcitation, and the secondary interaction on final-state particles.","sentences":["Atmospheric neutrinos contribute significantly to irreducible backgrounds through their neutral-current (NC) interactions with $^{12}$C nuclei in liquid-scintillator detectors, impacting diffuse supernova neutrino background, nucleon decay, and reactor neutrinos.","This paper extends our prior work by systematically studying the NC backgrounds towards the MeV region of reactor neutrinos.","We employ contemporary neutrino generator models from GENIE and NuWro for calculations, with a focus on predicting NC background in experimental searches for inverse-beta-decay signals below 100 MeV visible energy.","We estimate the systematic uncertainty to our estimation of the NC background using various data-driven neutrino generator models, addressing factors such as the initial neutrino-nucleon NC interaction, the nuclear model, the final-state interaction model, the nucleus deexcitation, and the secondary interaction on final-state particles."],"url":"http://arxiv.org/abs/2404.07429v1","category":"hep-ph"}
{"created":"2024-04-11 01:58:38","title":"Diversity's Double-Edged Sword: Analyzing Race's Effect on Remote Pair Programming Interactions","abstract":"Remote pair programming is widely used in software development, but no research has examined how race affects these interactions. We embarked on this study due to the historical under representation of Black developers in the tech industry, with White developers comprising the majority. Our study involved 24 experienced developers, forming 12 gender-balanced same- and mixed-race pairs. Pairs collaborated on a programming task using the think-aloud method, followed by individual retrospective interviews. Our findings revealed elevated productivity scores for mixed-race pairs, with no differences in code quality between same- and mixed-race pairs. Mixed-race pairs excelled in task distribution, shared decision-making, and role-exchange but encountered communication challenges, discomfort, and anxiety, shedding light on the complexity of diversity dynamics. Our study emphasizes race's impact on remote pair programming and underscores the need for diverse tools and methods to address racial disparities for collaboration.","sentences":["Remote pair programming is widely used in software development, but no research has examined how race affects these interactions.","We embarked on this study due to the historical under representation of Black developers in the tech industry, with White developers comprising the majority.","Our study involved 24 experienced developers, forming 12 gender-balanced same- and mixed-race pairs.","Pairs collaborated on a programming task using the think-aloud method, followed by individual retrospective interviews.","Our findings revealed elevated productivity scores for mixed-race pairs, with no differences in code quality between same- and mixed-race pairs.","Mixed-race pairs excelled in task distribution, shared decision-making, and role-exchange but encountered communication challenges, discomfort, and anxiety, shedding light on the complexity of diversity dynamics.","Our study emphasizes race's impact on remote pair programming and underscores the need for diverse tools and methods to address racial disparities for collaboration."],"url":"http://arxiv.org/abs/2404.07427v1","category":"cs.SE"}
{"created":"2024-04-11 01:33:45","title":"CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models","abstract":"Computer-aided diagnosis systems hold great promise to aid radiologists and clinicians in radiological clinical practice and enhance diagnostic accuracy and efficiency. However, the conventional systems primarily focus on delivering diagnostic results through text report generation or medical image classification, positioning them as standalone decision-makers rather than helpers and ignoring radiologists' expertise. This study introduces an innovative paradigm to create an assistive co-pilot system for empowering radiologists by leveraging Large Language Models (LLMs) and medical image analysis tools. Specifically, we develop a collaborative framework to integrate LLMs and quantitative medical image analysis results generated by foundation models with radiologists in the loop, achieving efficient and safe generation of radiology reports and effective utilization of computational power of AI and the expertise of medical professionals. This approach empowers radiologists to generate more precise and detailed diagnostic reports, enhancing patient outcomes while reducing the burnout of clinicians. Our methodology underscores the potential of AI as a supportive tool in medical diagnostics, promoting a harmonious integration of technology and human expertise to advance the field of radiology.","sentences":["Computer-aided diagnosis systems hold great promise to aid radiologists and clinicians in radiological clinical practice and enhance diagnostic accuracy and efficiency.","However, the conventional systems primarily focus on delivering diagnostic results through text report generation or medical image classification, positioning them as standalone decision-makers rather than helpers and ignoring radiologists' expertise.","This study introduces an innovative paradigm to create an assistive co-pilot system for empowering radiologists by leveraging Large Language Models (LLMs) and medical image analysis tools.","Specifically, we develop a collaborative framework to integrate LLMs and quantitative medical image analysis results generated by foundation models with radiologists in the loop, achieving efficient and safe generation of radiology reports and effective utilization of computational power of AI and the expertise of medical professionals.","This approach empowers radiologists to generate more precise and detailed diagnostic reports, enhancing patient outcomes while reducing the burnout of clinicians.","Our methodology underscores the potential of AI as a supportive tool in medical diagnostics, promoting a harmonious integration of technology and human expertise to advance the field of radiology."],"url":"http://arxiv.org/abs/2404.07424v1","category":"cs.CV"}
{"created":"2024-04-11 01:14:15","title":"Waveguide Tailored Radiation Pattern of Nanoparticles for Tunable Multimodal Guided Surface Lattice Resonances in Asymmetric Environment","abstract":"Surface lattice resonances (SLR) in metasurfaces is promising in applications of sub-wavelength devices, with high quality factors (high-Q), large local field enhancement and long-range interaction properties. Tunable peak position and multimodal resonances make SLR further appealing to flexible and multiple light-matter interactions. However, reported multimodal SLRs lack flexible and economic tunability. Moreover, current high-Q SLR requires a homogeneous index of operational environment, which hampered potential applications such as biosensors that are usually operated in aqueous or air environment. Here we report a guided-SLR (gSLR) that is easily accessible in index-discontinuous environment along with multimodal property and active tunability of resonating wavelength, mode number and mode coupling strength. The gSLR is realized via coupling scattered light from metasurfaces units into a slab waveguide, which opens up a light propagating channel in the lattice plane in index-asymmetric environment. Tailoring radiation pattern of each units with guided transverse electric (TE) and transverse magnetic (TM) modes, multimodal resonances of both orthogonal and parallel coupling directions are accomplished. Mode number and mode frequency positions can be easily adjusted by waveguide configuration, while mode strength is tuned by vertical position in the slab. The easy-to-access, actively tunable and multimodal gSLR in inhomogeneous medium will promote the realization of ultrathin and ultracompact nano-optical and optoelectronic devices.","sentences":["Surface lattice resonances (SLR) in metasurfaces is promising in applications of sub-wavelength devices, with high quality factors (high-Q), large local field enhancement and long-range interaction properties.","Tunable peak position and multimodal resonances make SLR further appealing to flexible and multiple light-matter interactions.","However, reported multimodal SLRs lack flexible and economic tunability.","Moreover, current high-Q SLR requires a homogeneous index of operational environment, which hampered potential applications such as biosensors that are usually operated in aqueous or air environment.","Here we report a guided-SLR (gSLR) that is easily accessible in index-discontinuous environment along with multimodal property and active tunability of resonating wavelength, mode number and mode coupling strength.","The gSLR is realized via coupling scattered light from metasurfaces units into a slab waveguide, which opens up a light propagating channel in the lattice plane in index-asymmetric environment.","Tailoring radiation pattern of each units with guided transverse electric (TE) and transverse magnetic (TM) modes, multimodal resonances of both orthogonal and parallel coupling directions are accomplished.","Mode number and mode frequency positions can be easily adjusted by waveguide configuration, while mode strength is tuned by vertical position in the slab.","The easy-to-access, actively tunable and multimodal gSLR in inhomogeneous medium will promote the realization of ultrathin and ultracompact nano-optical and optoelectronic devices."],"url":"http://arxiv.org/abs/2404.07417v1","category":"physics.optics"}
{"created":"2024-04-11 00:51:52","title":"Brock-type isoperimetric inequality for Steklov eigenvalues of the Witten-Laplacian","abstract":"In this paper, by imposing suitable assumptions on the weighted function, (under the constraint of fixed weighted volume) a Brock-type isoperimetric inequality for Steklov-type eigenvalues of the Witten-Laplacian on bounded domains in a Euclidean space or a hyperbolic space has been proven. This conclusion is actually an interesting extension of F. Brock's classical result about the isoperimetric inequality for Steklov eigenvalues of the Laplacian given in the influential paper [Z. Angew. Math. Mech. 81 (2001) 69-71]. Besides, a related open problem has also been proposed in this paper.","sentences":["In this paper, by imposing suitable assumptions on the weighted function, (under the constraint of fixed weighted volume) a Brock-type isoperimetric inequality for Steklov-type eigenvalues of the Witten-Laplacian on bounded domains in a Euclidean space or a hyperbolic space has been proven.","This conclusion is actually an interesting extension of F. Brock's classical result about the isoperimetric inequality for Steklov eigenvalues of the Laplacian given in the influential paper","[Z. Angew.","Math.","Mech. 81 (2001) 69-71].","Besides, a related open problem has also been proposed in this paper."],"url":"http://arxiv.org/abs/2404.07412v1","category":"math.AP"}
{"created":"2024-04-11 00:49:08","title":"Too good to be true: People reject free gifts from robots because they infer bad intentions","abstract":"A recent psychology study found that people sometimes reject overly generous offers from people because they imagine hidden ''phantom costs'' must be part of the transaction. Phantom costs occur when a person seems overly generous for no apparent reason. This study aims to explore whether people can imagine phantom costs when interacting with a robot. To this end, screen or physically embodied agents (human or robot) offered to people either a cookie or a cookie + \\$2. Participants were then asked to make a choice whether they would accept or decline the offer. Results showed that people did perceive phantom costs in the offer + \\$2 conditions when interacting with a human, but also with a robot, across both embodiment levels, leading to the characteristic behavioral effect that offering more money made people less likely to accept the offer. While people were more likely to accept offers from a robot than from a human, people more often accepted offers from humans when they were physically compared to screen embodied but were equally likely to accept the offer from a robot whether it was screen or physically embodied. This suggests that people can treat robots (and humans) as social agents with hidden intentions and knowledge, and that this influences their behavior toward them. This provides not only new insights on how people make decisions when interacting with a robot but also how robot embodiment impacts HRI research.","sentences":["A recent psychology study found that people sometimes reject overly generous offers from people because they imagine hidden ''phantom costs'' must be part of the transaction.","Phantom costs occur when a person seems overly generous for no apparent reason.","This study aims to explore whether people can imagine phantom costs when interacting with a robot.","To this end, screen or physically embodied agents (human or robot) offered to people either a cookie or a cookie + \\$2.","Participants were then asked to make a choice whether they would accept or decline the offer.","Results showed that people did perceive phantom costs in the offer + \\$2 conditions when interacting with a human, but also with a robot, across both embodiment levels, leading to the characteristic behavioral effect that offering more money made people less likely to accept the offer.","While people were more likely to accept offers from a robot than from a human, people more often accepted offers from humans when they were physically compared to screen embodied but were equally likely to accept the offer from a robot whether it was screen or physically embodied.","This suggests that people can treat robots (and humans) as social agents with hidden intentions and knowledge, and that this influences their behavior toward them.","This provides not only new insights on how people make decisions when interacting with a robot but also how robot embodiment impacts HRI research."],"url":"http://arxiv.org/abs/2404.07409v1","category":"cs.HC"}
{"created":"2024-04-11 00:47:14","title":"Unveiling Behavioral Transparency of Protocols Communicated by IoT Networked Assets (Full Version)","abstract":"Behavioral transparency for Internet-of-Things (IoT) networked assets involves two distinct yet interconnected tasks: (a) characterizing device types by discerning the patterns exhibited in their network traffic, and (b) assessing vulnerabilities they introduce to the network. While identifying communication protocols, particularly at the application layer, plays a vital role in effective network management, current methods are, at best, ad-hoc. Accurate protocol identification and attribute extraction from packet payloads are crucial for distinguishing devices and discovering vulnerabilities. This paper makes three contributions: (1) We process a public dataset to construct specific packet traces pertinent to six standard protocols (TLS, HTTP, DNS, NTP, DHCP, and SSDP) of ten commercial IoT devices. We manually analyze TLS and HTTP flows, highlighting their characteristics, parameters, and adherence to best practices-we make our data publicly available; (2) We develop a common model to describe protocol signatures that help with the systematic analysis of protocols even when communicated through non-standard port numbers; and, (3) We evaluate the efficacy of our data models for the six protocols, which constitute approximately 97% of our dataset. Our data models, except for SSDP in 0.3% of Amazon Echo's flows, produce no false positives for protocol detection. We draw insights into how various IoT devices behave across those protocols by applying these models to our IoT traces.","sentences":["Behavioral transparency for Internet-of-Things (IoT) networked assets involves two distinct yet interconnected tasks: (a) characterizing device types by discerning the patterns exhibited in their network traffic, and (b) assessing vulnerabilities they introduce to the network.","While identifying communication protocols, particularly at the application layer, plays a vital role in effective network management, current methods are, at best, ad-hoc.","Accurate protocol identification and attribute extraction from packet payloads are crucial for distinguishing devices and discovering vulnerabilities.","This paper makes three contributions: (1) We process a public dataset to construct specific packet traces pertinent to six standard protocols (TLS, HTTP, DNS, NTP, DHCP, and SSDP) of ten commercial IoT devices.","We manually analyze TLS and HTTP flows, highlighting their characteristics, parameters, and adherence to best practices-we make our data publicly available; (2) We develop a common model to describe protocol signatures that help with the systematic analysis of protocols even when communicated through non-standard port numbers; and, (3) We evaluate the efficacy of our data models for the six protocols, which constitute approximately 97% of our dataset.","Our data models, except for SSDP in 0.3% of Amazon Echo's flows, produce no false positives for protocol detection.","We draw insights into how various IoT devices behave across those protocols by applying these models to our IoT traces."],"url":"http://arxiv.org/abs/2404.07408v1","category":"cs.NI"}
{"created":"2024-04-10 23:57:45","title":"High-power even- and odd mode emission from linear arrays of resonant-tunneling-diode (RTD) oscillators in the 0.4- to 0.8-THz frequency range","abstract":"Resonant tunneling diode (RTD) oscillators possess the highest oscillation frequency among all electronic THz emitters. However, the emitted power from RTDs remains limited. Here, we propose linear RTD-oscillator arrays capable of supporting coherent emission from both odd and even coupled modes. Both modes exhibit constructive interference in the far field, enabling high power emission. Experimental demonstrations of coherent emission from 11-RTD-oscillator linear arrays are presented. The odd mode oscillates at approximately 450 GHz, emitting about 0.5 mW, while the even mode oscillates at around 750 GHz, emitting about 1 mW. Moreover, certain RTD-oscillator arrays demonstrate dual-band oscillation under different biases, allowing for controllable switching between two coupled modes. In addition, during bias sweeping in both directions, a notable hysteresis feature is observed in the switching bias for the odd and even modes. Our linear RTD-oscillator array represents a significant step forward in the realization of high-power large RTD-oscillator arrays and enables large-scale applications of RTD devices.","sentences":["Resonant tunneling diode (RTD) oscillators possess the highest oscillation frequency among all electronic THz emitters.","However, the emitted power from RTDs remains limited.","Here, we propose linear RTD-oscillator arrays capable of supporting coherent emission from both odd and even coupled modes.","Both modes exhibit constructive interference in the far field, enabling high power emission.","Experimental demonstrations of coherent emission from 11-RTD-oscillator linear arrays are presented.","The odd mode oscillates at approximately 450 GHz, emitting about 0.5 mW, while the even mode oscillates at around 750 GHz, emitting about 1 mW. Moreover, certain RTD-oscillator arrays demonstrate dual-band oscillation under different biases, allowing for controllable switching between two coupled modes.","In addition, during bias sweeping in both directions, a notable hysteresis feature is observed in the switching bias for the odd and even modes.","Our linear RTD-oscillator array represents a significant step forward in the realization of high-power large RTD-oscillator arrays and enables large-scale applications of RTD devices."],"url":"http://arxiv.org/abs/2404.07394v1","category":"physics.optics"}
{"created":"2024-04-10 23:49:38","title":"Kinematic age of the $\u03b2$-Pictoris moving group","abstract":"Accurate age estimation of nearby young moving groups (NYMGs) is important as they serve as crucial testbeds in various fields of astrophysics, including formation and evolution of stars, planets, as well as loose stellar associations. The $\\beta$-Pictoris moving group (BPMG), being one of the closest and youngest NYMGs, has been extensively investigated, and its estimated ages have a wide range from $\\sim$10 to 25 Myr, depending on the age estimation methods and data used. Unlike other age dating methods, kinematic traceback analysis offers a model-independent age assessment hence the merit in comparing many seemingly discordant age estimates. In this study, we determine the kinematic ages of the BPMG using three methods: probabilistic volume calculation, mean pairwise distance calculation, and covariance matrix analysis. These methods yield consistent results, with estimated ages in the range of 14 to 20 Myr. Implementing corrections to radial velocities due to gravitational redshift and convectional blueshift increases the ages by $\\sim2-4$ Myr. Conversely, considering data uncertainties decreases the estimated ages by 1 to 2 Myr. Taken together, our analysis determined the kinematic age of BPMG to be 16.3$^{+3.4}_{-2.1}$ Myr. This age is significantly younger than the commonly accepted age of the BPMG ($\\sim$24 Myr) determined primarily from the lithium depletion boundary analysis. This younger kinematic age may point to the discrepancy between the luminosity evolution and lithium depletion models or the presence of unaccounted systematic error in the method. This result underscores the necessity for systematic reevaluations of age-dating methods for nearby, young moving groups.","sentences":["Accurate age estimation of nearby young moving groups (NYMGs) is important as they serve as crucial testbeds in various fields of astrophysics, including formation and evolution of stars, planets, as well as loose stellar associations.","The $\\beta$-Pictoris moving group (BPMG), being one of the closest and youngest NYMGs, has been extensively investigated, and its estimated ages have a wide range from $\\sim$10 to 25 Myr, depending on the age estimation methods and data used.","Unlike other age dating methods, kinematic traceback analysis offers a model-independent age assessment hence the merit in comparing many seemingly discordant age estimates.","In this study, we determine the kinematic ages of the BPMG using three methods: probabilistic volume calculation, mean pairwise distance calculation, and covariance matrix analysis.","These methods yield consistent results, with estimated ages in the range of 14 to 20 Myr.","Implementing corrections to radial velocities due to gravitational redshift and convectional blueshift increases the ages by $\\sim2-4$ Myr.","Conversely, considering data uncertainties decreases the estimated ages by 1 to 2 Myr.","Taken together, our analysis determined the kinematic age of BPMG to be 16.3$^{+3.4}_{-2.1}$ Myr.","This age is significantly younger than the commonly accepted age of the BPMG ($\\sim$24 Myr) determined primarily from the lithium depletion boundary analysis.","This younger kinematic age may point to the discrepancy between the luminosity evolution and lithium depletion models or the presence of unaccounted systematic error in the method.","This result underscores the necessity for systematic reevaluations of age-dating methods for nearby, young moving groups."],"url":"http://arxiv.org/abs/2404.07391v1","category":"astro-ph.SR"}
{"created":"2024-04-10 23:32:47","title":"Nonlinear dynamics of confined cell migration -- modeling and inference","abstract":"The motility of eukaryotic cells is strongly influenced by their environment, with confined cells often developing qualitatively different motility patterns from those migrating on simple two-dimensional substrates. Recent experiments, coupled with data-driven methods to extract a cell's equation of motion, showed that cancerous MDA-MB-231 cells persistently hop in a limit cycle when placed on two-state adhesive micropatterns (two large squares connected by a narrow bridge), while they remain stationary on average in rectangular confinements. In contrast, healthy MCF10A cells migrating on the two-state micropattern are bistable, i.e., they settle into either basin on average with only noise-induced hops between the two states. We can capture all these behaviors with a single computational phase field model of a crawling cell, under the assumption that contact with non-adhesive substrate inhibits the cell front. Our model predicts that larger and softer cells are more likely to persistently hop, while smaller and stiffer cells are more likely to be bistable. Other key factors controlling cell migration are the frequency of protrusions and their magnitude of noise. Our results show that relatively simple assumptions about how cells sense their geometry can explain a wide variety of different cell behaviors, and show the power of data-driven approaches to characterize both experiment and simulation.","sentences":["The motility of eukaryotic cells is strongly influenced by their environment, with confined cells often developing qualitatively different motility patterns from those migrating on simple two-dimensional substrates.","Recent experiments, coupled with data-driven methods to extract a cell's equation of motion, showed that cancerous MDA-MB-231 cells persistently hop in a limit cycle when placed on two-state adhesive micropatterns (two large squares connected by a narrow bridge), while they remain stationary on average in rectangular confinements.","In contrast, healthy MCF10A cells migrating on the two-state micropattern are bistable, i.e., they settle into either basin on average with only noise-induced hops between the two states.","We can capture all these behaviors with a single computational phase field model of a crawling cell, under the assumption that contact with non-adhesive substrate inhibits the cell front.","Our model predicts that larger and softer cells are more likely to persistently hop, while smaller and stiffer cells are more likely to be bistable.","Other key factors controlling cell migration are the frequency of protrusions and their magnitude of noise.","Our results show that relatively simple assumptions about how cells sense their geometry can explain a wide variety of different cell behaviors, and show the power of data-driven approaches to characterize both experiment and simulation."],"url":"http://arxiv.org/abs/2404.07390v1","category":"q-bio.CB"}
{"created":"2024-04-10 23:05:10","title":"Lyapunov-Based Deep Residual Neural Network (ResNet) Adaptive Control","abstract":"Deep Neural Network (DNN)-based controllers have emerged as a tool to compensate for unstructured uncertainties in nonlinear dynamical systems. A recent breakthrough in the adaptive control literature provides a Lyapunov-based approach to derive weight adaptation laws for each layer of a fully-connected feedforward DNN-based adaptive controller. However, deriving weight adaptation laws from a Lyapunov-based analysis remains an open problem for deep residual neural networks (ResNets). This paper provides the first result on Lyapunov-derived weight adaptation for a ResNet-based adaptive controller. A nonsmooth Lyapunov-based analysis is provided to guarantee asymptotic tracking error convergence. Comparative Monte Carlo simulations are provided to demonstrate the performance of the developed ResNet-based adaptive controller. The ResNet-based adaptive controller shows a 64% improvement in the tracking and function approximation performance, in comparison to a fully-connected DNN-based adaptive controller.","sentences":["Deep Neural Network (DNN)-based controllers have emerged as a tool to compensate for unstructured uncertainties in nonlinear dynamical systems.","A recent breakthrough in the adaptive control literature provides a Lyapunov-based approach to derive weight adaptation laws for each layer of a fully-connected feedforward DNN-based adaptive controller.","However, deriving weight adaptation laws from a Lyapunov-based analysis remains an open problem for deep residual neural networks (ResNets).","This paper provides the first result on Lyapunov-derived weight adaptation for a ResNet-based adaptive controller.","A nonsmooth Lyapunov-based analysis is provided to guarantee asymptotic tracking error convergence.","Comparative Monte Carlo simulations are provided to demonstrate the performance of the developed ResNet-based adaptive controller.","The ResNet-based adaptive controller shows a 64% improvement in the tracking and function approximation performance, in comparison to a fully-connected DNN-based adaptive controller."],"url":"http://arxiv.org/abs/2404.07385v1","category":"eess.SY"}
{"created":"2024-04-10 22:20:47","title":"Subcritical Fourier uncertainty principles","abstract":"It is well known that if a function $f$ satisfies $$ \\norm{f(x) e^{\\pi \\alpha |x|^2}}_p + \\norm{\\widehat{f}(\\xi) e^{\\pi \\alpha |\\xi|^2}}_q<\\infty \\eqno{(*)}$$ with $\\alpha=1$ and $1\\le p,q<\\infty$, then $f\\equiv 0.$   We prove that if $f$ satisfies $(*)$ with some $0<\\alpha<1$ and $1\\le p,q\\leq \\infty$, then $$ |f(y)|\\le C   (1+|y|)^{\\frac{d}{p}}   e^{- \\pi \\alpha |y|^2}, \\quad y\\in \\mathbb{R}^d, $$ with $ C=C(\\alpha,d,p,q)$ and this bound is sharp for $p\\neq 1$. We also study a related uncertainty principle for functions satisfying $\\;\\;\\displaystyle\\norm{f(x)|x|^m}_p+ \\norm{\\widehat{f}(\\xi)|\\xi|^n}_q <\\infty.$","sentences":["It is well known that if a function $f$ satisfies $$ \\norm{f(x) e^{\\pi \\alpha |x|^2}}_p + \\norm{\\widehat{f}(\\xi) e^{\\pi \\alpha |\\xi|^2}}_q<\\infty \\eqno{(*)}$$ with $\\alpha=1$ and $1\\le p,q<\\infty$, then $f\\equiv 0.$   We prove that if $f$ satisfies $(*)$ with some $0<\\alpha<1$ and $1\\le p,q\\leq \\infty$, then $$ |f(y)|\\le C   (1+|y|)^{\\frac{d}{p}}   e^{- \\pi \\alpha |y|^2}, \\quad y\\in \\mathbb{R}^d, $$ with $ C=C(\\alpha,d,p,q)$ and this bound is sharp for $p\\neq 1$.","We also study a related uncertainty principle for functions satisfying $\\;\\;\\displaystyle\\norm{f(x)|x|^m}_p+ \\norm{\\widehat{f}(\\xi)|\\xi|^n}_q <\\infty.$"],"url":"http://arxiv.org/abs/2404.07375v1","category":"math.CA"}
{"created":"2024-04-10 22:15:28","title":"Synthesizing Neural Network Controllers with Closed-Loop Dissipativity Guarantees","abstract":"In this paper, a method is presented to synthesize neural network controllers such that the feedback system of plant and controller is dissipative, certifying performance requirements such as L2 gain bounds. The class of plants considered is that of linear time-invariant (LTI) systems interconnected with an uncertainty, including nonlinearities treated as an uncertainty for convenience of analysis. The uncertainty of the plant and the nonlinearities of the neural network are both described using integral quadratic constraints (IQCs). First, a dissipativity condition is derived for uncertain LTI systems. Second, this condition is used to construct a linear matrix inequality (LMI) which can be used to synthesize neural network controllers. Finally, this convex condition is used in a projection-based training method to synthesize neural network controllers with dissipativity guarantees. Numerical examples on an inverted pendulum and a flexible rod on a cart are provided to demonstrate the effectiveness of this approach.","sentences":["In this paper, a method is presented to synthesize neural network controllers such that the feedback system of plant and controller is dissipative, certifying performance requirements such as L2 gain bounds.","The class of plants considered is that of linear time-invariant (LTI) systems interconnected with an uncertainty, including nonlinearities treated as an uncertainty for convenience of analysis.","The uncertainty of the plant and the nonlinearities of the neural network are both described using integral quadratic constraints (IQCs).","First, a dissipativity condition is derived for uncertain LTI systems.","Second, this condition is used to construct a linear matrix inequality (LMI) which can be used to synthesize neural network controllers.","Finally, this convex condition is used in a projection-based training method to synthesize neural network controllers with dissipativity guarantees.","Numerical examples on an inverted pendulum and a flexible rod on a cart are provided to demonstrate the effectiveness of this approach."],"url":"http://arxiv.org/abs/2404.07373v1","category":"eess.SY"}
{"created":"2024-04-10 21:38:14","title":"ALMA-IMF XV: N$_2$H$^+$ kinematic analysis on the intermediate protocluster G353.41","abstract":"The ALMA-IMF Large Program provides multi-tracer observations of 15 Galactic massive protoclusters at matched sensitivity and spatial resolution. We focus on the dense gas kinematics of the G353.41 protocluster traced by N$_2$H$^+$ (1$-$0), with a critical density of $2\\times10^5$~cm$^{-3}$, and spatial resolution $\\sim$0.02~pc. G353.41, at a distance of 2~kpc, is embedded in a larger scale ($\\sim$8~pc) filament and has a mass of 2500~M$_{\\odot}$ within $1.3\\times1.3$~pc$^2$. We extract the N$_2$H$^+$ isolated line component and we decompose it by fitting up to 3 Gaussian velocity components. This allows us to identify velocity structures that are either muddled or impossible to identify in the traditional position-velocity diagram. We identify multiple velocity gradients (VGs) on large and small scales. We find good agreement between the N$_2$H$^+$ and the previously reported DCN core velocities, suggesting that cores are kinematically coupled to the dense gas in which they form. We measure 9 converging V-shaped VGs, located in filaments, that are sometimes associated with cores near their point of convergence. The average timescale associated with the V-shapes are $\\sim$67~kyr, or about twice the free-fall time of cores in the same area ($\\sim$~33~kyr) but substantially shorter than protostar lifetime estimates ($\\sim$~0.5~Myr). We interpret these V-shapes as inflowing gas feeding the regions near cores and we derive their mass accretion rates. This feeding might lead to further filament collapse and formation of new cores. We suggest that the protocluster is collapsing on large scales, but the velocity signature of collapse is slow compared to pure free-fall. Thus these data are consistent with a comparatively slow global protocluster contraction under gravity, and faster core formation within, suggesting the formation of multiple generations of stars over the protocluster lifetime.","sentences":["The ALMA-IMF Large Program provides multi-tracer observations of 15 Galactic massive protoclusters at matched sensitivity and spatial resolution.","We focus on the dense gas kinematics of the G353.41 protocluster traced by N$_2$H$^+$ (1$-$0), with a critical density of $2\\times10^5$~cm$^{-3}$, and spatial resolution $\\sim$0.02~pc.","G353.41, at a distance of 2~kpc, is embedded in a larger scale ($\\sim$8~pc) filament and has a mass of 2500~M$_{\\odot}$ within $1.3\\times1.3$~pc$^2$. We extract the N$_2$H$^+$ isolated line component and we decompose it by fitting up to 3 Gaussian velocity components.","This allows us to identify velocity structures that are either muddled or impossible to identify in the traditional position-velocity diagram.","We identify multiple velocity gradients (VGs) on large and small scales.","We find good agreement between the N$_2$H$^+$ and the previously reported DCN core velocities, suggesting that cores are kinematically coupled to the dense gas in which they form.","We measure 9 converging V-shaped VGs, located in filaments, that are sometimes associated with cores near their point of convergence.","The average timescale associated with the V-shapes are $\\sim$67~kyr, or about twice the free-fall time of cores in the same area ($\\sim$~33~kyr) but substantially shorter than protostar lifetime estimates ($\\sim$~0.5~Myr).","We interpret these V-shapes as inflowing gas feeding the regions near cores and we derive their mass accretion rates.","This feeding might lead to further filament collapse and formation of new cores.","We suggest that the protocluster is collapsing on large scales, but the velocity signature of collapse is slow compared to pure free-fall.","Thus these data are consistent with a comparatively slow global protocluster contraction under gravity, and faster core formation within, suggesting the formation of multiple generations of stars over the protocluster lifetime."],"url":"http://arxiv.org/abs/2404.07363v1","category":"astro-ph.GA"}
{"created":"2024-04-10 21:35:17","title":"Enhancing Accessibility in Soft Robotics: Exploring Magnet-Embedded Paper-Based Interactions","abstract":"This paper explores the implementation of embedded magnets to enhance paper-based interactions. The integration of magnets in paper-based interactions simplifies the fabrication process, making it more accessible for building soft robotics systems. We discuss various interaction patterns achievable through this approach and highlight their potential applications.","sentences":["This paper explores the implementation of embedded magnets to enhance paper-based interactions.","The integration of magnets in paper-based interactions simplifies the fabrication process, making it more accessible for building soft robotics systems.","We discuss various interaction patterns achievable through this approach and highlight their potential applications."],"url":"http://arxiv.org/abs/2404.07360v1","category":"cs.HC"}
{"created":"2024-04-10 21:26:24","title":"On the nature of inner light-rings","abstract":"Non-singular horizonless ultracompact objects provide a simple resolution to the black holes singularity problem. It has been shown that, if these objects are compact enough to exhibit the presence of the light-ring required to mimic the phenomenology of general relativity black holes, they must have at least one additional light-ring. The stability of the inner light ring has been proven under the assumption of Einstein equations and the validity of the null energy condition. Since this can have important repercussions on the instability of a horizonless ultracompact object and the existence of the latter requires some modified gravitational dynamics and/or exotic matter, it is desirable to obtain a model-independent proof of the stability of the additional light-ring. In this paper, we prove the stability of the inner light-ring without any assumption on the dynamics of the theory, while assuming that the outer light-ring has the same properties as the Kerr light-ring. Given the stringent observational constraints on the geometry at the outer light-ring scale, our result now rests solely on geometric considerations and applies to any theory of gravity with any matter content that cannot be ruled out by observations.","sentences":["Non-singular horizonless ultracompact objects provide a simple resolution to the black holes singularity problem.","It has been shown that, if these objects are compact enough to exhibit the presence of the light-ring required to mimic the phenomenology of general relativity black holes, they must have at least one additional light-ring.","The stability of the inner light ring has been proven under the assumption of Einstein equations and the validity of the null energy condition.","Since this can have important repercussions on the instability of a horizonless ultracompact object and the existence of the latter requires some modified gravitational dynamics and/or exotic matter, it is desirable to obtain a model-independent proof of the stability of the additional light-ring.","In this paper, we prove the stability of the inner light-ring without any assumption on the dynamics of the theory, while assuming that the outer light-ring has the same properties as the Kerr light-ring.","Given the stringent observational constraints on the geometry at the outer light-ring scale, our result now rests solely on geometric considerations and applies to any theory of gravity with any matter content that cannot be ruled out by observations."],"url":"http://arxiv.org/abs/2404.07357v1","category":"gr-qc"}
{"created":"2024-04-10 21:08:06","title":"Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites","abstract":"In this paper we introduce two world-first Mixed Reality (MR) experiences that fuse smart AR glasses and live theatre and take place in a heritage site with the purpose to reveal the site's hidden and difficult histories about slavery. We term these unique general audience experiences Mixed Reality Heritage Performances (MRHP). Along with the development of our initial two performances we designed and developed a tool and guidelines that can help heritage organisations with their decolonising process by critically engaging the public with under-represented voices and viewpoints of troubled European and colonial narratives. The evaluations showed the embodied and affective potential of MRHP to attract and educate heritage audiences visitors. Insights of the design process are being formulated into an extensive design toolkit that aims to support experience design, theatre and heritage professionals to collaboratively carry out similar projects.","sentences":["In this paper we introduce two world-first Mixed Reality (MR) experiences that fuse smart AR glasses and live theatre and take place in a heritage site with the purpose to reveal the site's hidden and difficult histories about slavery.","We term these unique general audience experiences Mixed Reality Heritage Performances (MRHP).","Along with the development of our initial two performances we designed and developed a tool and guidelines that can help heritage organisations with their decolonising process by critically engaging the public with under-represented voices and viewpoints of troubled European and colonial narratives.","The evaluations showed the embodied and affective potential of MRHP to attract and educate heritage audiences visitors.","Insights of the design process are being formulated into an extensive design toolkit that aims to support experience design, theatre and heritage professionals to collaboratively carry out similar projects."],"url":"http://arxiv.org/abs/2404.07348v1","category":"cs.HC"}
{"created":"2024-04-10 21:03:10","title":"Phase-Field Modeling of Fracture for Ferromagnetic Materials through Maxwell's Equation","abstract":"Electro-active materials are classified as electrostrictive and piezoelectric materials. They deform under the action of an external electric field. Piezoelectric material, as a special class of active materials, can produce an internal electric field when subjected to mechanical stress or strain. In return, there is the converse piezoelectric response, which expresses the induction of the mechanical deformation in the material when it is subjected to the application of the electric field. This work presents a variational-based computational modeling approach for failure prediction of ferromagnetic materials. In order to solve this problem, a coupling between magnetostriction and mechanics is modeled, then the fracture mechanism in ferromagnetic materials is investigated. Furthermore, the failure mechanics of ferromagnetic materials under the magnetostrictive effects is studied based on a variational phase-field model of fracture. Phase-field fracture is numerically challenging since the energy functional may admit several local minima, imposing the global irreversibility of the fracture field and dependency of regularization parameters related discretization size. Here, the failure behavior of a magnetoelastic solid body is formulated based on the Helmholtz free energy function, in which the strain tensor, the magnetic induction vector, and the crack phase-field are introduced as state variables. This coupled formulation leads to a continuity equation for the magnetic vector potential through well-known Maxwell's equations. Hence, the energetic crack driving force is governed by the coupled magneto-mechanical effects under the magneto-static state. Several numerical results substantiate our developments.","sentences":["Electro-active materials are classified as electrostrictive and piezoelectric materials.","They deform under the action of an external electric field.","Piezoelectric material, as a special class of active materials, can produce an internal electric field when subjected to mechanical stress or strain.","In return, there is the converse piezoelectric response, which expresses the induction of the mechanical deformation in the material when it is subjected to the application of the electric field.","This work presents a variational-based computational modeling approach for failure prediction of ferromagnetic materials.","In order to solve this problem, a coupling between magnetostriction and mechanics is modeled, then the fracture mechanism in ferromagnetic materials is investigated.","Furthermore, the failure mechanics of ferromagnetic materials under the magnetostrictive effects is studied based on a variational phase-field model of fracture.","Phase-field fracture is numerically challenging since the energy functional may admit several local minima, imposing the global irreversibility of the fracture field and dependency of regularization parameters related discretization size.","Here, the failure behavior of a magnetoelastic solid body is formulated based on the Helmholtz free energy function, in which the strain tensor, the magnetic induction vector, and the crack phase-field are introduced as state variables.","This coupled formulation leads to a continuity equation for the magnetic vector potential through well-known Maxwell's equations.","Hence, the energetic crack driving force is governed by the coupled magneto-mechanical effects under the magneto-static state.","Several numerical results substantiate our developments."],"url":"http://arxiv.org/abs/2404.07346v1","category":"math.NA"}
{"created":"2024-04-10 20:27:33","title":"Determination of $K^0_S$ Fragmentation Functions including BESIII Measurements and using Neural Networks","abstract":"In this study, we revisit the extraction of parton-to-$K^0_S$ hadron fragmentation functions, named FF24-$K^0_S$, focusing on both next-to-leading-order and next-to-next-to-leading-order accuracy in perturbative QCD. Our approach involves the analysis of single inclusive electron-positron annihilation (SIA) data. The two key improvements are, on the one hand, the incorporation of the latest experimental data from the BESIII experiment and, on the other hand, the adoption of Neural Networks in the fitting procedure. To address experimental uncertainties, the Monte Carlo method is employed. Our investigation also explores the impact of hadron mass corrections on the description of SIA data, spanning a broad kinematic regime with a particular emphasis on the range of small $z$ values. The theory prediction for $K^0_S$ production at both NLO and NNLO accuracy exhibits good agreement with experimental data within their respective uncertainties.","sentences":["In this study, we revisit the extraction of parton-to-$K^0_S$ hadron fragmentation functions, named FF24-$K^0_S$, focusing on both next-to-leading-order and next-to-next-to-leading-order accuracy in perturbative QCD.","Our approach involves the analysis of single inclusive electron-positron annihilation (SIA) data.","The two key improvements are, on the one hand, the incorporation of the latest experimental data from the BESIII experiment and, on the other hand, the adoption of Neural Networks in the fitting procedure.","To address experimental uncertainties, the Monte Carlo method is employed.","Our investigation also explores the impact of hadron mass corrections on the description of SIA data, spanning a broad kinematic regime with a particular emphasis on the range of small $z$ values.","The theory prediction for $K^0_S$ production at both NLO and NNLO accuracy exhibits good agreement with experimental data within their respective uncertainties."],"url":"http://arxiv.org/abs/2404.07334v1","category":"hep-ph"}
{"created":"2024-04-10 20:20:53","title":"Proper motion study of the 6.7 GHz methanol maser rings. I. A sample of sources with little variation","abstract":"Methanol masers at 6.7~GHz are well-known signposts of high-mass star-forming regions. [...] We aim to understand the origin of the ring-like structures outlined by methanol maser emission in a number of sources. This emission could be, a priori, spatially associated with an outflow and/or disc around a high-mass protostar. [...] Using sensitive, three-epoch observations spanning over eight years with the European VLBI Network, we have started the most direct investigations of maser rings using very accurate proper motion measurements with uncertainties below 1\\,km~s$^{-1}$. We present full results for the five targets of our sample, G23.207-00.377, G23.389+00.185, G28.817+00.365, G31.047+00.356, and G31.581+00.077, where proper motions show similar characteristics; maser cloudlets do not move inwards towards the centre of the rings but rather outwards. We also include the most circular source, G23.657-00.127, in the discussion as a reference. The magnitude of maser proper motions ranges from a maximum of about 13\\,km~s$^{-1}$ to 0.5~km~s$^{-1}$. In two of the five sources with a high number of maser spots (>100), namely G23.207-00.377 and G23.389+00.185, we show that the size of the best elliptical model, fitted to the distribution of persistent masers, increases in time in a manner similar to the case of G23.657-00.127. Moreover, we checked the separations between the pairs of spots from distinct regions, and we were able to assess that G28.817+00.365 and G31.047+00.356 can be interpreted as showing expanding motions. We analysed the profiles of single maser cloudlets and studied their variability. Contrary to single-dish studies, the interferometric data indicate variability of the emission of single-masing cloudlets. In five of the six targets expansion motions prevail. Only in the case of G31.581+00.077 can a scenario of disc-like rotation not be excluded. [...]","sentences":["Methanol masers at 6.7~GHz are well-known signposts of high-mass star-forming regions.","[...] We aim to understand the origin of the ring-like structures outlined by methanol maser emission in a number of sources.","This emission could be, a priori, spatially associated with an outflow and/or disc around a high-mass protostar.","[...] Using sensitive, three-epoch observations spanning over eight years with the European VLBI Network, we have started the most direct investigations of maser rings using very accurate proper motion measurements with uncertainties below 1\\,km~s$^{-1}$.","We present full results for the five targets of our sample, G23.207-00.377, G23.389+00.185, G28.817+00.365, G31.047+00.356, and G31.581+00.077, where proper motions show similar characteristics; maser cloudlets do not move inwards towards the centre of the rings but rather outwards.","We also include the most circular source, G23.657-00.127, in the discussion as a reference.","The magnitude of maser proper motions ranges from a maximum of about 13\\,km~s$^{-1}$ to 0.5~km~s$^{-1}$.","In two of the five sources with a high number of maser spots (>100), namely G23.207-00.377 and G23.389+00.185, we show that the size of the best elliptical model, fitted to the distribution of persistent masers, increases in time in a manner similar to the case of G23.657-00.127.","Moreover, we checked the separations between the pairs of spots from distinct regions, and we were able to assess that G28.817+00.365 and G31.047+00.356 can be interpreted as showing expanding motions.","We analysed the profiles of single maser cloudlets and studied their variability.","Contrary to single-dish studies, the interferometric data indicate variability of the emission of single-masing cloudlets.","In five of the six targets expansion motions prevail.","Only in the case of G31.581+00.077 can a scenario of disc-like rotation not be excluded.","[...]"],"url":"http://arxiv.org/abs/2404.07333v1","category":"astro-ph.GA"}
{"created":"2024-04-10 20:18:12","title":"Primordial Intermediate and Supermassive Black Hole formation during the electron-positron annihilation epoch","abstract":"Some of the Intermediate Mass Black Hole (IMBH) candidates observed at the center of galaxies or in globular clusters and some of the Supermassive Black Holes (SMBHs) seen at the center of many galaxies might be of primordial origin. Indeed, Primordial Black Holes (PBHs) of such mass could have formed when the Universe was $\\sim$1-10$^3$ s old, due to the collapse of density fluctuations. In particular, when the Universe was $\\sim 1$ s in age, Electron-Positron Annihilation (EPA) took place. We explore the formation of intermediate mass and supermassive PBHs, taking into account the effect of the EPA when the fluctuations have a running-tilt power-law spectrum: when these cross the $10^{-0.5}$-$10^{3.0}$ s Universe horizon they could produce $5\\times 10^{3}$ - $5\\times 10^{8}M_{\\odot}$ PBHs with a density as high as $\\sim 10^{10}$/Gpc$^3$. On average, this implies a population of about one thousand PBHs in the Local Group of Galaxies, with the nearest one at about 250 kpc, just under half the distance to the Andromeda galaxy (M31).","sentences":["Some of the Intermediate Mass Black Hole (IMBH) candidates observed at the center of galaxies or in globular clusters and some of the Supermassive Black Holes (SMBHs) seen at the center of many galaxies might be of primordial origin.","Indeed, Primordial Black Holes (PBHs) of such mass could have formed when the Universe was $\\sim$1-10$^3$ s old, due to the collapse of density fluctuations.","In particular, when the Universe was $\\sim 1$ s in age, Electron-Positron Annihilation (EPA) took place.","We explore the formation of intermediate mass and supermassive PBHs, taking into account the effect of the EPA when the fluctuations have a running-tilt power-law spectrum: when these cross the $10^{-0.5}$-$10^{3.0}$ s Universe horizon they could produce $5\\times 10^{3}$ - $5\\times 10^{8}M_{\\odot}$ PBHs with a density as high as $\\sim 10^{10}$/Gpc$^3$. On average, this implies a population of about one thousand PBHs in the Local Group of Galaxies, with the nearest one at about 250 kpc, just under half the distance to the Andromeda galaxy (M31)."],"url":"http://arxiv.org/abs/2404.07332v1","category":"astro-ph.CO"}
{"created":"2024-04-10 20:17:40","title":"A Modified Depolarization Approach for Efficient Quantum Machine Learning","abstract":"Quantum Computing in the Noisy Intermediate-Scale Quantum (NISQ) era has shown promising applications in machine learning, optimization, and cryptography. Despite the progress, challenges persist due to system noise, errors, and decoherence that complicate the simulation of quantum systems. The depolarization channel is a standard tool for simulating a quantum system's noise. However, modeling such noise for practical applications is computationally expensive when we have limited hardware resources, as is the case in the NISQ era. We propose a modified representation for a single-qubit depolarization channel with two Kraus operators based only on X and Z Pauli matrices. Our approach reduces the computational complexity from six to four matrix multiplications per execution of a channel. Experiments on a Quantum Machine Learning (QML) model on the Iris dataset across various circuit depths and depolarization rates validate that our approach maintains the model's accuracy while improving efficiency. This simplified noise model enables more scalable simulations of quantum circuits under depolarization, advancing capabilities in the NISQ era.","sentences":["Quantum Computing in the Noisy Intermediate-Scale Quantum (NISQ) era has shown promising applications in machine learning, optimization, and cryptography.","Despite the progress, challenges persist due to system noise, errors, and decoherence that complicate the simulation of quantum systems.","The depolarization channel is a standard tool for simulating a quantum system's noise.","However, modeling such noise for practical applications is computationally expensive when we have limited hardware resources, as is the case in the NISQ era.","We propose a modified representation for a single-qubit depolarization channel with two Kraus operators based only on X and Z Pauli matrices.","Our approach reduces the computational complexity from six to four matrix multiplications per execution of a channel.","Experiments on a Quantum Machine Learning (QML) model on the Iris dataset across various circuit depths and depolarization rates validate that our approach maintains the model's accuracy while improving efficiency.","This simplified noise model enables more scalable simulations of quantum circuits under depolarization, advancing capabilities in the NISQ era."],"url":"http://arxiv.org/abs/2404.07330v1","category":"quant-ph"}
{"created":"2024-04-10 19:58:30","title":"Surrogate modeling for probability distribution estimation:uniform or adaptive design?","abstract":"The active learning (AL) technique, one of the state-of-the-art methods for constructing surrogate models, has shown high accuracy and efficiency in forward uncertainty quantification (UQ) analysis. This paper provides a comprehensive study on AL-based global surrogates for computing the full distribution function, i.e., the cumulative distribution function (CDF) and the complementary CDF (CCDF). To this end, we investigate the three essential components for building surrogates, i.e., types of surrogate models, enrichment methods for experimental designs, and stopping criteria. For each component, we choose several representative methods and study their desirable configurations. In addition, we devise a uniform design (i.e., space-filling design) as a baseline for measuring the improvement of using AL. Combining all the representative methods, a total of 1,920 UQ analyses are carried out to solve 16 benchmark examples. The performance of the selected strategies is evaluated based on accuracy and efficiency. In the context of full distribution estimation, this study concludes that (i) AL techniques cannot provide a systematic improvement compared with uniform designs, (ii) the recommended surrogate modeling methods depend on the features of the problems (especially the local nonlinearity), target accuracy, and computational budget.","sentences":["The active learning (AL) technique, one of the state-of-the-art methods for constructing surrogate models, has shown high accuracy and efficiency in forward uncertainty quantification (UQ) analysis.","This paper provides a comprehensive study on AL-based global surrogates for computing the full distribution function, i.e., the cumulative distribution function (CDF) and the complementary CDF (CCDF).","To this end, we investigate the three essential components for building surrogates, i.e., types of surrogate models, enrichment methods for experimental designs, and stopping criteria.","For each component, we choose several representative methods and study their desirable configurations.","In addition, we devise a uniform design (i.e., space-filling design) as a baseline for measuring the improvement of using AL.","Combining all the representative methods, a total of 1,920 UQ analyses are carried out to solve 16 benchmark examples.","The performance of the selected strategies is evaluated based on accuracy and efficiency.","In the context of full distribution estimation, this study concludes that (i) AL techniques cannot provide a systematic improvement compared with uniform designs, (ii) the recommended surrogate modeling methods depend on the features of the problems (especially the local nonlinearity), target accuracy, and computational budget."],"url":"http://arxiv.org/abs/2404.07323v1","category":"stat.ME"}
{"created":"2024-04-10 19:12:26","title":"Exact results for the Ising model on a small-world network","abstract":"Small-world networks provide an interesting framework for studying the interplay between regular and random graphs, where links are located in a regular and random way, respectively. On one hand, the random links make the model to obey some kind of mean-field behavior. On the other hand, the links of the regular lattice make the system to retain some related non trivial correlations. The coexistence of these two features in general prevent a closed analytical treatment. Here we consider a one-dimensional small-world Ising model and derive analytically its equation of state, critical point, critical behavior, and critical correlations. Despite being one of the simplest small-world models, our exact and intuitive analysis reveals some intriguing properties.","sentences":["Small-world networks provide an interesting framework for studying the interplay between regular and random graphs, where links are located in a regular and random way, respectively.","On one hand, the random links make the model to obey some kind of mean-field behavior.","On the other hand, the links of the regular lattice make the system to retain some related non trivial correlations.","The coexistence of these two features in general prevent a closed analytical treatment.","Here we consider a one-dimensional small-world Ising model and derive analytically its equation of state, critical point, critical behavior, and critical correlations.","Despite being one of the simplest small-world models, our exact and intuitive analysis reveals some intriguing properties."],"url":"http://arxiv.org/abs/2404.07310v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 18:57:02","title":"Generalized Quasikernels in Digraphs","abstract":"Given a digraph $D$, we say that a set of vertices $Q\\subseteq V(D)$ is a $q$-kernel if $Q$ is an independent set and if every vertex of $D$ can be reached from $Q$ by a path of length at most $q$. In this paper, we initiate the study of several extremal problems for $q$-kernels. For example, we introduce and make progress on (what turns out to be) a weak version of the Small Quasikernel Conjecture, namely that every digraph contains a $q$-kernel with $|N^+[Q]|\\ge \\frac{1}{2}|V(D)|$ for all $q\\ge 2$.","sentences":["Given a digraph $D$, we say that a set of vertices $Q\\subseteq V(D)$ is a $q$-kernel if $Q$ is an independent set and if every vertex of $D$ can be reached from $Q$ by a path of length at most $q$. In this paper, we initiate the study of several extremal problems for $q$-kernels.","For example, we introduce and make progress on (what turns out to be) a weak version of the Small Quasikernel Conjecture, namely that every digraph contains a $q$-kernel with $|N^+[Q]|\\ge \\frac{1}{2}|V(D)|$ for all $q\\ge 2$."],"url":"http://arxiv.org/abs/2404.07305v1","category":"math.CO"}
{"created":"2024-04-10 18:56:53","title":"We're Calling an Intervention: Taking a Closer Look at Language Model Adaptation to Different Types of Linguistic Variation","abstract":"We present a suite of interventions and experiments that allow us to understand language model adaptation to text with linguistic variation (e.g., nonstandard or dialectal text). Our interventions address several features of linguistic variation, resulting in character, subword, and word-level changes. Applying our interventions during language model adaptation with varying size and nature of training data, we gain important insights into what makes linguistic variation particularly difficult for language models to deal with. For instance, on text with character-level variation, performance improves with even a few training examples but approaches a plateau, suggesting that more data is not the solution. In contrast, on text with variation involving new words or meanings, far more data is needed, but it leads to a massive breakthrough in performance. Our findings inform future work on dialectal NLP and making language models more robust to linguistic variation overall. We make the code for our interventions, which can be applied to any English text data, publicly available.","sentences":["We present a suite of interventions and experiments that allow us to understand language model adaptation to text with linguistic variation (e.g., nonstandard or dialectal text).","Our interventions address several features of linguistic variation, resulting in character, subword, and word-level changes.","Applying our interventions during language model adaptation with varying size and nature of training data, we gain important insights into what makes linguistic variation particularly difficult for language models to deal with.","For instance, on text with character-level variation, performance improves with even a few training examples but approaches a plateau, suggesting that more data is not the solution.","In contrast, on text with variation involving new words or meanings, far more data is needed, but it leads to a massive breakthrough in performance.","Our findings inform future work on dialectal NLP and making language models more robust to linguistic variation overall.","We make the code for our interventions, which can be applied to any English text data, publicly available."],"url":"http://arxiv.org/abs/2404.07304v1","category":"cs.CL"}
{"created":"2024-04-10 18:53:22","title":"Quantum algorithms to simulate quadratic classical Hamiltonians and optimal control","abstract":"Simulation of realistic classical mechanical systems is of great importance to many areas of engineering such as robotics, dynamics of rotating machinery and control theory. In this work, we develop quantum algorithms to estimate quantities of interest such as the kinetic energy in a given classical mechanical system in the presence of friction or damping as well as forcing or source terms, which makes the algorithm of practical interest. We show that for such systems, the quantum algorithm scales polynomially with the logarithm of the dimension of the system. We cast this problem in terms of Hamilton's equations of motion (equivalent to the first variation of the Lagrangian) and solve them using quantum algorithms for differential equations. We then consider the hardness of estimating the kinetic energy of a damped coupled oscillator system. We show that estimating the kinetic energy at a given time of this system to within additive precision is BQP hard when the strength of the damping term is bounded by an inverse polynomial in the number of qubits. We then consider the problem of designing optimal control of classical systems, which can be cast as the second variation of the Lagrangian. In this direction, we first consider the Riccati equation, which is a nonlinear differential equation ubiquitous in control theory. We give an efficient quantum algorithm to solve the Riccati differential equation well into the nonlinear regime. To our knowledge, this is the first example of any nonlinear differential equation that can be solved when the strength of the nonlinearity is asymptotically greater than the amount of dissipation. We then show how to use this algorithm to solve the linear quadratic regulator problem, which is an example of the Hamilton-Jacobi-Bellman equation.","sentences":["Simulation of realistic classical mechanical systems is of great importance to many areas of engineering such as robotics, dynamics of rotating machinery and control theory.","In this work, we develop quantum algorithms to estimate quantities of interest such as the kinetic energy in a given classical mechanical system in the presence of friction or damping as well as forcing or source terms, which makes the algorithm of practical interest.","We show that for such systems, the quantum algorithm scales polynomially with the logarithm of the dimension of the system.","We cast this problem in terms of Hamilton's equations of motion (equivalent to the first variation of the Lagrangian) and solve them using quantum algorithms for differential equations.","We then consider the hardness of estimating the kinetic energy of a damped coupled oscillator system.","We show that estimating the kinetic energy at a given time of this system to within additive precision is BQP hard when the strength of the damping term is bounded by an inverse polynomial in the number of qubits.","We then consider the problem of designing optimal control of classical systems, which can be cast as the second variation of the Lagrangian.","In this direction, we first consider the Riccati equation, which is a nonlinear differential equation ubiquitous in control theory.","We give an efficient quantum algorithm to solve the Riccati differential equation well into the nonlinear regime.","To our knowledge, this is the first example of any nonlinear differential equation that can be solved when the strength of the nonlinearity is asymptotically greater than the amount of dissipation.","We then show how to use this algorithm to solve the linear quadratic regulator problem, which is an example of the Hamilton-Jacobi-Bellman equation."],"url":"http://arxiv.org/abs/2404.07303v1","category":"quant-ph"}
{"created":"2024-04-10 18:43:51","title":"Nature of Optical Thermodynamic Pressure Exerted in Highly Multimoded Nonlinear Systems","abstract":"The theory of optical thermodynamics provides a comprehensive framework that enables a self-consistent description of the intricate dynamics of nonlinear multimoded photonic systems. This theory, among others, predicts a pressure-like intensive quantity ($\\hat{p}$) that is conjugate to the system's total number of modes ($M$) - its corresponding extensive variable. Yet at this point, the nature of this intensive quantity is still nebulous. In this Letter, we elucidate the physical origin of the optical thermodynamic pressure and demonstrate its dual essence. In this context, we rigorously derive an expression that splits $\\hat{p}$ into two distinct components, a term that is explicitly tied to the electrodynamic radiation pressure and a second entropic part that is responsible for the entropy change. We utilize this result to establish a formalism that simplifies the quantification of radiation pressure under nonlinear equilibrium conditions, thus eliminating the need for a tedious evaluation of the Maxwell stress tensor. Our theoretical analysis is corroborated by numerical simulations carried out in highly multimoded nonlinear optical structures. These results may provide a novel way in predicting and controlling radiation pressure processes in a variety of nonlinear electromagnetic settings.","sentences":["The theory of optical thermodynamics provides a comprehensive framework that enables a self-consistent description of the intricate dynamics of nonlinear multimoded photonic systems.","This theory, among others, predicts a pressure-like intensive quantity ($\\hat{p}$) that is conjugate to the system's total number of modes ($M$) - its corresponding extensive variable.","Yet at this point, the nature of this intensive quantity is still nebulous.","In this Letter, we elucidate the physical origin of the optical thermodynamic pressure and demonstrate its dual essence.","In this context, we rigorously derive an expression that splits $\\hat{p}$ into two distinct components, a term that is explicitly tied to the electrodynamic radiation pressure and a second entropic part that is responsible for the entropy change.","We utilize this result to establish a formalism that simplifies the quantification of radiation pressure under nonlinear equilibrium conditions, thus eliminating the need for a tedious evaluation of the Maxwell stress tensor.","Our theoretical analysis is corroborated by numerical simulations carried out in highly multimoded nonlinear optical structures.","These results may provide a novel way in predicting and controlling radiation pressure processes in a variety of nonlinear electromagnetic settings."],"url":"http://arxiv.org/abs/2404.07295v1","category":"physics.optics"}
{"created":"2024-04-10 18:41:23","title":"Rigorous analysis of optical forces in dielectric structures based on the Minkowski-Helmholtz formula","abstract":"Optical forces in dielectric structures are typically analyzed by utilizing either the Maxwell stress tensor or energy-based methods from which they can be derived by means of the eigenfrequencies and the effective refractive indices involved. While the equivalence of these two methods has been discussed in several studies, it would seem that a general electrodynamic proof of this aspect is still lacking. In this work, we provide a rigorous electrodynamic derivation based on the Minkowski-Helmholtz formula and the electromagnetic variation theorem, from which one can directly conclude that under Hermitian conditions these two approaches are formally equivalent to each other. The results of our study universally apply to any dielectric waveguide or cavity configuration. In addition, this methodology can be employed in graded index systems that do not exhibit sharp interfaces. Importantly, our analysis offers a straightforward route for predicting optical forces in a variety of photonic arrangements, including dielectric scatterers and multielement array configurations.","sentences":["Optical forces in dielectric structures are typically analyzed by utilizing either the Maxwell stress tensor or energy-based methods from which they can be derived by means of the eigenfrequencies and the effective refractive indices involved.","While the equivalence of these two methods has been discussed in several studies, it would seem that a general electrodynamic proof of this aspect is still lacking.","In this work, we provide a rigorous electrodynamic derivation based on the Minkowski-Helmholtz formula and the electromagnetic variation theorem, from which one can directly conclude that under Hermitian conditions these two approaches are formally equivalent to each other.","The results of our study universally apply to any dielectric waveguide or cavity configuration.","In addition, this methodology can be employed in graded index systems that do not exhibit sharp interfaces.","Importantly, our analysis offers a straightforward route for predicting optical forces in a variety of photonic arrangements, including dielectric scatterers and multielement array configurations."],"url":"http://arxiv.org/abs/2404.07294v1","category":"physics.optics"}
{"created":"2024-04-10 18:39:08","title":"Large, defect-free FCC colloidal crystals under microgravity","abstract":"Here we report the results of microgravity experiments performed on the International Space Station (ISS) to study crystallized, metastable colloidal liquids in the region of the hard sphere phase diagram that has been found to be glassy on Earth. Using confocal microscopy, we observed the self-assembly of high density, three-dimensional colloidal crystals from micron-size hard spheres suspended within a fluid medium. The largest face-centered cubic (FCC) phase measured 27 x 1.5 x 0.15 mm. From a practical aspect, the fact that a single, topological defect-free FCC colloidal crystal can be grown in microgravity and the crystal returned from orbit suggests new routes for manufacturing colloidal devices, particularly optical elements in space.","sentences":["Here we report the results of microgravity experiments performed on the International Space Station (ISS) to study crystallized, metastable colloidal liquids in the region of the hard sphere phase diagram that has been found to be glassy on Earth.","Using confocal microscopy, we observed the self-assembly of high density, three-dimensional colloidal crystals from micron-size hard spheres suspended within a fluid medium.","The largest face-centered cubic (FCC) phase measured 27 x 1.5 x 0.15 mm.","From a practical aspect, the fact that a single, topological defect-free FCC colloidal crystal can be grown in microgravity and the crystal returned from orbit suggests new routes for manufacturing colloidal devices, particularly optical elements in space."],"url":"http://arxiv.org/abs/2404.07291v1","category":"cond-mat.soft"}
{"created":"2024-04-10 18:32:17","title":"Nash Equilibrium Seeking for Noncooperative Duopoly Games via Event-Triggered Control","abstract":"This paper proposes a novel approach for locally stable convergence to Nash equilibrium in duopoly noncooperative games based on a distributed event-triggered control scheme. The proposed approach employs extremum seeking, with sinusoidal perturbation signals applied to estimate the Gradient (first derivative) of unknown quadratic payoff functions. This is the first instance of noncooperative games being tackled in a model-free fashion integrated with the event-triggered methodology. Each player evaluates independently the deviation between the corresponding current state variable and its last broadcasted value to update the player action, while they preserve control performance under limited bandwidth of the actuation paths and still guarantee stability for the closed-loop dynamics. In particular, the stability analysis is carried out using time-scaling technique, Lyapunov's direct method and averaging theory for discontinuous systems. We quantify the size of the ultimate small residual sets around the Nash equilibrium and illustrate the theoretical results numerically on an example.","sentences":["This paper proposes a novel approach for locally stable convergence to Nash equilibrium in duopoly noncooperative games based on a distributed event-triggered control scheme.","The proposed approach employs extremum seeking, with sinusoidal perturbation signals applied to estimate the Gradient (first derivative) of unknown quadratic payoff functions.","This is the first instance of noncooperative games being tackled in a model-free fashion integrated with the event-triggered methodology.","Each player evaluates independently the deviation between the corresponding current state variable and its last broadcasted value to update the player action, while they preserve control performance under limited bandwidth of the actuation paths and still guarantee stability for the closed-loop dynamics.","In particular, the stability analysis is carried out using time-scaling technique, Lyapunov's direct method and averaging theory for discontinuous systems.","We quantify the size of the ultimate small residual sets around the Nash equilibrium and illustrate the theoretical results numerically on an example."],"url":"http://arxiv.org/abs/2404.07287v1","category":"math.OC"}
{"created":"2024-04-10 18:29:00","title":"Flexible Asymmetrically Transparent Conductive Electrode based on Photonic Nanojet Arrays","abstract":"Flexible transparent electrodes, encompassing the combination of optical transparency and electrical conductivity, empower numerous optoelectronic applications. While the main efforts nowadays concentrate on developing wire meshes and conductive oxides, those technologies are still in a quest to find a balance between price, performance, and versatility. Here we propose a new platform, encompassing the advantages of nanophotonic design and roll-to-roll large-scale lithography fabrication tools, granting an ultimate balance between optical, electrical, and mechanical properties. The design is based on an array of silica microspheres deposited on a patterned thin aluminum film attached to a flexible polymer matrix. Microspheres are designed to squeeze 80% light through nanoscale apertures with the aid of the photonic nanojet effect given the light impinges the structure from the top. The photonic structure blocks the transmission for the backpropagation direction thus granting the device with the high 5-fold level of asymmetry. The patterned layer demonstrates a remarkable 2.8 {\\Omega}/sq sheet resistance comparable to that of a continuous metal layer. The high conductivity is shown to be maintained after a repeatable application of strain on the flexible electrode. The technical specifications of the demonstrated transparent electrode establish it as a viable option for integrating into advanced optoelectronic devices such as solar cells, touchscreens, and organic light-emitting diodes to name a few. Its notable capacity to optimize light transmittance while ensuring consistent electrical performance, alongside its mechanical flexibility, makes the demonstrated device an essential component for applications, where such attributes are critically required.","sentences":["Flexible transparent electrodes, encompassing the combination of optical transparency and electrical conductivity, empower numerous optoelectronic applications.","While the main efforts nowadays concentrate on developing wire meshes and conductive oxides, those technologies are still in a quest to find a balance between price, performance, and versatility.","Here we propose a new platform, encompassing the advantages of nanophotonic design and roll-to-roll large-scale lithography fabrication tools, granting an ultimate balance between optical, electrical, and mechanical properties.","The design is based on an array of silica microspheres deposited on a patterned thin aluminum film attached to a flexible polymer matrix.","Microspheres are designed to squeeze 80% light through nanoscale apertures with the aid of the photonic nanojet effect given the light impinges the structure from the top.","The photonic structure blocks the transmission for the backpropagation direction thus granting the device with the high 5-fold level of asymmetry.","The patterned layer demonstrates a remarkable 2.8 {\\Omega}/sq sheet resistance comparable to that of a continuous metal layer.","The high conductivity is shown to be maintained after a repeatable application of strain on the flexible electrode.","The technical specifications of the demonstrated transparent electrode establish it as a viable option for integrating into advanced optoelectronic devices such as solar cells, touchscreens, and organic light-emitting diodes to name a few.","Its notable capacity to optimize light transmittance while ensuring consistent electrical performance, alongside its mechanical flexibility, makes the demonstrated device an essential component for applications, where such attributes are critically required."],"url":"http://arxiv.org/abs/2404.07286v1","category":"physics.optics"}
{"created":"2024-04-10 18:09:15","title":"Pointwise two-point function estimates and a non-pertubative proof of mean-field critical behaviour for long-range percolation","abstract":"In long-range percolation on $\\mathbb{Z}^d$, we connect each pair of distinct points $x$ and $y$ by an edge independently at random with probability $1-\\exp(-\\beta\\|x-y\\|^{-d-\\alpha})$, where $\\alpha>0$ is fixed and $\\beta\\geq 0$ is a parameter. In a previous paper, we proved that if $0<\\alpha<d$ then the critical two-point function satisfies the spatially averaged upper bound \\[ \\frac{1}{r^d}\\sum_{x\\in [-r,r]^d} \\mathbb{P}_{\\beta_c}(0\\leftrightarrow x) \\preceq r^{-d+\\alpha} \\] for every $r\\geq 1$. This upper bound is believed to be sharp for values of $\\alpha$ strictly below the crossover value $\\alpha_c(d)$, and a matching lower bound for $\\alpha<1$ was proven by B\\\"aumler and Berger (AIHP 2022). In this paper, we prove pointwise upper and lower bounds of the same order under the same assumption that $\\alpha<1$. We also prove analogous two-sided pointwise estimates on the slightly subcritical two-point function under the same hypotheses, interpolating between $\\| x \\|^{-d+\\alpha}$ decay below the correlation length and $\\| x \\|^{-d-\\alpha}$ decay above the correlation length. In dimensions $d=1,2,3$, we deduce that the triangle condition holds under the minimal assumption that $0<\\alpha<d/3$. While this result had previously been established under additional perturbative assumptions using the lace expansion, our proof is completely non-perturbative and does not rely on the lace expansion in any way. In dimensions $1$ and $2$ our results also treat the marginal case $\\alpha=d/3$, implying that the triangle diagram diverges at most logarithmically and hence that mean-field critical behaviour holds to within polylogarithmic factors.","sentences":["In long-range percolation on $\\mathbb{Z}^d$, we connect each pair of distinct points $x$ and $y$ by an edge independently at random with probability $1-\\exp(-\\beta\\|x-y\\|^{-d-\\alpha})$, where $\\alpha>0$ is fixed and $\\beta\\geq 0$ is a parameter.","In a previous paper, we proved that if $0<\\alpha<d$ then the critical two-point function satisfies the spatially averaged upper bound \\[ \\frac{1}{r^d}\\sum_{x\\in [-r,r]^d} \\mathbb{P}_{\\beta_c}(0\\leftrightarrow x) \\preceq r^{-d+\\alpha} \\] for every $r\\geq 1$.","This upper bound is believed to be sharp for values of $\\alpha$ strictly below the crossover value $\\alpha_c(d)$, and a matching lower bound for $\\alpha<1$ was proven by B\\\"aumler and Berger (AIHP 2022).","In this paper, we prove pointwise upper and lower bounds of the same order under the same assumption that $\\alpha<1$. We also prove analogous two-sided pointwise estimates on the slightly subcritical two-point function under the same hypotheses, interpolating between $\\| x \\|^{-d+\\alpha}$ decay below the correlation length and $\\| x \\|^{-d-\\alpha}$ decay above the correlation length.","In dimensions $d=1,2,3$, we deduce that the triangle condition holds under the minimal assumption that $0<\\alpha<d/3$. While this result had previously been established under additional perturbative assumptions using the lace expansion, our proof is completely non-perturbative and does not rely on the lace expansion in any way.","In dimensions $1$ and $2$ our results also treat the marginal case $\\alpha=d/3$, implying that the triangle diagram diverges at most logarithmically and hence that mean-field critical behaviour holds to within polylogarithmic factors."],"url":"http://arxiv.org/abs/2404.07276v1","category":"math.PR"}
{"created":"2024-04-10 18:07:30","title":"Certification of MPC-based zonal controller security properties using accuracy-aware machine learning proxies","abstract":"The fast growth of renewable energies increases the power congestion risk. To address this issue, the French Transmission System Operator (RTE) has developed closed-loop controllers to handle congestion. RTE wishes to estimate the probability that the controllers ensure the equipment's safety to guarantee their proper functioning. The naive approach to estimating this probability relies on simulating many randomly drawn scenarios and then using all the outcomes to build a confidence interval around the probability. Although theory ensures convergence, the computational cost of power system simulations makes such a process intractable.   The present paper aims to propose a faster process using machine-learning-based proxies. The amount of required simulations is significantly reduced thanks to an accuracy-aware proxy built with Multivariate Gaussian Processes. However, using a proxy instead of the simulator adds uncertainty to the outcomes. An adaptation of the Central Limit Theorem is thus proposed to include the uncertainty of the outcomes predicted with the proxy into the confidence interval. As a case study, we designed a simple simulator that was tested on a small network. Results show that the proxy learns to approximate the simulator's answer accurately, allowing a significant time gain for the machine-learning-based process.","sentences":["The fast growth of renewable energies increases the power congestion risk.","To address this issue, the French Transmission System Operator (RTE) has developed closed-loop controllers to handle congestion.","RTE wishes to estimate the probability that the controllers ensure the equipment's safety to guarantee their proper functioning.","The naive approach to estimating this probability relies on simulating many randomly drawn scenarios and then using all the outcomes to build a confidence interval around the probability.","Although theory ensures convergence, the computational cost of power system simulations makes such a process intractable.   ","The present paper aims to propose a faster process using machine-learning-based proxies.","The amount of required simulations is significantly reduced thanks to an accuracy-aware proxy built with Multivariate Gaussian Processes.","However, using a proxy instead of the simulator adds uncertainty to the outcomes.","An adaptation of the Central Limit Theorem is thus proposed to include the uncertainty of the outcomes predicted with the proxy into the confidence interval.","As a case study, we designed a simple simulator that was tested on a small network.","Results show that the proxy learns to approximate the simulator's answer accurately, allowing a significant time gain for the machine-learning-based process."],"url":"http://arxiv.org/abs/2404.07275v1","category":"stat.AP"}
{"created":"2024-04-10 18:01:06","title":"Cautionary tales on heating-rate prescriptions in kilonovae","abstract":"A major ingredient for kilonova lightcurves is the radioactive heating rate and its dependence on the electron fraction and velocity of the ejecta and, in principle, on the nuclear mass formula. Heating-rate formulae commonly used as the basis for kilonova models are, strictly speaking, incorrect for electron fractions other than $Y_{e} = 0.04$. Here, we introduce new semi-analytical models for kilonovae with better heating rate prescriptions valid for the full parameter space of kilonova velocities and electron fractions. This new prescription produces, on average, dimmer kilonovae at peak that decay more slowly as compared to previously used prescriptions for otherwise identical kilonova physics. We show the dangers of using inappropriate heating rate estimates by simulating realistic observations and inferring the kilonova parameters via a misspecified heating-rate prescription. While providing great fits to the photometry, an incorrect heating-rate prescription fails to recover the input ejecta masses at greater than $5\\sigma$. This bias from an incorrect prescription has disastrous consequences for interpreting kilonovae, their use as additional components in gamma-ray burst afterglows, and understanding their role in cosmic chemical evolution or for multi-messenger constraints on the nuclear equation of state. Given the true heating-rate is uncertain, we estimate there is a $\\approx 5$ and $\\approx 10\\%$ systematic uncertainty in the measured ejecta masses and velocities, respectively. For lanthanide-rich ejecta, the systematic uncertainty could be as high as $\\approx 50\\%$. This systematic uncertainty limits the precision of any measurements that rely on accurate estimates of kilonova ejecta properties.","sentences":["A major ingredient for kilonova lightcurves is the radioactive heating rate and its dependence on the electron fraction and velocity of the ejecta and, in principle, on the nuclear mass formula.","Heating-rate formulae commonly used as the basis for kilonova models are, strictly speaking, incorrect for electron fractions other than $Y_{e} = 0.04$. Here, we introduce new semi-analytical models for kilonovae with better heating rate prescriptions valid for the full parameter space of kilonova velocities and electron fractions.","This new prescription produces, on average, dimmer kilonovae at peak that decay more slowly as compared to previously used prescriptions for otherwise identical kilonova physics.","We show the dangers of using inappropriate heating rate estimates by simulating realistic observations and inferring the kilonova parameters via a misspecified heating-rate prescription.","While providing great fits to the photometry, an incorrect heating-rate prescription fails to recover the input ejecta masses at greater than $5\\sigma$. This bias from an incorrect prescription has disastrous consequences for interpreting kilonovae, their use as additional components in gamma-ray burst afterglows, and understanding their role in cosmic chemical evolution or for multi-messenger constraints on the nuclear equation of state.","Given the true heating-rate is uncertain, we estimate there is a $\\approx 5$ and $\\approx 10\\%$ systematic uncertainty in the measured ejecta masses and velocities, respectively.","For lanthanide-rich ejecta, the systematic uncertainty could be as high as $\\approx 50\\%$.","This systematic uncertainty limits the precision of any measurements that rely on accurate estimates of kilonova ejecta properties."],"url":"http://arxiv.org/abs/2404.07271v1","category":"astro-ph.HE"}
{"created":"2024-04-10 18:00:21","title":"Closed-Loop Model Identification and MPC-based Navigation of Quadcopters: A Case Study of Parrot Bebop 2","abstract":"The growing potential of quadcopters in various domains, such as aerial photography, search and rescue, and infrastructure inspection, underscores the need for real-time control under strict safety and operational constraints. This challenge is compounded by the inherent nonlinear dynamics of quadcopters and the on-board computational limitations they face. This paper aims at addressing these challenges. First, this paper presents a comprehensive procedure for deriving a linear yet efficient model to describe the dynamics of quadrotors, thereby reducing complexity without compromising efficiency. Then, this paper develops a steady-state-aware Model Predictive Control (MPC) to effectively navigate quadcopters, while guaranteeing constraint satisfaction at all times. The main advantage of the steady-state-aware MPC is its low computational complexity, which makes it an appropriate choice for systems with limited computing capacity, like quadcopters. This paper considers Parrot Bebop 2 as the running example, and experimentally validates and evaluates the proposed algorithms.","sentences":["The growing potential of quadcopters in various domains, such as aerial photography, search and rescue, and infrastructure inspection, underscores the need for real-time control under strict safety and operational constraints.","This challenge is compounded by the inherent nonlinear dynamics of quadcopters and the on-board computational limitations they face.","This paper aims at addressing these challenges.","First, this paper presents a comprehensive procedure for deriving a linear yet efficient model to describe the dynamics of quadrotors, thereby reducing complexity without compromising efficiency.","Then, this paper develops a steady-state-aware Model Predictive Control (MPC) to effectively navigate quadcopters, while guaranteeing constraint satisfaction at all times.","The main advantage of the steady-state-aware MPC is its low computational complexity, which makes it an appropriate choice for systems with limited computing capacity, like quadcopters.","This paper considers Parrot Bebop 2 as the running example, and experimentally validates and evaluates the proposed algorithms."],"url":"http://arxiv.org/abs/2404.07267v1","category":"cs.RO"}
{"created":"2024-04-10 18:00:17","title":"Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity","abstract":"We study the problem of online sequential decision-making given auxiliary demonstrations from experts who made their decisions based on unobserved contextual information. These demonstrations can be viewed as solving related but slightly different tasks than what the learner faces. This setting arises in many application domains, such as self-driving cars, healthcare, and finance, where expert demonstrations are made using contextual information, which is not recorded in the data available to the learning agent. We model the problem as a zero-shot meta-reinforcement learning setting with an unknown task distribution and a Bayesian regret minimization objective, where the unobserved tasks are encoded as parameters with an unknown prior. We propose the Experts-as-Priors algorithm (ExPerior), a non-parametric empirical Bayes approach that utilizes the principle of maximum entropy to establish an informative prior over the learner's decision-making problem. This prior enables the application of any Bayesian approach for online decision-making, such as posterior sampling. We demonstrate that our strategy surpasses existing behaviour cloning and online algorithms for multi-armed bandits and reinforcement learning, showcasing the utility of our approach in leveraging expert demonstrations across different decision-making setups.","sentences":["We study the problem of online sequential decision-making given auxiliary demonstrations from experts who made their decisions based on unobserved contextual information.","These demonstrations can be viewed as solving related but slightly different tasks than what the learner faces.","This setting arises in many application domains, such as self-driving cars, healthcare, and finance, where expert demonstrations are made using contextual information, which is not recorded in the data available to the learning agent.","We model the problem as a zero-shot meta-reinforcement learning setting with an unknown task distribution and a Bayesian regret minimization objective, where the unobserved tasks are encoded as parameters with an unknown prior.","We propose the Experts-as-Priors algorithm (ExPerior), a non-parametric empirical Bayes approach that utilizes the principle of maximum entropy to establish an informative prior over the learner's decision-making problem.","This prior enables the application of any Bayesian approach for online decision-making, such as posterior sampling.","We demonstrate that our strategy surpasses existing behaviour cloning and online algorithms for multi-armed bandits and reinforcement learning, showcasing the utility of our approach in leveraging expert demonstrations across different decision-making setups."],"url":"http://arxiv.org/abs/2404.07266v1","category":"cs.LG"}
{"created":"2024-04-10 18:00:04","title":"Transit Spectroscopy of K2-33b with Subaru/IRD: Spin-Orbit Alignment and Tentative Atmospheric Helium","abstract":"Exoplanets in their infancy are ideal targets to probe the formation and evolution history of planetary systems, including the planet migration and atmospheric evolution and dissipation. In this paper, we present spectroscopic observations and analyses of two planetary transits of K2-33b, which is known to be one of the youngest transiting planets (age $\\approx 8-11$ Myr) around a pre-main-sequence M-type star. Analysing K2-33's near-infrared spectra obtained by the IRD instrument on Subaru, we investigate the spin-orbit angle and transit-induced excess absorption for K2-33b. We attempt both classical modelling of the Rossiter-McLaughlin (RM) effect and Doppler-shadow analyses for the measurements of the projected stellar obliquity, finding a low angle of $\\lambda=-6_{-58}^{+61}$ deg (for RM analysis) and $\\lambda=-10_{-24}^{+22}$ deg (for Doppler-shadow analysis). In the modelling of the RM effect, we allow the planet-to-star radius ratio to float freely to take into account the possible smaller radius in the near infrared, but the constraint we obtain ($R_p/R_s=0.037_{-0.017}^{+0.013}$) is inconclusive due to the low radial-velocity precision. Comparison spectra of K2-33 of the 1083 nm triplet of metastable ortho-He I obtained in and out of the 2021 transit reveal excess absorption that could be due to an escaping He-rich atmosphere. Under certain conditions on planet mass and stellar XUV emission, the implied escape rate is sufficient to remove an Earth-mass H/He in $\\sim$1 Gyr, transforming this object from a Neptune to a super-Earth.","sentences":["Exoplanets in their infancy are ideal targets to probe the formation and evolution history of planetary systems, including the planet migration and atmospheric evolution and dissipation.","In this paper, we present spectroscopic observations and analyses of two planetary transits of K2-33b, which is known to be one of the youngest transiting planets (age $\\approx 8-11$ Myr) around a pre-main-sequence M-type star.","Analysing K2-33's near-infrared spectra obtained by the IRD instrument on Subaru, we investigate the spin-orbit angle and transit-induced excess absorption for K2-33b.","We attempt both classical modelling of the Rossiter-McLaughlin (RM) effect and Doppler-shadow analyses for the measurements of the projected stellar obliquity, finding a low angle of $\\lambda=-6_{-58}^{+61}$ deg (for RM analysis) and $\\lambda=-10_{-24}^{+22}$ deg (for Doppler-shadow analysis).","In the modelling of the RM effect, we allow the planet-to-star radius ratio to float freely to take into account the possible smaller radius in the near infrared, but the constraint we obtain ($R_p/R_s=0.037_{-0.017}^{+0.013}$) is inconclusive due to the low radial-velocity precision.","Comparison spectra of K2-33 of the 1083 nm triplet of metastable ortho-He I obtained in and out of the 2021 transit reveal excess absorption that could be due to an escaping He-rich atmosphere.","Under certain conditions on planet mass and stellar XUV emission, the implied escape rate is sufficient to remove an Earth-mass H/He in $\\sim$1 Gyr, transforming this object from a Neptune to a super-Earth."],"url":"http://arxiv.org/abs/2404.07262v1","category":"astro-ph.EP"}
{"created":"2024-04-10 18:00:01","title":"Accretion Flares from Stellar Collisions in Galactic Nuclei","abstract":"The strong tidal force in a supermassive black hole's (SMBH) vicinity, coupled with a higher stellar density at the center of a galaxy, make it an ideal location to study the interaction between stars and black holes. Two stars moving near the SMBH could collide at a very high speed, which can result in a high energy flare. The resulting debris can then accrete onto the SMBH, which could be observed as a separate event. We simulate the light curves resulting from the fallback accretion in the aftermath of a stellar collision near a SMBH. We investigate how it varies with physical parameters of the system. With all other physical parameters of the system held constant, the direction of the relative velocity vector at time of impact plays a large role in determining the overall form of the light curve. One distinctive light curve we notice is characterized by a sustained increase in the luminosity some time after accretion has started. We compare this form to the light curves of some candidate tidal disruption events (TDEs). Stellar collision accretion flares can take on unique appearances that would allow them to be easily distinguished, as well as elucidate underlying physical parameters of the system. There exist several ways to distinguish these events from TDEs, including the much wider range of SMBH masses stellar collisions may exist around.","sentences":["The strong tidal force in a supermassive black hole's (SMBH) vicinity, coupled with a higher stellar density at the center of a galaxy, make it an ideal location to study the interaction between stars and black holes.","Two stars moving near the SMBH could collide at a very high speed, which can result in a high energy flare.","The resulting debris can then accrete onto the SMBH, which could be observed as a separate event.","We simulate the light curves resulting from the fallback accretion in the aftermath of a stellar collision near a SMBH.","We investigate how it varies with physical parameters of the system.","With all other physical parameters of the system held constant, the direction of the relative velocity vector at time of impact plays a large role in determining the overall form of the light curve.","One distinctive light curve we notice is characterized by a sustained increase in the luminosity some time after accretion has started.","We compare this form to the light curves of some candidate tidal disruption events (TDEs).","Stellar collision accretion flares can take on unique appearances that would allow them to be easily distinguished, as well as elucidate underlying physical parameters of the system.","There exist several ways to distinguish these events from TDEs, including the much wider range of SMBH masses stellar collisions may exist around."],"url":"http://arxiv.org/abs/2404.07255v1","category":"astro-ph.HE"}
{"created":"2024-04-10 18:00:00","title":"Stability of mixed-state quantum phases via finite Markov length","abstract":"For quantum phases of Hamiltonian ground states, the energy gap plays a central role in ensuring the stability of the phase as long as the gap remains finite. We propose Markov length, the length scale at which the quantum conditional mutual information (CMI) decays exponentially, as an equally essential quantity characterizing mixed-state phases and transitions. For a state evolving under a local Lindbladian, we argue that if its Markov length remains finite along the evolution, then it remains in the same phase, meaning there exists another quasi-local Lindbladian evolution that can reverse the former one. We apply this diagnostic to toric code subject to decoherence and show that the Markov length is finite everywhere except at its decodability transition, at which it diverges. CMI in this case can be mapped to the free energy cost of point defects in the random bond Ising model. This implies that the mixed state phase transition coincides with the decodability transition and also suggests a quasi-local decoding channel.","sentences":["For quantum phases of Hamiltonian ground states, the energy gap plays a central role in ensuring the stability of the phase as long as the gap remains finite.","We propose Markov length, the length scale at which the quantum conditional mutual information (CMI) decays exponentially, as an equally essential quantity characterizing mixed-state phases and transitions.","For a state evolving under a local Lindbladian, we argue that if its Markov length remains finite along the evolution, then it remains in the same phase, meaning there exists another quasi-local Lindbladian evolution that can reverse the former one.","We apply this diagnostic to toric code subject to decoherence and show that the Markov length is finite everywhere except at its decodability transition, at which it diverges.","CMI in this case can be mapped to the free energy cost of point defects in the random bond Ising model.","This implies that the mixed state phase transition coincides with the decodability transition and also suggests a quasi-local decoding channel."],"url":"http://arxiv.org/abs/2404.07251v1","category":"quant-ph"}
{"created":"2024-04-10 18:00:00","title":"Gravity-Induced Photon Interactions and Infrared Consistency in any Dimensions","abstract":"We compute the four-photon ($F^4$) operators generated by loops of charged particles of spin $0$, $\\frac{1}{2}$, $1$ in the presence of gravity and in any spacetime dimension $d$. To this end, we expand the one-loop effective action via the heat kernel coefficients, which provide both the gravity-induced renormalization of the $F^4$ operators and the low-energy Einstein-Maxwell effective field theory (EFT) produced by massive charged particles. Reduction of the operator basis is achieved using that the Gauss-Bonnet combination vanishes at quadratic order in any dimension. A standard infrared consistency argument applies to four-photon scattering in any dimension $d\\geq 3$, setting a positivity bound on the $F^4$ operators. We assume that the graviton $t$-channel pole may be discarded. Surprisingly, the $d=6$ gravity-induced beta functions of $F^4$ operators from charged particles of any spin are positive. This implies that the EFT of massless charged particles is infrared-inconsistent in $d=6$. For massive charged particles in $d$ dimensions, infrared consistency implies a variety of bounds on the charge-to-mass ratio, under the condition that the Planckian $F^4$ operators are sufficiently small or negative. These bounds imply a version of the $d$-dimensional Weak Gravity Conjecture (WGC) in most but not all dimensions. In the special case of $d=6$, the WGC-like bounds are logarithmically enhanced.","sentences":["We compute the four-photon ($F^4$) operators generated by loops of charged particles of spin $0$, $\\frac{1}{2}$, $1$ in the presence of gravity and in any spacetime dimension $d$. To this end, we expand the one-loop effective action via the heat kernel coefficients, which provide both the gravity-induced renormalization of the $F^4$ operators and the low-energy Einstein-Maxwell effective field theory (EFT) produced by massive charged particles.","Reduction of the operator basis is achieved using that the Gauss-Bonnet combination vanishes at quadratic order in any dimension.","A standard infrared consistency argument applies to four-photon scattering in any dimension $d\\geq 3$, setting a positivity bound on the $F^4$ operators.","We assume that the graviton $t$-channel pole may be discarded.","Surprisingly, the $d=6$ gravity-induced beta functions of $F^4$ operators from charged particles of any spin are positive.","This implies that the EFT of massless charged particles is infrared-inconsistent in $d=6$. For massive charged particles in $d$ dimensions, infrared consistency implies a variety of bounds on the charge-to-mass ratio, under the condition that the Planckian $F^4$ operators are sufficiently small or negative.","These bounds imply a version of the $d$-dimensional Weak Gravity Conjecture (WGC) in most but not all dimensions.","In the special case of $d=6$, the WGC-like bounds are logarithmically enhanced."],"url":"http://arxiv.org/abs/2404.07254v1","category":"hep-th"}
{"created":"2024-04-11 15:35:57","title":"Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations","abstract":"In this paper, we develop a new adaptive hyperbolic-cross-space mapped Jacobi (AHMJ) method for solving multidimensional spatiotemporal integrodifferential equations in unbounded domains. By devising adaptive techniques for sparse mapped Jacobi spectral expansions defined in a hyperbolic cross space, our proposed AHMJ method can efficiently solve various spatiotemporal integrodifferential equations such as the anomalous diffusion model with reduced numbers of basis functions. Our analysis of the AHMJ method gives a uniform upper error bound for solving a class of spatiotemporal integrodifferential equations, leading to effective error control.","sentences":["In this paper, we develop a new adaptive hyperbolic-cross-space mapped Jacobi (AHMJ) method for solving multidimensional spatiotemporal integrodifferential equations in unbounded domains.","By devising adaptive techniques for sparse mapped Jacobi spectral expansions defined in a hyperbolic cross space, our proposed AHMJ method can efficiently solve various spatiotemporal integrodifferential equations such as the anomalous diffusion model with reduced numbers of basis functions.","Our analysis of the AHMJ method gives a uniform upper error bound for solving a class of spatiotemporal integrodifferential equations, leading to effective error control."],"url":"http://arxiv.org/abs/2404.07844v1","category":"math.NA"}
{"created":"2024-04-11 12:42:18","title":"Flatness Improves Backbone Generalisation in Few-shot Classification","abstract":"Deployment of deep neural networks in real-world settings typically requires adaptation to new tasks with few examples. Few-shot classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes. Surprisingly, most efforts have only focused on developing architectures for easing the adaptation to the target domain without considering the importance of backbone training for good generalisation. We show that flatness-aware backbone training with vanilla fine-tuning results in a simpler yet competitive baseline compared to the state-of-the-art. Our results indicate that for in- and cross-domain FSC, backbone training is crucial to achieving good generalisation across different adaptation methods. We advocate more care should be taken when training these models.","sentences":["Deployment of deep neural networks in real-world settings typically requires adaptation to new tasks with few examples.","Few-shot classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes.","Surprisingly, most efforts have only focused on developing architectures for easing the adaptation to the target domain without considering the importance of backbone training for good generalisation.","We show that flatness-aware backbone training with vanilla fine-tuning results in a simpler yet competitive baseline compared to the state-of-the-art.","Our results indicate that for in- and cross-domain FSC, backbone training is crucial to achieving good generalisation across different adaptation methods.","We advocate more care should be taken when training these models."],"url":"http://arxiv.org/abs/2404.07696v1","category":"cs.LG"}
{"created":"2024-04-11 09:43:07","title":"GLID: Pre-training a Generalist Encoder-Decoder Vision Model","abstract":"This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method for better handling various downstream computer vision tasks. While self-supervised pre-training approaches, e.g., Masked Autoencoder, have shown success in transfer learning, task-specific sub-architectures are still required to be appended for different downstream tasks, which cannot enjoy the benefits of large-scale pre-training. GLID overcomes this challenge by allowing the pre-trained generalist encoder-decoder to be fine-tuned on various vision tasks with minimal task-specific architecture modifications. In the GLID training scheme, pre-training pretext task and other downstream tasks are modeled as \"query-to-answer\" problems, including the pre-training pretext task and other downstream tasks. We pre-train a task-agnostic encoder-decoder with query-mask pairs. During fine-tuning, GLID maintains the pre-trained encoder-decoder and queries, only replacing the topmost linear transformation layer with task-specific linear heads. This minimizes the pretrain-finetune architecture inconsistency and enables the pre-trained model to better adapt to downstream tasks. GLID achieves competitive performance on various vision tasks, including object detection, image segmentation, pose estimation, and depth estimation, outperforming or matching specialist models such as Mask2Former, DETR, ViTPose, and BinsFormer.","sentences":["This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method for better handling various downstream computer vision tasks.","While self-supervised pre-training approaches, e.g., Masked Autoencoder, have shown success in transfer learning, task-specific sub-architectures are still required to be appended for different downstream tasks, which cannot enjoy the benefits of large-scale pre-training.","GLID overcomes this challenge by allowing the pre-trained generalist encoder-decoder to be fine-tuned on various vision tasks with minimal task-specific architecture modifications.","In the GLID training scheme, pre-training pretext task and other downstream tasks are modeled as \"query-to-answer\" problems, including the pre-training pretext task and other downstream tasks.","We pre-train a task-agnostic encoder-decoder with query-mask pairs.","During fine-tuning, GLID maintains the pre-trained encoder-decoder and queries, only replacing the topmost linear transformation layer with task-specific linear heads.","This minimizes the pretrain-finetune architecture inconsistency and enables the pre-trained model to better adapt to downstream tasks.","GLID achieves competitive performance on various vision tasks, including object detection, image segmentation, pose estimation, and depth estimation, outperforming or matching specialist models such as Mask2Former, DETR, ViTPose, and BinsFormer."],"url":"http://arxiv.org/abs/2404.07603v1","category":"cs.CV"}
{"created":"2024-04-11 09:21:13","title":"Differentiable Rendering as a Way to Program Cable-Driven Soft Robots","abstract":"Soft robots have gained increased popularity in recent years due to their adaptability and compliance. In this paper, we use a digital twin model of cable-driven soft robots to learn control parameters in simulation. In doing so, we take advantage of differentiable rendering as a way to instruct robots to complete tasks such as point reach, gripping an object, and obstacle avoidance. This approach simplifies the mathematical description of such complicated tasks and removes the need for landmark points and their tracking. Our experiments demonstrate the applicability of our method.","sentences":["Soft robots have gained increased popularity in recent years due to their adaptability and compliance.","In this paper, we use a digital twin model of cable-driven soft robots to learn control parameters in simulation.","In doing so, we take advantage of differentiable rendering as a way to instruct robots to complete tasks such as point reach, gripping an object, and obstacle avoidance.","This approach simplifies the mathematical description of such complicated tasks and removes the need for landmark points and their tracking.","Our experiments demonstrate the applicability of our method."],"url":"http://arxiv.org/abs/2404.07590v1","category":"cs.RO"}
{"created":"2024-04-11 08:11:36","title":"Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening","abstract":"Currently, machine learning-based methods for remote sensing pansharpening have progressed rapidly. However, existing pansharpening methods often do not fully exploit differentiating regional information in non-local spaces, thereby limiting the effectiveness of the methods and resulting in redundant learning parameters. In this paper, we introduce a so-called content-adaptive non-local convolution (CANConv), a novel method tailored for remote sensing image pansharpening. Specifically, CANConv employs adaptive convolution, ensuring spatial adaptability, and incorporates non-local self-similarity through the similarity relationship partition (SRP) and the partition-wise adaptive convolution (PWAC) sub-modules. Furthermore, we also propose a corresponding network architecture, called CANNet, which mainly utilizes the multi-scale self-similarity. Extensive experiments demonstrate the superior performance of CANConv, compared with recent promising fusion methods. Besides, we substantiate the method's effectiveness through visualization, ablation experiments, and comparison with existing methods on multiple test sets. The source code is publicly available at https://github.com/duanyll/CANConv.","sentences":["Currently, machine learning-based methods for remote sensing pansharpening have progressed rapidly.","However, existing pansharpening methods often do not fully exploit differentiating regional information in non-local spaces, thereby limiting the effectiveness of the methods and resulting in redundant learning parameters.","In this paper, we introduce a so-called content-adaptive non-local convolution (CANConv), a novel method tailored for remote sensing image pansharpening.","Specifically, CANConv employs adaptive convolution, ensuring spatial adaptability, and incorporates non-local self-similarity through the similarity relationship partition (SRP) and the partition-wise adaptive convolution (PWAC) sub-modules.","Furthermore, we also propose a corresponding network architecture, called CANNet, which mainly utilizes the multi-scale self-similarity.","Extensive experiments demonstrate the superior performance of CANConv, compared with recent promising fusion methods.","Besides, we substantiate the method's effectiveness through visualization, ablation experiments, and comparison with existing methods on multiple test sets.","The source code is publicly available at https://github.com/duanyll/CANConv."],"url":"http://arxiv.org/abs/2404.07543v1","category":"cs.CV"}
{"created":"2024-04-11 07:38:50","title":"Enhancing Policy Gradient with the Polyak Step-Size Adaption","abstract":"Policy gradient is a widely utilized and foundational algorithm in the field of reinforcement learning (RL). Renowned for its convergence guarantees and stability compared to other RL algorithms, its practical application is often hindered by sensitivity to hyper-parameters, particularly the step-size. In this paper, we introduce the integration of the Polyak step-size in RL, which automatically adjusts the step-size without prior knowledge. To adapt this method to RL settings, we address several issues, including unknown f* in the Polyak step-size. Additionally, we showcase the performance of the Polyak step-size in RL through experiments, demonstrating faster convergence and the attainment of more stable policies.","sentences":["Policy gradient is a widely utilized and foundational algorithm in the field of reinforcement learning (RL).","Renowned for its convergence guarantees and stability compared to other RL algorithms, its practical application is often hindered by sensitivity to hyper-parameters, particularly the step-size.","In this paper, we introduce the integration of the Polyak step-size in RL, which automatically adjusts the step-size without prior knowledge.","To adapt this method to RL settings, we address several issues, including unknown f* in the Polyak step-size.","Additionally, we showcase the performance of the Polyak step-size in RL through experiments, demonstrating faster convergence and the attainment of more stable policies."],"url":"http://arxiv.org/abs/2404.07525v1","category":"cs.LG"}
{"created":"2024-04-11 04:22:15","title":"Scalable Language Model with Generalized Continual Learning","abstract":"Continual learning has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models. However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID. In this study, we introduce the Scalable Language Model (SLM) to overcome these limitations within a more challenging and generalized setting, representing a significant advancement toward practical applications for continual learning. Specifically, we propose the Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models based on specific downstream tasks. This approach leverages the task distribution within the vector space, aiming to achieve a smooth and effortless continual learning process. Our method demonstrates state-of-the-art performance on diverse backbones and benchmarks, achieving effective continual learning in both full-set and few-shot scenarios with minimal forgetting. Moreover, while prior research primarily focused on a single task type such as classification, our study goes beyond, with the large language model, i.e., LLaMA-2, to explore the effects across diverse domains and task types, such that a single language model can be decently scaled to broader applications.","sentences":["Continual learning has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models.","However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID.","In this study, we introduce the Scalable Language Model (SLM) to overcome these limitations within a more challenging and generalized setting, representing a significant advancement toward practical applications for continual learning.","Specifically, we propose the Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models based on specific downstream tasks.","This approach leverages the task distribution within the vector space, aiming to achieve a smooth and effortless continual learning process.","Our method demonstrates state-of-the-art performance on diverse backbones and benchmarks, achieving effective continual learning in both full-set and few-shot scenarios with minimal forgetting.","Moreover, while prior research primarily focused on a single task type such as classification, our study goes beyond, with the large language model, i.e., LLaMA-2, to explore the effects across diverse domains and task types, such that a single language model can be decently scaled to broader applications."],"url":"http://arxiv.org/abs/2404.07470v1","category":"cs.CL"}
{"created":"2024-04-11 03:55:45","title":"Generic representations, open parameters and ABV-packets for $p$-adic groups","abstract":"If $\\pi$ is a representation of a $p$-adic group $G(F)$, and $\\phi$ is its Langlands parameter, can we use the moduli space of Langlands parameters to find a geometric property of $\\phi$ that will detect when $\\pi$ is generic? In this paper we show that if $G$ is classical or if we assume the Kazhdan-Lusztig hypothesis for $G$, then the answer is yes, and the property is that the orbit of $\\phi$ is open. We also propose an adaptation of Shahidi's enhanced genericity conjecture to ABV-packets: for every Langlands parameter $\\phi$ for a $p$-adic group $G(F)$, the ABV-packet $\\Pi^{\\mathrm{ABV}}_\\phi(G(F))$ contains a generic representation if and only if the local adjoint L-function $L(s,\\phi,\\mathop{\\text{Ad}})$ is regular at $s=1$, and show that this condition is equivalent to the \"open parameter\" condition above. We show that this genericity conjecture for ABV-packets follows from other standard conjectures and we verify its validity with the same conditions on $G$. We show that, in this case, the ABV-packet for $\\phi$ coincides with its $L$-packet. Finally, we prove Vogan's conjecture on $A$-packets for tempered parameters.","sentences":["If $\\pi$ is a representation of a $p$-adic group $G(F)$, and $\\phi$ is its Langlands parameter, can we use the moduli space of Langlands parameters to find a geometric property of $\\phi$ that will detect when $\\pi$ is generic?","In this paper we show that if $G$ is classical or if we assume the Kazhdan-Lusztig hypothesis for $G$, then the answer is yes, and the property is that the orbit of $\\phi$ is open.","We also propose an adaptation of Shahidi's enhanced genericity conjecture to ABV-packets: for every Langlands parameter $\\phi$ for a $p$-adic group $G(F)$, the ABV-packet $\\Pi^{\\mathrm{ABV}}_\\phi(G(F))$ contains a generic representation if and only if the local adjoint L-function $L(s,\\phi,\\mathop{\\text{Ad}})$ is regular at $s=1$, and show that this condition is equivalent to the \"open parameter\" condition above.","We show that this genericity conjecture for ABV-packets follows from other standard conjectures and we verify its validity with the same conditions on $G$. We show that, in this case, the ABV-packet for $\\phi$ coincides with its $L$-packet.","Finally, we prove Vogan's conjecture on $A$-packets for tempered parameters."],"url":"http://arxiv.org/abs/2404.07463v1","category":"math.RT"}
{"created":"2024-04-11 03:55:38","title":"ACRONYM: Augmented degree corrected, Community Reticulated Organized Network Yielding Model","abstract":"Modeling networks can serve as a means of summarizing high-dimensional complex systems. Adapting an approach devised for dense, weighted networks, we propose a new method for generating and estimating unweighted networks. This approach can describe a broader class of potential networks than existing models, including those where nodes in different subnetworks connect to one another via various attachment mechanisms, inducing flexible and varied community structures. While unweighted edges provide less resolution than continuous weights, restricting to the binary case permits the use of likelihood-based estimation techniques, which can improve estimation of nodal features. The extra flexibility may contribute a different understanding of network generating structures, particularly for networks with heterogeneous densities in different regions.","sentences":["Modeling networks can serve as a means of summarizing high-dimensional complex systems.","Adapting an approach devised for dense, weighted networks, we propose a new method for generating and estimating unweighted networks.","This approach can describe a broader class of potential networks than existing models, including those where nodes in different subnetworks connect to one another via various attachment mechanisms, inducing flexible and varied community structures.","While unweighted edges provide less resolution than continuous weights, restricting to the binary case permits the use of likelihood-based estimation techniques, which can improve estimation of nodal features.","The extra flexibility may contribute a different understanding of network generating structures, particularly for networks with heterogeneous densities in different regions."],"url":"http://arxiv.org/abs/2404.07462v1","category":"physics.soc-ph"}
{"created":"2024-04-11 03:04:59","title":"Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments","abstract":"This paper introduces a real-time algorithm for navigating complex unknown environments cluttered with movable obstacles. Our algorithm achieves fast, adaptable routing by actively attempting to manipulate obstacles during path planning and adjusting the global plan from sensor feedback. The main contributions include an improved dynamic Directed Visibility Graph (DV-graph) for rapid global path searching, a real-time interaction planning method that adapts online from new sensory perceptions, and a comprehensive framework designed for interactive navigation in complex unknown or partially known environments. Our algorithm is capable of replanning the global path in several milliseconds. It can also attempt to move obstacles, update their affordances, and adapt strategies accordingly. Extensive experiments validate that our algorithm reduces the travel time by 33%, achieves up to 49% higher path efficiency, and runs faster than traditional methods by orders of magnitude in complex environments. It has been demonstrated to be the most efficient solution in terms of speed and efficiency for interactive navigation in environments of such complexity. We also open-source our code in the docker demo to facilitate future research.","sentences":["This paper introduces a real-time algorithm for navigating complex unknown environments cluttered with movable obstacles.","Our algorithm achieves fast, adaptable routing by actively attempting to manipulate obstacles during path planning and adjusting the global plan from sensor feedback.","The main contributions include an improved dynamic Directed Visibility Graph (DV-graph) for rapid global path searching, a real-time interaction planning method that adapts online from new sensory perceptions, and a comprehensive framework designed for interactive navigation in complex unknown or partially known environments.","Our algorithm is capable of replanning the global path in several milliseconds.","It can also attempt to move obstacles, update their affordances, and adapt strategies accordingly.","Extensive experiments validate that our algorithm reduces the travel time by 33%, achieves up to 49% higher path efficiency, and runs faster than traditional methods by orders of magnitude in complex environments.","It has been demonstrated to be the most efficient solution in terms of speed and efficiency for interactive navigation in environments of such complexity.","We also open-source our code in the docker demo to facilitate future research."],"url":"http://arxiv.org/abs/2404.07447v1","category":"cs.RO"}
{"created":"2024-04-11 02:13:27","title":"Parameterized Fast and Safe Tracking (FaSTrack) using Deepreach","abstract":"Fast and Safe Tracking (FaSTrack) is a modular framework that provides safety guarantees while planning and executing trajectories in real time via value functions of Hamilton-Jacobi (HJ) reachability. These value functions are computed through dynamic programming, which is notorious for being computationally inefficient. Moreover, the resulting trajectory does not adapt online to the environment, such as sudden disturbances or obstacles. DeepReach is a scalable deep learning method to HJ reachability that allows parameterization of states, which opens up possibilities for online adaptation to various controls and disturbances. In this paper, we propose Parametric FaSTrack, which uses DeepReach to approximate a value function that parameterizes the control bounds of the planning model. The new framework can smoothly trade off between the navigation speed and the tracking error (therefore maneuverability) while guaranteeing obstacle avoidance in a priori unknown environments. We demonstrate our method through two examples and a benchmark comparison with existing methods, showing the safety, efficiency, and faster solution times of the framework.","sentences":["Fast and Safe Tracking (FaSTrack) is a modular framework that provides safety guarantees while planning and executing trajectories in real time via value functions of Hamilton-Jacobi (HJ) reachability.","These value functions are computed through dynamic programming, which is notorious for being computationally inefficient.","Moreover, the resulting trajectory does not adapt online to the environment, such as sudden disturbances or obstacles.","DeepReach is a scalable deep learning method to HJ reachability that allows parameterization of states, which opens up possibilities for online adaptation to various controls and disturbances.","In this paper, we propose Parametric FaSTrack, which uses DeepReach to approximate a value function that parameterizes the control bounds of the planning model.","The new framework can smoothly trade off between the navigation speed and the tracking error (therefore maneuverability) while guaranteeing obstacle avoidance in a priori unknown environments.","We demonstrate our method through two examples and a benchmark comparison with existing methods, showing the safety, efficiency, and faster solution times of the framework."],"url":"http://arxiv.org/abs/2404.07431v1","category":"cs.RO"}
{"created":"2024-04-10 23:52:04","title":"Synthetic Spectra from Particle-in-cell Simulations of Relativistic Jets containing an initial Toroidal Magnetic Field","abstract":"The properties of relativistic jets, their interaction with the environment, and their emission of radiation can be self-consistently studied by using particle-in-cell (PIC) numerical simulations. Using three-dimensional (3D), relativistic PIC simulations, we present the first self-consistently calculated synthetic spectra of head-on and off-axis emission from electrons accelerated in cylindrical, relativistic plasma jets containing an initial toroidal magnetic field. The jet particles are initially accelerated during the linear stage of growing plasma instabilities, which are the Weibel instability (WI), kinetic Kelvin-Helmholtz instability (kKHI), and mushroom instability (MI). In the nonlinear stage, these instabilities are dissipated and generate turbulent magnetic fields which accelerate particles further. We calculate the synthetic spectra by tracing a large number of jet electrons in the nonlinear stage, near the jet head where the magnetic fields are turbulent. Our results show the basic properties of jitter-like radiation emitted by relativistic electrons when they travel through a magnetized plasma with the plasma waves driven by kinetic instabilities (WI, kKHI, and MI) growing into the nonlinear regime. At low frequencies, the slope of the spectrum is ~ 0.94, which is similar to that of the jitter radiation. The results are relevant to active galactic nuclei/blazars and gamma-ray burst jet emission and set the ground for future studies on synthetic spectra from relativistic jets.","sentences":["The properties of relativistic jets, their interaction with the environment, and their emission of radiation can be self-consistently studied by using particle-in-cell (PIC) numerical simulations.","Using three-dimensional (3D), relativistic PIC simulations, we present the first self-consistently calculated synthetic spectra of head-on and off-axis emission from electrons accelerated in cylindrical, relativistic plasma jets containing an initial toroidal magnetic field.","The jet particles are initially accelerated during the linear stage of growing plasma instabilities, which are the Weibel instability (WI), kinetic Kelvin-Helmholtz instability (kKHI), and mushroom instability (MI).","In the nonlinear stage, these instabilities are dissipated and generate turbulent magnetic fields which accelerate particles further.","We calculate the synthetic spectra by tracing a large number of jet electrons in the nonlinear stage, near the jet head where the magnetic fields are turbulent.","Our results show the basic properties of jitter-like radiation emitted by relativistic electrons when they travel through a magnetized plasma with the plasma waves driven by kinetic instabilities (WI, kKHI, and MI) growing into the nonlinear regime.","At low frequencies, the slope of the spectrum is ~ 0.94, which is similar to that of the jitter radiation.","The results are relevant to active galactic nuclei/blazars and gamma-ray burst jet emission and set the ground for future studies on synthetic spectra from relativistic jets."],"url":"http://arxiv.org/abs/2404.07392v1","category":"astro-ph.HE"}
{"created":"2024-04-10 20:36:32","title":"Doomed Worlds I: No new evidence for orbital decay in a long-term survey of 43 ultra-hot Jupiters","abstract":"Ultra-hot Jupiters are likely doomed by tidal forces to undergo orbital decay and eventual disruption by their stars, but the timescale over which this process unfolds is unknown. We present results from a long-term project to monitor ultra-hot Jupiters transits. We recovered WASP-12 b's orbital decay rate of dP/dt = -29.8 +/- 1.6 ms yr-1, in agreement with prior work. Five other systems initially had promising non-linear transit ephemerides. However, a closer examination of two -- WASP-19 b and CoRoT-2 b, both with prior tentative detections -- revealed several independent errors with the literature timing data; after correction neither planet shows signs of orbital decay. Meanwhile, a potential decreasing period for TrES-1 b, dP/dt = -16 +/- 5 ms yr-1, corresponds to a tidal quality factor Q*' = 160 and likely does not result from orbital decay, if driven by dissipation within the host star. Nominal period increases in two systems, WASP-121 b and WASP-46 b, rest on a small handful of points. Only 1/43 planets (WASP-12 b) in our sample is experiencing detectable orbital decay. For nearly half (20/42) we can rule out dP/dt as high as observed for WASP-12 b. Thus while many ultra-hot Jupiters could still be experiencing rapid decay that we cannot yet detect, a sizeable sub-population of UHJs are decaying at least an order of magnitude more slowly than WASP-12 b. Our reanalysis of Kepler-1658 b with no new data finds that it remains a promising orbital decay candidate. Finally, we recommend that the scientific community take steps to avoid spurious detections through better management of the multi-decade-spanning datasets needed to search for and study planetary orbital decay.","sentences":["Ultra-hot Jupiters are likely doomed by tidal forces to undergo orbital decay and eventual disruption by their stars, but the timescale over which this process unfolds is unknown.","We present results from a long-term project to monitor ultra-hot Jupiters transits.","We recovered WASP-12 b's orbital decay rate of dP/dt =","-29.8 +/- 1.6 ms yr-1, in agreement with prior work.","Five other systems initially had promising non-linear transit ephemerides.","However, a closer examination of two -- WASP-19 b and CoRoT-2 b, both with prior tentative detections -- revealed several independent errors with the literature timing data; after correction neither planet shows signs of orbital decay.","Meanwhile, a potential decreasing period for TrES-1 b, dP/dt = -16","+/- 5 ms yr-1, corresponds to a tidal quality factor Q*' = 160 and likely does not result from orbital decay, if driven by dissipation within the host star.","Nominal period increases in two systems, WASP-121 b","and WASP-46 b, rest on a small handful of points.","Only 1/43 planets (WASP-12 b) in our sample is experiencing detectable orbital decay.","For nearly half (20/42) we can rule out dP/dt as high as observed for WASP-12 b.","Thus while many ultra-hot Jupiters could still be experiencing rapid decay that we cannot yet detect, a sizeable sub-population of UHJs are decaying at least an order of magnitude more slowly than WASP-12 b.","Our reanalysis of Kepler-1658 b with no new data finds that it remains a promising orbital decay candidate.","Finally, we recommend that the scientific community take steps to avoid spurious detections through better management of the multi-decade-spanning datasets needed to search for and study planetary orbital decay."],"url":"http://arxiv.org/abs/2404.07339v1","category":"astro-ph.EP"}
{"created":"2024-04-10 20:16:46","title":"Experimental and numerical investigation of inertial particles in underexpanded jets","abstract":"Experiments and numerical simulations of inertial particles in underexpanded jets are performed. The structure of the jet is controlled by varying the nozzle pressure ratio, while the influence of particles on emerging shocks and rarefaction patterns is controlled by varying the particle size and mass loading. Ultra-high-speed schlieren and Lagrangian particle tracking are used to experimentally determine the two-phase flow quantities. Three-dimensional simulations are performed using a high-order, low dissipative discretization of the gas phase while particles are tracked individually in a Lagrangian manner. A simple two-way coupling strategy is proposed to handle interphase exchange in the vicinity of shocks. Velocity statistics of each phase are reported for a wide range of pressure ratios, particle sizes, and volume fractions. The extent to which particles affect the location of the Mach disk are quantified and compared to previous work from the literature. Furthermore, a semi-analytic model is presented based on a one-dimensional Fanno flow that takes into account volume displacement by particles and interphase exchange due to drag and heat transfer. The percent shift in Mach disk is found to scale with the mass loading, nozzle pressure ratio, interphase slip velocity, and inversely with the particle diameter.","sentences":["Experiments and numerical simulations of inertial particles in underexpanded jets are performed.","The structure of the jet is controlled by varying the nozzle pressure ratio, while the influence of particles on emerging shocks and rarefaction patterns is controlled by varying the particle size and mass loading.","Ultra-high-speed schlieren and Lagrangian particle tracking are used to experimentally determine the two-phase flow quantities.","Three-dimensional simulations are performed using a high-order, low dissipative discretization of the gas phase while particles are tracked individually in a Lagrangian manner.","A simple two-way coupling strategy is proposed to handle interphase exchange in the vicinity of shocks.","Velocity statistics of each phase are reported for a wide range of pressure ratios, particle sizes, and volume fractions.","The extent to which particles affect the location of the Mach disk are quantified and compared to previous work from the literature.","Furthermore, a semi-analytic model is presented based on a one-dimensional Fanno flow that takes into account volume displacement by particles and interphase exchange due to drag and heat transfer.","The percent shift in Mach disk is found to scale with the mass loading, nozzle pressure ratio, interphase slip velocity, and inversely with the particle diameter."],"url":"http://arxiv.org/abs/2404.07329v1","category":"physics.flu-dyn"}
{"created":"2024-04-10 19:23:07","title":"Effects of dynamical friction on perturbations for evolving dark energy","abstract":"We explore the impact of dynamical friction on cosmological scales and show its influence on the evolution of perturbations. In particular, considering smooth and clustering dark energy models, we describe the role played by friction by selecting two main hierarchical models, \\textit{i.e.}, the first where the friction term is proportional to the Hubble rate, whereas the second where friction is induced by the dark energy pressure. The second approach generalises the first and translates the idea that pressure is a general relativistic effect, motivating why friction might arise once barotropic dark energy fluids are considered. The corresponding effects of friction are investigated at the level of linear and nonlinear perturbations, using the formalism of the spherical collapse model. Whilst dynamical friction has very small effects and thus it cannot be excluded \\emph{a priori}, dissipative pressure friction leads to a substantial slow down in the evolution of perturbations. This can be particularly inferred within the halo mass function, for which we also employ corrections due to dark energy clustering. To this end, in order to discern detectable deviations from the standard cosmological model, we thus highlight where dissipation effects might play a significant role at large scales.","sentences":["We explore the impact of dynamical friction on cosmological scales and show its influence on the evolution of perturbations.","In particular, considering smooth and clustering dark energy models, we describe the role played by friction by selecting two main hierarchical models, \\textit{i.e.}, the first where the friction term is proportional to the Hubble rate, whereas the second where friction is induced by the dark energy pressure.","The second approach generalises the first and translates the idea that pressure is a general relativistic effect, motivating why friction might arise once barotropic dark energy fluids are considered.","The corresponding effects of friction are investigated at the level of linear and nonlinear perturbations, using the formalism of the spherical collapse model.","Whilst dynamical friction has very small effects and thus it cannot be excluded \\emph{a priori}, dissipative pressure friction leads to a substantial slow down in the evolution of perturbations.","This can be particularly inferred within the halo mass function, for which we also employ corrections due to dark energy clustering.","To this end, in order to discern detectable deviations from the standard cosmological model, we thus highlight where dissipation effects might play a significant role at large scales."],"url":"http://arxiv.org/abs/2404.07313v1","category":"astro-ph.CO"}
{"created":"2024-04-09 18:29:42","title":"Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs","abstract":"Large Language Models (LLMs) are increasingly being developed and applied, but their widespread use faces challenges. These include aligning LLMs' responses with human values to prevent harmful outputs, which is addressed through safety training methods. Even so, bad actors and malicious users have succeeded in attempts to manipulate the LLMs to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights. Another challenge is the multilingual capabilities of LLMs, which enable the model to understand and respond in multiple languages. Consequently, attackers exploit the unbalanced pre-training datasets of LLMs in different languages and the comparatively lower model performance in low-resource languages than high-resource ones. As a result, attackers use a low-resource languages to intentionally manipulate the model to create harmful responses. Many of the similar attack vectors have been patched by model providers, making the LLMs more robust against language-based manipulation. In this paper, we introduce a new black-box attack vector called the \\emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses. Our experiments with five different models, namely Google's Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse.","sentences":["Large Language Models (LLMs) are increasingly being developed and applied, but their widespread use faces challenges.","These include aligning LLMs' responses with human values to prevent harmful outputs, which is addressed through safety training methods.","Even so, bad actors and malicious users have succeeded in attempts to manipulate the LLMs to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights.","Another challenge is the multilingual capabilities of LLMs, which enable the model to understand and respond in multiple languages.","Consequently, attackers exploit the unbalanced pre-training datasets of LLMs in different languages and the comparatively lower model performance in low-resource languages than high-resource ones.","As a result, attackers use a low-resource languages to intentionally manipulate the model to create harmful responses.","Many of the similar attack vectors have been patched by model providers, making the LLMs more robust against language-based manipulation.","In this paper, we introduce a new black-box attack vector called the \\emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses.","Our experiments with five different models, namely Google's Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models.","By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse."],"url":"http://arxiv.org/abs/2404.07242v1","category":"cs.CR"}
{"created":"2024-04-11 17:59:59","title":"Connecting NeRFs, Images, and Text","abstract":"Neural Radiance Fields (NeRFs) have emerged as a standard framework for representing 3D scenes and objects, introducing a novel data type for information exchange and storage. Concurrently, significant progress has been made in multimodal representation learning for text and image data. This paper explores a novel research direction that aims to connect the NeRF modality with other modalities, similar to established methodologies for images and text. To this end, we propose a simple framework that exploits pre-trained models for NeRF representations alongside multimodal models for text and image processing. Our framework learns a bidirectional mapping between NeRF embeddings and those obtained from corresponding images and text. This mapping unlocks several novel and useful applications, including NeRF zero-shot classification and NeRF retrieval from images or text.","sentences":["Neural Radiance Fields (NeRFs) have emerged as a standard framework for representing 3D scenes and objects, introducing a novel data type for information exchange and storage.","Concurrently, significant progress has been made in multimodal representation learning for text and image data.","This paper explores a novel research direction that aims to connect the NeRF modality with other modalities, similar to established methodologies for images and text.","To this end, we propose a simple framework that exploits pre-trained models for NeRF representations alongside multimodal models for text and image processing.","Our framework learns a bidirectional mapping between NeRF embeddings and those obtained from corresponding images and text.","This mapping unlocks several novel and useful applications, including NeRF zero-shot classification and NeRF retrieval from images or text."],"url":"http://arxiv.org/abs/2404.07993v1","category":"cs.CV"}
{"created":"2024-04-11 17:43:50","title":"Existence of Optimal Stationary Singular Controls and Mean Field Game Equilibria","abstract":"In this paper, we examine the stationary relaxed singular control problem within a multi-dimensional framework for a single agent, as well as its Mean Field Game (MFG) equivalent. We demonstrate that optimal relaxed controls exist for both maximization and minimization cases. These relaxed controls are defined by random measures across the state and control spaces, with the state process described as a solution to the associated martingale problem. By leveraging findings from [Kurtz-Stockbridge 2001], we establish the equivalence between the martingale problem and the stationary forward equation. This allows us to reformulate the relaxed control problem into a linear programming problem within the measure space. We prove the sequential compactness of these measures, thereby confirming the feasibility of achieving an optimal solution. Subsequently, our focus shifts to Mean Field Games. Drawing on insights from the single-agent problem and employing Kakutani--Glicksberg--Fan fixed point theorem, we derive the existence of a mean field game equilibria.","sentences":["In this paper, we examine the stationary relaxed singular control problem within a multi-dimensional framework for a single agent, as well as its Mean Field Game (MFG) equivalent.","We demonstrate that optimal relaxed controls exist for both maximization and minimization cases.","These relaxed controls are defined by random measures across the state and control spaces, with the state process described as a solution to the associated martingale problem.","By leveraging findings from [Kurtz-Stockbridge 2001], we establish the equivalence between the martingale problem and the stationary forward equation.","This allows us to reformulate the relaxed control problem into a linear programming problem within the measure space.","We prove the sequential compactness of these measures, thereby confirming the feasibility of achieving an optimal solution.","Subsequently, our focus shifts to Mean Field Games.","Drawing on insights from the single-agent problem and employing Kakutani--Glicksberg--Fan fixed point theorem, we derive the existence of a mean field game equilibria."],"url":"http://arxiv.org/abs/2404.07945v1","category":"math.OC"}
{"created":"2024-04-11 17:43:17","title":"On the Durdevic approach to quantum principal bundles","abstract":"We revisit and extend the Durdevic theory of complete calculi on quantum principal bundles. In this setting one naturally obtains a graded Hopf-Galois extension of the higher order calculus and an intrinsic decomposition of degree 1-forms into horizontal and vertical forms. This proposal is appealing, since it is consistently equipped with a canonical braiding and exactness of the Atiyah sequence is guaranteed. Moreover, we provide examples of complete calculi, including the noncommutative 2-torus, the quantum Hopf fibration and differential calculi on crossed product algebras.","sentences":["We revisit and extend the Durdevic theory of complete calculi on quantum principal bundles.","In this setting one naturally obtains a graded Hopf-Galois extension of the higher order calculus and an intrinsic decomposition of degree 1-forms into horizontal and vertical forms.","This proposal is appealing, since it is consistently equipped with a canonical braiding and exactness of the Atiyah sequence is guaranteed.","Moreover, we provide examples of complete calculi, including the noncommutative 2-torus, the quantum Hopf fibration and differential calculi on crossed product algebras."],"url":"http://arxiv.org/abs/2404.07944v1","category":"math.QA"}
{"created":"2024-04-11 17:30:24","title":"Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation","abstract":"Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions.","sentences":["Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision.","While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions.","With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields.","Unlike explicit approaches.","e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering.","In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction.","To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data.","Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS.","It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions."],"url":"http://arxiv.org/abs/2404.07933v1","category":"cs.CV"}
{"created":"2024-04-11 17:29:27","title":"Representability of Elliptic Moduli Problems in Derived $C^{\\infty}$-Geometry","abstract":"We study moduli spaces of solutions of nonlinear Partial Differential Equations on manifolds in the framework of derived $C^{\\infty}$-geometry. For an arbitrary smooth stack $S$, we define $S$-families of nonlinear PDEs acting between $S$-families of submersions over an $S$-family of manifolds and show that in case the family of PDEs is elliptic and the base family of manifolds is proper over $S$, then the moduli stack of solutions is relatively representable by quasi-smooth derived $C^{\\infty}$-schemes over $S$. Along the way, we develop tools to analyse the local structure of families of mapping stacks between manifolds and explain how to compare mapping stacks in smooth and in derived geometry. To access the notion of a family of PDEs over an arbitrary smooth base stack, we introduce a formalism of stacks of relative jets. Finally, we show how natural ideas from (higher) topos theory can be leveraged to facilitate the application of nonlinear Fredholm analysis to derived stacks of solutions of elliptic PDEs.","sentences":["We study moduli spaces of solutions of nonlinear Partial Differential Equations on manifolds in the framework of derived $C^{\\infty}$-geometry.","For an arbitrary smooth stack $S$, we define $S$-families of nonlinear PDEs acting between $S$-families of submersions over an $S$-family of manifolds and show that in case the family of PDEs is elliptic and the base family of manifolds is proper over $S$, then the moduli stack of solutions is relatively representable by quasi-smooth derived $C^{\\infty}$-schemes over $S$. Along the way, we develop tools to analyse the local structure of families of mapping stacks between manifolds and explain how to compare mapping stacks in smooth and in derived geometry.","To access the notion of a family of PDEs over an arbitrary smooth base stack, we introduce a formalism of stacks of relative jets.","Finally, we show how natural ideas from (higher) topos theory can be leveraged to facilitate the application of nonlinear Fredholm analysis to derived stacks of solutions of elliptic PDEs."],"url":"http://arxiv.org/abs/2404.07931v1","category":"math.AG"}
{"created":"2024-04-11 17:10:57","title":"A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM","abstract":"Significant strides have been made in advancing streamflow predictions, notably with the introduction of cutting-edge machine-learning models. Predominantly, Long Short-Term Memories (LSTMs) and Convolution Neural Networks (CNNs) have been widely employed in this domain. While LSTMs are applicable in both rainfall-runoff and time series settings, CNN-LSTMs have primarily been utilized in rainfall-runoff scenarios. In this study, we extend the application of CNN-LSTMs to time series settings, leveraging lagged streamflow data in conjunction with precipitation and temperature data to predict streamflow. Our results show a substantial improvement in predictive performance in 21 out of 32 HUC8 basins in Nebraska, showcasing noteworthy increases in the Kling-Gupta Efficiency (KGE) values. These results highlight the effectiveness of CNN-LSTMs in time series settings, particularly for spatiotemporal hydrological modeling, for more accurate and robust streamflow predictions.","sentences":["Significant strides have been made in advancing streamflow predictions, notably with the introduction of cutting-edge machine-learning models.","Predominantly, Long Short-Term Memories (LSTMs) and Convolution Neural Networks (CNNs) have been widely employed in this domain.","While LSTMs are applicable in both rainfall-runoff and time series settings, CNN-LSTMs have primarily been utilized in rainfall-runoff scenarios.","In this study, we extend the application of CNN-LSTMs to time series settings, leveraging lagged streamflow data in conjunction with precipitation and temperature data to predict streamflow.","Our results show a substantial improvement in predictive performance in 21 out of 32 HUC8 basins in Nebraska, showcasing noteworthy increases in the Kling-Gupta Efficiency (KGE) values.","These results highlight the effectiveness of CNN-LSTMs in time series settings, particularly for spatiotemporal hydrological modeling, for more accurate and robust streamflow predictions."],"url":"http://arxiv.org/abs/2404.07924v1","category":"cs.LG"}
{"created":"2024-04-11 16:58:24","title":"Convergence, divergence, and inherent oscillations in MFS solutions of two-dimensional Laplace-Neumann problems","abstract":"The method of fundamental solutions (MFS), also known as the method of auxiliary sources (MAS), is a well-known computational method for the solution of boundary-value problems. The final solution (\"MAS solution\") is obtained once we have found the amplitudes of $N$ auxiliary \"MAS sources.\" Past studies have demonstrated that it is possible for the MAS solution to converge to the true solution even when the $N$ auxiliary sources diverge and oscillate. The present paper extends the past studies by demonstrating this possibility within the context of Laplace's equation with Neumann boundary conditions. One can thus obtain the correct solution from sources that, when $N$ is large, must be considered unphysical. We carefully explain the underlying reasons for the unphysical results, distinguish from other difficulties that might concurrently arise, and point to significant differences with time-dependent problems that were studied in the past.","sentences":["The method of fundamental solutions (MFS), also known as the method of auxiliary sources (MAS), is a well-known computational method for the solution of boundary-value problems.","The final solution (\"MAS solution\") is obtained once we have found the amplitudes of $N$ auxiliary \"MAS sources.\"","Past studies have demonstrated that it is possible for the MAS solution to converge to the true solution even when the $N$ auxiliary sources diverge and oscillate.","The present paper extends the past studies by demonstrating this possibility within the context of Laplace's equation with Neumann boundary conditions.","One can thus obtain the correct solution from sources that, when $N$ is large, must be considered unphysical.","We carefully explain the underlying reasons for the unphysical results, distinguish from other difficulties that might concurrently arise, and point to significant differences with time-dependent problems that were studied in the past."],"url":"http://arxiv.org/abs/2404.07914v1","category":"math.NA"}
{"created":"2024-04-11 14:56:17","title":"Illposedness of incompressible fluids in supercritical Sobolev spaces","abstract":"We prove that the 3D Euler and Navier-Stokes equations are strongly illposed in supercritical Sobolev spaces. In the inviscid case, for any $0<s< \\frac{5}{2} $, we construct a $C^\\infty_c$ initial velocity field with arbitrarily small $H^{s} $ norm for which the unique local-in-time smooth solution of the 3D Euler equation develops large $\\dot{H}^{s}$ norm inflation almost instantaneously. In the viscous case, the same $\\dot{H}^{s}$ norm inflation occurs in the 3D Navier-Stokes equations for $0<s< \\frac{1}{2} $, where $s = \\frac{1}{2}$ is scaling critical for this equation.","sentences":["We prove that the 3D Euler and Navier-Stokes equations are strongly illposed in supercritical Sobolev spaces.","In the inviscid case, for any $0<s< \\frac{5}{2} $, we construct a $C^\\infty_c$ initial velocity field with arbitrarily small $H^{s} $ norm for which the unique local-in-time smooth solution of the 3D Euler equation develops large $\\dot{H}^{s}$ norm inflation almost instantaneously.","In the viscous case, the same $\\dot{H}^{s}$ norm inflation occurs in the 3D Navier-Stokes equations for $0<s< \\frac{1}{2} $, where $s = \\frac{1}{2}$ is scaling critical for this equation."],"url":"http://arxiv.org/abs/2404.07813v1","category":"math.AP"}
{"created":"2024-04-11 14:50:37","title":"Probing Three-Dimensional Magnetic Fields: III -- Synchrotron Emission and Machine Learning","abstract":"Synchrotron observation serves as a fundamental tool for studying magnetic fields in various astrophysical settings, yet its ability to unveil three-dimensional (3D) magnetic fields-including plane-of-the-sky orientation, inclination angle relative to the line of sight, and magnetization-remains largely underexplored. Inspired by the latest insights into anisotropic magnetohydrodynamic (MHD) turbulence, we found that synchrotron emission's intensity structures inherently reflect this anisotropy, carrying detailed information about 3D magnetic fields. Capitalizing on this foundation, we integrate a machine learning approach-Convolutional Neural Network (CNN)-to extract this latent information, thereby facilitating the exploration of 3D magnetic fields. The model is trained on synthetic synchrotron emission maps, derived from 3D MHD turbulence simulations encompassing a range of sub-Alfv\\'enic to super-Alfv\\'enic conditions. We show that the CNN model is physically interpretable and the CNN is capable of reconstructing 3D magnetic field topology and assessing magnetization. In addition, we test our methodology against noise and resolution effects. We show that this CNN-based approach maintains a high degree of robustness in tracing 3D magnetic fields, even when the low spatial frequencies of the synchrotron image are absent. This renders the method particularly suitable for application to interferometric data lacking single-dish measurements.","sentences":["Synchrotron observation serves as a fundamental tool for studying magnetic fields in various astrophysical settings, yet its ability to unveil three-dimensional (3D) magnetic fields-including plane-of-the-sky orientation, inclination angle relative to the line of sight, and magnetization-remains largely underexplored.","Inspired by the latest insights into anisotropic magnetohydrodynamic (MHD) turbulence, we found that synchrotron emission's intensity structures inherently reflect this anisotropy, carrying detailed information about 3D magnetic fields.","Capitalizing on this foundation, we integrate a machine learning approach-Convolutional Neural Network (CNN)-to extract this latent information, thereby facilitating the exploration of 3D magnetic fields.","The model is trained on synthetic synchrotron emission maps, derived from 3D MHD turbulence simulations encompassing a range of sub-Alfv\\'enic to super-Alfv\\'enic conditions.","We show that the CNN model is physically interpretable and the CNN is capable of reconstructing 3D magnetic field topology and assessing magnetization.","In addition, we test our methodology against noise and resolution effects.","We show that this CNN-based approach maintains a high degree of robustness in tracing 3D magnetic fields, even when the low spatial frequencies of the synchrotron image are absent.","This renders the method particularly suitable for application to interferometric data lacking single-dish measurements."],"url":"http://arxiv.org/abs/2404.07806v1","category":"astro-ph.GA"}
{"created":"2024-04-11 14:50:11","title":"Tensor Neural Network Interpolation and Its Applications","abstract":"Based on tensor neural network, we propose an interpolation method for high dimensional non-tensor-product-type functions. This interpolation scheme is designed by using the tensor neural network based machine learning method. This means that we use a tensor neural network to approximate high dimensional functions which has no tensor product structure. In some sense, the non-tenor-product-type high dimensional function is transformed to the tensor neural network which has tensor product structure. It is well known that the tensor product structure can bring the possibility to design highly accurate and efficient numerical methods for dealing with high dimensional functions. In this paper, we will concentrate on computing the high dimensional integrations and solving high dimensional partial differential equations. The corresponding numerical methods and numerical examples will be provided to validate the proposed tensor neural network interpolation.","sentences":["Based on tensor neural network, we propose an interpolation method for high dimensional non-tensor-product-type functions.","This interpolation scheme is designed by using the tensor neural network based machine learning method.","This means that we use a tensor neural network to approximate high dimensional functions which has no tensor product structure.","In some sense, the non-tenor-product-type high dimensional function is transformed to the tensor neural network which has tensor product structure.","It is well known that the tensor product structure can bring the possibility to design highly accurate and efficient numerical methods for dealing with high dimensional functions.","In this paper, we will concentrate on computing the high dimensional integrations and solving high dimensional partial differential equations.","The corresponding numerical methods and numerical examples will be provided to validate the proposed tensor neural network interpolation."],"url":"http://arxiv.org/abs/2404.07805v1","category":"math.NA"}
{"created":"2024-04-11 12:49:39","title":"Evaluating matrix power series with the Cayley-Hamilton theorem","abstract":"The Cayley-Hamilton theorem is used to implement an iterative process for the efficient numerical computation of matrix power series and their differentials. In addition to straight-forward applications in lattice gauge theory simulations e.g. to reduce the computational cost of smearing, the method can also be used to simplify the evaluation of SU(N) one-link integrals or the computation of SU(N) matrix logarithms.","sentences":["The Cayley-Hamilton theorem is used to implement an iterative process for the efficient numerical computation of matrix power series and their differentials.","In addition to straight-forward applications in lattice gauge theory simulations e.g. to reduce the computational cost of smearing, the method can also be used to simplify the evaluation of SU(N) one-link integrals or the computation of SU(N) matrix logarithms."],"url":"http://arxiv.org/abs/2404.07704v1","category":"hep-lat"}
{"created":"2024-04-11 12:18:16","title":"Haldane graphene billiards versus relativistic neutrino billiards","abstract":"We study fluctuation properties in the energy spectra of finite-size honeycomb lattices, graphene billiards, subject to the Haldane-model onsite potential and next-nearest neighbor interaction at critical points, referred to as Haldane graphene billiards in the following. The billiards had the shapes of a rectangular billiard with integrable dynamics, one with chaotic dynamics, and one whose shape has, in addition, threefold rotational symmetry. It had been shown that the spectral properties of the graphene billiards coincide with those of the nonrelativistic quantum billiard with the corresponding shape, both at the band edges and in the region of low energy excitations around the Dirac points at zero energy. There, the dispersion relation is linear and, accordingly, the spectrum is described by the same relativistic Dirac equation for massless half-spin particles as relativistic neutrino billiards, whose spectral properties agree with those of nonrelativistic quantum billiards with violated time-reversal invariance. Deviations from the expected behavior are attributed to differing boundary conditions and backscattering at the boundary, which leads to a mixing of valley states corresponding to the two Dirac points, that are mapped into each other through time reversal. We employ a Haldane model to introduce a gap at one of the two Dirac points so that backscattering is suppressed in the energy region of the gap and demonstrate that there the correlations in the spectra comply with those of the neutrino billiard of the corresponding shape.","sentences":["We study fluctuation properties in the energy spectra of finite-size honeycomb lattices, graphene billiards, subject to the Haldane-model onsite potential and next-nearest neighbor interaction at critical points, referred to as Haldane graphene billiards in the following.","The billiards had the shapes of a rectangular billiard with integrable dynamics, one with chaotic dynamics, and one whose shape has, in addition, threefold rotational symmetry.","It had been shown that the spectral properties of the graphene billiards coincide with those of the nonrelativistic quantum billiard with the corresponding shape, both at the band edges and in the region of low energy excitations around the Dirac points at zero energy.","There, the dispersion relation is linear and, accordingly, the spectrum is described by the same relativistic Dirac equation for massless half-spin particles as relativistic neutrino billiards, whose spectral properties agree with those of nonrelativistic quantum billiards with violated time-reversal invariance.","Deviations from the expected behavior are attributed to differing boundary conditions and backscattering at the boundary, which leads to a mixing of valley states corresponding to the two Dirac points, that are mapped into each other through time reversal.","We employ a Haldane model to introduce a gap at one of the two Dirac points so that backscattering is suppressed in the energy region of the gap and demonstrate that there the correlations in the spectra comply with those of the neutrino billiard of the corresponding shape."],"url":"http://arxiv.org/abs/2404.07679v1","category":"nlin.CD"}
{"created":"2024-04-11 12:09:47","title":"Curated Datasets and Neural Models for Machine Translation of Informal Registers between Mayan and Spanish Vernaculars","abstract":"The Mayan languages comprise a language family with an ancient history, millions of speakers, and immense cultural value, that, nevertheless, remains severely underrepresented in terms of resources and global exposure. In this paper we develop, curate, and publicly release a set of corpora in several Mayan languages spoken in Guatemala and Southern Mexico, which we call MayanV. The datasets are parallel with Spanish, the dominant language of the region, and are taken from official native sources focused on representing informal, day-to-day, and non-domain-specific language. As such, and according to our dialectometric analysis, they differ in register from most other available resources. Additionally, we present neural machine translation models, trained on as many resources and Mayan languages as possible, and evaluated exclusively on our datasets. We observe lexical divergences between the dialects of Spanish in our resources and the more widespread written standard of Spanish, and that resources other than the ones we present do not seem to improve translation performance, indicating that many such resources may not accurately capture common, real-life language usage. The MayanV dataset is available at https://github.com/transducens/mayanv.","sentences":["The Mayan languages comprise a language family with an ancient history, millions of speakers, and immense cultural value, that, nevertheless, remains severely underrepresented in terms of resources and global exposure.","In this paper we develop, curate, and publicly release a set of corpora in several Mayan languages spoken in Guatemala and Southern Mexico, which we call MayanV. The datasets are parallel with Spanish, the dominant language of the region, and are taken from official native sources focused on representing informal, day-to-day, and non-domain-specific language.","As such, and according to our dialectometric analysis, they differ in register from most other available resources.","Additionally, we present neural machine translation models, trained on as many resources and Mayan languages as possible, and evaluated exclusively on our datasets.","We observe lexical divergences between the dialects of Spanish in our resources and the more widespread written standard of Spanish, and that resources other than the ones we present do not seem to improve translation performance, indicating that many such resources may not accurately capture common, real-life language usage.","The MayanV dataset is available at https://github.com/transducens/mayanv."],"url":"http://arxiv.org/abs/2404.07673v1","category":"cs.CL"}
{"created":"2024-04-11 11:12:02","title":"Structure-dependent QED in $B^- \\to \\ell^- \\bar \u03bd(\u03b3)$","abstract":"Based on explicitly gauge invariant interpolating operators we compute complete next-leading order QED-corrections for leptonic decays. These are sizeable since the helicity-suppression in V-A interactions allows for structure-dependent collinear logs. We have explicitly checked that these logs are absent for helicity-unsuppressed Yukawa-type transitions. Based on $B \\to \\gamma$ form factors we present the rates for $B^- \\to (\\mu^-,\\tau^-) \\bar \\nu (\\gamma)$ in differential and integrated form as a function of the photon energy cut-off $E_\\gamma^{\\text{cut}}$. The effect of the virtual structure-dependent corrections are approximately $+5\\%$ and $+3\\%$ for the $\\mu$- and $\\tau$-channel respectively. The structure dependence of the real radiation exceeds that of the virtual one for $E_\\gamma^{\\text{cut}}|_{\\mu} > 0.18(3)$ GeV and is subdominant for the tau channel even when fully inclusive.","sentences":["Based on explicitly gauge invariant interpolating operators we compute complete next-leading order QED-corrections for leptonic decays.","These are sizeable since the helicity-suppression in V-A interactions allows for structure-dependent collinear logs.","We have explicitly checked that these logs are absent for helicity-unsuppressed Yukawa-type transitions.","Based on $B \\to \\gamma$ form factors we present the rates for $B^- \\to (\\mu^-,\\tau^-) \\bar \\nu (\\gamma)$ in differential and integrated form as a function of the photon energy cut-off $E_\\gamma^{\\text{cut}}$. The effect of the virtual structure-dependent corrections are approximately $+5\\%$ and $+3\\%$ for the $\\mu$- and $\\tau$-channel respectively.","The structure dependence of the real radiation exceeds that of the virtual one for $E_\\gamma^{\\text{cut}}|_{\\mu} > 0.18(3)$","GeV and is subdominant for the tau channel even when fully inclusive."],"url":"http://arxiv.org/abs/2404.07648v1","category":"hep-ph"}
{"created":"2024-04-11 11:10:36","title":"Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck","abstract":"Recent advances in language modeling consist in pretraining highly parameterized neural networks on extremely large web-mined text corpora. Training and inference with such models can be costly in practice, which incentivizes the use of smaller counterparts. However, it has been observed that smaller models can suffer from saturation, characterized as a drop in performance at some advanced point in training followed by a plateau. In this paper, we find that such saturation can be explained by a mismatch between the hidden dimension of smaller models and the high rank of the target contextual probability distribution. This mismatch affects the performance of the linear prediction head used in such models through the well-known softmax bottleneck phenomenon. We measure the effect of the softmax bottleneck in various settings and find that models based on less than 1000 hidden dimensions tend to adopt degenerate latent representations in late pretraining, which leads to reduced evaluation performance.","sentences":["Recent advances in language modeling consist in pretraining highly parameterized neural networks on extremely large web-mined text corpora.","Training and inference with such models can be costly in practice, which incentivizes the use of smaller counterparts.","However, it has been observed that smaller models can suffer from saturation, characterized as a drop in performance at some advanced point in training followed by a plateau.","In this paper, we find that such saturation can be explained by a mismatch between the hidden dimension of smaller models and the high rank of the target contextual probability distribution.","This mismatch affects the performance of the linear prediction head used in such models through the well-known softmax bottleneck phenomenon.","We measure the effect of the softmax bottleneck in various settings and find that models based on less than 1000 hidden dimensions tend to adopt degenerate latent representations in late pretraining, which leads to reduced evaluation performance."],"url":"http://arxiv.org/abs/2404.07647v1","category":"cs.CL"}
{"created":"2024-04-11 11:04:03","title":"Structure-Preserving Numerical Methods for Fokker-Planck Equations","abstract":"A common way to numerically solve Fokker-Planck equations is the Chang-Cooper method in space combined with one of the Euler methods in time. However, the explicit Euler method is only conditionally positive, leading to severe restrictions on the time step to ensure positivity. On the other hand, the implicit Euler method is robust but nonlinearly implicit. Instead, we propose to combine the Chang-Cooper method with unconditionally positive Patankar-type time integration methods, since they are unconditionally positive, robust for stiff problems, only linearly implicit, and also higher-order accurate. We describe the combined approach, analyse it, and present a relevant numerical example demonstrating advantages compared to schemes proposed in the literature.","sentences":["A common way to numerically solve Fokker-Planck equations is the Chang-Cooper method in space combined with one of the Euler methods in time.","However, the explicit Euler method is only conditionally positive, leading to severe restrictions on the time step to ensure positivity.","On the other hand, the implicit Euler method is robust but nonlinearly implicit.","Instead, we propose to combine the Chang-Cooper method with unconditionally positive Patankar-type time integration methods, since they are unconditionally positive, robust for stiff problems, only linearly implicit, and also higher-order accurate.","We describe the combined approach, analyse it, and present a relevant numerical example demonstrating advantages compared to schemes proposed in the literature."],"url":"http://arxiv.org/abs/2404.07641v1","category":"math.NA"}
{"created":"2024-04-11 10:10:13","title":"Optimal State Equation for the Control of a Diffusion with Two Distinct Dynamics","abstract":"We consider a class of stochastic control problems which has been widely used in optimal foraging theory. The state processes have two distinct dynamics, characterized by two pairs of drift and diffusion coefficients, depending on whether it takes values bigger or smaller than a threshold value. Adopting a perturbation type approach, we find an expression for potential measure of the optimal state process. We then obtain an expression for the transition density of the optimal state process by inverting the associated Laplace transform. Properties including the stationary distribution of the optimal state process are discussed. Finally, the expression of the value function is given for this class stochastic control problems.","sentences":["We consider a class of stochastic control problems which has been widely used in optimal foraging theory.","The state processes have two distinct dynamics, characterized by two pairs of drift and diffusion coefficients, depending on whether it takes values bigger or smaller than a threshold value.","Adopting a perturbation type approach, we find an expression for potential measure of the optimal state process.","We then obtain an expression for the transition density of the optimal state process by inverting the associated Laplace transform.","Properties including the stationary distribution of the optimal state process are discussed.","Finally, the expression of the value function is given for this class stochastic control problems."],"url":"http://arxiv.org/abs/2404.07618v1","category":"math.OC"}
{"created":"2024-04-11 09:50:05","title":"Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning and Satellite Imagery","abstract":"Despite extensive research into ship detection via remote sensing, no studies identify ship-to-ship transfers in satellite imagery. Given the importance of transshipment in illicit shipping practices, this is a significant gap. In what follows, I train a convolutional neural network to accurately detect 4 different types of cargo vessel and two different types of Ship-to-Ship transfer in PlanetScope satellite imagery. I then elaborate a pipeline for the automatic detection of suspected illicit ship-to-ship transfers by cross-referencing satellite detections with vessel borne GPS data. Finally, I apply this method to the Kerch Strait between Ukraine and Russia to identify over 400 dark transshipment events since 2022.","sentences":["Despite extensive research into ship detection via remote sensing, no studies identify ship-to-ship transfers in satellite imagery.","Given the importance of transshipment in illicit shipping practices, this is a significant gap.","In what follows, I train a convolutional neural network to accurately detect 4 different types of cargo vessel and two different types of Ship-to-Ship transfer in PlanetScope satellite imagery.","I then elaborate a pipeline for the automatic detection of suspected illicit ship-to-ship transfers by cross-referencing satellite detections with vessel borne GPS data.","Finally, I apply this method to the Kerch Strait between Ukraine and Russia to identify over 400 dark transshipment events since 2022."],"url":"http://arxiv.org/abs/2404.07607v1","category":"cs.CV"}
{"created":"2024-04-11 09:23:19","title":"Non-homogeneous fourth order elliptic inequalities with a convolution term motivated by the suspension bridge problem","abstract":"We are concerned with the study of the twin non-local inequalities featuring non-homogeneous differential operators $$\\displaystyle -\\Delta^2 u + \\lambda\\Delta u \\geq (K_{\\alpha, \\beta} * u^p)u^q \\quad\\text{ in } \\mathbb{R}^N (N\\geq 1),$$ and $$\\displaystyle \\Delta^2 u - \\lambda\\Delta u \\geq (K_{\\alpha, \\beta} * u^p)u^q \\quad\\text{ in } \\mathbb{R}^N (N\\geq 1),$$ with parameters $\\lambda, p, q >0$, $0\\leq \\alpha \\leq N$ and $\\beta>\\alpha-N$. In the above inequalities the potential $K_{\\alpha,\\beta}$ is given by $K_{\\alpha, \\beta}(x) = |x|^{-\\alpha}\\log^{\\beta}(1 + |x|)$ while $K_{\\alpha, \\beta} * u^p$ denotes the standard convolution operator in $\\mathbb{R}^N$. We discuss the existence and non-existence of non-negative solutions in terms of $N, p, q, \\lambda, \\alpha$ and $\\beta$.","sentences":["We are concerned with the study of the twin non-local inequalities featuring non-homogeneous differential operators $$\\displaystyle -\\Delta^2 u + \\lambda\\Delta u \\geq (K_{\\alpha, \\beta} * u^p)u^q \\quad\\text{ in } \\mathbb{R}^N (N\\geq 1),$$ and $$\\displaystyle \\Delta^2 u - \\lambda\\Delta u \\geq (K_{\\alpha, \\beta} * u^p)u^q \\quad\\text{ in } \\mathbb{R}^N (N\\geq 1),$$ with parameters $\\lambda, p, q >0$, $0\\leq \\alpha \\leq N$ and $\\beta>\\alpha-N$.","In the above inequalities the potential $K_{\\alpha,\\beta}$ is given by $K_{\\alpha, \\beta}(x) = |x|^{-\\alpha}\\log^{\\beta}(1 + |x|)$ while $K_{\\alpha, \\beta} * u^p$ denotes the standard convolution operator in $\\mathbb{R}^N$. We discuss the existence and non-existence of non-negative solutions in terms of $N, p, q, \\lambda, \\alpha$ and $\\beta$."],"url":"http://arxiv.org/abs/2404.07592v1","category":"math.AP"}
{"created":"2024-04-11 08:30:57","title":"On the Borisov-Gunnells relations for products of Eisenstein series","abstract":"Borisov and Gunnells have proved that certain linear combinations of products of Eisenstein series are Eisenstein series themselves, in analogy with the Manin 3-term relations for modular symbols. We devise a new method to determine and prove such relations, by differentiating with respect to the parameters of the Eisenstein series.","sentences":["Borisov and Gunnells have proved that certain linear combinations of products of Eisenstein series are Eisenstein series themselves, in analogy with the Manin 3-term relations for modular symbols.","We devise a new method to determine and prove such relations, by differentiating with respect to the parameters of the Eisenstein series."],"url":"http://arxiv.org/abs/2404.07550v1","category":"math.NT"}
{"created":"2024-04-11 07:16:00","title":"Efficient sEMG-based Cross-Subject Joint Angle Estimation via Hierarchical Spiking Attentional Feature Decomposition Network","abstract":"Surface electromyography (sEMG) has demonstrated significant potential in simultaneous and proportional control (SPC). However, existing algorithms for predicting joint angles based on sEMG often suffer from high inference costs or are limited to specific subjects rather than cross-subject scenarios. To address these challenges, we introduced a hierarchical Spiking Attentional Feature Decomposition Network (SAFE-Net). This network initially compresses sEMG signals into neural spiking forms using a Spiking Sparse Attention Encoder (SSAE). Subsequently, the compressed features are decomposed into kinematic and biological features through a Spiking Attentional Feature Decomposition (SAFD) module. Finally, the kinematic and biological features are used to predict joint angles and identify subject identities, respectively. Our validation on two datasets (SIAT-DB1 and SIAT-DB2) and comparison with two existing methods, Informer and Spikformer, demonstrate that SSAE achieves significant power consumption savings of 39.1% and 37.5% respectively over them in terms of inference costs. Furthermore, SAFE-Net surpasses Informer and Spikformer in recognition accuracy on both datasets. This study underscores the potential of SAFE-Net to advance the field of SPC in lower limb rehabilitation exoskeleton robots.","sentences":["Surface electromyography (sEMG) has demonstrated significant potential in simultaneous and proportional control (SPC).","However, existing algorithms for predicting joint angles based on sEMG often suffer from high inference costs or are limited to specific subjects rather than cross-subject scenarios.","To address these challenges, we introduced a hierarchical Spiking Attentional Feature Decomposition Network (SAFE-Net).","This network initially compresses sEMG signals into neural spiking forms using a Spiking Sparse Attention Encoder (SSAE).","Subsequently, the compressed features are decomposed into kinematic and biological features through a Spiking Attentional Feature Decomposition (SAFD) module.","Finally, the kinematic and biological features are used to predict joint angles and identify subject identities, respectively.","Our validation on two datasets (SIAT-DB1 and SIAT-DB2) and comparison with two existing methods, Informer and Spikformer, demonstrate that SSAE achieves significant power consumption savings of 39.1% and 37.5% respectively over them in terms of inference costs.","Furthermore, SAFE-Net surpasses Informer and Spikformer in recognition accuracy on both datasets.","This study underscores the potential of SAFE-Net to advance the field of SPC in lower limb rehabilitation exoskeleton robots."],"url":"http://arxiv.org/abs/2404.07517v1","category":"cs.HC"}
{"created":"2024-04-11 05:46:28","title":"Hook length biases in ordinary and $t$-regular partitions","abstract":"In this article, we study hook lengths of ordinary partitions and $t$-regular partitions. We establish hook length biases for the ordinary partitions and motivated by them we find a few interesting hook length biases in $2$-regular partitions. For a positive integer $k$, let $p_{(k)}(n)$ denote the number of hooks of length $k$ in all the partitions of $n$. We prove that $p_{(k)}(n)\\geq p_{(k+1)}(n)$ for all $n\\geq0$ and $n\\ne k+1$; and $p_{(k)}(k+1)- p_{(k+1)}(k+1)=-1$ for $k\\geq 2$. For integers $t\\geq2$ and $k\\geq1$, let $b_{t,k}(n)$ denote the number of hooks of length $k$ in all the $t$-regular partitions of $n$. We find generating functions of $b_{t,k}(n)$ for certain values of $t$ and $k$. Exploring hook length biases for $b_{t,k}(n)$, we observe that in certain cases biases are opposite to the biases for ordinary partitions. We prove that $b_{2,2}(n)\\geq b_{2,1}(n)$ for all $n>4$, whereas $b_{2,2}(n)\\geq b_{2,3}(n)$ for all $n\\geq 0$. Our proofs are both combinatorial and analytic in nature. We also propose some conjectures on biases among $b_{t,k}(n)$.","sentences":["In this article, we study hook lengths of ordinary partitions and $t$-regular partitions.","We establish hook length biases for the ordinary partitions and motivated by them we find a few interesting hook length biases in $2$-regular partitions.","For a positive integer $k$, let $p_{(k)}(n)$ denote the number of hooks of length $k$ in all the partitions of $n$. We prove that $p_{(k)}(n)\\geq p_{(k+1)}(n)$ for all $n\\geq0$ and $n\\ne k+1$; and $p_{(k)}(k+1)- p_{(k+1)}(k+1)=-1$ for $k\\geq 2$.","For integers $t\\geq2$ and $k\\geq1$, let $b_{t,k}(n)$ denote the number of hooks of length $k$ in all the $t$-regular partitions of $n$. We find generating functions of $b_{t,k}(n)$ for certain values of $t$ and $k$. Exploring hook length biases for $b_{t,k}(n)$, we observe that in certain cases biases are opposite to the biases for ordinary partitions.","We prove that $b_{2,2}(n)\\geq b_{2,1}(n)$ for all $n>4$, whereas $b_{2,2}(n)\\geq b_{2,3}(n)$ for all $n\\geq 0$.","Our proofs are both combinatorial and analytic in nature.","We also propose some conjectures on biases among $b_{t,k}(n)$."],"url":"http://arxiv.org/abs/2404.07485v1","category":"math.CO"}
{"created":"2024-04-11 05:11:06","title":"Ricci curvature and fundamental groups of effective regular sets","abstract":"For a Gromov-Hausdorff convergent sequence of closed manifolds $M_i^n\\overset{GH}\\longrightarrow X$ with $\\mathrm{Ric}\\ge-(n-1)$, $\\mathrm{diam}(M_i)\\le D$, and $\\mathrm{vol}(M_i)\\ge v>0$, we study the relation between $\\pi_1(M_i)$ and $X$. It was known before that there is a surjective homomorphism $\\phi_i:\\pi_1(M_i)\\to \\pi_1(X)$ by the work of Pan-Wei. In this paper, we construct a surjective homomorphism from the interior of the effective regular set in $X$ back to $M_i$, that is, $\\psi_i:\\pi_1(\\mathcal{R}_{\\epsilon,\\delta}^\\circ)\\to \\pi_1(M_i)$. These surjective homomorphisms $\\phi_i$ and $\\psi_i$ are natural in the sense that their composition $\\phi_i \\circ \\psi_i$ is exactly the homomorphism induced by the inclusion map $\\mathcal{R}_{\\epsilon,\\delta}^\\circ \\hookrightarrow X$.","sentences":["For a Gromov-Hausdorff convergent sequence of closed manifolds $M_i^n\\overset{GH}\\longrightarrow X$ with $\\mathrm{Ric}\\ge-(n-1)$, $\\mathrm{diam}(M_i)\\le D$, and $\\mathrm{vol}(M_i)\\ge v>0$, we study the relation between $\\pi_1(M_i)$ and $X$. It was known before that there is a surjective homomorphism $\\phi_i:\\pi_1(M_i)\\to \\pi_1(X)$ by the work of Pan-Wei.","In this paper, we construct a surjective homomorphism from the interior of the effective regular set in $X$ back to $M_i$, that is, $\\psi_i:\\pi_1(\\mathcal{R}_{\\epsilon,\\delta}^\\circ)\\to \\pi_1(M_i)$. These surjective homomorphisms $\\phi_i$ and $\\psi_i$ are natural in the sense that their composition $\\phi_i \\circ \\psi_i$ is exactly the homomorphism induced by the inclusion map $\\mathcal{R}_{\\epsilon,\\delta}^\\circ \\hookrightarrow X$."],"url":"http://arxiv.org/abs/2404.07478v1","category":"math.DG"}
{"created":"2024-04-11 02:54:17","title":"1-bit Quantized On-chip Hybrid Diffraction Neural Network Enabled by Authentic All-optical Fully-connected Architecture","abstract":"Optical Diffraction Neural Networks (DNNs), a subset of Optical Neural Networks (ONNs), show promise in mirroring the prowess of electronic networks. This study introduces the Hybrid Diffraction Neural Network (HDNN), a novel architecture that incorporates matrix multiplication into DNNs, synergizing the benefits of conventional ONNs with those of DNNs to surmount the modulation limitations inherent in optical diffraction neural networks. Utilizing a singular phase modulation layer and an amplitude modulation layer, the trained neural network demonstrated remarkable accuracies of 96.39% and 89% in digit recognition tasks in simulation and experiment, respectively. Additionally, we develop the Binning Design (BD) method, which effectively mitigates the constraints imposed by sampling intervals on diffraction units, substantially streamlining experimental procedures. Furthermore, we propose an on-chip HDNN that not only employs a beam-splitting phase modulation layer for enhanced integration level but also significantly relaxes device fabrication requirements, replacing metasurfaces with relief surfaces designed by 1-bit quantization. Besides, we conceptualized an all-optical HDNN-assisted lesion detection network, achieving detection outcomes that were 100% aligned with simulation predictions. This work not only advances the performance of DNNs but also streamlines the path towards industrial optical neural network production.","sentences":["Optical Diffraction Neural Networks (DNNs), a subset of Optical Neural Networks (ONNs), show promise in mirroring the prowess of electronic networks.","This study introduces the Hybrid Diffraction Neural Network (HDNN), a novel architecture that incorporates matrix multiplication into DNNs, synergizing the benefits of conventional ONNs with those of DNNs to surmount the modulation limitations inherent in optical diffraction neural networks.","Utilizing a singular phase modulation layer and an amplitude modulation layer, the trained neural network demonstrated remarkable accuracies of 96.39% and 89% in digit recognition tasks in simulation and experiment, respectively.","Additionally, we develop the Binning Design (BD) method, which effectively mitigates the constraints imposed by sampling intervals on diffraction units, substantially streamlining experimental procedures.","Furthermore, we propose an on-chip HDNN that not only employs a beam-splitting phase modulation layer for enhanced integration level but also significantly relaxes device fabrication requirements, replacing metasurfaces with relief surfaces designed by 1-bit quantization.","Besides, we conceptualized an all-optical HDNN-assisted lesion detection network, achieving detection outcomes that were 100% aligned with simulation predictions.","This work not only advances the performance of DNNs but also streamlines the path towards industrial optical neural network production."],"url":"http://arxiv.org/abs/2404.07443v1","category":"physics.optics"}
{"created":"2024-04-11 02:39:48","title":"Privacy preserving layer partitioning for Deep Neural Network models","abstract":"MLaaS (Machine Learning as a Service) has become popular in the cloud computing domain, allowing users to leverage cloud resources for running private inference of ML models on their data. However, ensuring user input privacy and secure inference execution is essential. One of the approaches to protect data privacy and integrity is to use Trusted Execution Environments (TEEs) by enabling execution of programs in secure hardware enclave. Using TEEs can introduce significant performance overhead due to the additional layers of encryption, decryption, security and integrity checks. This can lead to slower inference times compared to running on unprotected hardware. In our work, we enhance the runtime performance of ML models by introducing layer partitioning technique and offloading computations to GPU. The technique comprises two distinct partitions: one executed within the TEE, and the other carried out using a GPU accelerator. Layer partitioning exposes intermediate feature maps in the clear which can lead to reconstruction attacks to recover the input. We conduct experiments to demonstrate the effectiveness of our approach in protecting against input reconstruction attacks developed using trained conditional Generative Adversarial Network(c-GAN). The evaluation is performed on widely used models such as VGG-16, ResNet-50, and EfficientNetB0, using two datasets: ImageNet for Image classification and TON IoT dataset for cybersecurity attack detection.","sentences":["MLaaS (Machine Learning as a Service) has become popular in the cloud computing domain, allowing users to leverage cloud resources for running private inference of ML models on their data.","However, ensuring user input privacy and secure inference execution is essential.","One of the approaches to protect data privacy and integrity is to use Trusted Execution Environments (TEEs) by enabling execution of programs in secure hardware enclave.","Using TEEs can introduce significant performance overhead due to the additional layers of encryption, decryption, security and integrity checks.","This can lead to slower inference times compared to running on unprotected hardware.","In our work, we enhance the runtime performance of ML models by introducing layer partitioning technique and offloading computations to GPU.","The technique comprises two distinct partitions: one executed within the TEE, and the other carried out using a GPU accelerator.","Layer partitioning exposes intermediate feature maps in the clear which can lead to reconstruction attacks to recover the input.","We conduct experiments to demonstrate the effectiveness of our approach in protecting against input reconstruction attacks developed using trained conditional Generative Adversarial Network(c-GAN).","The evaluation is performed on widely used models such as VGG-16, ResNet-50, and EfficientNetB0, using two datasets: ImageNet for Image classification and TON IoT dataset for cybersecurity attack detection."],"url":"http://arxiv.org/abs/2404.07437v1","category":"cs.CR"}
{"created":"2024-04-11 01:27:51","title":"Fermi sea and sky in the Bogoliubov-de Gennes equation","abstract":"We develop a comprehensive logical framework for effectively handling the overcomplete basis set in the Bogoliubov-de Gennes equation that contains two orthonormal basis sets conjugate with each other, such as particle and hole orthonormal basis sets. We highlight the significant implications of our logical framework from theoretical concepts and experimental predictions. Firstly, we rigorously derive all many-body eigenfunctions of arbitrary nonuniform superconductors and uncover that the many-body eigenstates are full of superconducting spin clouds-the electron configuration within the Cooper-like pair of an arbitrary nonuniform superconductor. Secondly, we demonstrate a conjugate loop formed by the effective vacuum states of two orthonormal basis sets conjugate with each other, such as the Fermi sea and sky-the effective vacuum states of positive and negative orthonormal basis sets, respectively. Thirdly, we present a gate-, field-, and phase-tunable tunnel spectroscopy asymmetry arising from the imbalanced particle-hole distribution of the subgap quasiparticles in a quantum-dot Josephson junction. These findings underscore the power of our logical framework and its implications for advancing our understanding and utilization of solid-state devices based on superconductivity.","sentences":["We develop a comprehensive logical framework for effectively handling the overcomplete basis set in the Bogoliubov-de Gennes equation that contains two orthonormal basis sets conjugate with each other, such as particle and hole orthonormal basis sets.","We highlight the significant implications of our logical framework from theoretical concepts and experimental predictions.","Firstly, we rigorously derive all many-body eigenfunctions of arbitrary nonuniform superconductors and uncover that the many-body eigenstates are full of superconducting spin clouds-the electron configuration within the Cooper-like pair of an arbitrary nonuniform superconductor.","Secondly, we demonstrate a conjugate loop formed by the effective vacuum states of two orthonormal basis sets conjugate with each other, such as the Fermi sea and sky-the effective vacuum states of positive and negative orthonormal basis sets, respectively.","Thirdly, we present a gate-, field-, and phase-tunable tunnel spectroscopy asymmetry arising from the imbalanced particle-hole distribution of the subgap quasiparticles in a quantum-dot Josephson junction.","These findings underscore the power of our logical framework and its implications for advancing our understanding and utilization of solid-state devices based on superconductivity."],"url":"http://arxiv.org/abs/2404.07423v1","category":"cond-mat.supr-con"}
{"created":"2024-04-11 00:49:38","title":"Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling","abstract":"Downsampling operators break the shift invariance of convolutional neural networks (CNNs) and this affects the robustness of features learned by CNNs when dealing with even small pixel-level shift. Through a large-scale correlation analysis framework, we study shift invariance of CNNs by inspecting existing downsampling operators in terms of their maximum-sampling bias (MSB), and find that MSB is negatively correlated with shift invariance. Based on this crucial insight, we propose a learnable pooling operator called Translation Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate feature maps of TIPS to reduce MSB and learn translation-invariant representations. TIPS can be integrated into any CNN and can be trained end-to-end with marginal computational overhead. Our experiments demonstrate that TIPS results in consistent performance gains in terms of accuracy, shift consistency, and shift fidelity on multiple benchmarks for image classification and semantic segmentation compared to previous methods and also leads to improvements in adversarial and distributional robustness. TIPS results in the lowest MSB compared to all previous methods, thus explaining our strong empirical results.","sentences":["Downsampling operators break the shift invariance of convolutional neural networks (CNNs) and this affects the robustness of features learned by CNNs when dealing with even small pixel-level shift.","Through a large-scale correlation analysis framework, we study shift invariance of CNNs by inspecting existing downsampling operators in terms of their maximum-sampling bias (MSB), and find that MSB is negatively correlated with shift invariance.","Based on this crucial insight, we propose a learnable pooling operator called Translation Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate feature maps of TIPS to reduce MSB and learn translation-invariant representations.","TIPS can be integrated into any CNN and can be trained end-to-end with marginal computational overhead.","Our experiments demonstrate that TIPS results in consistent performance gains in terms of accuracy, shift consistency, and shift fidelity on multiple benchmarks for image classification and semantic segmentation compared to previous methods and also leads to improvements in adversarial and distributional robustness.","TIPS results in the lowest MSB compared to all previous methods, thus explaining our strong empirical results."],"url":"http://arxiv.org/abs/2404.07410v1","category":"cs.CV"}
{"created":"2024-04-11 00:39:56","title":"Schr\u00f6dinger's bridges with stopping: Steering of stochastic flows towards spatio-temporal marginals","abstract":"The purpose of the present work is to expand substantially the type of control and estimation problems that can be addressed following the paradigm of Schr\\\"odinger bridges, by incorporating stopping (freezing) of a given stochastic flow. Specifically, in the context of estimation, we seek the most likely evolution realizing prescribed spatio-temporal marginals of stopped particles. In the context of control, we seek the control action directing the stochastic flow toward spatio-temporal probabilistic constraints. To this end, we derive a new Schr\\\"odinger system of coupled, in space and time, partial differential equations to construct the solution of the proposed problem. Further, we show that a Fortet-Sinkhorn type of algorithm is, once again, available to attain the associated bridge. A key feature of the framework is that the obtained bridge retains the Markovian structure in the prior process, and thereby, the corresponding control action takes the form of state feedback.","sentences":["The purpose of the present work is to expand substantially the type of control and estimation problems that can be addressed following the paradigm of Schr\\\"odinger bridges, by incorporating stopping (freezing) of a given stochastic flow.","Specifically, in the context of estimation, we seek the most likely evolution realizing prescribed spatio-temporal marginals of stopped particles.","In the context of control, we seek the control action directing the stochastic flow toward spatio-temporal probabilistic constraints.","To this end, we derive a new Schr\\\"odinger system of coupled, in space and time, partial differential equations to construct the solution of the proposed problem.","Further, we show that a Fortet-Sinkhorn type of algorithm is, once again, available to attain the associated bridge.","A key feature of the framework is that the obtained bridge retains the Markovian structure in the prior process, and thereby, the corresponding control action takes the form of state feedback."],"url":"http://arxiv.org/abs/2404.07402v1","category":"math.OC"}
{"created":"2024-04-11 00:24:49","title":"A fine-tuning workflow for automatic first-break picking with deep learning","abstract":"First-break picking is an essential step in seismic data processing. First arrivals should be picked by an expert. This is a time-consuming procedure and subjective to a certain degree, leading to different results for different operators. In this study, we used a U-Net architecture with residual blocks to perform automatic first-break picking based on deep learning. Focusing on the effects of weight initialization on this process, we conduct this research by using the weights of a pretrained network that is used for object detection on the ImageNet dataset. The efficiency of the proposed method is tested on two real datasets. For both datasets, we pick manually the first breaks for less than 10% of the seismic shots. The pretrained network is fine-tuned on the picked shots and the rest of the shots are automatically picked by the neural network. It is shown that this strategy allows to reduce the size of the training set, requiring fine tuning with only a few picked shots per survey. Using random weights and more training epochs can lead to a lower training loss, but such a strategy leads to overfitting as the test error is higher than the one of the pretrained network. We also assess the possibility of using a general dataset by training a network with data from three different projects that are acquired with different equipment and at different locations. This study shows that if the general dataset is created carefully it can lead to more accurate first-break picking, otherwise the general dataset can decrease the accuracy. Focusing on near-surface geophysics, we perform traveltime tomography and compare the inverted velocity models based on different first-break picking methodologies. The results of the inversion show that the first breaks obtained by the pretrained network lead to a velocity model that is closer to the one obtained from the inversion of expert-picked first breaks.","sentences":["First-break picking is an essential step in seismic data processing.","First arrivals should be picked by an expert.","This is a time-consuming procedure and subjective to a certain degree, leading to different results for different operators.","In this study, we used a U-Net architecture with residual blocks to perform automatic first-break picking based on deep learning.","Focusing on the effects of weight initialization on this process, we conduct this research by using the weights of a pretrained network that is used for object detection on the ImageNet dataset.","The efficiency of the proposed method is tested on two real datasets.","For both datasets, we pick manually the first breaks for less than 10% of the seismic shots.","The pretrained network is fine-tuned on the picked shots and the rest of the shots are automatically picked by the neural network.","It is shown that this strategy allows to reduce the size of the training set, requiring fine tuning with only a few picked shots per survey.","Using random weights and more training epochs can lead to a lower training loss, but such a strategy leads to overfitting as the test error is higher than the one of the pretrained network.","We also assess the possibility of using a general dataset by training a network with data from three different projects that are acquired with different equipment and at different locations.","This study shows that if the general dataset is created carefully it can lead to more accurate first-break picking, otherwise the general dataset can decrease the accuracy.","Focusing on near-surface geophysics, we perform traveltime tomography and compare the inverted velocity models based on different first-break picking methodologies.","The results of the inversion show that the first breaks obtained by the pretrained network lead to a velocity model that is closer to the one obtained from the inversion of expert-picked first breaks."],"url":"http://arxiv.org/abs/2404.07400v1","category":"physics.geo-ph"}
{"created":"2024-04-10 21:43:16","title":"First Eigenvalue Estimates for Asymptotically Hyperbolic Manifolds and their Submanifolds","abstract":"We derive a sharp upper bound for the first eigenvalue $\\lambda_{1,p}$ of the $p$-Laplacian on asymptotically hyperbolic manifolds for $1<p<\\infty$. We then prove that a particular class of conformally compact submanifolds within asymptotically hyperbolic manifolds are themselves asymptotically hyperbolic. As a corollary, we show that for any minimal conformally compact submanifold $Y^{k+1}$ within $\\mathbb{H}^{n+1}(-1)$, $\\lambda_{1,p}(Y)=\\left(\\frac{k}{p}\\right)^{p}$. We then obtain lower bounds on $\\lambda_{1,2}(Y)$ in the case where minimality is replaced with a bounded mean curvature assumption and where the ambient space is a general Poincar\\'e-Einstein space whose conformal infinity is of non-negative Yamabe type. In the process, we introduce an invariant $\\hat \\beta^Y$ for each such submanifold, enabling us to generalize a result due to Cheung-Leung.","sentences":["We derive a sharp upper bound for the first eigenvalue $\\lambda_{1,p}$ of the $p$-Laplacian on asymptotically hyperbolic manifolds for $1<p<\\infty$. We then prove that a particular class of conformally compact submanifolds within asymptotically hyperbolic manifolds are themselves asymptotically hyperbolic.","As a corollary, we show that for any minimal conformally compact submanifold $Y^{k+1}$ within $\\mathbb{H}^{n+1}(-1)$, $\\lambda_{1,p}(Y)=\\left(\\frac{k}{p}\\right)^{p}$. We then obtain lower bounds on $\\lambda_{1,2}(Y)$ in the case where minimality is replaced with a bounded mean curvature assumption and where the ambient space is a general Poincar\\'e-Einstein space whose conformal infinity is of non-negative Yamabe type.","In the process, we introduce an invariant $\\hat \\beta^Y$ for each such submanifold, enabling us to generalize a result due to Cheung-Leung."],"url":"http://arxiv.org/abs/2404.07365v1","category":"math.DG"}
{"created":"2024-04-10 21:36:59","title":"Gradient Networks","abstract":"Directly parameterizing and learning gradients of functions has widespread significance, with specific applications in optimization, generative modeling, and optimal transport. This paper introduces gradient networks (GradNets): novel neural network architectures that parameterize gradients of various function classes. GradNets exhibit specialized architectural constraints that ensure correspondence to gradient functions. We provide a comprehensive GradNet design framework that includes methods for transforming GradNets into monotone gradient networks (mGradNets), which are guaranteed to represent gradients of convex functions. We establish the approximation capabilities of the proposed GradNet and mGradNet. Our results demonstrate that these networks universally approximate the gradients of (convex) functions. Furthermore, these networks can be customized to correspond to specific spaces of (monotone) gradient functions, including gradients of transformed sums of (convex) ridge functions. Our analysis leads to two distinct GradNet architectures, GradNet-C and GradNet-M, and we describe the corresponding monotone versions, mGradNet-C and mGradNet-M. Our empirical results show that these architectures offer efficient parameterizations and outperform popular methods in gradient field learning tasks.","sentences":["Directly parameterizing and learning gradients of functions has widespread significance, with specific applications in optimization, generative modeling, and optimal transport.","This paper introduces gradient networks (GradNets): novel neural network architectures that parameterize gradients of various function classes.","GradNets exhibit specialized architectural constraints that ensure correspondence to gradient functions.","We provide a comprehensive GradNet design framework that includes methods for transforming GradNets into monotone gradient networks (mGradNets), which are guaranteed to represent gradients of convex functions.","We establish the approximation capabilities of the proposed GradNet and mGradNet.","Our results demonstrate that these networks universally approximate the gradients of (convex) functions.","Furthermore, these networks can be customized to correspond to specific spaces of (monotone) gradient functions, including gradients of transformed sums of (convex) ridge functions.","Our analysis leads to two distinct GradNet architectures, GradNet-C and GradNet-M, and we describe the corresponding monotone versions, mGradNet-C and mGradNet-M. Our empirical results show that these architectures offer efficient parameterizations and outperform popular methods in gradient field learning tasks."],"url":"http://arxiv.org/abs/2404.07361v1","category":"cs.LG"}
{"created":"2024-04-10 21:28:54","title":"A solution for the mean parametrization of the von Mises-Fisher distribution","abstract":"The von Mises-Fisher distribution as an exponential family can be expressed in terms of either its natural or its mean parameters. Unfortunately, however, the normalization function for the distribution in terms of its mean parameters is not available in closed form, limiting the practicality of the mean parametrization and complicating maximum-likelihood estimation more generally. We derive a second-order ordinary differential equation, the solution to which yields the mean-parameter normalizer along with its first two derivatives, as well as the variance function of the family. We also provide closed-form approximations to the solution of the differential equation. This allows rapid evaluation of both densities and natural parameters in terms of mean parameters. We show applications to topic modeling with mixtures of von Mises-Fisher distributions using Bregman Clustering.","sentences":["The von Mises-Fisher distribution as an exponential family can be expressed in terms of either its natural or its mean parameters.","Unfortunately, however, the normalization function for the distribution in terms of its mean parameters is not available in closed form, limiting the practicality of the mean parametrization and complicating maximum-likelihood estimation more generally.","We derive a second-order ordinary differential equation, the solution to which yields the mean-parameter normalizer along with its first two derivatives, as well as the variance function of the family.","We also provide closed-form approximations to the solution of the differential equation.","This allows rapid evaluation of both densities and natural parameters in terms of mean parameters.","We show applications to topic modeling with mixtures of von Mises-Fisher distributions using Bregman Clustering."],"url":"http://arxiv.org/abs/2404.07358v1","category":"stat.CO"}
{"created":"2024-04-10 21:02:58","title":"Indoor Location Fingerprinting Privacy: A Comprehensive Survey","abstract":"The pervasive integration of Indoor Positioning Systems (IPS) arises from the limitations of Global Navigation Satellite Systems (GNSS) in indoor environments, leading to the widespread adoption of Location-Based Services (LBS). Specifically, indoor location fingerprinting employs diverse signal fingerprints from user devices, enabling precise location identification by Location Service Providers (LSP). Despite its broad applications across various domains, indoor location fingerprinting introduces a notable privacy risk, as both LSP and potential adversaries inherently have access to this sensitive information, compromising users' privacy. Consequently, concerns regarding privacy vulnerabilities in this context necessitate a focused exploration of privacy-preserving mechanisms. In response to these concerns, this survey presents a comprehensive review of Privacy-Preserving Mechanisms in Indoor Location Fingerprinting (ILFPPM) based on cryptographic, anonymization, differential privacy (DP), and federated learning (FL) techniques. We also propose a distinctive and novel grouping of privacy vulnerabilities, adversary and attack models, and available evaluation metrics specific to indoor location fingerprinting systems. Given the identified limitations and research gaps in this survey, we highlight numerous prospective opportunities for future investigation, aiming to motivate researchers interested in advancing this field. This survey serves as a valuable reference for researchers and provides a clear overview for those beyond this specific research domain.","sentences":["The pervasive integration of Indoor Positioning Systems (IPS) arises from the limitations of Global Navigation Satellite Systems (GNSS) in indoor environments, leading to the widespread adoption of Location-Based Services (LBS).","Specifically, indoor location fingerprinting employs diverse signal fingerprints from user devices, enabling precise location identification by Location Service Providers (LSP).","Despite its broad applications across various domains, indoor location fingerprinting introduces a notable privacy risk, as both LSP and potential adversaries inherently have access to this sensitive information, compromising users' privacy.","Consequently, concerns regarding privacy vulnerabilities in this context necessitate a focused exploration of privacy-preserving mechanisms.","In response to these concerns, this survey presents a comprehensive review of Privacy-Preserving Mechanisms in Indoor Location Fingerprinting (ILFPPM) based on cryptographic, anonymization, differential privacy (DP), and federated learning (FL) techniques.","We also propose a distinctive and novel grouping of privacy vulnerabilities, adversary and attack models, and available evaluation metrics specific to indoor location fingerprinting systems.","Given the identified limitations and research gaps in this survey, we highlight numerous prospective opportunities for future investigation, aiming to motivate researchers interested in advancing this field.","This survey serves as a valuable reference for researchers and provides a clear overview for those beyond this specific research domain."],"url":"http://arxiv.org/abs/2404.07345v1","category":"cs.CR"}
{"created":"2024-04-10 19:42:55","title":"Non-trivial Integer Solutions of $x^r+y^r=Dz^p$","abstract":"In this paper, we use the modular method together with some standard conjectures to prove that infinitely many equations of the type $x^r+y^r=Dz^p$ do not have any non-trivial primitive integer solutions, where $r>5$ is a fixed prime, whenever $p$ is large enough.","sentences":["In this paper, we use the modular method together with some standard conjectures to prove that infinitely many equations of the type $x^r+y^r=Dz^p$ do not have any non-trivial primitive integer solutions, where $r>5$ is a fixed prime, whenever $p$ is large enough."],"url":"http://arxiv.org/abs/2404.07319v1","category":"math.NT"}
{"created":"2024-04-10 19:18:40","title":"Average entropy of Gaussian mixtures","abstract":"We calculate the average differential entropy of a $q$-component Gaussian mixture in $\\mathbb R^n$. For simplicity, all components have covariance matrix $\\sigma^2 {\\mathbf 1}$, while the means $\\{\\mathbf{W}_i\\}_{i=1}^{q}$ are i.i.d. Gaussian vectors with zero mean and covariance $s^2 {\\mathbf 1}$. We obtain a series expansion in $\\mu=s^2/\\sigma^2$ for the average differential entropy up to order $\\mathcal{O}(\\mu^2)$, and we provide a recipe to calculate higher order terms. Our result provides an analytic approximation with a quantifiable order of magnitude for the error, which is not achieved in previous literature.","sentences":["We calculate the average differential entropy of a $q$-component Gaussian mixture in $\\mathbb R^n$. For simplicity, all components have covariance matrix $\\sigma^2 {\\mathbf 1}$, while the means $\\{\\mathbf{W}_i\\}_{i=1}^{q}$ are i.i.d.","Gaussian vectors with zero mean and covariance $s^2 {\\mathbf 1}$. We obtain a series expansion in $\\mu=s^2/\\sigma^2$ for the average differential entropy up to order $\\mathcal{O}(\\mu^2)$, and we provide a recipe to calculate higher order terms.","Our result provides an analytic approximation with a quantifiable order of magnitude for the error, which is not achieved in previous literature."],"url":"http://arxiv.org/abs/2404.07311v1","category":"cs.IT"}
{"created":"2024-04-10 18:59:05","title":"Peierls Transition in Gross-Neveu Model from Bethe Ansatz","abstract":"The two-dimensional Gross-Neveu model is anticipated to undergo a crystalline phase transition at high baryon charge densities. This conclusion is drawn from the mean-field approximation, which closely resembles models of Peierls instability. We demonstrate that this transition indeed occurs when both the rank of the symmetry group and the dimension of the particle representation contributing to the baryon density are large (the large-N limit). We derive this result through the exact solution of the model, developing the large-N limit of the Bethe Ansatz. Our analytical construction of the large-N solution of the Bethe Ansatz equations aligns perfectly with the periodic (finite-gap) solution of the Korteweg-de Vries (KdV) of the mean-field analysis.","sentences":["The two-dimensional Gross-Neveu model is anticipated to undergo a crystalline phase transition at high baryon charge densities.","This conclusion is drawn from the mean-field approximation, which closely resembles models of Peierls instability.","We demonstrate that this transition indeed occurs when both the rank of the symmetry group and the dimension of the particle representation contributing to the baryon density are large (the large-N limit).","We derive this result through the exact solution of the model, developing the large-N limit of the Bethe Ansatz.","Our analytical construction of the large-N solution of the Bethe Ansatz equations aligns perfectly with the periodic (finite-gap) solution of the Korteweg-de Vries (KdV) of the mean-field analysis."],"url":"http://arxiv.org/abs/2404.07307v1","category":"hep-th"}
{"created":"2024-04-10 18:40:58","title":"sCWatter: Open source coupled wave scattering simulation for spectroscopy and microscopy","abstract":"Several emerging microscopy imaging methods rely on complex interactions between the incident light and the sample. These include interferometry, spectroscopy, and nonlinear optics. Reconstructing a sample from the measured scattered field relies on fast and accurate optical models. Fast approaches like ray tracing and the Born approximation have limitations that are limited when working with high numerical apertures. This paper presents sCWatter, an open-source tool that utilizes coupled wave theory (CWT) to simulate and visualize the 3D electric field scattered by complex samples. The sample refractive index is specified on a volumetric grid, while the incident field is provided as a 2D image orthogonal to the optical path. We introduce connection equations between layers that significantly reduce the dimensionality of the CW linear system, enabling efficient parallel processing on consumer hardware. Further optimizations using Intel MKL and CUDA significantly accelerate both field simulation and visualization.","sentences":["Several emerging microscopy imaging methods rely on complex interactions between the incident light and the sample.","These include interferometry, spectroscopy, and nonlinear optics.","Reconstructing a sample from the measured scattered field relies on fast and accurate optical models.","Fast approaches like ray tracing and the Born approximation have limitations that are limited when working with high numerical apertures.","This paper presents sCWatter, an open-source tool that utilizes coupled wave theory (CWT) to simulate and visualize the 3D electric field scattered by complex samples.","The sample refractive index is specified on a volumetric grid, while the incident field is provided as a 2D image orthogonal to the optical path.","We introduce connection equations between layers that significantly reduce the dimensionality of the CW linear system, enabling efficient parallel processing on consumer hardware.","Further optimizations using Intel MKL and CUDA significantly accelerate both field simulation and visualization."],"url":"http://arxiv.org/abs/2404.07293v1","category":"physics.optics"}
{"created":"2024-04-10 18:36:37","title":"Quantifying the Errors Introduced by Continuum Scattering Models on the Inferred Structural Properties of Proteins","abstract":"Atomistic force fields that are tuned to describe folded proteins predict overly compact structures for intrinsically disordered proteins (IDPs). To correct this, improvements in force fields to better model IDPs are usually paired with scattering models for validation against experiments. For scattering calculations, protein configurations from all-atom simulations are used within the continuum-solvent model CRYSOL for comparison with experiments. To check this approach, we develop an equation to evaluate the radius of gyration (Rg) for any defined inner-hydration shell thickness given all-atom simulation data. Rg based on an explicit description of hydration waters compares well with the reference value of Rg obtained using Guinier analysis of the all-atom scattering model. However, these internally consistent estimates disagree with Rg from CRYSOL for the same definition of the inner-shell. CRYSOL can over-predict Rg by up to 2.5 Angstroms. We rationalize the reason for this behavior and highlight the consequences for force field design.","sentences":["Atomistic force fields that are tuned to describe folded proteins predict overly compact structures for intrinsically disordered proteins (IDPs).","To correct this, improvements in force fields to better model IDPs are usually paired with scattering models for validation against experiments.","For scattering calculations, protein configurations from all-atom simulations are used within the continuum-solvent model CRYSOL for comparison with experiments.","To check this approach, we develop an equation to evaluate the radius of gyration (Rg) for any defined inner-hydration shell thickness given all-atom simulation data.","Rg based on an explicit description of hydration waters compares well with the reference value of Rg obtained using Guinier analysis of the all-atom scattering model.","However, these internally consistent estimates disagree with Rg from CRYSOL for the same definition of the inner-shell.","CRYSOL can over-predict Rg by up to 2.5 Angstroms.","We rationalize the reason for this behavior and highlight the consequences for force field design."],"url":"http://arxiv.org/abs/2404.07289v1","category":"physics.chem-ph"}
{"created":"2024-04-10 18:26:53","title":"A Gauss-Bonnet-Chern type obstruction for Killing vector fields on Lorentzian manifolds","abstract":"A new curvature obstruction to the existence of a timelike (resp. causal) Killing or homothetic vector field $X$ on an even-dimensional (odd-dimensional) Lorentzian manifold, in terms of its timelike (resp. null) sectional curvature is given. As a consequence for the compact case, the well-known Gauss-Bonnet-Chern obstruction to the existence of semi-Riemannian metrics is extended from non-zero constant sectional curvature to non-zero timelike sectional curvature on $X$.","sentences":["A new curvature obstruction to the existence of a timelike (resp. causal) Killing or homothetic vector field $X$ on an even-dimensional (odd-dimensional) Lorentzian manifold, in terms of its timelike (resp.","null) sectional curvature is given.","As a consequence for the compact case, the well-known Gauss-Bonnet-Chern obstruction to the existence of semi-Riemannian metrics is extended from non-zero constant sectional curvature to non-zero timelike sectional curvature on $X$."],"url":"http://arxiv.org/abs/2404.07284v1","category":"math.DG"}
{"created":"2024-04-10 18:21:11","title":"Certifying almost all quantum states with few single-qubit measurements","abstract":"Certifying that an n-qubit state synthesized in the lab is close to the target state is a fundamental task in quantum information science. However, existing rigorous protocols either require deep quantum circuits or exponentially many single-qubit measurements. In this work, we prove that almost all n-qubit target states, including those with exponential circuit complexity, can be certified from only O(n^2) single-qubit measurements. This result is established by a new technique that relates certification to the mixing time of a random walk. Our protocol has applications for benchmarking quantum systems, for optimizing quantum circuits to generate a desired target state, and for learning and verifying neural networks, tensor networks, and various other representations of quantum states using only single-qubit measurements. We show that such verified representations can be used to efficiently predict highly non-local properties that would otherwise require an exponential number of measurements. We demonstrate these applications in numerical experiments with up to 120 qubits, and observe advantage over existing methods such as cross-entropy benchmarking (XEB).","sentences":["Certifying that an n-qubit state synthesized in the lab is close to the target state is a fundamental task in quantum information science.","However, existing rigorous protocols either require deep quantum circuits or exponentially many single-qubit measurements.","In this work, we prove that almost all n-qubit target states, including those with exponential circuit complexity, can be certified from only O(n^2) single-qubit measurements.","This result is established by a new technique that relates certification to the mixing time of a random walk.","Our protocol has applications for benchmarking quantum systems, for optimizing quantum circuits to generate a desired target state, and for learning and verifying neural networks, tensor networks, and various other representations of quantum states using only single-qubit measurements.","We show that such verified representations can be used to efficiently predict highly non-local properties that would otherwise require an exponential number of measurements.","We demonstrate these applications in numerical experiments with up to 120 qubits, and observe advantage over existing methods such as cross-entropy benchmarking (XEB)."],"url":"http://arxiv.org/abs/2404.07281v1","category":"quant-ph"}
{"created":"2024-04-10 18:10:26","title":"Well-posedness for integro-differential sweeping processes of Volterra type","abstract":"In this paper, we study the well-posedness of integro-differential sweeping processes of Volterra type. Using new enhanced versions of Gronwall's inequality, a reparametrization technique, and a fixed point argument for history-dependent operators, we obtain the existence of solutions and provide a fully continuous dependence result for the integro-differential sweeping process. The paper ends with an application to projected dynamical systems.","sentences":["In this paper, we study the well-posedness of integro-differential sweeping processes of Volterra type.","Using new enhanced versions of Gronwall's inequality, a reparametrization technique, and a fixed point argument for history-dependent operators, we obtain the existence of solutions and provide a fully continuous dependence result for the integro-differential sweeping process.","The paper ends with an application to projected dynamical systems."],"url":"http://arxiv.org/abs/2404.07279v1","category":"math.OC"}
{"created":"2024-04-10 18:00:54","title":"Comparing Compressed and Full-modeling Analyses with FOLPS: Implications for DESI 2024 and beyond","abstract":"The Dark Energy Spectroscopic Instrument (DESI) will provide unprecedented information about the large-scale structure of our Universe. In this work, we study the robustness of the theoretical modelling of the power spectrum of FOLPS, a novel effective field theory-based package for evaluating the redshift space power spectrum in the presence of massive neutrinos. We perform this validation by fitting the AbacusSummit high-accuracy $N$-body simulations for Luminous Red Galaxies, Emission Line Galaxies and Quasar tracers, calibrated to describe DESI observations. We quantify the potential systematic error budget of FOLPS, finding that the modelling errors are fully sub-dominant for the DESI statistical precision within the studied range of scales. Additionally, we study two complementary approaches to fit and analyse the power spectrum data, one based on direct Full-Modelling fits and the other on the ShapeFit compression variables, both resulting in very good agreement in precision and accuracy. In each of these approaches, we study a set of potential systematic errors induced by several assumptions, such as the choice of template cosmology, the effect of prior choice in the nuisance parameters of the model, or the range of scales used in the analysis. Furthermore, we show how opening up the parameter space beyond the vanilla $\\Lambda$CDM model affects the DESI observables. These studies include the addition of massive neutrinos, spatial curvature, and dark energy equation of state. We also examine how relaxing the usual Cosmic Microwave Background and Big Bang Nucleosynthesis priors on the primordial spectral index and the baryonic matter abundance, respectively, impacts the inference on the rest of the parameters of interest. This paper pathways towards performing a robust and reliable analysis of the shape of the power spectrum of DESI galaxy and quasar clustering using FOLPS.","sentences":["The Dark Energy Spectroscopic Instrument (DESI) will provide unprecedented information about the large-scale structure of our Universe.","In this work, we study the robustness of the theoretical modelling of the power spectrum of FOLPS, a novel effective field theory-based package for evaluating the redshift space power spectrum in the presence of massive neutrinos.","We perform this validation by fitting the AbacusSummit high-accuracy $N$-body simulations for Luminous Red Galaxies, Emission Line Galaxies and Quasar tracers, calibrated to describe DESI observations.","We quantify the potential systematic error budget of FOLPS, finding that the modelling errors are fully sub-dominant for the DESI statistical precision within the studied range of scales.","Additionally, we study two complementary approaches to fit and analyse the power spectrum data, one based on direct Full-Modelling fits and the other on the ShapeFit compression variables, both resulting in very good agreement in precision and accuracy.","In each of these approaches, we study a set of potential systematic errors induced by several assumptions, such as the choice of template cosmology, the effect of prior choice in the nuisance parameters of the model, or the range of scales used in the analysis.","Furthermore, we show how opening up the parameter space beyond the vanilla $\\Lambda$CDM model affects the DESI observables.","These studies include the addition of massive neutrinos, spatial curvature, and dark energy equation of state.","We also examine how relaxing the usual Cosmic Microwave Background and Big Bang Nucleosynthesis priors on the primordial spectral index and the baryonic matter abundance, respectively, impacts the inference on the rest of the parameters of interest.","This paper pathways towards performing a robust and reliable analysis of the shape of the power spectrum of DESI galaxy and quasar clustering using FOLPS."],"url":"http://arxiv.org/abs/2404.07269v1","category":"astro-ph.CO"}
{"created":"2024-04-10 18:00:05","title":"The planted directed polymer: inferring a random walk from noisy images","abstract":"We introduce and study the planted directed polymer, in which the path of a random walker is inferred from noisy 'images' accumulated at each timestep. Formulated as a nonlinear problem of Bayesian inference for a hidden Markov model, this problem is a generalization of the directed polymer problem of statistical physics, coinciding with it in the limit of zero signal to noise. For a 1D walker we present numerical investigations and analytical arguments that no phase transition is present. When formulated on a Cayley tree, methods developed for the directed polymer are used to show that there is a transition with decreasing signal to noise where effective inference becomes impossible, meaning that the average fractional overlap between the inferred and true paths falls from one to zero.","sentences":["We introduce and study the planted directed polymer, in which the path of a random walker is inferred from noisy 'images' accumulated at each timestep.","Formulated as a nonlinear problem of Bayesian inference for a hidden Markov model, this problem is a generalization of the directed polymer problem of statistical physics, coinciding with it in the limit of zero signal to noise.","For a 1D walker we present numerical investigations and analytical arguments that no phase transition is present.","When formulated on a Cayley tree, methods developed for the directed polymer are used to show that there is a transition with decreasing signal to noise where effective inference becomes impossible, meaning that the average fractional overlap between the inferred and true paths falls from one to zero."],"url":"http://arxiv.org/abs/2404.07263v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-11 17:59:59","title":"GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo","abstract":"Matching cost aggregation plays a fundamental role in learning-based multi-view stereo networks. However, directly aggregating adjacent costs can lead to suboptimal results due to local geometric inconsistency. Related methods either seek selective aggregation or improve aggregated depth in the 2D space, both are unable to handle geometric inconsistency in the cost volume effectively. In this paper, we propose GoMVS to aggregate geometrically consistent costs, yielding better utilization of adjacent geometries. More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals. We achieve this by the geometric consistent propagation (GCP) module. It computes the correspondence from the adjacent depth hypothesis space to the reference depth space using surface normals, then uses the correspondence to propagate adjacent costs to the reference geometry, followed by a convolution for aggregation. Our method achieves new state-of-the-art performance on DTU, Tanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks & Temple Advanced benchmark.","sentences":["Matching cost aggregation plays a fundamental role in learning-based multi-view stereo networks.","However, directly aggregating adjacent costs can lead to suboptimal results due to local geometric inconsistency.","Related methods either seek selective aggregation or improve aggregated depth in the 2D space, both are unable to handle geometric inconsistency in the cost volume effectively.","In this paper, we propose GoMVS to aggregate geometrically consistent costs, yielding better utilization of adjacent geometries.","More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals.","We achieve this by the geometric consistent propagation (GCP) module.","It computes the correspondence from the adjacent depth hypothesis space to the reference depth space using surface normals, then uses the correspondence to propagate adjacent costs to the reference geometry, followed by a convolution for aggregation.","Our method achieves new state-of-the-art performance on DTU, Tanks & Temple, and ETH3D datasets.","Notably, our method ranks 1st on the Tanks & Temple Advanced benchmark."],"url":"http://arxiv.org/abs/2404.07992v1","category":"cs.CV"}
{"created":"2024-04-11 17:58:06","title":"Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning","abstract":"Contrastive vision-language models like CLIP have gained popularity for their versatile applicable learned representations in various downstream tasks. Despite their successes in some tasks, like zero-shot image recognition, they also perform surprisingly poor on other tasks, like attribute detection. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and a bias towards objects over other factors, such as attributes. In this work we investigate both phenomena. We find that only a few embedding dimensions drive the modality gap. Further, we propose a measure for object bias and find that object bias does not lead to worse performance on other concepts, such as attributes. But what leads to the emergence of the modality gap and object bias? To answer this question we carefully designed an experimental setting which allows us to control the amount of shared information between the modalities. This revealed that the driving factor behind both, the modality gap and the object bias, is the information imbalance between images and captions.","sentences":["Contrastive vision-language models like CLIP have gained popularity for their versatile applicable learned representations in various downstream tasks.","Despite their successes in some tasks, like zero-shot image recognition, they also perform surprisingly poor on other tasks, like attribute detection.","Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and a bias towards objects over other factors, such as attributes.","In this work we investigate both phenomena.","We find that only a few embedding dimensions drive the modality gap.","Further, we propose a measure for object bias and find that object bias does not lead to worse performance on other concepts, such as attributes.","But what leads to the emergence of the modality gap and object bias?","To answer this question we carefully designed an experimental setting which allows us to control the amount of shared information between the modalities.","This revealed that the driving factor behind both, the modality gap and the object bias, is the information imbalance between images and captions."],"url":"http://arxiv.org/abs/2404.07983v1","category":"cs.CV"}
{"created":"2024-04-11 17:58:05","title":"Language Imbalance Can Boost Cross-lingual Generalisation","abstract":"Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive.","sentences":["Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities.","To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others.","Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment.","In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance.","In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages.","Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split.","Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data.","As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive."],"url":"http://arxiv.org/abs/2404.07982v1","category":"cs.CL"}
{"created":"2024-04-11 17:56:05","title":"Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models","abstract":"While Ferret seamlessly integrates regional understanding into the Large Language Model (LLM) to facilitate its referring and grounding capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks. In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs. (1) Any resolution grounding and referring: A flexible approach that effortlessly handles higher image resolution, improving the model's ability to process and understand images in greater detail. (2) Multi-granularity visual encoding: By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information. (3) A three-stage training paradigm: Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final instruction tuning. Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing.","sentences":["While Ferret seamlessly integrates regional understanding into the Large Language Model (LLM) to facilitate its referring and grounding capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks.","In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs.","(1) Any resolution grounding and referring: A flexible approach that effortlessly handles higher image resolution, improving the model's ability to process and understand images in greater detail.","(2) Multi-granularity visual encoding:","By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information.","(3) A three-stage training paradigm:","Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final instruction tuning.","Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing."],"url":"http://arxiv.org/abs/2404.07973v1","category":"cs.CV"}
{"created":"2024-04-11 15:55:53","title":"Backdoor Contrastive Learning via Bi-level Trigger Optimization","abstract":"Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., $99\\%$ ASR on ImageNet-100) with a very low poisoning rate ($1\\%$). Besides, our attack can effectively evade existing state-of-the-art defenses. Code is available at: https://github.com/SWY666/SSL-backdoor-BLTO.","sentences":["Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning.","However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target.","Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class.","However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR).","This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework.","In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure.","Extensive experiments show that our attack can achieve a higher attack success rate (e.g., $99\\%$ ASR on ImageNet-100) with a very low poisoning rate ($1\\%$).","Besides, our attack can effectively evade existing state-of-the-art defenses.","Code is available at: https://github.com/SWY666/SSL-backdoor-BLTO."],"url":"http://arxiv.org/abs/2404.07863v1","category":"cs.CR"}
{"created":"2024-04-11 15:43:11","title":"Overparameterized Multiple Linear Regression as Hyper-Curve Fitting","abstract":"The paper shows that the application of the fixed-effect multiple linear regression model to an overparameterized dataset is equivalent to fitting the data with a hyper-curve parameterized by a single scalar parameter. This equivalence allows for a predictor-focused approach, where each predictor is described by a function of the chosen parameter. It is proven that a linear model will produce exact predictions even in the presence of nonlinear dependencies that violate the model assumptions. Parameterization in terms of the dependent variable and the monomial basis in the predictor function space are applied here to both synthetic and experimental data. The hyper-curve approach is especially suited for the regularization of problems with noise in predictor variables and can be used to remove noisy and \"improper\" predictors from the model.","sentences":["The paper shows that the application of the fixed-effect multiple linear regression model to an overparameterized dataset is equivalent to fitting the data with a hyper-curve parameterized by a single scalar parameter.","This equivalence allows for a predictor-focused approach, where each predictor is described by a function of the chosen parameter.","It is proven that a linear model will produce exact predictions even in the presence of nonlinear dependencies that violate the model assumptions.","Parameterization in terms of the dependent variable and the monomial basis in the predictor function space are applied here to both synthetic and experimental data.","The hyper-curve approach is especially suited for the regularization of problems with noise in predictor variables and can be used to remove noisy and \"improper\" predictors from the model."],"url":"http://arxiv.org/abs/2404.07849v1","category":"stat.ML"}
{"created":"2024-04-11 14:31:11","title":"VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing","abstract":"Image dehazing poses significant challenges in environmental perception. Recent research mainly focus on deep learning-based methods with single modality, while they may result in severe information loss especially in dense-haze scenarios. The infrared image exhibits robustness to the haze, however, existing methods have primarily treated the infrared modality as auxiliary information, failing to fully explore its rich information in dehazing. To address this challenge, the key insight of this study is to design a visible-infrared fusion network for image dehazing. In particular, we propose a multi-scale Deep Structure Feature Extraction (DSFE) module, which incorporates the Channel-Pixel Attention Block (CPAB) to restore more spatial and marginal information within the deep structural features. Additionally, we introduce an inconsistency weighted fusion strategy to merge the two modalities by leveraging the more reliable information. To validate this, we construct a visible-infrared multimodal dataset called AirSim-VID based on the AirSim simulation platform. Extensive experiments performed on challenging real and simulated image datasets demonstrate that VIFNet can outperform many state-of-the-art competing methods. The code and dataset are available at https://github.com/mengyu212/VIFNet_dehazing.","sentences":["Image dehazing poses significant challenges in environmental perception.","Recent research mainly focus on deep learning-based methods with single modality, while they may result in severe information loss especially in dense-haze scenarios.","The infrared image exhibits robustness to the haze, however, existing methods have primarily treated the infrared modality as auxiliary information, failing to fully explore its rich information in dehazing.","To address this challenge, the key insight of this study is to design a visible-infrared fusion network for image dehazing.","In particular, we propose a multi-scale Deep Structure Feature Extraction (DSFE) module, which incorporates the Channel-Pixel Attention Block (CPAB) to restore more spatial and marginal information within the deep structural features.","Additionally, we introduce an inconsistency weighted fusion strategy to merge the two modalities by leveraging the more reliable information.","To validate this, we construct a visible-infrared multimodal dataset called AirSim-VID based on the AirSim simulation platform.","Extensive experiments performed on challenging real and simulated image datasets demonstrate that VIFNet can outperform many state-of-the-art competing methods.","The code and dataset are available at https://github.com/mengyu212/VIFNet_dehazing."],"url":"http://arxiv.org/abs/2404.07790v1","category":"cs.CV"}
{"created":"2024-04-11 13:37:51","title":"Exploiting Object-based and Segmentation-based Semantic Features for Deep Learning-based Indoor Scene Classification","abstract":"Indoor scenes are usually characterized by scattered objects and their relationships, which turns the indoor scene classification task into a challenging computer vision task. Despite the significant performance boost in classification tasks achieved in recent years, provided by the use of deep-learning-based methods, limitations such as inter-category ambiguity and intra-category variation have been holding back their performance. To overcome such issues, gathering semantic information has been shown to be a promising source of information towards a more complete and discriminative feature representation of indoor scenes. Therefore, the work described in this paper uses both semantic information, obtained from object detection, and semantic segmentation techniques. While object detection techniques provide the 2D location of objects allowing to obtain spatial distributions between objects, semantic segmentation techniques provide pixel-level information that allows to obtain, at a pixel-level, a spatial distribution and shape-related features of the segmentation categories. Hence, a novel approach that uses a semantic segmentation mask to provide Hu-moments-based segmentation categories' shape characterization, designated by Segmentation-based Hu-Moments Features (SHMFs), is proposed. Moreover, a three-main-branch network, designated by GOS$^2$F$^2$App, that exploits deep-learning-based global features, object-based features, and semantic segmentation-based features is also proposed. GOS$^2$F$^2$App was evaluated in two indoor scene benchmark datasets: SUN RGB-D and NYU Depth V2, where, to the best of our knowledge, state-of-the-art results were achieved on both datasets, which present evidences of the effectiveness of the proposed approach.","sentences":["Indoor scenes are usually characterized by scattered objects and their relationships, which turns the indoor scene classification task into a challenging computer vision task.","Despite the significant performance boost in classification tasks achieved in recent years, provided by the use of deep-learning-based methods, limitations such as inter-category ambiguity and intra-category variation have been holding back their performance.","To overcome such issues, gathering semantic information has been shown to be a promising source of information towards a more complete and discriminative feature representation of indoor scenes.","Therefore, the work described in this paper uses both semantic information, obtained from object detection, and semantic segmentation techniques.","While object detection techniques provide the 2D location of objects allowing to obtain spatial distributions between objects, semantic segmentation techniques provide pixel-level information that allows to obtain, at a pixel-level, a spatial distribution and shape-related features of the segmentation categories.","Hence, a novel approach that uses a semantic segmentation mask to provide Hu-moments-based segmentation categories' shape characterization, designated by Segmentation-based Hu-Moments Features (SHMFs), is proposed.","Moreover, a three-main-branch network, designated by GOS$^2$F$^2$App, that exploits deep-learning-based global features, object-based features, and semantic segmentation-based features is also proposed.","GOS$^2$F$^2$App was evaluated in two indoor scene benchmark datasets: SUN RGB-D and NYU Depth V2, where, to the best of our knowledge, state-of-the-art results were achieved on both datasets, which present evidences of the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2404.07739v1","category":"cs.CV"}
{"created":"2024-04-11 12:59:38","title":"Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning","abstract":"Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer semantic knowledge from seen classes to unseen ones, supported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for representing semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we propose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly considers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specifically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual information for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.","sentences":["Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer semantic knowledge from seen classes to unseen ones, supported by semantic information (e.g., attributes).","However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for representing semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions.","To tackle this issue, we propose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT).","ZSLViT mainly considers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information.","Specifically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention.","Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual information for visual enhancement.","These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL.","The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2."],"url":"http://arxiv.org/abs/2404.07713v1","category":"cs.CV"}
{"created":"2024-04-11 06:34:17","title":"Best Practices and Lessons Learned on Synthetic Data for Language Models","abstract":"The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.","sentences":["The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs.","Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns.","This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions.","We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness.","We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models."],"url":"http://arxiv.org/abs/2404.07503v1","category":"cs.CL"}
{"created":"2024-04-11 05:51:06","title":"Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition","abstract":"Skeleton-based zero-shot action recognition aims to recognize unknown human actions based on the learned priors of the known skeleton-based actions and a semantic descriptor space shared by both known and unknown categories. However, previous works focus on establishing the bridges between the known skeleton representation space and semantic descriptions space at the coarse-grained level for recognizing unknown action categories, ignoring the fine-grained alignment of these two spaces, resulting in suboptimal performance in distinguishing high-similarity action categories. To address these challenges, we propose a novel method via Side information and dual-prompts learning for skeleton-based zero-shot action recognition (STAR) at the fine-grained level. Specifically, 1) we decompose the skeleton into several parts based on its topology structure and introduce the side information concerning multi-part descriptions of human body movements for alignment between the skeleton and the semantic space at the fine-grained level; 2) we design the visual-attribute and semantic-part prompts to improve the intra-class compactness within the skeleton space and inter-class separability within the semantic space, respectively, to distinguish the high-similarity actions. Extensive experiments show that our method achieves state-of-the-art performance in ZSL and GZSL settings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.","sentences":["Skeleton-based zero-shot action recognition aims to recognize unknown human actions based on the learned priors of the known skeleton-based actions and a semantic descriptor space shared by both known and unknown categories.","However, previous works focus on establishing the bridges between the known skeleton representation space and semantic descriptions space at the coarse-grained level for recognizing unknown action categories, ignoring the fine-grained alignment of these two spaces, resulting in suboptimal performance in distinguishing high-similarity action categories.","To address these challenges, we propose a novel method via Side information and dual-prompts learning for skeleton-based zero-shot action recognition (STAR) at the fine-grained level.","Specifically, 1) we decompose the skeleton into several parts based on its topology structure and introduce the side information concerning multi-part descriptions of human body movements for alignment between the skeleton and the semantic space at the fine-grained level; 2) we design the visual-attribute and semantic-part prompts to improve the intra-class compactness within the skeleton space and inter-class separability within the semantic space, respectively, to distinguish the high-similarity actions.","Extensive experiments show that our method achieves state-of-the-art performance in ZSL and GZSL settings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets."],"url":"http://arxiv.org/abs/2404.07487v1","category":"cs.CV"}
{"created":"2024-04-11 03:09:34","title":"Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs","abstract":"Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.","sentences":["Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA).","However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness.","Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location.","In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs.","We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs.","Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions.","Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework."],"url":"http://arxiv.org/abs/2404.07449v1","category":"cs.CV"}
{"created":"2024-04-11 02:29:08","title":"Encoding Urban Ecologies: Automated Building Archetype Generation through Self-Supervised Learning for Energy Modeling","abstract":"As the global population and urbanization expand, the building sector has emerged as the predominant energy consumer and carbon emission contributor. The need for innovative Urban Building Energy Modeling grows, yet existing building archetypes often fail to capture the unique attributes of local buildings and the nuanced distinctions between different cities, jeopardizing the precision of energy modeling. This paper presents an alternative tool employing self-supervised learning to distill complex geometric data into representative, locale-specific archetypes. This study attempts to foster a new paradigm of interaction with built environments, incorporating local parameters to conduct bespoke energy simulations at the community level. The catered archetypes can augment the precision and applicability of energy consumption modeling at different scales across diverse building inventories. This tool provides a potential solution that encourages the exploration of emerging local ecologies. By integrating building envelope characteristics and cultural granularity into the building archetype generation process, we seek a future where architecture and urban design are intricately interwoven with the energy sector in shaping our built environments.","sentences":["As the global population and urbanization expand, the building sector has emerged as the predominant energy consumer and carbon emission contributor.","The need for innovative Urban Building Energy Modeling grows, yet existing building archetypes often fail to capture the unique attributes of local buildings and the nuanced distinctions between different cities, jeopardizing the precision of energy modeling.","This paper presents an alternative tool employing self-supervised learning to distill complex geometric data into representative, locale-specific archetypes.","This study attempts to foster a new paradigm of interaction with built environments, incorporating local parameters to conduct bespoke energy simulations at the community level.","The catered archetypes can augment the precision and applicability of energy consumption modeling at different scales across diverse building inventories.","This tool provides a potential solution that encourages the exploration of emerging local ecologies.","By integrating building envelope characteristics and cultural granularity into the building archetype generation process, we seek a future where architecture and urban design are intricately interwoven with the energy sector in shaping our built environments."],"url":"http://arxiv.org/abs/2404.07435v1","category":"cs.CV"}
{"created":"2024-04-11 00:45:10","title":"Simplifying Two-Stage Detectors for On-Device Inference in Remote Sensing","abstract":"Deep learning has been successfully applied to object detection from remotely sensed images. Images are typically processed on the ground rather than on-board due to the computation power of the ground system. Such offloaded processing causes delays in acquiring target mission information, which hinders its application to real-time use cases. For on-device object detection, researches have been conducted on designing efficient detectors or model compression to reduce inference latency. However, highly accurate two-stage detectors still need further exploitation for acceleration. In this paper, we propose a model simplification method for two-stage object detectors. Instead of constructing a general feature pyramid, we utilize only one feature extraction in the two-stage detector. To compensate for the accuracy drop, we apply a high pass filter to the RPN's score map. Our approach is applicable to any two-stage detector using a feature pyramid network. In the experiments with state-of-the-art two-stage detectors such as ReDet, Oriented-RCNN, and LSKNet, our method reduced computation costs upto 61.2% with the accuracy loss within 2.1% on the DOTAv1.5 dataset. Source code will be released.","sentences":["Deep learning has been successfully applied to object detection from remotely sensed images.","Images are typically processed on the ground rather than on-board due to the computation power of the ground system.","Such offloaded processing causes delays in acquiring target mission information, which hinders its application to real-time use cases.","For on-device object detection, researches have been conducted on designing efficient detectors or model compression to reduce inference latency.","However, highly accurate two-stage detectors still need further exploitation for acceleration.","In this paper, we propose a model simplification method for two-stage object detectors.","Instead of constructing a general feature pyramid, we utilize only one feature extraction in the two-stage detector.","To compensate for the accuracy drop, we apply a high pass filter to the RPN's score map.","Our approach is applicable to any two-stage detector using a feature pyramid network.","In the experiments with state-of-the-art two-stage detectors such as ReDet, Oriented-RCNN, and LSKNet, our method reduced computation costs upto 61.2% with the accuracy loss within 2.1% on the DOTAv1.5 dataset.","Source code will be released."],"url":"http://arxiv.org/abs/2404.07405v1","category":"cs.CV"}
{"created":"2024-04-11 00:11:32","title":"Mediated probabilities of causation","abstract":"We propose a set of causal estimands that we call ``the mediated probabilities of causation.'' These estimands quantify the probabilities that an observed negative outcome was induced via a mediating pathway versus a direct pathway in a stylized setting involving a binary exposure or intervention, a single binary mediator, and a binary outcome. We outline a set of conditions sufficient to identify these effects given observed data, and propose a doubly-robust projection based estimation strategy that allows for the use of flexible non-parametric and machine learning methods for estimation. We argue that these effects may be more relevant than the probability of causation, particularly in settings where we observe both some negative outcome and negative mediating event, and we wish to distinguish between settings where the outcome was induced via the exposure inducing the mediator versus the exposure inducing the outcome directly. We motivate our quantities of interest by discussing applications to legal and medical questions of causal attribution.","sentences":["We propose a set of causal estimands that we call ``the mediated probabilities of causation.''","These estimands quantify the probabilities that an observed negative outcome was induced via a mediating pathway versus a direct pathway in a stylized setting involving a binary exposure or intervention, a single binary mediator, and a binary outcome.","We outline a set of conditions sufficient to identify these effects given observed data, and propose a doubly-robust projection based estimation strategy that allows for the use of flexible non-parametric and machine learning methods for estimation.","We argue that these effects may be more relevant than the probability of causation, particularly in settings where we observe both some negative outcome and negative mediating event, and we wish to distinguish between settings where the outcome was induced via the exposure inducing the mediator versus the exposure inducing the outcome directly.","We motivate our quantities of interest by discussing applications to legal and medical questions of causal attribution."],"url":"http://arxiv.org/abs/2404.07397v1","category":"stat.ME"}
{"created":"2024-04-11 00:02:57","title":"Global versus Local: Evaluating AlexNet Architectures for Tropical Cyclone Intensity Estimation","abstract":"Given the destructive impacts of tropical cyclones, it is critical to have a reliable system for cyclone intensity detection. Various techniques are available for this purpose, each with differing levels of accuracy. In this paper, we introduce two ensemble-based models based on AlexNet architecture to estimate tropical cyclone intensity using visible satellite images. The first model, trained on the entire dataset, is called the global AlexNet model. The second model is a distributed version of AlexNet in which multiple AlexNets are trained separately on subsets of the training data categorized according to the Saffir-Simpson wind speed scale prescribed by the meterologists. We evaluated the performance of both models against a deep learning benchmark model called \\textit{Deepti} using a publicly available cyclone image dataset. Results indicate that both the global model (with a root mean square error (RMSE) of 9.03 knots) and the distributed model (with a RMSE of 9.3 knots) outperform the benchmark model (with a RMSE of 13.62 knots). We provide a thorough discussion of our solution approach, including an explanantion of the AlexNet's performance using gradient class activation maps (grad-CAM). Our proposed solution strategy allows future experimentation with various deep learning models in both single and multi-channel settings.","sentences":["Given the destructive impacts of tropical cyclones, it is critical to have a reliable system for cyclone intensity detection.","Various techniques are available for this purpose, each with differing levels of accuracy.","In this paper, we introduce two ensemble-based models based on AlexNet architecture to estimate tropical cyclone intensity using visible satellite images.","The first model, trained on the entire dataset, is called the global AlexNet model.","The second model is a distributed version of AlexNet in which multiple AlexNets are trained separately on subsets of the training data categorized according to the Saffir-Simpson wind speed scale prescribed by the meterologists.","We evaluated the performance of both models against a deep learning benchmark model called \\textit{Deepti} using a publicly available cyclone image dataset.","Results indicate that both the global model (with a root mean square error (RMSE) of 9.03 knots) and the distributed model (with a RMSE of 9.3 knots) outperform the benchmark model (with a RMSE of 13.62 knots).","We provide a thorough discussion of our solution approach, including an explanantion of the AlexNet's performance using gradient class activation maps (grad-CAM).","Our proposed solution strategy allows future experimentation with various deep learning models in both single and multi-channel settings."],"url":"http://arxiv.org/abs/2404.07395v1","category":"cs.LG"}
{"created":"2024-04-10 22:53:04","title":"Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS","abstract":"Exploring the structural, chemical, and physical properties of matter on the nano- and atomic scales has become possible with the recent advances in aberration-corrected electron energy-loss spectroscopy (EELS) in scanning transmission electron microscopy (STEM). However, the current paradigm of STEM-EELS relies on the classical rectangular grid sampling, in which all surface regions are assumed to be of equal a priori interest. This is typically not the case for real-world scenarios, where phenomena of interest are concentrated in a small number of spatial locations. One of foundational problems is the discovery of nanometer- or atomic scale structures having specific signatures in EELS spectra. Here we systematically explore the hyperparameters controlling deep kernel learning (DKL) discovery workflows for STEM-EELS and identify the role of the local structural descriptors and acquisition functions on the experiment progression. In agreement with actual experiment, we observe that for certain parameter combinations the experiment path can be trapped in the local minima. We demonstrate the approaches for monitoring automated experiment in the real and feature space of the system and monitor knowledge acquisition of the DKL model. Based on these, we construct intervention strategies, thus defining human-in the loop automated experiment (hAE). This approach can be further extended to other techniques including 4D STEM and other forms of spectroscopic imaging.","sentences":["Exploring the structural, chemical, and physical properties of matter on the nano- and atomic scales has become possible with the recent advances in aberration-corrected electron energy-loss spectroscopy (EELS) in scanning transmission electron microscopy (STEM).","However, the current paradigm of STEM-EELS relies on the classical rectangular grid sampling, in which all surface regions are assumed to be of equal a priori interest.","This is typically not the case for real-world scenarios, where phenomena of interest are concentrated in a small number of spatial locations.","One of foundational problems is the discovery of nanometer- or atomic scale structures having specific signatures in EELS spectra.","Here we systematically explore the hyperparameters controlling deep kernel learning (DKL) discovery workflows for STEM-EELS and identify the role of the local structural descriptors and acquisition functions on the experiment progression.","In agreement with actual experiment, we observe that for certain parameter combinations the experiment path can be trapped in the local minima.","We demonstrate the approaches for monitoring automated experiment in the real and feature space of the system and monitor knowledge acquisition of the DKL model.","Based on these, we construct intervention strategies, thus defining human-in the loop automated experiment (hAE).","This approach can be further extended to other techniques including 4D STEM and other forms of spectroscopic imaging."],"url":"http://arxiv.org/abs/2404.07381v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 22:16:20","title":"Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning","abstract":"Generative Adversarial Network (GAN)-based synthesis of fat suppressed (FS) MRIs from non-FS proton density sequences has the potential to accelerate acquisition of knee MRIs. However, GANs trained on single-site data have poor generalizability to external data. We show that federated learning can improve multi-center generalizability of GANs for synthesizing FS MRIs, while facilitating privacy-preserving multi-institutional collaborations.","sentences":["Generative Adversarial Network (GAN)-based synthesis of fat suppressed (FS) MRIs from non-FS proton density sequences has the potential to accelerate acquisition of knee MRIs.","However, GANs trained on single-site data have poor generalizability to external data.","We show that federated learning can improve multi-center generalizability of GANs for synthesizing FS MRIs, while facilitating privacy-preserving multi-institutional collaborations."],"url":"http://arxiv.org/abs/2404.07374v1","category":"eess.IV"}
{"created":"2024-04-10 21:19:33","title":"FairEM360: A Suite for Responsible Entity Matching","abstract":"Entity matching is one the earliest tasks that occur in the big data pipeline and is alarmingly exposed to unintentional biases that affect the quality of data. Identifying and mitigating the biases that exist in the data or are introduced by the matcher at this stage can contribute to promoting fairness in downstream tasks. This demonstration showcases FairEM360, a framework for 1) auditing the output of entity matchers across a wide range of fairness measures and paradigms, 2) providing potential explanations for the underlying reasons for unfairness, and 3) providing resolutions for the unfairness issues through an exploratory process with human-in-the-loop feedback, utilizing an ensemble of matchers. We aspire for FairEM360 to contribute to the prioritization of fairness as a key consideration in the evaluation of EM pipelines.","sentences":["Entity matching is one the earliest tasks that occur in the big data pipeline and is alarmingly exposed to unintentional biases that affect the quality of data.","Identifying and mitigating the biases that exist in the data or are introduced by the matcher at this stage can contribute to promoting fairness in downstream tasks.","This demonstration showcases FairEM360, a framework for 1) auditing the output of entity matchers across a wide range of fairness measures and paradigms, 2) providing potential explanations for the underlying reasons for unfairness, and 3) providing resolutions for the unfairness issues through an exploratory process with human-in-the-loop feedback, utilizing an ensemble of matchers.","We aspire for FairEM360 to contribute to the prioritization of fairness as a key consideration in the evaluation of EM pipelines."],"url":"http://arxiv.org/abs/2404.07354v1","category":"cs.DB"}
{"created":"2024-04-10 21:14:33","title":"A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos","abstract":"Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.","sentences":["Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important.","To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior.","However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns.","In this work, we introduce a novel method for simulating human gaze behavior.","Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior.","We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition.","Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input."],"url":"http://arxiv.org/abs/2404.07351v1","category":"cs.CV"}
{"created":"2024-04-10 20:40:24","title":"Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping","abstract":"This paper presents Conformer-1, an end-to-end Automatic Speech Recognition (ASR) model trained on an extensive dataset of 570k hours of speech audio data, 91% of which was acquired from publicly available sources. To achieve this, we perform Noisy Student Training after generating pseudo-labels for the unlabeled public data using a strong Conformer RNN-T baseline model. The addition of these pseudo-labeled data results in remarkable improvements in relative Word Error Rate (WER) by 11.5% and 24.3% for our asynchronous and realtime models, respectively. Additionally, the model is more robust to background noise owing to the addition of these data. The results obtained in this study demonstrate that the incorporation of pseudo-labeled publicly available data is a highly effective strategy for improving ASR accuracy and noise robustness.","sentences":["This paper presents Conformer-1, an end-to-end Automatic Speech Recognition (ASR) model trained on an extensive dataset of 570k hours of speech audio data, 91% of which was acquired from publicly available sources.","To achieve this, we perform Noisy Student Training after generating pseudo-labels for the unlabeled public data using a strong Conformer RNN-T baseline model.","The addition of these pseudo-labeled data results in remarkable improvements in relative Word Error Rate (WER) by 11.5% and 24.3% for our asynchronous and realtime models, respectively.","Additionally, the model is more robust to background noise owing to the addition of these data.","The results obtained in this study demonstrate that the incorporation of pseudo-labeled publicly available data is a highly effective strategy for improving ASR accuracy and noise robustness."],"url":"http://arxiv.org/abs/2404.07341v1","category":"eess.AS"}
{"created":"2024-04-10 20:32:24","title":"PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers' Opinion Scores","abstract":"Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich benchmarks. However, the growth is not attributed solely to models and benchmarks. Universally accepted evaluation metrics also play an important role in advancing the field. While there are many metrics available to evaluate audio and visual content separately, there is a lack of metrics that offer a quantitative and interpretable measure of audio-visual synchronization for videos \"in the wild\". To address this gap, we first created a large scale human annotated dataset (100+ hrs) representing nine types of synchronization errors in audio-visual content and how human perceive them. We then developed a PEAVS (Perceptual Evaluation of Audio-Visual Synchrony) score, a novel automatic metric with a 5-point scale that evaluates the quality of audio-visual synchronization. We validate PEAVS using a newly generated dataset, achieving a Pearson correlation of 0.79 at the set level and 0.54 at the clip level when compared to human labels. In our experiments, we observe a relative gain 50% over a natural extension of Fr\\'echet based metrics for Audio-Visual synchrony, confirming PEAVS efficacy in objectively modeling subjective perceptions of audio-visual synchronization for videos \"in the wild\".","sentences":["Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich benchmarks.","However, the growth is not attributed solely to models and benchmarks.","Universally accepted evaluation metrics also play an important role in advancing the field.","While there are many metrics available to evaluate audio and visual content separately, there is a lack of metrics that offer a quantitative and interpretable measure of audio-visual synchronization for videos \"in the wild\".","To address this gap, we first created a large scale human annotated dataset (100+ hrs) representing nine types of synchronization errors in audio-visual content and how human perceive them.","We then developed a PEAVS (Perceptual Evaluation of Audio-Visual Synchrony) score, a novel automatic metric with a 5-point scale that evaluates the quality of audio-visual synchronization.","We validate PEAVS using a newly generated dataset, achieving a Pearson correlation of 0.79 at the set level and 0.54 at the clip level when compared to human labels.","In our experiments, we observe a relative gain 50% over a natural extension of Fr\\'echet based metrics for Audio-Visual synchrony, confirming PEAVS efficacy in objectively modeling subjective perceptions of audio-visual synchronization for videos \"in the wild\"."],"url":"http://arxiv.org/abs/2404.07336v1","category":"cs.CV"}
{"created":"2024-04-10 19:39:43","title":"Rethinking Perceptual Metrics for Medical Image Translation","abstract":"Modern medical image translation methods use generative models for tasks such as the conversion of CT images to MRI. Evaluating these methods typically relies on some chosen downstream task in the target domain, such as segmentation. On the other hand, task-agnostic metrics are attractive, such as the network feature-based perceptual metrics (e.g., FID) that are common to image translation in general computer vision. In this paper, we investigate evaluation metrics for medical image translation on two medical image translation tasks (GE breast MRI to Siemens breast MRI and lumbar spine MRI to CT), tested on various state-of-the-art translation methods. We show that perceptual metrics do not generally correlate with segmentation metrics due to them extending poorly to the anatomical constraints of this sub-field, with FID being especially inconsistent. However, we find that the lesser-used pixel-level SWD metric may be useful for subtle intra-modality translation. Our results demonstrate the need for further research into helpful metrics for medical image translation.","sentences":["Modern medical image translation methods use generative models for tasks such as the conversion of CT images to MRI.","Evaluating these methods typically relies on some chosen downstream task in the target domain, such as segmentation.","On the other hand, task-agnostic metrics are attractive, such as the network feature-based perceptual metrics (e.g., FID) that are common to image translation in general computer vision.","In this paper, we investigate evaluation metrics for medical image translation on two medical image translation tasks (GE breast MRI to Siemens breast MRI and lumbar spine MRI to CT), tested on various state-of-the-art translation methods.","We show that perceptual metrics do not generally correlate with segmentation metrics due to them extending poorly to the anatomical constraints of this sub-field, with FID being especially inconsistent.","However, we find that the lesser-used pixel-level SWD metric may be useful for subtle intra-modality translation.","Our results demonstrate the need for further research into helpful metrics for medical image translation."],"url":"http://arxiv.org/abs/2404.07318v1","category":"eess.IV"}
{"created":"2024-04-10 19:27:05","title":"Closing the stellar labels gap: Stellar label independent evidence for [$\u03b1$/M] information in $\\textit{Gaia}$ BP/RP spectra","abstract":"Data-driven models for stellar spectra which depend on stellar labels suffer from label systematics which decrease model performance: the \"stellar labels gap\". To close the stellar labels gap, we present a stellar label independent model for $\\textit{Gaia}$ BP/RP (XP) spectra. We develop a novel implementation of a variational auto-encoder; a $\\textit{scatter}$ VAE, which learns to generate an XP spectrum and intrinsic scatter without relying on stellar labels. We demonstrate that our model achieves competitive XP spectra reconstructions in comparison to stellar label dependent models. We find that our model learns stellar properties directly from the data itself. We then apply our model to XP/APOGEE giant stars to study the [$\\alpha$/M] information in $\\textit{Gaia}$ XP. We provide strong evidence that the XP spectra contain meaningful [$\\alpha$/M] information by demonstrating that our model learns the $\\alpha$-bimodality $\\textit{without relying on stellar label correlations}$, for stars with $T_{\\rm eff} <$ 5000 K. We publicly release our trained model, codebase and data. Importantly, our stellar label independent model can be implemented for any/all XP spectra because our model performance scales with training object density, not training label density.","sentences":["Data-driven models for stellar spectra which depend on stellar labels suffer from label systematics which decrease model performance: the \"stellar labels gap\".","To close the stellar labels gap, we present a stellar label independent model for $\\textit{Gaia}$ BP/RP (XP) spectra.","We develop a novel implementation of a variational auto-encoder; a $\\textit{scatter}$ VAE, which learns to generate an XP spectrum and intrinsic scatter without relying on stellar labels.","We demonstrate that our model achieves competitive XP spectra reconstructions in comparison to stellar label dependent models.","We find that our model learns stellar properties directly from the data itself.","We then apply our model to XP/APOGEE giant stars to study the [$\\alpha$/M] information in $\\textit{Gaia}$ XP.","We provide strong evidence that the XP spectra contain meaningful","[$\\alpha$/M] information by demonstrating that our model learns the $\\alpha$-bimodality $\\textit{without relying on stellar label correlations}$, for stars with $T_{\\rm eff} <$ 5000","K. We publicly release our trained model, codebase and data.","Importantly, our stellar label independent model can be implemented for any/all XP spectra because our model performance scales with training object density, not training label density."],"url":"http://arxiv.org/abs/2404.07316v1","category":"astro-ph.SR"}
{"created":"2024-04-10 19:01:44","title":"Transfer Learning via Latent Dependency Factor for Estimating PM 2.5","abstract":"Air pollution, especially particulate matter 2.5 (PM 2.5), is a pressing concern for public health and is difficult to estimate in developing countries (data-poor regions) due to a lack of ground sensors. Transfer learning models can be leveraged to solve this problem, as they use alternate data sources to gain knowledge (i.e., data from data-rich regions). However, current transfer learning methodologies do not account for dependencies between the source and the target domains. We recognize this transfer problem as spatial transfer learning and propose a new feature named Latent Dependency Factor (LDF) that captures spatial and semantic dependencies of both domains and is subsequently added to the datasets. We generate LDF using a novel two-stage autoencoder model that learns from clusters of similar source and target domain data. Our experiments show that transfer models using LDF have a $19.34\\%$ improvement over the best-performing baselines. We additionally support our experiments with qualitative results.","sentences":["Air pollution, especially particulate matter 2.5 (PM 2.5), is a pressing concern for public health and is difficult to estimate in developing countries (data-poor regions) due to a lack of ground sensors.","Transfer learning models can be leveraged to solve this problem, as they use alternate data sources to gain knowledge (i.e., data from data-rich regions).","However, current transfer learning methodologies do not account for dependencies between the source and the target domains.","We recognize this transfer problem as spatial transfer learning and propose a new feature named Latent Dependency Factor (LDF) that captures spatial and semantic dependencies of both domains and is subsequently added to the datasets.","We generate LDF using a novel two-stage autoencoder model that learns from clusters of similar source and target domain data.","Our experiments show that transfer models using LDF have a $19.34\\%$ improvement over the best-performing baselines.","We additionally support our experiments with qualitative results."],"url":"http://arxiv.org/abs/2404.07308v1","category":"cs.LG"}
{"created":"2024-04-10 18:48:19","title":"Predicting Mergers and Acquisitions in Competitive Industries: A Model Based on Temporal Dynamics and Industry Networks","abstract":"M&A activities are pivotal for market consolidation, enabling firms to augment market power through strategic complementarities. Existing research often overlooks the peer effect, the mutual influence of M&A behaviors among firms, and fails to capture complex interdependencies within industry networks. Common approaches suffer from reliance on ad-hoc feature engineering, data truncation leading to significant information loss, reduced predictive accuracy, and challenges in real-world application. Additionally, the rarity of M&A events necessitates data rebalancing in conventional models, introducing bias and undermining prediction reliability. We propose an innovative M&A predictive model utilizing the Temporal Dynamic Industry Network (TDIN), leveraging temporal point processes and deep learning to adeptly capture industry-wide M&A dynamics. This model facilitates accurate, detailed deal-level predictions without arbitrary data manipulation or rebalancing, demonstrated through superior evaluation results from M&A cases between January 1997 and December 2020. Our approach marks a significant improvement over traditional models by providing detailed insights into M&A activities and strategic recommendations for specific firms.","sentences":["M&A activities are pivotal for market consolidation, enabling firms to augment market power through strategic complementarities.","Existing research often overlooks the peer effect, the mutual influence of M&A behaviors among firms, and fails to capture complex interdependencies within industry networks.","Common approaches suffer from reliance on ad-hoc feature engineering, data truncation leading to significant information loss, reduced predictive accuracy, and challenges in real-world application.","Additionally, the rarity of M&A events necessitates data rebalancing in conventional models, introducing bias and undermining prediction reliability.","We propose an innovative M&A predictive model utilizing the Temporal Dynamic Industry Network (TDIN), leveraging temporal point processes and deep learning to adeptly capture industry-wide M&A dynamics.","This model facilitates accurate, detailed deal-level predictions without arbitrary data manipulation or rebalancing, demonstrated through superior evaluation results from M&A cases between January 1997 and December 2020.","Our approach marks a significant improvement over traditional models by providing detailed insights into M&A activities and strategic recommendations for specific firms."],"url":"http://arxiv.org/abs/2404.07298v1","category":"q-fin.ST"}
{"created":"2024-04-10 18:10:19","title":"Generating Reservoir State Descriptions with Random Matrices","abstract":"We demonstrate a novel approach to reservoir computer measurements through the use of a simple quantum system and random matrices to motivate how atomic-scale devices might be used for real-world computing applications. In our approach, random matrices are used to construct reservoir measurements, introducing a simple, scalable means for producing state descriptions. In our studies, systems as simple as a five-atom Heisenberg spin chain are used to perform several tasks, including time series prediction and data interpolation. The performance of the measurement technique as well as their current limitations are discussed in detail alongside an exploration of the diversity of measurements yielded by the random matrices. Additionally, we explore the role of the parameters of the spin chain, adjusting coupling strength and the measurement dimension, yielding insights into how these learning machines might be automatically tuned for different problems. This research highlights the use of random matrices as measurements of simple quantum systems for natural learning devices and outlines a path forward for improving their performance and experimental realisation.","sentences":["We demonstrate a novel approach to reservoir computer measurements through the use of a simple quantum system and random matrices to motivate how atomic-scale devices might be used for real-world computing applications.","In our approach, random matrices are used to construct reservoir measurements, introducing a simple, scalable means for producing state descriptions.","In our studies, systems as simple as a five-atom Heisenberg spin chain are used to perform several tasks, including time series prediction and data interpolation.","The performance of the measurement technique as well as their current limitations are discussed in detail alongside an exploration of the diversity of measurements yielded by the random matrices.","Additionally, we explore the role of the parameters of the spin chain, adjusting coupling strength and the measurement dimension, yielding insights into how these learning machines might be automatically tuned for different problems.","This research highlights the use of random matrices as measurements of simple quantum systems for natural learning devices and outlines a path forward for improving their performance and experimental realisation."],"url":"http://arxiv.org/abs/2404.07278v1","category":"quant-ph"}
{"created":"2024-04-10 18:09:22","title":"Bounds and guarantees for learning and entanglement","abstract":"Information theory provides tools to predict the performance of a learning algorithm on a given dataset. For instance, the accuracy of learning an unknown parameter can be upper bounded by reducing the learning task to hypothesis testing for a discrete random variable, with Fano's inequality then stating that a small conditional entropy between a learner's observations and the unknown parameter is necessary for successful estimation. This work first extends this relationship by demonstrating that a small conditional entropy is also sufficient for successful learning, thereby establishing an information-theoretic lower bound on the accuracy of a learner. This connection between information theory and learning suggests that we might similarly apply quantum information theory to characterize learning tasks involving quantum systems. Observing that the fidelity of a finite-dimensional quantum system with a maximally entangled state (the singlet fraction) generalizes the success probability for estimating a discrete random variable, we introduce an entanglement manipulation task for infinite-dimensional quantum systems that similarly generalizes classical learning. We derive information-theoretic bounds for succeeding at this task in terms of the maximal singlet fraction of an appropriate finite-dimensional discretization. As classical learning is recovered as a special case of this task, our analysis suggests a deeper relationship at the interface of learning, entanglement, and information.","sentences":["Information theory provides tools to predict the performance of a learning algorithm on a given dataset.","For instance, the accuracy of learning an unknown parameter can be upper bounded by reducing the learning task to hypothesis testing for a discrete random variable, with Fano's inequality then stating that a small conditional entropy between a learner's observations and the unknown parameter is necessary for successful estimation.","This work first extends this relationship by demonstrating that a small conditional entropy is also sufficient for successful learning, thereby establishing an information-theoretic lower bound on the accuracy of a learner.","This connection between information theory and learning suggests that we might similarly apply quantum information theory to characterize learning tasks involving quantum systems.","Observing that the fidelity of a finite-dimensional quantum system with a maximally entangled state (the singlet fraction) generalizes the success probability for estimating a discrete random variable, we introduce an entanglement manipulation task for infinite-dimensional quantum systems that similarly generalizes classical learning.","We derive information-theoretic bounds for succeeding at this task in terms of the maximal singlet fraction of an appropriate finite-dimensional discretization.","As classical learning is recovered as a special case of this task, our analysis suggests a deeper relationship at the interface of learning, entanglement, and information."],"url":"http://arxiv.org/abs/2404.07277v1","category":"quant-ph"}
{"created":"2024-04-11 16:52:48","title":"A new scheme for isomer pumping and depletion with high-power lasers","abstract":"We propose a novel scheme for the population and depletion of nuclear isomers. The scheme combines the $\\gamma$-photons with energies $\\gtrsim 10$ keV emitted during the interaction of a contemporary high-intensity laser pulse with a plasma and one or multiple photon beams supplied by intense lasers. Due to nonlinear effects, two- or multi-photon absorption dominates over the conventional multi-step one-photon process for an optimized gamma flash. Moreover, this nonlinear effect can be greatly enhanced with the help of externally supplied photons. These photons act such that the effective cross-section experienced by the $\\gamma$-photons becomes tunable, growing with the intensity $I_0$ of the beam. Assuming $I_{0}\\sim 10^{18}$ Wcm$^{-2}$ for the photon beam, an effective cross-section as large as $10^{-21}$ cm$^2$ to $10^{-28}$ cm$^2$ for the $\\gamma-$photon can be achieved. Thus, within state-of-the-art 10 PW laser facilities, the yields from two-photon absorption can reach $10^6$ to $10^9$ isomers per shot for selected states that are separated from their ground state by E2 transitions. Similar yields for transitions with higher multipolarities can be accommodated by multi-photon absorption with additional photons provided.","sentences":["We propose a novel scheme for the population and depletion of nuclear isomers.","The scheme combines the $\\gamma$-photons with energies $\\gtrsim 10$ keV emitted during the interaction of a contemporary high-intensity laser pulse with a plasma and one or multiple photon beams supplied by intense lasers.","Due to nonlinear effects, two- or multi-photon absorption dominates over the conventional multi-step one-photon process for an optimized gamma flash.","Moreover, this nonlinear effect can be greatly enhanced with the help of externally supplied photons.","These photons act such that the effective cross-section experienced by the $\\gamma$-photons becomes tunable, growing with the intensity $I_0$ of the beam.","Assuming $I_{0}\\sim 10^{18}$ Wcm$^{-2}$ for the photon beam, an effective cross-section as large as $10^{-21}$ cm$^2$ to $10^{-28}$ cm$^2$ for the $\\gamma-$photon can be achieved.","Thus, within state-of-the-art 10 PW laser facilities, the yields from two-photon absorption can reach $10^6$ to $10^9$ isomers per shot for selected states that are separated from their ground state by E2 transitions.","Similar yields for transitions with higher multipolarities can be accommodated by multi-photon absorption with additional photons provided."],"url":"http://arxiv.org/abs/2404.07909v1","category":"nucl-th"}
{"created":"2024-04-11 16:16:47","title":"Data-driven methods for quantitative imaging","abstract":"In the field of quantitative imaging, the image information at a pixel or voxel in an underlying domain entails crucial information about the imaged matter. This is particularly important in medical imaging applications, such as quantitative Magnetic Resonance Imaging (qMRI), where quantitative maps of biophysical parameters can characterize the imaged tissue and thus lead to more accurate diagnoses. Such quantitative values can also be useful in subsequent, automatized classification tasks in order to discriminate normal from abnormal tissue, for instance. The accurate reconstruction of these quantitative maps is typically achieved by solving two coupled inverse problems which involve a (forward) measurement operator, typically ill-posed, and a physical process that links the wanted quantitative parameters to the reconstructed qualitative image, given some underlying measurement data. In this review, by considering qMRI as a prototypical application, we provide a mathematically-oriented overview on how data-driven approaches can be employed in these inverse problems eventually improving the reconstruction of the associated quantitative maps.","sentences":["In the field of quantitative imaging, the image information at a pixel or voxel in an underlying domain entails crucial information about the imaged matter.","This is particularly important in medical imaging applications, such as quantitative Magnetic Resonance Imaging (qMRI), where quantitative maps of biophysical parameters can characterize the imaged tissue and thus lead to more accurate diagnoses.","Such quantitative values can also be useful in subsequent, automatized classification tasks in order to discriminate normal from abnormal tissue, for instance.","The accurate reconstruction of these quantitative maps is typically achieved by solving two coupled inverse problems which involve a (forward) measurement operator, typically ill-posed, and a physical process that links the wanted quantitative parameters to the reconstructed qualitative image, given some underlying measurement data.","In this review, by considering qMRI as a prototypical application, we provide a mathematically-oriented overview on how data-driven approaches can be employed in these inverse problems eventually improving the reconstruction of the associated quantitative maps."],"url":"http://arxiv.org/abs/2404.07886v1","category":"math.OC"}
{"created":"2024-04-11 16:10:16","title":"LeapFrog: The Rowhammer Instruction Skip Attack","abstract":"Since its inception, Rowhammer exploits have rapidly evolved into increasingly sophisticated threats not only compromising data integrity but also the control flow integrity of victim processes. Nevertheless, it remains a challenge for an attacker to identify vulnerable targets (i.e., Rowhammer gadgets), understand the outcome of the attempted fault, and formulate an attack that yields useful results.   In this paper, we present a new type of Rowhammer gadget, called a LeapFrog gadget, which, when present in the victim code, allows an adversary to subvert code execution to bypass a critical piece of code (e.g., authentication check logic, encryption rounds, padding in security protocols). The Leapfrog gadget manifests when the victim code stores the Program Counter (PC) value in the user or kernel stack (e.g., a return address during a function call) which, when tampered with, re-positions the return address to a location that bypasses a security-critical code pattern.   This research also presents a systematic process to identify Leapfrog gadgets. This methodology enables the automated detection of susceptible targets and the determination of optimal attack parameters. We first showcase this new attack vector through a practical demonstration on a TLS handshake client/server scenario, successfully inducing an instruction skip in a client application. We then demonstrate the attack on real-world code found in the wild, implementing an attack on OpenSSL.   Our findings extend the impact of Rowhammer attacks on control flow and contribute to the development of more robust defenses against these increasingly sophisticated threats.","sentences":["Since its inception, Rowhammer exploits have rapidly evolved into increasingly sophisticated threats not only compromising data integrity but also the control flow integrity of victim processes.","Nevertheless, it remains a challenge for an attacker to identify vulnerable targets (i.e., Rowhammer gadgets), understand the outcome of the attempted fault, and formulate an attack that yields useful results.   ","In this paper, we present a new type of Rowhammer gadget, called a LeapFrog gadget, which, when present in the victim code, allows an adversary to subvert code execution to bypass a critical piece of code (e.g., authentication check logic, encryption rounds, padding in security protocols).","The Leapfrog gadget manifests when the victim code stores the Program Counter (PC) value in the user or kernel stack (e.g., a return address during a function call) which, when tampered with, re-positions the return address to a location that bypasses a security-critical code pattern.   ","This research also presents a systematic process to identify Leapfrog gadgets.","This methodology enables the automated detection of susceptible targets and the determination of optimal attack parameters.","We first showcase this new attack vector through a practical demonstration on a TLS handshake client/server scenario, successfully inducing an instruction skip in a client application.","We then demonstrate the attack on real-world code found in the wild, implementing an attack on OpenSSL.   ","Our findings extend the impact of Rowhammer attacks on control flow and contribute to the development of more robust defenses against these increasingly sophisticated threats."],"url":"http://arxiv.org/abs/2404.07878v1","category":"cs.CR"}
{"created":"2024-04-11 16:03:27","title":"Counting statistics of ultra-broadband microwave photons","abstract":"We report measurements of counting statistics, average and variance, of microwave photons of ill-defined frequency: bichromatic photons, i.e. photons involving two well separated frequencies, and \"white\", broadband photons. Our setup allows for the detection of single photonic modes of arbitrary waveform over the 1-10 GHz frequency range. The photon statistics is obtained by on-the-fly numerical calculation from the measured time-dependent voltage. After validating our procedure with thermal- and squeezed- radiation of such photons, we relate the detected statistics to the squeezing spectrum of an ac+dc biased tunnel junction. We observe an optimal squeezing of $\\sim$1.5dB over a bandwidth $>1$ GHz, better than 1dB over 3.5 GHz and still see squeezing over a bandwidth of $\\sim6$ GHz around 6 GHz. We also show how the waveform of a bichromatic photon can be optimized for maximum squeezing.","sentences":["We report measurements of counting statistics, average and variance, of microwave photons of ill-defined frequency: bichromatic photons, i.e. photons involving two well separated frequencies, and \"white\", broadband photons.","Our setup allows for the detection of single photonic modes of arbitrary waveform over the 1-10 GHz frequency range.","The photon statistics is obtained by on-the-fly numerical calculation from the measured time-dependent voltage.","After validating our procedure with thermal- and squeezed- radiation of such photons, we relate the detected statistics to the squeezing spectrum of an ac+dc biased tunnel junction.","We observe an optimal squeezing of $\\sim$1.5dB over a bandwidth $>1$ GHz, better than 1dB over 3.5 GHz and still see squeezing over a bandwidth of $\\sim6$ GHz around 6 GHz.","We also show how the waveform of a bichromatic photon can be optimized for maximum squeezing."],"url":"http://arxiv.org/abs/2404.07868v1","category":"quant-ph"}
{"created":"2024-04-11 15:54:41","title":"Konnektor: Connection Protocol for Ensuring Peer Uniqueness in Decentralized P2P Networks","abstract":"Konnektor is a connection protocol designed to solve the challenge of managing unique peers within distributed peer-to-peer networks. By prioritizing network integrity and efficiency, Konnektor offers a comprehensive solution that safeguards against the spread of duplicate peers while optimizing resource utilization. This paper provides a detailed explanation of the protocol's key components, including peer addressing, connection initialization, detecting peer duplications and mitigation strategies against potential security threats.","sentences":["Konnektor is a connection protocol designed to solve the challenge of managing unique peers within distributed peer-to-peer networks.","By prioritizing network integrity and efficiency, Konnektor offers a comprehensive solution that safeguards against the spread of duplicate peers while optimizing resource utilization.","This paper provides a detailed explanation of the protocol's key components, including peer addressing, connection initialization, detecting peer duplications and mitigation strategies against potential security threats."],"url":"http://arxiv.org/abs/2404.07861v1","category":"cs.NI"}
{"created":"2024-04-11 14:35:28","title":"First-Principles Study of Penta-CN2 Quantum Dots for Efficient Hydrogen Evolution Reaction","abstract":"The objective of our research is to investigate the electrocatalytic properties of novel metal-free quantum dots (QDs) composed of the recently discovered 2D material penta-CN2, with the aim of replacing costly and scarce catalysts such as Pt and Pd. Employing a first-principles density functional theory (DFT) based approach, the geometries of the three penta-CN2 quantum dots (QDs) of increasing sizes, 3 x 3, 3 x 4, 4 x 4 are optimized. Through comprehensive analysis, our research extensively explored the structural stability of penta-CN2 QDs, delved into their electronic properties, and assessed their catalytic performance concerning the Hydrogen Evolution Reaction (HER). Notably, the H-adsorbed penta-CN2 QDs exhibit a significant reduction in the HOMO-LUMO gap (E_{g}) ranging from 35% to 49% compared to the pristine QD. Next, we investigated their catalytic performance relevant to HER, using well-known descriptors: (i) adsorption energy, (ii) over-potential, (iii) Gibbs free energy and (iv) exchange current density along with the volcano curve. As far as size dependence of the catalytic performance is concerned, the value of Delta G^{(av)} is minimum for 3 x 3 penta-CN2 QD, with those of 3 x 4 and 4 x 4 QDs being slightly larger. Our calculations predict a high value of exchange current density 2.24 x 10^{-3} A-cm^{-2} for one of the sites (N11 for 3 x 3 QD), which we believe will lead to significantly enhanced HER properties. Our research outcomes hold great promise in advancing the discovery of abundant, non-toxic, and cost-effective catalysts for HER, playing a vital role in facilitating large-scale hydrogen production.","sentences":["The objective of our research is to investigate the electrocatalytic properties of novel metal-free quantum dots (QDs) composed of the recently discovered 2D material penta-CN2, with the aim of replacing costly and scarce catalysts such as Pt and Pd.","Employing a first-principles density functional theory (DFT) based approach, the geometries of the three penta-CN2 quantum dots (QDs) of increasing sizes, 3 x 3, 3 x 4, 4 x 4 are optimized.","Through comprehensive analysis, our research extensively explored the structural stability of penta-CN2 QDs, delved into their electronic properties, and assessed their catalytic performance concerning the Hydrogen Evolution Reaction (HER).","Notably, the H-adsorbed penta-CN2 QDs exhibit a significant reduction in the HOMO-LUMO gap (E_{g}) ranging from 35% to 49% compared to the pristine QD.","Next, we investigated their catalytic performance relevant to HER, using well-known descriptors: (i) adsorption energy, (ii) over-potential, (iii) Gibbs free energy and (iv) exchange current density along with the volcano curve.","As far as size dependence of the catalytic performance is concerned, the value of Delta G^{(av)} is minimum for 3 x 3 penta-CN2 QD, with those of 3 x 4 and 4 x 4 QDs being slightly larger.","Our calculations predict a high value of exchange current density 2.24 x 10^{-3} A-cm^{-2} for one of the sites (N11 for 3 x 3 QD), which we believe will lead to significantly enhanced HER properties.","Our research outcomes hold great promise in advancing the discovery of abundant, non-toxic, and cost-effective catalysts for HER, playing a vital role in facilitating large-scale hydrogen production."],"url":"http://arxiv.org/abs/2404.07793v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 14:20:38","title":"Estimating Visibility from Alternate Perspectives for Motion Planning with Occlusions","abstract":"Visibility is a crucial aspect of planning and control of autonomous vehicles (AV), particularly when navigating environments with occlusions. However, when an AV follows a trajectory with multiple occlusions, existing methods evaluate each occlusion individually, calculate a visibility cost for each, and rely on the planner to minimize the overall cost. This can result in conflicting priorities for the planner, as individual occlusion costs may appear to be in opposition. We solve this problem by creating an alternate perspective cost map that allows for an aggregate view of the occlusions in the environment. The value of each cell on the cost map is a measure of the amount of visual information that the vehicle can gain about the environment by visiting that location. Our proposed method identifies observation locations and occlusion targets drawn from both map data and sensor data. We show how to estimate an alternate perspective for each observation location and then combine all estimates into a single alternate perspective cost map for motion planning.","sentences":["Visibility is a crucial aspect of planning and control of autonomous vehicles (AV), particularly when navigating environments with occlusions.","However, when an AV follows a trajectory with multiple occlusions, existing methods evaluate each occlusion individually, calculate a visibility cost for each, and rely on the planner to minimize the overall cost.","This can result in conflicting priorities for the planner, as individual occlusion costs may appear to be in opposition.","We solve this problem by creating an alternate perspective cost map that allows for an aggregate view of the occlusions in the environment.","The value of each cell on the cost map is a measure of the amount of visual information that the vehicle can gain about the environment by visiting that location.","Our proposed method identifies observation locations and occlusion targets drawn from both map data and sensor data.","We show how to estimate an alternate perspective for each observation location and then combine all estimates into a single alternate perspective cost map for motion planning."],"url":"http://arxiv.org/abs/2404.07781v1","category":"cs.RO"}
{"created":"2024-04-11 13:42:44","title":"Scattering Shadows","abstract":"We discuss the regions forbidden to classical scattering trajectories by repulsive potentials. We give explicit results for the asymptotic form of these regions, far from the scattering center, in terms of the scattering angle function.","sentences":["We discuss the regions forbidden to classical scattering trajectories by repulsive potentials.","We give explicit results for the asymptotic form of these regions, far from the scattering center, in terms of the scattering angle function."],"url":"http://arxiv.org/abs/2404.07745v1","category":"physics.class-ph"}
{"created":"2024-04-11 13:05:59","title":"Si Superstrate Lenses on Patch-Antenna-Coupled TeraFETs: NEP Optimization and Frequency Fine-Tuning","abstract":"This paper presents a study on performance optimization and resonant frequency modification of terahertz detectors by the use of hyper-hemispherical silicon superstrate lenses. The detectors are patch-TeraFETs, i.e., field-effect transistors with monolithically integrated patch antennas fabricated with a commercial 65-nm CMOS foundry process and designed for an operation frequency of 580 GHz. We demonstrate a strong improvement of the optical noise-equivalent power optical NEP, referenced against the total radiation power) reaching a value of 16 pW/Hz^(1/2). We show, furthermore, that the resonance frequency can be efficiently fine-tuned by the choice of the material and the thickness of a dielectric layer placed between the transistor and the superstrate lens. The resonance frequency can be shifted by more than 15 % of the center frequency (up to 100 GHz for the 580 GHz devices). The design of the on-chip optics can be employed for post-fabrication tailoring of the detector's resonance frequency to target specific spectral positions.","sentences":["This paper presents a study on performance optimization and resonant frequency modification of terahertz detectors by the use of hyper-hemispherical silicon superstrate lenses.","The detectors are patch-TeraFETs, i.e., field-effect transistors with monolithically integrated patch antennas fabricated with a commercial 65-nm CMOS foundry process and designed for an operation frequency of 580 GHz.","We demonstrate a strong improvement of the optical noise-equivalent power optical NEP, referenced against the total radiation power) reaching a value of 16 pW/Hz^(1/2).","We show, furthermore, that the resonance frequency can be efficiently fine-tuned by the choice of the material and the thickness of a dielectric layer placed between the transistor and the superstrate lens.","The resonance frequency can be shifted by more than 15 % of the center frequency (up to 100 GHz for the 580 GHz devices).","The design of the on-chip optics can be employed for post-fabrication tailoring of the detector's resonance frequency to target specific spectral positions."],"url":"http://arxiv.org/abs/2404.07715v1","category":"physics.optics"}
{"created":"2024-04-11 12:18:51","title":"Optimal run-and-tumble in slit-like confinement","abstract":"Run-and-tumble is a basic model of persistent motion and a motility strategy widespread in micro-organisms and individual cells. In many natural settings, movement occurs in the presence of confinement. While accumulation at the surface has been extensively studied, the transport parallel to the boundary has received less attention. We consider a run-and-tumble particle confined inside a slit, where motion in the bulk alternates with intermittent sojourns at the wall. We first propose a discrete-direction model that is fully tractable and obtain the exact diffusion coefficient characterizing the long-time exploration of the slit. We then use numerical simulations to show that with an adequate choice of parameters, our analytical prediction provides a useful approximation for the diffusion coefficient of run-and-tumble with continuous direction. Finally, we identify the conditions that maximize diffusion within the slit and discuss the optimal mean run time. For swimming bacteria, we find that the optimum is typically reached when the mean run length is comparable to the confinement size.","sentences":["Run-and-tumble is a basic model of persistent motion and a motility strategy widespread in micro-organisms and individual cells.","In many natural settings, movement occurs in the presence of confinement.","While accumulation at the surface has been extensively studied, the transport parallel to the boundary has received less attention.","We consider a run-and-tumble particle confined inside a slit, where motion in the bulk alternates with intermittent sojourns at the wall.","We first propose a discrete-direction model that is fully tractable and obtain the exact diffusion coefficient characterizing the long-time exploration of the slit.","We then use numerical simulations to show that with an adequate choice of parameters, our analytical prediction provides a useful approximation for the diffusion coefficient of run-and-tumble with continuous direction.","Finally, we identify the conditions that maximize diffusion within the slit and discuss the optimal mean run time.","For swimming bacteria, we find that the optimum is typically reached when the mean run length is comparable to the confinement size."],"url":"http://arxiv.org/abs/2404.07680v1","category":"cond-mat.soft"}
{"created":"2024-04-11 10:21:58","title":"An improvement of degree-based hashing (DBH) graph partition method, using a novel metric","abstract":"This paper examines the graph partition problem and introduces a new metric, MSIDS (maximal sum of inner degrees squared). We establish its connection to the replication factor (RF) optimization, which has been the main focus of theoretical work in this field. Additionally, we propose a new partition algorithm, DBH-X, based on the DBH partitioner. We demonstrate that DBH-X significantly improves both the RF and MSIDS, compared to the baseline DBH algorithm. In addition, we provide test results that show the runtime acceleration of GraphX-based PageRank and Label propagation algorithms.","sentences":["This paper examines the graph partition problem and introduces a new metric, MSIDS (maximal sum of inner degrees squared).","We establish its connection to the replication factor (RF) optimization, which has been the main focus of theoretical work in this field.","Additionally, we propose a new partition algorithm, DBH-X, based on the DBH partitioner.","We demonstrate that DBH-X significantly improves both the RF and MSIDS, compared to the baseline DBH algorithm.","In addition, we provide test results that show the runtime acceleration of GraphX-based PageRank and Label propagation algorithms."],"url":"http://arxiv.org/abs/2404.07624v1","category":"cs.DS"}
{"created":"2024-04-11 07:44:53","title":"Seashell-inspired polarization-sensitive tonotopic metasensor","abstract":"Bioinspiration has widely been demonstrated to be a powerful approach for the design of innovative structures and devices. Recently, this concept has been extended to the field of elasticity, dynamics, and metamaterials. In this paper, we propose a seashell-inspired metasensor that can simultaneously perform spatial frequency mapping and act as a polarizer. The structure emerges from a universal parametric design that encompasses diverse spiral geometries with varying circular cross sections and curvature radii, all leading to tonotopic behavior. Adoption of an optimization process leads to a planar geometry that enables us to simultaneously achieve tonotopy for orthogonally polarized modes, leading to the possibility to control polarization as well as the spatial distribution of frequency maxima along the spiral axis. We demonstrate the versatility of the device and discuss the possible applications in the field of acoustics and sensing.","sentences":["Bioinspiration has widely been demonstrated to be a powerful approach for the design of innovative structures and devices.","Recently, this concept has been extended to the field of elasticity, dynamics, and metamaterials.","In this paper, we propose a seashell-inspired metasensor that can simultaneously perform spatial frequency mapping and act as a polarizer.","The structure emerges from a universal parametric design that encompasses diverse spiral geometries with varying circular cross sections and curvature radii, all leading to tonotopic behavior.","Adoption of an optimization process leads to a planar geometry that enables us to simultaneously achieve tonotopy for orthogonally polarized modes, leading to the possibility to control polarization as well as the spatial distribution of frequency maxima along the spiral axis.","We demonstrate the versatility of the device and discuss the possible applications in the field of acoustics and sensing."],"url":"http://arxiv.org/abs/2404.07529v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 01:39:50","title":"Precoder Design for User-Centric Network Massive MIMO with Matrix Manifold Optimization","abstract":"In this paper, we investigate the precoder design for user-centric network (UCN) massive multiple-input multiple-output (mMIMO) downlink with matrix manifold optimization. In UCN mMIMO systems, each user terminal (UT) is served by a subset of base stations (BSs) instead of all the BSs, facilitating the implementation of the system and lowering the dimension of the precoders to be designed. By proving that the precoder set satisfying the per-BS power constraints forms a Riemannian submanifold of a linear product manifold, we transform the constrained precoder design problem in Euclidean space to an unconstrained one on the Riemannian submanifold. Riemannian ingredients, including orthogonal projection, Riemannian gradient, retraction and vector transport, of the problem on the Riemannian submanifold are further derived, with which the Riemannian conjugate gradient (RCG) design method is proposed for solving the unconstrained problem. The proposed method avoids the inverses of large dimensional matrices, which is beneficial in practice. The complexity analyses show the high computational efficiency of RCG precoder design. Simulation results demonstrate the numerical superiority of the proposed precoder design and the high efficiency of the UCN mMIMO system.","sentences":["In this paper, we investigate the precoder design for user-centric network (UCN) massive multiple-input multiple-output (mMIMO) downlink with matrix manifold optimization.","In UCN mMIMO systems, each user terminal (UT) is served by a subset of base stations (BSs) instead of all the BSs, facilitating the implementation of the system and lowering the dimension of the precoders to be designed.","By proving that the precoder set satisfying the per-BS power constraints forms a Riemannian submanifold of a linear product manifold, we transform the constrained precoder design problem in Euclidean space to an unconstrained one on the Riemannian submanifold.","Riemannian ingredients, including orthogonal projection, Riemannian gradient, retraction and vector transport, of the problem on the Riemannian submanifold are further derived, with which the Riemannian conjugate gradient (RCG) design method is proposed for solving the unconstrained problem.","The proposed method avoids the inverses of large dimensional matrices, which is beneficial in practice.","The complexity analyses show the high computational efficiency of RCG precoder design.","Simulation results demonstrate the numerical superiority of the proposed precoder design and the high efficiency of the UCN mMIMO system."],"url":"http://arxiv.org/abs/2404.07425v1","category":"eess.SP"}
{"created":"2024-04-11 01:22:26","title":"Active particle motion in Poiseuille flow through rectangular channels","abstract":"We investigate the dynamics of a point-like active particle suspended in fluid flow through a straight channel. For this particle-fluid system, we derive a constant of motion for a general unidirectional fluid flow, and apply it to an approximation of Poiseuille flow through rectangular cross-sections. For a given rectangular cross-section, this results in a $4$D nonlinear conservative dynamical system with one constant of motion and a dimensionless parameter as the ratio of maximum flow speed to intrinsic active particle speed. We observe a diverse set of active particle trajectories with variations in system parameters and initial conditions which we classify into different types of swinging, trapping, tumbling and wandering motion. Regular (periodic/quasiperiodic) motion as well as chaotic active particle motion are observed for these trajectories and quantified using largest Lyapunov exponents. We explore the transition to chaotic motion using Poincar\\'e maps and show ``sticky\" chaotic tumbling trajectories that have long transients near a periodic state. Outcomes of this work may have implications for dynamics of natural and artificial microswimmers in experimental microfluidic channels that typically have rectangular cross-sections.","sentences":["We investigate the dynamics of a point-like active particle suspended in fluid flow through a straight channel.","For this particle-fluid system, we derive a constant of motion for a general unidirectional fluid flow, and apply it to an approximation of Poiseuille flow through rectangular cross-sections.","For a given rectangular cross-section, this results in a $4$D nonlinear conservative dynamical system with one constant of motion and a dimensionless parameter as the ratio of maximum flow speed to intrinsic active particle speed.","We observe a diverse set of active particle trajectories with variations in system parameters and initial conditions which we classify into different types of swinging, trapping, tumbling and wandering motion.","Regular (periodic/quasiperiodic) motion as well as chaotic active particle motion are observed for these trajectories and quantified using largest Lyapunov exponents.","We explore the transition to chaotic motion using Poincar\\'e maps and show ``sticky\" chaotic tumbling trajectories that have long transients near a periodic state.","Outcomes of this work may have implications for dynamics of natural and artificial microswimmers in experimental microfluidic channels that typically have rectangular cross-sections."],"url":"http://arxiv.org/abs/2404.07420v1","category":"physics.flu-dyn"}
{"created":"2024-04-11 01:01:35","title":"Grouping of $N-1$ Contingencies for Controller Synthesis: A Study for Power Line Failures","abstract":"The problem of maintaining power system stability and performance after the failure of any single line in a power system (an \"N-1 contingency\") is investigated. Due to the large number of possible N-1 contingencies for a power network, it is impractical to optimize controller parameters for each possible contingency a priori. A method to partition a set of contingencies into groups of contingencies that are similar to each other from a control perspective is presented. Design of a single controller for each group, rather than for each contingency, provides a computationally tractable method for maintaining stability and performance after element failures. The choice of number of groups tunes a trade-off between computation time and controller performance for a given set of contingencies. Results are simulated on the IEEE 39-bus and 68-bus systems, illustrating that, with controllers designed for a relatively small number of groups, power system stability may be significantly improved after an N-1 contingency compared to continued use of the nominal controller. Furthermore, performance is comparable to that of controllers designed for each contingency individually.","sentences":["The problem of maintaining power system stability and performance after the failure of any single line in a power system (an \"N-1 contingency\") is investigated.","Due to the large number of possible N-1 contingencies for a power network, it is impractical to optimize controller parameters for each possible contingency a priori.","A method to partition a set of contingencies into groups of contingencies that are similar to each other from a control perspective is presented.","Design of a single controller for each group, rather than for each contingency, provides a computationally tractable method for maintaining stability and performance after element failures.","The choice of number of groups tunes a trade-off between computation time and controller performance for a given set of contingencies.","Results are simulated on the IEEE 39-bus and 68-bus systems, illustrating that, with controllers designed for a relatively small number of groups, power system stability may be significantly improved after an N-1 contingency compared to continued use of the nominal controller.","Furthermore, performance is comparable to that of controllers designed for each contingency individually."],"url":"http://arxiv.org/abs/2404.07415v1","category":"eess.SY"}
{"created":"2024-04-10 21:45:23","title":"Lock-Key Microfluidics: Simulating Nematic Colloid Advection along Wavy-Walled Channels","abstract":"Liquid crystalline media mediate interactions between suspended particles and confining geometries, which not only has potential to guide patterning and bottom-up colloidal assembly, but can also control colloidal migration in microfluidic devices. However, simulating such dynamics is challenging because nemato-elasticity, diffusivity and hydrodynamic interactions must all be accounted for within complex boundaries. We model the advection of colloids dispersed in flowing and fluctuating nematic fluids confined within 2D wavy channels. A lock-key mechanism between colloids and troughs is found to be stronger for planar anchoring compared to homeotropic anchoring due to the relative location of the colloid-associated defects. Sufficiently large amplitudes result in stick-slip trajectories and even permanent locking of colloids in place. These results demonstrate that wavy walls not only have potential to direct colloids to specific docking sites but also to control site-specific resting duration and intermittent elution.","sentences":["Liquid crystalline media mediate interactions between suspended particles and confining geometries, which not only has potential to guide patterning and bottom-up colloidal assembly, but can also control colloidal migration in microfluidic devices.","However, simulating such dynamics is challenging because nemato-elasticity, diffusivity and hydrodynamic interactions must all be accounted for within complex boundaries.","We model the advection of colloids dispersed in flowing and fluctuating nematic fluids confined within 2D wavy channels.","A lock-key mechanism between colloids and troughs is found to be stronger for planar anchoring compared to homeotropic anchoring due to the relative location of the colloid-associated defects.","Sufficiently large amplitudes result in stick-slip trajectories and even permanent locking of colloids in place.","These results demonstrate that wavy walls not only have potential to direct colloids to specific docking sites but also to control site-specific resting duration and intermittent elution."],"url":"http://arxiv.org/abs/2404.07367v1","category":"cond-mat.soft"}
{"created":"2024-04-10 19:53:15","title":"On the effect of flux-surface shaping on trapped-electron modes in quasi-helically symmetric stellarators","abstract":"Using a novel optimization procedure it has been shown that the Helically Symmetric eXperiment (HSX) stellarator can be optimized for reduced trapped-electron-mode (TEM) instability [M.J.~Gerard et al., \\textit{Nucl.~Fusion} \\textbf{63} (2023) 056004]. Presently, with a set of 563 experimental candidate configurations, gyrokinetic simulations are performed to investigate the efficacy of available energy $E_\\mathrm{A}$, quasi-helical symmetry, and flux-surface shaping parameters as metrics for TEM stabilization. It is found that lower values of $E_\\mathrm{A}$ correlate with reduced growth rates, but only when separate flux-surface shaping regimes are considered. Moreover, configurations with improved quasi-helical symmetry demonstrate a similar reduction in growth rates and less scatter compared to $E_\\mathrm{A}$. Regarding flux-surface shaping, a set of helical shaping parameters is introduced that show increased elongation is strongly correlated with reduced TEM growth rates, however, only when the quasi-helical symmetry is preserved. Using a newly derived velocity-space-averaged TEM resonance operator, these trends are analyzed to provide insights into the physical mechanism of the observed stabilization. For elongation, stabilization is attributed to geometric effects that reduce the destabilizing particle drifts across the magnetic field. Regarding quasi-helical symmetry, the TEM resonance in the maximally resonant trapping well is shown to increase as the quasi-helical symmetry is broken, and breaking quasi-helical symmetry increases the prevalence of highly resonant trapping wells. While these results demonstrate the limitations of using any single metric as a linear TEM proxy, it is shown that quasi-helical symmetry and plasma elongation are highly effective metrics for reducing TEM growth rates in helical equilibria.","sentences":["Using a novel optimization procedure it has been shown that the Helically Symmetric eXperiment (HSX) stellarator can be optimized for reduced trapped-electron-mode (TEM) instability [M.J.~Gerard et al., \\textit{Nucl.~Fusion} \\textbf{63} (2023) 056004].","Presently, with a set of 563 experimental candidate configurations, gyrokinetic simulations are performed to investigate the efficacy of available energy $E_\\mathrm{A}$, quasi-helical symmetry, and flux-surface shaping parameters as metrics for TEM stabilization.","It is found that lower values of $E_\\mathrm{A}$ correlate with reduced growth rates, but only when separate flux-surface shaping regimes are considered.","Moreover, configurations with improved quasi-helical symmetry demonstrate a similar reduction in growth rates and less scatter compared to $E_\\mathrm{A}$. Regarding flux-surface shaping, a set of helical shaping parameters is introduced that show increased elongation is strongly correlated with reduced TEM growth rates, however, only when the quasi-helical symmetry is preserved.","Using a newly derived velocity-space-averaged TEM resonance operator, these trends are analyzed to provide insights into the physical mechanism of the observed stabilization.","For elongation, stabilization is attributed to geometric effects that reduce the destabilizing particle drifts across the magnetic field.","Regarding quasi-helical symmetry, the TEM resonance in the maximally resonant trapping well is shown to increase as the quasi-helical symmetry is broken, and breaking quasi-helical symmetry increases the prevalence of highly resonant trapping wells.","While these results demonstrate the limitations of using any single metric as a linear TEM proxy, it is shown that quasi-helical symmetry and plasma elongation are highly effective metrics for reducing TEM growth rates in helical equilibria."],"url":"http://arxiv.org/abs/2404.07322v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 19:02:11","title":"8x8 Patch-Antenna-Coupled TeraFET Detector Array for Terahertz Quantum-Cascade-Laser Applications","abstract":"Monolithically integrated, antenna-coupled field-effect transistors (TeraFETs) constitute rapid and sensitive detectors for the terahertz range (0.3 THz to 10 THz) that can operate at room temperature. Here, we conducted an experimental characterization of a single patch-antenna coupled TeraFET optimized for 3.4 THz operation. This characterization utilized a single-mode high-power terahertz Quantum-Cascade-Laser (QCL) emitting at the designated frequency (2.85 THz and 3.4 THz). Subsequently, we integrated 8x8 of the TeraFET elements into a parallel readout circuitry, facilitated by the process maturity of a commercial 65-nm process node. In this configuration, the entire TeraFET network operates as a unified pixel, amalgamating the output signals of all rectifying elements. We emphasize two significant enhancements for sensitive power detection experiments. Firstly, the larger detector area enhances alignment and signal stability. Additionally, it leads to a significantly reduced source--drain resistance, resulting in lower noise and enabling higher modulation bandwidth. In the following, we introduce a TeraFET-based detector system implementation achieving a bandwidth of 15 MHz. Furthermore, we show potential to extend achievable bandwidth up to 21 MHz . Finally, we validate the system's performance by employing high-resolution spectroscopic data to investigate methanol vapor in the vicinity of 3.4 THz with an estimated detection limit of 2.6e11 molecules/cm3.","sentences":["Monolithically integrated, antenna-coupled field-effect transistors (TeraFETs) constitute rapid and sensitive detectors for the terahertz range (0.3 THz to 10 THz) that can operate at room temperature.","Here, we conducted an experimental characterization of a single patch-antenna coupled TeraFET optimized for 3.4 THz operation.","This characterization utilized a single-mode high-power terahertz Quantum-Cascade-Laser (QCL) emitting at the designated frequency (2.85 THz and 3.4 THz).","Subsequently, we integrated 8x8 of the TeraFET elements into a parallel readout circuitry, facilitated by the process maturity of a commercial 65-nm process node.","In this configuration, the entire TeraFET network operates as a unified pixel, amalgamating the output signals of all rectifying elements.","We emphasize two significant enhancements for sensitive power detection experiments.","Firstly, the larger detector area enhances alignment and signal stability.","Additionally, it leads to a significantly reduced source--drain resistance, resulting in lower noise and enabling higher modulation bandwidth.","In the following, we introduce a TeraFET-based detector system implementation achieving a bandwidth of 15 MHz.","Furthermore, we show potential to extend achievable bandwidth up to 21 MHz .","Finally, we validate the system's performance by employing high-resolution spectroscopic data to investigate methanol vapor in the vicinity of 3.4 THz with an estimated detection limit of 2.6e11 molecules/cm3."],"url":"http://arxiv.org/abs/2404.07309v1","category":"physics.ins-det"}
{"created":"2024-04-10 18:00:53","title":"Full Modeling and Parameter Compression Methods in configuration space for DESI 2024 and beyond","abstract":"In the contemporary era of high-precision spectroscopic surveys, led by projects like DESI, there is an increasing demand for optimizing the extraction of cosmological information from clustering data. This work conducts a thorough comparison of various methodologies for modeling the full shape of the two-point statistics in configuration space. We investigate the performance of both direct fits (Full-Modeling) and the parameter compression approaches (ShapeFit and Standard). We utilize the ABACUS-SUMMIT simulations, tailored to exceed DESI's precision requirements. Particularly, we fit the two-point statistics of three distinct tracers (LRG, ELG, and QSO), by employing a Gaussian Streaming Model in tandem with Convolution Lagrangian Perturbation Theory and Effective Field Theory. We explore methodological setup variations, including the range of scales, the set of galaxy bias parameters, the inclusion of the hexadecapole, as well as model extensions encompassing varying $n_s$ and allowing for $w_0w_a$CDM dark energy model. Throughout these varied explorations, while precision levels fluctuate and certain configurations exhibit tighter parameter constraints, our pipeline consistently recovers the parameter values of the mocks within $1\\sigma$ in all cases for a 1-year DESI volume. Additionally, we compare the performance of configuration space analysis with its Fourier space counterpart using three models: PyBird, FOLPS and velocileptors, presented in companion papers. We find good agreement with the results from all these models.","sentences":["In the contemporary era of high-precision spectroscopic surveys, led by projects like DESI, there is an increasing demand for optimizing the extraction of cosmological information from clustering data.","This work conducts a thorough comparison of various methodologies for modeling the full shape of the two-point statistics in configuration space.","We investigate the performance of both direct fits (Full-Modeling) and the parameter compression approaches (ShapeFit and Standard).","We utilize the ABACUS-SUMMIT simulations, tailored to exceed DESI's precision requirements.","Particularly, we fit the two-point statistics of three distinct tracers (LRG, ELG, and QSO), by employing a Gaussian Streaming Model in tandem with Convolution Lagrangian Perturbation Theory and Effective Field Theory.","We explore methodological setup variations, including the range of scales, the set of galaxy bias parameters, the inclusion of the hexadecapole, as well as model extensions encompassing varying $n_s$ and allowing for $w_0w_a$CDM dark energy model.","Throughout these varied explorations, while precision levels fluctuate and certain configurations exhibit tighter parameter constraints, our pipeline consistently recovers the parameter values of the mocks within $1\\sigma$ in all cases for a 1-year DESI volume.","Additionally, we compare the performance of configuration space analysis with its Fourier space counterpart using three models: PyBird, FOLPS and velocileptors, presented in companion papers.","We find good agreement with the results from all these models."],"url":"http://arxiv.org/abs/2404.07268v1","category":"astro-ph.CO"}
{"created":"2024-04-10 18:00:01","title":"Complete Optimal Non-Resonant Anomaly Detection","abstract":"We propose the first-ever complete, model-agnostic search strategy based on the optimal anomaly score, for new physics on the tails of distributions. Signal sensitivity is achieved via a classifier trained on auxiliary features in a weakly-supervised fashion, and backgrounds are predicted using the ABCD method in the classifier output and the primary tail feature. The independence between the classifier output and the tail feature required for ABCD is achieved by first training a conditional normalizing flow that yields a decorrelated version of the auxiliary features; the classifier is then trained on these features. Both the signal sensitivity and background prediction require a sample of events accurately approximating the SM background; we assume this can be furnished by closely related control processes in the data or by accurate simulations, as is the case in countless conventional analyses. The viability of our approach is demonstrated for signatures consisting of (mono)jets and missing transverse energy, where the main SM background is $Z(\\nu \\nu) +\\text{jets}$, and the data-driven control process is $\\gamma+\\text{jets}$.","sentences":["We propose the first-ever complete, model-agnostic search strategy based on the optimal anomaly score, for new physics on the tails of distributions.","Signal sensitivity is achieved via a classifier trained on auxiliary features in a weakly-supervised fashion, and backgrounds are predicted using the ABCD method in the classifier output and the primary tail feature.","The independence between the classifier output and the tail feature required for ABCD is achieved by first training a conditional normalizing flow that yields a decorrelated version of the auxiliary features; the classifier is then trained on these features.","Both the signal sensitivity and background prediction require a sample of events accurately approximating the SM background; we assume this can be furnished by closely related control processes in the data or by accurate simulations, as is the case in countless conventional analyses.","The viability of our approach is demonstrated for signatures consisting of (mono)jets and missing transverse energy, where the main SM background is $Z(\\nu \\nu) +\\text{jets}$, and the data-driven control process is $\\gamma+\\text{jets}$."],"url":"http://arxiv.org/abs/2404.07258v1","category":"hep-ph"}
{"created":"2024-04-10 18:00:00","title":"PolyBin3D: A Suite of Optimal and Efficient Power Spectrum and Bispectrum Estimators for Large-Scale Structure","abstract":"By measuring, modeling and interpreting cosmological datasets, one can place strong constraints on models of the Universe. Central to this effort are summary statistics such as power spectra and bispectra, which condense the high-dimensional dataset into low-dimensional representations. In this work, we introduce a modern set of estimators for computing such statistics from three-dimensional clustering data, and provide a flexible Python implementation; PolyBin3D. Working in a maximum-likelihood formalism, we derive general estimators for the two- and three-point functions, which yield unbiased spectra regardless of the survey mask, weighting scheme, and presence of holes in the window function. These can be directly compared to theory without the need for mask-convolution. Furthermore, we present a numerical scheme for computing the optimal (minimum-variance) estimators for a given survey, which is shown to reduce error-bars on large-scales. Our Python package includes both general \"unwindowed'' estimators and idealized equivalents (appropriate for simulations), each of which are efficiently implemented using fast Fourier transforms and Monte Carlo summation tricks, and additionally supports GPU acceleration. These are extensively validated in this work, with Monte Carlo convergence (relevant for masked data) achieved using only a small number of iterations (typically $<10$ for bispectra). This will allow for fast and unified measurement of two- and three-point functions from current and upcoming survey data.","sentences":["By measuring, modeling and interpreting cosmological datasets, one can place strong constraints on models of the Universe.","Central to this effort are summary statistics such as power spectra and bispectra, which condense the high-dimensional dataset into low-dimensional representations.","In this work, we introduce a modern set of estimators for computing such statistics from three-dimensional clustering data, and provide a flexible Python implementation;","PolyBin3D. Working in a maximum-likelihood formalism, we derive general estimators for the two- and three-point functions, which yield unbiased spectra regardless of the survey mask, weighting scheme, and presence of holes in the window function.","These can be directly compared to theory without the need for mask-convolution.","Furthermore, we present a numerical scheme for computing the optimal (minimum-variance) estimators for a given survey, which is shown to reduce error-bars on large-scales.","Our Python package includes both general \"unwindowed'' estimators and idealized equivalents (appropriate for simulations), each of which are efficiently implemented using fast Fourier transforms and Monte Carlo summation tricks, and additionally supports GPU acceleration.","These are extensively validated in this work, with Monte Carlo convergence (relevant for masked data) achieved using only a small number of iterations (typically $<10$ for bispectra).","This will allow for fast and unified measurement of two- and three-point functions from current and upcoming survey data."],"url":"http://arxiv.org/abs/2404.07249v1","category":"astro-ph.CO"}
{"created":"2024-04-10 08:02:57","title":"Prospects of the multi-channel photometric survey telescope in the cosmological application of Type Ia supernovae","abstract":"The Multi-channel Photometric Survey Telescope (Mephisto) is a real-time, three-color photometric system designed to capture the color evolution of stars and transients accurately. This telescope system can be crucial in cosmological distance measurements of low-redshift (low-$z$, $z$ $\\lesssim 0.1$) Type Ia supernovae (SNe Ia). To optimize the capabilities of this instrument, we perform a comprehensive simulation study before its official operation is scheduled to start. By considering the impact of atmospheric extinction, weather conditions, and the lunar phase at the observing site involving the instrumental features, we simulate the light curves of SNe Ia obtained by the Mephisto. The best strategy in the case of SN Ia cosmology is to take the image at an exposure time of 130 s with a cadence of 3 days. In this condition, Mephisto can obtain hundreds of high-quality SNe Ia to achieve a distance measurement better than $4.5\\%$. Given the on-time spectral classification and monitoring of the Lijiang 2.4 m Telescope at the same observatory, Mephisto, in the whole operation, can significantly enrich the well-calibrated sample of supernovae at low-$z$ and improve the calibration accuracy of high-$z$ SNe Ia.","sentences":["The Multi-channel Photometric Survey Telescope (Mephisto) is a real-time, three-color photometric system designed to capture the color evolution of stars and transients accurately.","This telescope system can be crucial in cosmological distance measurements of low-redshift (low-$z$, $z$ $\\lesssim 0.1$) Type Ia supernovae (SNe Ia).","To optimize the capabilities of this instrument, we perform a comprehensive simulation study before its official operation is scheduled to start.","By considering the impact of atmospheric extinction, weather conditions, and the lunar phase at the observing site involving the instrumental features, we simulate the light curves of SNe Ia obtained by the Mephisto.","The best strategy in the case of SN Ia cosmology is to take the image at an exposure time of 130 s with a cadence of 3 days.","In this condition, Mephisto can obtain hundreds of high-quality SNe Ia to achieve a distance measurement better than $4.5\\%$. Given the on-time spectral classification and monitoring of the Lijiang 2.4 m Telescope at the same observatory, Mephisto, in the whole operation, can significantly enrich the well-calibrated sample of supernovae at low-$z$ and improve the calibration accuracy of high-$z$ SNe Ia."],"url":"http://arxiv.org/abs/2404.07246v1","category":"astro-ph.IM"}
{"created":"2024-04-11 17:51:40","title":"Magnetostatic bounds on stability of hopfions in bulk helimagnets","abstract":"Magnetic hopfions are three-dimensional localized topological solitons in the volume of a magnet. In this work, starting with a classical free energy density of a helimagnet, an approximate variational model of hopfions is studied. The hopfion stability regions on the uniaxial anisotropy-external magnetic field phase diagram are computed and their evolution with increasing magnetostatic interaction strength is considered. It is found that magnetostatic interaction destabilizes the hopfions and, above the certain strength (relative to Dzyaloshinskii-Moriya interaction), destroys their stability completely. Numerical estimates for this bound are provided. They can help focus the search for materials, supporting bulk magnetic hopfions.","sentences":["Magnetic hopfions are three-dimensional localized topological solitons in the volume of a magnet.","In this work, starting with a classical free energy density of a helimagnet, an approximate variational model of hopfions is studied.","The hopfion stability regions on the uniaxial anisotropy-external magnetic field phase diagram are computed and their evolution with increasing magnetostatic interaction strength is considered.","It is found that magnetostatic interaction destabilizes the hopfions and, above the certain strength (relative to Dzyaloshinskii-Moriya interaction), destroys their stability completely.","Numerical estimates for this bound are provided.","They can help focus the search for materials, supporting bulk magnetic hopfions."],"url":"http://arxiv.org/abs/2404.07964v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 17:26:31","title":"Orders of Magnitude Improved Cyclotron-Mode Cooling for Non-Destructive Spin Quantum Transition Spectroscopy with Single Trapped Antiprotons","abstract":"We demonstrate efficient sub-thermal cooling of the modified cyclotron mode of a single trapped antiproton and reach particle temperatures $T_+=E_+/k_\\text{B}$ below $200\\,$mK in preparation times shorter than $500\\,$s. This corresponds to the fastest resistive single-particle cyclotron cooling to sub-thermal temperatures ever demonstrated. By cooling trapped particles to such low energies, we demonstrate the detection of antiproton spin transitions with an error-rate $<0.000025$, more than three orders of magnitude better than in previous best experiments. This method will have enormous impact on multi-Penning-trap experiments that measure magnetic moments with single nuclear spins for tests of matter/antimatter symmetry, high-precision mass-spectrometry, and measurements of electron $g$-factors bound to highly-charged ions that test quantum electrodynamics.","sentences":["We demonstrate efficient sub-thermal cooling of the modified cyclotron mode of a single trapped antiproton and reach particle temperatures $T_+=E_+/k_\\text{B}$ below $200\\,$mK in preparation times shorter than $500\\,$s.","This corresponds to the fastest resistive single-particle cyclotron cooling to sub-thermal temperatures ever demonstrated.","By cooling trapped particles to such low energies, we demonstrate the detection of antiproton spin transitions with an error-rate $<0.000025$, more than three orders of magnitude better than in previous best experiments.","This method will have enormous impact on multi-Penning-trap experiments that measure magnetic moments with single nuclear spins for tests of matter/antimatter symmetry, high-precision mass-spectrometry, and measurements of electron $g$-factors bound to highly-charged ions that test quantum electrodynamics."],"url":"http://arxiv.org/abs/2404.07928v1","category":"physics.atom-ph"}
{"created":"2024-04-11 17:05:09","title":"The effects of stellar feedback on molecular clumps in the Lagoon Nebula (M8)","abstract":"The Lagoon Nebula (M8) is host to multiple regions with recent and ongoing massive star formation. With M8-Main and M8 East, two prominent regions of massive star formation have been studied in detail over the past years, while large parts of the nebula have received little attention. These largely unexplored regions comprise a large sample of molecular clumps that are affected by the presence of massive O- and B-type stars. We establish an inventory of species observed towards 37 known molecular clumps in M8 by conducting an unbiased line survey for each clump. For this, we used APEX and the IRAM 30m telescope for pointed on-off observations on the clumps. These observations cover bandwidths of 53GHz and 40GHz in frequency ranges from 210GHz to 280GHz and from 70GHz to 117GHz, respectively. Temperatures are derived from rotational transitions of CH3CN, CH3C2H and para-H2CO. Additional archival data from the Spitzer, Herschel, MSX, APEX, WISE, JCMT and AKARI telescopes are used to derive physical parameters of the dust emission by fitting spectral energy distributions to the observed flux densities. Across the observed M8 region, we identify 346 transitions from 70 different molecular species, including isotopologues. We detect tracers of photo-dissociation regions across all the clumps and 38% of these clumps show signs of star formation. We find that PDR tracers are most abundant in clumps with relatively lower H2 column densities. When comparing M8 clumps to ATLASGAL sources at similar distances, we find them to be slightly less massive and have compatible luminosities and radii. This possibly indicates a fragmentation of the gas caused by the O- and B-type stars. In contrast, dust temperatures of the clumps in M8 are found to be increased by approximately 5K (25%) indicating substantial external heating of the clumps by radiation of the present massive stars.","sentences":["The Lagoon Nebula (M8) is host to multiple regions with recent and ongoing massive star formation.","With M8-Main and M8 East, two prominent regions of massive star formation have been studied in detail over the past years, while large parts of the nebula have received little attention.","These largely unexplored regions comprise a large sample of molecular clumps that are affected by the presence of massive O- and B-type stars.","We establish an inventory of species observed towards 37 known molecular clumps in M8 by conducting an unbiased line survey for each clump.","For this, we used APEX and the IRAM 30m telescope for pointed on-off observations on the clumps.","These observations cover bandwidths of 53GHz and 40GHz in frequency ranges from 210GHz to 280GHz and from 70GHz to 117GHz, respectively.","Temperatures are derived from rotational transitions of CH3CN, CH3C2H and para-H2CO.","Additional archival data from the Spitzer, Herschel, MSX, APEX, WISE, JCMT and AKARI telescopes are used to derive physical parameters of the dust emission by fitting spectral energy distributions to the observed flux densities.","Across the observed M8 region, we identify 346 transitions from 70 different molecular species, including isotopologues.","We detect tracers of photo-dissociation regions across all the clumps and 38% of these clumps show signs of star formation.","We find that PDR tracers are most abundant in clumps with relatively lower H2 column densities.","When comparing M8 clumps to ATLASGAL sources at similar distances, we find them to be slightly less massive and have compatible luminosities and radii.","This possibly indicates a fragmentation of the gas caused by the O- and B-type stars.","In contrast, dust temperatures of the clumps in M8 are found to be increased by approximately 5K (25%) indicating substantial external heating of the clumps by radiation of the present massive stars."],"url":"http://arxiv.org/abs/2404.07920v1","category":"astro-ph.GA"}
{"created":"2024-04-11 16:43:03","title":"HGRN2: Gated Linear RNNs with State Expansion","abstract":"Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.","sentences":["Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference.","However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.","To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters.","The linear attention form also allows for hardware-efficient training.","Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.","Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens."],"url":"http://arxiv.org/abs/2404.07904v1","category":"cs.CL"}
{"created":"2024-04-11 16:37:50","title":"Novel first-order phase transition and critical points in SU(3) Yang-Mills theory with spatial compactification","abstract":"We investigate the thermodynamics and phase structure of $SU(3)$ Yang-Mills theory on $\\mathbb{T}^2\\times\\mathbb{R}^2$ in Euclidean spacetime in an effective-model approach. The model incorporates two Polyakov loops along two compactified directions as dynamical variables, and is constructed to reproduce thermodynamics on $\\mathbb{T}^2\\times\\mathbb{R}^2$ measured on the lattice. The model analysis indicates the existence of a novel first-order phase transition on $\\mathbb{T}^2\\times\\mathbb{R}^2$ in the deconfined phase, which terminates at critical points that should belong to the two-dimensional $Z_2$ universality class. We argue that the interplay of the Polyakov loops induced by their cross term in the Polyakov-loop potential is responsible for the manifestation of the first-order transition.","sentences":["We investigate the thermodynamics and phase structure of $SU(3)$ Yang-Mills theory on $\\mathbb{T}^2\\times\\mathbb{R}^2$ in Euclidean spacetime in an effective-model approach.","The model incorporates two Polyakov loops along two compactified directions as dynamical variables, and is constructed to reproduce thermodynamics on $\\mathbb{T}^2\\times\\mathbb{R}^2$ measured on the lattice.","The model analysis indicates the existence of a novel first-order phase transition on $\\mathbb{T}^2\\times\\mathbb{R}^2$ in the deconfined phase, which terminates at critical points that should belong to the two-dimensional $Z_2$ universality class.","We argue that the interplay of the Polyakov loops induced by their cross term in the Polyakov-loop potential is responsible for the manifestation of the first-order transition."],"url":"http://arxiv.org/abs/2404.07899v1","category":"hep-ph"}
{"created":"2024-04-11 15:39:10","title":"TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising","abstract":"Blind-spot networks (BSN) have been prevalent network architectures in self-supervised image denoising (SSID). Existing BSNs are mostly conducted with convolution layers. Although transformers offer potential solutions to the limitations of convolutions and have demonstrated success in various image restoration tasks, their attention mechanisms may violate the blind-spot requirement, thus restricting their applicability in SSID. In this paper, we present a transformer-based blind-spot network (TBSN) by analyzing and redesigning the transformer operators that meet the blind-spot requirement. Specifically, TBSN follows the architectural principles of dilated BSNs, and incorporates spatial as well as channel self-attention layers to enhance the network capability. For spatial self-attention, an elaborate mask is applied to the attention matrix to restrict its receptive field, thus mimicking the dilated convolution. For channel self-attention, we observe that it may leak the blind-spot information when the channel number is greater than spatial size in the deep layers of multi-scale architectures. To eliminate this effect, we divide the channel into several groups and perform channel attention separately. Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-the-art SSID methods. The code and pre-trained models will be publicly available at https://github.com/nagejacob/TBSN.","sentences":["Blind-spot networks (BSN) have been prevalent network architectures in self-supervised image denoising (SSID).","Existing BSNs are mostly conducted with convolution layers.","Although transformers offer potential solutions to the limitations of convolutions and have demonstrated success in various image restoration tasks, their attention mechanisms may violate the blind-spot requirement, thus restricting their applicability in SSID.","In this paper, we present a transformer-based blind-spot network (TBSN) by analyzing and redesigning the transformer operators that meet the blind-spot requirement.","Specifically, TBSN follows the architectural principles of dilated BSNs, and incorporates spatial as well as channel self-attention layers to enhance the network capability.","For spatial self-attention, an elaborate mask is applied to the attention matrix to restrict its receptive field, thus mimicking the dilated convolution.","For channel self-attention, we observe that it may leak the blind-spot information when the channel number is greater than spatial size in the deep layers of multi-scale architectures.","To eliminate this effect, we divide the channel into several groups and perform channel attention separately.","Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance.","Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-the-art SSID methods.","The code and pre-trained models will be publicly available at https://github.com/nagejacob/TBSN."],"url":"http://arxiv.org/abs/2404.07846v1","category":"cs.CV"}
{"created":"2024-04-11 15:28:59","title":"Ultra-high spin emission from antiferromagnetic FeRh","abstract":"An antiferromagnet emits spin currents when time-reversal symmetry is broken. This is typically achieved by applying an external magnetic field below and above the spin-flop transition or by optical pumping. In this work we apply optical pump-THz emission spectroscopy to study picosecond spin pumping from metallic FeRh as a function of temperature. Intriguingly we find that in the low-temperature antiferromagnetic phase the laser pulse induces a large and coherent spin pumping, while not crossing into the ferromagnetic phase. With temperature and magnetic field dependent measurements combined with atomistic spin dynamics simulations we show that the antiferromagnetic spin-lattice is destabilised by the combined action of optical pumping and picosecond spin-biasing by the conduction electron population, which results in spin accumulation. We propose that the amplitude of the effect is inherent to the nature of FeRh, particularly the Rh atoms and their high spin susceptibility. We believe that the principles shown here could be used to produce more effective spin current emitters. Our results also corroborate the work of others showing that the magnetic phase transition begins on a very fast picosecond timescale, but this timescale is often hidden by measurements which are confounded by the slower domain dynamics.","sentences":["An antiferromagnet emits spin currents when time-reversal symmetry is broken.","This is typically achieved by applying an external magnetic field below and above the spin-flop transition or by optical pumping.","In this work we apply optical pump-THz emission spectroscopy to study picosecond spin pumping from metallic FeRh as a function of temperature.","Intriguingly we find that in the low-temperature antiferromagnetic phase the laser pulse induces a large and coherent spin pumping, while not crossing into the ferromagnetic phase.","With temperature and magnetic field dependent measurements combined with atomistic spin dynamics simulations we show that the antiferromagnetic spin-lattice is destabilised by the combined action of optical pumping and picosecond spin-biasing by the conduction electron population, which results in spin accumulation.","We propose that the amplitude of the effect is inherent to the nature of FeRh, particularly the Rh atoms and their high spin susceptibility.","We believe that the principles shown here could be used to produce more effective spin current emitters.","Our results also corroborate the work of others showing that the magnetic phase transition begins on a very fast picosecond timescale, but this timescale is often hidden by measurements which are confounded by the slower domain dynamics."],"url":"http://arxiv.org/abs/2404.07841v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 15:19:46","title":"Towards a realistic noise modelling of quantum sensors for future satellite gravity missions","abstract":"Cold Atom Interferometry accelerometers and gradiometers have emerged as promising candidates for future gravimetric satellite missions due to their potential for detecting gravitational forces and gradients with high precision and accuracy. Mapping the Earth's gravity field from space offers valuable insights into climate change, hydro- and biosphere evolution, and seismic activity prediction. Current satellite gravimetry missions have demonstrated the utility of gravity data in understanding global mass transport phenomena, climate dynamics, and geological processes. However, state-of-the-art measurement techniques face noise and long-term drift limitations, which might propagate on the recovery of Earth's time-varying gravity field. Quantum sensors, particularly atom interferometry-based devices, offer promise for improving the accuracy and stability of space-based gravity measurements. This study explores the sensitivity of CAI accelerometers and gradiometers. We explore the low-low satellite-to-satellite and gravity gradiometry measurements to build analytical models of measurements and associated errors. We selected an ambitious scenario for CAI parameters that illustrates a potential path for increasing instrument accuracies and capabilities for space gravimetry. Two operational modes, concurrent and sequential, are compared to mitigate the effects of inaccurately known attitude rates on Coriolis accelerations. The sequential mode shows the potential to reduce these effects, enabling accurate measurements for low-low Satellite-to-Satellite Tracking missions in the near future. Attitude determination is discussed, highlighting the importance of accurate measurements to reconstruct Coriolis accelerations and related to errors in the reference frame rotation from body or local frames to the Earth co-rotating frame.","sentences":["Cold Atom Interferometry accelerometers and gradiometers have emerged as promising candidates for future gravimetric satellite missions due to their potential for detecting gravitational forces and gradients with high precision and accuracy.","Mapping the Earth's gravity field from space offers valuable insights into climate change, hydro- and biosphere evolution, and seismic activity prediction.","Current satellite gravimetry missions have demonstrated the utility of gravity data in understanding global mass transport phenomena, climate dynamics, and geological processes.","However, state-of-the-art measurement techniques face noise and long-term drift limitations, which might propagate on the recovery of Earth's time-varying gravity field.","Quantum sensors, particularly atom interferometry-based devices, offer promise for improving the accuracy and stability of space-based gravity measurements.","This study explores the sensitivity of CAI accelerometers and gradiometers.","We explore the low-low satellite-to-satellite and gravity gradiometry measurements to build analytical models of measurements and associated errors.","We selected an ambitious scenario for CAI parameters that illustrates a potential path for increasing instrument accuracies and capabilities for space gravimetry.","Two operational modes, concurrent and sequential, are compared to mitigate the effects of inaccurately known attitude rates on Coriolis accelerations.","The sequential mode shows the potential to reduce these effects, enabling accurate measurements for low-low Satellite-to-Satellite Tracking missions in the near future.","Attitude determination is discussed, highlighting the importance of accurate measurements to reconstruct Coriolis accelerations and related to errors in the reference frame rotation from body or local frames to the Earth co-rotating frame."],"url":"http://arxiv.org/abs/2404.07835v1","category":"physics.ins-det"}
{"created":"2024-04-11 15:00:39","title":"Topology-engineered orbital Hall effect in two-dimensional ferromagnets","abstract":"Recent advances in manipulation of orbital angular momentum (OAM) within the paradigm of orbitronics present a promising avenue for the design of future electronic devices. In this context, the recently observed orbital Hall effect (OHE) occupies a special place. Here, focusing on both the second-order topological and quantum anomalous Hall insulators in two-dimensional ferromagnets, we demonstrate that topological phase transitions present an efficient and straightforward way to engineer the OHE, where the OAM distribution can be controlled by the nature of the band inversion. Using first-principles calculations, we identify Janus RuBrCl and three septuple layers of MnBi$_2$Te$_4$ as experimentally feasible examples of the proposed mechanism of OHE engineering by topology. With our work we open up new possibilities for innovative applications in topological spintronics and orbitronics.","sentences":["Recent advances in manipulation of orbital angular momentum (OAM) within the paradigm of orbitronics present a promising avenue for the design of future electronic devices.","In this context, the recently observed orbital Hall effect (OHE) occupies a special place.","Here, focusing on both the second-order topological and quantum anomalous Hall insulators in two-dimensional ferromagnets, we demonstrate that topological phase transitions present an efficient and straightforward way to engineer the OHE, where the OAM distribution can be controlled by the nature of the band inversion.","Using first-principles calculations, we identify Janus RuBrCl and three septuple layers of MnBi$_2$Te$_4$ as experimentally feasible examples of the proposed mechanism of OHE engineering by topology.","With our work we open up new possibilities for innovative applications in topological spintronics and orbitronics."],"url":"http://arxiv.org/abs/2404.07820v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 14:43:23","title":"X-ray imaging and electron temperature evolution in laser-driven magnetic reconnection experiments at the National Ignition Facility","abstract":"We present results from X-ray imaging of high-aspect-ratio magnetic reconnection experiments driven at the National Ignition Facility. Two parallel, self-magnetized, elongated laser-driven plumes are produced by tiling 40 laser beams. A magnetic reconnection layer is formed by the collision of the plumes. A gated X-ray framing pinhole camera with micro-channel plate (MCP) detector produces multiple images through various filters of the formation and evolution of both the plumes and current sheet. As the diagnostic integrates plasma self-emission along the line of sight, 2-dimensional electron temperature maps $\\langle T_e \\rangle_Y$ are constructed by taking the ratio of intensity of these images obtained with different filters. The plumes have a characteristic temperature $\\langle T_e \\rangle_Y = 240 \\pm 20$ eV at 2 ns after the initial laser irradiation and exhibit a slow cooling up to 4 ns. The reconnection layer forms at 3 ns with a temperature $\\langle T_e \\rangle_Y = 280 \\pm 50$ eV as the result of the collision of the plumes. The error bars of the plumes and current sheet temperatures separate at $4$ ns, showing the heating of the current sheet from colder inflows. Using a semi-analytical model, we find that the observed heating of the current sheet is consistent with being produced by electron-ion drag, rather than the conversion of magnetic to kinetic energy.","sentences":["We present results from X-ray imaging of high-aspect-ratio magnetic reconnection experiments driven at the National Ignition Facility.","Two parallel, self-magnetized, elongated laser-driven plumes are produced by tiling 40 laser beams.","A magnetic reconnection layer is formed by the collision of the plumes.","A gated X-ray framing pinhole camera with micro-channel plate (MCP) detector produces multiple images through various filters of the formation and evolution of both the plumes and current sheet.","As the diagnostic integrates plasma self-emission along the line of sight, 2-dimensional electron temperature maps $\\langle T_e \\rangle_Y$ are constructed by taking the ratio of intensity of these images obtained with different filters.","The plumes have a characteristic temperature $\\langle T_e \\rangle_Y = 240 \\pm 20$ eV at 2 ns after the initial laser irradiation and exhibit a slow cooling up to 4 ns.","The reconnection layer forms at 3 ns with a temperature $\\langle T_e \\rangle_Y = 280 \\pm 50$ eV as the result of the collision of the plumes.","The error bars of the plumes and current sheet temperatures separate at $4$ ns, showing the heating of the current sheet from colder inflows.","Using a semi-analytical model, we find that the observed heating of the current sheet is consistent with being produced by electron-ion drag, rather than the conversion of magnetic to kinetic energy."],"url":"http://arxiv.org/abs/2404.07799v1","category":"physics.plasm-ph"}
{"created":"2024-04-11 14:38:51","title":"Illicit Promotion on Twitter","abstract":"In this paper, we present an extensive study of the promotion of illicit goods and services on Twitter, a popular online social network(OSN). This study is made possible through the design and implementation of multiple novel tools for detecting and analyzing illicit promotion activities as well as their underlying campaigns. As the results, we observe that illicit promotion is prevalent on Twitter, along with noticeable existence on other three popular OSNs including Youtube, Facebook, and TikTok. Particularly, 12 million distinct posts of illicit promotion (PIPs) have been observed on the Twitter platform, which are widely distributed in 5 major natural languages and 10 categories of illicit goods and services, e.g., drugs, data leakage, gambling, and weapon sales. What are also observed are 580K Twitter accounts publishing PIPs as well as 37K distinct instant messaging (IM) accounts that are embedded in PIPs and serve as next hops of communication, which strongly indicates that the campaigns underpinning PIPs are also of a large scale. Also, an arms race between Twitter and illicit promotion operators is also observed. On one hand, Twitter is observed to conduct content moderation in a continuous manner and almost 80% PIPs will get gradually unpublished within six months since posted. However, in the meantime, miscreants adopt various evasion tactics to masquerade their PIPs, which renders more than 90% PIPs keeping hidden from the detection radar for two months or longer.","sentences":["In this paper, we present an extensive study of the promotion of illicit goods and services on Twitter, a popular online social network(OSN).","This study is made possible through the design and implementation of multiple novel tools for detecting and analyzing illicit promotion activities as well as their underlying campaigns.","As the results, we observe that illicit promotion is prevalent on Twitter, along with noticeable existence on other three popular OSNs including Youtube, Facebook, and TikTok.","Particularly, 12 million distinct posts of illicit promotion (PIPs) have been observed on the Twitter platform, which are widely distributed in 5 major natural languages and 10 categories of illicit goods and services, e.g., drugs, data leakage, gambling, and weapon sales.","What are also observed are 580K Twitter accounts publishing PIPs as well as 37K distinct instant messaging (IM) accounts that are embedded in PIPs and serve as next hops of communication, which strongly indicates that the campaigns underpinning PIPs are also of a large scale.","Also, an arms race between Twitter and illicit promotion operators is also observed.","On one hand, Twitter is observed to conduct content moderation in a continuous manner and almost 80% PIPs will get gradually unpublished within six months since posted.","However, in the meantime, miscreants adopt various evasion tactics to masquerade their PIPs, which renders more than 90% PIPs keeping hidden from the detection radar for two months or longer."],"url":"http://arxiv.org/abs/2404.07797v1","category":"cs.CR"}
{"created":"2024-04-11 14:29:35","title":"An equilibrium-seeking search algorithm for integrating large-scale activity-based and dynamic traffic assignment models","abstract":"This paper proposes an iterative methodology to integrate large-scale behavioral activity-based models with dynamic traffic assignment models. The main novelty of the proposed approach is the decoupling of the two parts, allowing the ex-post integration of any existing model as long as certain assumptions are satisfied. A measure of error is defined to characterize a search space easily explorable within its boundaries. Within it, a joint distribution of the number of trips and travel times is identified as the equilibrium distribution, i.e., the distribution for which trip numbers and travel times are bound in the neighborhood of the equilibrium between supply and demand. The approach is tested on a medium-sized city of 400,000 inhabitants and the results suggest that the proposed iterative approach does perform well, reaching equilibrium between demand and supply in a limited number of iterations thanks to its perturbation techniques. Overall, 15 iterations are needed to reach values of the measure of error lower than 10%. The equilibrium identified this way is then validated against baseline distributions to demonstrate the goodness of the results.","sentences":["This paper proposes an iterative methodology to integrate large-scale behavioral activity-based models with dynamic traffic assignment models.","The main novelty of the proposed approach is the decoupling of the two parts, allowing the ex-post integration of any existing model as long as certain assumptions are satisfied.","A measure of error is defined to characterize a search space easily explorable within its boundaries.","Within it, a joint distribution of the number of trips and travel times is identified as the equilibrium distribution, i.e., the distribution for which trip numbers and travel times are bound in the neighborhood of the equilibrium between supply and demand.","The approach is tested on a medium-sized city of 400,000 inhabitants and the results suggest that the proposed iterative approach does perform well, reaching equilibrium between demand and supply in a limited number of iterations thanks to its perturbation techniques.","Overall, 15 iterations are needed to reach values of the measure of error lower than 10%.","The equilibrium identified this way is then validated against baseline distributions to demonstrate the goodness of the results."],"url":"http://arxiv.org/abs/2404.07789v1","category":"cs.CY"}
{"created":"2024-04-11 14:25:00","title":"On the approximation of the Dirac operator coupled with confining Lorentz scalar $\u03b4$-shell interactions","abstract":"Let $\\Omega_+\\subset\\mathbb{R}^{3}$ be a fixed bounded domain with boundary $\\Sigma = \\partial\\Omega_{+}$. We consider $\\mathcal{U}^\\varepsilon$ a tubular neighborhood of the surface $\\Sigma$ with a thickness parameter $\\varepsilon>0$, and we define the perturbed Dirac operator $\\mathfrak{D}^{\\varepsilon}_{M}=D_m +M\\beta \\mathbb{1}_{\\mathcal{U}^{\\varepsilon}},$ with $D_m$ the free Dirac operator, $M>0$, and $\\mathbb{1}_{\\mathcal{U }^{\\varepsilon}}$ the characteristic function of $\\mathcal{U}^{\\varepsilon}$. Then, in the norm resolvent sense, the Dirac operator $\\mathfrak{D}^{\\varepsilon}_M$ converges to the Dirac operator coupled with Lorentz scalar $\\delta$-shell interactions as $\\varepsilon = M^{-1}$ tends to $0$, with a convergence rate of $\\mathcal{O}(M^{-1})$.","sentences":["Let $\\Omega_+\\subset\\mathbb{R}^{3}$ be a fixed bounded domain with boundary $\\Sigma = \\partial\\Omega_{+}$. We consider $\\mathcal{U}^\\varepsilon$ a tubular neighborhood of the surface $\\Sigma$ with a thickness parameter $\\varepsilon>0$, and we define the perturbed Dirac operator $\\mathfrak{D}^{\\varepsilon}_{M}=D_m +M\\beta \\mathbb{1}_{\\mathcal{U}^{\\varepsilon}},$ with $D_m$ the free Dirac operator, $M>0$, and $\\mathbb{1}_{\\mathcal{U }^{\\varepsilon}}$ the characteristic function of $\\mathcal{U}^{\\varepsilon}$. Then, in the norm resolvent sense, the Dirac operator $\\mathfrak{D}^{\\varepsilon}_M$ converges to the Dirac operator coupled with Lorentz scalar $\\delta$-shell interactions as $\\varepsilon = M^{-1}$ tends to $0$, with a convergence rate of $\\mathcal{O}(M^{-1})$."],"url":"http://arxiv.org/abs/2404.07784v1","category":"math.SP"}
{"created":"2024-04-11 14:14:30","title":"Improved analysis of double $J/\u03c8$ production in $Z$-boson decay","abstract":"In this paper, we present an improved calculation for the decay rate of the rare $Z$-boson decay into $J/\\psi + J/\\psi$. This decay is dominated by the photon fragmentation mechanism, i.e., the transition $Z\\to J/\\psi + \\gamma^{*}$ followed by the fragmentation $\\gamma^{*}\\to J/\\psi$. In our calculation, the amplitude of $\\gamma^{*}\\to J/\\psi$ is extracted from the measured value of $\\Gamma(J/\\psi \\to e^+ e^-)$, and the amplitude of $Z\\to J/\\psi + \\gamma^{*}$ is calculate through the light-cone approach. The higher-order QCD and relativistic corrections in the amplitude of $\\gamma^{*}\\to J/\\psi$ and the large logarithms of $m_{_Z}^2/m_c^2$ that appear in the amplitude of $Z\\to J/\\psi + \\gamma^{*}$ are resummed in our calculation. Besides, the non-fragmentation amplitude is calculated based on the NRQCD factorization, and the next-to-leading order QCD and relativistic corrections are included. The obtained branching fraction for this $Z$ decay channel is $8.66 ^{+1.48} _{-0.69}\\times 10^{-11}$.","sentences":["In this paper, we present an improved calculation for the decay rate of the rare $Z$-boson decay into $J/\\psi + J/\\psi$.","This decay is dominated by the photon fragmentation mechanism, i.e., the transition $Z\\to J/\\psi + \\gamma^{*}$ followed by the fragmentation $\\gamma^{*}\\to J/\\psi$.","In our calculation, the amplitude of $\\gamma^{*}\\to J/\\psi$ is extracted from the measured value of $\\Gamma(J/\\psi \\to e^+ e^-)$, and the amplitude of $Z\\to J/\\psi + \\gamma^{*}$ is calculate through the light-cone approach.","The higher-order QCD and relativistic corrections in the amplitude of $\\gamma^{*}\\to J/\\psi$ and the large logarithms of $m_{_Z}^2/m_c^2$ that appear in the amplitude of $Z\\to J/\\psi + \\gamma^{*}$ are resummed in our calculation.","Besides, the non-fragmentation amplitude is calculated based on the NRQCD factorization, and the next-to-leading order QCD and relativistic corrections are included.","The obtained branching fraction for this $Z$ decay channel is $8.66 ^{+1.48} _{-0.69}\\times 10^{-11}$."],"url":"http://arxiv.org/abs/2404.07777v1","category":"hep-ph"}
{"created":"2024-04-11 14:08:37","title":"Respective Roles of Electron-Phonon and Electron-Electron Interactions in the Transport and Quasiparticle Properties of SrVO$_3$","abstract":"The spectral and transport properties of strongly correlated metals, such as SrVO$_3$ (SVO), are widely attributed to electron-electron ($e$-$e$) interactions, with lattice vibrations (phonons) playing a secondary role. Here, using first-principles electron-phonon ($e$-ph) and dynamical mean field theory calculations, we show that $e$-ph interactions play an essential role in SVO: they govern the electron scattering and resistivity in a wide temperature range down to 30 K, and induce an experimentally observed kink in the spectral function. In contrast, the $e$-$e$ interactions control quasiparticle renormalizations and low temperature transport, and enhance the $e$-ph coupling. We clarify the origin of the near $T^2$ temperature dependence of the resistivity by analyzing the $e$-$e$ and $e$-ph limited transport regimes. Our work disentangles the electronic and lattice degrees of freedom in a prototypical correlated metal, revealing the dominant role of $e$-ph interactions in SVO.","sentences":["The spectral and transport properties of strongly correlated metals, such as SrVO$_3$ (SVO), are widely attributed to electron-electron ($e$-$e$) interactions, with lattice vibrations (phonons) playing a secondary role.","Here, using first-principles electron-phonon ($e$-ph) and dynamical mean field theory calculations, we show that $e$-ph interactions play an essential role in SVO: they govern the electron scattering and resistivity in a wide temperature range down to 30 K, and induce an experimentally observed kink in the spectral function.","In contrast, the $e$-$e$ interactions control quasiparticle renormalizations and low temperature transport, and enhance the $e$-ph coupling.","We clarify the origin of the near $T^2$ temperature dependence of the resistivity by analyzing the $e$-$e$ and $e$-ph limited transport regimes.","Our work disentangles the electronic and lattice degrees of freedom in a prototypical correlated metal, revealing the dominant role of $e$-ph interactions in SVO."],"url":"http://arxiv.org/abs/2404.07772v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 14:02:27","title":"Low-symmetry polymorph of GaP upends bonding paradigms of metallic high-pressure III-V compounds","abstract":"The pressure-induced polymorphism of binary octect compounds has long been considered a settled problem although the possible atomic disordering of some phases remains a puzzling observation. Taking GaP as a case study, we conclude, through x-ray microdiffraction and first-principles calculations, that its high-pressure phase II (previously reported as being disordered) adopts in fact an ordered base-centered monoclinic structure previously unknown in this class of compounds. The formation of layered patterns with variable degrees of interlayer dimerization, as observed in GaP, marks a paradigm shift of our understanding of ordering in octect high-pressure phases which calls for a more extensive re-examination. A rich polymorphism with fine tuning of chemical and physical properties can be envisioned.","sentences":["The pressure-induced polymorphism of binary octect compounds has long been considered a settled problem although the possible atomic disordering of some phases remains a puzzling observation.","Taking GaP as a case study, we conclude, through x-ray microdiffraction and first-principles calculations, that its high-pressure phase II (previously reported as being disordered) adopts in fact an ordered base-centered monoclinic structure previously unknown in this class of compounds.","The formation of layered patterns with variable degrees of interlayer dimerization, as observed in GaP, marks a paradigm shift of our understanding of ordering in octect high-pressure phases which calls for a more extensive re-examination.","A rich polymorphism with fine tuning of chemical and physical properties can be envisioned."],"url":"http://arxiv.org/abs/2404.07758v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 14:01:44","title":"Flavor asymmetry from the non-perturbative nucleon sea","abstract":"We demonstrate, in the context of a scalar version of the chiral effective field theory, that the multi-sea quark contribution to the nucleon is significant and highly non-trivial in sharp contrast to the prediction of perturbation theory. The non-perturbative calculation is performed in the Fock sector dependent renormalization scheme on the light front, in which the non-perturbative renormalization is incorporated. The calculation suggests that a fully non-perturbative calculation of the chiral EFT is needed to obtain a robust result to be compared with the recent experimental measurement of flavor asymmetry in the proton.","sentences":["We demonstrate, in the context of a scalar version of the chiral effective field theory, that the multi-sea quark contribution to the nucleon is significant and highly non-trivial in sharp contrast to the prediction of perturbation theory.","The non-perturbative calculation is performed in the Fock sector dependent renormalization scheme on the light front, in which the non-perturbative renormalization is incorporated.","The calculation suggests that a fully non-perturbative calculation of the chiral EFT is needed to obtain a robust result to be compared with the recent experimental measurement of flavor asymmetry in the proton."],"url":"http://arxiv.org/abs/2404.07755v1","category":"hep-ph"}
{"created":"2024-04-11 13:42:24","title":"An Investigation of the Current Star Formation Rate of Star-forming Galaxies at z < 0.5 with FADO","abstract":"The star formation rate (SFR) is a crucial astrophysical tracer for understanding the formation and evolution of galaxies, determining the interaction between interstellar medium properties and star formation. The mainstream approach to study the stellar content in galaxies relies on pure stellar population synthesis models. However, these methods fail to account for the contamination of SFR caused by nebular gas radiation. Recent studies have indicated that neglecting nebular radiation contamination appears to be non-negligible in galaxies with intense star-forming activities and at relatively high redshifts, potentially leading to overestimation of stellar masses. However, there is currently limited targeted research, particularly regarding galaxies at higher redshifts (z < 0.5). In this investigation, we employ the BPT diagram to select a sample of 2575 star-formation galaxies (SFG) from the SDSS-DR18 dataset, all within specified signal-to-noise ratio (S/N) ranges. Using the spectroscopic fitting tool FADO, which is capable of excluding nebular radiation contributions in spectral fitting, we conduct a tentative investigation of the SFR of star-forming galaxies in SDSS- DR18 with redshifts z < 0.5. Our results show that 45% of the samples show H{\\alpha}{\\lambda}6563 obtained from FADO fitting to be smaller than that derived solely from the pure stellar population synthesis model qsofitmore, particularly pronounced between redshifts 0.2 and 0.4. We find that the contribution of nebulae is significant and exhibits an evolutionary trend with redshift. We anticipate that by combining optical and near-infrared spectral data, the influence of nebulae may become more prominent in star-forming galaxies at higher redshifts.","sentences":["The star formation rate (SFR) is a crucial astrophysical tracer for understanding the formation and evolution of galaxies, determining the interaction between interstellar medium properties and star formation.","The mainstream approach to study the stellar content in galaxies relies on pure stellar population synthesis models.","However, these methods fail to account for the contamination of SFR caused by nebular gas radiation.","Recent studies have indicated that neglecting nebular radiation contamination appears to be non-negligible in galaxies with intense star-forming activities and at relatively high redshifts, potentially leading to overestimation of stellar masses.","However, there is currently limited targeted research, particularly regarding galaxies at higher redshifts (z < 0.5).","In this investigation, we employ the BPT diagram to select a sample of 2575 star-formation galaxies (SFG) from the SDSS-DR18 dataset, all within specified signal-to-noise ratio (S/N) ranges.","Using the spectroscopic fitting tool FADO, which is capable of excluding nebular radiation contributions in spectral fitting, we conduct a tentative investigation of the SFR of star-forming galaxies in SDSS- DR18 with redshifts z < 0.5.","Our results show that 45% of the samples show H{\\alpha}{\\lambda}6563 obtained from FADO fitting to be smaller than that derived solely from the pure stellar population synthesis model qsofitmore, particularly pronounced between redshifts 0.2 and 0.4.","We find that the contribution of nebulae is significant and exhibits an evolutionary trend with redshift.","We anticipate that by combining optical and near-infrared spectral data, the influence of nebulae may become more prominent in star-forming galaxies at higher redshifts."],"url":"http://arxiv.org/abs/2404.07744v1","category":"astro-ph.GA"}
{"created":"2024-04-11 13:40:59","title":"Baryon junction effects on charmonium production in pp and pPb collisions","abstract":"In this work we study the effects of the proton and $Pb$ internal structure on charmonium production at high multiplicities. To do this, we assume that the nucleon has an Y shape, which means that the effective quarks (quarks + antiquarks + gluons) are in the extremities of the Y, connected by gluon lines. Since the quarks are in the periphery and the gluons are in the center, as $pp$ collisions go from peripheral to more central and then to ultra-central collisions, the interactions change from quark-quark to gluon-gluon. The enhanced growth of $J/\\psi$ occurs because $\\sigma( g + g \\to c + \\bar{c}) \\gg \\sigma( q + \\bar{q} \\to c + \\bar{c})$. In the $p-Pb$ collisions the enhancement is caused by the high concentration of partons in the center of the nuclei. These effects can explain the growth seen in the data and give support to the Y picture of the proton.","sentences":["In this work we study the effects of the proton and $Pb$ internal structure on charmonium production at high multiplicities.","To do this, we assume that the nucleon has an Y shape, which means that the effective quarks (quarks + antiquarks + gluons) are in the extremities of the Y, connected by gluon lines.","Since the quarks are in the periphery and the gluons are in the center, as $pp$ collisions go from peripheral to more central and then to ultra-central collisions, the interactions change from quark-quark to gluon-gluon.","The enhanced growth of $J/\\psi$ occurs because $\\sigma( g + g \\to c + \\bar{c}) \\gg \\sigma( q + \\bar{q} \\to c +","\\bar{c})$.","In the $p-Pb$ collisions the enhancement is caused by the high concentration of partons in the center of the nuclei.","These effects can explain the growth seen in the data and give support to the Y picture of the proton."],"url":"http://arxiv.org/abs/2404.07743v1","category":"hep-ph"}
{"created":"2024-04-11 13:28:22","title":"A twist for nonlocal thermoelectricity in quantum wires as a signature of Bogoliubov-Fermi points","abstract":"We study nonlocal thermoelectricity in a superconducting wire subject to spin-orbit coupling and a magnetic field with a relative orientation $\\theta$ between them. We calculate the current flowing in a normal probe attached to the bulk of a superconducting wire, as a result of a temperature difference applied at the ends of the wire. We find that the thermoelectric response occurs in ranges of the angles $\\theta$ which correspond to the emergence of Bogoliubov-Fermi points in the energy spectrum of the superconducting wire.","sentences":["We study nonlocal thermoelectricity in a superconducting wire subject to spin-orbit coupling and a magnetic field with a relative orientation $\\theta$ between them.","We calculate the current flowing in a normal probe attached to the bulk of a superconducting wire, as a result of a temperature difference applied at the ends of the wire.","We find that the thermoelectric response occurs in ranges of the angles $\\theta$ which correspond to the emergence of Bogoliubov-Fermi points in the energy spectrum of the superconducting wire."],"url":"http://arxiv.org/abs/2404.07734v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-11 13:25:43","title":"Characteristics of temporal variability of long-duration bursts of high-energy radiation associated with thunderclouds on the Tibetan plateau","abstract":"From 1998 to 2017, neutron monitors located at an altitude of 4300 m on the Tibetan plateau detected 127 long-duration bursts of high-energy radiation in association with thunderclouds. These bursts typically lasted for 10 to 40 minutes, and 89\\% of them occurred between 10:00 and 24:00 local time. They were also found to be more likely to occur at night, especially during 18:00$-$06:00 local time period. The observed diurnal and seasonal variations in burst frequency were consistent with the frequencies of lightning and precipitation on the Tibetan plateau. Based on 19 years of data, the present study suggests that an annual variation in burst frequency has a periodicity of $\\sim$16 years and a lag of $\\sim$3 years relative to solar activity.","sentences":["From 1998 to 2017, neutron monitors located at an altitude of 4300 m on the Tibetan plateau detected 127 long-duration bursts of high-energy radiation in association with thunderclouds.","These bursts typically lasted for 10 to 40 minutes, and 89\\% of them occurred between 10:00 and 24:00 local time.","They were also found to be more likely to occur at night, especially during 18:00$-$06:00 local time period.","The observed diurnal and seasonal variations in burst frequency were consistent with the frequencies of lightning and precipitation on the Tibetan plateau.","Based on 19 years of data, the present study suggests that an annual variation in burst frequency has a periodicity of $\\sim$16 years and a lag of $\\sim$3 years relative to solar activity."],"url":"http://arxiv.org/abs/2404.07733v1","category":"physics.geo-ph"}
{"created":"2024-04-11 12:51:15","title":"Thirty years of the Beauty Conference","abstract":"This paper remembers thirty years of the Beauty conference series and celebrates its 20th meeting. The conference highlights are reviewed.","sentences":["This paper remembers thirty years of the Beauty conference series and celebrates its 20th meeting.","The conference highlights are reviewed."],"url":"http://arxiv.org/abs/2404.07706v1","category":"hep-ex"}
{"created":"2024-04-11 12:41:37","title":"Leptonic and semileptonic decays of mesons in the Domain model of QCD vacuum","abstract":"The leptonic and semileptonic decays of mesons are investigated within the Domain model of QCD vacuum and hadronization. The Domain Model is the mean-field approach based on the statistical ensemble of almost everywhere homogeneous Abelian (anti-)self-dual gluon fields which reproduces main features of low-energy QCD and allows to deduce a nonlocal effective meson action. Using this meson action, the leptonic decay constants, form factors and branching ratios of semileptonic decays are evaluated simultaneously with masses of mesons. The results are compared to experimental data or other approaches.","sentences":["The leptonic and semileptonic decays of mesons are investigated within the Domain model of QCD vacuum and hadronization.","The Domain Model is the mean-field approach based on the statistical ensemble of almost everywhere homogeneous Abelian (anti-)self-dual gluon fields which reproduces main features of low-energy QCD and allows to deduce a nonlocal effective meson action.","Using this meson action, the leptonic decay constants, form factors and branching ratios of semileptonic decays are evaluated simultaneously with masses of mesons.","The results are compared to experimental data or other approaches."],"url":"http://arxiv.org/abs/2404.07695v1","category":"hep-ph"}
{"created":"2024-04-11 12:18:59","title":"Productions of $X(3872)$/$Z_c(3900)$ and $X_2(4013)$/$Z_c(4020)$ in $Y(4220)$ and $Y(4360)$ decays","abstract":"The two excited vector charmonium states $Y(4220)$ and $Y(4360)$ are difficult to be understood as pure $c\\bar{c}$ charmonium states. Since they are located close to the mass thresholds of $\\bar{D}D_{1}$ and $\\bar{D}^*D_{1}$, they can be viewed as $\\bar{D}D_{1}$ and $\\bar{D}^*D_{1}$ molecules. Furthermore, recent studies indicated that the exotic states $X(3872)$/$Z_c(3900)$ and $X_2(4013)$/$Z_c(4020)$ are the isoscalar/isovector $\\bar{D}D^{*}$ and isoscalar/isovector $\\bar{D}^*D^{*}$ molecules, respectively. In this work, in the molecular picture, we employ the triangle diagram mechanism to study the productions of $ Z_{c}(3900) $ and $X(3872)$ in the pionic and radiative decays of $Y(4220)$, as well as their heavy-quark spin symmetry (HQSS) partners, i.e., the productions of $Z_{c}(4020)$ and $X_2(4013)$ in the pionic and radiative decays of $Y(4360)$. Using the effective Lagrangian approach, we obtain the ratios of the branching fractions $\\mathcal{B}[Y(4360) \\to Z_c(4020)\\pi]/\\mathcal{B}[Y(4220)\\to Z_c(3900)\\pi]=1.2$ and $\\mathcal{B}[Y(4360)\\to X_2(4013) \\gamma]/\\mathcal{B}[Y(4220)\\to X(3872)\\gamma]=0.5$, almost independent of model parameters, which indicate that the productions of $X_2(4013)$ and $Z_c(4020)$ in the radiative and pionic decays of $Y(4360)$ are likely to be measured in the future. The experimental studies of the predicted decay modes will help verify the molecular nature of $X(3872)$, $Z_c(3900)$, and $Y(4220)$. We hope the present work can stimulate experimental and further theoretical studies on these decay modes.","sentences":["The two excited vector charmonium states $Y(4220)$ and $Y(4360)$ are difficult to be understood as pure $c\\bar{c}$ charmonium states.","Since they are located close to the mass thresholds of $\\bar{D}D_{1}$ and $\\bar{D}^*D_{1}$, they can be viewed as $\\bar{D}D_{1}$ and $\\bar{D}^*D_{1}$ molecules.","Furthermore, recent studies indicated that the exotic states $X(3872)$/$Z_c(3900)$ and $X_2(4013)$/$Z_c(4020)$ are the isoscalar/isovector $\\bar{D}D^{*}$ and isoscalar/isovector $\\bar{D}^*D^{*}$ molecules, respectively.","In this work, in the molecular picture, we employ the triangle diagram mechanism to study the productions of $ Z_{c}(3900) $ and $X(3872)$ in the pionic and radiative decays of $Y(4220)$, as well as their heavy-quark spin symmetry (HQSS) partners, i.e., the productions of $Z_{c}(4020)$ and $X_2(4013)$ in the pionic and radiative decays of $Y(4360)$. Using the effective Lagrangian approach, we obtain the ratios of the branching fractions $\\mathcal{B}[Y(4360)","\\to Z_c(4020)\\pi]/\\mathcal{B}[Y(4220)\\to Z_c(3900)\\pi]=1.2$ and $\\mathcal{B}[Y(4360)\\to X_2(4013) \\gamma]/\\mathcal{B}[Y(4220)\\to X(3872)\\gamma]=0.5$, almost independent of model parameters, which indicate that the productions of $X_2(4013)$ and $Z_c(4020)$ in the radiative and pionic decays of $Y(4360)$ are likely to be measured in the future.","The experimental studies of the predicted decay modes will help verify the molecular nature of $X(3872)$, $Z_c(3900)$, and $Y(4220)$. We hope the present work can stimulate experimental and further theoretical studies on these decay modes."],"url":"http://arxiv.org/abs/2404.07681v1","category":"hep-ph"}
{"created":"2024-04-11 11:56:18","title":"Humboldt Highway II -- computer cluster on renewable energies","abstract":"In August 2023, IT experts and scientists came together for a workshop to discuss the possibilities of building a computer cluster fully on renewable energies, as a test-case at Havana University in Cuba. The discussion covered the scientific needs for a computer cluster for particle physics at the InSTEC institute at Havana University, the possibilities to use solar energy, new developments in computing technologies, and computer cluster operation as well as operational needs for computing in particle physics. This computer cluster on renewable energies at the InSTEC institute is seen as a prototype for a large-scale computer cluster on renewable energies for scientific computing in the Caribbean, hosted in Cuba. The project is called \"Humboldt Highway\", to remember Alexander von Humboldt's achievements in bringing cultures of the American and European continents closer together by exchange and travel. In this spirit, we propose a project that enables and intensifies the scientific exchange between research laboratories and universities in Europe and the Caribbean, in particular Cuba.","sentences":["In August 2023, IT experts and scientists came together for a workshop to discuss the possibilities of building a computer cluster fully on renewable energies, as a test-case at Havana University in Cuba.","The discussion covered the scientific needs for a computer cluster for particle physics at the InSTEC institute at Havana University, the possibilities to use solar energy, new developments in computing technologies, and computer cluster operation as well as operational needs for computing in particle physics.","This computer cluster on renewable energies at the InSTEC institute is seen as a prototype for a large-scale computer cluster on renewable energies for scientific computing in the Caribbean, hosted in Cuba.","The project is called \"Humboldt Highway\", to remember Alexander von Humboldt's achievements in bringing cultures of the American and European continents closer together by exchange and travel.","In this spirit, we propose a project that enables and intensifies the scientific exchange between research laboratories and universities in Europe and the Caribbean, in particular Cuba."],"url":"http://arxiv.org/abs/2404.07665v1","category":"physics.soc-ph"}
{"created":"2024-04-11 10:38:57","title":"Symmetric top molecule YbOCH$_3$ in the fundamental $\\mathcal{P}$, $\\mathcal{T}$-violation searches","abstract":"The symmetric top molecule YbOCH$_3$ is studied for its potential to $\\mathcal{P}$, $\\mathcal{T}$-violation searches. The influence of the rotations and vibrations of the YbOCH$_3$ on such violating effects as the electron electric dipole moment (eEDM) and the scalar-pseudoscalar electron-nucleon interaction (Ne-SPS) is studied using the coupled channels method. The corresponding sensitivity parameters $E_{\\rm eff}$ and $E_{\\rm s}$ are computed.","sentences":["The symmetric top molecule YbOCH$_3$ is studied for its potential to $\\mathcal{P}$, $\\mathcal{T}$-violation searches.","The influence of the rotations and vibrations of the YbOCH$_3$ on such violating effects as the electron electric dipole moment (eEDM) and the scalar-pseudoscalar electron-nucleon interaction (Ne-SPS) is studied using the coupled channels method.","The corresponding sensitivity parameters $E_{\\rm eff}$ and $E_{\\rm s}$ are computed."],"url":"http://arxiv.org/abs/2404.07629v1","category":"quant-ph"}
{"created":"2024-04-11 10:26:23","title":"Odd-Parity Nucleon Electromagnetic Transitions in Lattice QCD","abstract":"The parity-expanded variational analysis (PEVA) technique enables the isolation of opposite-parity eigenstates at finite momentum. The approach has been used to perform the first lattice QCD calculations of excited-baryon form factors. In particular, these calculations show that the low-lying odd-parity nucleon excitations are described well by constituent quark models at moderate u and d quark masses approaching the strange quark mass. Herein, we extend the PEVA technique to establish a formalism for the determination of odd-parity nucleon electromagnetic transition form factors in lattice QCD. The formalism is implemented in the first calculation of the helicity amplitudes for transitions from the ground state nucleon to the first two odd-parity excitations. Through a comparison with constituent quark model calculations of these amplitudes, these new results give important insight into the structure of these excitations. This work is a critical step towards confronting experimental electroproduction amplitudes for the $N^*(1535)$ and $N^*(1650)$ resonances with ab-initio lattice QCD calculations.","sentences":["The parity-expanded variational analysis (PEVA) technique enables the isolation of opposite-parity eigenstates at finite momentum.","The approach has been used to perform the first lattice QCD calculations of excited-baryon form factors.","In particular, these calculations show that the low-lying odd-parity nucleon excitations are described well by constituent quark models at moderate u and d quark masses approaching the strange quark mass.","Herein, we extend the PEVA technique to establish a formalism for the determination of odd-parity nucleon electromagnetic transition form factors in lattice QCD.","The formalism is implemented in the first calculation of the helicity amplitudes for transitions from the ground state nucleon to the first two odd-parity excitations.","Through a comparison with constituent quark model calculations of these amplitudes, these new results give important insight into the structure of these excitations.","This work is a critical step towards confronting experimental electroproduction amplitudes for the $N^*(1535)$ and $N^*(1650)$ resonances with ab-initio lattice QCD calculations."],"url":"http://arxiv.org/abs/2404.07625v1","category":"hep-lat"}
{"created":"2024-04-11 10:15:42","title":"Wiener-Hopf solution of the free energy TBA problem and instanton sectors in the O(3) sigma model","abstract":"Perturbation theory in asymptotically free quantum field theories is asymptotic. The factorially growing perturbative coefficients carry information about non-perturbative corrections, which can be related to renormalons and instantons. Using the Wiener-Hopf technique we determine the full analytic solution for the free energy density in the two dimensional $O(N)$ sigma models. For $N>3$ there are no instantons, and we found that the perturbative series carries all the information about the non-perturbative corrections. However, in the $O(3)$ case, we identify several non-perturbative sectors that are not related to the asymptotics of the perturbative series. The number of sectors depends on the observables: for the ground-state energy density we identify three sectors, which we attribute to instantons. For the free energy density in the running perturbative coupling we found infinitely many sectors.","sentences":["Perturbation theory in asymptotically free quantum field theories is asymptotic.","The factorially growing perturbative coefficients carry information about non-perturbative corrections, which can be related to renormalons and instantons.","Using the Wiener-Hopf technique we determine the full analytic solution for the free energy density in the two dimensional $O(N)$ sigma models.","For $N>3$ there are no instantons, and we found that the perturbative series carries all the information about the non-perturbative corrections.","However, in the $O(3)$ case, we identify several non-perturbative sectors that are not related to the asymptotics of the perturbative series.","The number of sectors depends on the observables: for the ground-state energy density we identify three sectors, which we attribute to instantons.","For the free energy density in the running perturbative coupling we found infinitely many sectors."],"url":"http://arxiv.org/abs/2404.07621v1","category":"hep-th"}
{"created":"2024-04-11 10:04:30","title":"Glauber dynamics for the hard-core model on bounded-degree $H$-free graphs","abstract":"The hard-core model has as its configurations the independent sets of some graph instance $G$. The probability distribution on independent sets is controlled by a `fugacity' $\\lambda>0$, with higher $\\lambda$ leading to denser configurations. We investigate the mixing time of Glauber (single-site) dynamics for the hard-core model on restricted classes of bounded-degree graphs in which a particular graph $H$ is excluded as an induced subgraph. If $H$ is a subdivided claw then, for all $\\lambda$, the mixing time is $O(n\\log n)$, where $n$ is the order of $G$. This extends a result of Chen and Gu for claw-free graphs. When $H$ is a path, the set of possible instances is finite. For all other $H$, the mixing time is exponential in $n$ for sufficiently large $\\lambda$, depending on $H$ and the maximum degree of $G$.","sentences":["The hard-core model has as its configurations the independent sets of some graph instance $G$. The probability distribution on independent sets is controlled by a `fugacity' $\\lambda>0$, with higher $\\lambda$ leading to denser configurations.","We investigate the mixing time of Glauber (single-site) dynamics for the hard-core model on restricted classes of bounded-degree graphs in which a particular graph $H$ is excluded as an induced subgraph.","If $H$ is a subdivided claw then, for all $\\lambda$, the mixing time is $O(n\\log n)$, where $n$ is the order of $G$. This extends a result of Chen and Gu for claw-free graphs.","When $H$ is a path, the set of possible instances is finite.","For all other $H$, the mixing time is exponential in $n$ for sufficiently large $\\lambda$, depending on $H$ and the maximum degree of $G$."],"url":"http://arxiv.org/abs/2404.07615v1","category":"math.PR"}
{"created":"2024-04-11 09:40:02","title":"Dilaton Forbidden Dark Matter","abstract":"Dilaton effective field theory (dEFT) describes the long distance behavior of certain confining, near-conformal gauge theories that have been studied via lattice computation. Pseudo-Nambu-Goldstone bosons (pNGBs), emerging from the breaking of approximate, continuous, internal symmetries, are coupled to an additional scalar particle, the dilaton, arising from the spontaneous breaking of approximate scale invariance. This effective theory has been employed to study possible extensions of the standard model. In this paper, we propose a complementary role for dEFT, as a description of the dark matter of the universe, with the pNGBs identified as the dark-matter particles. We show that this theory provides a natural implementation of the ``forbidden'' dark matter mechanism, and we identify regions of parameter space for which the thermal history of dEFT yields the measured dark matter relic density.","sentences":["Dilaton effective field theory (dEFT) describes the long distance behavior of certain confining, near-conformal gauge theories that have been studied via lattice computation.","Pseudo-Nambu-Goldstone bosons (pNGBs), emerging from the breaking of approximate, continuous, internal symmetries, are coupled to an additional scalar particle, the dilaton, arising from the spontaneous breaking of approximate scale invariance.","This effective theory has been employed to study possible extensions of the standard model.","In this paper, we propose a complementary role for dEFT, as a description of the dark matter of the universe, with the pNGBs identified as the dark-matter particles.","We show that this theory provides a natural implementation of the ``forbidden'' dark matter mechanism, and we identify regions of parameter space for which the thermal history of dEFT yields the measured dark matter relic density."],"url":"http://arxiv.org/abs/2404.07601v1","category":"hep-ph"}
{"created":"2024-04-11 09:39:49","title":"Linear structures of norm-attaining Lipschitz functions and their complements","abstract":"We solve two main questions on linear structures of (non-)norm-attaining Lipschitz functions. First, we show that for every infinite metric space $M$, the set consisting of Lipschitz functions on $M$ which do not strongly attain their norm and the zero contains an isometric copy of $\\ell_\\infty$, and moreover, those functions can be chosen not to attain their norm as functionals on the Lipschitz-free space over $M$. Second, we prove that for every infinite metric space $M$, neither the set of strongly norm-attaining Lipschitz functions on $M$ nor the union of its complement with zero is ever a linear space. Furthermore, we observe that the set consisting of Lipschitz functions which cannot be approximated by strongly norm-attaining ones and the zero element contains $\\ell_\\infty$ isometrically in all the known cases. Some natural observations and spaceability results are also investigated for Lipschitz functions that attain their norm in one way but do not in another, for several norm-attainment notions considered in the literature.","sentences":["We solve two main questions on linear structures of (non-)norm-attaining Lipschitz functions.","First, we show that for every infinite metric space $M$, the set consisting of Lipschitz functions on $M$ which do not strongly attain their norm and the zero contains an isometric copy of $\\ell_\\infty$, and moreover, those functions can be chosen not to attain their norm as functionals on the Lipschitz-free space over $M$. Second, we prove that for every infinite metric space $M$, neither the set of strongly norm-attaining Lipschitz functions on $M$ nor the union of its complement with zero is ever a linear space.","Furthermore, we observe that the set consisting of Lipschitz functions which cannot be approximated by strongly norm-attaining ones and the zero element contains $\\ell_\\infty$ isometrically in all the known cases.","Some natural observations and spaceability results are also investigated for Lipschitz functions that attain their norm in one way but do not in another, for several norm-attainment notions considered in the literature."],"url":"http://arxiv.org/abs/2404.07599v1","category":"math.FA"}
{"created":"2024-04-11 09:24:52","title":"$\\mathbb{Z}_3$ lattice gauge theory as a toy model for dense QCD","abstract":"We propose the $(3+1)$-dimensional $\\mathbb{Z}_3$ lattice gauge theory coupled with the 2-flavor Wilson-Dirac fermion as a toy model for studying quantum chromodynamics (QCD) at nonzero density. We study its phase diagram in the space of the lattice gauge couplings $g^2$ and the quark chemical potentials $\\mu$ and discuss the similarity and difference compared with anticipated behaviors of actual QCD. This model also provides a testing ground for various algorithms of the numerical Hamiltonian formalism as its Hilbert space is finite-dimensional in a finite box.","sentences":["We propose the $(3+1)$-dimensional $\\mathbb{Z}_3$ lattice gauge theory coupled with the 2-flavor Wilson-Dirac fermion as a toy model for studying quantum chromodynamics (QCD) at nonzero density.","We study its phase diagram in the space of the lattice gauge couplings $g^2$ and the quark chemical potentials $\\mu$ and discuss the similarity and difference compared with anticipated behaviors of actual QCD.","This model also provides a testing ground for various algorithms of the numerical Hamiltonian formalism as its Hilbert space is finite-dimensional in a finite box."],"url":"http://arxiv.org/abs/2404.07595v1","category":"hep-lat"}
{"created":"2024-04-11 09:18:18","title":"Stein's method and a cubic mean-field model","abstract":"In this paper, we study a mean-field spin model with three- and two-body interactions. In a recent paper by Contucci, Mingione and Osabutey, the equilibrium measure for large volumes was shown to have three pure states, two with opposite magnetization and an unpolarized one with zero magnetization, merging at the critical point. The authors proved a central limit theorem for the suitably rescaled magnetization. The aim of our paper is presenting a prove of a central limit theorem for the rescaled magnetization applying the exchangeable pair approach due to Stein. Moreover we prove (non-uniform) Berry-Esseen bounds, a concentration inequality, Cram\\'er-type moderate deviations and a moderate deviations principle for the suitably rescaled magnetization. Interestingly we analyze Berry-Esseen bounds in case the model-parameters $(K_n,J_n)$ converge to the critical point $(0,1)$ on lines with different slopes and with a certain speed, and obtain new limiting distributions and thresholds for the speed of convergence.","sentences":["In this paper, we study a mean-field spin model with three- and two-body interactions.","In a recent paper by Contucci, Mingione and Osabutey, the equilibrium measure for large volumes was shown to have three pure states, two with opposite magnetization and an unpolarized one with zero magnetization, merging at the critical point.","The authors proved a central limit theorem for the suitably rescaled magnetization.","The aim of our paper is presenting a prove of a central limit theorem for the rescaled magnetization applying the exchangeable pair approach due to Stein.","Moreover we prove (non-uniform) Berry-Esseen bounds, a concentration inequality, Cram\\'er-type moderate deviations and a moderate deviations principle for the suitably rescaled magnetization.","Interestingly we analyze Berry-Esseen bounds in case the model-parameters $(K_n,J_n)$ converge to the critical point $(0,1)$ on lines with different slopes and with a certain speed, and obtain new limiting distributions and thresholds for the speed of convergence."],"url":"http://arxiv.org/abs/2404.07587v1","category":"math.PR"}
{"created":"2024-04-11 08:48:09","title":"Effects of phase separation on extinction times in population models","abstract":"We study the effect of phase separating diffusive dynamics on the mean time to extinction in several reaction-diffusion models with slow reactions. We consider a continuum theory similar to model AB, and a simple model where individual particles on two sites undergo on-site reactions and hopping between the sites. In the slow-reaction limit, we project the models' dynamics onto suitable one-dimensional reaction coordinates, which allows derivation of quasi-equilibrium effective free energies. For weak noise, this enables characterisation of the mean time to extinction. This time can be enhanced or suppressed by the addition of phase separation, compared with homogeneous reference cases. We also discuss how Allee effects can be affected by phase separation.","sentences":["We study the effect of phase separating diffusive dynamics on the mean time to extinction in several reaction-diffusion models with slow reactions.","We consider a continuum theory similar to model AB, and a simple model where individual particles on two sites undergo on-site reactions and hopping between the sites.","In the slow-reaction limit, we project the models' dynamics onto suitable one-dimensional reaction coordinates, which allows derivation of quasi-equilibrium effective free energies.","For weak noise, this enables characterisation of the mean time to extinction.","This time can be enhanced or suppressed by the addition of phase separation, compared with homogeneous reference cases.","We also discuss how Allee effects can be affected by phase separation."],"url":"http://arxiv.org/abs/2404.07563v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-11 08:36:36","title":"Attention-Aware Laparoscopic Image Desmoking Network with Lightness Embedding and Hybrid Guided Embedding","abstract":"This paper presents a novel method of smoke removal from the laparoscopic images. Due to the heterogeneous nature of surgical smoke, a two-stage network is proposed to estimate the smoke distribution and reconstruct a clear, smoke-free surgical scene. The utilization of the lightness channel plays a pivotal role in providing vital information pertaining to smoke density. The reconstruction of smoke-free image is guided by a hybrid embedding, which combines the estimated smoke mask with the initial image. Experimental results demonstrate that the proposed method boasts a Peak Signal to Noise Ratio that is $2.79\\%$ higher than the state-of-the-art methods, while also exhibits a remarkable $38.2\\%$ reduction in run-time. Overall, the proposed method offers comparable or even superior performance in terms of both smoke removal quality and computational efficiency when compared to existing state-of-the-art methods. This work will be publicly available on http://homepage.hit.edu.cn/wpgao","sentences":["This paper presents a novel method of smoke removal from the laparoscopic images.","Due to the heterogeneous nature of surgical smoke, a two-stage network is proposed to estimate the smoke distribution and reconstruct a clear, smoke-free surgical scene.","The utilization of the lightness channel plays a pivotal role in providing vital information pertaining to smoke density.","The reconstruction of smoke-free image is guided by a hybrid embedding, which combines the estimated smoke mask with the initial image.","Experimental results demonstrate that the proposed method boasts a Peak Signal to Noise Ratio that is $2.79\\%$ higher than the state-of-the-art methods, while also exhibits a remarkable $38.2\\%$ reduction in run-time.","Overall, the proposed method offers comparable or even superior performance in terms of both smoke removal quality and computational efficiency when compared to existing state-of-the-art methods.","This work will be publicly available on http://homepage.hit.edu.cn/wpgao"],"url":"http://arxiv.org/abs/2404.07556v1","category":"eess.IV"}
{"created":"2024-04-11 08:36:21","title":"X-ray polarimetric features of Gamma-ray Bursts across varied redshifts and hints for Axion-Like-Particles","abstract":"Polarimetric features during the prompt phase of Gamma-ray Bursts (GRBs) have been essential for elucidating the debated emission mechanisms and gaining insight into the inner structure of GRBs. However, the potential impact of photon-Axion-Like-Particle (ALP) mixing in extragalactic magnetic fields, leading to significant modifications to the initial polarization state, has been overlooked in discussions concerning prompt phase constraints. In this work, we first examine the statistical characteristics of linear polarization degree ($\\Pi_{L}$) in GRBs, by utilizing data from polarimetric missions focusing on sub-MeV emissions. Our analysis, conducted with a restricted sample of GRBs spanning various redshifts, reveals a diverse distribution of $\\Pi_{L}$, which currently shows no correlation with the GRBs' spectral parameters or properties of candidate host galaxies. We then explore alternations to the initial $\\Pi_{L}$ due to photon-ALP mixing within a domain-like structure of the intergalactic magnetic field (${\\bf B}_{\\rm IGM} $). With the existence of ALPs with $m_{a}$$~$$\\lesssim$$~$$10^{-14}$$~$eV and $g_{a\\gamma}~$$\\simeq$$~0.5\\times10^{-11}$, the mixing leads to a decrease in the polarization degree of initially fully linearly polarized photons, while it induces a certain degree of polarization to initially unpolarized photons. To ensure that the effect of mixing is small enough to be negligible, the mixing term $\\Delta_{a\\gamma} \\equiv 1/2\\ g_{a\\gamma} {\\bf B}_{\\rm IGM}$ should be less than $1.5\\times 10^{-4}$ Mpc$^{-1}$. Currently, the number of GRBs with both sub-MeV polarization measurement and redshift confirmation remains very limited. Certification of redshift for GRBs with low $\\Pi_{L}$ would further constrain the parameter space of ALPs or provide an independent means to determine the upper limit on ${\\bf B}_{\\rm IGM}$.","sentences":["Polarimetric features during the prompt phase of Gamma-ray Bursts (GRBs) have been essential for elucidating the debated emission mechanisms and gaining insight into the inner structure of GRBs.","However, the potential impact of photon-Axion-Like-Particle (ALP) mixing in extragalactic magnetic fields, leading to significant modifications to the initial polarization state, has been overlooked in discussions concerning prompt phase constraints.","In this work, we first examine the statistical characteristics of linear polarization degree ($\\Pi_{L}$) in GRBs, by utilizing data from polarimetric missions focusing on sub-MeV emissions.","Our analysis, conducted with a restricted sample of GRBs spanning various redshifts, reveals a diverse distribution of $\\Pi_{L}$, which currently shows no correlation with the GRBs' spectral parameters or properties of candidate host galaxies.","We then explore alternations to the initial $\\Pi_{L}$ due to photon-ALP mixing within a domain-like structure of the intergalactic magnetic field (${\\bf B}_{\\rm IGM} $).","With the existence of ALPs with $m_{a}$$~$$\\lesssim$$~$$10^{-14}$$~$eV and $g_{a\\gamma}~$$\\simeq$$~0.5\\times10^{-11}$, the mixing leads to a decrease in the polarization degree of initially fully linearly polarized photons, while it induces a certain degree of polarization to initially unpolarized photons.","To ensure that the effect of mixing is small enough to be negligible, the mixing term $\\Delta_{a\\gamma} \\equiv 1/2\\ g_{a\\gamma} {\\bf B}_{\\rm IGM}$ should be less than $1.5\\times 10^{-4}$ Mpc$^{-1}$. Currently, the number of GRBs with both sub-MeV polarization measurement and redshift confirmation remains very limited.","Certification of redshift for GRBs with low $\\Pi_{L}$ would further constrain the parameter space of ALPs or provide an independent means to determine the upper limit on ${\\bf B}_{\\rm IGM}$."],"url":"http://arxiv.org/abs/2404.07555v1","category":"astro-ph.HE"}
{"created":"2024-04-11 08:11:25","title":"Exclusive four pion photoproduction in ultraperipheral Pb-Pb collisions at $\\sqrt{s_{\\rm NN}} = 5.02$ TeV","abstract":"The intense photon fluxes from relativistic nuclei provide an opportunity to study photonuclear interactions in ultraperipheral collisions. The measurement of coherently photoproduced $\\pi^+\\pi^-\\pi^+\\pi^-$ final states in ultraperipheral Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}=5.02$ TeV is presented for the first time. The cross section, d$\\sigma$/d$y$, times the branching ratio ($\\rho\\rightarrow \\pi^+ \\pi^+ \\pi^- \\pi^-$) is found to be $47.8\\pm2.3~\\rm{(stat.)}\\pm7.7~\\rm{(syst.)}$ mb in the rapidity interval $|y| < 0.5$. The invariant mass distribution is not well described with a single Breit-Wigner resonance. The production of two interfering resonances, $\\rho(1450)$ and $\\rho(1700)$, provides a good description of the data. The values of the masses ($m$) and widths ($\\Gamma$) of the resonances extracted from the fit are $m_{1}=1385\\pm14~\\rm{(stat.)}\\pm3~\\rm{(syst.)}$ MeV/$c^2$, $\\Gamma_{1}=431\\pm36~\\rm{(stat.)}\\pm82~\\rm{(syst.)}$ MeV/$c^2$, $m_{2}=1663\\pm13~\\rm{(stat.)}\\pm22~\\rm{(syst.)}$ MeV/$c^2$ and $\\Gamma_{2}=357 \\pm31~\\rm{(stat.)}\\pm49~\\rm{(syst.)}$ MeV/$c^2$, respectively. The measured cross sections times the branching ratios are compared to recent theoretical predictions.","sentences":["The intense photon fluxes from relativistic nuclei provide an opportunity to study photonuclear interactions in ultraperipheral collisions.","The measurement of coherently photoproduced $\\pi^+\\pi^-\\pi^+\\pi^-$ final states in ultraperipheral Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}=5.02$ TeV is presented for the first time.","The cross section, d$\\sigma$/d$y$, times the branching ratio ($\\rho\\rightarrow \\pi^+ \\pi^+ \\pi^- \\pi^-$) is found to be $47.8\\pm2.3~\\rm{(stat.)}\\pm7.7~\\rm{(syst.)}$ mb in the rapidity interval $|y| <","0.5$. The invariant mass distribution is not well described with a single Breit-Wigner resonance.","The production of two interfering resonances, $\\rho(1450)$ and $\\rho(1700)$, provides a good description of the data.","The values of the masses ($m$) and widths ($\\Gamma$) of the resonances extracted from the fit are $m_{1}=1385\\pm14~\\rm{(stat.)}\\pm3~\\rm{(syst.)}$ MeV/$c^2$, $\\Gamma_{1}=431\\pm36~\\rm{(stat.)}\\pm82~\\rm{(syst.)}$ MeV/$c^2$, $m_{2}=1663\\pm13~\\rm{(stat.)}\\pm22~\\rm{(syst.)}$ MeV/$c^2$ and $\\Gamma_{2}=357 \\pm31~\\rm{(stat.)}\\pm49~\\rm{(syst.)}$ MeV/$c^2$, respectively.","The measured cross sections times the branching ratios are compared to recent theoretical predictions."],"url":"http://arxiv.org/abs/2404.07542v1","category":"nucl-ex"}
{"created":"2024-04-11 08:03:39","title":"Reduced-dimensional modelling for nonlinear convection-dominated flow in cylindric domains","abstract":"The aim of the paper is to construct and justify asymptotic approximations for solutions to quasilinear convection-diffusion problems with a predominance of nonlinear convective flow in a thin cylinder, where an inhomogeneous nonlinear Robin-type boundary condition involving convective and diffusive fluxes is imposed on the lateral surface. The limit problem for vanishing diffusion and the cylinder shrinking to an interval is a nonlinear first-order conservation law. For a time span that allows for a classical solution of this limit problem corresponding uniform pointwise and energy estimates are proven. They provide precise model error estimates with respect to the small parameter that controls the double viscosity-geometric limit. In addition, other problems with more higher P\\'eclet numbers are also considered.","sentences":["The aim of the paper is to construct and justify asymptotic approximations for solutions to quasilinear convection-diffusion problems with a predominance of nonlinear convective flow in a thin cylinder, where an inhomogeneous nonlinear Robin-type boundary condition involving convective and diffusive fluxes is imposed on the lateral surface.","The limit problem for vanishing diffusion and the cylinder shrinking to an interval is a nonlinear first-order conservation law.","For a time span that allows for a classical solution of this limit problem corresponding uniform pointwise and energy estimates are proven.","They provide precise model error estimates with respect to the small parameter that controls the double viscosity-geometric limit.","In addition, other problems with more higher P\\'eclet numbers are also considered."],"url":"http://arxiv.org/abs/2404.07538v1","category":"math.AP"}
{"created":"2024-04-11 07:57:52","title":"Free boundary regularity for the inhomogeneous one-phase Stefan problem","abstract":"In this paper, we prove that flat solutions to inhomogeneous one-phase Stefan problem are $C^{1,\\alpha}$ in the $x_n$ direction.","sentences":["In this paper, we prove that flat solutions to inhomogeneous one-phase Stefan problem are $C^{1,\\alpha}$ in the $x_n$ direction."],"url":"http://arxiv.org/abs/2404.07535v1","category":"math.AP"}
{"created":"2024-04-11 07:09:56","title":"Three-loop renormalization of the quantum action for a five-dimensional scalar cubic model with the usage of the background field method and a cutoff regularization","abstract":"The paper studies the quantum action for the five-dimensional real $\\phi^3$-theory in the case of a general formulation using the background field method. The three-loop renormalization is performed with the usage of a cutoff regularization in the coordinate representation. The explicit form of the first three coefficients for the renormalization constants is presented. The absence of non-local singular contributions and partial results for the fourth correction are discussed.","sentences":["The paper studies the quantum action for the five-dimensional real $\\phi^3$-theory in the case of a general formulation using the background field method.","The three-loop renormalization is performed with the usage of a cutoff regularization in the coordinate representation.","The explicit form of the first three coefficients for the renormalization constants is presented.","The absence of non-local singular contributions and partial results for the fourth correction are discussed."],"url":"http://arxiv.org/abs/2404.07513v1","category":"hep-th"}
{"created":"2024-04-11 06:58:57","title":"Multiparameter cascaded quantum interferometer","abstract":"We theoretically propose a multiparameter cascaded quantum interferometer in which a two-input and two-output setup is obtained by concatenating 50:50 beam splitters with n independent and adjustable time delays. A general method for deriving the coincidence probability of such an interferometer is given based on the linear transformation of the matrix of beam splitters. As examples, we analyze the interference characteristics of one-, two- and three-parameter cascaded quantum interferometers with different frequency correlations and input states. Some typical interferograms of such interferometers are provided to reveal more rich and complicated two-photon interference phenomena. In principle, arbitrary two-input and two-output experimental setups can be designed with the proposal. This work offers a toolbox for designing versatile quantum interferometers and provides a convenient method for deriving the coincidence probabilities involved. Potential applications can be found in the complete spectral characterization of two-photon states, multiparameter estimation, and quantum metrology.","sentences":["We theoretically propose a multiparameter cascaded quantum interferometer in which a two-input and two-output setup is obtained by concatenating 50:50 beam splitters with n independent and adjustable time delays.","A general method for deriving the coincidence probability of such an interferometer is given based on the linear transformation of the matrix of beam splitters.","As examples, we analyze the interference characteristics of one-, two- and three-parameter cascaded quantum interferometers with different frequency correlations and input states.","Some typical interferograms of such interferometers are provided to reveal more rich and complicated two-photon interference phenomena.","In principle, arbitrary two-input and two-output experimental setups can be designed with the proposal.","This work offers a toolbox for designing versatile quantum interferometers and provides a convenient method for deriving the coincidence probabilities involved.","Potential applications can be found in the complete spectral characterization of two-photon states, multiparameter estimation, and quantum metrology."],"url":"http://arxiv.org/abs/2404.07509v1","category":"quant-ph"}
{"created":"2024-04-11 05:47:20","title":"Extremal triangle-free graphs with chromatic number at least four","abstract":"Let $G$ be an $n$-vertex triangle-free graph. The celebrated Mantel's theorem showed that $e(G)\\leq \\lfloor\\frac{n^2}{4}\\rfloor$. In 1962, Erd\\H{o}s (together with Gallai), and independently Andr\\'{a}sfai, proved that if $G$ is non-bipartite then $e(G)\\leq \\lfloor\\frac{(n-1)^2}{4}\\rfloor+1$. In this paper, we extend this result and show that if $G$ has chromatic number at least four and $n\\geq 150$, then $e(G)\\leq \\lfloor\\frac{(n-3)^2}{4}\\rfloor+5$. The blow-up of Gr\\\"{o}tzsch graph shows that this bound is best possible.","sentences":["Let $G$ be an $n$-vertex triangle-free graph.","The celebrated Mantel's theorem showed that $e(G)\\leq \\lfloor\\frac{n^2}{4}\\rfloor$. In 1962, Erd\\H{o}s (together with Gallai), and independently Andr\\'{a}sfai, proved that if $G$ is non-bipartite then $e(G)\\leq \\lfloor\\frac{(n-1)^2}{4}\\rfloor+1$.","In this paper, we extend this result and show that if $G$ has chromatic number at least four and $n\\geq 150$, then $e(G)\\leq \\lfloor\\frac{(n-3)^2}{4}\\rfloor+5$.","The blow-up of Gr\\\"{o}tzsch graph shows that this bound is best possible."],"url":"http://arxiv.org/abs/2404.07486v1","category":"math.CO"}
{"created":"2024-04-11 03:37:02","title":"I-mode Plasma Confinement Improvement by Real-time Lithium Injection and its Classification on EAST Tokamak","abstract":"I-mode is a promising regime for future fusion reactors due to the high energy confinement and the moderate particle confinement. However, the effect of lithium, which has been widely applied for particle recycling and impurity control, on I-mode plasma is still unclear. Recently, experiments of real-time lithium powder injection on I-mode plasma have been carried out in EAST Tokamak. It was found that the confinement performance of the I-mode can be improved by the lithium powder injection, which can strongly reduce electron turbulence (ET) and then trigger ion turbulence (IT). Four different regimes of I-mode have been identified in EAST. The Type I I-mode plasma is characterized by the weakly coherent mode (WCM) and the geodesic-acoustic mode (GAM). The Type II I-mode is featured as the WCM and the edge temperature ring oscillation (ETRO). The Type III I-mode corresponds to the plasma with the co-existence of ETRO, GAM, and WCM. The Type IV I-mode denotes the plasma with only WCM but without ETRO and GAM. It has been observed that WCM and ETRO are increased with lithium powder injection due to the reduction of ion and electron turbulence, and the enhancement of the pedestal electron temperature gradient. EAST experiments demonstrate that lithium powder injection is an effective tool for real-time control and confinement improvement of I-mode plasma.","sentences":["I-mode is a promising regime for future fusion reactors due to the high energy confinement and the moderate particle confinement.","However, the effect of lithium, which has been widely applied for particle recycling and impurity control, on I-mode plasma is still unclear.","Recently, experiments of real-time lithium powder injection on I-mode plasma have been carried out in EAST Tokamak.","It was found that the confinement performance of the I-mode can be improved by the lithium powder injection, which can strongly reduce electron turbulence (ET) and then trigger ion turbulence (IT).","Four different regimes of I-mode have been identified in EAST.","The Type I I-mode plasma is characterized by the weakly coherent mode (WCM) and the geodesic-acoustic mode (GAM).","The Type II I-mode is featured as the WCM and the edge temperature ring oscillation (ETRO).","The Type III I-mode corresponds to the plasma with the co-existence of ETRO, GAM, and WCM.","The Type IV I-mode denotes the plasma with only WCM but without ETRO and GAM.","It has been observed that WCM and ETRO are increased with lithium powder injection due to the reduction of ion and electron turbulence, and the enhancement of the pedestal electron temperature gradient.","EAST experiments demonstrate that lithium powder injection is an effective tool for real-time control and confinement improvement of I-mode plasma."],"url":"http://arxiv.org/abs/2404.07458v1","category":"physics.plasm-ph"}
{"created":"2024-04-11 03:29:59","title":"Supernova 1987A's keyhole: A long-lived jet-pair in the final explosion phase of core-collapse supernovae","abstract":"I further study the manner by which a pair of opposite jets shape the `keyhole' morphological structure of the core-collapse supernova (CCSN) SN 1997A, now the CCSN remnant (CCSNR) 1987A. By doing so, I strengthen the claim that the jittering-jet explosion mechanism (JJEM) accounts for most, likely all, CCSNe. The `keyhole' structure comprises a northern low-intensity zone closed with a bright rim on its front and an elongated low-intensity nozzle in the south. This rim-nozzle asymmetry is observed in some cooling flow clusters and planetary nebulae that are observed to be shaped by jets. I build a toy model that uses the planar jittering jets pattern, where consecutive pairs of jets tend to jitter in a common plane, implying that the accreted gas onto the newly born neutron star at the late explosion phase flows perpendicular to that plane. This allows for a long-lived jet-launching episode. This long-lasting jet-launching episode launches more mass into the jets that can inflate larger pairs of ears or bubbles, forming the main jets' axis of the CCSNR that is not necessarily related to a possible pre-collapse core rotation. I discuss the relation of the main jets' axis to the neutron star's natal kick velocity.","sentences":["I further study the manner by which a pair of opposite jets shape the `keyhole' morphological structure of the core-collapse supernova (CCSN) SN 1997A, now the CCSN remnant (CCSNR) 1987A. By doing so, I strengthen the claim that the jittering-jet explosion mechanism (JJEM) accounts for most, likely all, CCSNe.","The `keyhole' structure comprises a northern low-intensity zone closed with a bright rim on its front and an elongated low-intensity nozzle in the south.","This rim-nozzle asymmetry is observed in some cooling flow clusters and planetary nebulae that are observed to be shaped by jets.","I build a toy model that uses the planar jittering jets pattern, where consecutive pairs of jets tend to jitter in a common plane, implying that the accreted gas onto the newly born neutron star at the late explosion phase flows perpendicular to that plane.","This allows for a long-lived jet-launching episode.","This long-lasting jet-launching episode launches more mass into the jets that can inflate larger pairs of ears or bubbles, forming the main jets' axis of the CCSNR that is not necessarily related to a possible pre-collapse core rotation.","I discuss the relation of the main jets' axis to the neutron star's natal kick velocity."],"url":"http://arxiv.org/abs/2404.07455v1","category":"astro-ph.HE"}
{"created":"2024-04-11 02:43:14","title":"BCM-thresholds of hypersurfaces","abstract":"In this paper, we use big Cohen-Macaulay algebras to define a characteristic free analog of the $F$-thresholds, which we call BCM-thresholds, in the case of principal ideals. We prove that, similarly to the case of the $F$-thresholds, the set of BCM-thresholds and the set of BCM-jumping numbers agree. We also relate some BCM-thresholds to splittings of maps from the ring to a big Cohen-Macaulay algebra.","sentences":["In this paper, we use big Cohen-Macaulay algebras to define a characteristic free analog of the $F$-thresholds, which we call BCM-thresholds, in the case of principal ideals.","We prove that, similarly to the case of the $F$-thresholds, the set of BCM-thresholds and the set of BCM-jumping numbers agree.","We also relate some BCM-thresholds to splittings of maps from the ring to a big Cohen-Macaulay algebra."],"url":"http://arxiv.org/abs/2404.07438v1","category":"math.AC"}
{"created":"2024-04-11 02:20:11","title":"A Chandra Search for Periodic X-ray Sources in the Bulge of M31","abstract":"We present a systematic search for periodic X-ray sources in the bulge of M31, using ~ 2 Ms of archival Chandra observations spanning a temporal baseline of 16 years. Utilizing the Gregory-Loredo algorithm that is designed for photon-counting, phase-folded light curves, we detect seven periodic X-ray sources, among which five are newly discovered. Three of these sources are novae, the identified periods of which range between 1.3-2.0 hour and is most likely the orbital period. The other four sources are low-mass X-ray binaries, the identified periods of which range between 0.13-19.3 hour and are also likely orbital due to a clear eclipsing/dipping behavior in the light curve. We address implications on the X-ray binary population of the M31 bulge. Our study demonstrates the potential of using archival X-ray observations to systematically identify periodic X-ray sources in external galaxies, which would provide valuable information about the underlying exotic stellar populations.","sentences":["We present a systematic search for periodic X-ray sources in the bulge of M31, using ~ 2 Ms of archival Chandra observations spanning a temporal baseline of 16 years.","Utilizing the Gregory-Loredo algorithm that is designed for photon-counting, phase-folded light curves, we detect seven periodic X-ray sources, among which five are newly discovered.","Three of these sources are novae, the identified periods of which range between 1.3-2.0 hour and is most likely the orbital period.","The other four sources are low-mass X-ray binaries, the identified periods of which range between 0.13-19.3 hour and are also likely orbital due to a clear eclipsing/dipping behavior in the light curve.","We address implications on the X-ray binary population of the M31 bulge.","Our study demonstrates the potential of using archival X-ray observations to systematically identify periodic X-ray sources in external galaxies, which would provide valuable information about the underlying exotic stellar populations."],"url":"http://arxiv.org/abs/2404.07432v1","category":"astro-ph.HE"}
{"created":"2024-04-11 01:25:50","title":"Bi-chromatic paraxial beam as a model of spatio-temporal light fields","abstract":"Optical fields with rich and well-developed spatio-temporal structure, including ultra-short structured light pulses, are essentially non-monochromatic and contain a continuous spectrum of monochromatic constituents. However, some substantial features of such fields and physical mechanisms determining their behavior can be understood based on simplified models including only two monochromatic paraxial components. We consider examples of such model beams, their specific spatial and temporal properties as well as their descriptive abilities for the meaningful characterization of realistic spatio-temporal light fields. In particular, the proposed model enables an explicit consistent analysis of the photon-probability distributions in non-monochromatic fields, which confirms a high degree of coincidence between the \"energy center\" and \"probability center\" of the field. Simultaneously, particular features of the two-component bi-chromatic paraxial fields (periodic and rotational character of the longitudinal and temporal evolution, specific deformations of the propagating-beam transverse intensity profile, etc.) are inspected using numerical examples.","sentences":["Optical fields with rich and well-developed spatio-temporal structure, including ultra-short structured light pulses, are essentially non-monochromatic and contain a continuous spectrum of monochromatic constituents.","However, some substantial features of such fields and physical mechanisms determining their behavior can be understood based on simplified models including only two monochromatic paraxial components.","We consider examples of such model beams, their specific spatial and temporal properties as well as their descriptive abilities for the meaningful characterization of realistic spatio-temporal light fields.","In particular, the proposed model enables an explicit consistent analysis of the photon-probability distributions in non-monochromatic fields, which confirms a high degree of coincidence between the \"energy center\" and \"probability center\" of the field.","Simultaneously, particular features of the two-component bi-chromatic paraxial fields (periodic and rotational character of the longitudinal and temporal evolution, specific deformations of the propagating-beam transverse intensity profile, etc.) are inspected using numerical examples."],"url":"http://arxiv.org/abs/2404.07422v1","category":"physics.optics"}
{"created":"2024-04-11 01:00:52","title":"A Pathway to Efficient Simulations of Charge Density Waves in Transition Metal Dichalcogenides: A Case Study for TiSe2","abstract":"Charge density waves (CDWs) in transition metal dichalcogenides are the subject of growing scientific interest due to their rich interplay with exotic phases of matter and their potential technological applications. Here, using density functional theory with advanced meta-generalized gradient approximations (meta-GGAs) and linear response time-dependent density functional theory (TDDFT) with state-of-the-art exchange-correlation kernels, we investigate the electronic, vibrational, and optical properties in 1T-TiSe2 with and without CDW. In both bulk and monolayer TiSe2, the electronic bands and phonon dispersions in either normal (semi-metallic) or CDW (semiconducting) phase are described well via meta-GGAs, which separate the valence and conduction bands just as HSE06 does but with significantly more computational feasibility. Instead of the underestimated gap with standard exchange-correlation approximations and the overestimated gap with screened hybrid functional HSE06, the band gap of the monolayer TiSe2 CDW phase calculated by the meta-GGA MVS (151 meV) is consistent with the angle-resolved photoemission spectroscopy (ARPES) gap of 153 meV measured at 10 K. In addition, the gap of bulk TiSe2 CDW phase reaches 67 meV within the TASK approximation, close to the ARPES gap of 82 meV. Regarding excitations of many-body nature, for bulk TiSe2 in normal and CDW phases, the experimentally observed humps of electron energy loss spectroscopy and plasmon peak are successfully reproduced in TDDFT, without an obvious kernel dependence. To unleash the full scientific and technological potential of CDWs in transition metal dichalcogenides, the chemical doping, heterostructure engineering, and pump-probe techniques are needed. Our study opens the door to simulating these complexities in CDW compounds from first principles by revealing meta-GGAs as an accurate low-cost alternative to HSE06.","sentences":["Charge density waves (CDWs) in transition metal dichalcogenides are the subject of growing scientific interest due to their rich interplay with exotic phases of matter and their potential technological applications.","Here, using density functional theory with advanced meta-generalized gradient approximations (meta-GGAs) and linear response time-dependent density functional theory (TDDFT) with state-of-the-art exchange-correlation kernels, we investigate the electronic, vibrational, and optical properties in 1T-TiSe2 with and without CDW.","In both bulk and monolayer TiSe2, the electronic bands and phonon dispersions in either normal (semi-metallic) or CDW (semiconducting) phase are described well via meta-GGAs, which separate the valence and conduction bands just as HSE06 does but with significantly more computational feasibility.","Instead of the underestimated gap with standard exchange-correlation approximations and the overestimated gap with screened hybrid functional HSE06, the band gap of the monolayer TiSe2 CDW phase calculated by the meta-GGA MVS (151 meV) is consistent with the angle-resolved photoemission spectroscopy (ARPES) gap of 153 meV measured at 10 K.","In addition, the gap of bulk TiSe2 CDW phase reaches 67 meV within the TASK approximation, close to the ARPES gap of 82 meV.","Regarding excitations of many-body nature, for bulk TiSe2 in normal and CDW phases, the experimentally observed humps of electron energy loss spectroscopy and plasmon peak are successfully reproduced in TDDFT, without an obvious kernel dependence.","To unleash the full scientific and technological potential of CDWs in transition metal dichalcogenides, the chemical doping, heterostructure engineering, and pump-probe techniques are needed.","Our study opens the door to simulating these complexities in CDW compounds from first principles by revealing meta-GGAs as an accurate low-cost alternative to HSE06."],"url":"http://arxiv.org/abs/2404.07414v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-11 00:50:03","title":"Joint mixed-effects models for causal inference in clustered network-based observational studies","abstract":"Causal inference on populations embedded in social networks poses technical challenges, since the typical no interference assumption frequently does not hold. Existing methods developed in the context of network interference rely upon the assumption of no unmeasured confounding. However, when faced with multilevel network data, there may be a latent factor influencing both the exposure and the outcome at the cluster level. We propose a Bayesian inference approach that combines a joint mixed-effects model for the outcome and the exposure with direct standardization to identify and estimate causal effects in the presence of network interference and unmeasured cluster confounding. In simulations, we compare our proposed method with linear mixed and fixed effects models and show that unbiased estimation is achieved using the joint model. Having derived valid tools for estimation, we examine the effect of maternal college education on adolescent school performance using data from the National Longitudinal Study of Adolescent Health.","sentences":["Causal inference on populations embedded in social networks poses technical challenges, since the typical no interference assumption frequently does not hold.","Existing methods developed in the context of network interference rely upon the assumption of no unmeasured confounding.","However, when faced with multilevel network data, there may be a latent factor influencing both the exposure and the outcome at the cluster level.","We propose a Bayesian inference approach that combines a joint mixed-effects model for the outcome and the exposure with direct standardization to identify and estimate causal effects in the presence of network interference and unmeasured cluster confounding.","In simulations, we compare our proposed method with linear mixed and fixed effects models and show that unbiased estimation is achieved using the joint model.","Having derived valid tools for estimation, we examine the effect of maternal college education on adolescent school performance using data from the National Longitudinal Study of Adolescent Health."],"url":"http://arxiv.org/abs/2404.07411v1","category":"stat.ME"}
{"created":"2024-04-11 00:45:32","title":"Stellar black holes can \"stretch'' supermassive black hole accretion disks","abstract":"Stellar black holes (sBHs) are widely believed to exist in the accretion disks of active galactic nuclei (AGNs). Previous studies often focus on the transient emission produced by embedded sBHs. Here, we explore the possible observational consequences of an AGN accretion disk that contains a population of accreting sBHs. Embedded accreting sBHs change the effective temperature distribution of the AGN accretion disk by heating gas in the outer regions. Two possible observational consequences are presented. First, the spectral energy distribution has a turnover feature at $\\sim 4700\\ \\textrm{\\AA}$ when the supermassive black hole (SMBH) mass is $\\sim 10^8\\ M_{\\odot}$, which can help explain the observed shallow spectral shape at wavelengths $>5000\\ \\textrm{\\AA}$ for the Sloan Digital Sky Survey quasar composite spectrum. Second, the half-light radius of a given relatively long wavelength is significantly larger than for an AGN disk without sBHs, which can be tested by microlensing observations. With appropriate sBH distributions, the model can be reconciled with quasar microlensing disk sizes. We propose that the half-light radius-wavelength relation can be utilized to investigate the distributions of embedded sBHs in AGN accretion disks.","sentences":["Stellar black holes (sBHs) are widely believed to exist in the accretion disks of active galactic nuclei (AGNs).","Previous studies often focus on the transient emission produced by embedded sBHs.","Here, we explore the possible observational consequences of an AGN accretion disk that contains a population of accreting sBHs.","Embedded accreting sBHs change the effective temperature distribution of the AGN accretion disk by heating gas in the outer regions.","Two possible observational consequences are presented.","First, the spectral energy distribution has a turnover feature at $\\sim 4700\\ \\textrm{\\AA}$ when the supermassive black hole (SMBH) mass is $\\sim 10^8\\ M_{\\odot}$, which can help explain the observed shallow spectral shape at wavelengths $>5000\\ \\textrm{\\AA}$ for the Sloan Digital Sky Survey quasar composite spectrum.","Second, the half-light radius of a given relatively long wavelength is significantly larger than for an AGN disk without sBHs, which can be tested by microlensing observations.","With appropriate sBH distributions, the model can be reconciled with quasar microlensing disk sizes.","We propose that the half-light radius-wavelength relation can be utilized to investigate the distributions of embedded sBHs in AGN accretion disks."],"url":"http://arxiv.org/abs/2404.07407v1","category":"astro-ph.HE"}
{"created":"2024-04-11 00:45:15","title":"New SiS destruction and formation routes via neutral-neutral reactions and their fundamental role in interstellar clouds at low and high metallicity values","abstract":"Among the silicon bearing species discovered in the interstellar medium, SiS and SiO stand out as key tracers due to their distinct chemistry and abundances in interstellar and circumstellar environments. Our objective is to enhance the network of Si- and S-bearing chemical reactions for a gas-grain model in molecular clouds, encompassing both low and high metallicities. We have calculated the energies and rate coefficients for 6 neutral atom-diatom reactions involved in the SiCS triatomic system, with a special focus on the C+SiS and S+SiC collisions. We employ the coupled cluster method with single and double substitutions and a perturbative treatment of triple substitutions (CCSD(T)) refined at the explicitly correlated CCSD(T)-F12 level. With these computational results in conjunction with data from the literature, we construct an extended network of neutral-neutral chemical reactions. We performed time-dependent models employing the Nautilus gas-grain code, setting the gas temperature to 10 K and the density to 2x10$^4$ cm$^{-3}$. The temperature dependence for the reactions involving SiS were modelled using $k(T)=\\alpha \\left( T/300 \\right)^{\\beta} \\exp{(-\\gamma/T)}$. The high-metallicity models significantly boost the SiS production. Higher initial abundances of C, S, and Si, roughly $\\sim$2, 190, and 210 times higher, respectively, contribute to this. Around 10$^3$ yr, destruction mechanisms become relevant. The proposed production reaction S + SiC $\\rightarrow$ C + SiS, mitigates these effects. By expanding the gas reaction network using a high metallicity model, we derived estimates for the abundances of interstellar molecules. The inclusion of neutral-neutral mechanisms, particularly via Si+HS and S+SiC channels, played a pivotal role in determining SiS abundance. These mechanisms carry a significance on a par with the well-known and fast ion-neutral reactions.","sentences":["Among the silicon bearing species discovered in the interstellar medium, SiS and SiO stand out as key tracers due to their distinct chemistry and abundances in interstellar and circumstellar environments.","Our objective is to enhance the network of Si- and S-bearing chemical reactions for a gas-grain model in molecular clouds, encompassing both low and high metallicities.","We have calculated the energies and rate coefficients for 6 neutral atom-diatom reactions involved in the SiCS triatomic system, with a special focus on the C+SiS and S+SiC collisions.","We employ the coupled cluster method with single and double substitutions and a perturbative treatment of triple substitutions (CCSD(T)) refined at the explicitly correlated CCSD(T)-F12 level.","With these computational results in conjunction with data from the literature, we construct an extended network of neutral-neutral chemical reactions.","We performed time-dependent models employing the Nautilus gas-grain code, setting the gas temperature to 10 K and the density to 2x10$^4$ cm$^{-3}$. The temperature dependence for the reactions involving SiS were modelled using $k(T)=\\alpha \\left( T/300 \\right)^{\\beta} \\exp{(-\\gamma/T)}$. The high-metallicity models significantly boost the SiS production.","Higher initial abundances of C, S, and Si, roughly $\\sim$2, 190, and 210 times higher, respectively, contribute to this.","Around 10$^3$ yr",", destruction mechanisms become relevant.","The proposed production reaction S + SiC $\\rightarrow$ C + SiS, mitigates these effects.","By expanding the gas reaction network using a high metallicity model, we derived estimates for the abundances of interstellar molecules.","The inclusion of neutral-neutral mechanisms, particularly via Si+HS and S+SiC channels, played a pivotal role in determining SiS abundance.","These mechanisms carry a significance on a par with the well-known and fast ion-neutral reactions."],"url":"http://arxiv.org/abs/2404.07406v1","category":"astro-ph.GA"}
{"created":"2024-04-11 00:40:27","title":"Statistical Analysis of High-frequency Whistler Waves at Earth's Bow Shock: Further Support for Stochastic Shock Drift Acceleration","abstract":"We statistically investigate high-frequency whistler waves (with frequencies higher than $\\sim 10$ % of the local elect ron cyclotron frequency) at Earth's bow shock using Magnetospheric Multi-Scale (MMS) spacecraft observations. We focus specifically on the wave power within the shock transition layer, where we expect electron acceleration via stochastic sh ock drift acceleration (SSDA) to occur associated with efficient pitch-angle scattering by whistler waves. We find that the wave power is positively correlated with both the Alfv\\'en Mach number in the normal incidence frame $M_{\\rm A}$ and in the de Hoffmann-Teller frame $M_{\\rm A}/\\cos \\theta_{Bn}$. The empirical relation with $M_{\\rm A}/\\cos \\theta_{Bn}$ is compared with the theory of SSDA that predicts a threshold wave power proportional to $(M_{\\rm A}/\\cos \\theta_{Bn})^{-2}$. The result suggests that the wave power exceeds the theoretical threshold for $M_{\\rm A} / \\cos \\theta_{Bn} \\gtrsim 30-60$, beyond which efficient electron acceleration is expected. This aligns very well with previous statistical analysis of electron acceleration at Earth's bow shock (M. Oka, G eophys.~Res.~Lett., 33, 5, 2006). Therefore, we consider that this study provides further support for SSDA as the mechanism of electron acceleration at Earth's bow shock. At higher-Mach-number astrophysical shocks, SSDA will be able to inject electrons into the diffusive shock acceleration process for subsequent acceleration to cosmic-ray energies.","sentences":["We statistically investigate high-frequency whistler waves (with frequencies higher than $\\sim 10$ % of the local elect ron cyclotron frequency) at Earth's bow shock using Magnetospheric Multi-Scale (MMS) spacecraft observations.","We focus specifically on the wave power within the shock transition layer, where we expect electron acceleration via stochastic sh ock drift acceleration (SSDA) to occur associated with efficient pitch-angle scattering by whistler waves.","We find that the wave power is positively correlated with both the Alfv\\'en Mach number in the normal incidence frame $M_{\\rm A}$ and","in the de Hoffmann-Teller frame $M_{\\rm A}/\\cos \\theta_{Bn}$. The empirical relation with $M_{\\rm A}/\\cos \\theta_{Bn}$","is compared with the theory of SSDA that predicts a threshold wave power proportional to $(M_{\\rm A}/\\cos \\theta_{Bn})^{-2}$.","The result suggests that the wave power exceeds the theoretical threshold for $M_{\\rm A} / \\cos \\theta_{Bn} \\gtrsim 30-60$, beyond which efficient electron acceleration is expected.","This aligns very well with previous statistical analysis of electron acceleration at Earth's bow shock (M. Oka, G eophys.~Res.~Lett., 33, 5, 2006).","Therefore, we consider that this study provides further support for SSDA as the mechanism of electron acceleration at Earth's bow shock.","At higher-Mach-number astrophysical shocks, SSDA will be able to inject electrons into the diffusive shock acceleration process for subsequent acceleration to cosmic-ray energies."],"url":"http://arxiv.org/abs/2404.07404v1","category":"physics.space-ph"}
{"created":"2024-04-11 00:19:15","title":"Implementation of implicit filter for spatial spectra extraction","abstract":"Scale analysis based on coarse-graining has been proposed recently as an alternative to Fourier analysis. It is now broadly used to analyze energy spectra and energy transfers in eddy-resolving ocean simulations. However, for data from unstructured-mesh models it requires interpolation to a regular grid. We present a high-performance Python implementation of an alternative coarse-graining method which relies on implicit filters using discrete Laplacians. This method can work on arbitrary (structured or unstructured) meshes and is applicable to the direct output of unstructured-mesh ocean circulation atmosphere models. The computation is split into two phases: preparation and solving. The first one is specific only to the mesh. This allows for auxiliary arrays that are then computed to be reused, significantly reducing the computation time. The second part consists of sparse matrix algebra and solving linear system. Our implementation is accelerated by GPUs to achieve unmatched performance and scalability. This results in processing data based on meshes with more than 10M surface vertices in a matter of seconds. As an illustration, the method is applied to compute spatial spectra of ocean currents from high-resolution FESOM2 simulations.","sentences":["Scale analysis based on coarse-graining has been proposed recently as an alternative to Fourier analysis.","It is now broadly used to analyze energy spectra and energy transfers in eddy-resolving ocean simulations.","However, for data from unstructured-mesh models it requires interpolation to a regular grid.","We present a high-performance Python implementation of an alternative coarse-graining method which relies on implicit filters using discrete Laplacians.","This method can work on arbitrary (structured or unstructured) meshes and is applicable to the direct output of unstructured-mesh ocean circulation atmosphere models.","The computation is split into two phases: preparation and solving.","The first one is specific only to the mesh.","This allows for auxiliary arrays that are then computed to be reused, significantly reducing the computation time.","The second part consists of sparse matrix algebra and solving linear system.","Our implementation is accelerated by GPUs to achieve unmatched performance and scalability.","This results in processing data based on meshes with more than 10M surface vertices in a matter of seconds.","As an illustration, the method is applied to compute spatial spectra of ocean currents from high-resolution FESOM2 simulations."],"url":"http://arxiv.org/abs/2404.07398v1","category":"physics.ao-ph"}
