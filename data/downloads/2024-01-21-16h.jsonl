{"created":"2024-01-18 18:59:19","title":"A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting","abstract":"Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.","sentences":["Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks.","This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities.","Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space.","The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation.","The experimental validation yields promising results for both panoptic segmentation and mask inpainting.","While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties."],"url":"http://arxiv.org/abs/2401.10227v1","category":"cs.CV"}
{"created":"2024-01-18 18:59:11","title":"ChatQA: Building GPT-4 Level Conversational QA Models","abstract":"In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.","sentences":["In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies.","Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs).","To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost.","Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models."],"url":"http://arxiv.org/abs/2401.10225v1","category":"cs.CL"}
{"created":"2024-01-18 18:59:05","title":"Free energy in spin glass models with conventional order","abstract":"Recently, [arXiv:2302.01361] considered spin glass models with additional conventional order parameters characterizing single-replica properties. These parameters are distinct from the standard order parameter used to measure correlations between replicas. A \"min-max\" formula for the free energy was prescribed in [arXiv:2302.01361]. We rigorously verify this prescription in the setting of vector spin glass models featuring additional deterministic spin interactions. Notably, our results can be viewed as a generalization of the Parisi formula for vector spin glass models in [arXiv:1512.04441], where the order parameter for self-overlap is already present.","sentences":["Recently, [arXiv:2302.01361] considered spin glass models with additional conventional order parameters characterizing single-replica properties.","These parameters are distinct from the standard order parameter used to measure correlations between replicas.","A \"min-max\" formula for the free energy was prescribed in [arXiv:2302.01361].","We rigorously verify this prescription in the setting of vector spin glass models featuring additional deterministic spin interactions.","Notably, our results can be viewed as a generalization of the Parisi formula for vector spin glass models in [arXiv:1512.04441], where the order parameter for self-overlap is already present."],"url":"http://arxiv.org/abs/2401.10223v1","category":"cond-mat.dis-nn"}
{"created":"2024-01-18 18:58:54","title":"Supervised Fine-tuning in turn Improves Visual Foundation Models","abstract":"Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.","sentences":["Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years.","Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets.","Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining.","Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models.","In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks.","With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios."],"url":"http://arxiv.org/abs/2401.10222v1","category":"cs.CV"}
{"created":"2024-01-18 18:58:49","title":"AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data","abstract":"Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\\%$ and $1.5\\%$, respectively.","sentences":["Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data.","However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions.","Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model.","Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other.","We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning.","AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set.","To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values.","We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts.","Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods.","Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\\%$ and $1.5\\%$, respectively."],"url":"http://arxiv.org/abs/2401.10220v1","category":"cs.CV"}
{"created":"2024-01-18 18:45:29","title":"Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems","abstract":"This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniques from pedagogical and decompositional approaches, eclectic rule extraction leverages the advantages of both, while mitigating some of their drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as a white box surrogate explainer for black box Deep Neural Networks (DNN). The presented eclectic RE algorithm extracts human-readable rules from hidden layers, facilitating explainable and trustworthy rulesets. Evaluations on UNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to generate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions of this work include the hybrid X-IDS architecture, the eclectic rule extraction algorithm applicable to intrusion detection datasets, and a thorough analysis of performance and explainability, demonstrating the trade-offs involved in rule extraction speed and accuracy.","sentences":["This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS).","While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust.","The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand.","To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE).","There are three types of RE algorithms: pedagogical, decompositional, and eclectic.","Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability.","This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness.","By combining techniques from pedagogical and decompositional approaches, eclectic rule extraction leverages the advantages of both, while mitigating some of their drawbacks.","The proposed Hybrid X-IDS architecture features eclectic RE as a white box surrogate explainer for black box Deep Neural Networks (DNN).","The presented eclectic RE algorithm extracts human-readable rules from hidden layers, facilitating explainable and trustworthy rulesets.","Evaluations on UNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to generate rulesets with 99.9% accuracy, mimicking DNN outputs.","The contributions of this work include the hybrid X-IDS architecture, the eclectic rule extraction algorithm applicable to intrusion detection datasets, and a thorough analysis of performance and explainability, demonstrating the trade-offs involved in rule extraction speed and accuracy."],"url":"http://arxiv.org/abs/2401.10207v1","category":"cs.CR"}
{"created":"2024-01-18 18:20:15","title":"Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction","abstract":"Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.","sentences":["Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges.","First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities.","Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types.","In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges.","Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities.","Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence.","Besides, we design a new contrastive loss to reduce excessive copying during the extraction process.","Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema.","Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively."],"url":"http://arxiv.org/abs/2401.10189v1","category":"cs.CL"}
{"created":"2024-01-18 18:06:22","title":"Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields","abstract":"In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.","sentences":["In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina.","We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence.","Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields.","Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights.","This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks."],"url":"http://arxiv.org/abs/2401.10178v1","category":"cs.CV"}
{"created":"2024-01-18 17:22:11","title":"Explicitly Disentangled Representations in Object-Centric Learning","abstract":"Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.","sentences":["Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning.","Recently, techniques for unsupervised learning of object-centric representations have raised growing interest.","In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks.","A promising step in this direction is to disentangle the factors that cause variation in the data.","Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features.","Extending this approach, we focus on separating the shape and texture components.","In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions.","These subsets are known a priori, hence before the training process.","Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases.","In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes."],"url":"http://arxiv.org/abs/2401.10148v1","category":"cs.CV"}
{"created":"2024-01-18 17:06:21","title":"Model Compression Techniques in Biometrics Applications: A Survey","abstract":"The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.","sentences":["The development of deep learning algorithms has extensively empowered humanity's task automatization capacity.","However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices.","This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation.","This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning.","We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods.","Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works."],"url":"http://arxiv.org/abs/2401.10139v1","category":"cs.CV"}
{"created":"2024-01-18 16:54:49","title":"Lower Ricci Curvature for Efficient Community Detection","abstract":"This study introduces the Lower Ricci Curvature (LRC), a novel, scalable, and scale-free discrete curvature designed to enhance community detection in networks. Addressing the computational challenges posed by existing curvature-based methods, LRC offers a streamlined approach with linear computational complexity, making it well-suited for large-scale network analysis. We further develop an LRC-based preprocessing method that effectively augments popular community detection algorithms. Through comprehensive simulations and applications on real-world datasets, including the NCAA football league network, the DBLP collaboration network, the Amazon product co-purchasing network, and the YouTube social network, we demonstrate the efficacy of our method in significantly improving the performance of various community detection algorithms.","sentences":["This study introduces the Lower Ricci Curvature (LRC), a novel, scalable, and scale-free discrete curvature designed to enhance community detection in networks.","Addressing the computational challenges posed by existing curvature-based methods, LRC offers a streamlined approach with linear computational complexity, making it well-suited for large-scale network analysis.","We further develop an LRC-based preprocessing method that effectively augments popular community detection algorithms.","Through comprehensive simulations and applications on real-world datasets, including the NCAA football league network, the DBLP collaboration network, the Amazon product co-purchasing network, and the YouTube social network, we demonstrate the efficacy of our method in significantly improving the performance of various community detection algorithms."],"url":"http://arxiv.org/abs/2401.10124v1","category":"stat.ME"}
{"created":"2024-01-18 16:44:57","title":"Interpreting deviations between AR-VTG and GR","abstract":"The Cosmic microwave background (CMB) anisotropies predicted by two cosmological models are compared, one of them is the standard model of general relativity with cold dark matter and cosmological constant, whereas the second model is based on a consistent vector-tensor theory of gravitation explaining solar system and cosmological observations. It is proved that the resulting differences -- between the anisotropies of both models -- are due to the so-called late integrated Sachs Wolfe effect and, consequently, cross correlations between maps of CMB temperatures and tracers of the dark matter distribution could be used in future to select one of the above models. The role of reionization is analysed in detail.","sentences":["The Cosmic microwave background (CMB) anisotropies predicted by two cosmological models are compared, one of them is the standard model of general relativity with cold dark matter and cosmological constant, whereas the second model is based on a consistent vector-tensor theory of gravitation explaining solar system and cosmological observations.","It is proved that the resulting differences -- between the anisotropies of both models -- are due to the so-called late integrated Sachs Wolfe effect and, consequently, cross correlations between maps of CMB temperatures and tracers of the dark matter distribution could be used in future to select one of the above models.","The role of reionization is analysed in detail."],"url":"http://arxiv.org/abs/2401.10116v1","category":"astro-ph.CO"}
